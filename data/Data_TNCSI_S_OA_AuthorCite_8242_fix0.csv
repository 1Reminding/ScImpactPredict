title,TNCSI,abstract,OA,authors_title
Textual Entailment Recognition with Semantic Features from Empirical Text Representation,0.0,"Textual entailment recognition is one of the basic natural language
understanding(NLU) tasks. Understanding the meaning of sentences is a
prerequisite before applying any natural language processing(NLP) techniques to
automatically recognize the textual entailment. A text entails a hypothesis if
and only if the true value of the hypothesis follows the text. Classical
approaches generally utilize the feature value of each word from word embedding
to represent the sentences. In this paper, we propose a novel approach to
identifying the textual entailment relationship between text and hypothesis,
thereby introducing a new semantic feature focusing on empirical
threshold-based semantic text representation. We employ an element-wise
Manhattan distance vector-based feature that can identify the semantic
entailment relationship between the text-hypothesis pair. We carried out
several experiments on a benchmark entailment classification(SICK-RTE) dataset.
We train several machine learning(ML) algorithms applying both semantic and
lexical features to classify the text-hypothesis pair as entailment, neutral,
or contradiction. Our empirical sentence representation technique enriches the
semantic information of the texts and hypotheses found to be more efficient
than the classical ones. In the end, our approach significantly outperforms
known methods in understanding the meaning of the sentences for the textual
entailment classification task.",None,57
Insights on Neural Representations for End-to-End Speech Recognition,0.00654443,"End-to-end automatic speech recognition (ASR) models aim to learn a
generalised speech representation. However, there are limited tools available
to understand the internal functions and the effect of hierarchical
dependencies within the model architecture. It is crucial to understand the
correlations between the layer-wise representations, to derive insights on the
relationship between neural representations and performance.
  Previous investigations of network similarities using correlation analysis
techniques have not been explored for End-to-End ASR models. This paper
analyses and explores the internal dynamics between layers during training with
CNN, LSTM and Transformer based approaches using Canonical correlation analysis
(CCA) and centered kernel alignment (CKA) for the experiments. It was found
that neural representations within CNN layers exhibit hierarchical correlation
dependencies as layer depth increases but this is mostly limited to cases where
neural representation correlates more closely. This behaviour is not observed
in LSTM architecture, however there is a bottom-up pattern observed across the
training process, while Transformer encoder layers exhibit irregular
coefficiency correlation as neural depth increases. Altogether, these results
provide new insights into the role that neural architectures have upon speech
recognition performance. More specifically, these techniques can be used as
indicators to build better performing speech recognition models.",None,14439
Linearizing Transformer with Key-Value Memory,0.2387,"Efficient transformer variants with linear time complexity have been
developed to mitigate the quadratic computational overhead of the vanilla
transformer. Among them are low-rank projection methods such as Linformer and
kernel-based Transformers. Despite their unique merits, they usually suffer
from a performance drop comparing with the vanilla transformer on many sequence
generation tasks, and often fail to obtain computation gain when the generation
is short. We propose MemSizer, an approach towards closing the performance gap
while improving the efficiency even with short generation. It projects the
source sequences into lower dimension representations like Linformer, while
enjoying efficient recurrent-style incremental computation similar to
kernel-based transformers. This yields linear computation time and constant
memory complexity at inference time. MemSizer also employs a lightweight
multi-head mechanism which renders the computation as light as a single-head
model. We demonstrate that MemSizer provides an improved balance between
efficiency and accuracy over the vanilla transformer and other efficient
transformer variants in three typical sequence generation tasks, including
machine translation, abstractive text summarization, and language modeling.",None,2710
Automated Audio Captioning with Epochal Difficult Captions for Curriculum Learning,0.0431766,"In this paper, we propose an algorithm, Epochal Difficult Captions, to
supplement the training of any model for the Automated Audio Captioning task.
Epochal Difficult Captions is an elegant evolution to the keyword estimation
task that previous work have used to train the encoder of the AAC model.
Epochal Difficult Captions modifies the target captions based on a curriculum
and a difficulty level determined as a function of current epoch. Epochal
Difficult Captions can be used with any model architecture and is a lightweight
function that does not increase training time. We test our results on three
systems and show that using Epochal Difficult Captions consistently improves
performance",https://github.com/nltk/nltk,8436
Data augmentation for efficient learning from parametric experts,0.0089734,"We present a simple, yet powerful data-augmentation technique to enable
data-efficient learning from parametric experts for reinforcement and imitation
learning. We focus on what we call the policy cloning setting, in which we use
online or offline queries of an expert or expert policy to inform the behavior
of a student policy. This setting arises naturally in a number of problems, for
instance as variants of behavior cloning, or as a component of other algorithms
such as DAGGER, policy distillation or KL-regularized RL. Our approach,
augmented policy cloning (APC), uses synthetic states to induce
feedback-sensitivity in a region around sampled trajectories, thus dramatically
reducing the environment interactions required for successful cloning of the
expert. We achieve highly data-efficient transfer of behavior from an expert to
a student policy for high-degrees-of-freedom control problems. We demonstrate
the benefit of our method in the context of several existing and widely used
algorithms that include policy cloning as a constituent part. Moreover, we
highlight the benefits of our approach in two practically relevant settings (a)
expert compression, i.e. transfer to a student with fewer parameters; and (b)
transfer from privileged experts, i.e. where the expert has a different
observation space than the student, usually including access to privileged
information.",None,49708
Generating Explanations from Deep Reinforcement Learning Using Episodic Memory,0.0366845,"Deep Reinforcement Learning (RL) involves the use of Deep Neural Networks
(DNNs) to make sequential decisions in order to maximize reward. For many tasks
the resulting sequence of actions produced by a Deep RL policy can be long and
difficult to understand for humans. A crucial component of human explanations
is selectivity, whereby only key decisions and causes are recounted. Imbuing
Deep RL agents with such an ability would make their resulting policies easier
to understand from a human perspective and generate a concise set of
instructions to aid the learning of future agents. To this end we use a Deep RL
agent with an episodic memory system to identify and recount key decisions
during policy execution. We show that these decisions form a short, human
readable explanation that can also be used to speed up the learning of naive
Deep RL agents in an algorithm-independent manner.",None,8495
Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding,0.00501456,"In recent years, large pre-trained Transformer networks have demonstrated
dramatic improvements in many natural language understanding tasks. However,
the huge size of these models brings significant challenges to their
fine-tuning and online deployment due to latency and cost constraints. New
hardware supporting both N:M semi-structured sparsity and low-precision integer
computation is a promising solution to boost DNN model serving efficiency.
However, there have been very few studies that systematically investigate to
what extent pre-trained Transformer networks benefit from the combination of
these techniques, as well as how to best compress each component of the
Transformer. We propose a flexible compression framework NxMiFormer that
performs simultaneous sparsification and quantization using ADMM and STE-based
QAT. Furthermore, we present and inexpensive, heuristic-driven search algorithm
that identifies promising heterogeneous compression configurations that meet a
compression ratio constraint. When evaluated across the GLUE suite of NLU
benchmarks, our approach can achieve up to 93% compression of the encoders of a
BERT model while retaining 98.2% of the original model accuracy and taking full
advantage of the hardware's capabilities. Heterogeneous configurations found
the by the search heuristic maintain 99.5% of the baseline accuracy while still
compressing the model by 87.5%.",None,7580
Dynamic Point Cloud Compression with Cross-Sectional Approach,0.063056,"The recent development of dynamic point clouds has introduced the possibility
of mimicking natural reality, and greatly assisting quality of life. However,
to broadcast successfully, the dynamic point clouds require higher compression
due to their huge volume of data compared to the traditional video. Recently,
MPEG finalized a Video-based Point Cloud Compression standard known as V-PCC.
However, V-PCC requires huge computational time due to expensive normal
calculation and segmentation, sacrifices some points to limit the number of 2D
patches, and cannot occupy all spaces in the 2D frame. The proposed method
addresses these limitations by using a novel cross-sectional approach. This
approach reduces expensive normal estimation and segmentation, retains more
points, and utilizes more spaces for 2D frame generation compared to the VPCC.
The experimental results using standard video sequences show that the proposed
technique can achieve better compression in both geometric and texture data
compared to the V-PCC standard.",None,5037
P$^3$LM: Probabilistically Permuted Prophet Language Modeling for Generative Pre-Training,0.0108149,"Conventional autoregressive left-to-right (L2R) sequence generation faces two
issues during decoding: limited to unidirectional target sequence modeling, and
constrained on strong local dependencies. To address the aforementioned
problem, we propose P$^3$LM, a probabilistically permuted prophet language
model, which strengthens the modeling of bidirectional information and long
token dependencies for sequence generation. Specifically, P$^3$LM learns to
generate tokens in permuted order upon an order-aware transformer decoder, as
well as to generate the corresponding future $N$ tokens with a multi-stream
attention mechanism. Extensive experiments are conducted on the GLGE benchmark,
which includes four datasets for summarization, two for question generation,
one for conversational question answering, and one for dialog response
generation, where P$^3$LM achieves state-of-the-art results compared with
strong publicly available generative pre-training methods.",https://github.com/JunweiBao/P3LM,53018
Recognition of Implicit Geographic Movement in Text,0.0282515,"Analyzing the geographic movement of humans, animals, and other phenomena is
a growing field of research. This research has benefited urban planning,
logistics, animal migration understanding, and much more. Typically, the
movement is captured as precise geographic coordinates and time stamps with
Global Positioning Systems (GPS). Although some research uses computational
techniques to take advantage of implicit movement in descriptions of route
directions, hiking paths, and historical exploration routes, innovation would
accelerate with a large and diverse corpus. We created a corpus of sentences
labeled as describing geographic movement or not and including the type of
entity moving. Creating this corpus proved difficult without any comparable
corpora to start with, high human labeling costs, and since movement can at
times be interpreted differently. To overcome these challenges, we developed an
iterative process employing hand labeling, crowd voting for confirmation, and
machine learning to predict more labels. By merging advances in word embeddings
with traditional machine learning models and model ensembling, prediction
accuracy is at an acceptable level to produce a large silver-standard corpus
despite the small gold-standard corpus training set. Our corpus will likely
benefit computational processing of geography in text and spatial cognition, in
addition to detection of movement.",None,1280
Multi-level Latent Space Structuring for Generative Control,0.0072254,"Truncation is widely used in generative models for improving the quality of
the generated samples, at the expense of reducing their diversity. We propose
to leverage the StyleGAN generative architecture to devise a new truncation
technique, based on a decomposition of the latent space into clusters, enabling
customized truncation to be performed at multiple semantic levels. We do so by
learning to re-generate W-space, the extended intermediate latent space of
StyleGAN, using a learnable mixture of Gaussians, while simultaneously training
a classifier to identify, for each latent vector, the cluster that it belongs
to. The resulting truncation scheme is more faithful to the original
untruncated samples and allows a better trade-off between quality and
diversity. We compare our method to other truncation approaches for StyleGAN,
both qualitatively and quantitatively.",None,62088
"Effectiveness of Text, Acoustic, and Lattice-based representations in Spoken Language Understanding tasks",0.0301609,"In this paper, we perform an exhaustive evaluation of different
representations to address the intent classification problem in a Spoken
Language Understanding (SLU) setup. We benchmark three types of systems to
perform the SLU intent detection task: 1) text-based, 2) lattice-based, and a
novel 3) multimodal approach. Our work provides a comprehensive analysis of
what could be the achievable performance of different state-of-the-art SLU
systems under different circumstances, e.g., automatically- vs.
manually-generated transcripts. We evaluate the systems on the publicly
available SLURP spoken language resource corpus. Our results indicate that
using richer forms of Automatic Speech Recognition (ASR) outputs, namely
word-consensus-networks, allows the SLU system to improve in comparison to the
1-best setup (5.5% relative improvement). However, crossmodal approaches, i.e.,
learning from acoustic and text embeddings, obtains performance similar to the
oracle setup, a relative improvement of 17.8% over the 1-best configuration,
being a recommended alternative to overcome the limitations of working with
automatically generated transcripts.",https://github.com/idiap/slu_representations,10700
Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability,0.0415954,"Great endeavors have been made to study AI's ability in abstract reasoning,
along with which different versions of RAVEN's progressive matrices (RPM) are
proposed as benchmarks. Previous works give inkling that without sophisticated
design or extra meta-data containing semantic information, neural networks may
still be indecisive in making decisions regarding RPM problems, after
relentless training. Evidenced by thorough experiments and ablation studies, we
showcase that end-to-end neural networks embodied with felicitous inductive
bias, intentionally design or serendipitously match, can solve RPM problems
elegantly, without the augment of any extra meta-data or preferences of any
specific backbone. Our work also reveals that multi-viewpoint with
multi-evaluation is a key learning strategy for successful reasoning. Finally,
potential explanations for the failure of connectionist models in
generalization are provided. We hope that these results will serve as
inspections of AI's ability beyond perception and toward abstract reasoning.
Source code can be found in https://github.com/QinglaiWeiCASIA/RavenSolver.",https://github.com/QinglaiWeiCASIA/RavenSolver,13516
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction,0.0315331,"Relation extraction (RE), which has relied on structurally annotated corpora
for model training, has been particularly challenging in low-resource scenarios
and domains. Recent literature has tackled low-resource RE by self-supervised
learning, where the solution involves pretraining the entity pair embedding by
RE-based objective and finetuning on labeled data by classification-based
objective. However, a critical challenge to this approach is the gap in
objectives, which prevents the RE model from fully utilizing the knowledge in
pretrained representations. In this paper, we aim at bridging the gap and
propose to pretrain and finetune the RE model using consistent objectives of
contrastive learning. Since in this kind of representation learning paradigm,
one relation may easily form multiple clusters in the representation space, we
further propose a multi-center contrastive loss that allows one relation to
form multiple clusters to better align with pretraining. Experiments on two
document-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness
of our method. Particularly, when using 1% end-task training data, our method
outperforms PLM-based RE classifier by 10.5% and 6.1% on the two datasets,
respectively.",None,10388
Winning the CVPR'2022 AQTC Challenge: A Two-stage Function-centric Approach,0.0251057,"Affordance-centric Question-driven Task Completion for Egocentric
Assistant(AQTC) is a novel task which helps AI assistant learn from
instructional videos and scripts and guide the user step-by-step. In this
paper, we deal with the AQTC via a two-stage Function-centric approach, which
consists of Question2Function Module to ground the question with the related
function and Function2Answer Module to predict the action based on the
historical steps. We evaluated several possible solutions in each module and
obtained significant gains compared to the given baselines. Our code is
available at \url{https://github.com/starsholic/LOVEU-CVPR22-AQTC}.",https://github.com/starsholic/LOVEU-CVPR22-AQTC,27779
Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages,0.0553044,"Translation Quality Estimation (QE) is the task of predicting the quality of
machine translation (MT) output without any reference. This task has gained
increasing attention as an important component in the practical applications of
MT. In this paper, we first propose XLMRScore, which is a cross-lingual
counterpart of BERTScore computed via the XLM-RoBERTa (XLMR) model. This metric
can be used as a simple unsupervised QE method, nevertheless facing two issues:
firstly, the untranslated tokens leading to unexpectedly high translation
scores, and secondly, the issue of mismatching errors between source and
hypothesis tokens when applying the greedy matching in XLMRScore. To mitigate
these issues, we suggest replacing untranslated words with the unknown token
and the cross-lingual alignment of the pre-trained model to represent aligned
words closer to each other, respectively. We evaluate the proposed method on
four low-resource language pairs of the WMT21 QE shared task, as well as a new
English$\rightarrow$Persian (En-Fa) test dataset introduced in this paper.
Experiments show that our method could get comparable results with the
supervised baseline for two zero-shot scenarios, i.e., with less than 0.01
difference in Pearson correlation, while outperforming unsupervised rivals in
all the low-resource language pairs for above 8%, on average.",https://github.com/fatemeh-azadi/Unsupervised-QE,1410
Visual Information Guided Zero-Shot Paraphrase Generation,0.0211587,"Zero-shot paraphrase generation has drawn much attention as the large-scale
high-quality paraphrase corpus is limited. Back-translation, also known as the
pivot-based method, is typical to this end. Several works leverage different
information as ""pivot"" such as language, semantic representation and so on. In
this paper, we explore using visual information such as image as the ""pivot"" of
back-translation. Different with the pipeline back-translation method, we
propose visual information guided zero-shot paraphrase generation (ViPG) based
only on paired image-caption data. It jointly trains an image captioning model
and a paraphrasing model and leverage the image captioning model to guide the
training of the paraphrasing model. Both automatic evaluation and human
evaluation show our model can generate paraphrase with good relevancy, fluency
and diversity, and image is a promising kind of pivot for zero-shot paraphrase
generation.",https://github.com/L-Zhe/ViPG,11436
Analytical Solutions for the Inverse Problem within Gradual Semantics,0.0492012,"Gradual semantics within abstract argumentation associate a numeric score
with every argument in a system, which represents the level of acceptability of
this argument, and from which a preference ordering over arguments can be
derived. While some semantics operate over standard argumentation frameworks,
many utilise a weighted framework, where a numeric initial weight is associated
with each argument. Recent work has examined the inverse problem within gradual
semantics. Rather than determining a preference ordering given an argumentation
framework and a semantics, the inverse problem takes an argumentation
framework, a gradual semantics, and a preference ordering as inputs, and
identifies what weights are needed to over arguments in the framework to obtain
the desired preference ordering. Existing work has attacked the inverse problem
numerically, using a root finding algorithm (the bisection method) to identify
appropriate initial weights. In this paper we demonstrate that for a class of
gradual semantics, an analytical approach can be used to solve the inverse
problem. Unlike the current state-of-the-art, such an analytic approach can
rapidly find a solution, and is guaranteed to do so. In obtaining this result,
we are able to prove several important properties which previous work had posed
as conjectures.",None,3518
Self-directed Learning of Action Models using Exploratory Planning,0.0171364,"Complex, real-world domains may not be fully modeled for an agent, especially
if the agent has never operated in the domain before. The agent's ability to
effectively plan and act in such a domain is influenced by its knowledge of
when it can perform specific actions and the effects of those actions. We
describe a novel exploratory planning agent that is capable of learning action
preconditions and effects without expert traces or a given goal. The agent's
architecture allows it to perform both exploratory actions as well as
goal-directed actions, which opens up important considerations for how
exploratory planning and goal planning should be controlled, as well as how the
agent's behavior should be explained to any teammates it may have. The
contributions of this work include a new representation for contexts called
Lifted Linked Clauses, a novel exploration action selection approach using
these clauses, an exploration planner that uses lifted linked clauses as goals
in order to reach new states, and an empirical evaluation in a scenario from an
exploration-focused video game demonstrating that lifted linked clauses improve
exploration and action model learning against non-planning baseline agents.",None,24302
Classification Of Fake News Headline Based On Neural Networks,0.0449038,"Over the last few years, Text classification is one of the fundamental tasks
in natural language processing (NLP) in which the objective is to categorize
text documents into one of the predefined classes. The news is full of our
life. Therefore, news headlines classification is a crucial task to connect
users with the right news. The news headline classification is a kind of text
classification, which can be generally divided into three mainly parts: feature
extraction, classifier selection, and evaluations. In this article, we use the
dataset, containing news over a period of eighteen years provided by Kaggle
platform to classify news headlines. We choose TF-IDF to extract features and
neural network as the classifier, while the evaluation metrics is accuracy.
From the experiment result, it is obvious that our NN model has the best
performance among these models in the metrics of accuracy. The higher the
accuracy is, the better performance the model will gain. Our NN model owns the
accuracy 0.8622, which is highest accuracy among these four models. And it is
0.0134, 0.033, 0.080 higher than its of other models.",None,33
Task-Adaptive Feature Transformer with Semantic Enrichment for Few-Shot Segmentation,0.0545921,"Few-shot learning allows machines to classify novel classes using only a few
labeled samples. Recently, few-shot segmentation aiming at semantic
segmentation on low sample data has also seen great interest. In this paper, we
propose a learnable module that can be placed on top of existing segmentation
networks for performing few-shot segmentation. This module, called the
task-adaptive feature transformer (TAFT), linearly transforms task-specific
high-level features to a set of task agnostic features well-suited to
conducting few-shot segmentation. The task-conditioned feature transformation
allows an effective utilization of the semantic information in novel classes to
generate tight segmentation masks. We also propose a semantic enrichment (SE)
module that utilizes a pixel-wise attention module for high-level feature and
an auxiliary loss from an auxiliary segmentation network conducting the
semantic segmentation for all training classes. Experiments on PASCAL-$5^i$ and
COCO-$20^i$ datasets confirm that the added modules successfully extend the
capability of existing segmentators to yield highly competitive few-shot
segmentation performances.",None,5113
Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective,0.00906834,"One of the most fundamental questions in quantitative finance is the
existence of continuous-time diffusion models that fit market prices of a given
set of options. Traditionally, one employs a mix of intuition, theoretical and
empirical analysis to find models that achieve exact or approximate fits. Our
contribution is to show how a suitable game theoretical formulation of this
problem can help solve this question by leveraging existing developments in
modern deep multi-agent reinforcement learning to search in the space of
stochastic processes. Our experiments show that we are able to learn local
volatility, as well as path-dependence required in the volatility process to
minimize the price of a Bermudan option. Our algorithm can be seen as a
particle method \textit{\`{a} la} Guyon \textit{et} Henry-Labordere where
particles, instead of being designed to ensure $\sigma_{loc}(t,S_t)^2 =
\mathbb{E}[\sigma_t^2|S_t]$, are learning RL-driven agents cooperating towards
more general calibration targets.",None,226
Who is we? Disambiguating the referents of first person plural pronouns in parliamentary debates,0.0363318,"This paper investigates the use of first person plural pronouns as a
rhetorical device in political speeches. We present an annotation schema for
disambiguating pronoun references and use our schema to create an annotated
corpus of debates from the German Bundestag. We then use our corpus to learn to
automatically resolve pronoun referents in parliamentary debates. We explore
the use of data augmentation with weak supervision to further expand our corpus
and report preliminary results.",None,1325
Bipartite-play Dialogue Collection for Practical Automatic Evaluation of Dialogue Systems,0.0232073,"Automation of dialogue system evaluation is a driving force for the efficient
development of dialogue systems. This paper introduces the bipartite-play
method, a dialogue collection method for automating dialogue system evaluation.
It addresses the limitations of existing dialogue collection methods: (i)
inability to compare with systems that are not publicly available, and (ii)
vulnerability to cheating by intentionally selecting systems to be compared.
Experimental results show that the automatic evaluation using the
bipartite-play method mitigates these two drawbacks and correlates as strongly
with human subjectivity as existing methods.",None,933
Classifiers are Better Experts for Controllable Text Generation,0.00179473,"This paper proposes a simple method for controllable text generation based on
weighting logits with a free-form classifier, namely CAIF sampling. Using an
arbitrary text classifier, we adjust a small part of a language model's logits
and guide text generation towards or away from classifier prediction. We
experimented with toxicity avoidance and sentiment control tasks and showed
that the proposed method significantly outperforms recent PPLM, GeDi, and
DExperts on PPL and task accuracy metrics based on the external classifier of
generated texts. In addition, compared to other approaches, it is easier to
implement and tune and has significantly fewer restrictions and requirements.",None,98
Descartes: Generating Short Descriptions of Wikipedia Articles,0.00421647,"Wikipedia is one of the richest knowledge sources on the Web today. In order
to facilitate navigating, searching, and maintaining its content, Wikipedia's
guidelines state that all articles should be annotated with a so-called short
description indicating the article's topic (e.g., the short description of beer
is ""Alcoholic drink made from fermented cereal grains""). Nonetheless, a large
fraction of articles (ranging from 10.2% in Dutch to 99.7% in Kazakh) have no
short description yet, with detrimental effects for millions of Wikipedia
users. Motivated by this problem, we introduce the novel task of automatically
generating short descriptions for Wikipedia articles and propose Descartes, a
multilingual model for tackling it. Descartes integrates three sources of
information to generate an article description in a target language: the text
of the article in all its language versions, the already-existing descriptions
(if any) of the article in other languages, and semantic type information
obtained from a knowledge graph. We evaluate a Descartes model trained for
handling 25 languages simultaneously, showing that it beats baselines
(including a strong translation-based baseline) and performs on par with
monolingual models tailored for specific languages. A human evaluation on three
languages further shows that the quality of Descartes's descriptions is largely
indistinguishable from that of human-written descriptions; e.g., 91.3% of our
English descriptions (vs. 92.1% of human-written descriptions) pass the bar for
inclusion in Wikipedia, suggesting that Descartes is ready for production, with
the potential to support human editors in filling a major gap in today's
Wikipedia across languages.",None,13236
kpfriends at SemEval-2022 Task 2: NEAMER -- Named Entity Augmented Multi-word Expression Recognizer,0.0952839,"We present NEAMER -- Named Entity Augmented Multi-word Expression Recognizer.
This system is inspired by non-compositionality characteristics shared between
Named Entity and Idiomatic Expressions. We utilize transfer learning and
locality features to enhance idiom classification task. This system is our
submission for SemEval Task 2: Multilingual Idiomaticity Detection and Sentence
Embedding Subtask A OneShot shared task. We achieve SOTA with F1 0.9395 during
post-evaluation phase. We also observe improvement in training stability.
Lastly, we experiment with non-compositionality knowledge transfer,
cross-lingual fine-tuning and locality features, which we also introduce in
this paper.",None,-1
Far Away in the Deep Space: Dense Nearest-Neighbor-Based Out-of-Distribution Detection,0.0294703,"The key to out-of-distribution detection is density estimation of the
in-distribution data or of its feature representations. This is particularly
challenging for dense anomaly detection in domains where the in-distribution
data has a complex underlying structure. Nearest-Neighbors approaches have been
shown to work well in object-centric data domains, such as industrial
inspection and image classification. In this paper, we show that
nearest-neighbor approaches also yield state-of-the-art results on dense
novelty detection in complex driving scenes when working with an appropriate
feature representation. In particular, we find that transformer-based
architectures produce representations that yield much better similarity metrics
for the task. We identify the multi-head structure of these models as one of
the reasons, and demonstrate a way to transfer some of the improvements to
CNNs. Ultimately, the approach is simple and non-invasive, i.e., it does not
affect the primary segmentation performance, refrains from training on examples
of anomalies, and achieves state-of-the-art results on RoadAnomaly,
StreetHazards, and SegmentMeIfYouCan-Anomaly.",https://github.com/silviogalesso/dense-ood-knns,155608
A Comparative Attention Framework for Better Few-Shot Object Detection on Aerial Images,0.0177603,"Few-Shot Object Detection (FSOD) methods are mainly designed and evaluated on
natural image datasets such as Pascal VOC and MS COCO. However, it is not clear
whether the best methods for natural images are also the best for aerial
images. Furthermore, direct comparison of performance between FSOD methods is
difficult due to the wide variety of detection frameworks and training
strategies. Therefore, we propose a benchmarking framework that provides a
flexible environment to implement and compare attention-based FSOD methods. The
proposed framework focuses on attention mechanisms and is divided into three
modules: spatial alignment, global attention, and fusion layer. To remain
competitive with existing methods, which often leverage complex training, we
propose new augmentation techniques designed for object detection. Using this
framework, several FSOD methods are reimplemented and compared. This comparison
highlights two distinct performance regimes on aerial and natural images: FSOD
performs worse on aerial images. Our experiments suggest that small objects,
which are harder to detect in the few-shot setting, account for the poor
performance. Finally, we develop a novel multiscale alignment method,
Cross-Scales Query-Support Alignment (XQSA) for FSOD, to improve the detection
of small objects. XQSA outperforms the state-of-the-art significantly on DOTA
and DIOR.",https://github.com/pierlj/aaf,-1
Learning Explicit Object-Centric Representations with Vision Transformers,0.0137459,"With the recent successful adaptation of transformers to the vision domain,
particularly when trained in a self-supervised fashion, it has been shown that
vision transformers can learn impressive object-reasoning-like behaviour and
features expressive for the task of object segmentation in images. In this
paper, we build on the self-supervision task of masked autoencoding and explore
its effectiveness for explicitly learning object-centric representations with
transformers. To this end, we design an object-centric autoencoder using
transformers only and train it end-to-end to reconstruct full images from
unmasked patches. We show that the model efficiently learns to decompose simple
scenes as measured by segmentation metrics on several multi-object benchmarks.",None,2675
AI Autonomy : Self-Initiated Open-World Continual Learning and Adaptation,0.0600174,"As more and more AI agents are used in practice, it is time to think about
how to make these agents fully autonomous so that they can (1) learn by
themselves continually in a self-motivated and self-initiated manner rather
than being retrained offline periodically on the initiation of human engineers
and (2) accommodate or adapt to unexpected or novel circumstances. As the
real-world is an open environment that is full of unknowns or novelties, the
capabilities of detecting novelties, characterizing them,
accommodating/adapting to them, gathering ground-truth training data and
incrementally learning the unknowns/novelties become critical in making the AI
agent more and more knowledgeable, powerful and self-sustainable over time. The
key challenge here is how to automate the process so that it is carried out
continually on the agent's own initiative and through its own interactions with
humans, other agents and the environment just like human on-the-job learning.
This paper proposes a framework (called SOLA) for this learning paradigm to
promote the research of building autonomous and continual learning enabled AI
agents. To show feasibility, an implemented agent is also described.",None,101106
N-RPN: Hard Example Learning for Region Proposal Networks,0.0134039,"The region proposal task is to generate a set of candidate regions that
contain an object. In this task, it is most important to propose as many
candidates of ground-truth as possible in a fixed number of proposals. In a
typical image, however, there are too few hard negative examples compared to
the vast number of easy negatives, so region proposal networks struggle to
train on hard negatives. Because of this problem, networks tend to propose hard
negatives as candidates, while failing to propose ground-truth candidates,
which leads to poor performance. In this paper, we propose a Negative Region
Proposal Network(nRPN) to improve Region Proposal Network(RPN). The nRPN learns
from the RPN's false positives and provide hard negative examples to the RPN.
Our proposed nRPN leads to a reduction in false positives and better RPN
performance. An RPN trained with an nRPN achieves performance improvements on
the PASCAL VOC 2007 dataset.",None,357
"Embodied, Situated, and Grounded Intelligence: Implications for AI",0.0187008,"In April of 2022, the Santa Fe Institute hosted a workshop on embodied,
situated, and grounded intelligence as part of the Institute's Foundations of
Intelligence project. The workshop brought together computer scientists,
psychologists, philosophers, social scientists, and others to discuss the
science of embodiment and related issues in human intelligence, and its
implications for building robust, human-level AI. In this report, we summarize
each of the talks and the subsequent discussions. We also draw out a number of
key themes and identify important frontiers for future research.",None,38181
The Shared Task on Gender Rewriting,0.0203204,"In this paper, we present the results and findings of the Shared Task on
Gender Rewriting, which was organized as part of the Seventh Arabic Natural
Language Processing Workshop. The task of gender rewriting refers to generating
alternatives of a given sentence to match different target user gender contexts
(e.g., female speaker with a male listener, a male speaker with a male
listener, etc.). This requires changing the grammatical gender (masculine or
feminine) of certain words referring to the users. In this task, we focus on
Arabic, a gender-marking morphologically rich language. A total of five teams
from four countries participated in the shared task.",https://github.com/CAMeL-Lab/gender-rewriting/,16583
A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning,0.0352569,"Subword tokenization is a commonly used input pre-processing step in most
recent NLP models. However, it limits the models' ability to leverage
end-to-end task learning. Its frequency-based vocabulary creation compromises
tokenization in low-resource languages, leading models to produce suboptimal
representations. Additionally, the dependency on a fixed vocabulary limits the
subword models' adaptability across languages and domains. In this work, we
propose a vocabulary-free neural tokenizer by distilling segmentation
information from heuristic-based subword tokenization. We pre-train our
character-based tokenizer by processing unique words from multilingual corpus,
thereby extensively increasing word diversity across languages. Unlike the
predefined and fixed vocabularies in subword methods, our tokenizer allows
end-to-end task learning, resulting in optimal task-specific tokenization. The
experimental results show that replacing the subword tokenizer with our neural
tokenizer consistently improves performance on multilingual (NLI) and
code-switching (sentiment analysis) tasks, with larger gains in low-resource
languages. Additionally, our neural tokenizer exhibits a robust performance on
downstream tasks when adversarial noise is present (typos and misspelling),
further increasing the initial improvements over statistical subword
tokenizers.",None,3040
Deep Leaning-Based Ultra-Fast Stair Detection,0.234668,"Staircases are some of the most common building structures in urban
environments. Stair detection is an important task for various applications,
including the environmental perception of exoskeleton robots, humanoid robots,
and rescue robots and the navigation of visually impaired people. Most existing
stair detection algorithms have difficulty dealing with the diversity of stair
structure materials, extreme light and serious occlusion. Inspired by human
perception, we propose an end-to-end method based on deep learning.
Specifically, we treat the process of stair line detection as a multitask
involving coarse-grained semantic segmentation and object detection. The input
images are divided into cells, and a simple neural network is used to judge
whether each cell contains stair lines. For cells containing stair lines, the
locations of the stair lines relative to each cell are regressed. Extensive
experiments on our dataset show that our method can achieve high performance in
terms of both speed and accuracy. A lightweight version can even achieve 300+
frames per second with the same resolution. Our code and dataset will be soon
available at GitHub.",None,3720
Extending Process Discovery with Model Complexity Optimization and Cyclic States Identification: Application to Healthcare Processes,0.0121903,"Within Process mining, discovery techniques had made it possible to construct
business process models automatically from event logs. However, results often
do not achieve the balance between model complexity and its fitting accuracy,
so there is a need for manual model adjusting. The paper presents an approach
to process mining providing semi-automatic support to model optimization based
on the combined assessment of the model complexity and fitness. To balance
between the two ingredients, a model simplification approach is proposed, which
essentially abstracts the raw model at the desired granularity. Additionally,
we introduce a concept of meta-states, a cycle collapsing in the model, which
can potentially simplify the model and interpret it. We aim to demonstrate the
capabilities of the technological solution using three datasets from different
applications in the healthcare domain. They are remote monitoring process for
patients with arterial hypertension and workflows of healthcare workers during
the COVID-19 pandemic. A case study also investigates the use of various
complexity measures and different ways of solution application providing
insights on better practices in improving interpretability and
complexity/fitness balance in process models.",None,12
Unsupervised Keyphrase Extraction via Interpretable Neural Networks,0.00976107,"Keyphrase extraction aims at automatically extracting a list of ""important""
phrases representing the key concepts in a document. Prior approaches for
unsupervised keyphrase extraction resorted to heuristic notions of phrase
importance via embedding clustering or graph centrality, requiring extensive
domain expertise. Our work presents a simple alternative approach which defines
keyphrases as document phrases that are salient for predicting the topic of the
document. To this end, we propose INSPECT -- an approach that uses
self-explaining models for identifying influential keyphrases in a document by
measuring the predictive impact of input phrases on the downstream task of the
document topic classification. We show that this novel method not only
alleviates the need for ad-hoc heuristics but also achieves state-of-the-art
results in unsupervised keyphrase extraction in four datasets across two
domains: scientific publications and news articles.",https://github.com/rishabhjoshi/inspect,8190
Font Representation Learning via Paired-glyph Matching,0.0062682,"Fonts can convey profound meanings of words in various forms of glyphs.
Without typography knowledge, manually selecting an appropriate font or
designing a new font is a tedious and painful task. To allow users to explore
vast font styles and create new font styles, font retrieval and font style
transfer methods have been proposed. These tasks increase the need for learning
high-quality font representations. Therefore, we propose a novel font
representation learning scheme to embed font styles into the latent space. For
the discriminative representation of a font from others, we propose a
paired-glyph matching-based font representation learning model that attracts
the representations of glyphs in the same font to one another, but pushes away
those of other fonts. Through evaluations on font retrieval with query glyphs
on new fonts, we show our font representation learning scheme achieves better
generalization performance than the existing font representation learning
techniques. Finally on the downstream font style transfer and generation tasks,
we confirm the benefits of transfer learning with the proposed method. The
source code is available at https://github.com/junhocho/paired-glyph-matching.",https://github.com/junhocho/paired-glyph-matching,645
A Structure-Guided Diffusion Model for Large-Hole Image Completion,0.0152297,"Image completion techniques have made significant progress in filling missing
regions (i.e., holes) in images. However, large-hole completion remains
challenging due to limited structural information. In this paper, we address
this problem by integrating explicit structural guidance into diffusion-based
image completion, forming our structure-guided diffusion model (SGDM). It
consists of two cascaded diffusion probabilistic models: structure and texture
generators. The structure generator generates an edge image representing
plausible structures within the holes, which is then used for guiding the
texture generation process. To train both generators jointly, we devise a novel
strategy that leverages optimal Bayesian denoising, which denoises the output
of the structure generator in a single step and thus allows backpropagation.
Our diffusion-based approach enables a diversity of plausible completions,
while the editable edges allow for editing parts of an image. Our experiments
on natural scene (Places) and face (CelebA-HQ) datasets demonstrate that our
method achieves a superior or comparable visual quality compared to
state-of-the-art approaches. The code is available for research purposes at
https://github.com/UdonDa/Structure_Guided_Diffusion_Model.",https://github.com/UdonDa/Structure_Guided_Diffusion_Model,42425
Learning Enriched Illuminants for Cross and Single Sensor Color Constancy,0.00132158,"Color constancy aims to restore the constant colors of a scene under
different illuminants. However, due to the existence of camera spectral
sensitivity, the network trained on a certain sensor, cannot work well on
others. Also, since the training datasets are collected in certain
environments, the diversity of illuminants is limited for complex real world
prediction. In this paper, we tackle these problems via two aspects. First, we
propose cross-sensor self-supervised training to train the network. In detail,
we consider both the general sRGB images and the white-balanced RAW images from
current available datasets as the white-balanced agents. Then, we train the
network by randomly sampling the artificial illuminants in a sensor-independent
manner for scene relighting and supervision. Second, we analyze a previous
cascaded framework and present a more compact and accurate model by sharing the
backbone parameters with learning attention specifically. Experiments show that
our cross-sensor model and single-sensor model outperform other
state-of-the-art methods by a large margin on cross and single sensor
evaluations, respectively, with only 16% parameters of the previous best model.",https://github.com/yhlscut/C4,25445
FORCE: A Framework of Rule-Based Conversational Recommender System,0.0295994,"The conversational recommender systems (CRSs) have received extensive
attention in recent years. However, most of the existing works focus on various
deep learning models, which are largely limited by the requirement of
large-scale human-annotated datasets. Such methods are not able to deal with
the cold-start scenarios in industrial products. To alleviate the problem, we
propose FORCE, a Framework Of Rule-based Conversational Recommender system that
helps developers to quickly build CRS bots by simple configuration. We conduct
experiments on two datasets in different languages and domains to verify its
effectiveness and usability.",None,122012
Mono-surrogate vs Multi-surrogate in Multi-objective Bayesian Optimisation,0.0741982,"Bayesian optimisation (BO) has been widely used to solve problems with
expensive function evaluations. In multi-objective optimisation problems, BO
aims to find a set of approximated Pareto optimal solutions. There are
typically two ways to build surrogates in multi-objective BO: One surrogate by
aggregating objective functions (by using a scalarising function, also called
mono-surrogate approach) and multiple surrogates (for each objective function,
also called multi-surrogate approach). In both approaches, an acquisition
function (AF) is used to guide the search process. Mono-surrogate has the
advantage that only one model is used, however, the approach has two major
limitations. Firstly, the fitness landscape of the scalarising function and the
objective functions may not be similar. Secondly, the approach assumes that the
scalarising function distribution is Gaussian, and thus a closed-form
expression of the AF can be used. In this work, we overcome these limitations
by building a surrogate model for each objective function and show that the
scalarising function distribution is not Gaussian. We approximate the
distribution using Generalised extreme value distribution. The results and
comparison with existing approaches on standard benchmark and real-world
optimisation problems show the potential of the multi-surrogate approach.",None,1960
CREATER: CTR-driven Advertising Text Generation with Controlled Pre-Training and Contrastive Fine-Tuning,0.0621283,"This paper focuses on automatically generating the text of an ad, and the
goal is that the generated text can capture user interest for achieving higher
click-through rate (CTR). We propose CREATER, a CTR-driven advertising text
generation approach, to generate ad texts based on high-quality user reviews.
To incorporate CTR objective, our model learns from online A/B test data with
contrastive learning, which encourages the model to generate ad texts that
obtain higher CTR. To alleviate the low-resource issue, we design a customized
self-supervised objective reducing the gap between pre-training and
fine-tuning. Experiments on industrial datasets show that CREATER significantly
outperforms current approaches. It has been deployed online in a leading
advertising platform and brings uplift on core online metrics.",https://github.com/pytorch/pytorch,235
Data Augmentation by Selecting Mixed Classes Considering Distance Between Classes,0.00598679,"Data augmentation is an essential technique for improving recognition
accuracy in object recognition using deep learning. Methods that generate mixed
data from multiple data sets, such as mixup, can acquire new diversity that is
not included in the training data, and thus contribute significantly to
accuracy improvement. However, since the data selected for mixing are randomly
sampled throughout the training process, there are cases where appropriate
classes or data are not selected. In this study, we propose a data augmentation
method that calculates the distance between classes based on class
probabilities and can select data from suitable classes to be mixed in the
training process. Mixture data is dynamically adjusted according to the
training trend of each class to facilitate training. The proposed method is
applied in combination with conventional methods for generating mixed data.
Evaluation experiments show that the proposed method improves recognition
performance on general and long-tailed image recognition datasets.",None,9497
Towards Device Efficient Conditional Image Generation,0.0306799,"We present a novel algorithm to reduce tensor compute required by a
conditional image generation autoencoder without sacrificing quality of
photo-realistic image generation. Our method is device agnostic, and can
optimize an autoencoder for a given CPU-only, GPU compute device(s) in about
normal time it takes to train an autoencoder on a generic workstation. We
achieve this via a two-stage novel strategy where, first, we condense the
channel weights, such that, as few as possible channels are used. Then, we
prune the nearly zeroed out weight activations, and fine-tune the autoencoder.
To maintain image quality, fine-tuning is done via student-teacher training,
where we reuse the condensed autoencoder as the teacher. We show performance
gains for various conditional image generation tasks: segmentation mask to face
images, face images to cartoonization, and finally CycleGAN-based model over
multiple compute devices. We perform various ablation studies to justify the
claims and design choices, and achieve real-time versions of various
autoencoders on CPU-only devices while maintaining image quality, thus enabling
at-scale deployment of such autoencoders.",https://github.com/bryandlee/animegan2-pytorch,1442
Mental Health Assessment for the Chatbots,0.0246056,"Previous researches on dialogue system assessment usually focus on the
quality evaluation (e.g. fluency, relevance, etc) of responses generated by the
chatbots, which are local and technical metrics. For a chatbot which responds
to millions of online users including minors, we argue that it should have a
healthy mental tendency in order to avoid the negative psychological impact on
them. In this paper, we establish several mental health assessment dimensions
for chatbots (depression, anxiety, alcohol addiction, empathy) and introduce
the questionnaire-based mental health assessment methods. We conduct
assessments on some well-known open-domain chatbots and find that there are
severe mental health issues for all these chatbots. We consider that it is due
to the neglect of the mental health risks during the dataset building and the
model training procedures. We expect to attract researchers' attention to the
serious mental health problems of chatbots and improve the chatbots' ability in
positive emotional interaction.",None,1215
Shapley value-based approaches to explain the robustness of classifiers in machine learning,0.0305259,"The use of algorithm-agnostic approaches is an emerging area of research for
explaining the contribution of individual features towards the predicted
outcome. Whilst there is a focus on explaining the prediction itself, a little
has been done on explaining the robustness of these models, that is, how each
feature contributes towards achieving that robustness. In this paper, we
propose the use of Shapley values to explain the contribution of each feature
towards the model's robustness, measured in terms of Receiver-operating
Characteristics (ROC) curve and the Area under the ROC curve (AUC). With the
help of an illustrative example, we demonstrate the proposed idea of explaining
the ROC curve, and visualising the uncertainties in these curves. For
imbalanced datasets, the use of Precision-Recall Curve (PRC) is considered more
appropriate, therefore we also demonstrate how to explain the PRCs with the
help of Shapley values. The explanation of robustness can help analysts in a
number of ways, for example, it can help in feature selection by identifying
the irrelevant features that can be removed to reduce the computational
complexity. It can also help in identifying the features having critical
contributions or negative contributions towards robustness.",https://github.com/shaprob/shaproc,1304
OCFormer: One-Class Transformer Network for Image Classification,0.0,"We propose a novel deep learning framework based on Vision Transformers (ViT)
for one-class classification. The core idea is to use zero-centered Gaussian
noise as a pseudo-negative class for latent space representation and then train
the network using the optimal loss function. In prior works, there have been
tremendous efforts to learn a good representation using varieties of loss
functions, which ensures both discriminative and compact properties. The
proposed one-class Vision Transformer (OCFormer) is exhaustively experimented
on CIFAR-10, CIFAR-100, Fashion-MNIST and CelebA eyeglasses datasets. Our
method has shown significant improvements over competing CNN based one-class
classifier approaches.",None,3158
Multi-Task Learning for Visual Scene Understanding,0.00737487,"Despite the recent progress in deep learning, most approaches still go for a
silo-like solution, focusing on learning each task in isolation: training a
separate neural network for each individual task. Many real-world problems,
however, call for a multi-modal approach and, therefore, for multi-tasking
models. Multi-task learning (MTL) aims to leverage useful information across
tasks to improve the generalization capability of a model. This thesis is
concerned with multi-task learning in the context of computer vision. First, we
review existing approaches for MTL. Next, we propose several methods that
tackle important aspects of multi-task learning. The proposed methods are
evaluated on various benchmarks. The results show several advances in the
state-of-the-art of multi-task learning. Finally, we discuss several
possibilities for future work.",https://github.com/SimonVandenhende,12
ViWOZ: A Multi-Domain Task-Oriented Dialogue Systems Dataset For Low-resource Language,0.0152438,"Most of the current task-oriented dialogue systems (ToD), despite having
interesting results, are designed for a handful of languages like Chinese and
English. Therefore, their performance in low-resource languages is still a
significant problem due to the absence of a standard dataset and evaluation
policy. To address this problem, we proposed ViWOZ, a fully-annotated
Vietnamese task-oriented dialogue dataset. ViWOZ is the first multi-turn,
multi-domain tasked oriented dataset in Vietnamese, a low-resource language.
The dataset consists of a total of 5,000 dialogues, including 60,946 fully
annotated utterances. Furthermore, we provide a comprehensive benchmark of both
modular and end-to-end models in low-resource language scenarios. With those
characteristics, the ViWOZ dataset enables future studies on creating a
multilingual task-oriented dialogue system.",None,897
Drawing Causal Inferences About Performance Effects in NLP,0.00458042,"This article emphasizes that NLP as a science seeks to make inferences about
the performance effects that result from applying one method (compared to
another method) in the processing of natural language. Yet NLP research in
practice usually does not achieve this goal: In NLP research articles,
typically only a few models are compared. Each model results from a specific
procedural pipeline (here named processing system) that is composed of a
specific collection of methods that are used in preprocessing, pretraining,
hyperparameter tuning, and training on the target task. To make generalizing
inferences about the performance effect that is caused by applying some method
A vs. another method B, it is not sufficient to compare a few specific models
that are produced by a few specific (probably incomparable) processing systems.
Rather, the following procedure would allow drawing inferences about methods'
performance effects: (1) A population of processing systems that researchers
seek to infer to has to be defined. (2) A random sample of processing systems
from this population is drawn. (The drawn processing systems in the sample will
vary with regard to the methods they apply along their procedural pipelines and
also will vary regarding the compositions of their training and test data sets
used for training and evaluation.) (3) Each processing system is applied once
with method A and once with method B. (4) Based on the sample of applied
processing systems, the expected generalization errors of method A and method B
are approximated. (5) The difference between the expected generalization errors
of method A and method B is the estimated average treatment effect due to
applying method A compared to method B in the population of processing systems.",None,-1
Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?,0.0358988,"Even though Generative Adversarial Networks (GANs) have shown a remarkable
ability to generate high-quality images, GANs do not always guarantee the
generation of photorealistic images. Occasionally, they generate images that
have defective or unnatural objects, which are referred to as 'artifacts'.
Research to investigate why these artifacts emerge and how they can be detected
and removed has yet to be sufficiently carried out. To analyze this, we first
hypothesize that rarely activated neurons and frequently activated neurons have
different purposes and responsibilities for the progress of generating images.
In this study, by analyzing the statistics and the roles for those neurons, we
empirically show that rarely activated neurons are related to the failure
results of making diverse objects and inducing artifacts. In addition, we
suggest a correction method, called 'Sequential Ablation', to repair the
defective part of the generated images without high computational cost and
manual efforts.",https://github.com/genforce/genforce,5416
Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization,0.0226756,"In zero-shot multilingual extractive text summarization, a model is typically
trained on English summarization dataset and then applied on summarization
datasets of other languages. Given English gold summaries and documents,
sentence-level labels for extractive summarization are usually generated using
heuristics. However, these monolingual labels created on English datasets may
not be optimal on datasets of other languages, for that there is the syntactic
or semantic discrepancy between different languages. In this way, it is
possible to translate the English dataset to other languages and obtain
different sets of labels again using heuristics. To fully leverage the
information of these different sets of labels, we propose NLSSum (Neural Label
Search for Summarization), which jointly learns hierarchical weights for these
different sets of labels together with our summarization model. We conduct
multilingual zero-shot summarization experiments on MLSUM and WikiLingua
datasets, and we achieve state-of-the-art results using both human and
automatic evaluations across these two datasets.",https://github.com/marian-nmt/marian,40158
Few-Shot Table-to-Text Generation with Prefix-Controlled Generator,0.232355,"Neural table-to-text generation approaches are data-hungry, limiting their
adaptation for low-resource real-world applications. Previous works mostly
resort to Pre-trained Language Models (PLMs) to generate fluent summaries of a
table. However, they often contain hallucinated contents due to the
uncontrolled nature of PLMs. Moreover, the topological differences between
tables and sequences are rarely studied. Last but not least, fine-tuning on
PLMs with a handful of instances may lead to over-fitting and catastrophic
forgetting. To alleviate these problems, we propose a prompt-based approach,
Prefix-Controlled Generator (i.e., PCG), for few-shot table-to-text generation.
We prepend a task-specific prefix for a PLM to make the table structure better
fit the pre-trained input. In addition, we generate an input-specific prefix to
control the factual contents and word order of the generated text. Both
automatic and human evaluations on different domains (humans, books and songs)
of the Wikibio dataset show substantial improvements over baseline approaches.",None,2888
Automatic Generation of Factual News Headlines in Finnish,0.13934,"We present a novel approach to generating news headlines in Finnish for a
given news story. We model this as a summarization task where a model is given
a news article, and its task is to produce a concise headline describing the
main topic of the article. Because there are no openly available GPT-2 models
for Finnish, we will first build such a model using several corpora. The model
is then fine-tuned for the headline generation task using a massive news
corpus. The system is evaluated by 3 expert journalists working in a Finnish
media house. The results showcase the usability of the presented approach as a
headline suggestion tool to facilitate the news production process.",None,1025
A multi-domain virtual network embedding algorithm with delay prediction,0.0305758,"Virtual network embedding (VNE) is an crucial part of network virtualization
(NV), which aims to map the virtual networks (VNs) to a shared substrate
network (SN). With the emergence of various delay-sensitive applications, how
to improve the delay performance of the system has become a hot topic in
academic circles. Based on extensive research, we proposed a multi-domain
virtual network embedding algorithm based on delay prediction (DP-VNE).
Firstly, the candidate physical nodes are selected by estimating the delay of
virtual requests, then particle swarm optimization (PSO) algorithm is used to
optimize the mapping process, so as to reduce the delay of the system. The
simulation results show that compared with the other three advanced algorithms,
the proposed algorithm can significantly reduce the system delay while keeping
other indicators unaffected.",None,9769
Improving Data Driven Inverse Text Normalization using Data Augmentation,0.021946,"Inverse text normalization (ITN) is used to convert the spoken form output of
an automatic speech recognition (ASR) system to a written form. Traditional
handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile
neural modeling approaches require quality large-scale spoken-written pair
examples in the same or similar domain as the ASR system (in-domain data), to
train. Both these approaches require costly and complex annotations. In this
paper, we present a data augmentation technique that effectively generates rich
spoken-written numeric pairs from out-of-domain textual data with minimal human
annotation. We empirically demonstrate that ITN model trained using our data
augmentation technique consistently outperform ITN model trained using only
in-domain data across all numeric surfaces like cardinal, currency, and
fraction, by an overall accuracy of 14.44%.",None,1187
Quantitative Method for Security Situation of the Power Information Network Based on the Evolutionary Neural Network,0.005721,"Cybersecurity is the security cornerstone of digital transformation of the
power grid and construction of new power systems. The traditional network
security situation quantification method only analyzes from the perspective of
network performance, ignoring the impact of various power application services
on the security situation, so the quantification results cannot fully reflect
the power information network risk state. This study proposes a method for
quantifying security situation of the power information network based on the
evolutionary neural network. First, the security posture system architecture is
designed by analyzing the business characteristics of power information network
applications. Second, combining the importance of power application business,
the spatial element index system of coupled interconnection is established from
three dimensions of network reliability, threat, and vulnerability. Then, the
BP neural network optimized by the genetic evolutionary algorithm is
incorporated into the element index calculation process, and the quantitative
model of security posture of the power information network based on the
evolutionary neural network is constructed. Finally, a simulation experiment
environment is built according to a power sector network topology, and the
effectiveness and robustness of the method proposed in the study are verified.",None,-1
Transformers as Neural Augmentors: Class Conditional Sentence Generation via Variational Bayes,0.00721154,"Data augmentation methods for Natural Language Processing tasks are explored
in recent years, however they are limited and it is hard to capture the
diversity on sentence level. Besides, it is not always possible to perform data
augmentation on supervised tasks. To address those problems, we propose a
neural data augmentation method, which is a combination of Conditional
Variational Autoencoder and encoder-decoder Transformer model. While encoding
and decoding the input sentence, our model captures the syntactic and semantic
representation of the input language with its class condition. Following the
developments in the past years on pre-trained language models, we train and
evaluate our models on several benchmarks to strengthen the downstream tasks.
We compare our method with 3 different augmentation techniques. The presented
results show that, our model increases the performance of current models
compared to other data augmentation techniques with a small amount of
computation power.",https://github.com/safakkbilici/Conditional-Variational-Transformer,-1
Pop-Out Motion: 3D-Aware Image Deformation via Learning the Shape Laplacian,0.0181975,"We propose a framework that can deform an object in a 2D image as it exists
in 3D space. Most existing methods for 3D-aware image manipulation are limited
to (1) only changing the global scene information or depth, or (2) manipulating
an object of specific categories. In this paper, we present a 3D-aware image
deformation method with minimal restrictions on shape category and deformation
type. While our framework leverages 2D-to-3D reconstruction, we argue that
reconstruction is not sufficient for realistic deformations due to the
vulnerability to topological errors. Thus, we propose to take a supervised
learning-based approach to predict the shape Laplacian of the underlying volume
of a 3D reconstruction represented as a point cloud. Given the deformation
energy calculated using the predicted shape Laplacian and user-defined
deformation handles (e.g., keypoints), we obtain bounded biharmonic weights to
model plausible handle-based image deformation. In the experiments, we present
our results of deforming 2D character and clothed human images. We also
quantitatively show that our approach can produce more accurate deformation
weights compared to alternative methods (i.e., mesh reconstruction and point
cloud Laplacian methods).",None,13889
End-to-End Semantic Video Transformer for Zero-Shot Action Recognition,0.0282583,"While video action recognition has been an active area of research for
several years, zero-shot action recognition has only recently started gaining
traction. In this work, we propose a novel end-to-end trained transformer model
which is capable of capturing long range spatiotemporal dependencies
efficiently, contrary to existing approaches which use 3D-CNNs. Moreover, to
address a common ambiguity in the existing works about classes that can be
considered as previously unseen, we propose a new experimentation setup that
satisfies the zero-shot learning premise for action recognition by avoiding
overlap between the training and testing classes. The proposed approach
significantly outperforms the state of the arts in zero-shot action recognition
in terms of the the top-1 accuracy on UCF-101, HMDB-51 and ActivityNet
datasets. The code and proposed experimentation setup are available in GitHub:
https://github.com/Secure-and-Intelligent-Systems-Lab/SemanticVideoTransformer",https://github.com/Secure-and-Intelligent-Systems-Lab/SemanticVideoTransformer,4
A Deep Learning Approach for Automatic Detection of Qualitative Features of Lecturing,0.0,"Artificial Intelligence in higher education opens new possibilities for
improving the lecturing process, such as enriching didactic materials, helping
in assessing students' works or even providing directions to the teachers on
how to enhance the lectures. We follow this research path, and in this work, we
explore how an academic lecture can be assessed automatically by quantitative
features. First, we prepare a set of qualitative features based on teaching
practices and then annotate the dataset of academic lecture videos collected
for this purpose. We then show how these features could be detected
automatically using machine learning and computer vision techniques. Our
results show the potential usefulness of our work.",None,16208
Non-iterative optimization of pseudo-labeling thresholds for training object detection models from multiple datasets,0.0144285,"We propose a non-iterative method to optimize pseudo-labeling thresholds for
learning object detection from a collection of low-cost datasets, each of which
is annotated for only a subset of all the object classes. A popular approach to
this problem is first to train teacher models and then to use their confident
predictions as pseudo ground-truth labels when training a student model. To
obtain the best result, however, thresholds for prediction confidence must be
adjusted. This process typically involves iterative search and repeated
training of student models and is time-consuming. Therefore, we develop a
method to optimize the thresholds without iterative optimization by maximizing
the $F_\beta$-score on a validation dataset, which measures the quality of
pseudo labels and can be measured without training a student model. We
experimentally demonstrate that our proposed method achieves an mAP comparable
to that of grid search on the COCO and VOC datasets.",None,286
Seeing a Rose in Five Thousand Ways,0.114308,"What is a rose, visually? A rose comprises its intrinsics, including the
distribution of geometry, texture, and material specific to its object
category. With knowledge of these intrinsic properties, we may render roses of
different sizes and shapes, in different poses, and under different lighting
conditions. In this work, we build a generative model that learns to capture
such object intrinsics from a single image, such as a photo of a bouquet. Such
an image includes multiple instances of an object type. These instances all
share the same intrinsics, but appear different due to a combination of
variance within these intrinsics and differences in extrinsic factors, such as
pose and illumination. Experiments show that our model successfully learns
object intrinsics (distribution of geometry, texture, and material) for a wide
range of objects, each from a single Internet image. Our method achieves
superior results on multiple downstream tasks, including intrinsic image
decomposition, shape and image generation, view synthesis, and relighting.",None,35102
Objects Matter: Learning Object Relation Graph for Robust Camera Relocalization,0.0748047,"Visual relocalization aims to estimate the pose of a camera from one or more
images. In recent years deep learning based pose regression methods have
attracted many attentions. They feature predicting the absolute poses without
relying on any prior built maps or stored images, making the relocalization
very efficient. However, robust relocalization under environments with complex
appearance changes and real dynamics remains very challenging. In this paper,
we propose to enhance the distinctiveness of the image features by extracting
the deep relationship among objects. In particular, we extract objects in the
image and construct a deep object relation graph (ORG) to incorporate the
semantic connections and relative spatial clues of the objects. We integrate
our ORG module into several popular pose regression models. Extensive
experiments on various public indoor and outdoor datasets demonstrate that our
method improves the performance significantly and outperforms the previous
approaches.",None,1155
SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic Specialisation for Chinese Sexism Detection in Social Media,0.028007,"The goal of sexism detection is to mitigate negative online content targeting
certain gender groups of people. However, the limited availability of labeled
sexism-related datasets makes it problematic to identify online sexism for
low-resource languages. In this paper, we address the task of automatic sexism
detection in social media for one low-resource language -- Chinese. Rather than
collecting new sexism data or building cross-lingual transfer learning models,
we develop a cross-lingual domain-aware semantic specialisation system in order
to make the most of existing data. Semantic specialisation is a technique for
retrofitting pre-trained distributional word vectors by integrating external
linguistic knowledge (such as lexico-semantic relations) into the specialised
feature space. To do this, we leverage semantic resources for sexism from a
high-resource language (English) to specialise pre-trained word vectors in the
target language (Chinese) to inject domain knowledge. We demonstrate the
benefit of our sexist word embeddings (SexWEs) specialised by our framework via
intrinsic evaluation of word similarity and extrinsic evaluation of sexism
detection. Compared with other specialisation approaches and Chinese baseline
word vectors, our SexWEs shows an average score improvement of 0.033 and 0.064
in both intrinsic and extrinsic evaluations, respectively. The ablative results
and visualisation of SexWEs also prove the effectiveness of our framework on
retrofitting word vectors in low-resource languages.",None,7490
UnShadowNet: Illumination Critic Guided Contrastive Learning For Shadow Removal,0.0347561,"Shadows are frequently encountered natural phenomena that significantly
hinder the performance of computer vision perception systems in practical
settings, e.g., autonomous driving. A solution to this would be to eliminate
shadow regions from the images before the processing of the perception system.
Yet, training such a solution requires pairs of aligned shadowed and
non-shadowed images which are difficult to obtain. We introduce a novel weakly
supervised shadow removal framework UnShadowNet trained using contrastive
learning. It is composed of a DeShadower network responsible for the removal of
the extracted shadow under the guidance of an Illumination network which is
trained adversarially by the illumination critic and a Refinement network to
further remove artefacts. We show that UnShadowNet can be easily extended to a
fully-supervised set-up to exploit the ground-truth when available. UnShadowNet
outperforms existing state-of-the-art approaches on three publicly available
shadow datasets (ISTD, adjusted ISTD, SRD) in both the weakly and fully
supervised setups.",None,7532
Feature Engineering vs BERT on Twitter Data,0.00430844,"In this paper, we compare the performances of traditional machine learning
models using feature engineering and word vectors and the state-of-the-art
language model BERT using word embeddings on three datasets. We also consider
the time and cost efficiency of feature engineering compared to BERT. From our
results we conclude that the use of the BERT model was only worth the time and
cost trade-off for one of the three datasets we used for comparison, where the
BERT model significantly outperformed any kind of traditional classifier that
uses feature vectors, instead of embeddings. Using the BERT model for the other
datasets only achieved an increase of 0.03 and 0.05 of accuracy and F1 score
respectively, which could be argued makes its use not worth the time and cost
of GPU.",None,180
Label-Efficient Online Continual Object Detection in Streaming Video,0.0507915,"Humans can watch a continuous video stream and effortlessly perform continual
acquisition and transfer of new knowledge with minimal supervision yet
retaining previously learnt experiences. In contrast, existing continual
learning (CL) methods require fully annotated labels to effectively learn from
individual frames in a video stream. Here, we examine a more realistic and
challenging problem$\unicode{x2014}$Label-Efficient Online Continual Object
Detection (LEOCOD) in streaming video. We propose a plug-and-play module,
Efficient-CLS, that can be easily inserted into and improve existing continual
learners for object detection in video streams with reduced data annotation
costs and model retraining time. We show that our method has achieved
significant improvement with minimal forgetting across all supervision levels
on two challenging CL benchmarks for streaming real-world videos. Remarkably,
with only 25% annotated video frames, our method still outperforms the base CL
learners, which are trained with 100% annotations on all video frames. The data
and source code will be publicly available at
https://github.com/showlab/Efficient-CLS.",https://github.com/showlab/Efficient-CLS,20788
A Compacted Structure for Cross-domain learning on Monocular Depth and Flow Estimation,0.00718011,"Accurate motion and depth recovery is important for many robot vision tasks
including autonomous driving. Most previous studies have achieved cooperative
multi-task interaction via either pre-defined loss functions or cross-domain
prediction. This paper presents a multi-task scheme that achieves mutual
assistance by means of our Flow to Depth (F2D), Depth to Flow (D2F), and
Exponential Moving Average (EMA). F2D and D2F mechanisms enable multi-scale
information integration between optical flow and depth domain based on
differentiable shallow nets. A dual-head mechanism is used to predict optical
flow for rigid and non-rigid motion based on a divide-and-conquer manner, which
significantly improves the optical flow estimation performance. Furthermore, to
make the prediction more robust and stable, EMA is used for our multi-task
training. Experimental results on KITTI datasets show that our multi-task
scheme outperforms other multi-task schemes and provide marked improvements on
the prediction results.",None,26399
Improve Ranking Correlation of Super-net through Training Scheme from One-shot NAS to Few-shot NAS,0.00417894,"The algorithms of one-shot neural architecture search(NAS) have been widely
used to reduce computation consumption. However, because of the interference
among the subnets in which weights are shared, the subnets inherited from these
super-net trained by those algorithms have poor consistency in precision
ranking. To address this problem, we propose a step-by-step training super-net
scheme from one-shot NAS to few-shot NAS. In the training scheme, we firstly
train super-net in a one-shot way, and then we disentangle the weights of
super-net by splitting them into multi-subnets and training them gradually.
Finally, our method ranks 4th place in the CVPR2022 3rd Lightweight NAS
Challenge Track1. Our code is available at
https://github.com/liujiawei2333/CVPR2022-NAS-competition-Track-1-4th-solution.",https://github.com/liujiawei2333/CVPR2022-NAS-competition-Track-1-4th-solution,2394
MMINR: Multi-frame-to-Multi-frame Inference with Noise Resistance for Precipitation Nowcasting with Radar,0.0198096,"Precipitation nowcasting based on radar echo maps is essential in
meteorological research. Recently, Convolutional RNNs based methods dominate
this field, but they cannot be solved by parallel computation resulting in
longer inference time. FCN based methods adopt a multi-frame-to-single-frame
inference (MSI) strategy to avoid this problem. They feedback into the model
again to predict the next time step to get multi-frame nowcasting results in
the prediction phase, which will lead to the accumulation of prediction errors.
In addition, precipitation noise is a crucial factor contributing to high
prediction errors because of its unpredictability. To address this problem, we
propose a novel Multi-frame-to-Multi-frame Inference (MMI) model with Noise
Resistance (NR) named MMINR. It avoids error accumulation and resists
precipitation noise\'s negative effect in parallel computation. NR contains a
Noise Dropout Module (NDM) and a Semantic Restore Module (SRM). NDM
deliberately dropout noise simple yet efficient, and SRM supplements semantic
information of features to alleviate the problem of semantic information
mistakenly lost by NDM. Experimental results demonstrate that MMINR can attain
competitive scores compared with other SOTAs. The ablation experiments show
that the proposed NDM and SRM can solve the aforementioned problems.",None,850
On the Typicality of Musical Sequences,0.0235056,"It has been shown in a recent publication that words in human-produced
English language tend to have an information content close to the conditional
entropy. In this paper, we show that the same is true for events in
human-produced monophonic musical sequences. We also show how ""typical
sampling"" influences the distribution of information around the entropy for
single events and sequences.",None,4373
Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model,0.0207227,"Pre-trained multilingual language models play an important role in
cross-lingual natural language understanding tasks. However, existing methods
did not focus on learning the semantic structure of representation, and thus
could not optimize their performance. In this paper, we propose Multi-level
Multilingual Knowledge Distillation (MMKD), a novel method for improving
multilingual language models. Specifically, we employ a teacher-student
framework to adopt rich semantic representation knowledge in English BERT. We
propose token-, word-, sentence-, and structure-level alignment objectives to
encourage multiple levels of consistency between source-target pairs and
correlation similarity between teacher and student models. We conduct
experiments on cross-lingual evaluation benchmarks including XNLI, PAWS-X, and
XQuAD. Experimental results show that MMKD outperforms other baseline models of
similar size on XNLI and XQuAD and obtains comparable performance on PAWS-X.
Especially, MMKD obtains significant performance gains on low-resource
languages.",None,8728
SaiNet: Stereo aware inpainting behind objects with generative networks,0.0296775,"In this work, we present an end-to-end network for stereo-consistent image
inpainting with the objective of inpainting large missing regions behind
objects. The proposed model consists of an edge-guided UNet-like network using
Partial Convolutions. We enforce multi-view stereo consistency by introducing a
disparity loss. More importantly, we develop a training scheme where the model
is learned from realistic stereo masks representing object occlusions, instead
of the more common random masks. The technique is trained in a supervised way.
Our evaluation shows competitive results compared to previous state-of-the-art
techniques.",None,6095
Multimodal Image Fusion based on Hybrid CNN-Transformer and Non-local Cross-modal Attention,0.0185176,"The fusion of images taken by heterogeneous sensors helps to enrich the
information and improve the quality of imaging. In this article, we present a
hybrid model consisting of a convolutional encoder and a Transformer-based
decoder to fuse multimodal images. In the encoder, a non-local cross-modal
attention block is proposed to capture both local and global dependencies of
multiple source images. A branch fusion module is designed to adaptively fuse
the features of the two branches. We embed a Transformer module with linear
complexity in the decoder to enhance the reconstruction capability of the
proposed network. Qualitative and quantitative experiments demonstrate the
effectiveness of the proposed method by comparing it with existing
state-of-the-art fusion models. The source code of our work is available at
https://github.com/pandayuanyu/HCFusion.",https://github.com/pandayuanyu/HCFusion,4962
Unsupervised multi-branch Capsule for Hyperspectral and LiDAR classification,0.0129823,"With the convenient availability of remote sensing data, how to make models
to interpret complex remote sensing data attracts wide attention. In remote
sensing data, hyperspectral images contain spectral information and LiDAR
contains elevation information. Hence, more explorations are warranted to
better fuse the features of different source data. In this paper, we introduce
semantic understanding to dynamically fuse data from two different sources,
extract features of HSI and LiDAR through different capsule network branches
and improve self-supervised loss and random rigid rotation in Canonical Capsule
to a high-dimensional situation. Canonical Capsule computes the capsule
decomposition of objects by permutation-equivariant attention and the process
is self-supervised by training pairs of randomly rotated objects. After fusing
the features of HSI and LiDAR with semantic understanding, the unsupervised
extraction of spectral-spatial-elevation fusion features is achieved. With two
real-world examples of HSI and LiDAR fused, the experimental results show that
the proposed multi-branch high-dimensional canonical capsule algorithm can be
effective for semantic understanding of HSI and LiDAR. It indicates that the
model can extract HSI and LiDAR data features effectively as opposed to
existing models for unsupervised extraction of multi-source RS data.",None,-1
"Computational analyses of the topics, sentiments, literariness, creativity and beauty of texts in a large Corpus of English Literature",0.0575012,"The Gutenberg Literary English Corpus (GLEC, Jacobs, 2018a) provides a rich
source of textual data for research in digital humanities, computational
linguistics or neurocognitive poetics. In this study we address differences
among the different literature categories in GLEC, as well as differences
between authors. We report the results of three studies providing i) topic and
sentiment analyses for six text categories of GLEC (i.e., children and youth,
essays, novels, plays, poems, stories) and its >100 authors, ii) novel measures
of semantic complexity as indices of the literariness, creativity and book
beauty of the works in GLEC (e.g., Jane Austen's six novels), and iii) two
experiments on text classification and authorship recognition using novel
features of semantic complexity. The data on two novel measures estimating a
text's literariness, intratextual variance and stepwise distance (van
Cranenburgh et al., 2019) revealed that plays are the most literary texts in
GLEC, followed by poems and novels. Computation of a novel index of text
creativity (Gray et al., 2016) revealed poems and plays as the most creative
categories with the most creative authors all being poets (Milton, Pope, Keats,
Byron, or Wordsworth). We also computed a novel index of perceived beauty of
verbal art (Kintsch, 2012) for the works in GLEC and predict that Emma is the
theoretically most beautiful of Austen's novels. Finally, we demonstrate that
these novel measures of semantic complexity are important features for text
classification and authorship recognition with overall predictive accuracies in
the range of .75 to .97. Our data pave the way for future computational and
empirical studies of literature or experiments in reading psychology and offer
multiple baselines and benchmarks for analysing and validating other book
corpora.",None,24970
Semantic Search for Large Scale Clinical Ontologies,0.0196114,"Finding concepts in large clinical ontologies can be challenging when queries
use different vocabularies. A search algorithm that overcomes this problem is
useful in applications such as concept normalisation and ontology matching,
where concepts can be referred to in different ways, using different synonyms.
In this paper, we present a deep learning based approach to build a semantic
search system for large clinical ontologies. We propose a Triplet-BERT model
and a method that generates training data directly from the ontologies. The
model is evaluated using five real benchmark data sets and the results show
that our approach achieves high results on both free text to concept and
concept to concept searching tasks, and outperforms all baseline methods.",https://github.com/dmis-lab/biobert,2314
Structure and position-aware graph neural network for airway labeling,0.00667309,"We present a novel graph-based approach for labeling the anatomical branches
of a given airway tree segmentation. The proposed method formulates airway
labeling as a branch classification problem in the airway tree graph, where
branch features are extracted using convolutional neural networks (CNN) and
enriched using graph neural networks. Our graph neural network is
structure-aware by having each node aggregate information from its local
neighbors and position-aware by encoding node positions in the graph.
  We evaluated the proposed method on 220 airway trees from subjects with
various severity stages of Chronic Obstructive Pulmonary Disease (COPD). The
results demonstrate that our approach is computationally efficient and
significantly improves branch classification performance than the baseline
method. The overall average accuracy of our method reaches 91.18\% for labeling
all 18 segmental airway branches, compared to 83.83\% obtained by the standard
CNN method. We published our source code at
https://github.com/DIAGNijmegen/spgnn. The proposed algorithm is also publicly
available at
https://grand-challenge.org/algorithms/airway-anatomical-labeling/.",https://github.com/DIAGNijmegen/spgnn,72744
Graph Neural Network Policies and Imitation Learning for Multi-Domain Task-Oriented Dialogues,0.030297,"Task-oriented dialogue systems are designed to achieve specific goals while
conversing with humans. In practice, they may have to handle simultaneously
several domains and tasks. The dialogue manager must therefore be able to take
into account domain changes and plan over different domains/tasks in order to
deal with multidomain dialogues. However, learning with reinforcement in such
context becomes difficult because the state-action dimension is larger while
the reward signal remains scarce. Our experimental results suggest that
structured policies based on graph neural networks combined with different
degrees of imitation learning can effectively handle multi-domain dialogues.
The reported experiments underline the benefit of structured policies over
standard policies.",None,3446
Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models,0.0,"With increasing scale, large language models demonstrate both quantitative
improvement and new qualitative capabilities, especially as zero-shot learners,
like GPT-3. However, these results rely heavily on delicate prompt design and
large computation. In this work, we explore whether the strong zero-shot
ability could be achieved at a smaller model scale without any external
supervised data. To achieve this goal, we revisit masked language modeling and
present a geometry-guided self-supervised learning method (Go-tuningfor short)
by taking a small number of task-aware self-supervised data to update language
models further. Experiments show that Go-tuning can enable T5-small (80M)
competitive zero-shot results compared with large language models, such as
T5-XL (3B). We also apply Go-tuning on multi-task settings and develop a
multi-task model, mgo-T5 (250M). It can reach the average performance of OPT
(175B) on 9 datasets.",None,3525
Low-rank Meets Sparseness: An Integrated Spatial-Spectral Total Variation Approach to Hyperspectral Denoising,0.0149297,"Spatial-Spectral Total Variation (SSTV) can quantify local smoothness of
image structures, so it is widely used in hyperspectral image (HSI) processing
tasks. Essentially, SSTV assumes a sparse structure of gradient maps calculated
along the spatial and spectral directions. In fact, these gradient tensors are
not only sparse, but also (approximately) low-rank under FFT, which we have
verified by numerical tests and theoretical analysis. Based on this fact, we
propose a novel TV regularization to simultaneously characterize the sparsity
and low-rank priors of the gradient map (LRSTV). The new regularization not
only imposes sparsity on the gradient map itself, but also penalize the rank on
the gradient map after Fourier transform along the spectral dimension. It
naturally encodes the sparsity and lowrank priors of the gradient map, and thus
is expected to reflect the inherent structure of the original image more
faithfully. Further, we use LRSTV to replace conventional SSTV and embed it in
the HSI processing model to improve its performance. Experimental results on
multiple public data-sets with heavy mixed noise show that the proposed model
can get 1.5dB improvement of PSNR.",None,16864
Quantifying Harm,0.0156861,"In a companion paper (Beckers et al. 2022), we defined a qualitative notion
of harm: either harm is caused, or it is not. For practical applications, we
often need to quantify harm; for example, we may want to choose the lest
harmful of a set of possible interventions. We first present a quantitative
definition of harm in a deterministic context involving a single individual,
then we consider the issues involved in dealing with uncertainty regarding the
context and going from a notion of harm for a single individual to a notion of
""societal harm"", which involves aggregating the harm to individuals. We show
that the ""obvious"" way of doing this (just taking the expected harm for an
individual and then summing the expected harm over all individuals can lead to
counterintuitive or inappropriate answers, and discuss alternatives, drawing on
work from the decision-theory literature.",None,2215
Reinforcement Learning for Economic Policy: A New Frontier?,0.00286439,"Agent-based computational economics is a field with a rich academic history,
yet one which has struggled to enter mainstream policy design toolboxes,
plagued by the challenges associated with representing a complex and dynamic
reality. The field of Reinforcement Learning (RL), too, has a rich history, and
has recently been at the centre of several exponential developments. Modern RL
implementations have been able to achieve unprecedented levels of
sophistication, handling previously unthinkable degrees of complexity. This
review surveys the historical barriers of classical agent-based techniques in
economic modelling, and contemplates whether recent developments in RL can
overcome any of them.",None,26
Rendering Nighttime Image Via Cascaded Color and Brightness Compensation,0.0,"Image signal processing (ISP) is crucial for camera imaging, and neural
networks (NN) solutions are extensively deployed for daytime scenes. The lack
of sufficient nighttime image dataset and insights on nighttime illumination
characteristics poses a great challenge for high-quality rendering using
existing NN ISPs. To tackle it, we first built a high-resolution nighttime
RAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert
professionals. Meanwhile, to best capture the characteristics of nighttime
illumination light sources, we develop the CBUnet, a two-stage NN ISP to
cascade the compensation of color and brightness attributes. Experiments show
that our method has better visual quality compared to traditional ISP pipeline,
and is ranked at the second place in the NTIRE 2022 Night Photography Rendering
Challenge for two tracks by respective People's and Professional Photographer's
choices. The code and relevant materials are avaiable on our website:
https://njuvision.github.io/CBUnet.",None,6
SparseFormer: Attention-based Depth Completion Network,0.076309,"Most pipelines for Augmented and Virtual Reality estimate the ego-motion of
the camera by creating a map of sparse 3D landmarks. In this paper, we tackle
the problem of depth completion, that is, densifying this sparse 3D map using
RGB images as guidance. This remains a challenging problem due to the low
density, non-uniform and outlier-prone 3D landmarks produced by SfM and SLAM
pipelines. We introduce a transformer block, SparseFormer, that fuses 3D
landmarks with deep visual features to produce dense depth. The SparseFormer
has a global receptive field, making the module especially effective for depth
completion with low-density and non-uniform landmarks. To address the issue of
depth outliers among the 3D landmarks, we introduce a trainable refinement
module that filters outliers through attention between the sparse landmarks.",None,1155
Gradient Obfuscation Checklist Test Gives a False Sense of Security,0.0152661,"One popular group of defense techniques against adversarial attacks is based
on injecting stochastic noise into the network. The main source of robustness
of such stochastic defenses however is often due to the obfuscation of the
gradients, offering a false sense of security. Since most of the popular
adversarial attacks are optimization-based, obfuscated gradients reduce their
attacking ability, while the model is still susceptible to stronger or
specifically tailored adversarial attacks. Recently, five characteristics have
been identified, which are commonly observed when the improvement in robustness
is mainly caused by gradient obfuscation. It has since become a trend to use
these five characteristics as a sufficient test, to determine whether or not
gradient obfuscation is the main source of robustness. However, these
characteristics do not perfectly characterize all existing cases of gradient
obfuscation, and therefore can not serve as a basis for a conclusive test. In
this work, we present a counterexample, showing this test is not sufficient for
concluding that gradient obfuscation is not the main cause of improvements in
robustness.",None,220689
SAVCHOI: Detecting Suspicious Activities using Dense Video Captioning with Human Object Interactions,0.00860225,"Detecting suspicious activities in surveillance videos is a longstanding
problem in real-time surveillance that leads to difficulties in detecting
crimes. Hence, we propose a novel approach for detecting and summarizing
suspicious activities in surveillance videos. We have also created ground truth
summaries for the UCF-Crime video dataset. We modify a pre-existing approach
for this task by leveraging the Human-Object Interaction (HOI) model for the
Visual features in the Bi-Modal Transformer. Further, we validate our approach
against the existing state-of-the-art algorithms for the Dense Video Captioning
task for the ActivityNet Captions dataset. We observe that this formulation for
Dense Captioning performs significantly better than other discussed BMT-based
approaches for BLEU@1, BLEU@2, BLEU@3, BLEU@4, and METEOR. We further perform a
comparative analysis of the dataset and the model to report the findings based
on different NMS thresholds (searched using Genetic Algorithms). Here, our
formulation outperforms all the models for BLEU@1, BLEU@2, BLEU@3, and most
models for BLEU@4 and METEOR falling short of only ADV-INF Global by 25% and
0.5%, respectively.",https://github.com/v-iashin/BMT,246
Physically-admissible polarimetric data augmentation for road-scene analysis,0.00740112,"Polarimetric imaging, along with deep learning, has shown improved
performances on different tasks including scene analysis. However, its
robustness may be questioned because of the small size of the training
datasets. Though the issue could be solved by data augmentation, polarization
modalities are subject to physical feasibility constraints unaddressed by
classical data augmentation techniques. To address this issue, we propose to
use CycleGAN, an image translation technique based on deep generative models
that solely relies on unpaired data, to transfer large labeled road scene
datasets to the polarimetric domain. We design several auxiliary loss terms
that, alongside the CycleGAN losses, deal with the physical constraints of
polarimetric images. The efficiency of this solution is demonstrated on road
scene object detection tasks where generated realistic polarimetric images
allow to improve performances on cars and pedestrian detection up to 9%. The
resulting constrained CycleGAN is publicly released, allowing anyone to
generate their own polarimetric images.",https://anonymous.4open.science/r/4a83820e-9c65-417c-af3a-ab2979d6e2e8/,11454
Explaining Causal Models with Argumentation: the Case of Bi-variate Reinforcement,0.0,"Causal models are playing an increasingly important role in machine learning,
particularly in the realm of explainable AI. We introduce a conceptualisation
for generating argumentation frameworks (AFs) from causal models for the
purpose of forging explanations for the models' outputs. The conceptualisation
is based on reinterpreting desirable properties of semantics of AFs as
explanation moulds, which are means for characterising the relations in the
causal model argumentatively. We demonstrate our methodology by reinterpreting
the property of bi-variate reinforcement as an explanation mould to forge
bipolar AFs as explanations for the outputs of causal models. We perform a
theoretical evaluation of these argumentative explanations, examining whether
they satisfy a range of desirable explanatory and argumentative properties.",None,12536
Pretraining a Neural Network before Knowing Its Architecture,0.0118744,"Training large neural networks is possible by training a smaller hypernetwork
that predicts parameters for the large ones. A recently released Graph
HyperNetwork (GHN) trained this way on one million smaller ImageNet
architectures is able to predict parameters for large unseen networks such as
ResNet-50. While networks with predicted parameters lose performance on the
source task, the predicted parameters have been found useful for fine-tuning on
other tasks. We study if fine-tuning based on the same GHN is still useful on
novel strong architectures that were published after the GHN had been trained.
We found that for recent architectures such as ConvNeXt, GHN initialization
becomes less useful than for ResNet-50. One potential reason is the increased
distribution shift of novel architectures from those used to train the GHN. We
also found that the predicted parameters lack the diversity necessary to
successfully fine-tune parameters with gradient descent. We alleviate this
limitation by applying simple post-processing techniques to predicted
parameters before fine-tuning them on a target task and improve fine-tuning of
ResNet-50 and ConvNeXt.",https://github.com/facebookresearch/ppuda,1042
Towards a Unified Approach to Homography Estimation Using Image Features and Pixel Intensities,0.0367754,"The homography matrix is a key component in various vision-based robotic
tasks. Traditionally, homography estimation algorithms are classified into
feature- or intensity-based. The main advantages of the latter are their
versatility, accuracy, and robustness to arbitrary illumination changes. On the
other hand, they have a smaller domain of convergence than the feature-based
solutions. Their combination is hence promising, but existing techniques only
apply them sequentially. This paper proposes a new hybrid method that unifies
both classes into a single nonlinear optimization procedure, applies the same
minimization method, and uses the same homography parametrization and warping
function. Experimental validation using a classical testing framework shows
that the proposed unified approach has improved convergence properties compared
to each individual class. These are also demonstrated in a visual tracking
application. As a final contribution, our ready-to-use implementation of the
algorithm is made publicly available to the research community.",https://github.com/visiotec/vtec_ros,1660
On Faithfulness and Coherence of Language Explanations for Recommendation Systems,0.0,"Reviews contain rich information about product characteristics and user
interests and thus are commonly used to boost recommender system performance.
Specifically, previous work show that jointly learning to perform review
generation improves rating prediction performance. Meanwhile, these
model-produced reviews serve as recommendation explanations, providing the user
with insights on predicted ratings. However, while existing models could
generate fluent, human-like reviews, it is unclear to what degree the reviews
fully uncover the rationale behind the jointly predicted rating. In this work,
we perform a series of evaluations that probes state-of-the-art models and
their review generation component. We show that the generated explanations are
brittle and need further evaluation before being taken as literal rationales
for the estimated ratings.",None,30843
RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset for Natural Disaster Damage Assessment,0.063903,"Recent advancements in computer vision and deep learning techniques have
facilitated notable progress in scene understanding, thereby assisting rescue
teams in achieving precise damage assessment. In this paper, we present
RescueNet, a meticulously curated high-resolution post-disaster dataset that
includes detailed classification and semantic segmentation annotations. This
dataset aims to facilitate comprehensive scene understanding in the aftermath
of natural disasters. RescueNet comprises post-disaster images collected after
Hurricane Michael, obtained using Unmanned Aerial Vehicles (UAVs) from multiple
impacted regions. The uniqueness of RescueNet lies in its provision of
high-resolution post-disaster imagery, accompanied by comprehensive annotations
for each image. Unlike existing datasets that offer annotations limited to
specific scene elements such as buildings, RescueNet provides pixel-level
annotations for all classes, including buildings, roads, pools, trees, and
more. Furthermore, we evaluate the utility of the dataset by implementing
state-of-the-art segmentation models on RescueNet, demonstrating its value in
enhancing existing methodologies for natural disaster damage assessment.",https://github.com/BinaLab/RescueNet-A-High-Resolution-Post-Disaster-UAV-Dataset-for-Semantic-Segmentation/tree/main,1500
Prediction of GPU Failures Under Deep Learning Workloads,0.0136165,"Graphics processing units (GPUs) are the de facto standard for processing
deep learning (DL) tasks. Meanwhile, GPU failures, which are inevitable, cause
severe consequences in DL tasks: they disrupt distributed trainings, crash
inference services, and result in service level agreement violations. To
mitigate the problem caused by GPU failures, we propose to predict failures by
using ML models. This paper is the first to study prediction models of GPU
failures under large-scale production deep learning workloads. As a starting
point, we evaluate classic prediction models and observe that predictions of
these models are both inaccurate and unstable. To improve the precision and
stability of predictions, we propose several techniques, including parallel and
cascade model-ensemble mechanisms and a sliding training method. We evaluate
the performances of our various techniques on a four-month production dataset
including 350 million entries. The results show that our proposed techniques
improve the prediction precision from 46.3\% to 84.0\%.",None,27020
Auto Machine Learning for Medical Image Analysis by Unifying the Search on Data Augmentation and Neural Architecture,0.00299789,"Automated data augmentation, which aims at engineering augmentation policy
automatically, recently draw a growing research interest. Many previous
auto-augmentation methods utilized a Density Matching strategy by evaluating
policies in terms of the test-time augmentation performance. In this paper, we
theoretically and empirically demonstrated the inconsistency between the train
and validation set of small-scale medical image datasets, referred to as
in-domain sampling bias. Next, we demonstrated that the in-domain sampling bias
might cause the inefficiency of Density Matching. To address the problem, an
improved augmentation search strategy, named Augmented Density Matching, was
proposed by randomly sampling policies from a prior distribution for training.
Moreover, an efficient automatical machine learning(AutoML) algorithm was
proposed by unifying the search on data augmentation and neural architecture.
Experimental results indicated that the proposed methods outperformed
state-of-the-art approaches on MedMNIST, a pioneering benchmark designed for
AutoML in medical image analysis.",None,456
SimSR: Simple Distance-based State Representation for Deep Reinforcement Learning,0.973652,"This work explores how to learn robust and generalizable state representation
from image-based observations with deep reinforcement learning methods.
Addressing the computational complexity, stringent assumptions and
representation collapse challenges in existing work of bisimulation metric, we
devise Simple State Representation (SimSR) operator. SimSR enables us to design
a stochastic approximation method that can practically learn the mapping
functions (encoders) from observations to latent representation space. In
addition to the theoretical analysis and comparison with the existing work, we
experimented and compared our work with recent state-of-the-art solutions in
visual MuJoCo tasks. The results shows that our model generally achieves better
performance and has better robustness and good generalization.",https://github.com/bit1029public/SimSR,9769
On the Convergence of Semi-Relaxed Sinkhorn with Marginal Constraint and OT Distance Gaps,0.0304123,"This paper presents consideration of the Semi-Relaxed Sinkhorn (SR-Sinkhorn)
algorithm for the semi-relaxed optimal transport (SROT) problem, which relaxes
one marginal constraint of the standard OT problem. For evaluation of how the
constraint relaxation affects the algorithm behavior and solution, it is
vitally necessary to present the theoretical convergence analysis in terms not
only of the functional value gap, but also of the marginal constraint gap as
well as the OT distance gap. However, no existing work has addressed all
analyses simultaneously. To this end, this paper presents a comprehensive
convergence analysis for SR-Sinkhorn. After presenting the
$\epsilon$-approximation of the functional value gap based on a new proof
strategy and exploiting this proof strategy, we give the upper bound of the
marginal constraint gap. We also provide its convergence to the
$\epsilon$-approximation when two distributions are in the probability simplex.
Furthermore, the convergence analysis of the OT distance gap to the
$\epsilon$-approximation is given as assisted by the obtained marginal
constraint gap. The latter two theoretical results are the first results
presented in the literature related to the SROT problem.",https://github.com/lntk/uot,1980
Language Model Classifier Aligns Better with Physician Word Sensitivity than XGBoost on Readmission Prediction,0.0147813,"Traditional evaluation metrics for classification in natural language
processing such as accuracy and area under the curve fail to differentiate
between models with different predictive behaviors despite their similar
performance metrics. We introduce sensitivity score, a metric that scrutinizes
models' behaviors at the vocabulary level to provide insights into disparities
in their decision-making logic. We assess the sensitivity score on a set of
representative words in the test set using two classifiers trained for hospital
readmission classification with similar performance statistics. Our experiments
compare the decision-making logic of clinicians and classifiers based on rank
correlations of sensitivity scores. The results indicate that the language
model's sensitivity score aligns better with the professionals than the xgboost
classifier on tf-idf embeddings, which suggests that xgboost uses some spurious
features. Overall, this metric offers a novel perspective on assessing models'
robustness by quantifying their discrepancy with professional opinions. Our
code is available on GitHub (https://github.com/nyuolab/Model_Sensitivity).",None,150035
Pixel is All You Need: Adversarial Trajectory-Ensemble Active Learning for Salient Object Detection,0.0872335,"Although weakly-supervised techniques can reduce the labeling effort, it is
unclear whether a saliency model trained with weakly-supervised data (e.g.,
point annotation) can achieve the equivalent performance of its
fully-supervised version. This paper attempts to answer this unexplored
question by proving a hypothesis: there is a point-labeled dataset where
saliency models trained on it can achieve equivalent performance when trained
on the densely annotated dataset. To prove this conjecture, we proposed a novel
yet effective adversarial trajectory-ensemble active learning (ATAL). Our
contributions are three-fold: 1) Our proposed adversarial attack triggering
uncertainty can conquer the overconfidence of existing active learning methods
and accurately locate these uncertain pixels. {2)} Our proposed
trajectory-ensemble uncertainty estimation method maintains the advantages of
the ensemble networks while significantly reducing the computational cost. {3)}
Our proposed relationship-aware diversity sampling algorithm can conquer
oversampling while boosting performance. Experimental results show that our
ATAL can find such a point-labeled dataset, where a saliency model trained on
it obtained $97\%$ -- $99\%$ performance of its fully-supervised version with
only ten annotated points per image.",None,14647
Approximating Constraint Manifolds Using Generative Models for Sampling-Based Constrained Motion Planning,0.0645846,"Sampling-based motion planning under task constraints is challenging because
the null-measure constraint manifold in the configuration space makes rejection
sampling extremely inefficient, if not impossible. This paper presents a
learning-based sampling strategy for constrained motion planning problems. We
investigate the use of two well-known deep generative models, the Conditional
Variational Autoencoder (CVAE) and the Conditional Generative Adversarial Net
(CGAN), to generate constraint-satisfying sample configurations. Instead of
precomputed graphs, we use generative models conditioned on constraint
parameters for approximating the constraint manifold. This approach allows for
the efficient drawing of constraint-satisfying samples online without any need
for modification of available sampling-based motion planning algorithms. We
evaluate the efficiency of these two generative models in terms of their
sampling accuracy and coverage of sampling distribution. Simulations and
experiments are also conducted for different constraint tasks on two robotic
platforms.",None,9510
IISERB Brains at SemEval 2022 Task 6: A Deep-learning Framework to Identify Intended Sarcasm in English,0.0272678,"This paper describes the system architectures and the models submitted by our
team ""IISERBBrains"" to SemEval 2022 Task 6 competition. We contested for all
three sub-tasks floated for the English dataset. On the leader-board, wegot19th
rank out of43 teams for sub-taskA, the 8th rank out of22 teams for sub-task
B,and13th rank out of 16 teams for sub-taskC. Apart from the submitted results
and models, we also report the other models and results that we obtained
through our experiments after organizers published the gold labels of their
evaluation data",https://github.com/manojmahan/,23278
Mobile Robot Manipulation using Pure Object Detection,0.0214074,"This paper addresses the problem of mobile robot manipulation using object
detection. Our approach uses detection and control as complimentary functions
that learn from real-world interactions. We develop an end-to-end manipulation
method based solely on detection and introduce Task-focused Few-shot Object
Detection (TFOD) to learn new objects and settings. Our robot collects its own
training data and automatically determines when to retrain detection to improve
performance across various subtasks (e.g., grasping). Notably, detection
training is low-cost, and our robot learns to manipulate new objects using as
few as four clicks of annotation. In physical experiments, our robot learns
visual control from a single click of annotation and a novel update
formulation, manipulates new objects in clutter and other mobile settings, and
achieves state-of-the-art results on an existing visual servo control and depth
estimation benchmark. Finally, we develop a TFOD Benchmark to support future
object detection research for robotics: https://github.com/griffbr/tfod.",https://github.com/griffbr/tfod,-1
Semantic Segmentation in Learned Compressed Domain,0.0400202,"Most machine vision tasks (e.g., semantic segmentation) are based on images
encoded and decoded by image compression algorithms (e.g., JPEG). However,
these decoded images in the pixel domain introduce distortion, and they are
optimized for human perception, making the performance of machine vision tasks
suboptimal. In this paper, we propose a method based on the compressed domain
to improve segmentation tasks. i) A dynamic and a static channel selection
method are proposed to reduce the redundancy of compressed representations that
are obtained by encoding. ii) Two different transform modules are explored and
analyzed to help the compressed representation be transformed as the features
in the segmentation network. The experimental results show that we can save up
to 15.8\% bitrates compared with a state-of-the-art compressed domain-based
work while saving up to about 83.6\% bitrates and 44.8\% inference time
compared with the pixel domain-based method.",None,4822
TYPIC: A Corpus of Template-Based Diagnostic Comments on Argumentation,0.115804,"Providing feedback on the argumentation of the learner is essential for
developing critical thinking skills, however, it requires a lot of time and
effort. To mitigate the overload on teachers, we aim to automate a process of
providing feedback, especially giving diagnostic comments which point out the
weaknesses inherent in the argumentation. It is recommended to give specific
diagnostic comments so that learners can recognize the diagnosis without
misinterpretation. However, it is not obvious how the task of providing
specific diagnostic comments should be formulated. We present a formulation of
the task as template selection and slot filling to make an automatic evaluation
easier and the behavior of the model more tractable. The key to the formulation
is the possibility of creating a template set that is sufficient for practical
use. In this paper, we define three criteria that a template set should
satisfy: expressiveness, informativeness, and uniqueness, and verify the
feasibility of creating a template set that satisfies these criteria as a first
trial. We will show that it is feasible through an annotation study that
converts diagnostic comments given in a text to a template format. The corpus
used in the annotation study is publicly available.",https://github.com/cl-tohoku/TYPIC,10162
HYU at SemEval-2022 Task 2: Effective Idiomaticity Detection with Consideration at Different Levels of Contextualization,0.0121256,"We propose a unified framework that enables us to consider various aspects of
contextualization at different levels to better identify the idiomaticity of
multi-word expressions. Through extensive experiments, we demonstrate that our
approach based on the inter- and inner-sentence context of a target MWE is
effective in improving the performance of related models. We also share our
experience in detail on the task of SemEval-2022 Tasks 2 such that future work
on the same task can be benefited from this.",None,614
An Adaptive Repeated-Intersection-Reduction Local Search for the Maximum Independent Set Problem,0.0106721,"The maximum independent set (MIS) problem, a classical NP-hard problem with
extensive applications in various areas, aims to find the largest set of
vertices with no edge among them. Due to its computational intractability, it
is difficult to solve the MIS problem effectively, especially on large graphs.
Employing heuristic approaches to obtain a good solution within an acceptable
amount of time has attracted much attention in literature. In this paper, we
propose an efficient local search framework for MIS called ARIR, which
encompasses two main parts: a lightweight adaptive mechanism and a novel
inexact efficient reduction rule to simplify instances. Based on ARIR, three
algorithms -- ARIR-I, ARIR-II, and ARIR-III -- are developed by adopting three
distinct reduction strategies. We conduct experiments on five benchmarks,
encompassing 92 instances. Compared with six state-of-the-art algorithms, our
ARIR-based algorithms offer the best accuracy on the majority of instances,
while obtaining competitive results on the remaining instances.",None,9354
Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias,0.00279971,"Artificial Intelligence (AI) is increasingly used to build Decision Support
Systems (DSS) across many domains. This paper describes a series of experiments
designed to observe human response to different characteristics of a DSS such
as accuracy and bias, particularly the extent to which participants rely on the
DSS, and the performance they achieve. In our experiments, participants play a
simple online game inspired by so-called ""wildcat"" (i.e., exploratory) drilling
for oil. The landscape has two layers: a visible layer describing the costs
(terrain), and a hidden layer describing the reward (oil yield). Participants
in the control group play the game without receiving any assistance, while in
treatment groups they are assisted by a DSS suggesting places to drill. For
certain treatments, the DSS does not consider costs, but only rewards, which
introduces a bias that is observable by users. Between subjects, we vary the
accuracy and bias of the DSS, and observe the participants' total score, time
to completion, the extent to which they follow or ignore suggestions. We also
measure the acceptability of the DSS in an exit survey. Our results show that
participants tend to score better with the DSS, that the score increase is due
to users following the DSS advice, and related to the difficulty of the game
and the accuracy of the DSS. We observe that this setting elicits mostly
rational behavior from participants, who place a moderate amount of trust in
the DSS and show neither algorithmic aversion (under-reliance) nor automation
bias (over-reliance).However, their stated willingness to accept the DSS in the
exit survey seems less sensitive to the accuracy of the DSS than their
behavior, suggesting that users are only partially aware of the (lack of)
accuracy of the DSS.",None,27850
Memory-Based Label-Text Tuning for Few-Shot Class-Incremental Learning,0.0301272,"Few-shot class-incremental learning(FSCIL) focuses on designing learning
algorithms that can continually learn a sequence of new tasks from a few
samples without forgetting old ones. The difficulties are that training on a
sequence of limited data from new tasks leads to severe overfitting issues and
causes the well-known catastrophic forgetting problem. Existing researches
mainly utilize the image information, such as storing the image knowledge of
previous tasks or limiting classifiers updating. However, they ignore analyzing
the informative and less noisy text information of class labels. In this work,
we propose leveraging the label-text information by adopting the memory prompt.
The memory prompt can learn new data sequentially, and meanwhile store the
previous knowledge. Furthermore, to optimize the memory prompt without
undermining the stored knowledge, we propose a stimulation-based training
strategy. It optimizes the memory prompt depending on the image embedding
stimulation, which is the distribution of the image embedding elements.
Experiments show that our proposed method outperforms all prior
state-of-the-art approaches, significantly mitigating the catastrophic
forgetting and overfitting problems.",None,1503
Skit-S2I: An Indian Accented Speech to Intent dataset,0.125586,"Conventional conversation assistants extract text transcripts from the speech
signal using automatic speech recognition (ASR) and then predict intent from
the transcriptions. Using end-to-end spoken language understanding (SLU), the
intents of the speaker are predicted directly from the speech signal without
requiring intermediate text transcripts. As a result, the model can optimize
directly for intent classification and avoid cascading errors from ASR. The
end-to-end SLU system also helps in reducing the latency of the intent
prediction model. Although many datasets are available publicly for
text-to-intent tasks, the availability of labeled speech-to-intent datasets is
limited, and there are no datasets available in the Indian accent. In this
paper, we release the Skit-S2I dataset, the first publicly available
Indian-accented SLU dataset in the banking domain in a conversational tonality.
We experiment with multiple baselines, compare different pretrained speech
encoder's representations, and find that SSL pretrained representations perform
slightly better than ASR pretrained representations lacking prosodic features
for speech-to-intent classification. The dataset and baseline code is available
at \url{https://github.com/skit-ai/speech-to-intent-dataset}",https://github.com/skit-ai/speech-to-intent-dataset,52
Multi-modal Video Chapter Generation,0.0444212,"Chapter generation becomes practical technique for online videos nowadays.
The chapter breakpoints enable users to quickly find the parts they want and
get the summative annotations. However, there is no public method and dataset
for this task. To facilitate the research along this direction, we introduce a
new dataset called Chapter-Gen, which consists of approximately 10k
user-generated videos with annotated chapter information. Our data collection
procedure is fast, scalable and does not require any additional manual
annotation. On top of this dataset, we design an effective baseline specificlly
for video chapters generation task. which captures two aspects of a
video,including visual dynamics and narration text. It disentangles local and
global video features for localization and title generation respectively. To
parse the long video efficiently, a skip sliding window mechanism is designed
to localize potential chapters. And a cross attention multi-modal fusion module
is developed to aggregate local features for title generation. Our experiments
demonstrate that the proposed framework achieves superior results over existing
methods which illustrate that the method design for similar task cannot be
transfered directly even after fine-tuning. Code and dataset are available at
https://github.com/czt117/MVCG.",https://github.com/czt117/MVCG,2561
Self-Supervised Speech Representations Preserve Speech Characteristics while Anonymizing Voices,0.0104756,"Collecting speech data is an important step in training speech recognition
systems and other speech-based machine learning models. However, the issue of
privacy protection is an increasing concern that must be addressed. The current
study investigates the use of voice conversion as a method for anonymizing
voices. In particular, we train several voice conversion models using
self-supervised speech representations including Wav2Vec2.0, Hubert and
UniSpeech. Converted voices retain a low word error rate within 1% of the
original voice. Equal error rate increases from 1.52% to 46.24% on the
LibriSpeech test set and from 3.75% to 45.84% on speakers from the VCTK corpus
which signifies degraded performance on speaker verification. Lastly, we
conduct experiments on dysarthric speech data to show that speech features
relevant to articulation, prosody, phonation and phonology can be extracted
from anonymized voices for discriminating between healthy and pathological
speech.",https://github.com/jcvasquezc/DisVoice,4486
Multi-Level Modeling Units for End-to-End Mandarin Speech Recognition,0.0337623,"The choice of modeling units is crucial for automatic speech recognition
(ASR) tasks. In mandarin scenarios, the Chinese characters represent meaning
but are not directly related to the pronunciation. Thus only considering the
writing of Chinese characters as modeling units is insufficient to capture
speech features. In this paper, we present a novel method involves with
multi-level modeling units, which integrates multi-level information for
mandarin speech recognition. Specifically, the encoder block considers
syllables as modeling units and the decoder block deals with character-level
modeling units. To facilitate the incremental conversion from syllable features
to character features, we design an auxiliary task that applies cross-entropy
(CE) loss to intermediate decoder layers. During inference, the input feature
sequences are converted into syllable sequences by the encoder block and then
converted into Chinese characters by the decoder block. Experiments on the
widely used AISHELL-1 corpus demonstrate that our method achieves promising
results with CER of 4.1%/4.6% and 4.6%/5.2%, using the Conformer and the
Transformer backbones respectively.",https://github.com/mozillazg/python-pinyin,11
Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning,0.0435659,"This paper presents Non-Axiomatic Term Logic (NATL) as a theoretical
computational framework of humanlike symbolic reasoning in artificial
intelligence. NATL unites a discrete syntactic system inspired from Aristotle's
term logic and a continuous semantic system based on the modern idea of
distributed representations, or embeddings. This paper positions the proposed
approach in the phylogeny and the literature of logic, and explains the
framework. As it is yet no more than a theory and it requires much further
elaboration to implement it, no quantitative evaluation is presented. Instead,
qualitative analyses of arguments using NATL, some applications to possible
cognitive science/robotics-related research, and remaining issues towards a
machinery implementation are discussed.",None,2609
Using Multiple Instance Learning to Build Multimodal Representations,0.0479948,"Image-text multimodal representation learning aligns data across modalities
and enables important medical applications, e.g., image classification, visual
grounding, and cross-modal retrieval. In this work, we establish a connection
between multimodal representation learning and multiple instance learning.
Based on this connection, we propose a generic framework for constructing
permutation-invariant score functions with many existing multimodal
representation learning approaches as special cases. Furthermore, we use the
framework to derive a novel contrastive learning approach and demonstrate that
our method achieves state-of-the-art results in several downstream tasks.",None,32032
Place Recognition under Occlusion and Changing Appearance via Disentangled Representations,0.0,"Place recognition is a critical and challenging task for mobile robots,
aiming to retrieve an image captured at the same place as a query image from a
database. Existing methods tend to fail while robots move autonomously under
occlusion (e.g., car, bus, truck) and changing appearance (e.g., illumination
changes, seasonal variation). Because they encode the image into only one code,
entangling place features with appearance and occlusion features. To overcome
this limitation, we propose PROCA, an unsupervised approach to decompose the
image representation into three codes: a place code used as a descriptor to
retrieve images, an appearance code that captures appearance properties, and an
occlusion code that encodes occlusion content. Extensive experiments show that
our model outperforms the state-of-the-art methods. Our code and data are
available at https://github.com/rover-xingyu/PROCA.",https://github.com/rover-xingyu/PROCA,204
Utilizing unsupervised learning to improve sward content prediction and herbage mass estimation,0.00813804,"Sward species composition estimation is a tedious one. Herbage must be
collected in the field, manually separated into components, dried and weighed
to estimate species composition. Deep learning approaches using neural networks
have been used in previous work to propose faster and more cost efficient
alternatives to this process by estimating the biomass information from a
picture of an area of pasture alone. Deep learning approaches have, however,
struggled to generalize to distant geographical locations and necessitated
further data collection to retrain and perform optimally in different climates.
In this work, we enhance the deep learning solution by reducing the need for
ground-truthed (GT) images when training the neural network. We demonstrate how
unsupervised contrastive learning can be used in the sward composition
prediction problem and compare with the state-of-the-art on the publicly
available GrassClover dataset collected in Denmark as well as a more recent
dataset from Ireland where we tackle herbage mass and height estimation.",https://git.io/JMrY1,4356
LHDR: HDR Reconstruction for Legacy Content using a Lightweight DNN,0.029289,"High dynamic range (HDR) image is widely-used in graphics and photography due
to the rich information it contains. Recently the community has started using
deep neural network (DNN) to reconstruct standard dynamic range (SDR) images
into HDR. Albeit the superiority of current DNN-based methods, their
application scenario is still limited: (1) heavy model impedes real-time
processing, and (2) inapplicable to legacy SDR content with more degradation
types. Therefore, we propose a lightweight DNN-based method trained to tackle
legacy SDR. For better design, we reform the problem modeling and emphasize
degradation model. Experiments show that our method reached appealing
performance with minimal computational cost compared with others.",https://www.github.com/AndreGuo/LHDR,1977
Toxicity Detection for Indic Multilingual Social Media Content,0.0555733,"Toxic content is one of the most critical issues for social media platforms
today. India alone had 518 million social media users in 2020. In order to
provide a good experience to content creators and their audience, it is crucial
to flag toxic comments and the users who post that. But the big challenge is
identifying toxicity in low resource Indic languages because of the presence of
multiple representations of the same text. Moreover, the posts/comments on
social media do not adhere to a particular format, grammar or sentence
structure; this makes the task of abuse detection even more challenging for
multilingual social media platforms. This paper describes the system proposed
by team 'Moj Masti' using the data provided by ShareChat/Moj in \emph{IIIT-D
Multilingual Abusive Comment Identification} challenge. We focus on how we can
leverage multilingual transformer based pre-trained and fine-tuned models to
approach code-mixed/code-switched classification tasks. Our best performing
system was an ensemble of XLM-RoBERTa and MuRIL which achieved a Mean F-1 score
of 0.9 on the test data/leaderboard. We also observed an increase in the
performance by adding transliterated data. Furthermore, using weak metadata,
ensembling and some post-processing techniques boosted the performance of our
system, thereby placing us 1st on the leaderboard.",https://github.com/isi-nlp/uroman,4373
Spatio-Temporal Crop Aggregation for Video Representation Learning,0.00665404,"We propose Spatio-temporal Crop Aggregation for video representation LEarning
(SCALE), a novel method that enjoys high scalability at both training and
inference time. Our model builds long-range video features by learning from
sets of video clip-level features extracted with a pre-trained backbone. To
train the model, we propose a self-supervised objective consisting of masked
clip feature prediction. We apply sparsity to both the input, by extracting a
random set of video clips, and to the loss function, by only reconstructing the
sparse inputs. Moreover, we use dimensionality reduction by working in the
latent space of a pre-trained backbone applied to single video clips. These
techniques make our method not only extremely efficient to train but also
highly effective in transfer learning. We demonstrate that our video
representation yields state-of-the-art performance with linear, non-linear, and
KNN probing on common action classification and video understanding datasets.",https://github.com/facebookresearch/slowfast,12585
Multi-domain Unsupervised Image-to-Image Translation with Appearance Adaptive Convolution,0.0206728,"Over the past few years, image-to-image (I2I) translation methods have been
proposed to translate a given image into diverse outputs. Despite the
impressive results, they mainly focus on the I2I translation between two
domains, so the multi-domain I2I translation still remains a challenge. To
address this problem, we propose a novel multi-domain unsupervised
image-to-image translation (MDUIT) framework that leverages the decomposed
content feature and appearance adaptive convolution to translate an image into
a target appearance while preserving the given geometric content. We also
exploit a contrast learning objective, which improves the disentanglement
ability and effectively utilizes multi-domain image data in the training
process by pairing the semantically similar images. This allows our method to
learn the diverse mappings between multiple visual domains with only a single
framework. We show that the proposed method produces visually diverse and
plausible results in multiple domains compared to the state-of-the-art methods.",None,8983
Visible and Near Infrared Image Fusion Based on Texture Information,0.0163406,"Multi-sensor fusion is widely used in the environment perception system of
the autonomous vehicle. It solves the interference caused by environmental
changes and makes the whole driving system safer and more reliable. In this
paper, a novel visible and near-infrared fusion method based on texture
information is proposed to enhance unstructured environmental images. It aims
at the problems of artifact, information loss and noise in traditional visible
and near infrared image fusion methods. Firstly, the structure information of
the visible image (RGB) and the near infrared image (NIR) after texture removal
is obtained by relative total variation (RTV) calculation as the base layer of
the fused image; secondly, a Bayesian classification model is established to
calculate the noise weight and the noise information and the noise information
in the visible image is adaptively filtered by joint bilateral filter; finally,
the fused image is acquired by color space conversion. The experimental results
demonstrate that the proposed algorithm can preserve the spectral
characteristics and the unique information of visible and near-infrared images
without artifacts and color distortion, and has good robustness as well as
preserving the unique texture.",None,11617
Applying a Generic Sequence-to-Sequence Model for Simple and Effective Keyphrase Generation,0.203218,"In recent years, a number of keyphrase generation (KPG) approaches were
proposed consisting of complex model architectures, dedicated training
paradigms and decoding strategies. In this work, we opt for simplicity and show
how a commonly used seq2seq language model, BART, can be easily adapted to
generate keyphrases from the text in a single batch computation using a simple
training procedure. Empirical results on five benchmarks show that our approach
is as good as the existing state-of-the-art KPG systems, but using a much
simpler and easy to deploy framework.",https://github.com/memray/OpenNMT-kpg-release,3252
Is Face Recognition Safe from Realizable Attacks?,0.0479591,"Face recognition is a popular form of biometric authentication and due to its
widespread use, attacks have become more common as well. Recent studies show
that Face Recognition Systems are vulnerable to attacks and can lead to
erroneous identification of faces. Interestingly, most of these attacks are
white-box, or they are manipulating facial images in ways that are not
physically realizable. In this paper, we propose an attack scheme where the
attacker can generate realistic synthesized face images with subtle
perturbations and physically realize that onto his face to attack black-box
face recognition systems. Comprehensive experiments and analyses show that
subtle perturbations realized on attackers face can create successful attacks
on state-of-the-art face recognition systems in black-box settings. Our study
exposes the underlying vulnerability posed by the Face Recognition Systems
against realizable black-box attacks.",None,9417
A Benchmark Generator for Combinatorial Testing,0.00787416,"Combinatorial Testing (CT) tools are essential to test properly a wide range
of systems (train systems, Graphical User Interfaces (GUIs), autonomous driving
systems, etc). While there is an active research community working on
developing CT tools, paradoxically little attention has been paid to making
available enough resources to test the CT tools themselves. In particular, the
set of available benchmarks to asses their correctness, effectiveness and
efficiency is rather limited. In this paper, we introduce a new generator of CT
benchmarks that essentially borrows the structure contained in the plethora of
available Combinatorial Problems from other research communities in order to
create meaningful benchmarks. We additionally perform an extensive evaluation
of CT tools with these new benchmarks. Thanks to this study we provide some
insights on under which circumstances a particular CT tool should be used.",None,3005
Proximal Policy Optimization with Graph Neural Networks for Optimal Power Flow,0.0277721,"Optimal Power Flow (OPF) is a very traditional research area within the power
systems field that seeks for the optimal operation point of electric power
plants, and which needs to be solved every few minutes in real-world scenarios.
However, due to the nonconvexities that arise in power generation systems,
there is not yet a fast, robust solution technique for the full Alternating
Current Optimal Power Flow (ACOPF). In the last decades, power grids have
evolved into a typical dynamic, non-linear and large-scale control system,
known as the power system, so searching for better and faster ACOPF solutions
is becoming crucial. Appearance of Graph Neural Networks (GNN) has allowed the
natural use of Machine Learning (ML) algorithms on graph data, such as power
networks. On the other hand, Deep Reinforcement Learning (DRL) is known for its
powerful capability to solve complex decision-making problems. Although
solutions that use these two methods separately are beginning to appear in the
literature, none has yet combined the advantages of both. We propose a novel
architecture based on the Proximal Policy Optimization algorithm with Graph
Neural Networks to solve the Optimal Power Flow. The objective is to design an
architecture that learns how to solve the optimization problem and that is at
the same time able to generalize to unseen scenarios. We compare our solution
with the DCOPF in terms of cost after having trained our DRL agent on IEEE 30
bus system and then computing the OPF on that base network with topology
changes",None,7078
Empirical Bayes approach to Truth Discovery problems,0.00369509,"When aggregating information from conflicting sources, one's goal is to find
the truth. Most real-value \emph{truth discovery} (TD) algorithms try to
achieve this goal by estimating the competence of each source and then
aggregating the conflicting information by weighing each source's answer
proportionally to her competence. However, each of those algorithms requires
more than a single source for such estimation and usually does not consider
different estimation methods other than a weighted mean. Therefore, in this
work we formulate, prove, and empirically test the conditions for an Empirical
Bayes Estimator (EBE) to dominate the weighted mean aggregation. Our main
result demonstrates that EBE, under mild conditions, can be used as a second
step of any TD algorithm in order to reduce the expected error.",None,1961
Z-BERT-A: a zero-shot Pipeline for Unknown Intent detection,0.0264381,"Intent discovery is a crucial task in natural language processing, and it is
increasingly relevant for various of industrial applications. Identifying
novel, unseen intents from user inputs remains one of the biggest challenges in
this field. Herein, we propose Zero-Shot-BERT-Adapters, a two-stage method for
multilingual intent discovery relying on a Transformer architecture, fine-tuned
with Adapters. We train the model for Natural Language Inference (NLI) and
later perform unknown intent classification in a zero-shot setting for multiple
languages. In our evaluation, we first analyze the quality of the model after
adaptive fine-tuning on known classes. Secondly, we evaluate its performance in
casting intent classification as an NLI task. Lastly, we test the zero-shot
performance of the model on unseen classes, showing how Zero-Shot-BERT-Adapters
can effectively perform intent discovery by generating semantically similar
intents, if not equal, to the ground-truth ones. Our experiments show how
Zero-Shot-BERT-Adapters outperforms various baselines in two zero-shot
settings: known intent classification and unseen intent discovery. The proposed
pipeline holds the potential for broad application in customer care. It enables
automated dynamic triage using a lightweight model that can be easily deployed
and scaled in various business scenarios, unlike large language models.
Zero-Shot-BERT-Adapters represents an innovative multi-language approach for
intent discovery, enabling the online generation of novel intents. A Python
package implementing the pipeline and the new datasets we compiled are
available at the following link:
https://github.com/GT4SD/zero-shot-bert-adapters.",https://github.com/GT4SD/zero-shot-bert-adapters,4092
Text normalization for low-resource languages: the case of Ligurian,0.270937,"Text normalization is a crucial technology for low-resource languages which
lack rigid spelling conventions or that have undergone multiple spelling
reforms. Low-resource text normalization has so far relied upon hand-crafted
rules, which are perceived to be more data efficient than neural methods. In
this paper we examine the case of text normalization for Ligurian, an
endangered Romance language. We collect 4,394 Ligurian sentences paired with
their normalized versions, as well as the first open source monolingual corpus
for Ligurian. We show that, in spite of the small amounts of data available, a
compact transformer-based model can be trained to achieve very low error rates
by the use of backtranslation and appropriate tokenization.",https://github.com/fleanend/fairseq-text-normalizer,51
Visualizing and Explaining Language Models,0.0,"During the last decade, Natural Language Processing has become, after
Computer Vision, the second field of Artificial Intelligence that was massively
changed by the advent of Deep Learning. Regardless of the architecture, the
language models of the day need to be able to process or generate text, as well
as predict missing words, sentences or relations depending on the task. Due to
their black-box nature, such models are difficult to interpret and explain to
third parties. Visualization is often the bridge that language model designers
use to explain their work, as the coloring of the salient words and phrases,
clustering or neuron activations can be used to quickly understand the
underlying models. This paper showcases the techniques used in some of the most
popular Deep Learning for NLP visualizations, with a special focus on
interpretability and explainability.",None,457
Deep Generative Framework for Interactive 3D Terrain Authoring and Manipulation,0.181999,"Automated generation and (user) authoring of the realistic virtual terrain is
most sought for by the multimedia applications like VR models and gaming. The
most common representation adopted for terrain is Digital Elevation Model
(DEM). Existing terrain authoring and modeling techniques have addressed some
of these and can be broadly categorized as: procedural modeling, simulation
method, and example-based methods. In this paper, we propose a novel realistic
terrain authoring framework powered by a combination of VAE and generative
conditional GAN model. Our framework is an example-based method that attempts
to overcome the limitations of existing methods by learning a latent space from
a real-world terrain dataset. This latent space allows us to generate multiple
variants of terrain from a single input as well as interpolate between terrains
while keeping the generated terrains close to real-world data distribution. We
also developed an interactive tool, that lets the user generate diverse
terrains with minimalist inputs. We perform thorough qualitative and
quantitative analysis and provide comparisons with other SOTA methods. We
intend to release our code/tool to the academic community.",None,775
Conservative Distributional Reinforcement Learning with Safety Constraints,0.0358157,"Safety exploration can be regarded as a constrained Markov decision problem
where the expected long-term cost is constrained. Previous off-policy
algorithms convert the constrained optimization problem into the corresponding
unconstrained dual problem by introducing the Lagrangian relaxation technique.
However, the cost function of the above algorithms provides inaccurate
estimations and causes the instability of the Lagrange multiplier learning. In
this paper, we present a novel off-policy reinforcement learning algorithm
called Conservative Distributional Maximum a Posteriori Policy Optimization
(CDMPO). At first, to accurately judge whether the current situation satisfies
the constraints, CDMPO adapts distributional reinforcement learning method to
estimate the Q-function and C-function. Then, CDMPO uses a conservative value
function loss to reduce the number of violations of constraints during the
exploration process. In addition, we utilize Weighted Average Proportional
Integral Derivative (WAPID) to update the Lagrange multiplier stably. Empirical
results show that the proposed method has fewer violations of constraints in
the early exploration process. The final test results also illustrate that our
method has better risk control.",None,6541
Race Bias Analysis of Bona Fide Errors in face anti-spoofing,0.0244182,"The study of bias in Machine Learning is receiving a lot of attention in
recent years, however, few only papers deal explicitly with the problem of race
bias in face anti-spoofing. In this paper, we present a systematic study of
race bias in face anti-spoofing with three key characteristics: the focus is on
analysing potential bias in the bona fide errors, where significant ethical and
legal issues lie; the analysis is not restricted to the final binary outcomes
of the classifier, but also covers the classifier's scalar responses and its
latent space; the threshold determining the operating point of the classifier
is considered a variable. We demonstrate the proposed bias analysis process on
a VQ-VAE based face anti-spoofing algorithm, trained on the Replay Attack and
the Spoof in the Wild (SiW) databases, and analysed for bias on the SiW and
Racial Faces in the Wild (RFW), databases. The results demonstrate that race
bias is not necessarily the result of different mean response values among the
various populations. Instead, it can be better understood as the combined
effect of several possible characteristics of the response distributions:
different means; different variances; bimodal behaviour; existence of outliers.",None,1378
Which country is this picture from? New data and methods for DNN-based country recognition,0.0236664,"Recognizing the country where a picture has been taken has many potential
applications, such as identification of fake news and prevention of
disinformation campaigns. Previous works focused on the estimation of the
geo-coordinates where a picture has been taken. Yet, recognizing in which
country an image was taken could be more critical, from a semantic and forensic
point of view, than estimating its spatial coordinates. In the above framework,
this paper provides two contributions. First, we introduce the VIPPGeo dataset,
containing 3.8 million geo-tagged images. Secondly, we used the dataset to
train a model casting the country recognition problem as a classification
problem. The experiments show that our model provides better results than the
current state of the art. Notably, we found that asking the network to identify
the country provides better results than estimating the geo-coordinates and
then tracing them back to the country where the picture was taken.",https://github.com/alamayreh/VIPPGeo_Dataset,19639
Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ),0.0643065,"A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed
for image coding in this work. The fundamental idea of image coding is to
remove correlations among pixels before quantization and entropy coding, e.g.,
the discrete cosine transform (DCT) and intra predictions, adopted by modern
image coding standards. We present a new method to remove pixel correlations.
First, by decomposing correlations into long- and short-range correlations, we
represent long-range correlations in coarser grids due to their smoothness,
thus leading to a multi-grid (MG) coding architecture. Second, we show that
short-range correlations can be effectively coded by a suite of vector
quantizers (VQs). Along this line, we argue the effectiveness of VQs of very
large block sizes and present a convenient way to implement them. It is shown
by experimental results that MGBVQ offers excellent rate-distortion (RD)
performance, which is comparable with existing image coders, at much lower
complexity. Besides, it provides a progressive coded bitstream.",None,48516
SubER: A Metric for Automatic Evaluation of Subtitle Quality,0.0396593,"This paper addresses the problem of evaluating the quality of automatically
generated subtitles, which includes not only the quality of the
machine-transcribed or translated speech, but also the quality of line
segmentation and subtitle timing. We propose SubER - a single novel metric
based on edit distance with shifts that takes all of these subtitle properties
into account. We compare it to existing metrics for evaluating transcription,
translation, and subtitle quality. A careful human evaluation in a post-editing
scenario shows that the new metric has a high correlation with the post-editing
effort and direct human assessment scores, outperforming baseline metrics
considering only the subtitle text, such as WER and BLEU, and existing methods
to integrate segmentation and timing features.",https://github.com/apptek/SubER,162
Utterance Rewriting with Contrastive Learning in Multi-turn Dialogue,0.0105128,"Context modeling plays a significant role in building multi-turn dialogue
systems. In order to make full use of context information, systems can use
Incomplete Utterance Rewriting(IUR) methods to simplify the multi-turn dialogue
into single-turn by merging current utterance and context information into a
self-contained utterance. However, previous approaches ignore the intent
consistency between the original query and rewritten query. The detection of
omitted or coreferred locations in the original query can be further improved.
In this paper, we introduce contrastive learning and multi-task learning to
jointly model the problem. Our method benefits from carefully designed
self-supervised objectives, which act as auxiliary tasks to capture semantics
at both sentence-level and token-level. The experiments show that our proposed
model achieves state-of-the-art performance on several public datasets.",None,199
WR-ONE2SET: Towards Well-Calibrated Keyphrase Generation,0.166182,"Keyphrase generation aims to automatically generate short phrases summarizing
an input document. The recently emerged ONE2SET paradigm (Ye et al., 2021)
generates keyphrases as a set and has achieved competitive performance.
Nevertheless, we observe serious calibration errors outputted by ONE2SET,
especially in the over-estimation of $\varnothing$ token (means ""no
corresponding keyphrase""). In this paper, we deeply analyze this limitation and
identify two main reasons behind: 1) the parallel generation has to introduce
excessive $\varnothing$ as padding tokens into training instances; and 2) the
training mechanism assigning target to each slot is unstable and further
aggravates the $\varnothing$ token over-estimation. To make the model
well-calibrated, we propose WR-ONE2SET which extends ONE2SET with an adaptive
instance-level cost Weighting strategy and a target Re-assignment mechanism.
The former dynamically penalizes the over-estimated slots for different
instances thus smoothing the uneven training distribution. The latter refines
the original inappropriate assignment and reduces the supervisory signals of
over-estimated slots. Experimental results on commonly-used datasets
demonstrate the effectiveness and generality of our proposed paradigm.",https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py,13445
Join-Chain Network: A Logical Reasoning View of the Multi-head Attention in Transformer,0.0225378,"Developing neural architectures that are capable of logical reasoning has
become increasingly important for a wide range of applications (e.g., natural
language processing). Towards this grand objective, we propose a symbolic
reasoning architecture that chains many join operators together to model output
logical expressions. In particular, we demonstrate that such an ensemble of
join-chains can express a broad subset of ''tree-structured'' first-order
logical expressions, named FOET, which is particularly useful for modeling
natural languages. To endow it with differentiable learning capability, we
closely examine various neural operators for approximating the symbolic
join-chains. Interestingly, we find that the widely used multi-head
self-attention module in transformer can be understood as a special neural
operator that implements the union bound of the join operator in probabilistic
predicate space. Our analysis not only provides a new perspective on the
mechanism of the pretrained models such as BERT for natural language
understanding but also suggests several important future improvement
directions.",None,620
Momentum Contrastive Pre-training for Question Answering,0.0166286,"Existing pre-training methods for extractive Question Answering (QA) generate
cloze-like queries different from natural questions in syntax structure, which
could overfit pre-trained models to simple keyword matching. In order to
address this problem, we propose a novel Momentum Contrastive pRe-training fOr
queStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS
introduces a momentum contrastive learning framework to align the answer
probability between cloze-like and natural query-passage sample pairs. Hence,
the pre-trained models can better transfer the knowledge learned in cloze-like
samples to answering natural questions. Experimental results on three
benchmarking QA datasets show that our method achieves noticeable improvement
compared with all baselines in both supervised and zero-shot scenarios.",https://github.com/mrqa/MRQA-Shared-Task-2019/tree/master/baseline,1294
OSLAT: Open Set Label Attention Transformer for Medical Entity Retrieval and Span Extraction,0.0224171,"Medical entity span extraction and linking are critical steps for many
healthcare NLP tasks. Most existing entity extraction methods either have a
fixed vocabulary of medical entities or require span annotations. In this
paper, we propose a method for linking an open set of entities that does not
require any span annotations. Our method, Open Set Label Attention Transformer
(OSLAT), uses the label-attention mechanism to learn candidate-entity
contextualized text representations. We find that OSLAT can not only link
entities but is also able to implicitly learn spans associated with entities.
We evaluate OSLAT on two tasks: (1) span extraction trained without explicit
span annotations, and (2) entity linking trained without span-level annotation.
We test the generalizability of our method by training two separate models on
two datasets with low entity overlap and comparing cross-dataset performance.",https://github.com/curai/curai-research/tree/main/OSLAT,6447
Towards Proper Contrastive Self-supervised Learning Strategies For Music Audio Representation,0.00872201,"The common research goal of self-supervised learning is to extract a general
representation which an arbitrary downstream task would benefit from. In this
work, we investigate music audio representation learned from different
contrastive self-supervised learning schemes and empirically evaluate the
embedded vectors on various music information retrieval (MIR) tasks where
different levels of the music perception are concerned. We analyze the results
to discuss the proper direction of contrastive learning strategies for
different MIR tasks. We show that these representations convey a comprehensive
information about the auditory characteristics of music in general, although
each of the self-supervision strategies has its own effectiveness in certain
aspect of information.",https://github.com/kunimi00/ContrastiveSSLMusicAudio,542
Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency,0.0103205,"The word embedding association test (WEAT) is an important method for
measuring linguistic biases against social groups such as ethnic minorities in
large text corpora. It does so by comparing the semantic relatedness of words
prototypical of the groups (e.g., names unique to those groups) and attribute
words (e.g., 'pleasant' and 'unpleasant' words). We show that anti-black WEAT
estimates from geo-tagged social media data at the level of metropolitan
statistical areas strongly correlate with several measures of racial
animus--even when controlling for sociodemographic covariates. However, we also
show that every one of these correlations is explained by a third variable: the
frequency of Black names in the underlying corpora relative to White names.
This occurs because word embeddings tend to group positive (negative) words and
frequent (rare) words together in the estimated semantic space. As the
frequency of Black names on social media is strongly correlated with Black
Americans' prevalence in the population, this results in spurious anti-Black
WEAT estimates wherever few Black Americans live. This suggests that research
using the WEAT to measure bias should consider term frequency, and also
demonstrates the potential consequences of using black-box models like word
embeddings to study human cognition and behavior.",None,22161
Needs and Artificial Intelligence,0.0,"Throughout their history, homo sapiens have used technologies to better
satisfy their needs. The relation between needs and technology is so
fundamental that the US National Research Council defined the distinguishing
characteristic of technology as its goal ""to make modifications in the world to
meet human needs"". Artificial intelligence (AI) is one of the most promising
emerging technologies of our time. Similar to other technologies, AI is
expected ""to meet [human] needs"". In this article, we reflect on the
relationship between needs and AI, and call for the realisation of needs-aware
AI systems. We argue that re-thinking needs for, through, and by AI can be a
very useful means towards the development of realistic approaches for
Sustainable, Human-centric, Accountable, Lawful, and Ethical (HALE) AI systems.
We discuss some of the most critical gaps, barriers, enablers, and drivers of
co-creating future AI-based socio-technical systems in which [human] needs are
well considered and met. Finally, we provide an overview of potential threats
and HALE considerations that should be carefully taken into account, and call
for joint, immediate, and interdisciplinary efforts and collaborations.",None,-1
Region-aware Attention for Image Inpainting,0.00770887,"Recent attention-based image inpainting methods have made inspiring progress
by modeling long-range dependencies within a single image. However, they tend
to generate blurry contents since the correlation between each pixel pairs is
always misled by ill-predicted features in holes. To handle this problem, we
propose a novel region-aware attention (RA) module. By avoiding the directly
calculating corralation between each pixel pair in a single samples and
considering the correlation between different samples, the misleading of
invalid information in holes can be avoided. Meanwhile, a learnable region
dictionary (LRD) is introduced to store important information in the entire
dataset, which not only simplifies correlation modeling, but also avoids
information redundancy. By applying RA in our architecture, our methodscan
generate semantically plausible results with realistic details. Extensive
experiments on CelebA, Places2 and Paris StreetView datasets validate the
superiority of our method compared with existing methods.",None,2173
Efficient Graph-Friendly COCO Metric Computation for Train-Time Model Evaluation,0.0,"Evaluating the COCO mean average precision (MaP) and COCO recall metrics as
part of the static computation graph of modern deep learning frameworks poses a
unique set of challenges. These challenges include the need for maintaining a
dynamic-sized state to compute mean average precision, reliance on global
dataset-level statistics to compute the metrics, and managing differing numbers
of bounding boxes between images in a batch. As a consequence, it is common
practice for researchers and practitioners to evaluate COCO metrics as a post
training evaluation step. With a graph-friendly algorithm to compute COCO Mean
Average Precision and recall, these metrics could be evaluated at training
time, improving visibility into the evolution of the metrics through training
curve plots, and decreasing iteration time when prototyping new model versions.
  Our contributions include an accurate approximation algorithm for Mean
Average Precision, an open source implementation of both COCO mean average
precision and COCO recall, extensive numerical benchmarks to verify the
accuracy of our implementations, and an open-source training loop that include
train-time evaluation of mean average precision and recall.",https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools,50970
Decomposing Counterfactual Explanations for Consequential Decision Making,0.0251091,"The goal of algorithmic recourse is to reverse unfavorable decisions (e.g.,
from loan denial to approval) under automated decision making by suggesting
actionable feature changes (e.g., reduce the number of credit cards). To
generate low-cost recourse the majority of methods work under the assumption
that the features are independently manipulable (IMF). To address the feature
dependency issue the recourse problem is usually studied through the causal
recourse paradigm. However, it is well known that strong assumptions, as
encoded in causal models and structural equations, hinder the applicability of
these methods in complex domains where causal dependency structures are
ambiguous. In this work, we develop \texttt{DEAR} (DisEntangling Algorithmic
Recourse), a novel and practical recourse framework that bridges the gap
between the IMF and the strong causal assumptions. \texttt{DEAR} generates
recourses by disentangling the latent representation of co-varying features
from a subset of promising recourse features to capture the main practical
recourse desiderata. Our experiments on real-world data corroborate our
theoretically motivated recourse model and highlight our framework's ability to
provide reliable, low-cost recourse in the presence of feature dependencies.",None,-1
Searching for fingerspelled content in American Sign Language,0.0117905,"Natural language processing for sign language video - including tasks like
recognition, translation, and search - is crucial for making artificial
intelligence technologies accessible to deaf individuals, and is gaining
research interest in recent years. In this paper, we address the problem of
searching for fingerspelled key-words or key phrases in raw sign language
videos. This is an important task since significant content in sign language is
often conveyed via fingerspelling, and to our knowledge the task has not been
studied before. We propose an end-to-end model for this task, FSS-Net, that
jointly detects fingerspelling and matches it to a text sequence. Our
experiments, done on a large public dataset of ASL fingerspelling in the wild,
show the importance of fingerspelling detection as a component of a search and
retrieval model. Our model significantly outperforms baseline methods adapted
from prior work on related tasks",None,15199
Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach,0.0359492,"Pre-trained models (PTMs) have lead to great improvements in natural language
generation (NLG). However, it is still unclear how much commonsense knowledge
they possess. With the goal of evaluating commonsense knowledge of NLG models,
recent work has proposed the problem of generative commonsense reasoning, e.g.,
to compose a logical sentence given a set of unordered concepts. Existing
approaches to this problem hypothesize that PTMs lack sufficient parametric
knowledge for this task, which can be overcome by introducing external
knowledge or task-specific pre-training objectives. Different from this trend,
we argue that PTM's inherent ability for generative commonsense reasoning is
underestimated due to the order-agnostic property of its input. In particular,
we hypothesize that the order of the input concepts can affect the PTM's
ability to utilize its commonsense knowledge. To this end, we propose a
pre-ordering approach to elaborately manipulate the order of the given concepts
before generation. Experiments show that our approach can outperform the more
sophisticated models that have access to a lot of external data and resources.",https://github.com/zhaochaocs/Planned-PTM,1956
Towards Soft Fairness in Restless Multi-Armed Bandits,0.0417937,"Restless multi-armed bandits (RMAB) is a framework for allocating limited
resources under uncertainty. It is an extremely useful model for monitoring
beneficiaries and executing timely interventions to ensure maximum benefit in
public health settings (e.g., ensuring patients take medicines in tuberculosis
settings, ensuring pregnant mothers listen to automated calls about good
pregnancy practices). Due to the limited resources, typically certain
communities or regions are starved of interventions that can have follow-on
effects. To avoid starvation in the executed interventions across
individuals/regions/communities, we first provide a soft fairness constraint
and then provide an approach to enforce the soft fairness constraint in RMABs.
The soft fairness constraint requires that an algorithm never probabilistically
favor one arm over another if the long-term cumulative reward of choosing the
latter arm is higher. Our approach incorporates softmax based value iteration
method in the RMAB setting to design selection algorithms that manage to
satisfy the proposed fairness constraint. Our method, referred to as SoftFair,
also provides theoretical performance guarantees and is asymptotically optimal.
Finally, we demonstrate the utility of our approaches on simulated benchmarks
and show that the soft fairness constraint can be handled without a significant
sacrifice on value.",None,4247
Language-Based Audio Retrieval with Converging Tied Layers and Contrastive Loss,0.0274361,"In this paper, we tackle the new Language-Based Audio Retrieval task proposed
in DCASE 2022. Firstly, we introduce a simple, scalable architecture which ties
both the audio and text encoder together. Secondly, we show that using this
architecture along with contrastive loss allows the model to significantly beat
the performance of the baseline model. Finally, in addition to having an
extremely low training memory requirement, we are able to use pretrained models
as it is without needing to finetune them. We test our methods and show that
using a combination of our methods beats the baseline scores significantly.",https://github.com/qiuqiangkong/audioset_tagging_cnn,8436
DEXTER: An end-to-end system to extract table contents from electronic medical health documents,0.0364185,"In this paper, we propose DEXTER, an end to end system to extract information
from tables present in medical health documents, such as electronic health
records (EHR) and explanation of benefits (EOB). DEXTER consists of four
sub-system stages: i) table detection ii) table type classification iii) cell
detection; and iv) cell content extraction. We propose a two-stage transfer
learning-based approach using CDeC-Net architecture along with Non-Maximal
suppression for table detection. We design a conventional computer vision-based
approach for table type classification and cell detection using parameterized
kernels based on image size for detecting rows and columns. Finally, we extract
the text from the detected cells using pre-existing OCR engine Tessaract. To
evaluate our system, we manually annotated a sample of the real-world medical
dataset (referred to as Meddata) consisting of wide variations of documents (in
terms of appearance) covering different table structures, such as bordered,
partially bordered, borderless, or coloured tables. We experimentally show that
DEXTER outperforms the commercially available Amazon Textract and Microsoft
Azure Form Recognizer systems on the annotated real-world medical dataset",None,22
Super-Resolution and Image Re-projection for Iris Recognition,0.0215979,"Several recent works have addressed the ability of deep learning to disclose
rich, hierarchical and discriminative models for the most diverse purposes.
Specifically in the super-resolution field, Convolutional Neural Networks
(CNNs) using different deep learning approaches attempt to recover realistic
texture and fine grained details from low resolution images. In this work we
explore the viability of these approaches for iris Super-Resolution (SR) in an
iris recognition environment. For this, we test different architectures with
and without a so called image re-projection to reduce artifacts applying it to
different iris databases to verify the viability of the different CNNs for iris
super-resolution. Results show that CNNs and image re-projection can improve
the results specially for the accuracy of recognition systems using a complete
different training database performing the transfer learning successfully.",None,5281
Mitigating Both Covariate and Conditional Shift for Domain Generalization,0.00892425,"Domain generalization (DG) aims to learn a model on several source domains,
hoping that the model can generalize well to unseen target domains. The
distribution shift between domains contains the covariate shift and conditional
shift, both of which the model must be able to handle for better
generalizability. In this paper, a novel DG method is proposed to deal with the
distribution shift via Visual Alignment and Uncertainty-guided belief Ensemble
(VAUE). Specifically, for the covariate shift, a visual alignment module is
designed to align the distribution of image style to a common empirical
Gaussian distribution so that the covariate shift can be eliminated in the
visual space. For the conditional shift, we adopt an uncertainty-guided belief
ensemble strategy based on the subjective logic and Dempster-Shafer theory. The
conditional distribution given a test sample is estimated by the dynamic
combination of that of source domains. Comprehensive experiments are conducted
to demonstrate the superior performance of the proposed method on four widely
used datasets, i.e., Office-Home, VLCS, TerraIncognita, and PACS.",None,505
Exploiting Feature Diversity for Make-up Temporal Video Grounding,0.0186548,"This technical report presents the 3rd winning solution for MTVG, a new task
introduced in the 4-th Person in Context (PIC) Challenge at ACM MM 2022. MTVG
aims at localizing the temporal boundary of the step in an untrimmed video
based on a textual description. The biggest challenge of this task is the fi
ne-grained video-text semantics of make-up steps. However, current methods
mainly extract video features using action-based pre-trained models. As actions
are more coarse-grained than make-up steps, action-based features are not
sufficient to provide fi ne-grained cues. To address this issue,we propose to
achieve fi ne-grained representation via exploiting feature diversities.
Specifically, we proposed a series of methods from feature extraction, network
optimization, to model ensemble. As a result, we achieved 3rd place in the MTVG
competition.",None,3977
The Inverse Problem for Argumentation Gradual Semantics,0.0249415,"Gradual semantics with abstract argumentation provide each argument with a
score reflecting its acceptability, i.e. how ""much"" it is attacked by other
arguments. Many different gradual semantics have been proposed in the
literature, each following different principles and producing different
argument rankings. A sub-class of such semantics, the so-called weighted
semantics, takes, in addition to the graph structure, an initial set of weights
over the arguments as input, with these weights affecting the resultant
argument ranking. In this work, we consider the inverse problem over such
weighted semantics. That is, given an argumentation framework and a desired
argument ranking, we ask whether there exist initial weights such that a
particular semantics produces the given ranking. The contribution of this paper
are: (1) an algorithm to answer this problem, (2) a characterisation of the
properties that a gradual semantics must satisfy for the algorithm to operate,
and (3) an empirical evaluation of the proposed algorithm.",https://github.com/jhudsy/,4513
Beyond Value: CHECKLIST for Testing Inferences in Planning-Based RL,0.0,"Reinforcement learning (RL) agents are commonly evaluated via their expected
value over a distribution of test scenarios. Unfortunately, this evaluation
approach provides limited evidence for post-deployment generalization beyond
the test distribution. In this paper, we address this limitation by extending
the recent CheckList testing methodology from natural language processing to
planning-based RL. Specifically, we consider testing RL agents that make
decisions via online tree search using a learned transition model and value
function. The key idea is to improve the assessment of future performance via a
CheckList approach for exploring and assessing the agent's inferences during
tree search. The approach provides the user with an interface and general
query-rule mechanism for identifying potential inference flaws and validating
expected inference invariances. We present a user study involving knowledgeable
AI researchers using the approach to evaluate an agent trained to play a
complex real-time strategy game. The results show the approach is effective in
allowing users to identify previously-unknown flaws in the agent's reasoning.
In addition, our analysis provides insight into how AI experts use this type of
testing approach, which may help improve future instantiations.",None,9498
Class Interference of Deep Neural Networks,0.00960967,"Recognizing and telling similar objects apart is even hard for human beings.
In this paper, we show that there is a phenomenon of class interference with
all deep neural networks. Class interference represents the learning difficulty
in data, and it constitutes the largest percentage of generalization errors by
deep networks. To understand class interference, we propose cross-class tests,
class ego directions and interference models. We show how to use these
definitions to study minima flatness and class interference of a trained model.
We also show how to detect class interference during training through label
dancing pattern and class dancing notes.",None,1068
Multi-Phase Multi-Objective Dexterous Manipulation with Adaptive Hierarchical Curriculum,0.0,"Dexterous manipulation tasks usually have multiple objectives, and the
priorities of these objectives may vary at different phases of a manipulation
task. Varying priority makes a robot hardly or even failed to learn an optimal
policy with a deep reinforcement learning (DRL) method. To solve this problem,
we develop a novel Adaptive Hierarchical Reward Mechanism (AHRM) to guide the
DRL agent to learn manipulation tasks with multiple prioritized objectives. The
AHRM can determine the objective priorities during the learning process and
update the reward hierarchy to adapt to the changing objective priorities at
different phases. The proposed method is validated in a multi-objective
manipulation task with a JACO robot arm in which the robot needs to manipulate
a target with obstacles surrounded. The simulation and physical experiment
results show that the proposed method improved robot learning in task
performance and learning efficiency.",None,1628
Box Supervised Video Segmentation Proposal Network,0.0135715,"Video Object Segmentation (VOS) has been targeted by various fully-supervised
and self-supervised approaches. While fully-supervised methods demonstrate
excellent results, self-supervised ones, which do not use pixel-level ground
truth, attract much attention. However, self-supervised approaches pose a
significant performance gap. Box-level annotations provide a balanced
compromise between labeling effort and result quality for image segmentation
but have not been exploited for the video domain. In this work, we propose a
box-supervised video object segmentation proposal network, which takes
advantage of intrinsic video properties. Our method incorporates object motion
in the following way: first, motion is computed using a bidirectional temporal
difference and a novel bounding box-guided motion compensation. Second, we
introduce a novel motion-aware affinity loss that encourages the network to
predict positive pixel pairs if they share similar motion and color. The
proposed method outperforms the state-of-the-art self-supervised benchmark by
16.4% and 6.9% $\mathcal{J}$ &$\mathcal{F}$ score and the majority of fully
supervised methods on the DAVIS and Youtube-VOS dataset without imposing
network architectural specifications. We provide extensive tests and ablations
on the datasets, demonstrating the robustness of our method.",https://github.com/Tanveer81/BoxVOS.git,4189
Revisiting Grammatical Error Correction Evaluation and Beyond,0.289992,"Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore
and BARTScore) have been widely used in several sentence generation tasks
(e.g., machine translation and text summarization) due to their better
correlation with human judgments over traditional overlap-based methods.
Although PT-based methods have become the de facto standard for training
grammatical error correction (GEC) systems, GEC evaluation still does not
benefit from pretrained knowledge. This paper takes the first step towards
understanding and improving GEC evaluation with pretraining. We first find that
arbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory
correlation results because of the excessive attention to inessential systems
outputs (e.g., unchanged parts). To alleviate the limitation, we propose a
novel GEC evaluation metric to achieve the best of both worlds, namely PT-M2
which only uses PT-based metrics to score those corrected parts. Experimental
results on the CoNLL14 evaluation task show that PT-M2 significantly
outperforms existing methods, achieving a new state-of-the-art result of 0.949
Pearson correlation. Further analysis reveals that PT-M2 is robust to evaluate
competitive GEC systems. Source code and scripts are freely available at
https://github.com/pygongnlp/PT-M2.",https://github.com/pygongnlp/PT-M2,803
ConceptNet infused DialoGPT for Underlying Commonsense Understanding and Reasoning in Dialogue Response Generation,0.0158676,"The pre-trained conversational models still fail to capture the implicit
commonsense (CS) knowledge hidden in the dialogue interaction, even though they
were pre-trained with an enormous dataset. In order to build a dialogue agent
with CS capability, we firstly inject external knowledge into a pre-trained
conversational model to establish basic commonsense through efficient Adapter
tuning (Section 4). Secondly, we propose the ``two-way learning'' method to
enable the bidirectional relationship between CS knowledge and sentence pairs
so that the model can generate a sentence given the CS triplets, also generate
the underlying CS knowledge given a sentence (Section 5). Finally, we leverage
this integrated CS capability to improve open-domain dialogue response
generation so that the dialogue agent is capable of understanding the CS
knowledge hidden in dialogue history on top of inferring related other
knowledge to further guide response generation (Section 6). The experiment
results demonstrate that CS\_Adapter fusion helps DialoGPT to be able to
generate series of CS knowledge. And the DialoGPT+CS\_Adapter response model
adapted from CommonGen training can generate underlying CS triplets that fits
better to dialogue context.",https://github.com/alexa/commonsense-dialogues,6007
Assembly Planning from Observations under Physical Constraints,0.031224,"This paper addresses the problem of copying an unknown assembly of primitives
with known shape and appearance using information extracted from a single
photograph by an off-the-shelf procedure for object detection and pose
estimation. The proposed algorithm uses a simple combination of physical
stability constraints, convex optimization and Monte Carlo tree search to plan
assemblies as sequences of pick-and-place operations represented by STRIPS
operators. It is efficient and, most importantly, robust to the errors in
object detection and pose estimation unavoidable in any real robotic system.
The proposed approach is demonstrated with thorough experiments on a UR5
manipulator.",None,147997
Machine Learning Challenges of Biological Factors in Insect Image Data,0.00373415,"The BIOSCAN project, led by the International Barcode of Life Consortium,
seeks to study changes in biodiversity on a global scale. One component of the
project is focused on studying the species interaction and dynamics of all
insects. In addition to genetically barcoding insects, over 1.5 million images
per year will be collected, each needing taxonomic classification. With the
immense volume of incoming images, relying solely on expert taxonomists to
label the images would be impossible; however, artificial intelligence and
computer vision technology may offer a viable high-throughput solution.
Additional tasks including manually weighing individual insects to determine
biomass, remain tedious and costly. Here again, computer vision may offer an
efficient and compelling alternative. While the use of computer vision methods
is appealing for addressing these problems, significant challenges resulting
from biological factors present themselves. These challenges are formulated in
the context of machine learning in this paper.",None,14680
MACC: Cross-Layer Multi-Agent Congestion Control with Deep Reinforcement Learning,0.00480452,"Congestion Control (CC), as the core networking task to efficiently utilize
network capacity, received great attention and widely used in various Internet
communication applications such as 5G, Internet-of-Things, UAN, and more.
Various CC algorithms have been proposed both on network and transport layers
such as Active Queue Management (AQM) algorithm and Transmission Control
Protocol (TCP) congestion control mechanism. But it is hard to model dynamic
AQM/TCP system and cooperate two algorithms to obtain excellent performance
under different communication scenarios. In this paper, we explore the
performance of multi-agent reinforcement learning-based cross-layer congestion
control algorithms and present cooperation performance of two agents, known as
MACC (Multi-agent Congestion Control). We implement MACC in NS3. The simulation
results show that our scheme outperforms other congestion control combination
in terms of throughput and delay, etc. Not only does it proves that networking
protocols based on multi-agent deep reinforcement learning is efficient for
communication managing, but also verifies that networking area can be used as
new playground for machine learning algorithms.",None,12818
Attention Mechanism with Energy-Friendly Operations,0.0361375,"Attention mechanism has become the dominant module in natural language
processing models. It is computationally intensive and depends on massive
power-hungry multiplications. In this paper, we rethink variants of attention
mechanism from the energy consumption aspects. After reaching the conclusion
that the energy costs of several energy-friendly operations are far less than
their multiplication counterparts, we build a novel attention model by
replacing multiplications with either selective operations or additions.
Empirical results on three machine translation tasks demonstrate that the
proposed model, against the vanilla one, achieves competitable accuracy while
saving 99\% and 66\% energy during alignment calculation and the whole
attention procedure. Code is available at: https://github.com/NLP2CT/E-Att.",https://github.com/NLP2CT/E-Att,4034
Detecting Shortcuts in Medical Images -- A Case Study in Chest X-rays,0.199263,"The availability of large public datasets and the increased amount of
computing power have shifted the interest of the medical community to
high-performance algorithms. However, little attention is paid to the quality
of the data and their annotations. High performance on benchmark datasets may
be reported without considering possible shortcuts or artifacts in the data,
besides, models are not tested on subpopulation groups. With this work, we aim
to raise awareness about shortcuts problems. We validate previous findings, and
present a case study on chest X-rays using two publicly available datasets. We
share annotations for a subset of pneumothorax images with drains. We conclude
with general recommendations for medical image classification.",https://github.com/ameliajimenez/shortcuts-chest-xray,3542
FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information,0.060499,"Online forms are widely used to collect data from human and have a
multi-billion market. Many software products provide online services for
creating semi-structured forms where questions and descriptions are organized
by pre-defined structures. However, the design and creation process of forms is
still tedious and requires expert knowledge. To assist form designers, in this
work we present FormLM to model online forms (by enhancing pre-trained language
model with form structural information) and recommend form creation ideas
(including question / options recommendations and block type suggestion). For
model training and evaluation, we collect the first public online form dataset
with 62K online forms. Experiment results show that FormLM significantly
outperforms general-purpose language models on all tasks, with an improvement
by 4.71 on Question Recommendation and 10.6 on Block Type Suggestion in terms
of ROUGE-1 and Macro-F1, respectively.",https://github.com/microsoft/FormLM,12944
Self-distilled Knowledge Delegator for Exemplar-free Class Incremental Learning,0.0461469,"Exemplar-free incremental learning is extremely challenging due to
inaccessibility of data from old tasks. In this paper, we attempt to exploit
the knowledge encoded in a previously trained classification model to handle
the catastrophic forgetting problem in continual learning. Specifically, we
introduce a so-called knowledge delegator, which is capable of transferring
knowledge from the trained model to a randomly re-initialized new model by
generating informative samples. Given the previous model only, the delegator is
effectively learned using a self-distillation mechanism in a data-free manner.
The knowledge extracted by the delegator is then utilized to maintain the
performance of the model on old tasks in incremental learning. This simple
incremental learning framework surpasses existing exemplar-free methods by a
large margin on four widely used class incremental benchmarks, namely
CIFAR-100, ImageNet-Subset, Caltech-101 and Flowers-102. Notably, we achieve
comparable performance to some exemplar-based methods without accessing any
exemplars.",https://github.com/google/deepdream,8286
Multi-task Safe Reinforcement Learning for Navigating Intersections in Dense Traffic,0.0684253,"Multi-task intersection navigation including the unprotected turning left,
turning right, and going straight in dense traffic is still a challenging task
for autonomous driving. For the human driver, the negotiation skill with other
interactive vehicles is the key to guarantee safety and efficiency. However, it
is hard to balance the safety and efficiency of the autonomous vehicle for
multi-task intersection navigation. In this paper, we formulate a multi-task
safe reinforcement learning with social attention to improve the safety and
efficiency when interacting with other traffic participants. Specifically, the
social attention module is used to focus on the states of negotiation vehicles.
In addition, a safety layer is added to the multi-task reinforcement learning
framework to guarantee safe negotiation. We compare the experiments in the
simulator SUMO with abundant traffic flows and CARLA with high-fidelity vehicle
models, which both show that the proposed algorithm can improve safety with
consistent traffic efficiency for multi-task intersection navigation.",None,11964
Multimodality Multi-Lead ECG Arrhythmia Classification using Self-Supervised Learning,0.0273177,"Electrocardiogram (ECG) signal is one of the most effective sources of
information mainly employed for the diagnosis and prediction of cardiovascular
diseases (CVDs) connected with the abnormalities in heart rhythm. Clearly,
single modality ECG (i.e. time series) cannot convey its complete
characteristics, thus, exploiting both time and time-frequency modalities in
the form of time-series data and spectrogram is needed. Leveraging the
cutting-edge self-supervised learning (SSL) technique on unlabeled data, we
propose SSL-based multimodality ECG classification. Our proposed network
follows SSL learning paradigm and consists of two modules corresponding to
pre-stream task, and down-stream task, respectively. In the SSL-pre-stream
task, we utilize self-knowledge distillation (KD) techniques with no labeled
data, on various transformations and in both time and frequency domains. In the
down-stream task, which is trained on labeled data, we propose a gate fusion
mechanism to fuse information from multimodality.To evaluate the effectiveness
of our approach, ten-fold cross validation on the 12-lead PhysioNet 2020
dataset has been conducted.",https://github.com/UARK-AICV/ECG SSL,6361
Mathematically Modeling the Lexicon Entropy of Emergent Language,0.0162712,"We formulate a stochastic process, FiLex, as a mathematical model of lexicon
entropy in deep learning-based emergent language systems. Defining a model
mathematically allows it to generate clear predictions which can be directly
and decisively tested. We empirically verify across four different environments
that FiLex predicts the correct correlation between hyperparameters (training
steps, lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax
temperature) and the emergent language's entropy in 20 out of 20
environment-hyperparameter combinations. Furthermore, our experiments reveal
that different environments show diverse relationships between their
hyperparameters and entropy which demonstrates the need for a model which can
make well-defined predictions at a precise level of granularity.",https://github.com/brendon-boldt/filex-emergent-language,1641
Towards Using Promises for Multi-Agent Cooperation in Goal Reasoning,0.0210063,"Reasoning and planning for mobile robots is a challenging problem, as the
world evolves over time and thus the robot's goals may change. One technique to
tackle this problem is goal reasoning, where the agent not only reasons about
its actions, but also about which goals to pursue. While goal reasoning for
single agents has been researched extensively, distributed, multi-agent goal
reasoning comes with additional challenges, especially in a distributed
setting. In such a context, some form of coordination is necessary to allow for
cooperative behavior. Previous goal reasoning approaches share the agent's
world model with the other agents, which already enables basic cooperation.
However, the agent's goals, and thus its intentions, are typically not shared.
  In this paper, we present a method to tackle this limitation. Extending an
existing goal reasoning framework, we propose enabling cooperative behavior
between multiple agents through promises, where an agent may promise that
certain facts will be true at some point in the future. Sharing these promises
allows other agents to not only consider the current state of the world, but
also the intentions of other agents when deciding on which goal to pursue next.
We describe how promises can be incorporated into the goal life cycle, a
commonly used goal refinement mechanism. We then show how promises can be used
when planning for a particular goal by connecting them to timed initial
literals (TILs) from PDDL planning. Finally, we evaluate our prototypical
implementation in a simplified logistics scenario.",None,7969
Interpretable Distribution Shift Detection using Optimal Transport,0.00358784,"We propose a method to identify and characterize distribution shifts in
classification datasets based on optimal transport. It allows the user to
identify the extent to which each class is affected by the shift, and retrieves
corresponding pairs of samples to provide insights on its nature. We illustrate
its use on synthetic and natural shift examples. While the results we present
are preliminary, we hope that this inspires future work on interpretable
methods for analyzing distribution shifts.",None,14789
Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks,0.00341851,"Parameter-efficient tuning aims to distill knowledge for downstream tasks by
optimizing a few introduced parameters while freezing the pretrained language
models (PLMs). Continuous prompt tuning which prepends a few trainable vectors
to the embeddings of input is one of these methods and has drawn much attention
due to its effectiveness and efficiency. This family of methods can be
illustrated as exerting nonlinear transformations of hidden states inside PLMs.
However, a natural question is ignored: can the hidden states be directly used
for classification without changing them? In this paper, we aim to answer this
question by proposing a simple tuning method which only introduces three
trainable vectors. Firstly, we integrate all layers hidden states using the
introduced vectors. And then, we input the integrated hidden state(s) to a
task-specific linear classifier to predict categories. This scheme is similar
to the way ELMo utilises hidden states except that they feed the hidden states
to LSTM-based models. Although our proposed tuning scheme is simple, it
achieves comparable performance with prompt tuning methods like P-tuning and
P-tuning v2, verifying that original hidden states do contain useful
information for classification tasks. Moreover, our method has an advantage
over prompt tuning in terms of time and the number of parameters.",None,334
Leveraging Deepfakes to Close the Domain Gap between Real and Synthetic Images in Facial Capture Pipelines,0.0181622,"We propose an end-to-end pipeline for both building and tracking 3D facial
models from personalized in-the-wild (cellphone, webcam, youtube clips, etc.)
video data. First, we present a method for automatic data curation and
retrieval based on a hierarchical clustering framework typical of collision
detection algorithms in traditional computer graphics pipelines. Subsequently,
we utilize synthetic turntables and leverage deepfake technology in order to
build a synthetic multi-view stereo pipeline for appearance capture that is
robust to imperfect synthetic geometry and image misalignment. The resulting
model is fit with an animation rig, which is then used to track facial
performances. Notably, our novel use of deepfake technology enables us to
perform robust tracking of in-the-wild data using differentiable renderers
despite a significant synthetic-to-real domain gap. Finally, we outline how we
train a motion capture regressor, leveraging the aforementioned techniques to
avoid the need for real-world ground truth data and/or a high-end calibrated
camera capture setup.",None,234
StyLandGAN: A StyleGAN based Landscape Image Synthesis using Depth-map,0.0146625,"Despite recent success in conditional image synthesis, prevalent input
conditions such as semantics and edges are not clear enough to express `Linear
(Ridges)' and `Planar (Scale)' representations. To address this problem, we
propose a novel framework StyLandGAN, which synthesizes desired landscape
images using a depth map which has higher expressive power. Our StyleLandGAN is
extended from the unconditional generation model to accept input conditions. We
also propose a '2-phase inference' pipeline which generates diverse depth maps
and shifts local parts so that it can easily reflect user's intend. As a
comparison, we modified the existing semantic image synthesis models to accept
a depth map as well. Experimental results show that our method is superior to
existing methods in quality, diversity, and depth-accuracy.",None,966
Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov Decision Processes,0.0186003,"Policy gradient (PG) is a reinforcement learning (RL) approach that optimizes
a parameterized policy model for an expected return using gradient ascent.
Given a well-parameterized policy model, such as a neural network model, with
appropriate initial parameters, the PG algorithms work well even when
environment does not have the Markov property. Otherwise, they can be trapped
on a plateau or suffer from peakiness effects. As another successful RL
approach, algorithms based on Monte-Carlo Tree Search (MCTS), which include
AlphaZero, have obtained groundbreaking results especially on the board game
playing domain. They are also suitable to be applied to non-Markov decision
processes. However, since the standard MCTS does not have the ability to learn
state representation, the size of the tree-search space can be too large to
search. In this work, we examine a mixture policy of PG and MCTS to complement
each other's difficulties and take advantage of them. We derive conditions for
asymptotic convergence with results of a two-timescale stochastic approximation
and propose an algorithm that satisfies these conditions. The effectivity of
the proposed methods is verified through numerical experiments on non-Markov
decision processes.",None,1194
Multi-fidelity Gaussian Process for Biomanufacturing Process Modeling with Small Data,0.0323653,"In biomanufacturing, developing an accurate model to simulate the complex
dynamics of bioprocesses is an important yet challenging task. This is
partially due to the uncertainty associated with bioprocesses, high data
acquisition cost, and lack of data availability to learn complex relations in
bioprocesses. To deal with these challenges, we propose to use a statistical
machine learning approach, multi-fidelity Gaussian process, for process
modelling in biomanufacturing. Gaussian process regression is a
well-established technique based on probability theory which can naturally
consider uncertainty in a dataset via Gaussian noise, and multi-fidelity
techniques can make use of multiple sources of information with different
levels of fidelity, thus suitable for bioprocess modeling with small data. We
apply the multi-fidelity Gaussian process to solve two significant problems in
biomanufacturing, bioreactor scale-up and knowledge transfer across cell lines,
and demonstrate its efficacy on real-world datasets.",None,14625
Enhanced Vehicle Re-identification for ITS: A Feature Fusion approach using Deep Learning,0.0163673,"In recent years, the development of robust Intelligent transportation systems
(ITS) is tackled across the globe to provide better traffic efficiency by
reducing frequent traffic problems. As an application of ITS, vehicle
re-identification has gained ample interest in the domain of computer vision
and robotics. Convolutional neural network (CNN) based methods are developed to
perform vehicle re-identification to address key challenges such as occlusion,
illumination change, scale, etc. The advancement of transformers in computer
vision has opened an opportunity to explore the re-identification process
further to enhance performance. In this paper, a framework is developed to
perform the re-identification of vehicles across CCTV cameras. To perform
re-identification, the proposed framework fuses the vehicle representation
learned using a CNN and a transformer model. The framework is tested on a
dataset that contains 81 unique vehicle identities observed across 20 CCTV
cameras. From the experiments, the fused vehicle re-identification framework
yields an mAP of 61.73% which is significantly better when compared with the
standalone CNN or transformer model.",None,2569
Explainability as statistical inference,0.00298812,"A wide variety of model explanation approaches have been proposed in recent
years, all guided by very different rationales and heuristics. In this paper,
we take a new route and cast interpretability as a statistical inference
problem. We propose a general deep probabilistic model designed to produce
interpretable predictions. The model parameters can be learned via maximum
likelihood, and the method can be adapted to any predictor network architecture
and any type of prediction problem. Our method is a case of amortized
interpretability models, where a neural network is used as a selector to allow
for fast interpretation at inference time. Several popular interpretability
methods are shown to be particular cases of regularised maximum likelihood for
our general model. We propose new datasets with ground truth selection which
allow for the evaluation of the features importance map. Using these datasets,
we show experimentally that using multiple imputation provides more reasonable
interpretations.",None,1641
A Hierarchical Deep Neural Network for Detecting Lines of Codes with Vulnerabilities,0.00734378,"Software vulnerabilities, caused by unintentional flaws in source codes, are
the main root cause of cyberattacks. Source code static analysis has been used
extensively to detect the unintentional defects, i.e. vulnerabilities,
introduced into the source codes by software developers. In this paper, we
propose a deep learning approach to detect vulnerabilities from their LLVM IR
representations based on the techniques that have been used in natural language
processing. The proposed approach uses a hierarchical process to first identify
source codes with vulnerabilities, and then it identifies the lines of codes
that contribute to the vulnerability within the detected source codes. This
proposed two-step approach reduces the false alarm of detecting vulnerable
lines. Our extensive experiment on real-world and synthetic codes collected in
NVD and SARD shows high accuracy (about 98\%) in detecting source code
vulnerabilities.",https://github.com/arashmahyari/PLP,-1
Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks,0.0192598,"Planning under uncertainty is an area of interest in artificial intelligence.
We present a novel approach based on tree search and graph machine learning for
the scheduling problem known as Disjunctive Temporal Networks with Uncertainty
(DTNU). Dynamic Controllability (DC) of DTNUs seeks a reactive scheduling
strategy to satisfy temporal constraints in response to uncontrollable action
durations. We introduce new semantics for reactive scheduling: Time-based
Dynamic Controllability (TDC) and a restricted subset of TDC, R-TDC. We design
a tree search algorithm to determine whether or not a DTNU is R-TDC. Moreover,
we leverage a graph neural network as a heuristic for tree search guidance.
Finally, we conduct experiments on a known benchmark on which we show R-TDC to
retain significant completeness with regard to DC, while being faster to prove.
This results in the tree search processing fifty percent more DTNU problems in
R-TDC than the state-of-the-art DC solver does in DC with the same time budget.
We also observe that graph neural network search guidance leads to substantial
performance gains on benchmarks of more complex DTNUs, with up to eleven times
more problems solved than the baseline tree search.",None,3149
Transformers in Action: Weakly Supervised Action Segmentation,0.0503694,"The video action segmentation task is regularly explored under weaker forms
of supervision, such as transcript supervision, where a list of actions is
easier to obtain than dense frame-wise labels. In this formulation, the task
presents various challenges for sequence modeling approaches due to the
emphasis on action transition points, long sequence lengths, and frame
contextualization, making the task well-posed for transformers. Given
developments enabling transformers to scale linearly, we demonstrate through
our architecture how they can be applied to improve action alignment accuracy
over the equivalent RNN-based models with the attention mechanism focusing
around salient action transition regions. Additionally, given the recent focus
on inference-time transcript selection, we propose a supplemental transcript
embedding approach to select transcripts more quickly at inference-time.
Furthermore, we subsequently demonstrate how this approach can also improve the
overall segmentation performance. Finally, we evaluate our proposed methods
across the benchmark datasets to better understand the applicability of
transformers and the importance of transcript selection on this video-driven
weakly-supervised task.",None,78284
From Pedestrian Detection to Crosswalk Estimation: An EM Algorithm and Analysis on Diverse Datasets,0.00413915,"In this work, we contribute an EM algorithm for estimation of corner points
and linear crossing segments for both marked and unmarked pedestrian crosswalks
using the detections of pedestrians from processed LiDAR point clouds or camera
images. We demonstrate the algorithmic performance by analyzing three
real-world datasets containing multiple periods of data collection for
four-corner and two-corner intersections with marked and unmarked crosswalks.
Additionally, we include a Python video tool to visualize the crossing
parameter estimation, pedestrian trajectories, and phase intervals in our
public source code.",None,39442
ANAct: Adaptive Normalization for Activation Functions,0.0122596,"In this paper, we investigate the negative effect of activation functions on
forward and backward propagation and how to counteract this effect. First, We
examine how activation functions affect the forward and backward propagation of
neural networks and derive a general form for gradient variance that extends
the previous work in this area. We try to use mini-batch statistics to
dynamically update the normalization factor to ensure the normalization
property throughout the training process, rather than only accounting for the
state of the neural network after weight initialization. Second, we propose
ANAct, a method that normalizes activation functions to maintain consistent
gradient variance across layers and demonstrate its effectiveness through
experiments. We observe that the convergence rate is roughly related to the
normalization property. We compare ANAct with several common activation
functions on CNNs and residual networks and show that ANAct consistently
improves their performance. For instance, normalized Swish achieves 1.4\%
higher top-1 accuracy than vanilla Swish on ResNet50 with the Tiny ImageNet
dataset and more than 1.2\% higher with CIFAR-100.",None,-1
Classification of Misinformation in New Articles using Natural Language Processing and a Recurrent Neural Network,0.0262376,"This paper seeks to address the classification of misinformation in news
articles using a Long Short Term Memory Recurrent Neural Network. Articles were
taken from 2018; a year that was filled with reporters writing about President
Donald Trump, Special Counsel Robert Mueller, the Fifa World Cup, and Russia.
The model presented successfully classifies these articles with an accuracy
score of 0.779944. We consider this to be successful because the model was
trained on articles that included languages other than English as well as
incomplete, or fragmented, articles.",None,2746
Variable Functioning and Its Application to Large Scale Steel Frame Design Optimization,0.0,"To solve complex real-world problems, heuristics and concept-based approaches
can be used in order to incorporate information into the problem. In this
study, a concept-based approach called variable functioning Fx is introduced to
reduce the optimization variables and narrow down the search space. In this
method, the relationships among one or more subset of variables are defined
with functions using information prior to optimization; thus, instead of
modifying the variables in the search process, the function variables are
optimized. By using problem structure analysis technique and engineering expert
knowledge, the $Fx$ method is used to enhance the steel frame design
optimization process as a complex real-world problem. The proposed approach is
coupled with particle swarm optimization and differential evolution algorithms
and used for three case studies. The algorithms are applied to optimize the
case studies by considering the relationships among column cross-section areas.
The results show that $Fx$ can significantly improve both the convergence rate
and the final design of a frame structure, even if it is only used for seeding.",None,204281
U-Attention to Textures: Hierarchical Hourglass Vision Transformer for Universal Texture Synthesis,0.0380449,"We present a novel U-Attention vision Transformer for universal texture
synthesis. We exploit the natural long-range dependencies enabled by the
attention mechanism to allow our approach to synthesize diverse textures while
preserving their structures in a single inference. We propose a hierarchical
hourglass backbone that attends to the global structure and performs patch
mapping at varying scales in a coarse-to-fine-to-coarse stream. Completed by
skip connection and convolution designs that propagate and fuse information at
different scales, our hierarchical U-Attention architecture unifies attention
to features from macro structures to micro details, and progressively refines
synthesis results at successive stages. Our method achieves stronger 2$\times$
synthesis than previous work on both stochastic and structured textures while
generalizing to unseen textures without fine-tuning. Ablation studies
demonstrate the effectiveness of each component of our architecture.",None,37852
Pre-Avatar: An Automatic Presentation Generation Framework Leveraging Talking Avatar,0.164914,"Since the beginning of the COVID-19 pandemic, remote conferencing and
school-teaching have become important tools. The previous applications aim to
save the commuting cost with real-time interactions. However, our application
is going to lower the production and reproduction costs when preparing the
communication materials. This paper proposes a system called Pre-Avatar,
generating a presentation video with a talking face of a target speaker with 1
front-face photo and a 3-minute voice recording. Technically, the system
consists of three main modules, user experience interface (UEI), talking face
module and few-shot text-to-speech (TTS) module. The system firstly clones the
target speaker's voice, and then generates the speech, and finally generate an
avatar with appropriate lip and head movements. Under any scenario, users only
need to replace slides with different notes to generate another new video. The
demo has been released here and will be published as free software for use.",None,11661
Improving Low-Resource Question Answering using Active Learning in Multiple Stages,0.00479158,"Neural approaches have become very popular in the domain of Question
Answering, however they require a large amount of annotated data. Furthermore,
they often yield very good performance but only in the domain they were trained
on. In this work we propose a novel approach that combines data augmentation
via question-answer generation with Active Learning to improve performance in
low resource settings, where the target domains are diverse in terms of
difficulty and similarity to the source domain. We also investigate Active
Learning for question answering in different stages, overall reducing the
annotation effort of humans. For this purpose, we consider target domains in
realistic settings, with an extremely low amount of annotated samples but with
many unlabeled documents, which we assume can be obtained with little effort.
Additionally, we assume sufficient amount of labeled data from the source
domain is available. We perform extensive experiments to find the best setup
for incorporating domain experts. Our findings show that our novel approach,
where humans are incorporated as early as possible in the process, boosts
performance in the low-resource, domain-specific setting, allowing for
low-labeling-effort question answering systems in new, specialized domains.
They further demonstrate how human annotation affects the performance of QA
depending on the stage it is performed.",https://github.com/primeqa/primeqa,1402
Data-Free Quantization with Accurate Activation Clipping and Adaptive Batch Normalization,0.0,"Data-free quantization is a task that compresses the neural network to low
bit-width without access to original training data. Most existing data-free
quantization methods cause severe performance degradation due to inaccurate
activation clipping range and quantization error, especially for low bit-width.
In this paper, we present a simple yet effective data-free quantization method
with accurate activation clipping and adaptive batch normalization. Accurate
activation clipping (AAC) improves the model accuracy by exploiting accurate
activation information from the full-precision model. Adaptive batch
normalization firstly proposes to address the quantization error from
distribution changes by updating the batch normalization layer adaptively.
Extensive experiments demonstrate that the proposed data-free quantization
method can yield surprisingly performance, achieving 64.33% top-1 accuracy of
ResNet18 on ImageNet dataset, with 3.7% absolute improvement outperforming the
existing state-of-the-art methods.",https://github.com/donnyyou/torchcv,2154
Facilitating Global Team Meetings Between Language-Based Subgroups: When and How Can Machine Translation Help?,0.00173796,"Global teams frequently consist of language-based subgroups who put together
complementary information to achieve common goals. Previous research outlines a
two-step work communication flow in these teams. There are team meetings using
a required common language (i.e., English); in preparation for those meetings,
people have subgroup conversations in their native languages. Work
communication at team meetings is often less effective than in subgroup
conversations. In the current study, we investigate the idea of leveraging
machine translation (MT) to facilitate global team meetings. We hypothesize
that exchanging subgroup conversation logs before a team meeting offers
contextual information that benefits teamwork at the meeting. MT can translate
these logs, which enables comprehension at a low cost. To test our hypothesis,
we conducted a between-subjects experiment where twenty quartets of
participants performed a personnel selection task. Each quartet included two
English native speakers (NS) and two non-native speakers (NNS) whose native
language was Mandarin. All participants began the task with subgroup
conversations in their native languages, then proceeded to team meetings in
English. We manipulated the exchange of subgroup conversation logs prior to
team meetings: with MT-mediated exchanges versus without. Analysis of
participants' subjective experience, task performance, and depth of discussions
as reflected through their conversational moves jointly indicates that team
meeting quality improved when there were MT-mediated exchanges of subgroup
conversation logs as opposed to no exchanges. We conclude with reflections on
when and how MT could be applied to enhance global teamwork across a language
barrier.",https://github.com/awslabs/sockeye,4903
A Comprehensive Gold Standard and Benchmark for Comics Text Detection and Recognition,0.0122093,"This study focuses on improving the optical character recognition (OCR) data
for panels in the COMICS dataset, the largest dataset containing text and
images from comic books. To do this, we developed a pipeline for OCR processing
and labeling of comic books and created the first text detection and
recognition datasets for western comics, called ""COMICS Text+: Detection"" and
""COMICS Text+: Recognition"". We evaluated the performance of state-of-the-art
text detection and recognition models on these datasets and found significant
improvement in word accuracy and normalized edit distance compared to the text
in COMICS. We also created a new dataset called ""COMICS Text+"", which contains
the extracted text from the textboxes in the COMICS dataset. Using the improved
text data of COMICS Text+ in the comics processing model from resulted in
state-of-the-art performance on cloze-style tasks without changing the model
architecture. The COMICS Text+ dataset can be a valuable resource for
researchers working on tasks including text detection, recognition, and
high-level processing of comics, such as narrative understanding, character
relations, and story generation. All the data and inference instructions can be
accessed in https://github.com/gsoykan/comics_text_plus.",https://github.com/gsoykan/comics-text-plus,6406
Which Student is Best? A Comprehensive Knowledge Distillation Exam for Task-Specific BERT Models,0.0215853,"We perform knowledge distillation (KD) benchmark from task-specific BERT-base
teacher models to various student models: BiLSTM, CNN, BERT-Tiny, BERT-Mini,
and BERT-Small. Our experiment involves 12 datasets grouped in two tasks: text
classification and sequence labeling in the Indonesian language. We also
compare various aspects of distillations including the usage of word embeddings
and unlabeled data augmentation. Our experiments show that, despite the rising
popularity of Transformer-based models, using BiLSTM and CNN student models
provide the best trade-off between performance and computational resource (CPU,
RAM, and storage) compared to pruned BERT models. We further propose some quick
wins on performing KD to produce small NLP models via efficient KD training
mechanisms involving simple choices of loss functions, word embeddings, and
unlabeled data preparation.",None,1109
CS-Insights: A System for Analyzing Computer Science Research,0.0491988,"This paper presents CS-Insights, an interactive web application to analyze
computer science publications from DBLP through multiple perspectives. The
dedicated interfaces allow its users to identify trends in research activity,
productivity, accessibility, author's productivity, venues' statistics, topics
of interest, and the impact of computer science research on other fields.
CS-Insightsis publicly available, and its modular architecture can be easily
adapted to domains other than computer science.",https://github.com/jpwahle/cs-insights,8244
A Multimodal Approach for Dementia Detection from Spontaneous Speech with Tensor Fusion Layer,0.0603767,"Alzheimer's disease (AD) is a progressive neurological disorder, meaning that
the symptoms develop gradually throughout the years. It is also the main cause
of dementia, which affects memory, thinking skills, and mental abilities.
Nowadays, researchers have moved their interest towards AD detection from
spontaneous speech, since it constitutes a time-effective procedure. However,
existing state-of-the-art works proposing multimodal approaches do not take
into consideration the inter- and intra-modal interactions and propose early
and late fusion approaches. To tackle these limitations, we propose deep neural
networks, which can be trained in an end-to-end trainable way and capture the
inter- and intra-modal interactions. Firstly, each audio file is converted to
an image consisting of three channels, i.e., log-Mel spectrogram, delta, and
delta-delta. Next, each transcript is passed through a BERT model followed by a
gated self-attention layer. Similarly, each image is passed through a Swin
Transformer followed by an independent gated self-attention layer. Acoustic
features are extracted also from each audio file. Finally, the representation
vectors from the different modalities are fed to a tensor fusion layer for
capturing the inter-modal interactions. Extensive experiments conducted on the
ADReSS Challenge dataset indicate that our introduced approaches obtain
valuable advantages over existing research initiatives reaching Accuracy and
F1-score up to 86.25% and 85.48% respectively.",None,10262
Identifying epidemic related Tweets using noisy learning,0.00172357,"Supervised learning algorithms are heavily reliant on annotated datasets to
train machine learning models. However, the curation of the annotated datasets
is laborious and time consuming due to the manual effort involved and has
become a huge bottleneck in supervised learning. In this work, we apply the
theory of noisy learning to generate weak supervision signals instead of manual
annotation. We curate a noisy labeled dataset using a labeling heuristic to
identify epidemic related tweets. We evaluated the performance using a large
epidemic corpus and our results demonstrate that models trained with noisy data
in a class imbalanced and multi-classification weak supervision setting
achieved performance greater than 90%.",None,514
Diversity Over Size: On the Effect of Sample and Topic Sizes for Argument Mining Datasets,0.0163633,"The task of Argument Mining, that is extracting argumentative sentences for a
specific topic from large document sources, is an inherently difficult task for
machine learning models and humans alike, as large Argument Mining datasets are
rare and recognition of argumentative sentences requires expert knowledge. The
task becomes even more difficult if it also involves stance detection of
retrieved arguments. Given the cost and complexity of creating suitably large
Argument Mining datasets, we ask whether it is necessary for acceptable
performance to have datasets growing in size. Our findings show that, when
using carefully composed training samples and a model pretrained on related
tasks, we can reach 95% of the maximum performance while reducing the training
sample size by at least 85%. This gain is consistent across three Argument
Mining tasks on three different datasets. We also publish a new dataset for
future benchmarking.",None,12
Logic-based Reward Shaping for Multi-Agent Reinforcement Learning,0.027276,"Reinforcement learning (RL) relies heavily on exploration to learn from its
environment and maximize observed rewards. Therefore, it is essential to design
a reward function that guarantees optimal learning from the received
experience. Previous work has combined automata and logic based reward shaping
with environment assumptions to provide an automatic mechanism to synthesize
the reward function based on the task. However, there is limited work on how to
expand logic-based reward shaping to Multi-Agent Reinforcement Learning (MARL).
The environment will need to consider the joint state in order to keep track of
other agents if the task requires cooperation, thus suffering from the curse of
dimensionality with respect to the number of agents. This project explores how
logic-based reward shaping for MARL can be designed for different scenarios and
tasks. We present a novel method for semi-centralized logic-based MARL reward
shaping that is scalable in the number of agents and evaluate it in multiple
scenarios.",https://github.com/IngyN/macsrl,2832
Introspective Deep Metric Learning for Image Retrieval,0.0101796,"This paper proposes an introspective deep metric learning (IDML) framework
for uncertainty-aware comparisons of images. Conventional deep metric learning
methods produce confident semantic distances between images regardless of the
uncertainty level. However, we argue that a good similarity model should
consider the semantic discrepancies with caution to better deal with ambiguous
images for more robust training. To achieve this, we propose to represent an
image using not only a semantic embedding but also an accompanying uncertainty
embedding, which describes the semantic characteristics and ambiguity of an
image, respectively. We further propose an introspective similarity metric to
make similarity judgments between images considering both their semantic
differences and ambiguities. The proposed IDML framework improves the
performance of deep metric learning through uncertainty modeling and attains
state-of-the-art results on the widely used CUB-200-2011, Cars196, and Stanford
Online Products datasets for image retrieval and clustering. We further provide
an in-depth analysis of our framework to demonstrate the effectiveness and
reliability of IDML. Code is available at: https://github.com/wzzheng/IDML.",https://github.com/wzzheng/IDML,30444
Vulnerability Prioritization: An Offensive Security Approach,0.0806939,"Organizations struggle to handle sheer number of vulnerabilities in their
cloud environments. The de facto methodology used for prioritizing
vulnerabilities is to use Common Vulnerability Scoring System (CVSS). However,
CVSS has inherent limitations that makes it not ideal for prioritization. In
this work, we propose a new way of prioritizing vulnerabilities. Our approach
is inspired by how offensive security practitioners perform penetration
testing. We evaluate our approach with a real world case study for a large
client, and the accuracy of machine learning to automate the process end to
end.",None,112
CoDo: Contrastive Learning with Downstream Background Invariance for Detection,0.00437056,"The prior self-supervised learning researches mainly select image-level
instance discrimination as pretext task. It achieves a fantastic classification
performance that is comparable to supervised learning methods. However, with
degraded transfer performance on downstream tasks such as object detection. To
bridge the performance gap, we propose a novel object-level self-supervised
learning method, called Contrastive learning with Downstream background
invariance (CoDo). The pretext task is converted to focus on instance location
modeling for various backgrounds, especially for downstream datasets. The
ability of background invariance is considered vital for object detection.
Firstly, a data augmentation strategy is proposed to paste the instances onto
background images, and then jitter the bounding box to involve background
information. Secondly, we implement architecture alignment between our
pretraining network and the mainstream detection pipelines. Thirdly,
hierarchical and multi views contrastive learning is designed to improve
performance of visual representation learning. Experiments on MSCOCO
demonstrate that the proposed CoDo with common backbones, ResNet50-FPN, yields
strong transfer learning results for object detection.",None,2655
Quantifying Robustness to Adversarial Word Substitutions,0.00425598,"Deep-learning-based NLP models are found to be vulnerable to word
substitution perturbations. Before they are widely adopted, the fundamental
issues of robustness need to be addressed. Along this line, we propose a formal
framework to evaluate word-level robustness. First, to study safe regions for a
model, we introduce robustness radius which is the boundary where the model can
resist any perturbation. As calculating the maximum robustness radius is
computationally hard, we estimate its upper and lower bound. We repurpose
attack methods as ways of seeking upper bound and design a pseudo-dynamic
programming algorithm for a tighter upper bound. Then verification method is
utilized for a lower bound. Further, for evaluating the robustness of regions
outside a safe radius, we reexamine robustness from another view:
quantification. A robustness metric with a rigorous statistical guarantee is
introduced to measure the quantification of adversarial examples, which
indicates the model's susceptibility to perturbations outside the safe radius.
The metric helps us figure out why state-of-the-art models like BERT can be
easily fooled by a few word substitutions, but generalize well in the presence
of real-world noises.",None,8526
"Paraphrasing, textual entailment, and semantic similarity above word level",0.0130521,"This dissertation explores the linguistic and computational aspects of the
meaning relations that can hold between two or more complex linguistic
expressions (phrases, clauses, sentences, paragraphs). In particular, it
focuses on Paraphrasing, Textual Entailment, Contradiction, and Semantic
Similarity.
  In Part I: ""Similarity at the Level of Words and Phrases"", I study the
Distributional Hypothesis (DH) and explore several different methodologies for
quantifying semantic similarity at the levels of words and short phrases.
  In Part II: ""Paraphrase Typology and Paraphrase Identification"", I focus on
the meaning relation of paraphrasing and the empirical task of automated
Paraphrase Identification (PI).
  In Part III: ""Paraphrasing, Textual Entailment, and Semantic Similarity"", I
present a novel direction in the research on textual meaning relations,
resulting from joint research carried out on on paraphrasing, textual
entailment, contradiction, and semantic similarity.",None,12
Unsupervised Opinion Summarisation in the Wasserstein Space,0.0336793,"Opinion summarisation synthesises opinions expressed in a group of documents
discussing the same topic to produce a single summary. Recent work has looked
at opinion summarisation of clusters of social media posts. Such posts are
noisy and have unpredictable structure, posing additional challenges for the
construction of the summary distribution and the preservation of meaning
compared to online reviews, which has been so far the focus of opinion
summarisation. To address these challenges we present \textit{WassOS}, an
unsupervised abstractive summarization model which makes use of the Wasserstein
distance. A Variational Autoencoder is used to get the distribution of
documents/posts, and the distributions are disentangled into separate semantic
and syntactic spaces. The summary distribution is obtained using the
Wasserstein barycenter of the semantic and syntactic distributions. A latent
variable sampled from the summary distribution is fed into a GRU decoder with a
transformer layer to produce the final summary. Our experiments on multiple
datasets including Twitter clusters, Reddit threads, and reviews show that
WassOS almost always outperforms the state-of-the-art on ROUGE metrics and
consistently produces the best summaries with respect to meaning preservation
according to human evaluations.",https://github.com/Maria-Liakata-NLP-Group/,18450
Addressing the Challenges of Cross-Lingual Hate Speech Detection,0.212605,"The goal of hate speech detection is to filter negative online content aiming
at certain groups of people. Due to the easy accessibility of social media
platforms it is crucial to protect everyone which requires building hate speech
detection systems for a wide range of languages. However, the available labeled
hate speech datasets are limited making it problematic to build systems for
many languages. In this paper we focus on cross-lingual transfer learning to
support hate speech detection in low-resource languages. We leverage
cross-lingual word embeddings to train our neural network systems on the source
language and apply it to the target language, which lacks labeled examples, and
show that good performance can be achieved. We then incorporate unlabeled
target language data for further model improvements by bootstrapping labels
using an ensemble of different model architectures. Furthermore, we investigate
the issue of label imbalance of hate speech datasets, since the high ratio of
non-hate examples compared to hate examples often leads to low model
performance. We test simple data undersampling and oversampling techniques and
show their effectiveness.",None,38141
Medical Dataset Classification for Kurdish Short Text over Social Media,0.603448,"The Facebook application is used as a resource for collecting the comments of
this dataset, The dataset consists of 6756 comments to create a Medical Kurdish
Dataset (MKD). The samples are comments of users, which are gathered from
different posts of pages (Medical, News, Economy, Education, and Sport). Six
steps as a preprocessing technique are performed on the raw dataset to clean
and remove noise in the comments by replacing characters. The comments (short
text) are labeled for positive class (medical comment) and negative class
(non-medical comment) as text classification. The percentage ratio of the
negative class is 55% while the positive class is 45%.",None,133
What do Models Learn From Training on More Than Text? Measuring Visual Commonsense Knowledge,0.0438929,"There are limitations in learning language from text alone. Therefore, recent
focus has been on developing multimodal models. However, few benchmarks exist
that can measure what language models learn about language from multimodal
training. We hypothesize that training on a visual modality should improve on
the visual commonsense knowledge in language models. Therefore, we introduce
two evaluation tasks for measuring visual commonsense knowledge in language
models and use them to evaluate different multimodal models and unimodal
baselines. Primarily, we find that the visual commonsense knowledge is not
significantly different between the multimodal models and unimodal baseline
models trained on visual text data.",https://github.com/lovhag/measure-visual-commonsense-knowledge,32
Position Paper: Online Modeling for Offline Planning,0.00456251,"The definition and representation of planning problems is at the heart of AI
planning research. A key part is the representation of action models. Decades
of advances improving declarative action model representations resulted in
numerous theoretical advances, and capable, working, domain-independent
planners. However, despite the maturity of the field, AI planning technology is
still rarely used outside the research community, suggesting that current
representations fail to capture real-world requirements, such as utilizing
complex mathematical functions and models learned from data. We argue that this
is because the modeling process is assumed to have taken place and completed
prior to the planning process, i.e., offline modeling for offline planning.
There are several challenges inherent to this approach, including: limited
expressiveness of declarative modeling languages; early commitment to modeling
choices and computation, that preclude using the most appropriate resolution
for each action model -- which can only be known during planning; and
difficulty in reliably using non-declarative, learned, models.
  We therefore suggest to change the AI planning process, such that is carries
out online modeling in offline planning, i.e., the use of action models that
are computed or even generated as part of the planning process, as they are
accessed. This generalizes the existing approach (offline modeling). The
proposed definition admits novel planning processes, and we suggest one
concrete implementation, demonstrating the approach. We sketch initial results
that were obtained as part of a first attempt to follow this approach by
planning with action cost estimators. We conclude by discussing open
challenges.",None,7922
Bridging the Gap between Local Semantic Concepts and Bag of Visual Words for Natural Scene Image Retrieval,0.0220956,"This paper addresses the problem of semantic-based image retrieval of natural
scenes. A typical content-based image retrieval system deals with the query
image and images in the dataset as a collection of low-level features and
retrieves a ranked list of images based on the similarities between features of
the query image and features of images in the image dataset. However, top
ranked images in the retrieved list, which have high similarities to the query
image, may be different from the query image in terms of the semantic
interpretation of the user which is known as the semantic gap. In order to
reduce the semantic gap, this paper investigates how natural scene retrieval
can be performed using the bag of visual word model and the distribution of
local semantic concepts. The paper studies the efficiency of using different
approaches for representing the semantic information, depicted in natural scene
images, for image retrieval. An extensive experimental work has been conducted
to study the efficiency of using semantic information as well as the bag of
visual words model for natural and urban scene image retrieval.",None,69
Learning Reduced Nonlinear State-Space Models: an Output-Error Based Canonical Approach,0.0153557,"The identification of a nonlinear dynamic model is an open topic in control
theory, especially from sparse input-output measurements. A fundamental
challenge of this problem is that very few to zero prior knowledge is available
on both the state and the nonlinear system model. To cope with this challenge,
we investigate the effectiveness of deep learning in the modeling of dynamic
systems with nonlinear behavior by advocating an approach which relies on three
main ingredients: (i) we show that under some structural conditions on the
to-be-identified model, the state can be expressed in function of a sequence of
the past inputs and outputs; (ii) this relation which we call the state map can
be modelled by resorting to the well-documented approximation power of deep
neural networks; (iii) taking then advantage of existing learning schemes, a
state-space model can be finally identified. After the formulation and analysis
of the approach, we show its ability to identify three different nonlinear
systems. The performances are evaluated in terms of open-loop prediction on
test data generated in simulation as well as a real world data-set of unmanned
aerial vehicle flight measurements.",None,8251
Unsupervised Explanation Generation via Correct Instantiations,0.0369707,"While large pre-trained language models (PLM) have shown their great skills
at solving discriminative tasks, a significant gap remains when compared with
humans for explanation-related tasks. Among them, explaining the reason why a
statement is wrong (e.g., against commonsense) is incredibly challenging. The
major difficulty is finding the conflict point, where the statement contradicts
our real world. This paper proposes Neon, a two-phrase, unsupervised
explanation generation framework. Neon first generates corrected instantiations
of the statement (phase I), then uses them to prompt large PLMs to find the
conflict point and complete the explanation (phase II). We conduct extensive
experiments on two standard explanation benchmarks, i.e., ComVE and e-SNLI.
According to both automatic and human evaluations, Neon outperforms baselines,
even for those with human-annotated instantiations. In addition to explaining a
negative prediction, we further demonstrate that Neon remains effective when
generalizing to different scenarios.",None,11617
Stability of Syntactic Dialect Classification Over Space and Time,0.0808759,"This paper analyses the degree to which dialect classifiers based on
syntactic representations remain stable over space and time. While previous
work has shown that the combination of grammar induction and geospatial text
classification produces robust dialect models, we do not know what influence
both changing grammars and changing populations have on dialect models. This
paper constructs a test set for 12 dialects of English that spans three years
at monthly intervals with a fixed spatial distribution across 1,120 cities.
Syntactic representations are formulated within the usage-based Construction
Grammar paradigm (CxG). The decay rate of classification performance for each
dialect over time allows us to identify regions undergoing syntactic change.
And the distribution of classification accuracy within dialect regions allows
us to identify the degree to which the grammar of a dialect is internally
heterogeneous. The main contribution of this paper is to show that a rigorous
evaluation of dialect classification models can be used to find both variation
over space and change over time.",None,717
"Markov categories, causal theories, and the do-calculus",0.0355463,"We give a category-theoretic treatment of causal models that formalizes the
syntax for causal reasoning over a directed acyclic graph (DAG) by associating
a free Markov category with the DAG in a canonical way. This framework enables
us to define and study important concepts in causal reasoning from an abstract
and ""purely causal"" point of view, such as causal independence/separation,
causal conditionals, and decomposition of intervention effects. Our results
regarding these concepts abstract away from the details of the commonly adopted
causal models such as (recursive) structural equation models or causal Bayesian
networks. They are therefore more widely applicable and in a way conceptually
clearer. Our results are also intimately related to Judea Pearl's celebrated
do-calculus, and yield a syntactic version of a core part of the calculus that
is inherited in all causal models. In particular, it induces a simpler and
specialized version of Pearl's do-calculus in the context of causal Bayesian
networks, which we show is as strong as the full version.",None,-1
Collaborative Image Understanding,0.0350558,"Automatically understanding the contents of an image is a highly relevant
problem in practice. In e-commerce and social media settings, for example, a
common problem is to automatically categorize user-provided pictures. Nowadays,
a standard approach is to fine-tune pre-trained image models with
application-specific data. Besides images, organizations however often also
collect collaborative signals in the context of their application, in
particular how users interacted with the provided online content, e.g., in
forms of viewing, rating, or tagging. Such signals are commonly used for item
recommendation, typically by deriving latent user and item representations from
the data. In this work, we show that such collaborative information can be
leveraged to improve the classification process of new images. Specifically, we
propose a multitask learning framework, where the auxiliary task is to
reconstruct collaborative latent item representations. A series of experiments
on datasets from e-commerce and social media demonstrates that considering
collaborative signals helps to significantly improve the performance of the
main task of image classification by up to 9.1%.",https://github.com/anonymous1e6/cactus,18602
Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling,0.013813,"Despite the extensive studies on Generative Adversarial Networks (GANs), how
to reliably sample high-quality images from their latent spaces remains an
under-explored topic. In this paper, we propose a novel GAN latent sampling
method by exploring and exploiting the hubness priors of GAN latent
distributions. Our key insight is that the high dimensionality of the GAN
latent space will inevitably lead to the emergence of hub latents that usually
have much larger sampling densities than other latents in the latent space. As
a result, these hub latents are better trained and thus contribute more to the
synthesis of high-quality images. Unlike the a posterior ""cherry-picking"", our
method is highly efficient as it is an a priori method that identifies
high-quality latents before the synthesis of images. Furthermore, we show that
the well-known but purely empirical truncation trick is a naive approximation
to the central clustering effect of hub latents, which not only uncovers the
rationale of the truncation trick, but also indicates the superiority and
fundamentality of our method. Extensive experimental results demonstrate the
effectiveness of the proposed method.",https://github.com/Byronliang8/HubnessGANSampling,10421
Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution,0.0313179,"Nowadays, thermal infrared satellite remote sensors enable to extract very
interesting information at large scale, in particular Land Surface Temperature
(LST). However such data are limited in spatial and/or temporal resolutions
which prevents from an analysis at fine scales. For example, MODIS satellite
provides daily acquisitions with 1Km spatial resolutions which is not
sufficient to deal with highly heterogeneous environments as agricultural
parcels. Therefore, image super-resolution is a crucial task to better exploit
MODIS LSTs. This issue is tackled in this paper. We introduce a deep
learning-based algorithm, named Multi-residual U-Net, for super-resolution of
MODIS LST single-images. Our proposed network is a modified version of U-Net
architecture, which aims at super-resolving the input LST image from 1Km to
250m per pixel. The results show that our Multi-residual U-Net outperforms
other state-of-the-art methods.",https://github.com/IMT-Project-LTS-SR/MRUNet-for-MODIS-super-resolution,3504
Caption supervision enables robust learners,0.0116782,"Vision language (VL) models like CLIP are robust to natural distribution
shifts, in part because CLIP learns on unstructured data using a technique
called caption supervision; the model inteprets image-linked texts as
ground-truth labels. In a carefully controlled comparison study, we show that
caption-supervised CNNs trained on a standard cross-entropy loss (with image
labels assigned by scanning captions for class names) can exhibit greater
distributional robustness than VL models trained on the same data. To
facilitate future experiments with high-accuracy caption-supervised models, we
introduce CaptionNet (https://github.com/penfever/CaptionNet/), which includes
a class-balanced, fully supervised dataset with over 50,000 new human-labeled
ImageNet-compliant samples which includes web-scraped captions. In a series of
experiments on CaptionNet, we show how the choice of loss function, data
filtration and supervision strategy enable robust computer vision. We also
provide the codebase necessary to reproduce our experiments at VL Hub
(https://github.com/penfever/vlhub/).",None,5798
Iterative collaborative routing among equivariant capsules for transformation-robust capsule networks,0.0255114,"Transformation-robustness is an important feature for machine learning models
that perform image classification. Many methods aim to bestow this property to
models by the use of data augmentation strategies, while more formal guarantees
are obtained via the use of equivariant models. We recognise that
compositional, or part-whole structure is also an important aspect of images
that has to be considered for building transformation-robust models. Thus, we
propose a capsule network model that is, at once, equivariant and
compositionality-aware. Equivariance of our capsule network model comes from
the use of equivariant convolutions in a carefully-chosen novel architecture.
The awareness of compositionality comes from the use of our proposed novel,
iterative, graph-based routing algorithm, termed Iterative collaborative
routing (ICR). ICR, the core of our contribution, weights the predictions made
for capsules based on an iteratively averaged score of the degree-centralities
of its nearest neighbours. Experiments on transformed image classification on
FashionMNIST, CIFAR-10, and CIFAR-100 show that our model that uses ICR
outperforms convolutional and capsule baselines to achieve state-of-the-art
performance.",None,2794
Towards Two-view 6D Object Pose Estimation: A Comparative Study on Fusion Strategy,0.0190405,"Current RGB-based 6D object pose estimation methods have achieved noticeable
performance on datasets and real world applications. However, predicting 6D
pose from single 2D image features is susceptible to disturbance from changing
of environment and textureless or resemblant object surfaces. Hence, RGB-based
methods generally achieve less competitive results than RGBD-based methods,
which deploy both image features and 3D structure features. To narrow down this
performance gap, this paper proposes a framework for 6D object pose estimation
that learns implicit 3D information from 2 RGB images. Combining the learned 3D
information and 2D image features, we establish more stable correspondence
between the scene and the object models. To seek for the methods best utilizing
3D information from RGB inputs, we conduct an investigation on three different
approaches, including Early- Fusion, Mid-Fusion, and Late-Fusion. We ascertain
the Mid- Fusion approach is the best approach to restore the most precise 3D
keypoints useful for object pose estimation. The experiments show that our
method outperforms state-of-the-art RGB-based methods, and achieves comparable
results with RGBD-based methods.",None,19514
Probabilistic Implicit Scene Completion,0.0206978,"We propose a probabilistic shape completion method extended to the continuous
geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a
considerable amount of missing data cluttered with unsegmented objects. The
problem of shape completion is inherently ill-posed, and high-quality result
requires scalable solutions that consider multiple possible outcomes. We employ
the Generative Cellular Automata that learns the multi-modal distribution and
transform the formulation to process large-scale continuous geometry. The local
continuous shape is incrementally generated as a sparse voxel embedding, which
contains the latent code for each occupied cell. We formally derive that our
training objective for the sparse voxel embedding maximizes the variational
lower bound of the complete shape distribution and therefore our progressive
generation constitutes a valid generative model. Experiments show that our
model successfully generates diverse plausible scenes faithful to the input,
especially when the input suffers from a significant amount of missing data. We
also demonstrate that our approach outperforms deterministic models even in
less ambiguous cases with a small amount of missing data, which infers that
probabilistic formulation is crucial for high-quality geometry completion on
input scans exhibiting any levels of completeness.",None,2080
Addressing Client Drift in Federated Continual Learning with Adaptive Optimization,0.026414,"Federated learning has been extensively studied and is the prevalent method
for privacy-preserving distributed learning in edge devices. Correspondingly,
continual learning is an emerging field targeted towards learning multiple
tasks sequentially. However, there is little attention towards additional
challenges emerging when federated aggregation is performed in a continual
learning system. We identify \textit{client drift} as one of the key weaknesses
that arise when vanilla federated averaging is applied in such a system,
especially since each client can independently have different order of tasks.
We outline a framework for performing Federated Continual Learning (FCL) by
using NetTailor as a candidate continual learning approach and show the extent
of the problem of client drift. We show that adaptive federated optimization
can reduce the adverse impact of client drift and showcase its effectiveness on
CIFAR100, MiniImagenet, and Decathlon benchmarks. Further, we provide an
empirical analysis highlighting the interplay between different hyperparameters
such as client and server learning rates, the number of local training
iterations, and communication rounds. Finally, we evaluate our framework on
useful characteristics of federated learning systems such as scalability,
robustness to the skewness in clients' data distribution, and stragglers.",None,6458
Non-Deterministic Face Mask Removal Based On 3D Priors,0.0200246,"This paper presents a novel image inpainting framework for face mask removal.
Although current methods have demonstrated their impressive ability in
recovering damaged face images, they suffer from two main problems: the
dependence on manually labeled missing regions and the deterministic result
corresponding to each input. The proposed approach tackles these problems by
integrating a multi-task 3D face reconstruction module with a face inpainting
module. Given a masked face image, the former predicts a 3DMM-based
reconstructed face together with a binary occlusion map, providing dense
geometrical and textural priors that greatly facilitate the inpainting task of
the latter. By gradually controlling the 3D shape parameters, our method
generates high-quality dynamic inpainting results with different expressions
and mouth movements. Qualitative and quantitative experiments verify the
effectiveness of the proposed method.",https://github.com/face3d0725/face_de_mask,11135
Decanus to Legatus: Synthetic training for 2D-3D human pose lifting,0.0112251,"3D human pose estimation is a challenging task because of the difficulty to
acquire ground-truth data outside of controlled environments. A number of
further issues have been hindering progress in building a universal and robust
model for this task, including domain gaps between different datasets, unseen
actions between train and test datasets, various hardware settings and high
cost of annotation, etc. In this paper, we propose an algorithm to generate
infinite 3D synthetic human poses (Legatus) from a 3D pose distribution based
on 10 initial handcrafted 3D poses (Decanus) during the training of a 2D to 3D
human pose lifter neural network. Our results show that we can achieve 3D pose
estimation performance comparable to methods using real data from specialized
datasets but in a zero-shot setup, showing the generalization potential of our
framework.",None,2981
Improving the Cross-Lingual Generalisation in Visual Question Answering,0.043198,"While several benefits were realized for multilingual vision-language
pretrained models, recent benchmarks across various tasks and languages showed
poor cross-lingual generalisation when multilingually pre-trained
vision-language models are applied to non-English data, with a large gap
between (supervised) English performance and (zero-shot) cross-lingual
transfer. In this work, we explore the poor performance of these models on a
zero-shot cross-lingual visual question answering (VQA) task, where models are
fine-tuned on English visual-question data and evaluated on 7 typologically
diverse languages. We improve cross-lingual transfer with three strategies: (1)
we introduce a linguistic prior objective to augment the cross-entropy loss
with a similarity-based loss to guide the model during training, (2) we learn a
task-specific subnetwork that improves cross-lingual generalisation and reduces
variance without model modification, (3) we augment training examples using
synthetic code-mixing to promote alignment of embeddings between source and
target languages. Our experiments on xGQA using the pretrained multilingual
multimodal transformers UC2 and M3P demonstrate the consistent effectiveness of
the proposed fine-tuning strategy for 7 languages, outperforming existing
transfer methods with sparse models. Code and data to reproduce our findings
are publicly available.",https://github.com/nooralahzadeh/CLG-VQA,21498
Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems,0.0467871,"Large transformer models can highly improve Answer Sentence Selection (AS2)
tasks, but their high computational costs prevent their use in many real-world
applications. In this paper, we explore the following research question: How
can we make the AS2 models more accurate without significantly increasing their
model complexity? To address the question, we propose a Multiple Heads Student
architecture (named CERBERUS), an efficient neural network designed to distill
an ensemble of large transformers into a single smaller model. CERBERUS
consists of two components: a stack of transformer layers that is used to
encode inputs, and a set of ranking heads; unlike traditional distillation
technique, each of them is trained by distilling a different large transformer
architecture in a way that preserves the diversity of the ensemble members. The
resulting model captures the knowledge of heterogeneous transformer models by
using just a few extra parameters. We show the effectiveness of CERBERUS on
three English datasets for AS2; our proposed approach outperforms all
single-model distillations we consider, rivaling the state-of-the-art large AS2
models that have 2.7x more parameters and run 2.5x slower. Code for our model
is available at https://github.com/amazon-research/wqa-cerberus",https://github.com/amazon-research/wqa-cerberus,14771
CSS: Combining Self-training and Self-supervised Learning for Few-shot Dialogue State Tracking,0.0246275,"Few-shot dialogue state tracking (DST) is a realistic problem that trains the
DST model with limited labeled data. Existing few-shot methods mainly transfer
knowledge learned from external labeled dialogue data (e.g., from question
answering, dialogue summarization, machine reading comprehension tasks, etc.)
into DST, whereas collecting a large amount of external labeled data is
laborious, and the external data may not effectively contribute to the
DST-specific task. In this paper, we propose a few-shot DST framework called
CSS, which Combines Self-training and Self-supervised learning methods. The
unlabeled data of the DST task is incorporated into the self-training
iterations, where the pseudo labels are predicted by a DST model trained on
limited labeled data in advance. Besides, a contrastive self-supervised method
is used to learn better representations, where the data is augmented by the
dropout operation to train the model. Experimental results on the MultiWOZ
dataset show that our proposed CSS achieves competitive performance in several
few-shot scenarios.",None,30637
Sequence-to-Sequence Models for Extracting Information from Registration and Legal Documents,0.00143522,"A typical information extraction pipeline consists of token- or span-level
classification models coupled with a series of pre- and post-processing
scripts. In a production pipeline, requirements often change, with classes
being added and removed, which leads to nontrivial modifications to the source
code and the possible introduction of bugs. In this work, we evaluate
sequence-to-sequence models as an alternative to token-level classification
methods for information extraction of legal and registration documents. We
finetune models that jointly extract the information and generate the output
already in a structured format. Post-processing steps are learned during
training, thus eliminating the need for rule-based methods and simplifying the
pipeline. Furthermore, we propose a novel method to align the output with the
input text, thus facilitating system inspection and auditing. Our experiments
on four real-world datasets show that the proposed method is an alternative to
classical pipelines.",None,10818
Zoom Text Detector,0.0134942,"To pursue comprehensive performance, recent text detectors improve detection
speed at the expense of accuracy. They adopt shrink-mask based text
representation strategies, which leads to a high dependency of detection
accuracy on shrink-masks. Unfortunately, three disadvantages cause unreliable
shrink-masks. Specifically, these methods try to strengthen the discrimination
of shrink-masks from the background by semantic information. However, the
feature defocusing phenomenon that coarse layers are optimized by fine-grained
objectives limits the extraction of semantic features. Meanwhile, since both
shrink-masks and the margins belong to texts, the detail loss phenomenon that
the margins are ignored hinders the distinguishment of shrink-masks from the
margins, which causes ambiguous shrink-mask edges. Moreover, false-positive
samples enjoy similar visual features with shrink-masks. They aggravate the
decline of shrink-masks recognition. To avoid the above problems, we propose a
Zoom Text Detector (ZTD) inspired by the zoom process of the camera.
Specifically, Zoom Out Module (ZOM) is introduced to provide coarse-grained
optimization objectives for coarse layers to avoid feature defocusing.
Meanwhile, Zoom In Module (ZIM) is presented to enhance the margins recognition
to prevent detail loss. Furthermore, Sequential-Visual Discriminator (SVD) is
designed to suppress false-positive samples by sequential and visual features.
Experiments verify the superior comprehensive performance of ZTD.",None,55805
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations,0.0117474,"Due to the huge amount of parameters, fine-tuning of pretrained language
models (PLMs) is prone to overfitting in the low resource scenarios. In this
work, we present a novel method that operates on the hidden representations of
a PLM to reduce overfitting. During fine-tuning, our method inserts random
autoencoders between the hidden layers of a PLM, which transform activations
from the previous layers into multi-view compressed representations before
feeding them into the upper layers. The autoencoders are plugged out after
fine-tuning, so our method does not add extra parameters or increase
computation cost during inference. Our method demonstrates promising
performance improvement across a wide range of sequence- and token-level
low-resource NLP tasks.",https://github.com/DAMO-NLP-SG/MVCR,11730
A Comparative Study of Graph Neural Networks for Shape Classification in Neuroimaging,0.0042152,"Graph neural networks have emerged as a promising approach for the analysis
of non-Euclidean data such as meshes. In medical imaging, mesh-like data plays
an important role for modelling anatomical structures, and shape classification
can be used in computer aided diagnosis and disease detection. However, with a
plethora of options, the best architectural choices for medical shape analysis
using GNNs remain unclear. We conduct a comparative analysis to provide
practitioners with an overview of the current state-of-the-art in geometric
deep learning for shape classification in neuroimaging. Using biological sex
classification as a proof-of-concept task, we find that using FPFH as node
features substantially improves GNN performance and generalisation to
out-of-distribution data; we compare the performance of three alternative
convolutional layers; and we reinforce the importance of data augmentation for
graph based learning. We then confirm these results hold for a clinically
relevant task, using the classification of Alzheimer's disease.",https://github.com/biomedia-mira/medmesh,41086
CoSe-Co: Text Conditioned Generative CommonSense Contextualizer,0.0667924,"Pre-trained Language Models (PTLMs) have been shown to perform well on
natural language tasks. Many prior works have leveraged structured commonsense
present in the form of entities linked through labeled relations in Knowledge
Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static
module which limits coverage since KGs contain finite knowledge. Generative
methods train PTLMs on KG triples to improve the scale at which knowledge can
be obtained. However, training on symbolic KG entities limits their
applicability in tasks involving natural language text where they ignore
overall context. To mitigate this, we propose a CommonSense Contextualizer
(CoSe-Co) conditioned on sentences as input to make it generically usable in
tasks for generating knowledge relevant to the overall context of input text.
To train CoSe-Co, we propose a novel dataset comprising of sentence and
commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and
contain novel entities not present in the underlying KG. We augment generated
knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading
to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets.
We also demonstrate its applicability in improving performance of a baseline
model for paraphrase generation task.",https://linktr.ee/coseco,2114
Learning Efficient Representations for Enhanced Object Detection on Large-scene SAR Images,0.0122216,"It is a challenging problem to detect and recognize targets on complex
large-scene Synthetic Aperture Radar (SAR) images. Recently developed deep
learning algorithms can automatically learn the intrinsic features of SAR
images, but still have much room for improvement on large-scene SAR images with
limited data. In this paper, based on learning representations and multi-scale
features of SAR images, we propose an efficient and robust deep learning based
target detection method. Especially, by leveraging the effectiveness of
adversarial autoencoder (AAE) which influences the distribution of the
investigated data explicitly, the raw SAR dataset is augmented into an enhanced
version with a large quantity and diversity. Besides, an auto-labeling scheme
is proposed to improve labeling efficiency. Finally, with jointly training
small target chips and large-scene images, an integrated YOLO network combining
non-maximum suppression on sub-images is used to realize multiple targets
detection of high resolution images. The numerical experimental results on the
MSTAR dataset show that our method can realize target detection and recognition
on large-scene images accurately and efficiently. The superior anti-noise
performance is also confirmed by experiments.",None,9295
A Flexible Diffusion Model,0.0148977,"Diffusion (score-based) generative models have been widely used for modeling
various types of complex data, including images, audios, and point clouds.
Recently, the deep connection between forward-backward stochastic differential
equations (SDEs) and diffusion-based models has been revealed, and several new
variants of SDEs are proposed (e.g., sub-VP, critically-damped Langevin) along
this line. Despite the empirical success of the hand-crafted fixed forward
SDEs, a great quantity of proper forward SDEs remain unexplored. In this work,
we propose a general framework for parameterizing the diffusion model,
especially the spatial part of the forward SDE. An abstract formalism is
introduced with theoretical guarantees, and its connection with previous
diffusion models is leveraged. We demonstrate the theoretical advantage of our
method from an optimization perspective. Numerical experiments on synthetic
datasets, MINIST and CIFAR10 are also presented to validate the effectiveness
of our framework.",None,2161
Self-Supervised Pre-training of Vision Transformers for Dense Prediction Tasks,0.00936174,"We present a new self-supervised pre-training of Vision Transformers for
dense prediction tasks. It is based on a contrastive loss across views that
compares pixel-level representations to global image representations. This
strategy produces better local features suitable for dense prediction tasks as
opposed to contrastive pre-training based on global image representation only.
Furthermore, our approach does not suffer from a reduced batch size since the
number of negative examples needed in the contrastive loss is in the order of
the number of local features. We demonstrate the effectiveness of our
pre-training strategy on two dense prediction tasks: semantic segmentation and
monocular depth estimation.",None,638
Text Simplification of College Admissions Instructions: A Professionally Simplified and Verified Corpus,0.0,"Access to higher education is critical for minority populations and emergent
bilingual students. However, the language used by higher education institutions
to communicate with prospective students is often too complex; concretely, many
institutions in the US publish admissions application instructions far above
the average reading level of a typical high school graduate, often near the
13th or 14th grade level. This leads to an unnecessary barrier between students
and access to higher education. This work aims to tackle this challenge via
text simplification. We present PSAT (Professionally Simplified Admissions
Texts), a dataset with 112 admissions instructions randomly selected from
higher education institutions across the US. These texts are then
professionally simplified, and verified and accepted by subject-matter experts
who are full-time employees in admissions offices at various institutions.
Additionally, PSAT comes with manual alignments of 1,883 original-simplified
sentence pairs. The result is a first-of-its-kind corpus for the evaluation and
fine-tuning of text simplification systems in a high-stakes genre distinct from
existing simplification resources.",None,-1
Spectral Probing,0.0782761,"Linguistic information is encoded at varying timescales (subwords, phrases,
etc.) and communicative levels, such as syntax and semantics. Contextualized
embeddings have analogously been found to capture these phenomena at
distinctive layers and frequencies. Leveraging these findings, we develop a
fully learnable frequency filter to identify spectral profiles for any given
task. It enables vastly more granular analyses than prior handcrafted filters,
and improves on efficiency. After demonstrating the informativeness of spectral
probing over manual filters in a monolingual setting, we investigate its
multilingual characteristics across seven diverse NLP tasks in six languages.
Our analyses identify distinctive spectral profiles which quantify cross-task
similarity in a linguistically intuitive manner, while remaining consistent
across languages-highlighting their potential as robust, lightweight task
descriptors.",https://github.com/mainlp/spectral-probing,6816
DeepTechnome: Mitigating Unknown Bias in Deep Learning Based Assessment of CT Images,0.0121462,"Reliably detecting diseases using relevant biological information is crucial
for real-world applicability of deep learning techniques in medical imaging. We
debias deep learning models during training against unknown bias - without
preprocessing/filtering the input beforehand or assuming specific knowledge
about its distribution or precise nature in the dataset. We use control regions
as surrogates that carry information regarding the bias, employ the classifier
model to extract features, and suppress biased intermediate features with our
custom, modular DecorreLayer. We evaluate our method on a dataset of 952 lung
computed tomography scans by introducing simulated biases w.r.t. reconstruction
kernel and noise level and propose including an adversarial test set in
evaluations of bias reduction techniques. In a moderately sized model
architecture, applying the proposed method to learn from data exhibiting a
strong bias, it near-perfectly recovers the classification performance observed
when training with corresponding unbiased data.",None,628
"Understanding, Detecting, and Separating Out-of-Distribution Samples and Adversarial Samples in Text Classification",0.00767297,"In this paper, we study the differences and commonalities between
statistically out-of-distribution (OOD) samples and adversarial (Adv) samples,
both of which hurting a text classification model's performance. We conduct
analyses to compare the two types of anomalies (OOD and Adv samples) with the
in-distribution (ID) ones from three aspects: the input features, the hidden
representations in each layer of the model, and the output probability
distributions of the classifier. We find that OOD samples expose their
aberration starting from the first layer, while the abnormalities of Adv
samples do not emerge until the deeper layers of the model. We also illustrate
that the models' output probabilities for Adv samples tend to be more
unconfident. Based on our observations, we propose a simple method to separate
ID, OOD, and Adv samples using the hidden representations and output
probabilities of the model. On multiple combinations of ID, OOD datasets, and
Adv attacks, our proposed method shows exceptional results on distinguishing
ID, OOD, and Adv samples.",None,9523
Too much information: why CDCL solvers need to forget learned clauses,0.0349126,"Conflict-driven clause learning (CDCL) is a remarkably successful paradigm
for solving the satisfiability problem of propositional logic. Instead of a
simple depth-first backtracking approach, this kind of solver learns the reason
behind occurring conflicts in the form of additional clauses. However, despite
the enormous success of CDCL solvers, there is still only a limited
understanding of what influences the performance of these solvers in what way.
  Considering different measures, this paper demonstrates, quite surprisingly,
that clause learning (without being able to get rid of some clauses) can not
only help the solver but can oftentimes deteriorate the solution process
dramatically. By conducting extensive empirical analysis, we furthermore find
that the runtime distributions of CDCL solvers are multimodal. This
multimodality can be seen as a reason for the deterioration phenomenon
described above. Simultaneously, it also gives an indication of why clause
learning in combination with clause deletion is virtually the de facto standard
of SAT solving, in spite of this phenomenon. As a final contribution, we show
that Weibull mixture distributions can accurately describe the multimodal
distributions. Thus, adding new clauses to a base instance has an inherent
effect of making runtimes long-tailed. This insight provides an explanation as
to why the technique of forgetting clauses is useful in CDCL solvers apart from
the optimization of unit propagation speed.",None,29
An Active Contour Model with Local Variance Force Term and Its Efficient Minimization Solver for Multi-phase Image Segmentation,0.00776508,"In this paper, we propose an active contour model with a local variance force
(LVF) term that can be applied to multi-phase image segmentation problems. With
the LVF, the proposed model is very effective in the segmentation of images
with noise. To solve this model efficiently, we represent the regularization
term by characteristic functions and then design a minimization algorithm based
on a modification of the iterative convolution-thresholding method (ICTM),
namely ICTM-LVF. This minimization algorithm enjoys the energy-decaying
property under some conditions and has highly efficient performance in the
segmentation. To overcome the initialization issue of active contour models, we
generalize the inhomogeneous graph Laplacian initialization method (IGLIM) to
the multi-phase case and then apply it to give the initial contour of the
ICTM-LVF solver. Numerical experiments are conducted on synthetic images and
real images to demonstrate the capability of our initialization method, and the
effectiveness of the local variance force for noise robustness in the
multi-phase image segmentation.",None,3116
An Unsupervised Masking Objective for Abstractive Multi-Document News Summarization,0.0102418,"We show that a simple unsupervised masking objective can approach near
supervised performance on abstractive multi-document news summarization. Our
method trains a state-of-the-art neural summarization model to predict the
masked out source document with highest lexical centrality relative to the
multi-document group. In experiments on the Multi-News dataset, our masked
training objective yields a system that outperforms past unsupervised methods
and, in human evaluation, surpasses the best supervised method without
requiring access to any ground-truth summaries. Further, we evaluate how
different measures of lexical centrality, inspired by past work on extractive
summarization, affect final performance.",https://github.com/sosuperic/MeanSum,7299
Generative Pre-Trained Transformers for Biologically Inspired Design,0.0336583,"Biological systems in nature have evolved for millions of years to adapt and
survive the environment. Many features they developed can be inspirational and
beneficial for solving technical problems in modern industries. This leads to a
novel form of design-by-analogy called bio-inspired design (BID). Although BID
as a design method has been proven beneficial, the gap between biology and
engineering continuously hinders designers from effectively applying the
method. Therefore, we explore the recent advance of artificial intelligence
(AI) for a computational approach to bridge the gap. This paper proposes a
generative design approach based on the pre-trained language model (PLM) to
automatically retrieve and map biological analogy and generate BID in the form
of natural language. The latest generative pre-trained transformer, namely
GPT-3, is used as the base PLM. Three types of design concept generators are
identified and fine-tuned from the PLM according to the looseness of the
problem space representation. Machine evaluators are also fine-tuned to assess
the correlation between the domains within the generated BID concepts. The
approach is then tested via a case study in which the fine-tuned models are
applied to generate and evaluate light-weighted flying car concepts inspired by
nature. The results show our approach can generate BID concepts with good
performance.",None,-1
Policy Regularization for Legible Behavior,0.0337435,"In Reinforcement Learning interpretability generally means to provide insight
into the agent's mechanisms such that its decisions are understandable by an
expert upon inspection. This definition, with the resulting methods from the
literature, may however fall short for online settings where the fluency of
interactions prohibits deep inspections of the decision-making algorithm. To
support interpretability in online settings it is useful to borrow from the
Explainable Planning literature methods that focus on the legibility of the
agent, by making its intention easily discernable in an observer model. As we
propose in this paper, injecting legible behavior inside an agent's policy
doesn't require modify components of its learning algorithm. Rather, the
agent's optimal policy can be regularized for legibility by evaluating how the
policy may produce observations that would make an observer infer an incorrect
policy. In our formulation, the decision boundary introduced by legibility
impacts the states in which the agent's policy returns an action that has high
likelihood also in other policies. In these cases, a trade-off between such
action, and legible/sub-optimal action is made.",None,2588
FAKD: Feature Augmented Knowledge Distillation for Semantic Segmentation,0.0779711,"In this work, we explore data augmentations for knowledge distillation on
semantic segmentation. To avoid over-fitting to the noise in the teacher
network, a large number of training examples is essential for knowledge
distillation. Imagelevel argumentation techniques like flipping, translation or
rotation are widely used in previous knowledge distillation framework. Inspired
by the recent progress on semantic directions on feature-space, we propose to
include augmentations in feature space for efficient distillation.
Specifically, given a semantic direction, an infinite number of augmentations
can be obtained for the student in the feature space. Furthermore, the analysis
shows that those augmentations can be optimized simultaneously by minimizing an
upper bound for the losses defined by augmentations. Based on the observation,
a new algorithm is developed for knowledge distillation in semantic
segmentation. Extensive experiments on four semantic segmentation benchmarks
demonstrate that the proposed method can boost the performance of current
knowledge distillation methods without any significant overhead. Code is
available at: https://github.com/jianlong-yuan/FAKD.",https://github.com/jianlong-yuan/FAKD,2777
Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data,0.00439231,"A number of studies have investigated the training of neural networks with
synthetic data for applications in the real world. The aim of this study is to
quantify how much real world data can be saved when using a mixed dataset of
synthetic and real world data. By modeling the relationship between the number
of training examples and detection performance by a simple power law, we find
that the need for real world data can be reduced by up to 70% without
sacrificing detection performance. The training of object detection networks is
especially enhanced by enriching the mixed dataset with classes
underrepresented in the real world dataset. The results indicate that mixed
datasets with real world data ratios between 5% and 20% reduce the need for
real world data the most without reducing the detection performance.",https://github.com/NVIDIA/pix2pixHD,12
A Human-Centric Perspective on Fairness and Transparency in Algorithmic Decision-Making,0.0,"Automated decision systems (ADS) are increasingly used for consequential
decision-making. These systems often rely on sophisticated yet opaque machine
learning models, which do not allow for understanding how a given decision was
arrived at. This is not only problematic from a legal perspective, but
non-transparent systems are also prone to yield unfair outcomes because their
sanity is challenging to assess and calibrate in the first place -- which is
particularly worrisome for human decision-subjects. Based on this observation
and building upon existing work, I aim to make the following three main
contributions through my doctoral thesis: (a) understand how (potential)
decision-subjects perceive algorithmic decisions (with varying degrees of
transparency of the underlying ADS), as compared to similar decisions made by
humans; (b) evaluate different tools for transparent decision-making with
respect to their effectiveness in enabling people to appropriately assess the
quality and fairness of ADS; and (c) develop human-understandable technical
artifacts for fair automated decision-making. Over the course of the first half
of my PhD program, I have already addressed substantial pieces of (a) and (c),
whereas (b) will be the major focus of the second half.",None,-1
Multi-Curve Translator for High-Resolution Photorealistic Image Translation,0.0150073,"The dominant image-to-image translation methods are based on fully
convolutional networks, which extract and translate an image's features and
then reconstruct the image. However, they have unacceptable computational costs
when working with high-resolution images. To this end, we present the
Multi-Curve Translator (MCT), which not only predicts the translated pixels for
the corresponding input pixels but also for their neighboring pixels. And if a
high-resolution image is downsampled to its low-resolution version, the lost
pixels are the remaining pixels' neighboring pixels. So MCT makes it possible
to feed the network only the downsampled image to perform the mapping for the
full-resolution image, which can dramatically lower the computational cost.
Besides, MCT is a plug-in approach that utilizes existing base models and
requires only replacing their output layers. Experiments demonstrate that the
MCT variants can process 4K images in real-time and achieve comparable or even
better performance than the base models on various photorealistic
image-to-image translation tasks.",None,1470
Identifying Electrocardiogram Abnormalities Using a Handcrafted-Rule-Enhanced Neural Network,0.139359,"A large number of people suffer from life-threatening cardiac abnormalities,
and electrocardiogram (ECG) analysis is beneficial to determining whether an
individual is at risk of such abnormalities. Automatic ECG classification
methods, especially the deep learning based ones, have been proposed to detect
cardiac abnormalities using ECG records, showing good potential to improve
clinical diagnosis and help early prevention of cardiovascular diseases.
However, the predictions of the known neural networks still do not
satisfactorily meet the needs of clinicians, and this phenomenon suggests that
some information used in clinical diagnosis may not be well captured and
utilized by these methods. In this paper, we introduce some rules into
convolutional neural networks, which help present clinical knowledge to deep
learning based ECG analysis, in order to improve automated ECG diagnosis
performance. Specifically, we propose a Handcrafted-Rule-enhanced Neural
Network (called HRNN) for ECG classification with standard 12-lead ECG input,
which consists of a rule inference module and a deep learning module.
Experiments on two large-scale public ECG datasets show that our new approach
considerably outperforms existing state-of-the-art methods. Further, our
proposed approach not only can improve the diagnosis performance, but also can
assist in detecting mislabelled ECG samples. Our codes are available at
https://github.com/alwaysbyx/ecg_processing.",https://github.com/alwaysbyx/ecg processing/,10744
Video Frame Interpolation with Transformer,0.0576022,"Video frame interpolation (VFI), which aims to synthesize intermediate frames
of a video, has made remarkable progress with development of deep convolutional
networks over past years. Existing methods built upon convolutional networks
generally face challenges of handling large motion due to the locality of
convolution operations. To overcome this limitation, we introduce a novel
framework, which takes advantage of Transformer to model long-range pixel
correlation among video frames. Further, our network is equipped with a novel
cross-scale window-based attention mechanism, where cross-scale windows
interact with each other. This design effectively enlarges the receptive field
and aggregates multi-scale information. Extensive quantitative and qualitative
experiments demonstrate that our method achieves new state-of-the-art results
on various benchmarks.",https://github.com/dvlab-research/VFIformer,76411
Bayesian Learning to Discover Mathematical Operations in Governing Equations of Dynamic Systems,0.243772,"Discovering governing equations from data is critical for diverse scientific
disciplines as they can provide insights into the underlying phenomenon of
dynamic systems. This work presents a new representation for governing
equations by designing the Mathematical Operation Network (MathONet) with a
deep neural network-like hierarchical structure. Specifically, the MathONet is
stacked by several layers of unary operations (e.g., sin, cos, log) and binary
operations (e.g., +,-), respectively. An initialized MathONet is typically
regarded as a super-graph with a redundant structure, a sub-graph of which can
yield the governing equation. We develop a sparse group Bayesian learning
algorithm to extract the sub-graph by employing structurally constructed priors
over the redundant mathematical operations. By demonstrating the chaotic Lorenz
system, Lotka-Volterra system, and Kolmogorov-Petrovsky-Piskunov system, the
proposed method can discover the ordinary differential equations (ODEs) and
partial differential equations (PDEs) from the observations given limited
mathematical operations, without any prior knowledge on possible expressions of
the ODEs and PDEs.",https://github.com/nips2021anonymous/MathONet,316
Overlapping Word Removal is All You Need: Revisiting Data Imbalance in Hope Speech Detection,0.0107542,"Hope Speech Detection, a task of recognizing positive expressions, has made
significant strides recently. However, much of the current works focus on model
development without considering the issue of inherent imbalance in the data.
Our work revisits this issue in hope-speech detection by introducing focal
loss, data augmentation, and pre-processing strategies. Accordingly, we find
that introducing focal loss as part of Multilingual-BERT's (M-BERT) training
process mitigates the effect of class imbalance and improves overall F1-Macro
by 0.11. At the same time, contextual and back-translation-based word
augmentation with M-BERT improves results by 0.10 over baseline despite
imbalance. Finally, we show that overlapping word removal based on
pre-processing, though simple, improves F1-Macro by 0.28. In due process, we
present detailed studies depicting various behaviors of each of these
strategies and summarize key findings from our empirical results for those
interested in getting the most out of M-BERT for hope speech detection under
real-world conditions of data imbalance.",None,4224
PInKS: Preconditioned Commonsense Inference with Minimal Supervision,0.0798573,"Reasoning with preconditions such as ""glass can be used for drinking water
unless the glass is shattered"" remains an open problem for language models. The
main challenge lies in the scarcity of preconditions data and the model's lack
of support for such reasoning. We present PInKS, Preconditioned Commonsense
Inference with WeaK Supervision, an improved model for reasoning with
preconditions through minimum supervision. We show, both empirically and
theoretically, that PInKS improves the results on benchmarks focused on
reasoning with the preconditions of commonsense knowledge (up to 40% Macro-F1
scores). We further investigate PInKS through PAC-Bayesian informativeness
analysis, precision measures, and ablation study.",https://github.com/luka-group/PInKS,-1
R2D2: Robust Data-to-Text with Replacement Detection,0.187147,"Unfaithful text generation is a common problem for text generation systems.
In the case of Data-to-Text (D2T) systems, the factuality of the generated text
is particularly crucial for any real-world applications. We introduce R2D2, a
training framework that addresses unfaithful Data-to-Text generation by
training a system both as a generator and a faithfulness discriminator with
additional replacement detection and unlikelihood learning tasks. To facilitate
such training, we propose two methods for sampling unfaithful sentences. We
argue that the poor entity retrieval capability of D2T systems is one of the
primary sources of unfaithfulness, so in addition to the existing metrics, we
further propose NER-based metrics to evaluate the fidelity of D2T generations.
Our experimental results show that R2D2 systems could effectively mitigate the
unfaithful text generation, and they achieve new state-of-the-art results on
FeTaQA, LogicNLG, and ToTTo, all with significant improvements.",https://github.com/Yale-LILY/r2d2,35424
Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons,0.0117894,"Recent studies have shown the advantages of evaluating NLG systems using
pairwise comparisons as opposed to direct assessment. Given $k$ systems, a
naive approach for identifying the top-ranked system would be to uniformly
obtain pairwise comparisons from all ${k \choose 2}$ pairs of systems. However,
this can be very expensive as the number of human annotations required would
grow quadratically with $k$. In this work, we introduce Active Evaluation, a
framework to efficiently identify the top-ranked system by actively choosing
system pairs for comparison using dueling bandit algorithms. We perform
extensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation
datasets spanning 5 tasks and show that the number of human annotations can be
reduced by 80%. To further reduce the number of human annotations, we propose
model-based dueling bandit algorithms which combine automatic evaluation
metrics with human evaluations. Specifically, we eliminate sub-optimal systems
even before the human annotation process and perform human evaluations only on
test examples where the automatic metric is highly uncertain. This reduces the
number of human annotations required further by 89%. In effect, we show that
identifying the top-ranked system requires only a few hundred human
annotations, which grow linearly with $k$. Lastly, we provide practical
recommendations and best practices to identify the top-ranked system
efficiently. Our code has been made publicly available at
https://github.com/akashkm99/duelnlg",https://github.com/akashkm99/duelnlg,-1
Mirror modular cloning and fast quantum associative retrieval,0.0124166,"We show that a quantum state can be perfectly cloned up to global mirroring
with a unitary transformation that depends on one single parameter. We then
show that this is equivalent to ""perfect"" cloning for quantum associative
memories which, as a consequence efficiently hold exponentially more
information than their classical counterparts. Finally, we present a quantum
associative retrieval algorithm which can correct corrupted inputs and is
exponentially faster than the Grover algorithm.",None,-1
Deceptive Planning for Resource Allocation,0.0134552,"We consider a team of autonomous agents that navigate in an adversarial
environment and aim to achieve a task by allocating their resources over a set
of target locations. An adversary in the environment observes the autonomous
team's behavior to infer their objective and responds against the team. In this
setting, we propose strategies for controlling the density of the autonomous
team so that they can deceive the adversary regarding their objective while
achieving the desired final resource allocation. We first develop a prediction
algorithm based on the principle of maximum entropy to express the team's
behavior expected by the adversary. Then, by measuring the deceptiveness via
Kullback-Leibler divergence, we devise convex optimization-based planning
algorithms that deceive the adversary by either exaggerating the behavior
towards a decoy allocation strategy or creating ambiguity regarding the final
allocation strategy. A user study with $320$ participants demonstrates that the
proposed algorithms are effective for deception and reveal the inherent biases
of participants towards proximate goals.",https://github.com/vivianchen98/deception_user_study_data,-1
MiQA: A Benchmark for Inference on Metaphorical Questions,0.625114,"We propose a benchmark to assess the capability of large language models to
reason with conventional metaphors. Our benchmark combines the previously
isolated topics of metaphor detection and commonsense reasoning into a single
task that requires a model to make inferences by accurately selecting between
the literal and metaphorical register. We examine the performance of
state-of-the-art pre-trained models on binary-choice tasks and find a large
discrepancy between the performance of small and very large models, going from
chance to near-human level. We also analyse the largest model in a generative
setting and find that although human performance is approached, careful
multiple-shot prompting is required.",https://github.com/google-research/language/tree/master/language/miqa,-1
Mixed-effects transformers for hierarchical adaptation,0.00532336,"Language use differs dramatically from context to context. To some degree,
modern language models like GPT-3 are able to account for such variance by
conditioning on a string of previous input text, or prompt. Yet prompting is
ineffective when contexts are sparse, out-of-sample, or extra-textual; for
instance, accounting for when and where the text was produced or who produced
it. In this paper, we introduce the mixed-effects transformer (MET), a novel
approach for learning hierarchically-structured prefixes -- lightweight modules
prepended to the input -- to account for structured variation. Specifically, we
show how the popular class of mixed-effects models may be extended to
transformer-based architectures using a regularized prefix-tuning procedure
with dropout. We evaluate this approach on several domain-adaptation
benchmarks, finding that it efficiently adapts to novel contexts with minimal
data while still effectively generalizing to unseen contexts.",https://github.com/juliaiwhite/mixed-effects-transformers,-1
Why Adversarial Training of ReLU Networks Is Difficult?,0.00430038,"This paper mathematically derives an analytic solution of the adversarial
perturbation on a ReLU network, and theoretically explains the difficulty of
adversarial training. Specifically, we formulate the dynamics of the
adversarial perturbation generated by the multi-step attack, which shows that
the adversarial perturbation tends to strengthen eigenvectors corresponding to
a few top-ranked eigenvalues of the Hessian matrix of the loss w.r.t. the
input. We also prove that adversarial training tends to strengthen the
influence of unconfident input samples with large gradient norms in an
exponential manner. Besides, we find that adversarial training strengthens the
influence of the Hessian matrix of the loss w.r.t. network parameters, which
makes the adversarial training more likely to oscillate along directions of a
few samples, and boosts the difficulty of adversarial training. Crucially, our
proofs provide a unified explanation for previous findings in understanding
adversarial training.",None,-1
Computational Complexity of Segmentation,0.0,"Computational feasibility is a widespread concern that guides the framing and
modeling of biological and artificial intelligence. The specification of
cognitive system capacities is often shaped by unexamined intuitive assumptions
about the search space and complexity of a subcomputation. However, a mistaken
intuition might make such initial conceptualizations misleading for what
empirical questions appear relevant later on. We undertake here
computational-level modeling and complexity analyses of segmentation - a widely
hypothesized subcomputation that plays a requisite role in explanations of
capacities across domains - as a case study to show how crucial it is to
formally assess these assumptions. We mathematically prove two sets of results
regarding hardness and search space size that may run counter to intuition, and
position their implications with respect to existing views on the subcapacity.",None,-1
MultiEarth 2022 Deforestation Challenge -- ForestGump,0.0047734,"The estimation of deforestation in the Amazon Forest is challenge task
because of the vast size of the area and the difficulty of direct human access.
However, it is a crucial problem in that deforestation results in serious
environmental problems such as global climate change, reduced biodiversity,
etc. In order to effectively solve the problems, satellite imagery would be a
good alternative to estimate the deforestation of the Amazon. With a
combination of optical images and Synthetic aperture radar (SAR) images,
observation of such a massive area regardless of weather conditions become
possible. In this paper, we present an accurate deforestation estimation method
with conventional UNet and comprehensive data processing. The diverse channels
of Sentinel-1, Sentinel-2 and Landsat 8 are carefully selected and utilized to
train deep neural networks. With the proposed method, deforestation status for
novel queries are successfully estimated with high accuracy.",None,-1
An Effective Approach for Multi-label Classification with Missing Labels,0.023667,"Compared with multi-class classification, multi-label classification that
contains more than one class is more suitable in real life scenarios. Obtaining
fully labeled high-quality datasets for multi-label classification problems,
however, is extremely expensive, and sometimes even infeasible, with respect to
annotation efforts, especially when the label spaces are too large. This
motivates the research on partial-label classification, where only a limited
number of labels are annotated and the others are missing. To address this
problem, we first propose a pseudo-label based approach to reduce the cost of
annotation without bringing additional complexity to the existing
classification networks. Then we quantitatively study the impact of missing
labels on the performance of classifier. Furthermore, by designing a novel loss
function, we are able to relax the requirement that each instance must contain
at least one positive label, which is commonly used in most existing
approaches. Through comprehensive experiments on three large-scale multi-label
image datasets, i.e. MS-COCO, NUS-WIDE, and Pascal VOC12, we show that our
method can handle the imbalance between positive labels and negative labels,
while still outperforming existing missing-label learning approaches in most
cases, and in some cases even approaches with fully labeled datasets.",None,-1
GANzzle: Reframing jigsaw puzzle solving as a retrieval task using a generative mental image,0.0497539,"Puzzle solving is a combinatorial challenge due to the difficulty of matching
adjacent pieces. Instead, we infer a mental image from all pieces, which a
given piece can then be matched against avoiding the combinatorial explosion.
Exploiting advancements in Generative Adversarial methods, we learn how to
reconstruct the image given a set of unordered pieces, allowing the model to
learn a joint embedding space to match an encoding of each piece to the cropped
layer of the generator. Therefore we frame the problem as a R@1 retrieval task,
and then solve the linear assignment using differentiable Hungarian attention,
making the process end-to-end. In doing so our model is puzzle size agnostic,
in contrast to prior deep learning methods which are single size. We evaluate
on two new large-scale datasets, where our model is on par with deep learning
methods, while generalizing to multiple puzzle sizes.",https://github.com/IIT-PAVIS/,-1
Continual Learning Approaches for Anomaly Detection,0.0113121,"Anomaly Detection is a relevant problem that arises in numerous real-world
applications, especially when dealing with images. However, there has been
little research for this task in the Continual Learning setting. In this work,
we introduce a novel approach called SCALE (SCALing is Enough) to perform
Compressed Replay in a framework for Anomaly Detection in Continual Learning
setting. The proposed technique scales and compresses the original images using
a Super Resolution model which, to the best of our knowledge, is studied for
the first time in the Continual Learning setting. SCALE can achieve a high
level of compression while maintaining a high level of image reconstruction
quality. In conjunction with other Anomaly Detection approaches, it can achieve
optimal results. To validate the proposed approach, we use a real-world dataset
of images with pixel-based anomalies, with the scope to provide a reliable
benchmark for Anomaly Detection in the context of Continual Learning, serving
as a foundation for further advancements in the field.",https://github.com/dallepezze/adcl_scale,-1
Online Motion Style Transfer for Interactive Character Control,0.0225261,"Motion style transfer is highly desired for motion generation systems for
gaming. Compared to its offline counterpart, the research on online motion
style transfer under interactive control is limited. In this work, we propose
an end-to-end neural network that can generate motions with different styles
and transfer motion styles in real-time under user control. Our approach
eliminates the use of handcrafted phase features, and could be easily trained
and directly deployed in game systems. In the experiment part, we evaluate our
approach from three aspects that are essential for industrial game design:
accuracy, flexibility, and variety, and our model performs a satisfying result.",None,-1
Spoken Term Detection and Relevance Score Estimation using Dot-Product of Pronunciation Embeddings,0.0704162,"The paper describes a novel approach to Spoken Term Detection (STD) in large
spoken archives using deep LSTM networks. The work is based on the previous
approach of using Siamese neural networks for STD and naturally extends it to
directly localize a spoken term and estimate its relevance score. The phoneme
confusion network generated by a phoneme recognizer is processed by the deep
LSTM network which projects each segment of the confusion network into an
embedding space. The searched term is projected into the same embedding space
using another deep LSTM network. The relevance score is then computed using a
simple dot-product in the embedding space and calibrated using a sigmoid
function to predict the probability of occurrence. The location of the searched
term is then estimated from the sequence of output probabilities. The deep LSTM
networks are trained in a self-supervised manner from paired recognition
hypotheses on word and phoneme levels. The method is experimentally evaluated
on MALACH data in English and Czech languages.",None,-1
Paddy Leaf diseases identification on Infrared Images based on Convolutional Neural Networks,0.0475261,"Agriculture is the mainstay of human society because it is an essential need
for every organism. Paddy cultivation is very significant so far as humans are
concerned, largely in the Asian continent, and it is one of the staple foods.
However, plant diseases in agriculture lead to depletion in productivity. Plant
diseases are generally caused by pests, insects, and pathogens that decrease
productivity to a large scale if not controlled within a particular time.
Eventually, one cannot see an increase in paddy yield. Accurate and timely
identification of plant diseases can help farmers mitigate losses due to pests
and diseases. Recently, deep learning techniques have been used to identify
paddy diseases and overcome these problems. This paper implements a
convolutional neural network (CNN) based on a model and tests a public dataset
consisting of 636 infrared image samples with five paddy disease classes and
one healthy class. The proposed model proficiently identified and classified
paddy diseases of five different types and achieved an accuracy of 88.28%",None,-1
Counterfactually Evaluating Explanations in Recommender Systems,0.0927823,"Modern recommender systems face an increasing need to explain their
recommendations. Despite considerable progress in this area, evaluating the
quality of explanations remains a significant challenge for researchers and
practitioners. Prior work mainly conducts human study to evaluate explanation
quality, which is usually expensive, time-consuming, and prone to human bias.
In this paper, we propose an offline evaluation method that can be computed
without human involvement. To evaluate an explanation, our method quantifies
its counterfactual impact on the recommendation. To validate the effectiveness
of our method, we carry out an online user study. We show that, compared to
conventional methods, our method can produce evaluation scores more correlated
with the real human judgments, and therefore can serve as a better proxy for
human evaluation. In addition, we show that explanations with high evaluation
scores are considered better by humans. Our findings highlight the promising
direction of using the counterfactual approach as one possible way to evaluate
recommendation explanations.",None,-1
Characterizing the Action-Generalization Gap in Deep Q-Learning,0.00913586,"We study the action generalization ability of deep Q-learning in discrete
action spaces. Generalization is crucial for efficient reinforcement learning
(RL) because it allows agents to use knowledge learned from past experiences on
new tasks. But while function approximation provides deep RL agents with a
natural way to generalize over state inputs, the same generalization mechanism
does not apply to discrete action outputs. And yet, surprisingly, our
experiments indicate that Deep Q-Networks (DQN), which use exactly this type of
function approximator, are still able to achieve modest action generalization.
Our main contribution is twofold: first, we propose a method of evaluating
action generalization using expert knowledge of action similarity, and
empirically confirm that action generalization leads to faster learning;
second, we characterize the action-generalization gap (the difference in
learning performance between DQN and the expert) in different domains. We find
that DQN can indeed generalize over actions in several simple domains, but that
its ability to do so decreases as the action space grows larger.",None,-1
Predicting Long-Term Citations from Short-Term Linguistic Influence,0.0313709,"A standard measure of the influence of a research paper is the number of
times it is cited. However, papers may be cited for many reasons, and citation
count offers limited information about the extent to which a paper affected the
content of subsequent publications. We therefore propose a novel method to
quantify linguistic influence in timestamped document collections. There are
two main steps: first, identify lexical and semantic changes using contextual
embeddings and word frequencies; second, aggregate information about these
changes into per-document influence scores by estimating a high-dimensional
Hawkes process with a low-rank parameter matrix. We show that this measure of
linguistic influence is predictive of $\textit{future}$ citations: the estimate
of linguistic influence from the two years after a paper's publication is
correlated with and predictive of its citation count in the following three
years. This is demonstrated using an online evaluation with incremental
temporal training/test splits, in comparison with a strong baseline that
includes predictors for initial citation counts, topics, and lexical features.",http://github.com/sandeepsoni/,-1
Learning Point Processes using Recurrent Graph Network,0.00213332,"We present a novel Recurrent Graph Network (RGN) approach for predicting
discrete marked event sequences by learning the underlying complex stochastic
process. Using the framework of Point Processes, we interpret a marked discrete
event sequence as the superposition of different sequences each of a unique
type. The nodes of the Graph Network use LSTM to incorporate past information
whereas a Graph Attention Network (GAT Network) introduces strong inductive
biases to capture the interaction between these different types of events. By
changing the self-attention mechanism from attending over past events to
attending over event types, we obtain a reduction in time and space complexity
from $\mathcal{O}(N^2)$ (total number of events) to
$\mathcal{O}(|\mathcal{Y}|^2)$ (number of event types). Experiments show that
the proposed approach improves performance in log-likelihood, prediction and
goodness-of-fit tasks with lower time and space complexity compared to
state-of-the art Transformer based architectures.",None,-1
Can GAN-induced Attribute Manipulations Impact Face Recognition?,0.0201071,"Impact due to demographic factors such as age, sex, race, etc., has been
studied extensively in automated face recognition systems. However, the impact
of \textit{digitally modified} demographic and facial attributes on face
recognition is relatively under-explored. In this work, we study the effect of
attribute manipulations induced via generative adversarial networks (GANs) on
face recognition performance. We conduct experiments on the CelebA dataset by
intentionally modifying thirteen attributes using AttGAN and STGAN and
evaluating their impact on two deep learning-based face verification methods,
ArcFace and VGGFace. Our findings indicate that some attribute manipulations
involving eyeglasses and digital alteration of sex cues can significantly
impair face recognition by up to 73% and need further analysis.",None,-1
Implicit Regularization with Polynomial Growth in Deep Tensor Factorization,0.0138873,"We study the implicit regularization effects of deep learning in tensor
factorization. While implicit regularization in deep matrix and 'shallow'
tensor factorization via linear and certain type of non-linear neural networks
promotes low-rank solutions with at most quadratic growth, we show that its
effect in deep tensor factorization grows polynomially with the depth of the
network. This provides a remarkably faithful description of the observed
experimental behaviour. Using numerical experiments, we demonstrate the
benefits of this implicit regularization in yielding a more accurate estimation
and better convergence properties.",None,-1
Assessing the Effects of Hyperparameters on Knowledge Graph Embedding Quality,0.0098751,"Embedding knowledge graphs into low-dimensional spaces is a popular method
for applying approaches, such as link prediction or node classification, to
these databases. This embedding process is very costly in terms of both
computational time and space. Part of the reason for this is the optimisation
of hyperparameters, which involves repeatedly sampling, by random, guided, or
brute-force selection, from a large hyperparameter space and testing the
resulting embeddings for their quality. However, not all hyperparameters in
this search space will be equally important. In fact, with prior knowledge of
the relative importance of the hyperparameters, some could be eliminated from
the search altogether without significantly impacting the overall quality of
the outputted embeddings. To this end, we ran a Sobol sensitivity analysis to
evaluate the effects of tuning different hyperparameters on the variance of
embedding quality. This was achieved by performing thousands of embedding
trials, each time measuring the quality of embeddings produced by different
hyperparameter configurations. We regressed the embedding quality on those
hyperparameter configurations, using this model to generate Sobol sensitivity
indices for each of the hyperparameters. By evaluating the correlation between
Sobol indices, we find substantial variability in the hyperparameter
sensitivities between knowledge graphs, with differing dataset characteristics
being the probable cause of these inconsistencies. As an additional
contribution of this work we identify several relations in the UMLS knowledge
graph that may cause data leakage via inverse relations, and derive and present
UMLS-43, a leakage-robust variant of that graph.",None,-1
Improving Shape Awareness and Interpretability in Deep Networks Using Geometric Moments,0.0460958,"Deep networks for image classification often rely more on texture information
than object shape. While efforts have been made to make deep-models
shape-aware, it is often difficult to make such models simple, interpretable,
or rooted in known mathematical definitions of shape. This paper presents a
deep-learning model inspired by geometric moments, a classically well
understood approach to measure shape-related properties. The proposed method
consists of a trainable network for generating coordinate bases and affine
parameters for making the features geometrically invariant yet in a
task-specific manner. The proposed model improves the final feature's
interpretation. We demonstrate the effectiveness of our method on standard
image classification datasets. The proposed model achieves higher
classification performance compared to the baseline and standard ResNet models
while substantially improving interpretability.",None,-1
Stacking Ensemble Learning in Deep Domain Adaptation for Ophthalmic Image Classification,0.00616899,"Domain adaptation is an attractive approach given the availability of a large
amount of labeled data with similar properties but different domains. It is
effective in image classification tasks where obtaining sufficient label data
is challenging. We propose a novel method, named SELDA, for stacking ensemble
learning via extending three domain adaptation methods for effectively solving
real-world problems. The major assumption is that when base domain adaptation
models are combined, we can obtain a more accurate and robust model by
exploiting the ability of each of the base models. We extend Maximum Mean
Discrepancy (MMD), Low-rank coding, and Correlation Alignment (CORAL) to
compute the adaptation loss in three base models. Also, we utilize a two-fully
connected layer network as a meta-model to stack the output predictions of
these three well-performing domain adaptation models to obtain high accuracy in
ophthalmic image classification tasks. The experimental results using
Age-Related Eye Disease Study (AREDS) benchmark ophthalmic dataset demonstrate
the effectiveness of the proposed model.",None,-1
Resolving Semantic Confusions for Improved Zero-Shot Detection,0.0529035,"Zero-shot detection (ZSD) is a challenging task where we aim to recognize and
localize objects simultaneously, even when our model has not been trained with
visual samples of a few target (""unseen"") classes. Recently, methods employing
generative models like GANs have shown some of the best results, where
unseen-class samples are generated based on their semantics by a GAN trained on
seen-class data, enabling vanilla object detectors to recognize unseen objects.
However, the problem of semantic confusion still remains, where the model is
sometimes unable to distinguish between semantically-similar classes. In this
work, we propose to train a generative model incorporating a triplet loss that
acknowledges the degree of dissimilarity between classes and reflects them in
the generated samples. Moreover, a cyclic-consistency loss is also enforced to
ensure that generated visual samples of a class highly correspond to their own
semantics. Extensive experiments on two benchmark ZSD datasets - MSCOCO and
PASCAL-VOC - demonstrate significant gains over the current ZSD methods,
reducing semantic confusion and improving detection for the unseen classes.",https://github.com/sandipan211/ZSD-SC-Resolver,-1
An Efficient FPGA-based Accelerator for Deep Forest,0.022816,"Deep Forest is a prominent machine learning algorithm known for its high
accuracy in forecasting. Compared with deep neural networks, Deep Forest has
almost no multiplication operations and has better performance on small
datasets. However, due to the deep structure and large forest quantity, it
suffers from large amounts of calculation and memory consumption. In this
paper, an efficient hardware accelerator is proposed for deep forest models,
which is also the first work to implement Deep Forest on FPGA. Firstly, a
delicate node computing unit (NCU) is designed to improve inference speed.
Secondly, based on NCU, an efficient architecture and an adaptive dataflow are
proposed, in order to alleviate the problem of node computing imbalance in the
classification process. Moreover, an optimized storage scheme in this design
also improves hardware utilization and power efficiency. The proposed design is
implemented on an FPGA board, Intel Stratix V, and it is evaluated by two
typical datasets, ADULT and Face Mask Detection. The experimental results show
that the proposed design can achieve around 40x speedup compared to that on a
40 cores high performance x86 CPU.",None,-1
Find a Way Forward: a Language-Guided Semantic Map Navigator,0.0759383,"In this paper, we introduce the map-language navigation task where an agent
executes natural language instructions and moves to the target position based
only on a given 3D semantic map. To tackle the task, we design the
instruction-aware Path Proposal and Discrimination model (iPPD). Our approach
leverages map information to provide instruction-aware path proposals, i.e., it
selects all potential instruction-aligned candidate paths to reduce the
solution space. Next, to represent the map observations along a path for a
better modality alignment, a novel Path Feature Encoding scheme tailored for
semantic maps is proposed. An attention-based Language Driven Discriminator is
designed to evaluate path candidates and determine the best path as the final
result. Our method can naturally avoid error accumulation compared with
single-step greedy decision methods. Comparing to a single-step imitation
learning approach, iPPD has performance gains above 17% on navigation success
and 0.18 on path matching measurement nDTW in challenging unseen environments.",None,-1
Cross-Align: Modeling Deep Cross-lingual Interactions for Word Alignment,0.0211242,"Word alignment which aims to extract lexicon translation equivalents between
source and target sentences, serves as a fundamental tool for natural language
processing. Recent studies in this area have yielded substantial improvements
by generating alignments from contextualized embeddings of the pre-trained
multilingual language models. However, we find that the existing approaches
capture few interactions between the input sentence pairs, which degrades the
word alignment quality severely, especially for the ambiguous words in the
monolingual context. To remedy this problem, we propose Cross-Align to model
deep interactions between the input sentence pairs, in which the source and
target sentences are encoded separately with the shared self-attention modules
in the shallow layers, while cross-lingual interactions are explicitly
constructed by the cross-attention modules in the upper layers. Besides, to
train our model effectively, we propose a two-stage training framework, where
the model is trained with a simple Translation Language Modeling (TLM)
objective in the first stage and then finetuned with a self-supervised
alignment objective in the second stage. Experiments show that the proposed
Cross-Align achieves the state-of-the-art (SOTA) performance on four out of
five language pairs.",https://github.com/lisasiyu/Cross-Align,-1
Offline Reinforcement Learning for Road Traffic Control,0.0403487,"Traffic signal control is an important problem in urban mobility with a
significant potential of economic and environmental impact. While there is a
growing interest in Reinforcement Learning (RL) for traffic signal control, the
work so far has focussed on learning through simulations which could lead to
inaccuracies due to simplifying assumptions. Instead, real experience data on
traffic is available and could be exploited at minimal costs. Recent progress
in offline or batch RL has enabled just that. Model-based offline RL methods,
in particular, have been shown to generalize from the experience data much
better than others.
  We build a model-based learning framework which infers a Markov Decision
Process (MDP) from a dataset collected using a cyclic traffic signal control
policy that is both commonplace and easy to gather. The MDP is built with
pessimistic costs to manage out-of-distribution scenarios using an adaptive
shaping of rewards which is shown to provide better regularization compared to
the prior related work in addition to being PAC-optimal. Our model is evaluated
on a complex signalized roundabout showing that it is possible to build highly
performant traffic control policies in a data efficient manner.",None,-1
"What's Different between Visual Question Answering for Machine ""Understanding"" Versus for Accessibility?",0.0,"In visual question answering (VQA), a machine must answer a question given an
associated image. Recently, accessibility researchers have explored whether VQA
can be deployed in a real-world setting where users with visual impairments
learn about their environment by capturing their visual surroundings and asking
questions. However, most of the existing benchmarking datasets for VQA focus on
machine ""understanding"" and it remains unclear how progress on those datasets
corresponds to improvements in this real-world use case. We aim to answer this
question by evaluating discrepancies between machine ""understanding"" datasets
(VQA-v2) and accessibility datasets (VizWiz) by evaluating a variety of VQA
models. Based on our findings, we discuss opportunities and challenges in VQA
for accessibility and suggest directions for future work.",https://github.com/kyleseelman/vqa_accessibility,-1
Unsupervised Fish Trajectory Tracking and Segmentation,0.0252777,"DNN for fish tracking and segmentation based on high-quality labels is
expensive. Alternative unsupervised approaches rely on spatial and temporal
variations that naturally occur in video data to generate noisy
pseudo-ground-truth labels. These pseudo-labels are used to train a multi-task
deep neural network. In this paper, we propose a three-stage framework for
robust fish tracking and segmentation, where the first stage is an optical flow
model, which generates the pseudo labels using spatial and temporal consistency
between frames. In the second stage, a self-supervised model refines the
pseudo-labels incrementally. In the third stage, the refined labels are used to
train a segmentation network. No human annotations are used during the training
or inference. Extensive experiments are performed to validate our method on
three public underwater video datasets and to demonstrate that it is highly
effective for video annotation and segmentation. We also evaluate the
robustness of our framework to different imaging conditions and discuss the
limitations of our current implementation.",None,-1
Sensor Data Fusion in Top-View Grid Maps using Evidential Reasoning with Advanced Conflict Resolution,0.0215914,"We present a new method to combine evidential top-view grid maps estimated
based on heterogeneous sensor sources. Dempster's combination rule that is
usually applied in this context provides undesired results with highly
conflicting inputs. Therefore, we use more advanced evidential reasoning
techniques and improve the conflict resolution by modeling the reliability of
the evidence sources. We propose a data-driven reliability estimation to
optimize the fusion quality using the Kitti-360 dataset. We apply the proposed
method to the fusion of LiDAR and stereo camera data and evaluate the results
qualitatively and quantitatively. The results demonstrate that our proposed
method robustly combines measurements from heterogeneous sensors and
successfully resolves sensor conflicts.",None,-1
Multimodal and Explainable Internet Meme Classification,0.0443941,"In the current context where online platforms have been effectively
weaponized in a variety of geo-political events and social issues, Internet
memes make fair content moderation at scale even more difficult. Existing work
on meme classification and tracking has focused on black-box methods that do
not explicitly consider the semantics of the memes or the context of their
creation. In this paper, we pursue a modular and explainable architecture for
Internet meme understanding. We design and implement multimodal classification
methods that perform example- and prototype-based reasoning over training
cases, while leveraging both textual and visual SOTA models to represent the
individual cases. We study the relevance of our modular and explainable models
in detecting harmful memes on two existing tasks: Hate Speech Detection and
Misogyny Classification. We compare the performance between example- and
prototype-based methods, and between text, vision, and multimodal models,
across different categories of harmfulness (e.g., stereotype and
objectification). We devise a user-friendly interface that facilitates the
comparative analysis of examples retrieved by all of our models for any given
meme, informing the community about the strengths and limitations of these
explainable methods.",https://github.com/usc-isi-i2/meme-understanding,-1
Argumentative Reward Learning: Reasoning About Human Preferences,0.0157905,"We define a novel neuro-symbolic framework, argumentative reward learning,
which combines preference-based argumentation with existing approaches to
reinforcement learning from human feedback. Our method improves prior work by
generalising human preferences, reducing the burden on the user and increasing
the robustness of the reward model. We demonstrate this with a number of
experiments.",None,-1
Efficient Hybrid Network: Inducting Scattering Features,0.0405541,"Recent work showed that hybrid networks, which combine predefined and learnt
filters within a single architecture, are more amenable to theoretical analysis
and less prone to overfitting in data-limited scenarios. However, their
performance has yet to prove competitive against the conventional counterparts
when sufficient amounts of training data are available. In an attempt to
address this core limitation of current hybrid networks, we introduce an
Efficient Hybrid Network (E-HybridNet). We show that it is the first scattering
based approach that consistently outperforms its conventional counterparts on a
diverse range of datasets. It is achieved with a novel inductive architecture
that embeds scattering features into the network flow using Hybrid Fusion
Blocks. We also demonstrate that the proposed design inherits the key property
of prior hybrid networks -- an effective generalisation in data-limited
scenarios. Our approach successfully combines the best of the two worlds:
flexibility and power of learnt features and stability and predictability of
scattering representations.",https://github.com/dminskiy/EHybridNet-icpr2022,-1
PSA-Det3D: Pillar Set Abstraction for 3D object Detection,0.00699342,"Small object detection for 3D point cloud is a challenging problem because of
two limitations: (1) Perceiving small objects is much more diffcult than normal
objects due to the lack of valid points. (2) Small objects are easily blocked
which breaks the shape of their meshes in 3D point cloud. In this paper, we
propose a pillar set abstraction (PSA) and foreground point compensation (FPC)
and design a point-based detection network, PSA-Det3D, to improve the detection
performance for small object. The PSA embeds a pillar query operation on the
basis of set abstraction (SA) to expand its receptive field of the network,
which can aggregate point-wise features effectively. To locate more occluded
objects, we persent a proposal generation layer consisting of a foreground
point segmentation and a FPC module. Both the foreground points and the
estimated centers are finally fused together to generate the detection result.
The experiments on the KITTI 3D detection benchmark show that our proposed
PSA-Det3D outperforms other algorithms with high accuracy for small object
detection.",https://github.com/open-mmlab/OpenPCDet,-1
Representing Spatial Trajectories as Distributions,0.0,"We introduce a representation learning framework for spatial trajectories. We
represent partial observations of trajectories as probability distributions in
a learned latent space, which characterize the uncertainty about unobserved
parts of the trajectory. Our framework allows us to obtain samples from a
trajectory for any continuous point in time, both interpolating and
extrapolating. Our flexible approach supports directly modifying specific
attributes of a trajectory, such as its pace, as well as combining different
partial observations into single representations. Experiments show our method's
advantage over baselines in prediction tasks.",https://github.com/open-mmlab/mmskeleton,-1
Understanding the Domain Gap in LiDAR Object Detection Networks,0.0348202,"In order to make autonomous driving a reality, artificial neural networks
have to work reliably in the open-world. However, the open-world is vast and
continuously changing, so it is not technically feasible to collect and
annotate training datasets which accurately represent this domain. Therefore,
there are always domain gaps between training datasets and the open-world which
must be understood. In this work, we investigate the domain gaps between
high-resolution and low-resolution LiDAR sensors in object detection networks.
Using a unique dataset, which enables us to study sensor resolution domain gaps
independent of other effects, we show two distinct domain gaps - an inference
domain gap and a training domain gap. The inference domain gap is characterised
by a strong dependence on the number of LiDAR points per object, while the
training gap shows no such dependence. These fndings show that different
approaches are required to close these inference and training domain gaps.",None,-1
A Baseline for Detecting Out-of-Distribution Examples in Image Captioning,0.0101605,"Image captioning research achieved breakthroughs in recent years by
developing neural models that can generate diverse and high-quality
descriptions for images drawn from the same distribution as training images.
However, when facing out-of-distribution (OOD) images, such as corrupted
images, or images containing unknown objects, the models fail in generating
relevant captions.
  In this paper, we consider the problem of OOD detection in image captioning.
We formulate the problem and suggest an evaluation setup for assessing the
model's performance on the task. Then, we analyze and show the effectiveness of
the caption's likelihood score at detecting and rejecting OOD images, which
implies that the relatedness between the input image and the generated caption
is encapsulated within the score.",None,-1
Low-resource Accent Classification in Geographically-proximate Settings: A Forensic and Sociophonetics Perspective,0.205297,"Accented speech recognition and accent classification are relatively
under-explored research areas in speech technology. Recently, deep
learning-based methods and Transformer-based pretrained models have achieved
superb performances in both areas. However, most accent classification tasks
focused on classifying different kinds of English accents and little attention
was paid to geographically-proximate accent classification, especially under a
low-resource setting where forensic speech science tasks usually encounter. In
this paper, we explored three main accent modelling methods combined with two
different classifiers based on 105 speaker recordings retrieved from five urban
varieties in Northern England. Although speech representations generated from
pretrained models generally have better performances in downstream
classification, traditional methods like Mel Frequency Cepstral Coefficients
(MFCCs) and formant measurements are equipped with specific strengths. These
results suggest that in forensic phonetics scenario where data are relatively
scarce, a simple modelling method and classifier could be competitive with
state-of-the-art pretrained speech models as feature extractors, which could
enhance a sooner estimation for the accent information in practices. Besides,
our findings also cross-validated a new methodology in quantifying
sociophonetic changes.",None,-1
Data Generation for Satellite Image Classification Using Self-Supervised Representation Learning,0.0080474,"Supervised deep neural networks are the-state-of-the-art for many tasks in
the remote sensing domain, against the fact that such techniques require the
dataset consisting of pairs of input and label, which are rare and expensive to
collect in term of both manpower and resources. On the other hand, there are
abundance of raw satellite images available both for commercial and academic
purposes. Hence, in this work, we tackle the insufficient labeled data problem
in satellite image classification task by introducing the process based on the
self-supervised learning technique to create the synthetic labels for satellite
image patches. These synthetic labels can be used as the training dataset for
the existing supervised learning techniques. In our experiments, we show that
the models trained on the synthetic labels give similar performance to the
models trained on the real labels. And in the process of creating the synthetic
labels, we also obtain the visual representation vectors that are versatile and
knowledge transferable.",None,-1
Few-Max: Few-Shot Domain Adaptation for Unsupervised Contrastive Representation Learning,0.0153355,"Contrastive self-supervised learning methods learn to map data points such as
images into non-parametric representation space without requiring labels. While
highly successful, current methods require a large amount of data in the
training phase. In situations where the target training set is limited in size,
generalization is known to be poor. Pretraining on a large source data set and
fine-tuning on the target samples is prone to overfitting in the few-shot
regime, where only a small number of target samples are available. Motivated by
this, we propose a domain adaption method for self-supervised contrastive
learning, termed Few-Max, to address the issue of adaptation to a target
distribution under few-shot learning. To quantify the representation quality,
we evaluate Few-Max on a range of source and target datasets, including
ImageNet, VisDA, and fastMRI, on which Few-Max consistently outperforms other
approaches.",https://github.com/utcsilab/fewmax,-1
MEAT: Maneuver Extraction from Agent Trajectories,0.0242818,"Advances in learning-based trajectory prediction are enabled by large-scale
datasets. However, in-depth analysis of such datasets is limited. Moreover, the
evaluation of prediction models is limited to metrics averaged over all samples
in the dataset. We propose an automated methodology that allows to extract
maneuvers (e.g., left turn, lane change) from agent trajectories in such
datasets. The methodology considers information about the agent dynamics and
information about the lane segments the agent traveled along. Although it is
possible to use the resulting maneuvers for training classification networks,
we exemplary use them for extensive trajectory dataset analysis and
maneuver-specific evaluation of multiple state-of-the-art trajectory prediction
models. Additionally, an analysis of the datasets and an evaluation of the
prediction models based on the agent dynamics is provided.",None,-1
Advanced Conditional Variational Autoencoders (A-CVAE): Towards interpreting open-domain conversation generation via disentangling latent feature representation,0.0108221,"Currently end-to-end deep learning based open-domain dialogue systems remain
black box models, making it easy to generate irrelevant contents with
data-driven models. Specifically, latent variables are highly entangled with
different semantics in the latent space due to the lack of priori knowledge to
guide the training. To address this problem, this paper proposes to harness the
generative model with a priori knowledge through a cognitive approach involving
mesoscopic scale feature disentanglement. Particularly, the model integrates
the macro-level guided-category knowledge and micro-level open-domain dialogue
data for the training, leveraging the priori knowledge into the latent space,
which enables the model to disentangle the latent variables within the
mesoscopic scale. Besides, we propose a new metric for open-domain dialogues,
which can objectively evaluate the interpretability of the latent space
distribution. Finally, we validate our model on different datasets and
experimentally demonstrate that our model is able to generate higher quality
and more interpretable dialogues than other models.",None,-1
DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships,0.0436572,"In this paper, we propose DimonGen, which aims to generate diverse sentences
describing concept relationships in various everyday scenarios. To support
this, we first create a benchmark dataset for this task by adapting the
existing CommonGen dataset. We then propose a two-stage model called MoREE to
generate the target sentences. MoREE consists of a mixture of retrievers model
that retrieves diverse context sentences related to the given concepts, and a
mixture of generators model that generates diverse sentences based on the
retrieved contexts. We conduct experiments on the DimonGen task and show that
MoREE outperforms strong baselines in terms of both the quality and diversity
of the generated sentences. Our results demonstrate that MoREE is able to
generate diverse sentences that reflect different relationships between
concepts, leading to a comprehensive understanding of concept relationships.",https://github.com/liuchenzhengyi/DimonGen,-1
"Attacking Face Recognition with T-shirts: Database, Vulnerability Assessment and Detection",0.0360243,"Face recognition systems are widely deployed for biometric authentication.
Despite this, it is well-known that, without any safeguards, face recognition
systems are highly vulnerable to presentation attacks. In response to this
security issue, several promising methods for detecting presentation attacks
have been proposed which show high performance on existing benchmarks. However,
an ongoing challenge is the generalization of presentation attack detection
methods to unseen and new attack types. To this end, we propose a new T-shirt
Face Presentation Attack (TFPA) database of 1,608 T-shirt attacks using 100
unique presentation attack instruments. In an extensive evaluation, we show
that this type of attack can compromise the security of face recognition
systems and that some state-of-the-art attack detection mechanisms trained on
popular benchmarks fail to robustly generalize to the new attacks. Further, we
propose three new methods for detecting T-shirt attack images, one which relies
on the statistical differences between depth maps of bona fide images and
T-shirt attacks, an anomaly detection approach trained on features only
extracted from bona fide RGB images, and a fusion approach which achieves
competitive detection performance.",None,-1
Korean-Specific Dataset for Table Question Answering,0.140139,"Existing question answering systems mainly focus on dealing with text data.
However, much of the data produced daily is stored in the form of tables that
can be found in documents and relational databases, or on the web. To solve the
task of question answering over tables, there exist many datasets for table
question answering written in English, but few Korean datasets. In this paper,
we demonstrate how we construct Korean-specific datasets for table question
answering: Korean tabular dataset is a collection of 1.4M tables with
corresponding descriptions for unsupervised pre-training language models.
Korean table question answering corpus consists of 70k pairs of questions and
answers created by crowd-sourced workers. Subsequently, we then build a
pre-trained language model based on Transformer and fine-tune the model for
table question answering with these datasets. We then report the evaluation
results of our model. We make our datasets publicly available via our GitHub
repository and hope that those datasets will help further studies for question
answering over tables, and for the transformation of table formats.",https://github.com/LG-NLP/KorWikiTableQuestions,-1
Multiple View Performers for Shape Completion,0.00556605,"We propose the Multiple View Performer (MVP) - a new architecture for 3D
shape completion from a series of temporally sequential views. MVP accomplishes
this task by using linear-attention Transformers called Performers. Our model
allows the current observation of the scene to attend to the previous ones for
more accurate infilling. The history of past observations is compressed via the
compact associative memory approximating modern continuous Hopfield memory, but
crucially of size independent from the history length. We compare our model
with several baselines for shape completion over time, demonstrating the
generalization gains that MVP provides. To the best of our knowledge, MVP is
the first multiple view voxel reconstruction method that does not require
registration of multiple depth views and the first causal Transformer based
model for 3D shape completion.",None,-1
SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images,0.0409459,"Airports have an important role in both military and civilian domains. The
synthetic aperture radar (SAR) based airport detection has received increasing
attention in recent years. However, due to the high cost of SAR imaging and
annotation process, there is no publicly available SAR dataset for airport
detection. As a result, deep learning methods have not been fully used in
airport detection tasks. To provide a benchmark for airport detection research
in SAR images, this paper introduces a large-scale SAR Airport Dataset (SAD).
In order to adequately reflect the demands of real world applications, it
contains 624 SAR images from Sentinel 1B and covers 104 airfield instances with
different scales, orientations and shapes. The experiments of multiple deep
learning approach on this dataset proves its effectiveness. It developing
state-of-the-art airport area detection algorithms or other relevant tasks.",https://github.com/searche?q=user%3ALongging+SAD,-1
Synonym Detection Using Syntactic Dependency And Neural Embeddings,0.00992617,"Recent advances on the Vector Space Model have significantly improved some
NLP applications such as neural machine translation and natural language
generation. Although word co-occurrences in context have been widely used in
counting-/predicting-based distributional models, the role of syntactic
dependencies in deriving distributional semantics has not yet been thoroughly
investigated. By comparing various Vector Space Models in detecting synonyms in
TOEFL, we systematically study the salience of syntactic dependencies in
accounting for distributional similarity. We separate syntactic dependencies
into different groups according to their various grammatical roles and then use
context-counting to construct their corresponding raw and SVD-compressed
matrices. Moreover, using the same training hyperparameters and corpora, we
study typical neural embeddings in the evaluation. We further study the
effectiveness of injecting human-compiled semantic knowledge into neural
embeddings on computing distributional similarity. Our results show that the
syntactically conditioned contexts can interpret lexical semantics better than
the unconditioned ones, whereas retrofitting neural embeddings with semantic
knowledge can significantly improve synonym detection.",None,-1
Improving Topic Segmentation by Injecting Discourse Dependencies,0.0786669,"Recent neural supervised topic segmentation models achieve distinguished
superior effectiveness over unsupervised methods, with the availability of
large-scale training corpora sampled from Wikipedia. These models may, however,
suffer from limited robustness and transferability caused by exploiting simple
linguistic cues for prediction, but overlooking more important inter-sentential
topical consistency. To address this issue, we present a discourse-aware neural
topic segmentation model with the injection of above-sentence discourse
dependency structures to encourage the model make topic boundary prediction
based more on the topical consistency between sentences. Our empirical study on
English evaluation datasets shows that injecting above-sentence discourse
structures to a neural topic segmenter with our proposed strategy can
substantially improve its performances on intra-domain and out-of-domain data,
with little increase of model's complexity.",None,-1
Real3D-Aug: Point Cloud Augmentation by Placing Real Objects with Occlusion Handling for 3D Detection and Segmentation,0.079046,"Object detection and semantic segmentation with the 3D lidar point cloud data
require expensive annotation. We propose a data augmentation method that takes
advantage of already annotated data multiple times. We propose an augmentation
framework that reuses real data, automatically finds suitable placements in the
scene to be augmented, and handles occlusions explicitly. Due to the usage of
the real data, the scan points of newly inserted objects in augmentation
sustain the physical characteristics of the lidar, such as intensity and
raydrop. The pipeline proves competitive in training top-performing models for
3D object detection and semantic segmentation. The new augmentation provides a
significant performance gain in rare and essential classes, notably 6.65%
average precision gain for ""Hard"" pedestrian class in KITTI object detection or
2.14 mean IoU gain in the SemanticKITTI segmentation challenge over the state
of the art.",https://github.com/ctu-vras/,-1
Featurized Query R-CNN,0.0817561,"The query mechanism introduced in the DETR method is changing the paradigm of
object detection and recently there are many query-based methods have obtained
strong object detection performance. However, the current query-based detection
pipelines suffer from the following two issues. Firstly, multi-stage decoders
are required to optimize the randomly initialized object queries, incurring a
large computation burden. Secondly, the queries are fixed after training,
leading to unsatisfying generalization capability. To remedy the above issues,
we present featurized object queries predicted by a query generation network in
the well-established Faster R-CNN framework and develop a Featurized Query
R-CNN. Extensive experiments on the COCO dataset show that our Featurized Query
R-CNN obtains the best speed-accuracy trade-off among all R-CNN detectors,
including the recent state-of-the-art Sparse R-CNN detector. The code is
available at {https://github.com/hustvl/Featurized-QueryRCNN.",https://github.com/hustvl/Featurized-QueryRCNN,-1
A Machine With Human-Like Memory Systems,0.00518037,"Inspired by the cognitive science theory, we explicitly model an agent with
both semantic and episodic memory systems, and show that it is better than
having just one of the two memory systems. In order to show this, we have
designed and released our own challenging environment, ""the Room"", compatible
with OpenAI Gym, where an agent has to properly learn how to encode, store, and
retrieve memories to maximize its rewards. The Room environment allows for a
hybrid intelligence setup where machines and humans can collaborate. We show
that two agents collaborating with each other results in better performance
than one agent acting alone. We have open-sourced our code and models at
https://github.com/tae898/explicit-memory.",https://github.com/tae898/explicit-memory,-1
AUTOLEX: An Automatic Framework for Linguistic Exploration,0.798104,"Each language has its own complex systems of word, phrase, and sentence
construction, the guiding principles of which are often summarized in grammar
descriptions for the consumption of linguists or language learners. However,
manual creation of such descriptions is a fraught process, as creating
descriptions which describe the language in ""its own terms"" without bias or
error requires both a deep understanding of the language at hand and
linguistics as a whole. We propose an automatic framework AutoLEX that aims to
ease linguists' discovery and extraction of concise descriptions of linguistic
phenomena. Specifically, we apply this framework to extract descriptions for
three phenomena: morphological agreement, case marking, and word order, across
several languages. We evaluate the descriptions with the help of language
experts and propose a method for automated evaluation when human evaluation is
infeasible.",https://github.com/Aditi138/auto-lex-learn/tree/master/code,-1
Unleashing the Power of Transformer for Graphs,0.0391515,"Despite recent successes in natural language processing and computer vision,
Transformer suffers from the scalability problem when dealing with graphs. The
computational complexity is unacceptable for large-scale graphs, e.g.,
knowledge graphs. One solution is to consider only the near neighbors, which,
however, will lose the key merit of Transformer to attend to the elements at
any distance. In this paper, we propose a new Transformer architecture, named
dual-encoding Transformer (DET). DET has a structural encoder to aggregate
information from connected neighbors and a semantic encoder to focus on
semantically useful distant nodes. In comparison with resorting to multi-hop
neighbors, DET seeks the desired distant neighbors via self-supervised
training. We further find these two encoders can be incorporated to boost each
others' performance. Our experiments demonstrate DET has achieved superior
performance compared to the respective state-of-the-art methods in dealing with
molecules, networks and knowledge graphs with various sizes.",None,-1
drsphelps at SemEval-2022 Task 2: Learning idiom representations using BERTRAM,0.0505911,"This paper describes our system for SemEval-2022 Task 2 Multilingual
Idiomaticity Detection and Sentence Embedding sub-task B. We modify a standard
BERT sentence transformer by adding embeddings for each idioms, which are
created using BERTRAM and a small number of contexts. We show that this
technique increases the quality of idiom representations and leads to better
performance on the task. We also perform analysis on our final results and show
that the quality of the produced idiom embeddings is highly sensitive to the
quality of the input contexts.",https://github.com/drsphelps/semeval-task-2,-1
Optimization of Directional Landmark Deployment for Visual Observer on SE(3),0.0140613,"An optimization method is proposed in this paper for novel deployment of
given number of directional landmarks (location and pose) within a given region
in the 3-D task space. This new deployment technique is built on the geometric
models of both landmarks and the monocular camera. In particular, a new concept
of Multiple Coverage Probability (MCP) is defined to characterize the
probability of at least n landmarks being covered simultaneously by a camera at
a fixed position. The optimization is conducted with respect to the position
and pose of the given number of landmarks to maximize MCP through globally
exploration of the given 3-D space. By adopting the elimination genetic
algorithm, the global optimal solutions can be obtained, which are then applied
to improve the convergent performance of the visual observer on SE(3) as a
demonstration example. Both simulation and experimental results are presented
to validate the effectiveness of the proposed landmark deployment optimization
method.",None,-1
Performance of different machine learning methods on activity recognition and pose estimation datasets,0.0031964,"With advancements in computer vision taking place day by day, recently a lot
of light is being shed on activity recognition. With the range for real-world
applications utilizing this field of study increasing across a multitude of
industries such as security and healthcare, it becomes crucial for businesses
to distinguish which machine learning methods perform better than others in the
area. This paper strives to aid in this predicament i.e. building upon previous
related work, it employs both classical and ensemble approaches on rich pose
estimation (OpenPose) and HAR datasets. Making use of appropriate metrics to
evaluate the performance for each model, the results show that overall, random
forest yields the highest accuracy in classifying ADLs. Relatively all the
models have excellent performance across both datasets, except for logistic
regression and AdaBoost perform poorly in the HAR one. With the limitations of
this paper also discussed in the end, the scope for further research is vast,
which can use this paper as a base in aims of producing better results.",None,-1
Data-driven Parsing Evaluation for Child-Parent Interactions,0.006907,"We present a syntactic dependency treebank for naturalistic child and
child-directed speech in English (MacWhinney, 2000). Our annotations largely
followed the guidelines of the Universal Dependencies project (UD (Zeman et
al., 2022)), with detailed extensions to lexical/syntactic structures unique to
conversational speech (in opposition to written texts). Compared to existing
UD-style spoken treebanks as well as other dependency corpora of child-parent
interactions specifically, our dataset is of (much) larger size (N of
utterances = 44,744; N of words = 233, 907) and contains speech from a total of
10 children covering a wide age range (18-66 months). With this dataset, we
ask: (1) How well would state-of-the-art dependency parsers, tailored for the
written domain, perform for speech of different interlocutors in spontaneous
conversations? (2) What is the relationship between parser performance and the
developmental stage of the child? To address these questions, in ongoing work,
we are conducting thorough dependency parser evaluations using both graph-based
and transition-based parsers with different hyperparameterization, trained from
three different types of out-of-domain written texts: news, tweets, and learner
data.",https://github.com/zoeyliu18/childes_parsing,-1
Contrastive Learning for Object Detection,0.00218767,"Contrastive learning is commonly used as a method of self-supervised learning
with the ""anchor"" and ""positive"" being two random augmentations of a given
input image, and the ""negative"" is the set of all other images. However, the
requirement of large batch sizes and memory banks has made it difficult and
slow to train. This has motivated the rise of Supervised Contrasative
approaches that overcome these problems by using annotated data. We look to
further improve supervised contrastive learning by ranking classes based on
their similarity, and observe the impact of human bias (in the form of ranking)
on the learned representations. We feel this is an important question to
address, as learning good feature embeddings has been a long sought after
problem in computer vision.",https://github.com/rishabbala/Contrastive_Learning_For_Object_Detection,-1
Instance-wise Prompt Tuning for Pretrained Language Models,0.193709,"Prompt Learning has recently gained great popularity in bridging the gap
between pretraining tasks and various downstream tasks. It freezes Pretrained
Language Models (PLMs) and only tunes a few task-related parameters (prompts)
for downstream tasks, greatly reducing the cost of tuning giant models. The key
enabler of this is the idea of querying PLMs with task-specific knowledge
implicated in prompts. This paper reveals a major limitation of existing
methods that the indiscriminate prompts for all input data in a task ignore the
intrinsic knowledge from input data, resulting in sub-optimal performance. We
introduce Instance-wise Prompt Tuning (IPT), the first prompt learning paradigm
that injects knowledge from the input data instances to the prompts, thereby
providing PLMs with richer and more concrete context information. We devise a
series of strategies to produce instance-wise prompts, addressing various
concerns like model quality and cost-efficiency. Across multiple tasks and
resource settings, IPT significantly outperforms task-based prompt learning
methods, and achieves comparable performance to conventional finetuning with
only 0.5% - 1.5% of tuned parameters.",None,-1
Towards Computing an Optimal Abstraction for Structural Causal Models,0.0,"Working with causal models at different levels of abstraction is an important
feature of science. Existing work has already considered the problem of
expressing formally the relation of abstraction between causal models. In this
paper, we focus on the problem of learning abstractions. We start by defining
the learning problem formally in terms of the optimization of a standard
measure of consistency. We then point out the limitation of this approach, and
we suggest extending the objective function with a term accounting for
information loss. We suggest a concrete measure of information loss, and we
illustrate its contribution to learning new abstractions.",https://github.com/FMZennaro/CategoricalCausalAbstraction/blob/main/P1%20-%20Motivating%20Example.ipynb,-1
A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning,0.0145814,"In recent years, spiking neural networks (SNNs) have been used in
reinforcement learning (RL) due to their low power consumption and event-driven
features. However, spiking reinforcement learning (SRL), which suffers from
fixed coding methods, still faces the problems of high latency and poor
versatility. In this paper, we use learnable matrix multiplication to encode
and decode spikes, improving the flexibility of the coders and thus reducing
latency. Meanwhile, we train the SNNs using the direct training method and use
two different structures for online and offline RL algorithms, which gives our
model a wider range of applications. Extensive experiments have revealed that
our method achieves optimal performance with ultra-low latency (as low as 0.8%
of other SRL methods) and excellent energy efficiency (up to 5X the DNNs) in
different algorithms and different environments.",None,-1
Unsupervised Image Representation Learning with Deep Latent Particles,0.0184537,"We propose a new representation of visual data that disentangles object
position from appearance. Our method, termed Deep Latent Particles (DLP),
decomposes the visual input into low-dimensional latent ``particles'', where
each particle is described by its spatial location and features of its
surrounding region. To drive learning of such representations, we follow a
VAE-based approach and introduce a prior for particle positions based on a
spatial-softmax architecture, and a modification of the evidence lower bound
loss inspired by the Chamfer distance between particles. We demonstrate that
our DLP representations are useful for downstream tasks such as unsupervised
keypoint (KP) detection, image manipulation, and video prediction for scenes
composed of multiple dynamic objects. In addition, we show that our
probabilistic interpretation of the problem naturally provides uncertainty
estimates for particle locations, which can be used for model selection, among
other tasks. Videos and code are available:
https://taldatech.github.io/deep-latent-particles-web/",https://github.com/taldatech/deep-latent-particles-pytorch,-1
Assessment of Massively Multilingual Sentiment Classifiers,0.0088405,"Models are increasing in size and complexity in the hunt for SOTA. But what
if those 2\% increase in performance does not make a difference in a production
use case? Maybe benefits from a smaller, faster model outweigh those slight
performance gains. Also, equally good performance across languages in
multilingual tasks is more important than SOTA results on a single one. We
present the biggest, unified, multilingual collection of sentiment analysis
datasets. We use these to assess 11 models and 80 high-quality sentiment
datasets (out of 342 raw datasets collected) in 27 languages and included
results on the internally annotated datasets. We deeply evaluate multiple
setups, including fine-tuning transformer-based models for measuring
performance. We compare results in numerous dimensions addressing the imbalance
in both languages coverage and dataset sizes. Finally, we present some best
practices for working with such a massive collection of datasets and models
from a multilingual perspective.",None,-1
An Ensemble Approach for Multiple Emotion Descriptors Estimation Using Multi-task Learning,0.00492264,"This paper illustrates our submission method to the fourth Affective Behavior
Analysis in-the-Wild (ABAW) Competition. The method is used for the Multi-Task
Learning Challenge. Instead of using only face information, we employ full
information from a provided dataset containing face and the context around the
face. We utilized the InceptionNet V3 model to extract deep features then we
applied the attention mechanism to refine the features. After that, we put
those features into the transformer block and multi-layer perceptron networks
to get the final multiple kinds of emotion. Our model predicts arousal and
valence, classifies the emotional expression and estimates the action units
simultaneously. The proposed system achieves the performance of 0.917 on the
MTL Challenge validation dataset.",https://github.com/tmtvaa/abaw4,-1
BOREx: Bayesian-Optimization--Based Refinement of Saliency Map for Image- and Video-Classification Models,0.864665,"Explaining a classification result produced by an image- and
video-classification model is one of the important but challenging issues in
computer vision. Many methods have been proposed for producing heat-map--based
explanations for this purpose, including ones based on the white-box approach
that uses the internal information of a model (e.g., LRP, Grad-CAM, and
Grad-CAM++) and ones based on the black-box approach that does not use any
internal information (e.g., LIME, SHAP, and RISE). We propose a new black-box
method BOREx (Bayesian Optimization for Refinement of visual model Explanation)
to refine a heat map produced by any method. Our observation is that a
heat-map--based explanation can be seen as a prior for an explanation method
based on Bayesian optimization. Based on this observation, BOREx conducts
Gaussian process regression (GPR) to estimate the saliency of each pixel in a
given image starting from the one produced by another explanation method. Our
experiments statistically demonstrate that the refinement by BOREx improves
low-quality heat maps for image- and video-classification results.",None,-1
Improved Consistency Training for Semi-Supervised Sequence-to-Sequence ASR via Speech Chain Reconstruction and Self-Transcribing,0.00701141,"Consistency regularization has recently been applied to semi-supervised
sequence-to-sequence (S2S) automatic speech recognition (ASR). This principle
encourages an ASR model to output similar predictions for the same input speech
with different perturbations. The existing paradigm of semi-supervised S2S ASR
utilizes SpecAugment as data augmentation and requires a static teacher model
to produce pseudo transcripts for untranscribed speech. However, this paradigm
fails to take full advantage of consistency regularization. First, the masking
operations of SpecAugment may damage the linguistic contents of the speech,
thus influencing the quality of pseudo labels. Second, S2S ASR requires both
input speech and prefix tokens to make the next prediction. The static prefix
tokens made by the offline teacher model cannot match dynamic pseudo labels
during consistency training. In this work, we propose an improved consistency
training paradigm of semi-supervised S2S ASR. We utilize speech chain
reconstruction as the weak augmentation to generate high-quality pseudo labels.
Moreover, we demonstrate that dynamic pseudo transcripts produced by the
student ASR model benefit the consistency training. Experiments on LJSpeech and
LibriSpeech corpora show that compared to supervised baselines, our improved
paradigm achieves a 12.2% CER improvement in the single-speaker setting and
38.6% in the multi-speaker setting.",None,-1
Probing Causes of Hallucinations in Neural Machine Translations,0.066356,"Hallucination, one kind of pathological translations that bothers Neural
Machine Translation, has recently drawn much attention. In simple terms,
hallucinated translations are fluent sentences but barely related to source
inputs. Arguably, it remains an open problem how hallucination occurs. In this
paper, we propose to use probing methods to investigate the causes of
hallucinations from the perspective of model architecture, aiming to avoid such
problems in future architecture designs. By conducting experiments over various
NMT datasets, we find that hallucination is often accompanied by the deficient
encoder, especially embeddings, and vulnerable cross-attentions, while,
interestingly, cross-attention mitigates some errors caused by the encoder.",None,-1
DPCL: a Language Template for Normative Specifications,0.007183,"Several solutions for specifying normative artefacts (norms, contracts,
policies) in a computational processable way have been presented in the
literature. Legal core ontologies have been proposed to systematize concepts
and relationships relevant to normative reasoning. However, no solution amongst
those has achieved general acceptance, and no common ground (representational,
computational) has been identified enabling us to easily compare them. Yet, all
these efforts share the same motivation of representing normative directives,
therefore it is plausible that there may be a representational model
encompassing all of them. This presentation will introduce DPCL, a
domain-specific language (DSL) for specifying higher-level policies (including
norms, contracts, etc.), centred on Hohfeld's framework of fundamental legal
concepts. DPCL has to be seen primarily as a ""template"", i.e. as an
informational model for architectural reference, rather than a fully-fledged
formal language; it aims to make explicit the general requirements that should
be expected in a language for norm specification. In this respect, it goes
rather in the direction of legal core ontologies, but differently from those,
our proposal aims to keep the character of a DSL, rather than a set of axioms
in a logical framework: it is meant to be cross-compiled to underlying
languages/tools adequate to the type of target application. We provide here an
overview of some of the language features.",None,-1
Chasing Streams with Existential Rules,0.0470145,"We study reasoning with existential rules to perform query answering over
streams of data. On static databases, this problem has been widely studied, but
its extension to rapidly changing data has not yet been considered. To bridge
this gap, we extend LARS, a well-known framework for rule-based stream
reasoning, to support existential rules. For that, we show how to translate
LARS with existentials into a semantics-preserving set of existential rules. As
query answering with such rules is undecidable in general, we describe how to
leverage the temporal nature of streams and present suitable notions of
acyclicity that ensure decidability.",https://github.com/karmaresearch/elars,-1
Iterative Learning for Instance Segmentation,0.00252473,"Instance segmentation is a computer vision task where separate objects in an
image are detected and segmented. State-of-the-art deep neural network models
require large amounts of labeled data in order to perform well in this task.
Making these annotations is time-consuming. We propose for the first time, an
iterative learning and annotation method that is able to detect, segment and
annotate instances in datasets composed of multiple similar objects. The
approach requires minimal human intervention and needs only a bootstrapping set
containing very few annotations. Experiments on two different datasets show the
validity of the approach in different applications related to visual
inspection.",https://github.com/Tony607/mmdetection_instance_segmentation_demo,-1
Mask-Guided Image Person Removal with Data Synthesis,0.0108284,"As a special case of common object removal, image person removal is playing
an increasingly important role in social media and criminal investigation
domains. Due to the integrity of person area and the complexity of human
posture, person removal has its own dilemmas. In this paper, we propose a novel
idea to tackle these problems from the perspective of data synthesis.
Concerning the lack of dedicated dataset for image person removal, two dataset
production methods are proposed to automatically generate images, masks and
ground truths respectively. Then, a learning framework similar to local image
degradation is proposed so that the masks can be used to guide the feature
extraction process and more texture information can be gathered for final
prediction. A coarse-to-fine training strategy is further applied to refine the
details. The data synthesis and learning framework combine well with each
other. Experimental results verify the effectiveness of our method
quantitatively and qualitatively, and the trained network proves to have good
generalization ability either on real or synthetic images.",None,-1
Semantics-Guided Moving Object Segmentation with 3D LiDAR,0.0622238,"Moving object segmentation (MOS) is a task to distinguish moving objects,
e.g., moving vehicles and pedestrians, from the surrounding static environment.
The segmentation accuracy of MOS can have an influence on odometry, map
construction, and planning tasks. In this paper, we propose a semantics-guided
convolutional neural network for moving object segmentation. The network takes
sequential LiDAR range images as inputs. Instead of segmenting the moving
objects directly, the network conducts single-scan-based semantic segmentation
and multiple-scan-based moving object segmentation in turn. The semantic
segmentation module provides semantic priors for the MOS module, where we
propose an adjacent scan association (ASA) module to convert the semantic
features of adjacent scans into the same coordinate system to fully exploit the
cross-scan semantic features. Finally, by analyzing the difference between the
transformed features, reliable MOS result can be obtained quickly. Experimental
results on the SemanticKITTI MOS dataset proves the effectiveness of our work.",None,-1
Computationally efficient joint coordination of multiple electric vehicle charging points using reinforcement learning,0.0180575,"A major challenge in todays power grid is to manage the increasing load from
electric vehicle (EV) charging. Demand response (DR) solutions aim to exploit
flexibility therein, i.e., the ability to shift EV charging in time and thus
avoid excessive peaks or achieve better balancing. Whereas the majority of
existing research works either focus on control strategies for a single EV
charger, or use a multi-step approach (e.g., a first high level aggregate
control decision step, followed by individual EV control decisions), we rather
propose a single-step solution that jointly coordinates multiple charging
points at once. In this paper, we further refine an initial proposal using
reinforcement learning (RL), specifically addressing computational challenges
that would limit its deployment in practice. More precisely, we design a new
Markov decision process (MDP) formulation of the EV charging coordination
process, exhibiting only linear space and time complexity (as opposed to the
earlier quadratic space complexity). We thus improve upon earlier
state-of-the-art, demonstrating 30% reduction of training time in our case
study using real-world EV charging session data. Yet, we do not sacrifice the
resulting performance in meeting the DR objectives: our new RL solutions still
improve the performance of charging demand coordination by 40-50% compared to a
business-as-usual policy (that charges EV fully upon arrival) and 20-30%
compared to a heuristic policy (that uniformly spreads individual EV charging
over time).",None,-1
Improving Multi-generation Robustness of Learned Image Compression,0.00553912,"Benefit from flexible network designs and end-to-end joint optimization
approach, learned image compression (LIC) has demonstrated excellent coding
performance and practical feasibility in recent years. However, existing
compression models suffer from serious multi-generation loss, which always
occurs during image editing and transcoding. During the process of repeatedly
encoding and decoding, the quality of the image will rapidly degrade, resulting
in various types of distortion, which significantly limits the practical
application of LIC. In this paper, a thorough analysis is carried out to
determine the source of generative loss in successive image compression (SIC).
We point out and solve the quantization drift problem that affects SIC,
reversibility loss function as well as channel relaxation method are proposed
to further reduce the generation loss. Experiments show that by using our
proposed solutions, LIC can achieve comparable performance to the first
compression of BPG even after 50 times reencoding without any change of the
network structure.",http://github.com/tensorﬂow/compression,-1
SG-Shuffle: Multi-aspect Shuffle Transformer for Scene Graph Generation,0.0269758,"Scene Graph Generation (SGG) serves a comprehensive representation of the
images for human understanding as well as visual understanding tasks. Due to
the long tail bias problem of the object and predicate labels in the available
annotated data, the scene graph generated from current methodologies can be
biased toward common, non-informative relationship labels. Relationship can
sometimes be non-mutually exclusive, which can be described from multiple
perspectives like geometrical relationships or semantic relationships, making
it even more challenging to predict the most suitable relationship label. In
this work, we proposed the SG-Shuffle pipeline for scene graph generation with
3 components: 1) Parallel Transformer Encoder, which learns to predict object
relationships in a more exclusive manner by grouping relationship labels into
groups of similar purpose; 2) Shuffle Transformer, which learns to select the
final relationship labels from the category-specific feature generated in the
previous step; and 3) Weighted CE loss, used to alleviate the training bias
caused by the imbalanced dataset.",None,-1
Revisiting Attention Weights as Explanations from an Information Theoretic Perspective,0.00412061,"Attention mechanisms have recently demonstrated impressive performance on a
range of NLP tasks, and attention scores are often used as a proxy for model
explainability. However, there is a debate on whether attention weights can, in
fact, be used to identify the most important inputs to a model. We approach
this question from an information theoretic perspective by measuring the mutual
information between the model output and the hidden states. From extensive
experiments, we draw the following conclusions: (i) Additive and Deep attention
mechanisms are likely to be better at preserving the information between the
hidden states and the model output (compared to Scaled Dot-product); (ii)
ablation studies indicate that Additive attention can actively learn to explain
the importance of its input hidden representations; (iii) when attention values
are nearly the same, the rank order of attention values is not consistent with
the rank order of the mutual information(iv) Using Gumbel-Softmax with a
temperature lower than one, tends to produce a more skewed attention score
distribution compared to softmax and hence is a better choice for explainable
design; (v) some building blocks are better at preserving the correlation
between the ordered list of mutual information and attention weights order (for
e.g., the combination of BiLSTM encoder and Additive attention). Our findings
indicate that attention mechanisms do have the potential to function as a
shortcut to model explanations when they are carefully combined with other
model elements.",https://github.com/successar/AttentionExplanation,-1
Do Children Texts Hold The Key To Commonsense Knowledge?,0.014281,"Compiling comprehensive repositories of commonsense knowledge is a
long-standing problem in AI. Many concerns revolve around the issue of
reporting bias, i.e., that frequency in text sources is not a good proxy for
relevance or truth. This paper explores whether children's texts hold the key
to commonsense knowledge compilation, based on the hypothesis that such content
makes fewer assumptions on the reader's knowledge, and therefore spells out
commonsense more explicitly. An analysis with several corpora shows that
children's texts indeed contain much more, and more typical commonsense
assertions. Moreover, experiments show that this advantage can be leveraged in
popular language-model-based commonsense knowledge extraction settings, where
task-unspecific fine-tuning on small amounts of children texts (childBERT)
already yields significant improvements. This provides a refreshing perspective
different from the common trend of deriving progress from ever larger models
and corpora.",https://www.mpi-inf.mpg.de/children-texts-for-commonsense,-1
Eye Gaze Estimation Model Analysis,0.0315869,"We explore techniques for eye gaze estimation using machine learning. Eye
gaze estimation is a common problem for various behavior analysis and
human-computer interfaces. The purpose of this work is to discuss various model
types for eye gaze estimation and present the results from predicting gaze
direction using eye landmarks in unconstrained settings. In unconstrained
real-world settings, feature-based and model-based methods are outperformed by
recent appearance-based methods due to factors like illumination changes and
other visual artifacts. We discuss a learning-based method for eye region
landmark localization trained exclusively on synthetic data. We discuss how to
use detected landmarks as input to iterative model-fitting and lightweight
learning-based gaze estimation methods and how to use the model for
person-independent and personalized gaze estimations.",https://github.com/aveenakottwani/EyeGazeEstimationModels,-1
Linguistic Elements of Engaging Customer Service Discourse on Social Media,0.0296371,"Customers are rapidly turning to social media for customer support. While
brand agents on these platforms are motivated and well-intentioned to help and
engage with customers, their efforts are often ignored if their initial
response to the customer does not match a specific tone, style, or topic the
customer is aiming to receive. The length of a conversation can reflect the
effort and quality of the initial response made by a brand toward collaborating
and helping consumers, even when the overall sentiment of the conversation
might not be very positive. Thus, through this study, we aim to bridge this
critical gap in the existing literature by analyzing language's content and
stylistic aspects such as expressed empathy, psycho-linguistic features,
dialogue tags, and metrics for quantifying personalization of the utterances
that can influence the engagement of an interaction. This paper demonstrates
that we can predict engagement using initial customer and brand posts.",None,-1
Anytime-Lidar: Deadline-aware 3D Object Detection,0.0414184,"In this work, we present a novel scheduling framework enabling anytime
perception for deep neural network (DNN) based 3D object detection pipelines.
We focus on computationally expensive region proposal network (RPN) and
per-category multi-head detector components, which are common in 3D object
detection pipelines, and make them deadline-aware. We propose a scheduling
algorithm, which intelligently selects the subset of the components to make
effective time and accuracy trade-off on the fly. We minimize accuracy loss of
skipping some of the neural network sub-components by projecting previously
detected objects onto the current scene through estimations. We apply our
approach to a state-of-art 3D object detection network, PointPillars, and
evaluate its performance on Jetson Xavier AGX using nuScenes dataset. Compared
to the baselines, our approach significantly improve the network's accuracy
under various deadline constraints.",https://github.com/open-mmlab/OpenPCDet,-1
Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study,0.0868635,"Neural models that do not rely on pre-training have excelled in the keyphrase
generation task with large annotated datasets. Meanwhile, new approaches have
incorporated pre-trained language models (PLMs) for their data efficiency.
However, there lacks a systematic study of how the two types of approaches
compare and how different design choices can affect the performance of
PLM-based models. To fill in this knowledge gap and facilitate a more informed
use of PLMs for keyphrase extraction and keyphrase generation, we present an
in-depth empirical study. Formulating keyphrase extraction as sequence labeling
and keyphrase generation as sequence-to-sequence generation, we perform
extensive experiments in three domains. After showing that PLMs have
competitive high-resource performance and state-of-the-art low-resource
performance, we investigate important design choices including in-domain PLMs,
PLMs with different pre-training objectives, using PLMs with a parameter
budget, and different formulations for present keyphrases. Further results show
that (1) in-domain BERT-like PLMs can be used to build strong and
data-efficient keyphrase generation models; (2) with a fixed parameter budget,
prioritizing model depth over width and allocating more layers in the encoder
leads to better encoder-decoder models; and (3) introducing four in-domain
PLMs, we achieve a competitive performance in the news domain and the
state-of-the-art performance in the scientific domain.",https://github.com/uclanlp/DeepKPG,-1
Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on Synthetic Images,0.0969228,"Most recent style-transfer techniques based on generative architectures are
able to obtain synthetic multimedia contents, or commonly called deepfakes,
with almost no artifacts. Researchers already demonstrated that synthetic
images contain patterns that can determine not only if it is a deepfake but
also the generative architecture employed to create the image data itself.
These traces can be exploited to study problems that have never been addressed
in the context of deepfakes. To this aim, in this paper a first approach to
investigate the image ballistics on deepfake images subject to style-transfer
manipulations is proposed. Specifically, this paper describes a study on
detecting how many times a digital image has been processed by a generative
architecture for style transfer. Moreover, in order to address and study
accurately forensic ballistics on deepfake images, some mathematical properties
of style-transfer operations were investigated.",https://github.com/deepfakes/faceswap,-1
Long Scale Error Control in Low Light Image and Video Enhancement Using Equivariance,0.0280266,"Image frames obtained in darkness are special. Just multiplying by a constant
doesn't restore the image. Shot noise, quantization effects and camera
non-linearities mean that colors and relative light levels are estimated
poorly. Current methods learn a mapping using real dark-bright image pairs.
These are very hard to capture. A recent paper has shown that simulated data
pairs produce real improvements in restoration, likely because huge volumes of
simulated data are easy to obtain. In this paper, we show that respecting
equivariance -- the color of a restored pixel should be the same, however the
image is cropped -- produces real improvements over the state of the art for
restoration. We show that a scale selection mechanism can be used to improve
reconstructions. Finally, we show that our approach produces improvements on
video restoration as well. Our methods are evaluated both quantitatively and
qualitatively.",None,-1
Object Goal Navigation Based on Semantics and RGB Ego View,0.0147968,"This paper presents an architecture and methodology to empower a service
robot to navigate an indoor environment with semantic decision making, given
RGB ego view. This method leverages the knowledge of robot's actuation
capability and that of scenes, objects and their relations -- represented in a
semantic form. The robot navigates based on GeoSem map - a relational
combination of geometric and semantic map. The goal given to the robot is to
find an object in a unknown environment with no navigational map and only
egocentric RGB camera perception. The approach is tested both on a simulation
environment and real life indoor settings. The presented approach was found to
outperform human users in gamified evaluations with respect to average
completion time.",None,-1
Deep Learning Hyperparameter Optimization for Breast Mass Detection in Mammograms,0.00523614,"Accurate breast cancer diagnosis through mammography has the potential to
save millions of lives around the world. Deep learning (DL) methods have shown
to be very effective for mass detection in mammograms. Additional improvements
of current DL models will further improve the effectiveness of these methods. A
critical issue in this context is how to pick the right hyperparameters for DL
models. In this paper, we present GA-E2E, a new approach for tuning the
hyperparameters of DL models for brest cancer detection using Genetic
Algorithms (GAs). Our findings reveal that differences in parameter values can
considerably alter the area under the curve (AUC), which is used to determine a
classifier's performance.",None,-1
Fairly Accurate: Learning Optimal Accuracy vs. Fairness Tradeoffs for Hate Speech Detection,0.0519239,"Recent work has emphasized the importance of balancing competing objectives
in model training (e.g., accuracy vs. fairness, or competing measures of
fairness). Such trade-offs reflect a broader class of multi-objective
optimization (MOO) problems in which optimization methods seek Pareto optimal
trade-offs between competing goals. In this work, we first introduce a
differentiable measure that enables direct optimization of group fairness
(specifically, balancing accuracy across groups) in model training. Next, we
demonstrate two model-agnostic MOO frameworks for learning Pareto optimal
parameterizations over different groups of neural classification models. We
evaluate our methods on the specific task of hate speech detection, in which
prior work has shown lack of group fairness across speakers of different
English dialects. Empirical results across convolutional, sequential, and
transformer-based neural architectures show superior empirical accuracy vs.
fairness trade-offs over prior work. More significantly, our measure enables
the Pareto machinery to ensure that each architecture achieves the best
possible trade-off between fairness and accuracy w.r.t. the dataset, given
user-prescribed error tolerance bounds.",None,-1
"Artificial Disfluency Detection, Uh No, Disfluency Generation for the Masses",0.0299316,"Existing approaches for disfluency detection typically require the existence
of large annotated datasets. However, current datasets for this task are
limited, suffer from class imbalance, and lack some types of disfluencies that
can be encountered in real-world scenarios. This work proposes LARD, a method
for automatically generating artificial disfluencies from fluent text. LARD can
simulate all the different types of disfluencies (repetitions, replacements and
restarts) based on the reparandum/interregnum annotation scheme. In addition,
it incorporates contextual embeddings into the disfluency generation to produce
realistic context-aware artificial disfluencies. Since the proposed method
requires only fluent text, it can be used directly for training, bypassing the
requirement of annotated disfluent data. Our empirical evaluation demonstrates
that LARD can indeed be effectively used when no or only a few data are
available. Furthermore, our detailed analysis suggests that the proposed method
generates realistic disfluencies and increases the accuracy of existing
disfluency detectors.",None,-1
Aggregated Text Transformer for Scene Text Detection,0.00372418,"This paper explores the multi-scale aggregation strategy for scene text
detection in natural images. We present the Aggregated Text TRansformer(ATTR),
which is designed to represent texts in scene images with a multi-scale
self-attention mechanism. Starting from the image pyramid with multiple
resolutions, the features are first extracted at different scales with shared
weight and then fed into an encoder-decoder architecture of Transformer. The
multi-scale image representations are robust and contain rich information on
text contents of various sizes. The text Transformer aggregates these features
to learn the interaction across different scales and improve text
representation. The proposed method detects scene texts by representing each
text instance as an individual binary mask, which is tolerant of curve texts
and regions with dense instances. Extensive experiments on public scene text
detection datasets demonstrate the effectiveness of the proposed framework.",None,-1
Harnessing Multilingual Resources to Question Answering in Arabic,0.0205627,"The goal of the paper is to predict answers to questions given a passage of
Qur'an. The answers are always found in the passage, so the task of the model
is to predict where an answer starts and where it ends. As the initial data set
is rather small for training, we make use of multilingual BERT so that we can
augment the training data by using data available for languages other than
Arabic. Furthermore, we crawl a large Arabic corpus that is domain specific to
religious discourse. Our approach consists of two steps, first we train a BERT
model to predict a set of possible answers in a passage. Finally, we use
another BERT based model to rank the candidate answers produced by the first
BERT model.",https://github.com/aliftype/quran-data,-1
DeepCuts: Single-Shot Interpretability based Pruning for BERT,0.0,"As language models have grown in parameters and layers, it has become much
harder to train and infer with them on single GPUs. This is severely
restricting the availability of large language models such as GPT-3,
BERT-Large, and many others. A common technique to solve this problem is
pruning the network architecture by removing transformer heads, fully-connected
weights, and other modules. The main challenge is to discern the important
parameters from the less important ones. Our goal is to find strong metrics for
identifying such parameters. We thus propose two strategies: Cam-Cut based on
the GradCAM interpretations, and Smooth-Cut based on the SmoothGrad, for
calculating the importance scores. Through this work, we show that our scoring
functions are able to assign more relevant task-based scores to the network
parameters, and thus both our pruning approaches significantly outperform the
standard weight and gradient-based strategies, especially at higher compression
ratios in BERT-based models. We also analyze our pruning masks and find them to
be significantly different from the ones obtained using standard metrics.",https://github.com/RuskinMan/DeepCuts,-1
FreeTransfer-X: Safe and Label-Free Cross-Lingual Transfer from Off-the-Shelf Models,0.0231278,"Cross-lingual transfer (CLT) is of various applications. However, labeled
cross-lingual corpus is expensive or even inaccessible, especially in the
fields where labels are private, such as diagnostic results of symptoms in
medicine and user profiles in business. Nevertheless, there are off-the-shelf
models in these sensitive fields. Instead of pursuing the original labels, a
workaround for CLT is to transfer knowledge from the off-the-shelf models
without labels. To this end, we define a novel CLT problem named FreeTransfer-X
that aims to achieve knowledge transfer from the off-the-shelf models in
rich-resource languages. To address the problem, we propose a 2-step knowledge
distillation (KD, Hinton et al., 2015) framework based on multilingual
pre-trained language models (mPLM). The significant improvement over strong
neural machine translation (NMT) baselines demonstrates the effectiveness of
the proposed method. In addition to reducing annotation cost and protecting
private labels, the proposed method is compatible with different networks and
easy to be deployed. Finally, a range of analyses indicate the great potential
of the proposed method.",https://github.com/huawei-noah/noah-research/tree/master/NLP/FreeTransfer-X,-1
A Unified Image Preprocessing Framework For Image Compression,0.0145753,"With the development of streaming media technology, increasing communication
relies on sound and visual information, which puts a massive burden on online
media. Data compression becomes increasingly important to reduce the volume of
data transmission and storage. To further improve the efficiency of image
compression, researchers utilize various image processing methods to compensate
for the limitations of conventional codecs and advanced learning-based
compression methods. Instead of modifying the image compression oriented
approaches, we propose a unified image compression preprocessing framework,
called Kuchen, which aims to further improve the performance of existing
codecs. The framework consists of a hybrid data labeling system along with a
learning-based backbone to simulate personalized preprocessing. As far as we
know, this is the first exploration of setting a unified preprocessing
benchmark in image compression tasks. Results demonstrate that the modern
codecs optimized by our unified preprocessing framework constantly improve the
efficiency of the state-of-the-art compression.",None,-1
A Machine Learning Approach for DeepFake Detection,0.0,"With the spread of DeepFake techniques, this technology has become quite
accessible and good enough that there is concern about its malicious use. Faced
with this problem, detecting forged faces is of utmost importance to ensure
security and avoid socio-political problems, both on a global and private
scale. This paper presents a solution for the detection of DeepFakes using
convolution neural networks and a dataset developed for this purpose -
Celeb-DF. The results show that, with an overall accuracy of 95% in the
classification of these images, the proposed model is close to what exists in
the state of the art with the possibility of adjustment for better results in
the manipulation techniques that arise in the future.",None,-1
Hierarchical Graph Structures for Congestion and ETA Prediction,0.00631613,"Traffic4cast is an annual competition to predict spatio temporal traffic
based on real world data. We propose an approach using Graph Neural Networks
that directly works on the road graph topology which was extracted from
OpenStreetMap data. Our architecture can incorporate a hierarchical graph
representation to improve the information flow between key intersections of the
graph and the shortest paths connecting them. Furthermore, we investigate how
the road graph can be compacted to ease the flow of information and make use of
a multi-task approach to predict congestion classes and ETA simultaneously. Our
code and models are released here:
https://github.com/floriangroetschla/NeurIPS2022-traffic4cast",https://github.com/floriangroetschla/NeurIPS2022-traffic4cast,-1
Confidence Calibration for Intent Detection via Hyperspherical Space and Rebalanced Accuracy-Uncertainty Loss,0.0216082,"Data-driven methods have achieved notable performance on intent detection,
which is a task to comprehend user queries. Nonetheless, they are controversial
for over-confident predictions. In some scenarios, users do not only care about
the accuracy but also the confidence of model. Unfortunately, mainstream neural
networks are poorly calibrated, with a large gap between accuracy and
confidence. To handle this problem defined as confidence calibration, we
propose a model using the hyperspherical space and rebalanced
accuracy-uncertainty loss. Specifically, we project the label vector onto
hyperspherical space uniformly to generate a dense label representation matrix,
which mitigates over-confident predictions due to overfitting sparce one-hot
label matrix. Besides, we rebalance samples of different accuracy and
uncertainty to better guide model training. Experiments on the open datasets
verify that our model outperforms the existing calibration methods and achieves
a significant improvement on the calibration metric.",None,-1
Efficient Speech Translation with Pre-trained Models,0.0114363,"When building state-of-the-art speech translation models, the need for large
computational resources is a significant obstacle due to the large training
data size and complex models. The availability of pre-trained models is a
promising opportunity to build strong speech translation systems efficiently.
In a first step, we investigate efficient strategies to build cascaded and
end-to-end speech translation systems based on pre-trained models. Using this
strategy, we can train and apply the models on a single GPU. While the
end-to-end models show superior translation performance to cascaded ones, the
application of this technology has a limitation on the need for additional
end-to-end training data. In a second step, we proposed an additional
similarity loss to encourage the model to generate similar hidden
representations for speech and transcript. Using this technique, we can
increase the data efficiency and improve the translation quality by 6 BLEU
points in scenarios with limited end-to-end training data.",None,-1
Artificial Intelligence in Concrete Materials: A Scientometric View,0.0117156,"Artificial intelligence (AI) has emerged as a transformative and versatile
tool, breaking new frontiers across scientific domains. Among its most
promising applications, AI research is blossoming in concrete science and
engineering, where it has offered new insights towards mixture design
optimization and service life prediction of cementitious systems. This chapter
aims to uncover the main research interests and knowledge structure of the
existing literature on AI for concrete materials. To begin with, a total of 389
journal articles published from 1990 to 2020 were retrieved from the Web of
Science. Scientometric tools such as keyword co-occurrence analysis and
documentation co-citation analysis were adopted to quantify features and
characteristics of the research field. The findings bring to light pressing
questions in data-driven concrete research and suggest future opportunities for
the concrete community to fully utilize the capabilities of AI techniques.",None,-1
Collaborative Propagation on Multiple Instance Graphs for 3D Instance Segmentation with Single-point Supervision,0.0233885,"Instance segmentation on 3D point clouds has been attracting increasing
attention due to its wide applications, especially in scene understanding
areas. However, most existing methods operate on fully annotated data while
manually preparing ground-truth labels at point-level is very cumbersome and
labor-intensive. To address this issue, we propose a novel weakly supervised
method RWSeg that only requires labeling one object with one point. With these
sparse weak labels, we introduce a unified framework with two branches to
propagate semantic and instance information respectively to unknown regions
using self-attention and a cross-graph random walk method. Specifically, we
propose a Cross-graph Competing Random Walks (CRW) algorithm that encourages
competition among different instance graphs to resolve ambiguities in closely
placed objects, improving instance assignment accuracy. RWSeg generates
high-quality instance-level pseudo labels. Experimental results on ScanNet-v2
and S3DIS datasets show that our approach achieves comparable performance with
fully-supervised methods and outperforms previous weakly-supervised methods by
a substantial margin.",None,-1
Language Model Pre-training on True Negatives,0.0201649,"Discriminative pre-trained language models (PLMs) learn to predict original
texts from intentionally corrupted ones. Taking the former text as positive and
the latter as negative samples, the PLM can be trained effectively for
contextualized representation. However, the training of such a type of PLMs
highly relies on the quality of the automatically constructed samples. Existing
PLMs simply treat all corrupted texts as equal negative without any
examination, which actually lets the resulting model inevitably suffer from the
false negative issue where training is carried out on pseudo-negative data and
leads to less efficiency and less robustness in the resulting PLMs. In this
work, on the basis of defining the false negative issue in discriminative PLMs
that has been ignored for a long time, we design enhanced pre-training methods
to counteract false negative predictions and encourage pre-training language
models on true negatives by correcting the harmful gradient updates subject to
false negative predictions. Experimental results on GLUE and SQuAD benchmarks
show that our counter-false-negative pre-training methods indeed bring about
better performance together with stronger robustness.",None,-1
Domain Mismatch Doesn't Always Prevent Cross-Lingual Transfer Learning,0.0231278,"Cross-lingual transfer learning without labeled target language data or
parallel text has been surprisingly effective in zero-shot cross-lingual
classification, question answering, unsupervised machine translation, etc.
However, some recent publications have claimed that domain mismatch prevents
cross-lingual transfer, and their results show that unsupervised bilingual
lexicon induction (UBLI) and unsupervised neural machine translation (UNMT) do
not work well when the underlying monolingual corpora come from different
domains (e.g., French text from Wikipedia but English text from UN
proceedings). In this work, we show that a simple initialization regimen can
overcome much of the effect of domain mismatch in cross-lingual transfer. We
pre-train word and contextual embeddings on the concatenated domain-mismatched
corpora, and use these as initializations for three tasks: MUSE UBLI, UN
Parallel UNMT, and the SemEval 2017 cross-lingual word similarity task. In all
cases, our results challenge the conclusions of prior work by showing that
proper initialization can recover a large portion of the losses incurred by
domain mismatch.",https://github.com/facebookresearch/XLM,-1
Assistive Recipe Editing through Critiquing,0.0620161,"There has recently been growing interest in the automatic generation of
cooking recipes that satisfy some form of dietary restrictions, thanks in part
to the availability of online recipe data. Prior studies have used pre-trained
language models, or relied on small paired recipe data (e.g., a recipe paired
with a similar one that satisfies a dietary constraint). However, pre-trained
language models generate inconsistent or incoherent recipes, and paired
datasets are not available at scale. We address these deficiencies with
RecipeCrit, a hierarchical denoising auto-encoder that edits recipes given
ingredient-level critiques. The model is trained for recipe completion to learn
semantic relationships within recipes. Our work's main innovation is our
unsupervised critiquing module that allows users to edit recipes by interacting
with the predicted ingredients; the system iteratively rewrites recipes to
satisfy users' feedback. Experiments on the Recipe1M recipe dataset show that
our model can more effectively edit recipes compared to strong
language-modeling baselines, creating recipes that satisfy user constraints and
are more correct, serendipitous, coherent, and relevant as measured by human
judges.",None,-1
Learning energy-efficient driving behaviors by imitating experts,0.0186687,"The rise of vehicle automation has generated significant interest in the
potential role of future automated vehicles (AVs). In particular, in highly
dense traffic settings, AVs are expected to serve as congestion-dampeners,
mitigating the presence of instabilities that arise from various sources.
However, in many applications, such maneuvers rely heavily on non-local sensing
or coordination by interacting AVs, thereby rendering their adaptation to
real-world settings a particularly difficult challenge. To address this
challenge, this paper examines the role of imitation learning in bridging the
gap between such control strategies and realistic limitations in communication
and sensing. Treating one such controller as an ""expert"", we demonstrate that
imitation learning can succeed in deriving policies that, if adopted by 5% of
vehicles, may boost the energy-efficiency of networks with varying traffic
conditions by 15% using only local observations. Results and code are available
online at https://sites.google.com/view/il-traffic/home.",https://sites.google.com/view/il-traffic/home,-1
Using EBGAN for Anomaly Intrusion Detection,0.0142173,"As an active network security protection scheme, intrusion detection system
(IDS) undertakes the important responsibility of detecting network attacks in
the form of malicious network traffic. Intrusion detection technology is an
important part of IDS. At present, many scholars have carried out extensive
research on intrusion detection technology. However, developing an efficient
intrusion detection method for massive network traffic data is still difficult.
Since Generative Adversarial Networks (GANs) have powerful modeling
capabilities for complex high-dimensional data, they provide new ideas for
addressing this problem. In this paper, we put forward an EBGAN-based intrusion
detection method, IDS-EBGAN, that classifies network records as normal traffic
or malicious traffic. The generator in IDS-EBGAN is responsible for converting
the original malicious network traffic in the training set into adversarial
malicious examples. This is because we want to use adversarial learning to
improve the ability of discriminator to detect malicious traffic. At the same
time, the discriminator adopts Autoencoder model. During testing, IDS-EBGAN
uses reconstruction error of discriminator to classify traffic records.",None,-1
Multi-Modal and Multi-Factor Branching Time Active Inference,0.0355227,"Active inference is a state-of-the-art framework for modelling the brain that
explains a wide range of mechanisms such as habit formation, dopaminergic
discharge and curiosity. Recently, two versions of branching time active
inference (BTAI) based on Monte-Carlo tree search have been developed to handle
the exponential (space and time) complexity class that occurs when computing
the prior over all possible policies up to the time horizon. However, those two
versions of BTAI still suffer from an exponential complexity class w.r.t the
number of observed and latent variables being modelled. In the present paper,
we resolve this limitation by first allowing the modelling of several
observations, each of them having its own likelihood mapping. Similarly, we
allow each latent state to have its own transition mapping. The inference
algorithm then exploits the factorisation of the likelihood and transition
mappings to accelerate the computation of the posterior. Those two
optimisations were tested on the dSprites environment in which the metadata of
the dSprites dataset was used as input to the model instead of the dSprites
images. On this task, $BTAI_{VMP}$ (Champion et al., 2022b,a) was able to solve
96.9\% of the task in 5.1 seconds, and $BTAI_{BF}$ (Champion et al., 2021a) was
able to solve 98.6\% of the task in 17.5 seconds. Our new approach
($BTAI_{3MF}$) outperformed both of its predecessors by solving the task
completly (100\%) in only 2.559 seconds. Finally, $BTAI_{3MF}$ has been
implemented in a flexible and easy to use (python) package, and we developed a
graphical user interface to enable the inspection of the model's beliefs,
planning process and behaviour.",https://github.com/ChampiB/Experiments_AI_TS,-1
Longtonotes: OntoNotes with Longer Coreference Chains,0.0170246,"Ontonotes has served as the most important benchmark for coreference
resolution. However, for ease of annotation, several long documents in
Ontonotes were split into smaller parts. In this work, we build a corpus of
coreference-annotated documents of significantly longer length than what is
currently available. We do so by providing an accurate, manually-curated,
merging of annotations from documents that were split into multiple parts in
the original Ontonotes annotation process. The resulting corpus, which we call
LongtoNotes contains documents in multiple genres of the English language with
varying lengths, the longest of which are up to 8x the length of documents in
Ontonotes, and 2x those in Litbank. We evaluate state-of-the-art neural
coreference systems on this new corpus, analyze the relationships between model
architectures/hyperparameters and document length on performance and efficiency
of the models, and demonstrate areas of improvement in long-document
coreference modeling revealed by our new corpus. Our data and code is available
at: https://github.com/kumar-shridhar/LongtoNotes.",https://github.com/kumar-shridhar/LongtoNotes,-1
A Means-End Account of Explainable Artificial Intelligence,0.040278,"Explainable artificial intelligence (XAI) seeks to produce explanations for
those machine learning methods which are deemed opaque. However, there is
considerable disagreement about what this means and how to achieve it. Authors
disagree on what should be explained (topic), to whom something should be
explained (stakeholder), how something should be explained (instrument), and
why something should be explained (goal). In this paper, I employ insights from
means-end epistemology to structure the field. According to means-end
epistemology, different means ought to be rationally adopted to achieve
different epistemic ends. Applied to XAI, different topics, stakeholders, and
goals thus require different instruments. I call this the means-end account of
XAI. The means-end account has a descriptive and a normative component: on the
one hand, I show how the specific means-end relations give rise to a taxonomy
of existing contributions to the field of XAI; on the other hand, I argue that
the suitability of XAI methods can be assessed by analyzing whether they are
prescribed by a given topic, stakeholder, and goal.",None,-1
Application of DatasetGAN in medical imaging: preliminary studies,0.0359242,"Generative adversarial networks (GANs) have been widely investigated for many
potential applications in medical imaging. DatasetGAN is a recently proposed
framework based on modern GANs that can synthesize high-quality segmented
images while requiring only a small set of annotated training images. The
synthesized annotated images could be potentially employed for many medical
imaging applications, where images with segmentation information are required.
However, to the best of our knowledge, there are no published studies focusing
on its applications to medical imaging. In this work, preliminary studies were
conducted to investigate the utility of DatasetGAN in medical imaging. Three
improvements were proposed to the original DatasetGAN framework, considering
the unique characteristics of medical images. The synthesized segmented images
by DatasetGAN were visually evaluated. The trained DatasetGAN was further
analyzed by evaluating the performance of a pre-defined image segmentation
technique, which was trained by the use of the synthesized datasets. The
effectiveness, concerns, and potential usage of DatasetGAN were discussed.",https://github.com/NVlabs/stylegan2-ada-pytorch,-1
Evaluating the Knowledge Dependency of Questions,0.0245631,"The automatic generation of Multiple Choice Questions (MCQ) has the potential
to reduce the time educators spend on student assessment significantly.
However, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE,
and METEOR, focus on the n-gram based similarity of the generated MCQ to the
gold sample in the dataset and disregard their educational value. They fail to
evaluate the MCQ's ability to assess the student's knowledge of the
corresponding target fact. To tackle this issue, we propose a novel automatic
evaluation metric, coined Knowledge Dependent Answerability (KDA), which
measures the MCQ's answerability given knowledge of the target fact.
Specifically, we first show how to measure KDA based on student responses from
a human survey. Then, we propose two automatic evaluation metrics, KDA_disc and
KDA_cont, that approximate KDA by leveraging pre-trained language models to
imitate students' problem-solving behavior. Through our human studies, we show
that KDA_disc and KDA_soft have strong correlations with both (1) KDA and (2)
usability in an actual classroom setting, labeled by experts. Furthermore, when
combined with n-gram based similarity metrics, KDA_disc and KDA_cont are shown
to have a strong predictive power for various expert-labeled MCQ quality
measures.",https://github.com/riiid/question-score,-1
Context-Dependent Anomaly Detection with Knowledge Graph Embedding Models,0.0098751,"Increasing the semantic understanding and contextual awareness of machine
learning models is important for improving robustness and reducing
susceptibility to data shifts. In this work, we leverage contextual awareness
for the anomaly detection problem. Although graphed-based anomaly detection has
been widely studied, context-dependent anomaly detection is an open problem and
without much current research. We develop a general framework for converting a
context-dependent anomaly detection problem to a link prediction problem,
allowing well-established techniques from this domain to be applied. We
implement a system based on our framework that utilizes knowledge graph
embedding models and demonstrates the ability to detect outliers using context
provided by a semantic knowledge base. We show that our method can detect
context-dependent anomalies with a high degree of accuracy and show that
current object detectors can detect enough classes to provide the needed
context for good performance within our example domain.",None,-1
A Frequency-aware Software Cache for Large Recommendation System Embeddings,0.0359143,"Deep learning recommendation models (DLRMs) have been widely applied in
Internet companies. The embedding tables of DLRMs are too large to fit on GPU
memory entirely. We propose a GPU-based software cache approaches to
dynamically manage the embedding table in the CPU and GPU memory space by
leveraging the id's frequency statistics of the target dataset. Our proposed
software cache is efficient in training entire DLRMs on GPU in a synchronized
update manner. It is also scaled to multiple GPUs in combination with the
widely used hybrid parallel training approaches. Evaluating our prototype
system shows that we can keep only 1.5% of the embedding parameters in the GPU
to obtain a decent end-to-end training speed.",https://github.com/zxgx/FreqCacheEmbedding,-1
Coordinated Topic Modeling,0.0100466,"We propose a new problem called coordinated topic modeling that imitates
human behavior while describing a text corpus. It considers a set of
well-defined topics like the axes of a semantic space with a reference
representation. It then uses the axes to model a corpus for easily
understandable representation. This new task helps represent a corpus more
interpretably by reusing existing knowledge and benefits the corpora comparison
task. We design ECTM, an embedding-based coordinated topic model that
effectively uses the reference representation to capture the target
corpus-specific aspects while maintaining each topic's global semantics. In
ECTM, we introduce the topic- and document-level supervision with a
self-training mechanism to solve the problem. Finally, extensive experiments on
multiple domains show the superiority of our model over other baselines.",https://github.com/pritomsaha/Coordinated-Topic-Modeling,-1
Deep Apprenticeship Learning for Playing Games,0.00533876,"In the last decade, deep learning has achieved great success in machine
learning tasks where the input data is represented with different levels of
abstractions. Driven by the recent research in reinforcement learning using
deep neural networks, we explore the feasibility of designing a learning model
based on expert behaviour for complex, multidimensional tasks where reward
function is not available. We propose a novel method for apprenticeship
learning based on the previous research on supervised learning techniques in
reinforcement learning. Our method is applied to video frames from Atari games
in order to teach an artificial agent to play those games. Even though the
reported results are not comparable with the state-of-the-art results in
reinforcement learning, we demonstrate that such an approach has the potential
to achieve strong performance in the future and is worthwhile for further
research.",None,-1
IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations,0.00416214,"What is the relation between a word and its description, or a word and its
embedding? Both descriptions and embeddings are semantic representations of
words. But, what information from the original word remains in these
representations? Or more importantly, which information about a word do these
two representations share? Definition Modeling and Reverse Dictionary are two
opposite learning tasks that address these questions. The goal of the
Definition Modeling task is to investigate the power of information laying
inside a word embedding to express the meaning of the word in a humanly
understandable way -- as a dictionary definition. Conversely, the Reverse
Dictionary task explores the ability to predict word embeddings directly from
its definition. In this paper, by tackling these two tasks, we are exploring
the relationship between words and their semantic representations. We present
our findings based on the descriptive, exploratory, and predictive data
analysis conducted on the CODWOE dataset. We give a detailed overview of the
systems that we designed for Definition Modeling and Reverse Dictionary tasks,
and that achieved top scores on SemEval-2022 CODWOE challenge in several
subtasks. We hope that our experimental results concerning the predictive
models and the data analyses we provide will prove useful in future
explorations of word representations and their relationships.",https://github.com/TimotheeMickus/codwoe/,-1
Spatial-temporal Concept based Explanation of 3D ConvNets,0.043041,"Recent studies have achieved outstanding success in explaining 2D image
recognition ConvNets. On the other hand, due to the computation cost and
complexity of video data, the explanation of 3D video recognition ConvNets is
relatively less studied. In this paper, we present a 3D ACE (Automatic
Concept-based Explanation) framework for interpreting 3D ConvNets. In our
approach: (1) videos are represented using high-level supervoxels, which is
straightforward for human to understand; and (2) the interpreting framework
estimates a score for each voxel, which reflects its importance in the decision
procedure. Experiments show that our method can discover spatial-temporal
concepts of different importance-levels, and thus can explore the influence of
the concepts on a target task, such as action classification, in-depth. The
codes are publicly available.",https://github.com/OrangeeJi/3D-ACE,-1
A Safety Assurable Human-Inspired Perception Architecture,0.0333602,"Although artificial intelligence-based perception (AIP) using deep neural
networks (DNN) has achieved near human level performance, its well-known
limitations are obstacles to the safety assurance needed in autonomous
applications. These include vulnerability to adversarial inputs, inability to
handle novel inputs and non-interpretability. While research in addressing
these limitations is active, in this paper, we argue that a fundamentally
different approach is needed to address them. Inspired by dual process models
of human cognition, where Type 1 thinking is fast and non-conscious while Type
2 thinking is slow and based on conscious reasoning, we propose a dual process
architecture for safe AIP. We review research on how humans address the
simplest non-trivial perception problem, image classification, and sketch a
corresponding AIP architecture for this task. We argue that this architecture
can provide a systematic way of addressing the limitations of AIP using DNNs
and an approach to assurance of human-level performance and beyond. We conclude
by discussing what components of the architecture may already be addressed by
existing work and what remains future work.",None,-1
Zero-shot Cross-Linguistic Learning of Event Semantics,0.020097,"Typologically diverse languages offer systems of lexical and grammatical
aspect that allow speakers to focus on facets of event structure in ways that
comport with the specific communicative setting and discourse constraints they
face. In this paper, we look specifically at captions of images across Arabic,
Chinese, Farsi, German, Russian, and Turkish and describe a computational model
for predicting lexical aspects. Despite the heterogeneity of these languages,
and the salient invocation of distinctive linguistic resources across their
caption corpora, speakers of these languages show surprising similarities in
the ways they frame image content. We leverage this observation for zero-shot
cross-lingual learning and show that lexical aspects can be predicted for a
given language despite not having observed any annotated data for this language
at all.",None,-1
Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with VAEs,0.0229743,"We propose a generative model for text generation, which exhibits
disentangled latent representations of syntax and semantics. Contrary to
previous work, this model does not need syntactic information such as
constituency parses, or semantic information such as paraphrase pairs. Our
model relies solely on the inductive bias found in attention-based
architectures such as Transformers.
  In the attention of Transformers, keys handle information selection while
values specify what information is conveyed. Our model, dubbed QKVAE, uses
Attention in its decoder to read latent variables where one latent variable
infers keys while another infers values. We run experiments on latent
representations and experiments on syntax/semantics transfer which show that
QKVAE displays clear signs of disentangled syntax and semantics. We also show
that our model displays competitive syntax transfer capabilities when compared
to supervised models and that comparable supervised models need a fairly large
amount of data (more than 50K samples) to outperform it on both syntactic and
semantic transfer. The code for our experiments is publicly available.",https://github.com/ghazi-f/QKVAE,-1
"""Diversity and Uncertainty in Moderation"" are the Key to Data Selection for Multilingual Few-shot Transfer",0.0444903,"Few-shot transfer often shows substantial gain over zero-shot
transfer~\cite{lauscher2020zero}, which is a practically useful trade-off
between fully supervised and unsupervised learning approaches for multilingual
pretrained model-based systems. This paper explores various strategies for
selecting data for annotation that can result in a better few-shot transfer.
The proposed approaches rely on multiple measures such as data entropy using
$n$-gram language model, predictive entropy, and gradient embedding. We propose
a loss embedding method for sequence labeling tasks, which induces diversity
and uncertainty sampling similar to gradient embedding. The proposed data
selection strategies are evaluated and compared for POS tagging, NER, and NLI
tasks for up to 20 languages. Our experiments show that the gradient and loss
embedding-based strategies consistently outperform random data selection
baselines, with gains varying with the initial performance of the zero-shot
transfer. Furthermore, the proposed method shows similar trends in improvement
even when the model is fine-tuned using a lower proportion of the original
task-specific labeled training data for zero-shot transfer.",None,-1
Incorporating Rivalry in Reinforcement Learning for a Competitive Game,0.0868578,"Recent advances in reinforcement learning with social agents have allowed
such models to achieve human-level performance on specific interaction tasks.
However, most interactive scenarios do not have a version alone as an end goal;
instead, the social impact of these agents when interacting with humans is as
important and largely unexplored. In this regard, this work proposes a novel
reinforcement learning mechanism based on the social impact of rivalry
behavior. Our proposed model aggregates objective and social perception
mechanisms to derive a rivalry score that is used to modulate the learning of
artificial agents. To investigate our proposed model, we design an interactive
game scenario, using the Chef's Hat Card Game, and examine how the rivalry
modulation changes the agent's playing style, and how this impacts the
experience of human players in the game. Our results show that humans can
detect specific social characteristics when playing against rival agents when
compared to common agents, which directly affects the performance of the human
players in subsequent games. We conclude our work by discussing how the
different social and objective features that compose the artificial rivalry
score contribute to our results.",https://github.com/pablovin/ChefsHatGYM,-1
Interactive Visual Reasoning under Uncertainty,0.0216128,"One of the fundamental cognitive abilities of humans is to quickly resolve
uncertainty by generating hypotheses and testing them via active trials.
Encountering a novel phenomenon accompanied by ambiguous cause-effect
relationships, humans make hypotheses against data, conduct inferences from
observation, test their theory via experimentation, and correct the proposition
if inconsistency arises. These iterative processes persist until the underlying
mechanism becomes clear. In this work, we devise the IVRE (pronounced as
""ivory"") environment for evaluating artificial agents' reasoning ability under
uncertainty. IVRE is an interactive environment featuring rich scenarios
centered around Blicket detection. Agents in IVRE are placed into environments
with various ambiguous action-effect pairs and asked to determine each object's
role. They are encouraged to propose effective and efficient experiments to
validate their hypotheses based on observations and actively gather new
information. The game ends when all uncertainties are resolved or the maximum
number of trials is consumed. By evaluating modern artificial agents in IVRE,
we notice a clear failure of today's learning methods compared to humans. Such
inefficacy in interactive reasoning ability under uncertainty calls for future
research in building human-like intelligence.",None,-1
Continual VQA for Disaster Response Systems,0.00756037,"Visual Question Answering (VQA) is a multi-modal task that involves answering
questions from an input image, semantically understanding the contents of the
image and answering it in natural language. Using VQA for disaster management
is an important line of research due to the scope of problems that are answered
by the VQA system. However, the main challenge is the delay caused by the
generation of labels in the assessment of the affected areas. To tackle this,
we deployed pre-trained CLIP model, which is trained on visual-image pairs.
however, we empirically see that the model has poor zero-shot performance.
Thus, we instead use pre-trained embeddings of text and image from this model
for our supervised training and surpass previous state-of-the-art results on
the FloodNet dataset. We expand this to a continual setting, which is a more
real-life scenario. We tackle the problem of catastrophic forgetting using
various experience replay methods. Our training runs are available at:
https://wandb.ai/compyle/continual_vqa_final. Our code is available at
https://github.com/AdityaKane2001/continual_vqa.",None,-1
Statistical Foundation Behind Machine Learning and Its Impact on Computer Vision,0.0118975,"This paper revisits the principle of uniform convergence in statistical
learning, discusses how it acts as the foundation behind machine learning, and
attempts to gain a better understanding of the essential problem that current
deep learning algorithms are solving. Using computer vision as an example
domain in machine learning, the discussion shows that recent research trends in
leveraging increasingly large-scale data to perform pre-training for
representation learning are largely to reduce the discrepancy between a
practically tractable empirical loss and its ultimately desired but intractable
expected loss. Furthermore, this paper suggests a few future research
directions, predicts the continued increase of data, and argues that more
fundamental research is needed on robustness, interpretability, and reasoning
capabilities of machine learning by incorporating structure and knowledge.",None,-1
Hierarchical Point Cloud Encoding and Decoding with Lightweight Self-Attention based Model,0.00890553,"In this paper we present SA-CNN, a hierarchical and lightweight
self-attention based encoding and decoding architecture for representation
learning of point cloud data. The proposed SA-CNN introduces convolution and
transposed convolution stacks to capture and generate contextual information
among unordered 3D points. Following conventional hierarchical pipeline, the
encoding process extracts feature in local-to-global manner, while the decoding
process generates feature and point cloud in coarse-to-fine, multi-resolution
stages. We demonstrate that SA-CNN is capable of a wide range of applications,
namely classification, part segmentation, reconstruction, shape retrieval, and
unsupervised classification. While achieving the state-of-the-art or comparable
performance in the benchmarks, SA-CNN maintains its model complexity several
order of magnitude lower than the others. In term of qualitative results, we
visualize the multi-stage point cloud reconstructions and latent walks on rigid
objects as well as deformable non-rigid human and robot models.",None,-1
Optical Flow Based Motion Detection for Autonomous Driving,0.0,"Motion detection is a fundamental but challenging task for autonomous
driving. In particular scenes like highway, remote objects have to be paid
extra attention for better controlling decision. Aiming at distant vehicles, we
train a neural network model to classify the motion status using optical flow
field information as the input. The experiments result in high accuracy,
showing that our idea is viable and promising. The trained model also achieves
an acceptable performance for nearby vehicles. Our work is implemented in
PyTorch. Open tools including nuScenes, FastFlowNet and RAFT are used.
Visualization videos are available at
https://www.youtube.com/playlist?list=PLVVrWgq4OrlBnRebmkGZO1iDHEksMHKGk .",https://github.com/kamanphoebe/MotionDetection.git,-1
Monitoring Shortcut Learning using Mutual Information,0.00556657,"The failure of deep neural networks to generalize to out-of-distribution data
is a well-known problem and raises concerns about the deployment of trained
networks in safety-critical domains such as healthcare, finance and autonomous
vehicles. We study a particular kind of distribution shift $\unicode{x2013}$
shortcuts or spurious correlations in the training data. Shortcut learning is
often only exposed when models are evaluated on real-world data that does not
contain the same spurious correlations, posing a serious dilemma for AI
practitioners to properly assess the effectiveness of a trained model for
real-world applications. In this work, we propose to use the mutual information
(MI) between the learned representation and the input as a metric to find where
in training, the network latches onto shortcuts. Experiments demonstrate that
MI can be used as a domain-agnostic metric for monitoring shortcut learning.",None,-1
Counterfactual reasoning: Do language models need world knowledge for causal understanding?,0.0126245,"Current pre-trained language models have enabled remarkable improvements in
downstream tasks, but it remains difficult to distinguish effects of
statistical correlation from more systematic logical reasoning grounded on
understanding of the real world. In this paper we tease these factors apart by
leveraging counterfactual conditionals, which force language models to predict
unusual consequences based on hypothetical propositions. We introduce a set of
tests drawn from psycholinguistic experiments, as well as larger-scale
controlled datasets, to probe counterfactual predictions from a variety of
popular pre-trained language models. We find that models are consistently able
to override real-world knowledge in counterfactual scenarios, and that this
effect is more robust in case of stronger baseline world knowledge -- however,
we also find that for most models this effect appears largely to be driven by
simple lexical cues. When we mitigate effects of both world knowledge and
lexical cues to test knowledge of linguistic nuances of counterfactuals, we
find that only GPT-3 shows sensitivity to these nuances, though this
sensitivity is also non-trivially impacted by lexical associative factors.",https://github.com/goldengua/Counterfactual_Inference_LM,-1
Are Neighbors Enough? Multi-Head Neural n-gram can be Alternative to Self-attention,0.0514567,"Impressive performance of Transformer has been attributed to self-attention,
where dependencies between entire input in a sequence are considered at every
position. In this work, we reform the neural $n$-gram model, which focuses on
only several surrounding representations of each position, with the multi-head
mechanism as in Vaswani et al.(2017). Through experiments on
sequence-to-sequence tasks, we show that replacing self-attention in
Transformer with multi-head neural $n$-gram can achieve comparable or better
performance than Transformer. From various analyses on our proposed method, we
find that multi-head neural $n$-gram is complementary to self-attention, and
their combinations can further improve performance of vanilla Transformer.",None,-1
Thinking the Fusion Strategy of Multi-reference Face Reenactment,0.00356055,"In recent advances of deep generative models, face reenactment -manipulating
and controlling human face, including their head movement-has drawn much
attention for its wide range of applicability. Despite its strong
expressiveness, it is inevitable that the models fail to reconstruct or
accurately generate unseen side of the face of a given single reference image.
Most of existing methods alleviate this problem by learning appearances of
human faces from large amount of data and generate realistic texture at
inference time. Rather than completely relying on what generative models learn,
we show that simple extension by using multiple reference images significantly
improves generation quality. We show this by 1) conducting the reconstruction
task on publicly available dataset, 2) conducting facial motion transfer on our
original dataset which consists of multi-person's head movement video
sequences, and 3) using a newly proposed evaluation metric to validate that our
method achieves better quantitative results.",None,-1
Why-So-Deep: Towards Boosting Previously Trained Models for Visual Place Recognition,0.00768425,"Deep learning-based image retrieval techniques for the loop closure detection
demonstrate satisfactory performance. However, it is still challenging to
achieve high-level performance based on previously trained models in different
geographical regions. This paper addresses the problem of their deployment with
simultaneous localization and mapping (SLAM) systems in the new environment.
The general baseline approach uses additional information, such as GPS,
sequential keyframes tracking, and re-training the whole environment to enhance
the recall rate. We propose a novel approach for improving image retrieval
based on previously trained models. We present an intelligent method, MAQBOOL,
to amplify the power of pre-trained models for better image recall and its
application to real-time multiagent SLAM systems. We achieve comparable image
retrieval results at a low descriptor dimension (512-D), compared to the high
descriptor dimension (4096-D) of state-of-the-art methods. We use spatial
information to improve the recall rate in image retrieval on pre-trained
models.",None,-1
Improving Persian Relation Extraction Models by Data Augmentation,0.0114042,"Relation extraction that is the task of predicting semantic relation type
between entities in a sentence or document is an important task in natural
language processing. Although there are many researches and datasets for
English, Persian suffers from sufficient researches and comprehensive datasets.
The only available Persian dataset for this task is PERLEX, which is a Persian
expert-translated version of the SemEval-2010-Task-8 dataset. In this paper, we
present our augmented dataset and the results and findings of our system,
participated in the Persian relation Extraction shared task of NSURL 2021
workshop. We use PERLEX as the base dataset and enhance it by applying some
text preprocessing steps and by increasing its size via data augmentation
techniques to improve the generalization and robustness of applied models. We
then employ two different models including ParsBERT and multilingual BERT for
relation extraction on the augmented PERLEX dataset. Our best model obtained
64.67% of Macro-F1 on the test phase of the contest and it achieved 83.68% of
Macro-F1 on the test set of PERLEX.",None,-1
Supplementing Missing Visions via Dialog for Scene Graph Generations,0.0269758,"Most current AI systems rely on the premise that the input visual data are
sufficient to achieve competitive performance in various computer vision tasks.
However, the classic task setup rarely considers the challenging, yet common
practical situations where the complete visual data may be inaccessible due to
various reasons (e.g., restricted view range and occlusions). To this end, we
investigate a computer vision task setting with incomplete visual input data.
Specifically, we exploit the Scene Graph Generation (SGG) task with various
levels of visual data missingness as input. While insufficient visual input
intuitively leads to performance drop, we propose to supplement the missing
visions via the natural language dialog interactions to better accomplish the
task objective. We design a model-agnostic Supplementary Interactive Dialog
(SI-Dial) framework that can be jointly learned with most existing models,
endowing the current AI systems with the ability of question-answer
interactions in natural language. We demonstrate the feasibility of such a task
setting with missing visual input and the effectiveness of our proposed dialog
module as the supplementary information source through extensive experiments
and analysis, by achieving promising performance improvement over multiple
baselines.",None,-1
SI-GAT: A method based on improved Graph Attention Network for sonar image classification,0.0125707,"The existing sonar image classification methods based on deep learning are
often analyzed in Euclidean space, only considering the local image features.
For this reason, this paper presents a sonar classification method based on
improved Graph Attention Network (GAT), namely SI-GAT, which is applicable to
multiple types imaging sonar. This method quantifies the correlation
relationship between nodes based on the joint calculation of color proximity
and spatial proximity that represent the sonar characteristics in non-Euclidean
space, then the KNN (K-Nearest Neighbor) algorithm is used to determine the
neighborhood range and adjacency matrix in the graph attention mechanism, which
are jointly considered with the attention coefficient matrix to construct the
key part of the SI-GAT. This SI-GAT is superior to several CNN (Convolutional
Neural Network) methods based on Euclidean space through validation of real
data.",https://github.com/huoguanying/SeabedObjects-Ship-and-Airplane-dataset.git,-1
Discovering and Explaining the Representation Bottleneck of Graph Neural Networks from Multi-order Interactions,0.0157281,"Graph neural networks (GNNs) mainly rely on the message-passing paradigm to
propagate node features and build interactions, and different graph learning
tasks require different ranges of node interactions. In this work, we explore
the capacity of GNNs to capture interactions between nodes under contexts with
different complexities. We discover that GNNs are usually unable to capture the
most informative kinds of interaction styles for diverse graph learning tasks,
and thus name this phenomenon as GNNs' representation bottleneck. As a
response, we demonstrate that the inductive bias introduced by existing graph
construction mechanisms can prevent GNNs from learning interactions of the most
appropriate complexity, i.e., resulting in the representation bottleneck. To
address that limitation, we propose a novel graph rewiring approach based on
interaction patterns learned by GNNs to adjust the receptive fields of each
node dynamically. Extensive experiments on both real-world and synthetic
datasets prove the effectiveness of our algorithm to alleviate the
representation bottleneck and its superiority to enhance the performance of
GNNs over state-of-the-art graph rewiring baselines.",None,-1
On resolving conflicts between arguments,0.0585948,"Argument systems are based on the idea that one can construct arguments for
propositions; i.e., structured reasons justifying the belief in a proposition.
Using defeasible rules, arguments need not be valid in all circumstances,
therefore, it might be possible to construct an argument for a proposition as
well as its negation. When arguments support conflicting propositions, one of
the arguments must be defeated, which raises the question of \emph{which
(sub-)arguments can be subject to defeat}?
  In legal argumentation, meta-rules determine the valid arguments by
considering the last defeasible rule of each argument involved in a conflict.
Since it is easier to evaluate arguments using their last rules, \emph{can a
conflict be resolved by considering only the last defeasible rules of the
arguments involved}?
  We propose a new argument system where, instead of deriving a defeat relation
between arguments, \emph{undercutting-arguments} for the defeat of defeasible
rules are constructed. This system allows us, (\textit{i}) to resolve conflicts
(a generalization of rebutting arguments) using only the last rules of the
arguments for inconsistencies, (\textit{ii}) to determine a set of valid
(undefeated) arguments in linear time using an algorithm based on a JTMS,
(\textit{iii}) to establish a relation with Default Logic, and (\textit{iv}) to
prove closure properties such as \emph{cumulativity}. We also propose an
extension of the argument system that enables \emph{reasoning by cases}.",None,-1
Effective Integration of Weighted Cost-to-go and Conflict Heuristic within Suboptimal CBS,0.0284467,"Conflict-Based Search (CBS) is a popular multi-agent path finding (MAPF)
solver that employs a low-level single agent planner and a high-level
constraint tree to resolve conflicts. The vast majority of modern MAPF solvers
focus on improving CBS by reducing the size of this tree through various
strategies with few methods modifying the low level planner. Typically low
level planners in existing CBS methods use an unweighted cost-to-go heuristic,
with suboptimal CBS methods also using a conflict heuristic to help the high
level search. In this paper, we show that, contrary to prevailing CBS beliefs,
a weighted cost-to-go heuristic can be used effectively alongside the conflict
heuristic in two possible variants. In particular, one of these variants can
obtain large speedups, 2-100x, across several scenarios and suboptimal CBS
methods. Importantly, we discover that performance is related not to the
weighted cost-to-go heuristic but rather to the relative conflict heuristic
weight's ability to effectively balance low-level and high-level work.
Additionally, to the best of our knowledge, we show the first theoretical
relation of prioritized planning and bounded suboptimal CBS and demonstrate
that our methods are their natural generalization. Update March 2024: We found
that the relative speedup decreases to around 1.2-10x depending on how the
conflict heuristic is computed (see appendix for more details).",None,-1
"Statistical Dataset Evaluation: Reliability, Difficulty, and Validity",0.00175829,"Datasets serve as crucial training resources and model performance trackers.
However, existing datasets have exposed a plethora of problems, inducing biased
models and unreliable evaluation results. In this paper, we propose a
model-agnostic dataset evaluation framework for automatic dataset quality
evaluation. We seek the statistical properties of the datasets and address
three fundamental dimensions: reliability, difficulty, and validity, following
a classical testing theory. Taking the Named Entity Recognition (NER) datasets
as a case study, we introduce $9$ statistical metrics for a statistical dataset
evaluation framework. Experimental results and human evaluation validate that
our evaluation framework effectively assesses various aspects of the dataset
quality. Furthermore, we study how the dataset scores on our statistical
metrics affect the model performance, and appeal for dataset quality evaluation
or targeted dataset improvement before training or testing models.",https://github.com/dqxiu/DataEval,-1
Measuring CLEVRness: Blackbox testing of Visual Reasoning Models,0.0151489,"How can we measure the reasoning capabilities of intelligence systems? Visual
question answering provides a convenient framework for testing the model's
abilities by interrogating the model through questions about the scene.
However, despite scores of various visual QA datasets and architectures, which
sometimes yield even a super-human performance, the question of whether those
architectures can actually reason remains open to debate. To answer this, we
extend the visual question answering framework and propose the following
behavioral test in the form of a two-player game. We consider black-box neural
models of CLEVR. These models are trained on a diagnostic dataset benchmarking
reasoning. Next, we train an adversarial player that re-configures the scene to
fool the CLEVR model. We show that CLEVR models, which otherwise could perform
at a human level, can easily be fooled by our agent. Our results put in doubt
whether data-driven approaches can do reasoning without exploiting the numerous
biases that are often present in those datasets. Finally, we also propose a
controlled experiment measuring the efficiency of such models to learn and
perform reasoning.",https://github.com/facebookresearch/clevr-iep,-1
CorrLoss: Integrating Co-Occurrence Domain Knowledge for Affect Recognition,0.0067367,"Neural networks are widely adopted, yet the integration of domain knowledge
is still underutilized. We propose to integrate domain knowledge about
co-occurring facial movements as a constraint in the loss function to enhance
the training of neural networks for affect recognition. As the co-ccurrence
patterns tend to be similar across datasets, applying our method can lead to a
higher generalizability of models and a lower risk of overfitting. We
demonstrate this by showing performance increases in cross-dataset testing for
various datasets. We also show the applicability of our method for calibrating
neural networks to different facial expressions.",None,-1
Can Language Models perform Abductive Commonsense Reasoning?,0.0272107,"Abductive Reasoning is a task of inferring the most plausible hypothesis
given a set of observations. In literature, the community has approached to
solve this challenge by classifying/generating a likely hypothesis that does
not contradict with a past observation and future observation. Some of the most
well-known benchmarks that tackle this problem are aNLI and aNLG (pronounced as
alpha-NLI and alpha-NLG). In this report, I review over some of the
methodologies that were attempted to solve this challenge, re-implement the
baseline models, and analyze some of the weaknesses that current approaches
have. The code and the re-implemented results are available at this link.",https://github.com/SeungoneKim/abductive-commonsense-reasoning,-1
HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python,0.0681994,"Large collections of time series data are commonly organized into structures
with different levels of aggregation; examples include product and geographical
groupings. It is often important to ensure that the forecasts are coherent so
that the predicted values at disaggregate levels add up to the aggregate
forecast. The growing interest of the Machine Learning community in
hierarchical forecasting systems indicates that we are in a propitious moment
to ensure that scientific endeavors are grounded on sound baselines. For this
reason, we put forward the HierarchicalForecast library, which contains
preprocessed publicly available datasets, evaluation metrics, and a compiled
set of statistical baseline models. Our Python-based reference framework aims
to bridge the gap between statistical and econometric modeling, and Machine
Learning forecasting research. Code and documentation are available in
https://github.com/Nixtla/hierarchicalforecast.",None,-1
"A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games",0.47237,"This work studies an algorithm, which we call magnetic mirror descent, that
is inspired by mirror descent and the non-Euclidean proximal gradient
algorithm. Our contribution is demonstrating the virtues of magnetic mirror
descent as both an equilibrium solver and as an approach to reinforcement
learning in two-player zero-sum games. These virtues include: 1) Being the
first quantal response equilibria solver to achieve linear convergence for
extensive-form games with first order feedback; 2) Being the first standard
reinforcement learning algorithm to achieve empirically competitive results
with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark
Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning
algorithm.",None,-1
Scene Aware Person Image Generation through Global Contextual Conditioning,0.187177,"Person image generation is an intriguing yet challenging problem. However,
this task becomes even more difficult under constrained situations. In this
work, we propose a novel pipeline to generate and insert contextually relevant
person images into an existing scene while preserving the global semantics.
More specifically, we aim to insert a person such that the location, pose, and
scale of the person being inserted blends in with the existing persons in the
scene. Our method uses three individual networks in a sequential pipeline. At
first, we predict the potential location and the skeletal structure of the new
person by conditioning a Wasserstein Generative Adversarial Network (WGAN) on
the existing human skeletons present in the scene. Next, the predicted skeleton
is refined through a shallow linear network to achieve higher structural
accuracy in the generated image. Finally, the target image is generated from
the refined skeleton using another generative network conditioned on a given
image of the target person. In our experiments, we achieve high-resolution
photo-realistic generation results while preserving the general context of the
scene. We conclude our paper with multiple qualitative and quantitative
benchmarks on the results.",None,-1
How good are deep models in understanding the generated images?,0.00346989,"My goal in this paper is twofold: to study how well deep models can
understand the images generated by DALL-E 2 and Midjourney, and to
quantitatively evaluate these generative models. Two sets of generated images
are collected for object recognition and visual question answering (VQA) tasks.
On object recognition, the best model, out of 10 state-of-the-art object
recognition models, achieves about 60\% and 80\% top-1 and top-5 accuracy,
respectively. These numbers are much lower than the best accuracy on the
ImageNet dataset (91\% and 99\%). On VQA, the OFA model scores 77.3\% on
answering 241 binary questions across 50 images. This model scores 94.7\% on
the binary VQA-v2 dataset. Humans are able to recognize the generated images
and answer questions on them easily. We conclude that a) deep models struggle
to understand the generated content, and may do better after fine-tuning, and
b) there is a large distribution shift between the generated images and the
real photographs. The distribution shift appears to be category-dependent. Data
is available at:
https://drive.google.com/file/d/1n2nCiaXtYJRRF2R73-LNE3zggeU_HeH0/view?usp=sharing.",None,-1
A device-interaction model for users with special needs,0.0147628,"Interaction is a fundamental part of using any computer system but it is
still an issue for people with special needs. In order to improve this
situation, this paper describes a new device-interaction model based on
adaptation rules for user models. The aim is the adaptation at the interaction
level, taking into account the interaction device features in order to improve
the usability through the user experience in the education sector. In the
evaluation process, several students from a special education center have
participated. These students have either a physical or sensory disability or
autism. The results are promising enough to consider that this model will be
able to help students with disabilities to interact with a computer system
which will inevitably provide tremendous benefits to their academic and
personal development.",None,-1
Label-dependent and event-guided interpretable disease risk prediction using EHRs,0.0447912,"Electronic health records (EHRs) contain patients' heterogeneous data that
are collected from medical providers involved in the patient's care, including
medical notes, clinical events, laboratory test results, symptoms, and
diagnoses. In the field of modern healthcare, predicting whether patients would
experience any risks based on their EHRs has emerged as a promising research
area, in which artificial intelligence (AI) plays a key role. To make AI models
practically applicable, it is required that the prediction results should be
both accurate and interpretable. To achieve this goal, this paper proposed a
label-dependent and event-guided risk prediction model (LERP) to predict the
presence of multiple disease risks by mainly extracting information from
unstructured medical notes. Our model is featured in the following aspects.
First, we adopt a label-dependent mechanism that gives greater attention to
words from medical notes that are semantically similar to the names of risk
labels. Secondly, as the clinical events (e.g., treatments and drugs) can also
indicate the health status of patients, our model utilizes the information from
events and uses them to generate an event-guided representation of medical
notes. Thirdly, both label-dependent and event-guided representations are
integrated to make a robust prediction, in which the interpretability is
enabled by the attention weights over words from medical notes. To demonstrate
the applicability of the proposed method, we apply it to the MIMIC-III dataset,
which contains real-world EHRs collected from hospitals. Our method is
evaluated in both quantitative and qualitative ways.",https://github.com/guoyinwang/LEAM,-1
RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning,0.0405903,"Transformers have been shown to be able to perform deductive reasoning on a
logical rulebase containing rules and statements written in English natural
language. While the progress is promising, it is currently unclear if these
models indeed perform logical reasoning by understanding the underlying logical
semantics in the language. To this end, we propose RobustLR, a suite of
evaluation datasets that evaluate the robustness of these models to minimal
logical edits in rulebases and some standard logical equivalence conditions. In
our experiments with RoBERTa and T5, we find that the models trained in prior
works do not perform consistently on the different perturbations in RobustLR,
thus showing that the models are not robust to the proposed logical
perturbations. Further, we find that the models find it especially hard to
learn logical negation and disjunction operators. Overall, using our evaluation
sets, we demonstrate some shortcomings of the deductive reasoning-based
language models, which can eventually help towards designing better models for
logical reasoning over natural language. All the datasets and code base have
been made publicly available.",https://github.com/INK-USC/RobustLR,-1
"A Machine with Short-Term, Episodic, and Semantic Memory Systems",0.0034153,"Inspired by the cognitive science theory of the explicit human memory
systems, we have modeled an agent with short-term, episodic, and semantic
memory systems, each of which is modeled with a knowledge graph. To evaluate
this system and analyze the behavior of this agent, we designed and released
our own reinforcement learning agent environment, ""the Room"", where an agent
has to learn how to encode, store, and retrieve memories to maximize its return
by answering questions. We show that our deep Q-learning based agent
successfully learns whether a short-term memory should be forgotten, or rather
be stored in the episodic or semantic memory systems. Our experiments indicate
that an agent with human-like memory systems can outperform an agent without
this memory structure in the environment.",https://github.com/tae898/room-env,-1
Reweighting Strategy based on Synthetic Data Identification for Sentence Similarity,0.00838028,"Semantically meaningful sentence embeddings are important for numerous tasks
in natural language processing. To obtain such embeddings, recent studies
explored the idea of utilizing synthetically generated data from pretrained
language models (PLMs) as a training corpus. However, PLMs often generate
sentences much different from the ones written by human. We hypothesize that
treating all these synthetic examples equally for training deep neural networks
can have an adverse effect on learning semantically meaningful embeddings. To
analyze this, we first train a classifier that identifies machine-written
sentences, and observe that the linguistic features of the sentences identified
as written by a machine are significantly different from those of human-written
sentences. Based on this, we propose a novel approach that first trains the
classifier to measure the importance of each sentence. The distilled
information from the classifier is then used to train a reliable sentence
embedding model. Through extensive evaluation on four real-world datasets, we
demonstrate that our model trained on synthetic data generalizes well and
outperforms the existing baselines. Our implementation is publicly available at
https://github.com/ddehun/coling2022_reweighting_sts.",https://github.com/ddehun/coling2022_reweighting_sts,-1
A high-precision underwater object detection based on joint self-supervised deblurring and improved spatial transformer network,0.122461,"Deep learning-based underwater object detection (UOD) remains a major
challenge due to the degraded visibility and difficulty to obtain sufficient
underwater object images captured from various perspectives for training. To
address these issues, this paper presents a high-precision UOD based on joint
self-supervised deblurring and improved spatial transformer network. A
self-supervised deblurring subnetwork is introduced into the designed
multi-task learning aided object detection architecture to force the shared
feature extraction module to output clean features for detection subnetwork.
Aiming at alleviating the limitation of insufficient photos from different
perspectives, an improved spatial transformer network is designed based on
perspective transformation, adaptively enriching image features within the
network. The experimental results show that the proposed UOD approach achieved
47.9 mAP in URPC2017 and 70.3 mAP in URPC2018, outperforming many
state-of-the-art UOD methods and indicating the designed method is more
suitable for UOD.",None,-1
Anisotropic Multi-Scale Graph Convolutional Network for Dense Shape Correspondence,0.00950532,"This paper studies 3D dense shape correspondence, a key shape analysis
application in computer vision and graphics. We introduce a novel hybrid
geometric deep learning-based model that learns geometrically meaningful and
discretization-independent features with a U-Net model as the primary node
feature extraction module, followed by a successive spectral-based graph
convolutional network. To create a diverse set of filters, we use anisotropic
wavelet basis filters, being sensitive to both different directions and
band-passes. This filter set overcomes the over-smoothing behavior of
conventional graph neural networks. To further improve the model's performance,
we add a function that perturbs the feature maps in the last layer ahead of
fully connected layers, forcing the network to learn more discriminative
features overall. The resulting correspondence maps show state-of-the-art
performance on the benchmark datasets based on average geodesic errors and
superior robustness to discretization in 3D meshes. Our approach provides new
insights and practical solutions to the dense shape correspondence research.",None,-1
PartAL: Efficient Partial Active Learning in Multi-Task Visual Settings,0.0272509,"Multi-task learning is central to many real-world applications.
Unfortunately, obtaining labelled data for all tasks is time-consuming,
challenging, and expensive. Active Learning (AL) can be used to reduce this
burden. Existing techniques typically involve picking images to be annotated
and providing annotations for all tasks.
  In this paper, we show that it is more effective to select not only the
images to be annotated but also a subset of tasks for which to provide
annotations at each AL iteration. Furthermore, the annotations that are
provided can be used to guess pseudo-labels for the tasks that remain
unannotated. We demonstrate the effectiveness of our approach on several
popular multi-task datasets.",None,-1
Debiased Large Language Models Still Associate Muslims with Uniquely Violent Acts,0.0438467,"Recent work demonstrates a bias in the GPT-3 model towards generating violent
text completions when prompted about Muslims, compared with Christians and
Hindus. Two pre-registered replication attempts, one exact and one approximate,
found only the weakest bias in the more recent Instruct Series version of
GPT-3, fine-tuned to eliminate biased and toxic outputs. Few violent
completions were observed. Additional pre-registered experiments, however,
showed that using common names associated with the religions in prompts yields
a highly significant increase in violent completions, also revealing a stronger
second-order bias against Muslims. Names of Muslim celebrities from non-violent
domains resulted in relatively fewer violent completions, suggesting that
access to individualized information can steer the model away from using
stereotypes. Nonetheless, content analysis revealed religion-specific violent
themes containing highly offensive ideas regardless of prompt format. Our
results show the need for additional debiasing of large language models to
address higher-order schemas and associations.",None,-1
Attention Hijacking in Trojan Transformers,0.0101418,"Trojan attacks pose a severe threat to AI systems. Recent works on
Transformer models received explosive popularity and the self-attentions are
now indisputable. This raises a central question: Can we reveal the Trojans
through attention mechanisms in BERTs and ViTs? In this paper, we investigate
the attention hijacking pattern in Trojan AIs, \ie, the trigger token
``kidnaps'' the attention weights when a specific trigger is present. We
observe the consistent attention hijacking pattern in Trojan Transformers from
both Natural Language Processing (NLP) and Computer Vision (CV) domains. This
intriguing property helps us to understand the Trojan mechanism in BERTs and
ViTs. We also propose an Attention-Hijacking Trojan Detector (AHTD) to
discriminate the Trojan AIs from the clean ones.",https://github.com/usnistgov/trojai-round-generation/,-1
I see what you hear: a vision-inspired method to localize words,0.02513,"This paper explores the possibility of using visual object detection
techniques for word localization in speech data. Object detection has been
thoroughly studied in the contemporary literature for visual data. Noting that
an audio can be interpreted as a 1-dimensional image, object localization
techniques can be fundamentally useful for word localization. Building upon
this idea, we propose a lightweight solution for word detection and
localization. We use bounding box regression for word localization, which
enables our model to detect the occurrence, offset, and duration of keywords in
a given audio stream. We experiment with LibriSpeech and train a model to
localize 1000 words. Compared to existing work, our method reduces model size
by 94%, and improves the F1 score by 6.5\%.",None,-1
WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning,0.116204,"A crucial issue of current text generation models is that they often
uncontrollably generate factually inconsistent text with respective of their
inputs. Limited by the lack of annotated data, existing works in evaluating
factual consistency directly transfer the reasoning ability of models trained
on other data-rich upstream tasks like question answering (QA) and natural
language inference (NLI) without any further adaptation. As a result, they
perform poorly on the real generated text and are biased heavily by their
single-source upstream tasks. To alleviate this problem, we propose a weakly
supervised framework that aggregates multiple resources to train a precise and
efficient factual metric, namely WeCheck. WeCheck first utilizes a generative
model to accurately label a real generated sample by aggregating its weak
labels, which are inferred from multiple resources. Then, we train the target
metric model with the weak supervision while taking noises into consideration.
Comprehensive experiments on a variety of tasks demonstrate the strong
performance of WeCheck, which achieves a 3.4\% absolute improvement over
previous state-of-the-art methods on TRUE benchmark on average.",None,-1
Intake Monitoring in Free-Living Conditions: Overview and Lessons we Have Learned,0.0625024,"The progress in artificial intelligence and machine learning algorithms over
the past decade has enabled the development of new methods for the objective
measurement of eating, including both the measurement of eating episodes as
well as the measurement of in-meal eating behavior. These allow the study of
eating behavior outside the laboratory in free-living conditions, without the
need for video recordings and laborious manual annotations. In this paper, we
present a high-level overview of our recent work on intake monitoring using a
smartwatch, as well as methods using an in-ear microphone. We also present
evaluation results of these methods in challenging, real-world datasets.
Furthermore, we discuss use-cases of such intake monitoring tools for advancing
research in eating behavior, for improving dietary monitoring, as well as for
developing evidence-based health policies. Our goal is to inform researchers
and users of intake monitoring methods regarding (i) the development of new
methods based on commercially available devices, (ii) what to expect in terms
of effectiveness, and (iii) how these methods can be used in research as well
as in practical applications.",None,-1
Training a Vision Transformer from scratch in less than 24 hours with 1 GPU,0.0169813,"Transformers have become central to recent advances in computer vision.
However, training a vision Transformer (ViT) model from scratch can be resource
intensive and time consuming. In this paper, we aim to explore approaches to
reduce the training costs of ViT models. We introduce some algorithmic
improvements to enable training a ViT model from scratch with limited hardware
(1 GPU) and time (24 hours) resources. First, we propose an efficient approach
to add locality to the ViT architecture. Second, we develop a new image size
curriculum learning strategy, which allows to reduce the number of patches
extracted from each image at the beginning of the training. Finally, we propose
a new variant of the popular ImageNet1k benchmark by adding hardware and time
constraints. We evaluate our contributions on this benchmark, and show they can
significantly improve performances given the proposed training budget. We will
share the code in https://github.com/BorealisAI/efficient-vit-training.",https://github.com/BorealisAI/efﬁcient-vit-training,-1
UKP-SQuARE v2: Explainability and Adversarial Attacks for Trustworthy QA,0.0,"Question Answering (QA) systems are increasingly deployed in applications
where they support real-world decisions. However, state-of-the-art models rely
on deep neural networks, which are difficult to interpret by humans. Inherently
interpretable models or post hoc explainability methods can help users to
comprehend how a model arrives at its prediction and, if successful, increase
their trust in the system. Furthermore, researchers can leverage these insights
to develop new methods that are more accurate and less biased. In this paper,
we introduce SQuARE v2, the new version of SQuARE, to provide an explainability
infrastructure for comparing models based on methods such as saliency maps and
graph-based explanations. While saliency maps are useful to inspect the
importance of each input token for the model's prediction, graph-based
explanations from external Knowledge Graphs enable the users to verify the
reasoning behind the model prediction. In addition, we provide multiple
adversarial attacks to compare the robustness of QA models. With these
explainability methods and adversarial attacks, we aim to ease the research on
trustworthy QA models. SQuARE is available on https://square.ukp-lab.de.",https://github.com/UKP-SQuARE/square-core,-1
Detecting danger in gridworlds using Gromov's Link Condition,0.0110839,"Gridworlds have been long-utilised in AI research, particularly in
reinforcement learning, as they provide simple yet scalable models for many
real-world applications such as robot navigation, emergent behaviour, and
operations research. We initiate a study of gridworlds using the mathematical
framework of reconfigurable systems and state complexes due to Abrams, Ghrist &
Peterson. State complexes represent all possible configurations of a system as
a single geometric space, thus making them conducive to study using geometric,
topological, or combinatorial methods. The main contribution of this work is a
modification to the original Abrams, Ghrist & Peterson setup which we introduce
to capture agent braiding and thereby more naturally represent the topology of
gridworlds. With this modification, the state complexes may exhibit geometric
defects (failure of Gromov's Link Condition). Serendipitously, we discover
these failures occur exactly where undesirable or dangerous states appear in
the gridworld. Our results therefore provide a novel method for seeking
guaranteed safety limitations in discrete task environments with single or
multiple agents, and offer useful safety information (in geometric and
topological forms) for incorporation in or analysis of machine learning
systems. More broadly, our work introduces tools from geometric group theory
and combinatorics to the AI community and demonstrates a proof-of-concept for
this geometric viewpoint of the task domain through the example of simple
gridworld environments.",https://github.com/tfburns/State-Complexes-of-Gridworlds,-1
A Unified Framework for Multi-intent Spoken Language Understanding with prompting,0.0491296,"Multi-intent Spoken Language Understanding has great potential for widespread
implementation. Jointly modeling Intent Detection and Slot Filling in it
provides a channel to exploit the correlation between intents and slots.
However, current approaches are apt to formulate these two sub-tasks
differently, which leads to two issues: 1) It hinders models from effective
extraction of shared features. 2) Pretty complicated structures are involved to
enhance expression ability while causing damage to the interpretability of
frameworks. In this work, we describe a Prompt-based Spoken Language
Understanding (PromptSLU) framework, to intuitively unify two sub-tasks into
the same form by offering a common pre-trained Seq2Seq model. In detail, ID and
SF are completed by concisely filling the utterance into task-specific prompt
templates as input, and sharing output formats of key-value pairs sequence.
Furthermore, variable intents are predicted first, then naturally embedded into
prompts to guide slot-value pairs inference from a semantic perspective.
Finally, we are inspired by prevalent multi-task learning to introduce an
auxiliary sub-task, which helps to learn relationships among provided labels.
Experiment results show that our framework outperforms several state-of-the-art
baselines on two public datasets.",None,-1
Learning Feynman Diagrams using Graph Neural Networks,0.0214212,"In the wake of the growing popularity of machine learning in particle
physics, this work finds a new application of geometric deep learning on
Feynman diagrams to make accurate and fast matrix element predictions with the
potential to be used in analysis of quantum field theory. This research uses
the graph attention layer which makes matrix element predictions to 1
significant figure accuracy above 90% of the time. Peak performance was
achieved in making predictions to 3 significant figure accuracy over 10% of the
time with less than 200 epochs of training, serving as a proof of concept on
which future works can build upon for better performance. Finally, a procedure
is suggested, to use the network to make advancements in quantum field theory
by constructing Feynman diagrams with effective particles that represent
non-perturbative calculations.",https://github.com/Clearbloo/Feynman_GNN.git,-1
PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals,0.00473166,"We propose a PiggyBack, a Visual Question Answering platform that allows
users to apply the state-of-the-art visual-language pretrained models easily.
The PiggyBack supports the full stack of visual question answering tasks,
specifically data processing, model fine-tuning, and result visualisation. We
integrate visual-language models, pretrained by HuggingFace, an open-source API
platform of deep learning technologies; however, it cannot be runnable without
programming skills or deep learning understanding. Hence, our PiggyBack
supports an easy-to-use browser-based user interface with several deep learning
visual language pretrained models for general users and domain experts. The
PiggyBack includes the following benefits: Free availability under the MIT
License, Portability due to web-based and thus runs on almost any platform, A
comprehensive data creation and processing technique, and ease of use on deep
learning-based visual language pretrained models. The demo video is available
on YouTube and can be found at https://youtu.be/iz44RZ1lF4s.",None,-1
Finite Entailment of UCRPQs over ALC Ontologies,0.35882,"We investigate the problem of finite entailment of ontology-mediated queries.
We consider the expressive query language, unions of conjunctive regular path
queries (UCRPQs), extending the well-known class of union of conjunctive
queries, with regular expressions over roles. We look at ontologies formulated
using the description logic ALC, and show a tight 2EXPTIME upper bound for
entailment of UCRPQs. At the core of our decision procedure, there is a novel
automata-based technique introducing a stratification of interpretations
induced by the deterministic finite automaton underlying the input UCRPQ",None,-1
HM: Hybrid Masking for Few-Shot Segmentation,0.0,"We study few-shot semantic segmentation that aims to segment a target object
from a query image when provided with a few annotated support images of the
target class. Several recent methods resort to a feature masking (FM) technique
to discard irrelevant feature activations which eventually facilitates the
reliable prediction of segmentation mask. A fundamental limitation of FM is the
inability to preserve the fine-grained spatial details that affect the accuracy
of segmentation mask, especially for small target objects. In this paper, we
develop a simple, effective, and efficient approach to enhance feature masking
(FM). We dub the enhanced FM as hybrid masking (HM). Specifically, we
compensate for the loss of fine-grained spatial details in FM technique by
investigating and leveraging a complementary basic input masking method.
Experiments have been conducted on three publicly available benchmarks with
strong few-shot segmentation (FSS) baselines. We empirically show improved
performance against the current state-of-the-art methods by visible margins
across different benchmarks. Our code and trained models are available at:
https://github.com/moonsh/HM-Hybrid-Masking",https://github.com/moonsh/HM-Hybrid-Masking,-1
"Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges",0.00464803,"Machine Learning algorithms have had a profound impact on the field of
computer science over the past few decades. These algorithms performance is
greatly influenced by the representations that are derived from the data in the
learning process. The representations learned in a successful learning process
should be concise, discrete, meaningful, and able to be applied across a
variety of tasks. A recent effort has been directed toward developing Deep
Learning models, which have proven to be particularly effective at capturing
high-dimensional, non-linear, and multi-modal characteristics. In this work, we
discuss the principles and developments that have been made in the process of
learning representations, and converting them into desirable applications. In
addition, for each framework or model, the key issues and open challenges, as
well as the advantages, are examined.",None,-1
Similarity and Content-based Phonetic Self Attention for Speech Recognition,0.0225578,"Transformer-based speech recognition models have achieved great success due
to the self-attention (SA) mechanism that utilizes every frame in the feature
extraction process. Especially, SA heads in lower layers capture various
phonetic characteristics by the query-key dot product, which is designed to
compute the pairwise relationship between frames. In this paper, we propose a
variant of SA to extract more representative phonetic features. The proposed
phonetic self-attention (phSA) is composed of two different types of phonetic
attention; one is similarity-based and the other is content-based. In short,
similarity-based attention captures the correlation between frames while
content-based attention only considers each frame without being affected by
other frames. We identify which parts of the original dot product equation are
related to two different attention patterns and improve each part with simple
modifications. Our experiments on phoneme classification and speech recognition
show that replacing SA with phSA for lower layers improves the recognition
performance without increasing the latency and the parameter size.",None,-1
Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning,0.0114084,"Despite recent advances in natural language understanding and generation, and
decades of research on the development of conversational bots, building
automated agents that can carry on rich open-ended conversations with humans
""in the wild"" remains a formidable challenge. In this work we develop a
real-time, open-ended dialogue system that uses reinforcement learning (RL) to
power a bot's conversational skill at scale. Our work pairs the succinct
embedding of the conversation state generated using SOTA (supervised) language
models with RL techniques that are particularly suited to a dynamic action
space that changes as the conversation progresses. Trained using crowd-sourced
data, our novel system is able to substantially exceeds the (strong) baseline
supervised model with respect to several metrics of interest in a live
experiment with real users of the Google Assistant.",https://github.com/google-research/bert,-1
An Entropy-based Measure of Intelligence Degree of System Structures,0.0177071,"In this paper, we investigate how to measure the intelligence of systems
under specific structures. Two indicators are adopted to characterize the
intelligence of a given structure, namely the function diversity of the
structure, and the ability to generate order under specific environments. A
measure of intelligence degree is proposed, with which the intelligence degree
of several basic structures is calculated. It is shown that some structures are
indeed ""smarter"" than the others under the proposed measure. The results add a
possible way of revealing the evolution mechanism of natural life and
constructing life-like structures with high intelligence degree.",None,-1
Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning,0.00887991,"Inspired by the curvature of space-time (Einstein, 1921), we introduce Curved
Contrastive Learning (CCL), a novel representation learning technique for
learning the relative turn distance between utterance pairs in multi-turn
dialogues. The resulting bi-encoder models can guide transformers as a response
ranking model towards a goal in a zero-shot fashion by projecting the goal
utterance and the corresponding reply candidates into a latent space. Here the
cosine similarity indicates the distance/reachability of a candidate utterance
toward the corresponding goal. Furthermore, we explore how these
forward-entailing language representations can be utilized for assessing the
likelihood of sequences by the entailment strength i.e. through the cosine
similarity of its individual members (encoded separately) as an emergent
property in the curved space. These non-local properties allow us to imagine
the likelihood of future patterns in dialogues, specifically by
ordering/identifying future goal utterances that are multiple turns away, given
a dialogue context. As part of our analysis, we investigate characteristics
that make conversations (un)plannable and find strong evidence of planning
capability over multiple turns (in 61.56% over 3 turns) in conversations from
the DailyDialog (Li et al., 2017) dataset. Finally, we show how we achieve
higher efficiency in sequence modeling tasks compared to previous work thanks
to our relativistic approach, where only the last utterance needs to be encoded
and computed during inference.",https://github.com/Justus-Jonas/imaginaryNLP,-1
Translated Skip Connections -- Expanding the Receptive Fields of Fully Convolutional Neural Networks,0.00766662,"The effective receptive field of a fully convolutional neural network is an
important consideration when designing an architecture, as it defines the
portion of the input visible to each convolutional kernel. We propose a neural
network module, extending traditional skip connections, called the translated
skip connection. Translated skip connections geometrically increase the
receptive field of an architecture with negligible impact on both the size of
the parameter space and computational complexity. By embedding translated skip
connections into a benchmark architecture, we demonstrate that our module
matches or outperforms four other approaches to expanding the effective
receptive fields of fully convolutional neural networks. We confirm this result
across five contemporary image segmentation datasets from disparate domains,
including the detection of COVID-19 infection, segmentation of aerial imagery,
common object segmentation, and segmentation for self-driving cars.",https://github.com/JoshuaDBruton/TSC,-1
KGRGRL: A User's Permission Reasoning Method Based on Knowledge Graph Reward Guidance Reinforcement Learning,0.0826617,"In general, multiple domain cyberspace security assessments can be
implemented by reasoning user's permissions. However, while existing methods
include some information from the physical and social domains, they do not
provide a comprehensive representation of cyberspace. Existing reasoning
methods are also based on expert-given rules, resulting in inefficiency and a
low degree of intelligence. To address this challenge, we create a Knowledge
Graph (KG) of multiple domain cyberspace in order to provide a standard
semantic description of the multiple domain cyberspace. Following that, we
proposed a user's permissions reasoning method based on reinforcement learning.
All permissions in cyberspace are represented as nodes, and an agent is trained
to find all permissions that user can have according to user's initial
permissions and cyberspace KG. We set 10 reward setting rules based on the
features of cyberspace KG in the reinforcement learning of reward information
setting, so that the agent can better locate user's all permissions and avoid
blindly finding user's permissions. The results of the experiments showed that
the proposed method can successfully reason about user's permissions and
increase the intelligence level of the user's permissions reasoning method. At
the same time, the F1 value of the proposed method is 6% greater than that of
the Translating Embedding (TransE) method.",None,-1
Active Labeling: Streaming Stochastic Gradients,0.0125272,"The workhorse of machine learning is stochastic gradient descent. To access
stochastic gradients, it is common to consider iteratively input/output pairs
of a training dataset. Interestingly, it appears that one does not need full
supervision to access stochastic gradients, which is the main motivation of
this paper. After formalizing the ""active labeling"" problem, which focuses on
active learning with partial supervision, we provide a streaming technique that
provably minimizes the ratio of generalization error over the number of
samples. We illustrate our technique in depth for robust regression.",https://github.com/VivienCabannes/active-labeling,-1
Contribution of the Temperature of the Objects to the Problem of Thermal Imaging Focusing,0.0119546,"When focusing an image, depth of field, aperture and distance from the camera
to the object, must be taking into account, both, in visible and in infrared
spectrum. Our experiments reveal that in addition, the focusing problem in
thermal spectrum is also hardly dependent of the temperature of the object
itself (and/or the scene).",None,-1
Learning to Mitigate AI Collusion on Economic Platforms,0.132606,"Algorithmic pricing on online e-commerce platforms raises the concern of
tacit collusion, where reinforcement learning algorithms learn to set collusive
prices in a decentralized manner and through nothing more than profit feedback.
This raises the question as to whether collusive pricing can be prevented
through the design of suitable ""buy boxes,"" i.e., through the design of the
rules that govern the elements of e-commerce sites that promote particular
products and prices to consumers. In this paper, we demonstrate that
reinforcement learning (RL) can also be used by platforms to learn buy box
rules that are effective in preventing collusion by RL sellers. For this, we
adopt the methodology of Stackelberg POMDPs, and demonstrate success in
learning robust rules that continue to provide high consumer welfare together
with sellers employing different behavior models or having out-of-distribution
costs for goods.",None,-1
A model-based approach to meta-Reinforcement Learning: Transformers and tree search,0.016292,"Meta-learning is a line of research that develops the ability to leverage
past experiences to efficiently solve new learning problems. Meta-Reinforcement
Learning (meta-RL) methods demonstrate a capability to learn behaviors that
efficiently acquire and exploit information in several meta-RL problems.
  In this context, the Alchemy benchmark has been proposed by Wang et al.
[2021]. Alchemy features a rich structured latent space that is challenging for
state-of-the-art model-free RL methods. These methods fail to learn to properly
explore then exploit.
  We develop a model-based algorithm. We train a model whose principal block is
a Transformer Encoder to fit the symbolic Alchemy environment dynamics. Then we
define an online planner with the learned model using a tree search method.
This algorithm significantly outperforms previously applied model-free RL
methods on the symbolic Alchemy problem.
  Our results reveal the relevance of model-based approaches with online
planning to perform exploration and exploitation successfully in meta-RL.
Moreover, we show the efficiency of the Transformer architecture to learn
complex dynamics that arise from latent spaces present in meta-RL problems.",None,-1
Regionalized Optimization,0.024653,"We propose a theoretical framework for non redundant reconstruction of a
global loss from a collection of local ones under constraints given by a
functor; we call this loss the regionalized loss in honor to Yedidia, Freeman,
Weiss' celebrated article `Constructing free-energy approximations and
generalized belief propagation algorithms' where a first example of
regionalized loss, for entropy and the marginal functor, is built. We show how
one can associate to these regionalized losses message passing algorithms for
finding their critical points. It is a natural mathematical framework for
optimization problems where there are multiple points of views on a dataset and
replaces message passing algorithms as canonical ways of finding the optima of
these problems. We explain how Generalized Belief propagation algorithms fall
into the framework we propose and propose novel message passing algorithms for
noisy channel networks.",None,-1
Deep Learning-Based Discrete Calibrated Survival Prediction,0.0526844,"Deep neural networks for survival prediction outper-form classical approaches
in discrimination, which is the ordering of patients according to their
time-of-event. Conversely, classical approaches like the Cox Proportional
Hazards model display much better calibration, the correct temporal prediction
of events of the underlying distribution. Especially in the medical domain,
where it is critical to predict the survival of a single patient, both
discrimination and calibration are important performance metrics. Here we
present Discrete Calibrated Survival (DCS), a novel deep neural network for
discriminated and calibrated survival prediction that outperforms competing
survival models in discrimination on three medical datasets, while achieving
best calibration among all discrete time models. The enhanced performance of
DCS can be attributed to two novel features, the variable temporal output node
spacing and the novel loss term that optimizes the use of uncensored and
censored patient data. We believe that DCS is an important step towards
clinical application of deep-learning-based survival prediction with
state-of-the-art discrimination and good calibration.",https://github.com/imsb-uke/dcsurv,-1
Protea: Client Profiling within Federated Systems using Flower,0.0826297,"Federated Learning (FL) has emerged as a prospective solution that
facilitates the training of a high-performing centralised model without
compromising the privacy of users. While successful, research is currently
limited by the possibility of establishing a realistic large-scale FL system at
the early stages of experimentation. Simulation can help accelerate this
process. To facilitate efficient scalable FL simulation of heterogeneous
clients, we design and implement Protea, a flexible and lightweight client
profiling component within federated systems using the FL framework Flower. It
allows automatically collecting system-level statistics and estimating the
resources needed for each client, thus running the simulation in a
resource-aware fashion. The results show that our design successfully increases
parallelism for 1.66 $\times$ faster wall-clock time and 2.6$\times$ better GPU
utilisation, which enables large-scale experiments on heterogeneous clients.",None,-1
Automatic Language Identification for Celtic Texts,0.0151376,"Language identification is an important Natural Language Processing task. It
has been thoroughly researched in the literature. However, some issues are
still open. This work addresses the identification of the related low-resource
languages on the example of the Celtic language family.
  This work's main goals were: (1) to collect the dataset of three Celtic
languages; (2) to prepare a method to identify the languages from the Celtic
family, i.e. to train a successful classification model; (3) to evaluate the
influence of different feature extraction methods, and explore the
applicability of the unsupervised models as a feature extraction technique; (4)
to experiment with the unsupervised feature extraction on a reduced annotated
set.
  We collected a new dataset including Irish, Scottish, Welsh and English
records. We tested supervised models such as SVM and neural networks with
traditional statistical features alongside the output of clustering,
autoencoder, and topic modelling methods. The analysis showed that the
unsupervised features could serve as a valuable extension to the n-gram feature
vectors. It led to an improvement in performance for more entangled classes.
The best model achieved a 98\% F1 score and 97\% MCC. The dense neural network
consistently outperformed the SVM model.
  The low-resource languages are also challenging due to the scarcity of
available annotated training data. This work evaluated the performance of the
classifiers using the unsupervised feature extraction on the reduced labelled
dataset to handle this issue. The results uncovered that the unsupervised
feature vectors are more robust to the labelled set reduction. Therefore, they
proved to help achieve comparable classification performance with much less
labelled data.",None,-1
L^3U-net: Low-Latency Lightweight U-net Based Image Segmentation Model for Parallel CNN Processors,0.0540508,"In this research, we propose a tiny image segmentation model, L^3U-net, that
works on low-resource edge devices in real-time. We introduce a data folding
technique that reduces inference latency by leveraging the parallel
convolutional layer processing capability of the CNN accelerators. We also
deploy the proposed model to such a device, MAX78000, and the results show that
L^3U-net achieves more than 90% accuracy over two different segmentation
datasets with 10 fps.",None,-1
Object Permanence Emerges in a Random Walk along Memory,0.0937389,"This paper proposes a self-supervised objective for learning representations
that localize objects under occlusion - a property known as object permanence.
A central question is the choice of learning signal in cases of total
occlusion. Rather than directly supervising the locations of invisible objects,
we propose a self-supervised objective that requires neither human annotation,
nor assumptions about object dynamics. We show that object permanence can
emerge by optimizing for temporal coherence of memory: we fit a Markov walk
along a space-time graph of memories, where the states in each time step are
non-Markovian features from a sequence encoder. This leads to a memory
representation that stores occluded objects and predicts their motion, to
better localize them. The resulting model outperforms existing approaches on
several datasets of increasing complexity and realism, despite requiring
minimal supervision, and hence being broadly applicable.",None,-1
FrOoDo: Framework for Out-of-Distribution Detection,0.0673319,"FrOoDo is an easy-to-use and flexible framework for Out-of-Distribution
detection tasks in digital pathology. It can be used with PyTorch
classification and segmentation models, and its modular design allows for easy
extension. The goal is to automate the task of OoD Evaluation such that
research can focus on the main goal of either designing new models, new methods
or evaluating a new dataset. The code can be found at
https://github.com/MECLabTUDA/FrOoDo.",None,-1
Otsu based Differential Evolution Method for Image Segmentation,0.0129284,"This paper proposes an OTSU based differential evolution method for satellite
image segmentation and compares it with four other methods such as Modified
Artificial Bee Colony Optimizer (MABC), Artificial Bee Colony (ABC), Genetic
Algorithm (GA), and Particle Swarm Optimization (PSO) using the objective
function proposed by Otsu for optimal multilevel thresholding. The experiments
conducted and their results illustrate that our proposed DE and OTSU algorithm
segmentation can effectively and precisely segment the input image, close to
results obtained by the other methods. In the proposed DE and OTSU algorithm,
instead of passing the fitness function variables, the entire image is passed
as an input to the DE algorithm after obtaining the threshold values for the
input number of levels in the OTSU algorithm. The image segmentation results
are obtained after learning about the image instead of learning about the
fitness variables. In comparison to other segmentation methods examined, the
proposed DE and OTSU algorithm yields promising results with minimized
computational time compared to some algorithms.",None,-1
Emotional Speech Recognition with Pre-trained Deep Visual Models,0.0372791,"In this paper, we propose a new methodology for emotional speech recognition
using visual deep neural network models. We employ the transfer learning
capabilities of the pre-trained computer vision deep models to have a mandate
for the emotion recognition in speech task. In order to achieve that, we
propose to use a composite set of acoustic features and a procedure to convert
them into images. Besides, we present a training paradigm for these models
taking into consideration the different characteristics between acoustic-based
images and regular ones. In our experiments, we use the pre-trained VGG-16
model and test the overall methodology on the Berlin EMO-DB dataset for
speaker-independent emotion recognition. We evaluate the proposed model on the
full list of the seven emotions and the results set a new state-of-the-art.",https://github.com/mehdi-mirzapour/Emotional_Speech_Recognition,-1
Learning to embed semantic similarity for joint image-text retrieval,0.0156948,"We present a deep learning approach for learning the joint semantic
embeddings of images and captions in a Euclidean space, such that the semantic
similarity is approximated by the L2 distances in the embedding space. For
that, we introduce a metric learning scheme that utilizes multitask learning to
learn the embedding of identical semantic concepts using a center loss. By
introducing a differentiable quantization scheme into the end-to-end trainable
network, we derive a semantic embedding of semantically similar concepts in
Euclidean space. We also propose a novel metric learning formulation using an
adaptive margin hinge loss, that is refined during the training phase. The
proposed scheme was applied to the MS-COCO, Flicke30K and Flickr8K datasets,
and was shown to compare favorably with contemporary state-of-the-art
approaches.",https://github.com/KunpengLi1994/VSRN,-1
Efficiency Comparison of AI classification algorithms for Image Detection and Recognition in Real-time,0.00172936,"Face detection and identification is the most difficult and often used task
in Artificial Intelligence systems. The goal of this study is to present and
compare the results of several face detection and recognition algorithms used
in the system. This system begins with a training image of a human, then
continues on to the test image, identifying the face, comparing it to the
trained face, and finally classifying it using OpenCV classifiers. This
research will discuss the most effective and successful tactics used in the
system, which are implemented using Python, OpenCV, and Matplotlib. It may also
be used in locations with CCTV, such as public spaces, shopping malls, and ATM
booths.",None,-1
CYBORGS: Contrastively Bootstrapping Object Representations by Grounding in Segmentation,0.0081173,"Many recent approaches in contrastive learning have worked to close the gap
between pretraining on iconic images like ImageNet and pretraining on complex
scenes like COCO. This gap exists largely because commonly used random crop
augmentations obtain semantically inconsistent content in crowded scene images
of diverse objects. Previous works use preprocessing pipelines to localize
salient objects for improved cropping, but an end-to-end solution is still
elusive. In this work, we propose a framework which accomplishes this goal via
joint learning of representations and segmentation. We leverage segmentation
masks to train a model with a mask-dependent contrastive loss, and use the
partially trained model to bootstrap better masks. By iterating between these
two components, we ground the contrastive updates in segmentation information,
and simultaneously improve segmentation throughout pretraining. Experiments
show our representations transfer robustly to downstream tasks in
classification, detection and segmentation.",https://github.com/renwang435/CYBORGS,-1
ANNA: Enhanced Language Representation for Question Answering,0.00650804,"Pre-trained language models have brought significant improvements in
performance in a variety of natural language processing tasks. Most existing
models performing state-of-the-art results have shown their approaches in the
separate perspectives of data processing, pre-training tasks, neural network
modeling, or fine-tuning. In this paper, we demonstrate how the approaches
affect performance individually, and that the language model performs the best
results on a specific question answering task when those approaches are jointly
considered in pre-training models. In particular, we propose an extended
pre-training task, and a new neighbor-aware mechanism that attends neighboring
tokens more to capture the richness of context for pre-training language
modeling. Our best model achieves new state-of-the-art results of 95.7\% F1 and
90.6\% EM on SQuAD 1.1 and also outperforms existing pre-trained language
models such as RoBERTa, ALBERT, ELECTRA, and XLNet on the SQuAD 2.0 benchmark.",None,-1
Modeling sequential annotations for sequence labeling with crowds,0.0161318,"Crowd sequential annotations can be an efficient and cost-effective way to
build large datasets for sequence labeling. Different from tagging independent
instances, for crowd sequential annotations the quality of label sequence
relies on the expertise level of annotators in capturing internal dependencies
for each token in the sequence. In this paper, we propose Modeling sequential
annotation for sequence labeling with crowds (SA-SLC). First, a conditional
probabilistic model is developed to jointly model sequential data and
annotators' expertise, in which categorical distribution is introduced to
estimate the reliability of each annotator in capturing local and non-local
label dependency for sequential annotation. To accelerate the marginalization
of the proposed model, a valid label sequence inference (VLSE) method is
proposed to derive the valid ground-truth label sequences from crowd sequential
annotations. VLSE derives possible ground-truth labels from the token-wise
level and further prunes sub-paths in the forward inference for label sequence
decoding. VLSE reduces the number of candidate label sequences and improves the
quality of possible ground-truth label sequences. The experimental results on
several sequence labeling tasks of Natural Language Processing show the
effectiveness of the proposed model.",None,-1
3rd Place Solution for Google Universal Image Embedding,0.034779,"This paper presents the 3rd place solution to the Google Universal Image
Embedding Competition on Kaggle. We use ViT-H/14 from OpenCLIP for the backbone
of ArcFace, and trained in 2 stage. 1st stage is done with freezed backbone,
and 2nd stage is whole model training. We achieve 0.692 mean Precision @5 on
private leaderboard. Code available at
https://github.com/YasumasaNamba/google-universal-image-embedding",https://github.com/YasumasaNamba/google-universal-image-embedding,-1
Tapping the Potential of Coherence and Syntactic Features in Neural Models for Automatic Essay Scoring,0.00642481,"In the prompt-specific holistic score prediction task for Automatic Essay
Scoring, the general approaches include pre-trained neural model, coherence
model, and hybrid model that incorporate syntactic features with neural model.
In this paper, we propose a novel approach to extract and represent essay
coherence features with prompt-learning NSP that shows to match the
state-of-the-art AES coherence model, and achieves the best performance for
long essays. We apply syntactic feature dense embedding to augment BERT-based
model and achieve the best performance for hybrid methodology for AES. In
addition, we explore various ideas to combine coherence, syntactic information
and semantic embeddings, which no previous study has done before. Our combined
model also performs better than the SOTA available for combined model, even
though it does not outperform our syntactic enhanced neural model. We further
offer analyses that can be useful for future study.",None,-1
Subsampling for Knowledge Graph Embedding Explained,0.0,"In this article, we explain the recent advance of subsampling methods in
knowledge graph embedding (KGE) starting from the original one used in
word2vec.",None,-1
Exploring Wasserstein Distance across Concept Embeddings for Ontology Matching,0.0261553,"Measuring the distance between ontological elements is fundamental for
ontology matching. String-based distance metrics are notorious for shallow
syntactic matching. In this exploratory study, we investigate Wasserstein
distance targeting continuous space that can incorporate various types of
information. We use a pre-trained word embeddings system to embed ontology
element labels. We examine the effectiveness of Wasserstein distance for
measuring similarity between ontologies, and discovering and refining matchings
between individual elements. Our experiments with the OAEI conference track and
MSE benchmarks achieved competitive results compared to the leading systems.",https://github.com/EngyNasr/MSE-Benchmark,-1
Towards a Responsible AI Development Lifecycle: Lessons From Information Security,0.0816087,"Legislation and public sentiment throughout the world have promoted fairness
metrics, explainability, and interpretability as prescriptions for the
responsible development of ethical artificial intelligence systems. Despite the
importance of these three pillars in the foundation of the field, they can be
challenging to operationalize and attempts to solve the problems in production
environments often feel Sisyphean. This difficulty stems from a number of
factors: fairness metrics are computationally difficult to incorporate into
training and rarely alleviate all of the harms perpetrated by these systems.
Interpretability and explainability can be gamed to appear fair, may
inadvertently reduce the privacy of personal information contained in training
data, and increase user confidence in predictions -- even when the explanations
are wrong. In this work, we propose a framework for responsibly developing
artificial intelligence systems by incorporating lessons from the field of
information security and the secure development lifecycle to overcome
challenges associated with protecting users in adversarial settings. In
particular, we propose leveraging the concepts of threat modeling, design
review, penetration testing, and incident response in the context of developing
AI systems as ways to resolve shortcomings in the aforementioned methods.",None,-1
Understanding Long Programming Languages with Structure-Aware Sparse Attention,0.019414,"Programming-based Pre-trained Language Models (PPLMs) such as CodeBERT have
achieved great success in many downstream code-related tasks. Since the memory
and computational complexity of self-attention in the Transformer grow
quadratically with the sequence length, PPLMs typically limit the code length
to 512. However, codes in real-world applications are generally long, such as
code searches, which cannot be processed efficiently by existing PPLMs. To
solve this problem, in this paper, we present SASA, a Structure-Aware Sparse
Attention mechanism, which reduces the complexity and improves performance for
long code understanding tasks. The key components in SASA are top-$k$ sparse
attention and Abstract Syntax Tree (AST)-based structure-aware attention. With
top-$k$ sparse attention, the most crucial attention relation can be obtained
with a lower computational cost. As the code structure represents the logic of
the code statements, which is a complement to the code sequence
characteristics, we further introduce AST structures into attention. Extensive
experiments on CodeXGLUE tasks show that SASA achieves better performance than
the competing baselines.",None,-1
TextMatcher: Cross-Attentional Neural Network to Compare Image and Text,0.0054497,"We study a novel multimodal-learning problem, which we call text matching:
given an image containing a single-line text and a candidate text
transcription, the goal is to assess whether the text represented in the image
corresponds to the candidate text. We devise the first machine-learning model
specifically designed for this problem. The proposed model, termed TextMatcher,
compares the two inputs by applying a cross-attention mechanism over the
embedding representations of image and text, and it is trained in an end-to-end
fashion. We extensively evaluate the empirical performance of TextMatcher on
the popular IAM dataset. Results attest that, compared to a baseline and
existing models designed for related problems, TextMatcher achieves higher
performance on a variety of configurations, while at the same time running
faster at inference time. We also showcase TextMatcher in a real-world
application scenario concerning the automatic processing of bank cheques.",https://github.com/ayumiymk/aster.pytorch,-1
Meta-Learning Parameterized Skills,0.00853412,"We propose a novel parameterized skill-learning algorithm that aims to learn
transferable parameterized skills and synthesize them into a new action space
that supports efficient learning in long-horizon tasks. We propose to leverage
off-policy Meta-RL combined with a trajectory-centric smoothness term to learn
a set of parameterized skills. Our agent can use these learned skills to
construct a three-level hierarchical framework that models a
Temporally-extended Parameterized Action Markov Decision Process. We
empirically demonstrate that the proposed algorithms enable an agent to solve a
set of difficult long-horizon (obstacle-course and robot manipulation) tasks.",https://github.com/Minusadd/,-1
A Closer Look at Branch Classifiers of Multi-exit Architectures,0.0793312,"Multi-exit architectures consist of a backbone and branch classifiers that
offer shortened inference pathways to reduce the run-time of deep neural
networks. In this paper, we analyze different branching patterns that vary in
their allocation of computational complexity for the branch classifiers.
Constant-complexity branching keeps all branches the same, while
complexity-increasing and complexity-decreasing branching place more complex
branches later or earlier in the backbone respectively. Through extensive
experimentation on multiple backbones and datasets, we find that
complexity-decreasing branches are more effective than constant-complexity or
complexity-increasing branches, which achieve the best accuracy-cost trade-off.
We investigate a cause by using knowledge consistency to probe the effect of
adding branches onto a backbone. Our findings show that complexity-decreasing
branching yields the least disruption to the feature abstraction hierarchy of
the backbone, which explains the effectiveness of the branching patterns.",https://github.com/pytorch/examples/tree/master/imagenet,-1
A new approach to calculating BERTScore for automatic assessment of translation quality,0.053701,"The study of the applicability of the BERTScore metric was conducted to
translation quality assessment at the sentence level for English -> Russian
direction. Experiments were performed with a pre-trained Multilingual BERT as
well as with a pair of Monolingual BERT models. To align monolingual
embeddings, an orthogonal transformation based on anchor tokens was used. It
was demonstrated that such transformation helps to prevent mismatching issue
and shown that this approach gives better results than using embeddings of the
Multilingual model. To improve the token matching process it is proposed to
combine all incomplete WorkPiece tokens into meaningful words and use simple
averaging of corresponding vectors and to calculate BERTScore based on anchor
tokens only. Such modifications allowed us to achieve a better correlation of
the model predictions with human judgments. In addition to evaluating machine
translation, several versions of human translation were evaluated as well, the
problems of this approach were listed.",None,-1
KeypartX: Graph-based Perception (Text) Representation,0.0149257,"The availability of big data has opened up big opportunities for individuals,
businesses and academics to view big into what is happening in their world.
Previous works of text representation mostly focused on informativeness from
massive words' frequency or cooccurrence. However, big data is a double-edged
sword which is big in volume but unstructured in format. The unstructured edge
requires specific techniques to transform 'big' into meaningful instead of
informative alone.
  This study presents KeypartX, a graph-based approach to represent perception
(text in general) by key parts of speech. Different from
bag-of-words/vector-based machine learning, this technique is human-like
learning that could extracts meanings from linguistic (semantic, syntactic and
pragmatic) information. Moreover, KeypartX is big-data capable but not hungry,
which is even applicable to the minimum unit of text:sentence.",https://github.com/pengKiina/KeypartX,-1
Motivating explanations in Bayesian networks using MAP-independence,0.0657923,"In decision support systems the motivation and justification of the system's
diagnosis or classification is crucial for the acceptance of the system by the
human user. In Bayesian networks a diagnosis or classification is typically
formalized as the computation of the most probable joint value assignment to
the hypothesis variables, given the observed values of the evidence variables
(generally known as the MAP problem). While solving the MAP problem gives the
most probable explanation of the evidence, the computation is a black box as
far as the human user is concerned and it does not give additional insights
that allow the user to appreciate and accept the decision. For example, a user
might want to know to whether an unobserved variable could potentially (upon
observation) impact the explanation, or whether it is irrelevant in this
aspect. In this paper we introduce a new concept, MAP- independence, which
tries to capture this notion of relevance, and explore its role towards a
potential justification of an inference to the best explanation. We formalize
several computational problems based on this concept and assess their
computational complexity.",https://gitlab.socsci.ru.nl/j.kwisthout/,-1
Brain Principles Programming,0.0167866,"In the monograph, STRONG ARTIFICIAL INTELLIGENCE. On the Approaches to
Superintelligence, published by Sberbank, provides a cross-disciplinary review
of general artificial intelligence. As an anthropomorphic direction of
research, it considers Brain Principles Programming, BPP) the formalization of
universal mechanisms (principles) of the brain's work with information, which
are implemented at all levels of the organization of nervous tissue. This
monograph provides a formalization of these principles in terms of the category
theory. However, this formalization is not enough to develop algorithms for
working with information. In this paper, for the description and modeling of
Brain Principles Programming, it is proposed to apply mathematical models and
algorithms developed by us earlier that model cognitive functions, which are
based on well-known physiological, psychological and other natural science
theories. The paper uses mathematical models and algorithms of the following
theories: P.K.Anokhin's Theory of Functional Brain Systems, Eleonor Rosh's
prototypical categorization theory, Bob Rehter's theory of causal models and
natural classification. As a result, the formalization of the BPP is obtained
and computer examples are given that demonstrate the algorithm's operation.",None,-1
Zero-Shot Classification by Logical Reasoning on Natural Language Explanations,0.0245557,"Humans can classify data of an unseen category by reasoning on its language
explanations. This ability is owing to the compositional nature of language: we
can combine previously seen attributes to describe the new category. For
example, we might describe a sage thrasher as ""it has a slim straight
relatively short bill, yellow eyes and a long tail"", so that others can use
their knowledge of attributes ""slim straight relatively short bill"", ""yellow
eyes"" and ""long tail"" to recognize a sage thrasher. Inspired by this
observation, in this work we tackle zero-shot classification task by logically
parsing and reasoning on natural language expla-nations. To this end, we
propose the framework CLORE (Classification by LOgical Reasoning on
Explanations). While previous methods usually regard textual information as
implicit features, CLORE parses explanations into logical structures and then
explicitly reasons along thess structures on the input to produce a
classification score. Experimental results on explanation-based zero-shot
classification benchmarks demonstrate that CLORE is superior to baselines,
which we further show mainly comes from higher scores on tasks requiring more
logical reasoning. We also demonstrate that our framework can be extended to
zero-shot classification on visual modality. Alongside classification
decisions, CLORE can provide the logical parsing and reasoning process as a
clear form of rationale. Through empirical analysis we demonstrate that CLORE
is also less affected by linguistic biases than baselines.",https://github.com/Glaciohound/CLORE,-1
PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN,0.0428207,"Beyond topical relevance, passage ranking for open-domain factoid question
answering also requires a passage to contain an answer (answerability). While a
few recent studies have incorporated some reading capability into a ranker to
account for answerability, the ranker is still hindered by the noisy nature of
the training data typically available in this area, which considers any passage
containing an answer entity as a positive sample. However, the answer entity in
a passage is not necessarily mentioned in relation with the given question. To
address the problem, we propose an approach called \ttt{PReGAN} for Passage
Reranking based on Generative Adversarial Neural networks, which incorporates a
discriminator on answerability, in addition to a discriminator on topical
relevance. The goal is to force the generator to rank higher a passage that is
topically relevant and contains an answer. Experiments on five public datasets
show that \ttt{PReGAN} can better rank appropriate passages, which in turn,
boosts the effectiveness of QA systems, and outperforms the existing approaches
without using external data.",https://github.com/facebookresearch/DPR,-1
A General Framework for Modelling Conditional Reasoning -- Preliminary Report,0.0351241,"We introduce and investigate here a formalisation for conditionals that
allows the definition of a broad class of reasoning systems. This framework
covers the most popular kinds of conditional reasoning in logic-based KR: the
semantics we propose is appropriate for a structural analysis of those
conditionals that do not satisfy closure properties associated to classical
logics.",None,-1
A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers,0.00493091,"Instead of using current deep-learning segmentation models (like the UNet and
variants), we approach the segmentation problem using trained Convolutional
Neural Network (CNN) classifiers, which automatically extract important
features from images for classification. Those extracted features can be
visualized and formed into heatmaps using Gradient-weighted Class Activation
Mapping (Grad-CAM). This study tested whether the heatmaps could be used to
segment the classified targets. We also proposed an evaluation method for the
heatmaps; that is, to re-train the CNN classifier using images filtered by
heatmaps and examine its performance. We used the mean-Dice coefficient to
evaluate segmentation results. Results from our experiments show that heatmaps
can locate and segment partial tumor areas. But use of only the heatmaps from
CNN classifiers may not be an optimal approach for segmentation. We have
verified that the predictions of CNN classifiers mainly depend on tumor areas,
and dark regions in Grad-CAM's heatmaps also contribute to classification.",None,-1
Elimination of Non-Novel Segments at Multi-Scale for Few-Shot Segmentation,0.0132412,"Few-shot segmentation aims to devise a generalizing model that segments query
images from unseen classes during training with the guidance of a few support
images whose class tally with the class of the query. There exist two
domain-specific problems mentioned in the previous works, namely spatial
inconsistency and bias towards seen classes. Taking the former problem into
account, our method compares the support feature map with the query feature map
at multi scales to become scale-agnostic. As a solution to the latter problem,
a supervised model, called as base learner, is trained on available classes to
accurately identify pixels belonging to seen classes. Hence, subsequent meta
learner has a chance to discard areas belonging to seen classes with the help
of an ensemble learning model that coordinates meta learner with the base
learner. We simultaneously address these two vital problems for the first time
and achieve state-of-the-art performances on both PASCAL-5i and COCO-20i
datasets.",None,-1
Dual Mechanism Priming Effects in Hindi Word Order,0.0130436,"Word order choices during sentence production can be primed by preceding
sentences. In this work, we test the DUAL MECHANISM hypothesis that priming is
driven by multiple different sources. Using a Hindi corpus of text productions,
we model lexical priming with an n-gram cache model and we capture more
abstract syntactic priming with an adaptive neural language model. We permute
the preverbal constituents of corpus sentences, and then use a logistic
regression model to predict which sentences actually occurred in the corpus
against artificially generated meaning-equivalent variants. Our results
indicate that lexical priming and lexically-independent syntactic priming
affect complementary sets of verb classes. By showing that different priming
influences are separable from one another, our results support the hypothesis
that multiple different cognitive mechanisms underlie priming.",https://github.com/vansky/neural-complexity,-1
Syntactic Substitutability as Unsupervised Dependency Syntax,0.0118561,"Syntax is a latent hierarchical structure which underpins the robust and
compositional nature of human language. In this work, we explore the hypothesis
that syntactic dependencies can be represented in language model attention
distributions and propose a new method to induce these structures
theory-agnostically. Instead of modeling syntactic relations as defined by
annotation schemata, we model a more general property implicit in the
definition of dependency relations, syntactic substitutability. This property
captures the fact that words at either end of a dependency can be substituted
with words from the same category. Substitutions can be used to generate a set
of syntactically invariant sentences whose representations are then used for
parsing. We show that increasing the number of substitutions used improves
parsing accuracy on natural data. On long-distance subject-verb agreement
constructions, our method achieves 79.5% recall compared to 8.9% using a
previous method. Our method also provides improvements when transferred to a
different parsing setup, demonstrating that it generalizes.",https://github.com/McGill-NLP/syntactic-substitutability,-1
UCEpic: Unifying Aspect Planning and Lexical Constraints for Generating Explanations in Recommendation,0.0255613,"Personalized natural language generation for explainable recommendations
plays a key role in justifying why a recommendation might match a user's
interests. Existing models usually control the generation process by aspect
planning. While promising, these aspect-planning methods struggle to generate
specific information correctly, which prevents generated explanations from
being convincing. In this paper, we claim that introducing lexical constraints
can alleviate the above issues. We propose a model, UCEpic, that generates
high-quality personalized explanations for recommendation results by unifying
aspect planning and lexical constraints in an insertion-based generation
manner.
  Methodologically, to ensure text generation quality and robustness to various
lexical constraints, we pre-train a non-personalized text generator via our
proposed robust insertion process. Then, to obtain personalized explanations
under this framework of insertion-based generation, we design a method of
incorporating aspect planning and personalized references into the insertion
process. Hence, UCEpic unifies aspect planning and lexical constraints into one
framework and generates explanations for recommendations under different
settings. Compared to previous recommendation explanation generators controlled
by only aspects, UCEpic incorporates specific information from keyphrases and
then largely improves the diversity and informativeness of generated
explanations for recommendations on datasets such as RateBeer and Yelp.",https://github.com/JiachengLi1995/UCEpic,-1
Playing Tic-Tac-Toe Games with Intelligent Single-pixel Imaging,0.0036696,"Single-pixel imaging (SPI) is a novel optical imaging technique by replacing
a two-dimensional pixelated sensor with a single-pixel detector and pattern
illuminations. SPI have been extensively used for various tasks related to
image acquisition and processing. In this work, a novel non-image-based task of
playing Tic-Tac-Toe games interactively is merged into the framework of SPI. An
optoelectronic artificial intelligent (AI) player with minimal digital
computation can detect the game states, generate optimal moves and display
output results mainly by pattern illumination and single-pixel detection.
Simulated and experimental results demonstrate the feasibility of proposed
scheme and its unbeatable performance against human players.",None,-1
Multiple Attribute Fairness: Application to Fraud Detection,0.022515,"We propose a fairness measure relaxing the equality conditions in the popular
equal odds fairness regime for classification. We design an iterative,
model-agnostic, grid-based heuristic that calibrates the outcomes per sensitive
attribute value to conform to the measure. The heuristic is designed to handle
high arity attribute values and performs a per attribute sanitization of
outcomes across different protected attribute values. We also extend our
heuristic for multiple attributes. Highlighting our motivating application,
fraud detection, we show that the proposed heuristic is able to achieve
fairness across multiple values of a single protected attribute, multiple
protected attributes. When compared to current fairness techniques, that focus
on two groups, we achieve comparable performance across several public data
sets.",None,-1
Operator Sketching for Deep Unrolling Networks,0.00837579,"In this work we propose a new paradigm for designing efficient deep unrolling
networks using operator sketching. The deep unrolling networks are currently
the state-of-the-art solutions for imaging inverse problems. However, for
high-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI
imaging, the deep unrolling schemes typically become inefficient both in terms
of memory and computation, due to the need of computing multiple times the
high-dimensional forward and adjoint operators. Recently researchers have found
that such limitations can be partially addressed by stochastic unrolling with
subsets of operators, inspired by the success of stochastic first-order
optimization. In this work, we propose a further acceleration upon stochastic
unrolling, using sketching techniques to approximate products in the
high-dimensional image space. The operator sketching can be jointly applied
with stochastic unrolling for the best acceleration and compression
performance. Our numerical experiments on X-ray CT image reconstruction
demonstrate the remarkable effectiveness of our sketched unrolling schemes.",None,-1
Plumber: A Modular Framework to Create Information Extraction Pipelines,0.00869976,"Information Extraction (IE) tasks are commonly studied topics in various
domains of research. Hence, the community continuously produces multiple
techniques, solutions, and tools to perform such tasks. However, running those
tools and integrating them within existing infrastructure requires time,
expertise, and resources. One pertinent task here is triples extraction and
linking, where structured triples are extracted from a text and aligned to an
existing Knowledge Graph (KG). In this paper, we present PLUMBER, the first
framework that allows users to manually and automatically create suitable IE
pipelines from a community-created pool of tools to perform triple extraction
and alignment on unstructured text. Our approach provides an interactive medium
to alter the pipelines and perform IE tasks. A short video to show the working
of the framework for different use-cases is available online under:
https://www.youtube.com/watch?v=XC9rJNIUv8g",https://github.com/YaserJaradeh/ThePlumber,-1
Models and Benchmarks for Representation Learning of Partially Observed Subgraphs,0.00116404,"Subgraphs are rich substructures in graphs, and their nodes and edges can be
partially observed in real-world tasks. Under partial observation, existing
node- or subgraph-level message-passing produces suboptimal representations. In
this paper, we formulate a novel task of learning representations of partially
observed subgraphs. To solve this problem, we propose Partial Subgraph InfoMax
(PSI) framework and generalize existing InfoMax models, including DGI,
InfoGraph, MVGRL, and GraphCL, into our framework. These models maximize the
mutual information between the partial subgraph's summary and various
substructures from nodes to full subgraphs. In addition, we suggest a novel
two-stage model with $k$-hop PSI, which reconstructs the representation of the
full subgraph and improves its expressiveness from different local-global
structures. Under training and evaluation protocols designed for this problem,
we conduct experiments on three real-world datasets and demonstrate that PSI
models outperform baselines.",https://github.com/dongkwan-kim/PSI,-1
Discover Life Skills for Planning with Bandits via Observing and Learning How the World Works,0.0358427,"We propose a novel approach for planning agents to compose abstract skills
via observing and learning from historical interactions with the world. Our
framework operates in a Markov state-space model via a set of actions under
unknown pre-conditions. We formulate skills as high-level abstract policies
that propose action plans based on the current state. Each policy learns new
plans by observing the states' transitions while the agent interacts with the
world. Such an approach automatically learns new plans to achieve specific
intended effects, but the success of such plans is often dependent on the
states in which they are applicable. Therefore, we formulate the evaluation of
such plans as infinitely many multi-armed bandit problems, where we balance the
allocation of resources on evaluating the success probability of existing arms
and exploring new options. The result is a planner capable of automatically
learning robust high-level skills under a noisy environment; such skills
implicitly learn the action pre-condition without explicit knowledge. We show
that this planning approach is experimentally very competitive in
high-dimensional state space domains.",None,-1
EGCR: Explanation Generation for Conversational Recommendation,0.0248015,"Growing attention has been paid in Conversational Recommendation System
(CRS), which works as a conversation-based and recommendation task-oriented
tool to provide items of interest and explore user preference. However,
existing work in CRS fails to explicitly show the reasoning logic to users and
the whole CRS still remains a black box. Therefore we propose a novel
end-to-end framework named Explanation Generation for Conversational
Recommendation (EGCR) based on generating explanations for conversational
agents to explain why they make the action. EGCR incorporates user reviews to
enhance the item representation and increase the informativeness of the whole
conversation. To the best of our knowledge, this is the first framework for
explainable conversational recommendation on real-world datasets. Moreover, we
evaluate EGCR on one benchmark conversational recommendation datasets and
achieve better performance on both recommendation accuracy and conversation
quality than other state-of-the art models. Finally, extensive experiments
demonstrate that generated explanations are not only having high quality and
explainability, but also making CRS more trustworthy. We will make our code
available to contribute to the CRS community",None,-1
Deeper Insights into the Robustness of ViTs towards Common Corruptions,0.00929564,"With Vision Transformers (ViTs) making great advances in a variety of
computer vision tasks, recent literature have proposed various variants of
vanilla ViTs to achieve better efficiency and efficacy. However, it remains
unclear how their unique architecture impact robustness towards common
corruptions. In this paper, we make the first attempt to probe into the
robustness gap among ViT variants and explore underlying designs that are
essential for robustness. Through an extensive and rigorous benchmarking, we
demonstrate that simple architecture designs such as overlapping patch
embedding and convolutional feed-forward network (FFN) can promote the
robustness of ViTs. Moreover, since training ViTs relies heavily on data
augmentation, whether previous CNN-based augmentation strategies that are
targeted at robustness purposes can still be useful is worth investigating. We
explore different data augmentation on ViTs and verify that adversarial noise
training is powerful while fourier-domain augmentation is inferior. Based on
these findings, we introduce a novel conditional method of generating dynamic
augmentation parameters conditioned on input images, offering state-of-the-art
robustness towards common corruptions.",None,-1
Semantic Decomposition Improves Learning of Large Language Models on EHR Data,0.00936095,"Electronic health records (EHR) are widely believed to hold a profusion of
actionable insights, encrypted in an irregular, semi-structured format, amidst
a loud noise background. To simplify learning patterns of health and disease,
medical codes in EHR can be decomposed into semantic units connected by
hierarchical graphs. Building on earlier synergy between Bidirectional Encoder
Representations from Transformers (BERT) and Graph Attention Networks (GAT), we
present H-BERT, which ingests complete graph tree expansions of hierarchical
medical codes as opposed to only ingesting the leaves and pushes patient-level
labels down to each visit. This methodology significantly improves prediction
of patient membership in over 500 medical diagnosis classes as measured by
aggregated AUC and APS, and creates distinct representations of patients in
closely related but clinically distinct phenotypes.",None,-1
Transfer Learning for Instance Segmentation of Waste Bottles using Mask R-CNN Algorithm,0.00335485,"This paper proposes a methodological approach with a transfer learning scheme
for plastic waste bottle detection and instance segmentation using the
\textit{mask region proposal convolutional neural network} (Mask R-CNN).
Plastic bottles constitute one of the major pollutants posing a serious threat
to the environment both in oceans and on land. The automated identification and
segregation of bottles can facilitate plastic waste recycling. We prepare a
custom-made dataset of 192 bottle images with pixel-by pixel-polygon annotation
for the automatic segmentation task. The proposed transfer learning scheme
makes use of a Mask R-CNN model pre-trained on the Microsoft COCO dataset. We
present a comprehensive scheme for fine-tuning the base pre-trained Mask-RCNN
model on our custom dataset. Our final fine-tuned model has achieved 59.4
\textit{mean average precision} (mAP), which corresponds to the MS COCO metric.
The results indicate a promising application of deep learning for detecting
waste bottles.",https://github.com/matterport/Mask_RCNN,-1
Physically Plausible Animation of Human Upper Body from a Single Image,0.0318677,"We present a new method for generating controllable, dynamically responsive,
and photorealistic human animations. Given an image of a person, our system
allows the user to generate Physically plausible Upper Body Animation (PUBA)
using interaction in the image space, such as dragging their hand to various
locations. We formulate a reinforcement learning problem to train a dynamic
model that predicts the person's next 2D state (i.e., keypoints on the image)
conditioned on a 3D action (i.e., joint torque), and a policy that outputs
optimal actions to control the person to achieve desired goals. The dynamic
model leverages the expressiveness of 3D simulation and the visual realism of
2D videos. PUBA generates 2D keypoint sequences that achieve task goals while
being responsive to forceful perturbation. The sequences of keypoints are then
translated by a pose-to-image generator to produce the final photorealistic
video.",None,-1
"AI Technical Considerations: Data Storage, Cloud usage and AI Pipeline",0.00475701,"Artificial intelligence (AI), especially deep learning, requires vast amounts
of data for training, testing, and validation. Collecting these data and the
corresponding annotations requires the implementation of imaging biobanks that
provide access to these data in a standardized way. This requires careful
design and implementation based on the current standards and guidelines and
complying with the current legal restrictions. However, the realization of
proper imaging data collections is not sufficient to train, validate and deploy
AI as resource demands are high and require a careful hybrid implementation of
AI pipelines both on-premise and in the cloud. This chapter aims to help the
reader when technical considerations have to be made about the AI environment
by providing a technical background of different concepts and implementation
aspects involved in data storage, cloud usage, and AI pipelines.",None,-1
Was that so hard? Estimating human classification difficulty,0.0151078,"When doctors are trained to diagnose a specific disease, they learn faster
when presented with cases in order of increasing difficulty. This creates the
need for automatically estimating how difficult it is for doctors to classify a
given case. In this paper, we introduce methods for estimating how hard it is
for a doctor to diagnose a case represented by a medical image, both when
ground truth difficulties are available for training, and when they are not.
Our methods are based on embeddings obtained with deep metric learning.
Additionally, we introduce a practical method for obtaining ground truth human
difficulty for each image case in a dataset using self-assessed certainty. We
apply our methods to two different medical datasets, achieving high Kendall
rank correlation coefficients, showing that we outperform existing methods by a
large margin on our problem and data.",None,-1
Construction and Evaluation of a Self-Attention Model for Semantic Understanding of Sentence-Final Particles,0.0149324,"Sentence-final particles serve an essential role in spoken Japanese because
they express the speaker's mental attitudes toward a proposition and/or an
interlocutor. They are acquired at early ages and occur very frequently in
everyday conversation. However, there has been little proposal for a
computational model of acquiring sentence-final particles. This paper proposes
Subjective BERT, a self-attention model that takes various subjective senses in
addition to language and images as input and learns the relationship between
words and subjective senses. An evaluation experiment revealed that the model
understands the usage of ""yo"", which expresses the speaker's intention to
communicate new information, and that of ""ne"", which denotes the speaker's
desire to confirm that some information is shared.",None,-1
EvEntS ReaLM: Event Reasoning of Entity States via Language Models,0.0612679,"This paper investigates models of event implications. Specifically, how well
models predict entity state-changes, by targeting their understanding of
physical attributes. Nominally, Large Language models (LLM) have been exposed
to procedural knowledge about how objects interact, yet our benchmarking shows
they fail to reason about the world. Conversely, we also demonstrate that
existing approaches often misrepresent the surprising abilities of LLMs via
improper task encodings and that proper model prompting can dramatically
improve performance of reported baseline results across multiple tasks. In
particular, our results indicate that our prompting technique is especially
useful for unseen attributes (out-of-domain) or when only limited data is
available.",https://github.com/spilioeve/eventsrealm,-1
Skeptical binary inferences in multi-label problems with sets of probabilities,0.00670648,"In this paper, we consider the problem of making distributionally robust,
skeptical inferences for the multi-label problem, or more generally for Boolean
vectors. By distributionally robust, we mean that we consider a set of possible
probability distributions, and by skeptical we understand that we consider as
valid only those inferences that are true for every distribution within this
set. Such inferences will provide partial predictions whenever the considered
set is sufficiently big. We study in particular the Hamming loss case, a common
loss function in multi-label problems, showing how skeptical inferences can be
made in this setting. Our experimental results are organised in three sections;
(1) the first one indicates the gain computational obtained from our
theoretical results by using synthetical data sets, (2) the second one
indicates that our approaches produce relevant cautiousness on those
hard-to-predict instances where its precise counterpart fails, and (3) the last
one demonstrates experimentally how our approach copes with imperfect
information (generated by a downsampling procedure) better than the partial
abstention [31] and the rejection rules.",https://github.com/sdestercke/classifip,-1
Best-$k$ Search Algorithm for Neural Text Generation,0.03091,"Modern natural language generation paradigms require a good decoding strategy
to obtain quality sequences out of the model. Beam search yields high-quality
but low diversity outputs; stochastic approaches suffer from high variance and
sometimes low quality, but the outputs tend to be more natural and creative. In
this work, we propose a deterministic search algorithm balancing both quality
and diversity. We first investigate the vanilla best-first search (BFS)
algorithm and then propose the Best-$k$ Search algorithm. Inspired by BFS, we
greedily expand the top $k$ nodes, instead of only the first node, to boost
efficiency and diversity. Upweighting recently discovered nodes accompanied by
heap pruning ensures the completeness of the search procedure. Experiments on
four NLG tasks, including question generation, commonsense generation, text
summarization, and translation, show that best-$k$ search yields more diverse
and natural outputs compared to strong baselines, while our approach maintains
high text quality. The proposed algorithm is parameter-free, lightweight,
efficient, and easy to use.",None,-1
On an Edge-Preserving Variational Model for Optical Flow Estimation,0.00134158,"It is well known that classical formulations resembling the Horn and Schunck
model are still largely competitive due to the modern implementation practices.
In most cases, these models outperform many modern flow estimation methods. In
view of this, we propose an effective implementation design for an
edge-preserving $L^1$ regularization approach to optical flow. The mathematical
well-posedness of our proposed model is studied in the space of functions of
bounded variations $BV(\Omega,\mathbb{R}^2)$. The implementation scheme is
designed in multiple steps. The flow field is computed using the robust
Chambolle-Pock primal-dual algorithm. Motivated by the recent studies of Castro
and Donoho we extend the heuristic of iterated median filtering to our flow
estimation. Further, to refine the flow edges we use the weighted median filter
established by Li and Osher as a post-processing step. Our experiments on the
Middlebury dataset show that the proposed method achieves the best average
angular and end-point errors compared to some of the state-of-the-art Horn and
Schunck based variational methods.",None,-1
Sphere-Guided Training of Neural Implicit Surfaces,0.0509623,"In recent years, neural distance functions trained via volumetric ray
marching have been widely adopted for multi-view 3D reconstruction. These
methods, however, apply the ray marching procedure for the entire scene volume,
leading to reduced sampling efficiency and, as a result, lower reconstruction
quality in the areas of high-frequency details. In this work, we address this
problem via joint training of the implicit function and our new coarse
sphere-based surface reconstruction. We use the coarse representation to
efficiently exclude the empty volume of the scene from the volumetric ray
marching procedure without additional forward passes of the neural surface
network, which leads to an increased fidelity of the reconstructions compared
to the base systems. We evaluate our approach by incorporating it into the
training procedures of several implicit surface modeling methods and observe
uniform improvements across both synthetic and real-world datasets. Our
codebase can be accessed via the project page:
https://andreeadogaru.github.io/SphereGuided",https://andreeadogaru.github.io/SphereGuided,-1
CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection,0.139091,"In autonomous driving, LiDAR point-clouds and RGB images are two major data
modalities with complementary cues for 3D object detection. However, it is
quite difficult to sufficiently use them, due to large inter-modal
discrepancies. To address this issue, we propose a novel framework, namely
Contrastively Augmented Transformer for multi-modal 3D object Detection
(CAT-Det). Specifically, CAT-Det adopts a two-stream structure consisting of a
Pointformer (PT) branch, an Imageformer (IT) branch along with a Cross-Modal
Transformer (CMT) module. PT, IT and CMT jointly encode intra-modal and
inter-modal long-range contexts for representing an object, thus fully
exploring multi-modal information for detection. Furthermore, we propose an
effective One-way Multi-modal Data Augmentation (OMDA) approach via
hierarchical contrastive learning at both the point and object levels,
significantly improving the accuracy only by augmenting point-clouds, which is
free from complex generation of paired samples of the two modalities. Extensive
experiments on the KITTI benchmark show that CAT-Det achieves a new
state-of-the-art, highlighting its effectiveness.",None,-1
Differentiable Logics for Neural Network Training and Verification,0.0515241,"The rising popularity of neural networks (NNs) in recent years and their
increasing prevalence in real-world applications have drawn attention to the
importance of their verification. While verification is known to be
computationally difficult theoretically, many techniques have been proposed for
solving it in practice. It has been observed in the literature that by default
neural networks rarely satisfy logical constraints that we want to verify. A
good course of action is to train the given NN to satisfy said constraint prior
to verifying them. This idea is sometimes referred to as continuous
verification, referring to the loop between training and verification. Usually
training with constraints is implemented by specifying a translation for a
given formal logic language into loss functions. These loss functions are then
used to train neural networks. Because for training purposes these functions
need to be differentiable, these translations are called differentiable logics
(DL). This raises several research questions. What kind of differentiable
logics are possible? What difference does a specific choice of DL make in the
context of continuous verification? What are the desirable criteria for a DL
viewed from the point of view of the resulting loss function? In this extended
abstract we will discuss and answer these questions.",None,-1
Biometric identification by means of hand geometry and a neural net classifier,0.113767,"This Paper describes a hand geometry biometric identification system. We have
acquired a database of 22 people using a conventional document scanner. The
experimental section consists of a study about the discrimination capability of
different extracted features, and the identification rate using different
classifiers based on neural networks.",None,-1
A Multi-modal Registration and Visualization Software Tool for Artworks using CraquelureNet,0.0,"For art investigations of paintings, multiple imaging technologies, such as
visual light photography, infrared reflectography, ultraviolet fluorescence
photography, and x-radiography are often used. For a pixel-wise comparison, the
multi-modal images have to be registered. We present a registration and
visualization software tool, that embeds a convolutional neural network to
extract cross-modal features of the crack structures in historical paintings
for automatic registration. The graphical user interface processes the user's
input to configure the registration parameters and to interactively adapt the
image views with the registered pair and image overlays, such as by individual
or synchronized zoom or movements of the views. In the evaluation, we
qualitatively and quantitatively show the effectiveness of our software tool in
terms of registration performance and short inference time on multi-modal
paintings and its transferability by applying our method to historical prints.",None,-1
2D Pose Estimation based Child Action Recognition,0.00670119,"We present a graph convolutional network with 2D pose estimation for the
first time on child action recognition task achieving on par results with an
RGB modality based model on a novel benchmark dataset containing unconstrained
environment based videos.",None,-1
Automatic Evaluation and Analysis of Idioms in Neural Machine Translation,0.213743,"A major open problem in neural machine translation (NMT) is the translation
of idiomatic expressions, such as ""under the weather"". The meaning of these
expressions is not composed by the meaning of their constituent words, and NMT
models tend to translate them literally (i.e., word-by-word), which leads to
confusing and nonsensical translations. Research on idioms in NMT is limited
and obstructed by the absence of automatic methods for quantifying these
errors. In this work, first, we propose a novel metric for automatically
measuring the frequency of literal translation errors without human
involvement. Equipped with this metric, we present controlled translation
experiments with models trained in different conditions (with/without the
test-set idioms) and across a wide range of (global and targeted) metrics and
test sets. We explore the role of monolingual pretraining and find that it
yields substantial targeted improvements, even without observing any
translation examples of the test-set idioms. In our analysis, we probe the role
of idiom context. We find that the randomly initialized models are more local
or ""myopic"" as they are relatively unaffected by variations of the idiom
context, unlike the pretrained ones.",https://github.com/amazon-research/idiom-mt,1321
TwistSLAM++: Fusing multiple modalities for accurate dynamic semantic SLAM,0.0446313,"Most classical SLAM systems rely on the static scene assumption, which limits
their applicability in real world scenarios. Recent SLAM frameworks have been
proposed to simultaneously track the camera and moving objects. However they
are often unable to estimate the canonical pose of the objects and exhibit a
low object tracking accuracy. To solve this problem we propose TwistSLAM++, a
semantic, dynamic, SLAM system that fuses stereo images and LiDAR information.
Using semantic information, we track potentially moving objects and associate
them to 3D object detections in LiDAR scans to obtain their pose and size.
Then, we perform registration on consecutive object scans to refine object pose
estimation. Finally, object scans are used to estimate the shape of the object
and constrain map points to lie on the estimated surface within the BA. We show
on classical benchmarks that this fusion approach based on multimodal
information improves the accuracy of object tracking.",https://github.com/facebookresearch/detectron2,1442
A Simple Strategy to Provable Invariance via Orbit Mapping,0.0,"Many applications require robustness, or ideally invariance, of neural
networks to certain transformations of input data. Most commonly, this
requirement is addressed by training data augmentation, using adversarial
training, or defining network architectures that include the desired invariance
by design. In this work, we propose a method to make network architectures
provably invariant with respect to group actions by choosing one element from a
(possibly continuous) orbit based on a fixed criterion. In a nutshell, we
intend to 'undo' any possible transformation before feeding the data into the
actual network. Further, we empirically analyze the properties of different
approaches which incorporate invariance via training or architecture, and
demonstrate the advantages of our method in terms of robustness and
computational efficiency. In particular, we investigate the robustness with
respect to rotations of images (which can hold up to discretization artifacts)
as well as the provable orientation and scaling invariance of 3D point cloud
classification.",https://github.com/QUVA-Lab/e2cnn_experiments,3895
BERT4Loc: BERT for Location -- POI Recommender System,0.0520889,"Recommending points of interest (POIs) is a challenging task that requires
extracting comprehensive location data from location-based social media
platforms. To provide effective location-based recommendations, it's important
to analyze users' historical behavior and preferences. In this study, we
present a sophisticated location-aware recommendation system that uses
Bidirectional Encoder Representations from Transformers (BERT) to offer
personalized location-based suggestions. Our model combines location
information and user preferences to provide more relevant recommendations
compared to models that predict the next POI in a sequence. Our experiments on
two benchmark dataset show that our BERT-based model outperforms various
state-of-the-art sequential models. Moreover, we see the effectiveness of the
proposed model for quality through additional experiments.",None,7807
Leaf: Multiple-Choice Question Generation,0.0700979,"Testing with quiz questions has proven to be an effective way to assess and
improve the educational process. However, manually creating quizzes is tedious
and time-consuming. To address this challenge, we present Leaf, a system for
generating multiple-choice questions from factual text. In addition to being
very well suited for the classroom, Leaf could also be used in an industrial
setting, e.g., to facilitate onboarding and knowledge sharing, or as a
component of chatbots, question answering systems, or Massive Open Online
Courses (MOOCs). The code and the demo are available on
https://github.com/KristiyanVachev/Leaf-Question-Generation.",https://github.com/KristiyanVachev/Leaf-Question-Generation,22966
Deep Vehicle Detection in Satellite Video,0.129491,"This work presents a deep learning approach for vehicle detection in
satellite video. Vehicle detection is perhaps impossible in single EO satellite
images due to the tininess of vehicles (4-10 pixel) and their similarity to the
background. Instead, we consider satellite video which overcomes the lack of
spatial information by temporal consistency of vehicle movement. A new
spatiotemporal model of a compact $3 \times 3$ convolutional, neural network is
proposed which neglects pooling layers and uses leaky ReLUs. Then we use a
reformulation of the output heatmap including Non-Maximum-Suppression (NMS) for
the final segmentation. Empirical results on two new annotated satellite videos
reconfirm the applicability of this approach for vehicle detection. They more
importantly indicate that pre-training on WAMI data and then fine-tuning on few
annotated video frames for a new video is sufficient. In our experiment only
five annotated images yield a $F_1$ score of 0.81 on a new video showing more
complex traffic patterns than the Las Vegas video. Our best result on Las Vegas
is a $F_1$ score of 0.87 which makes the proposed approach a leading method for
this benchmark.",None,6296
Autoregressive GAN for Semantic Unconditional Head Motion Generation,0.00525344,"In this work, we address the task of unconditional head motion generation to
animate still human faces in a low-dimensional semantic space from a single
reference pose. Different from traditional audio-conditioned talking head
generation that seldom puts emphasis on realistic head motions, we devise a
GAN-based architecture that learns to synthesize rich head motion sequences
over long duration while maintaining low error accumulation levels.In
particular, the autoregressive generation of incremental outputs ensures smooth
trajectories, while a multi-scale discriminator on input pairs drives
generation toward better handling of high- and low-frequency signals and less
mode collapse.We experimentally demonstrate the relevance of the proposed
method and show its superiority compared to models that attained
state-of-the-art performances on similar tasks.",https://github.com/LouisBearing/UnconditionalHeadMotion,4332
A New Amharic Speech Emotion Dataset and Classification Benchmark,0.167651,"In this paper we present the Amharic Speech Emotion Dataset (ASED), which
covers four dialects (Gojjam, Wollo, Shewa and Gonder) and five different
emotions (neutral, fearful, happy, sad and angry). We believe it is the first
Speech Emotion Recognition (SER) dataset for the Amharic language. 65 volunteer
participants, all native speakers, recorded 2,474 sound samples, two to four
seconds in length. Eight judges assigned emotions to the samples with high
agreement level (Fleiss kappa = 0.8). The resulting dataset is freely available
for download. Next, we developed a four-layer variant of the well-known VGG
model which we call VGGb. Three experiments were then carried out using VGGb
for SER, using ASED. First, we investigated whether Mel-spectrogram features or
Mel-frequency Cepstral coefficient (MFCC) features work best for Amharic. This
was done by training two VGGb SER models on ASED, one using Mel-spectrograms
and the other using MFCC. Four forms of training were tried, standard
cross-validation, and three variants based on sentences, dialects and speaker
groups. Thus, a sentence used for training would not be used for testing, and
the same for a dialect and speaker group. The conclusion was that MFCC features
are superior under all four training schemes. MFCC was therefore adopted for
Experiment 2, where VGGb and three other existing models were compared on ASED:
RESNet50, Alex-Net and LSTM. VGGb was found to have very good accuracy (90.73%)
as well as the fastest training time. In Experiment 3, the performance of VGGb
was compared when trained on two existing SER datasets, RAVDESS (English) and
EMO-DB (German) as well as on ASED (Amharic). Results are comparable across
these languages, with ASED being the highest. This suggests that VGGb can be
successfully applied to other languages. We hope that ASED will encourage
researchers to experiment with other models for Amharic SER.",https://github.com/Ethio2021/ASED_V1,2233
Incorporating Multi-armed Bandit with Local Search for MaxSAT,0.0120932,"Partial MaxSAT (PMS) and Weighted PMS (WPMS) are two practical
generalizations of the MaxSAT problem. In this paper, we propose a local search
algorithm for these problems, called BandHS, which applies two multi-armed
bandits to guide the search directions when escaping local optima. One bandit
is combined with all the soft clauses to help the algorithm select to satisfy
appropriate soft clauses, and the other bandit with all the literals in hard
clauses to help the algorithm select appropriate literals to satisfy the hard
clauses. These two bandits can improve the algorithm's search ability in both
feasible and infeasible solution spaces. We further propose an initialization
method for (W)PMS that prioritizes both unit and binary clauses when producing
the initial solutions. Extensive experiments demonstrate the excellent
performance and generalization capability of our proposed methods, that greatly
boost the state-of-the-art local search algorithm, SATLike3.0, and the
state-of-the-art SAT-based incomplete solver, NuWLS-c.",https://github.com/JHL-HUST/BandHS/,5393
Bias-Scalable Near-Memory CMOS Analog Processor for Machine Learning,0.0356846,"Bias-scalable analog computing is attractive for implementing machine
learning (ML) processors with distinct power-performance specifications. For
instance, ML implementations for server workloads are focused on higher
computational throughput for faster training, whereas ML implementations for
edge devices are focused on energy-efficient inference. In this paper, we
demonstrate the implementation of bias-scalable approximate analog computing
circuits using the generalization of the margin-propagation principle called
shape-based analog computing (S-AC). The resulting S-AC core integrates several
near-memory compute elements, which include: (a) non-linear activation
functions; (b) inner-product compute circuits; and (c) a mixed-signal
compressive memory, all of which can be scaled for performance or power while
preserving its functionality. Using measured results from prototypes fabricated
in a 180nm CMOS process, we demonstrate that the performance of computing
modules remains robust to transistor biasing and variations in temperature. In
this paper, we also demonstrate the effect of bias-scalability and
computational accuracy on a simple ML regression task.",None,3496
Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding,0.0328939,"Pre-trained language models (PLM) have demonstrated their effectiveness for a
broad range of information retrieval and natural language processing tasks. As
the core part of PLM, multi-head self-attention is appealing for its ability to
jointly attend to information from different positions. However, researchers
have found that PLM always exhibits fixed attention patterns regardless of the
input (e.g., excessively paying attention to [CLS] or [SEP]), which we argue
might neglect important information in the other positions. In this work, we
propose a simple yet effective attention guiding mechanism to improve the
performance of PLM by encouraging attention towards the established goals.
Specifically, we propose two kinds of attention guiding methods, i.e., map
discrimination guiding (MDG) and attention pattern decorrelation guiding (PDG).
The former definitely encourages the diversity among multiple self-attention
heads to jointly attend to information from different representation subspaces,
while the latter encourages self-attention to attend to as many different
positions of the input as possible. We conduct experiments with multiple
general pre-trained models (i.e., BERT, ALBERT, and Roberta) and
domain-specific pre-trained models (i.e., BioBERT, ClinicalBERT, BlueBert, and
SciBERT) on three benchmark datasets (i.e., MultiNLI, MedNLI, and
Cross-genre-IR). Extensive experimental results demonstrate that our proposed
MDG and PDG bring stable performance improvements on all datasets with high
efficiency and low cost.",https://anonymous.4open.science/r/attentionGuiding-F6C0,4699
What Makes Data-to-Text Generation Hard for Pretrained Language Models?,0.044083,"Expressing natural language descriptions of structured facts or relations --
data-to-text generation (D2T) -- increases the accessibility of structured
knowledge repositories. Previous work shows that pre-trained language
models(PLMs) perform remarkably well on this task after fine-tuning on a
significant amount of task-specific training data. On the other hand, while
auto-regressive PLMs can generalize from a few task examples, their efficacy at
D2T is largely unexplored. Furthermore, we have an incomplete understanding of
the limits of PLMs on D2T.
  In this work, we conduct an empirical study of both fine-tuned and
auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their
performance as a function of the amount of task-specific data and how these
data are incorporated into the models: zero and few-shot learning, and
fine-tuning of model weights. In addition, we probe the limits of PLMs by
measuring performance on subsets of the evaluation data: novel predicates and
abstractive test examples. To improve the performance on these subsets, we
investigate two techniques: providing predicate descriptions in the context and
re-ranking generated candidates by information reflected in the source.
Finally, we conduct a human evaluation of model errors and show that D2T
generation tasks would benefit from datasets with more careful manual curation.",https://github.com/UKPLab/plms-graph2text,28189
WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition,0.00437056,"Named Entity Recognition task is one of the core tasks of information
extraction. Word ambiguity and word abbreviation are important reasons for the
low recognition rate of named entities. In this paper, we propose a novel named
entity recognition model WCL-BBCD (Word Contrastive Learning with
BERT-BiLSTM-CRF-DBpedia), which incorporates the idea of contrastive learning.
The model first trains the sentence pairs in the text, calculate similarity
between sentence pairs, and fine-tunes BERT used for the named entity
recognition task according to the similarity, so as to alleviate word
ambiguity. Then, the fine-tuned BERT is combined with BiLSTM-CRF to perform the
named entity recognition task. Finally, the recognition results are corrected
in combination with prior knowledge such as knowledge graphs, so as to
alleviate the low-recognition-rate problem caused by word abbreviations. The
results of experimentals conducted on the CoNLL-2003 English dataset and
OntoNotes V5 English dataset show that our model outperforms other similar
models on.",None,5311
Multi-Spectral Image Classification with Ultra-Lean Complex-Valued Models,0.0410324,"Multi-spectral imagery is invaluable for remote sensing due to different
spectral signatures exhibited by materials that often appear identical in
greyscale and RGB imagery. Paired with modern deep learning methods, this
modality has great potential utility in a variety of remote sensing
applications, such as humanitarian assistance and disaster recovery efforts.
State-of-the-art deep learning methods have greatly benefited from large-scale
annotations like in ImageNet, but existing MSI image datasets lack annotations
at a similar scale. As an alternative to transfer learning on such data with
few annotations, we apply complex-valued co-domain symmetric models to classify
real-valued MSI images. Our experiments on 8-band xView data show that our
ultra-lean model trained on xView from scratch without data augmentations can
outperform ResNet with data augmentation and modified transfer learning on
xView. Our work is the first to demonstrate the value of complex-valued deep
learning on real-valued MSI data.",None,12734
Ad Hoc Teamwork in the Presence of Adversaries,0.0384489,"Advances in ad hoc teamwork have the potential to create agents that
collaborate robustly in real-world applications. Agents deployed in the real
world, however, are vulnerable to adversaries with the intent to subvert them.
There has been little research in ad hoc teamwork that assumes the presence of
adversaries. We explain the importance of extending ad hoc teamwork to include
the presence of adversaries and clarify why this problem is difficult. We then
propose some directions for new research opportunities in ad hoc teamwork that
leads to more robust multi-agent cyber-physical infrastructure systems.",None,1082
First do not fall: learning to exploit a wall with a damaged humanoid robot,0.0219169,"Humanoid robots could replace humans in hazardous situations but most of such
situations are equally dangerous for them, which means that they have a high
chance of being damaged and falling. We hypothesize that humanoid robots would
be mostly used in buildings, which makes them likely to be close to a wall. To
avoid a fall, they can therefore lean on the closest wall, as a human would do,
provided that they find in a few milliseconds where to put the hand(s). This
article introduces a method, called D-Reflex, that learns a neural network that
chooses this contact position given the wall orientation, the wall distance,
and the posture of the robot. This contact position is then used by a
whole-body controller to reach a stable posture. We show that D-Reflex allows a
simulated TALOS robot (1.75m, 100kg, 30 degrees of freedom) to avoid more than
75% of the avoidable falls and can work on the real robot.",https://github.com/resibots/robot-dart,9262
BBA-net: A bi-branch attention network for crowd counting,0.161621,"In the field of crowd counting, the current mainstream CNN-based regression
methods simply extract the density information of pedestrians without finding
the position of each person. This makes the output of the network often found
to contain incorrect responses, which may erroneously estimate the total number
and not conducive to the interpretation of the algorithm. To this end, we
propose a Bi-Branch Attention Network (BBA-NET) for crowd counting, which has
three innovation points. i) A two-branch architecture is used to estimate the
density information and location information separately. ii) Attention
mechanism is used to facilitate feature extraction, which can reduce false
responses. iii) A new density map generation method combining geometric
adaptation and Voronoi split is introduced. Our method can integrate the
pedestrian's head and body information to enhance the feature expression
ability of the density map. Extensive experiments performed on two public
datasets show that our method achieves a lower crowd counting error compared to
other state-of-the-art methods.",None,5878
State Dropout-Based Curriculum Reinforcement Learning for Self-Driving at Unsignalized Intersections,0.0253503,"Traversing intersections is a challenging problem for autonomous vehicles,
especially when the intersections do not have traffic control. Recently deep
reinforcement learning has received massive attention due to its success in
dealing with autonomous driving tasks. In this work, we address the problem of
traversing unsignalized intersections using a novel curriculum for deep
reinforcement learning. The proposed curriculum leads to: 1) A faster training
process for the reinforcement learning agent, and 2) Better performance
compared to an agent trained without curriculum. Our main contribution is
two-fold: 1) Presenting a unique curriculum for training deep reinforcement
learning agents, and 2) showing the application of the proposed curriculum for
the unsignalized intersection traversal task. The framework expects processed
observations of the surroundings from the perception system of the autonomous
vehicle. We test our method in the CommonRoad motion planning simulator on
T-intersections and four-way intersections.",None,842
Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding,0.731829,"Contrastive learning has become a new paradigm for unsupervised sentence
embeddings. Previous studies focus on instance-wise contrastive learning,
attempting to construct positive pairs with textual data augmentation. In this
paper, we propose a novel Contrastive learning method with Prompt-derived
Virtual semantic Prototypes (ConPVP). Specifically, with the help of prompts,
we construct virtual semantic prototypes to each instance, and derive negative
prototypes by using the negative form of the prompts. Using a prototypical
contrastive loss, we enforce the anchor sentence embedding to be close to its
corresponding semantic prototypes, and far apart from the negative prototypes
as well as the prototypes of other sentences. Extensive experimental results on
semantic textual similarity, transfer, and clustering tasks demonstrate the
effectiveness of our proposed model compared to strong baselines. Code is
available at https://github.com/lemon0830/promptCSE.",https://github.com/lemon0830/promptCSE,5610
Moving Other Way: Exploring Word Mover Distance Extensions,0.00991219,"The word mover's distance (WMD) is a popular semantic similarity metric for
two texts. This position paper studies several possible extensions of WMD. We
experiment with the frequency of words in the corpus as a weighting factor and
the geometry of the word vector space. We validate possible extensions of WMD
on six document classification datasets. Some proposed extensions show better
results in terms of the k-nearest neighbor classification error than WMD.",https://github.com/mkusner/wmd,388
Training-Free Robust Multimodal Learning via Sample-Wise Jacobian Regularization,0.00838028,"Multimodal fusion emerges as an appealing technique to improve model
performances on many tasks. Nevertheless, the robustness of such fusion methods
is rarely involved in the present literature. In this paper, we propose a
training-free robust late-fusion method by exploiting conditional independence
assumption and Jacobian regularization. Our key is to minimize the Frobenius
norm of a Jacobian matrix, where the resulting optimization problem is relaxed
to a tractable Sylvester equation. Furthermore, we provide a theoretical error
bound of our method and some insights about the function of the extra modality.
Several numerical experiments on AV-MNIST, RAVDESS, and VGGsound demonstrate
the efficacy of our method under both adversarial attacks and random
corruptions.",None,18787
Interpretation Quality Score for Measuring the Quality of interpretability methods,0.0398972,"Machine learning (ML) models have been applied to a wide range of natural
language processing (NLP) tasks in recent years. In addition to making accurate
decisions, the necessity of understanding how models make their decisions has
become apparent in many applications. To that end, many interpretability
methods that help explain the decision processes of ML models have been
developed. Yet, there currently exists no widely-accepted metric to evaluate
the quality of explanations generated by these methods. As a result, there
currently is no standard way of measuring to what degree an interpretability
method achieves an intended objective. Moreover, there is no accepted standard
of performance by which we can compare and rank the current existing
interpretability methods. In this paper, we propose a novel metric for
quantifying the quality of explanations generated by interpretability methods.
We compute the metric on three NLP tasks using six interpretability methods and
present our results.",None,12089
PAMI-AD: An Activity Detector Exploiting Part-attention and Motion Information in Surveillance Videos,0.0132488,"Activity detection in surveillance videos is a challenging task caused by
small objects, complex activity categories, its untrimmed nature, etc. Existing
methods are generally limited in performance due to inaccurate proposals, poor
classifiers or inadequate post-processing method. In this work, we propose a
comprehensive and effective activity detection system in untrimmed surveillance
videos for person-centered and vehicle-centered activities. It consists of four
modules, i.e., object localizer, proposal filter, activity classifier and
activity refiner. For person-centered activities, a novel part-attention
mechanism is proposed to explore detailed features in different body parts. As
for vehicle-centered activities, we propose a localization masking method to
jointly encode motion and foreground attention features. We conduct experiments
on the large-scale activity detection datasets VIRAT, and achieve the best
results for both groups of activities. Furthermore, our team won the 1st place
in the TRECVID 2021 ActEV challenge.",None,-1
EDU-level Extractive Summarization with Varying Summary Lengths,0.0112355,"Extractive models usually formulate text summarization as extracting fixed
top-$k$ salient sentences from the document as a summary. Few works exploited
extracting finer-grained Elementary Discourse Unit (EDU) with little analysis
and justification for the extractive unit selection. Further, the selection
strategy of the fixed top-$k$ salient sentences fits the summarization need
poorly, as the number of salient sentences in different documents varies and
therefore a common or best $k$ does not exist in reality. To fill these gaps,
this paper first conducts the comparison analysis of oracle summaries based on
EDUs and sentences, which provides evidence from both theoretical and
experimental perspectives to justify and quantify that EDUs make summaries with
higher automatic evaluation scores than sentences. Then, considering this merit
of EDUs, this paper further proposes an EDU-level extractive model with Varying
summary Lengths and develops the corresponding learning algorithm. EDU-VL
learns to encode and predict probabilities of EDUs in the document, generate
multiple candidate summaries with varying lengths based on various $k$ values,
and encode and score candidate summaries, in an end-to-end training manner.
Finally, EDU-VL is experimented on single and multi-document benchmark datasets
and shows improved performances on ROUGE scores in comparison with
state-of-the-art extractive models, and further human evaluation suggests that
EDU-constituent summaries maintain good grammaticality and readability.",https://github.com/yuping-wu/EDU-VL,15518
Evolution of a Web-Scale Near Duplicate Image Detection System,0.0207109,"Detecting near duplicate images is fundamental to the content ecosystem of
photo sharing web applications. However, such a task is challenging when
involving a web-scale image corpus containing billions of images. In this
paper, we present an efficient system for detecting near duplicate images
across 8 billion images. Our system consists of three stages: candidate
generation, candidate selection, and clustering. We also demonstrate that this
system can be used to greatly improve the quality of recommendations and search
results across a number of real-world applications.
  In addition, we include the evolution of the system over the course of six
years, bringing out experiences and lessons on how new systems are designed to
accommodate organic content growth as well as the latest technology. Finally,
we are releasing a human-labeled dataset of ~53,000 pairs of images introduced
in this paper.",None,1492
Planning Landscape Analysis for Self-Adaptive Systems,0.0877092,"To assure performance on the fly, planning is arguably one of the most
important steps for self-adaptive systems (SASs), especially when they are
highly configurable with a daunting number of adaptation options. However,
there has been little understanding of the planning landscape or ways by which
it can be analyzed. This inevitably creates barriers to the design of better
and tailored planners for SASs. In this paper, we showcase how the planning
landscapes of SASs can be quantified and reasoned, particularly with respect to
the different environments. By studying four diverse real-world SASs and 14
environments, we found that (1) the SAS planning landscapes often provide
strong guidance to the planner, but their ruggedness and multi-modality can be
the major obstacle; (2) the extents of guidance and number of global/local
optima are sensitive to the changing environment, but not the ruggedness of the
surface; (3) the local optima are often closer to the global optimum than other
random points; and (4) there are considerable (and useful) overlaps on the
global/local optima between landscapes under different environments. We then
discuss the potential implications to the future work of planner designs for
SASs.",https://doi.org/10.5281/zenodo.5866808,-1
Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes,0.0120638,"In many sequential tasks, a model needs to remember relevant events from the
distant past to make correct predictions. Unfortunately, a straightforward
application of gradient based training requires intermediate computations to be
stored for every element of a sequence. This requires to store prohibitively
large intermediate data if a sequence consists of thousands or even millions
elements, and as a result, makes learning of very long-term dependencies
infeasible. However, the majority of sequence elements can usually be predicted
by taking into account only temporally local information. On the other hand,
predictions affected by long-term dependencies are sparse and characterized by
high uncertainty given only local information. We propose MemUP, a new training
method that allows to learn long-term dependencies without backpropagating
gradients through the whole sequence at a time. This method can potentially be
applied to any recurrent architecture. LSTM network trained with MemUP performs
better or comparable to baselines while requiring to store less intermediate
data.",https://github.com/griver/memup,1912
YORO -- Lightweight End to End Visual Grounding,0.207477,"We present YORO - a multi-modal transformer encoder-only architecture for the
Visual Grounding (VG) task. This task involves localizing, in an image, an
object referred via natural language. Unlike the recent trend in the literature
of using multi-stage approaches that sacrifice speed for accuracy, YORO seeks a
better trade-off between speed an accuracy by embracing a single-stage design,
without CNN backbone. YORO consumes natural language queries, image patches,
and learnable detection tokens and predicts coordinates of the referred object,
using a single transformer encoder. To assist the alignment between text and
visual objects, a novel patch-text alignment loss is proposed. Extensive
experiments are conducted on 5 different datasets with ablations on
architecture design choices. YORO is shown to support real-time inference and
outperform all approaches in this class (single-stage methods) by large
margins. It is also the fastest VG model and achieves the best speed/accuracy
trade-off in the literature.",https://github.com/chihhuiho/yoro,35192
Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations,0.0676316,"Visual Entailment with natural language explanations aims to infer the
relationship between a text-image pair and generate a sentence to explain the
decision-making process. Previous methods rely mainly on a pre-trained
vision-language model to perform the relation inference and a language model to
generate the corresponding explanation. However, the pre-trained
vision-language models mainly build token-level alignment between text and
image yet ignore the high-level semantic alignment between the phrases (chunks)
and visual contents, which is critical for vision-language reasoning. Moreover,
the explanation generator based only on the encoded joint representation does
not explicitly consider the critical decision-making points of relation
inference. Thus the generated explanations are less faithful to visual-language
reasoning. To mitigate these problems, we propose a unified Chunk-aware
Alignment and Lexical Constraint based method, dubbed as CALeC. It contains a
Chunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical
Constraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence
structure inherent in language and various image regions to build chunk-aware
semantic alignment. Relation inferrer uses an attention-based reasoning network
to incorporate the token-level and chunk-level vision-language representations.
LeCG utilizes lexical constraints to expressly incorporate the words or chunks
focused by the relation inferrer into explanation generation, improving the
faithfulness and informativeness of the explanations. We conduct extensive
experiments on three datasets, and experimental results indicate that CALeC
significantly outperforms other competitor models on inference accuracy and
quality of generated explanations.",https://github.com/HITsz-TMG/ExplainableVisualEntailment,13445
Mere Contrastive Learning for Cross-Domain Sentiment Analysis,0.0216626,"Cross-domain sentiment analysis aims to predict the sentiment of texts in the
target domain using the model trained on the source domain to cope with the
scarcity of labeled data. Previous studies are mostly cross-entropy-based
methods for the task, which suffer from instability and poor generalization. In
this paper, we explore contrastive learning on the cross-domain sentiment
analysis task. We propose a modified contrastive objective with in-batch
negative samples so that the sentence representations from the same class will
be pushed close while those from the different classes become further apart in
the latent space. Experiments on two widely used datasets show that our model
can achieve state-of-the-art performance in both cross-domain and multi-domain
sentiment analysis tasks. Meanwhile, visualizations demonstrate the
effectiveness of transferring knowledge learned in the source domain to the
target domain and the adversarial test verifies the robustness of our model.",https://github.com/LuoXiaoHeics/COBE,3063
Controlling the Focus of Pretrained Language Generation Models,0.00477097,"The finetuning of pretrained transformer-based language generation models are
typically conducted in an end-to-end manner, where the model learns to attend
to relevant parts of the input by itself. However, there does not exist a
mechanism to directly control the model's focus. This work aims to develop a
control mechanism by which a user can select spans of context as ""highlights""
for the model to focus on, and generate relevant output. To achieve this goal,
we augment a pretrained model with trainable ""focus vectors"" that are directly
applied to the model's embeddings, while the model itself is kept fixed. These
vectors, trained on automatic annotations derived from attribution methods, act
as indicators for context importance. We test our approach on two core
generation tasks: dialogue response generation and abstractive summarization.
We also collect evaluation data where the highlight-generation pairs are
annotated by humans. Our experiments show that the trained focus vectors are
effective in steering the model to generate outputs that are relevant to
user-selected highlights.",https://github.com/Question406/LearningToFocus,28783
A Comprehensive Study on Occlusion Invariant Face Recognition under Face Mask Occlusion,0.028006,"The face mask is an essential sanitaryware in daily lives growing during the
pandemic period and is a big threat to current face recognition systems. The
masks destroy a lot of details in a large area of face, and it makes it
difficult to recognize them even for humans. The evaluation report shows the
difficulty well when recognizing masked faces. Rapid development and
breakthrough of deep learning in the recent past have witnessed most promising
results from face recognition algorithms. But they fail to perform far from
satisfactory levels in the unconstrained environment during the challenges such
as varying lighting conditions, low resolution, facial expressions, pose
variation and occlusions. Facial occlusions are considered one of the most
intractable problems. Especially when the occlusion occupies a large region of
the face because it destroys lots of official features.",None,11
STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding,0.0226383,"In this technical report, we introduce our solution to human-centric
spatio-temporal video grounding task. We propose a concise and effective
framework named STVGFormer, which models spatiotemporal visual-linguistic
dependencies with a static branch and a dynamic branch. The static branch
performs cross-modal understanding in a single frame and learns to localize the
target object spatially according to intra-frame visual cues like object
appearances. The dynamic branch performs cross-modal understanding across
multiple frames. It learns to predict the starting and ending time of the
target moment according to dynamic visual cues like motions. Both the static
and dynamic branches are designed as cross-modal transformers. We further
design a novel static-dynamic interaction block to enable the static and
dynamic branches to transfer useful and complementary information from each
other, which is shown to be effective to improve the prediction on hard cases.
Our proposed method achieved 39.6% vIoU and won the first place in the HC-STVG
track of the 4th Person in Context Challenge.",None,20700
LiDAR-MIMO: Efficient Uncertainty Estimation for LiDAR-based 3D Object Detection,0.00550928,"The estimation of uncertainty in robotic vision, such as 3D object detection,
is an essential component in developing safe autonomous systems aware of their
own performance. However, the deployment of current uncertainty estimation
methods in 3D object detection remains challenging due to timing and
computational constraints. To tackle this issue, we propose LiDAR-MIMO, an
adaptation of the multi-input multi-output (MIMO) uncertainty estimation method
to the LiDAR-based 3D object detection task. Our method modifies the original
MIMO by performing multi-input at the feature level to ensure the detection,
uncertainty estimation, and runtime performance benefits are retained despite
the limited capacity of the underlying detector and the large computational
costs of point cloud processing. We compare LiDAR-MIMO with MC dropout and
ensembles as baselines and show comparable uncertainty estimation results with
only a small number of output heads. Further, LiDAR-MIMO can be configured to
be twice as fast as MC dropout and ensembles, while achieving higher mAP than
MC dropout and approaching that of ensembles.",https://github.com/open-mmlab/OpenPCDet,12580
The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents,0.0021119,"Learned communication between agents is a powerful tool when approaching
decision-making problems that are hard to overcome by any single agent in
isolation. However, continual coordination and communication learning between
machine agents or human-machine partnerships remains a challenging open
problem. As a stepping stone toward solving the continual communication
learning problem, in this paper we contribute a multi-faceted study into what
we term Pavlovian signalling -- a process by which learned, temporally extended
predictions made by one agent inform decision-making by another agent with
different perceptual access to their shared environment. We seek to establish
how different temporal processes and representational choices impact Pavlovian
signalling between learning agents. To do so, we introduce a partially
observable decision-making domain we call the Frost Hollow. In this domain a
prediction learning agent and a reinforcement learning agent are coupled into a
two-part decision-making system that seeks to acquire sparse reward while
avoiding time-conditional hazards. We evaluate two domain variations: 1)
machine prediction and control learning in a linear walk, and 2) a prediction
learning machine interacting with a human participant in a virtual reality
environment. Our results showcase the speed of learning for Pavlovian
signalling, the impact that different temporal representations do (and do not)
have on agent-agent coordination, and how temporal aliasing impacts agent-agent
and human-agent interactions differently. As a main contribution, we establish
Pavlovian signalling as a natural bridge between fixed signalling paradigms and
fully adaptive communication learning. Our results therefore point to an
actionable, constructivist path towards continual communication learning
between reinforcement learning agents, with potential impact in a range of
real-world settings.",None,84402
Learning Program Synthesis for Integer Sequences from Scratch,0.0230053,"We present a self-learning approach for synthesizing programs from integer
sequences. Our method relies on a tree search guided by a learned policy. Our
system is tested on the On-Line Encyclopedia of Integer Sequences. There, it
discovers, on its own, solutions for 27987 sequences starting from basic
operators and without human-written training examples.",https://github.com/barakeel/oeis-synthesis,6083
CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models,0.0816835,"Users often ask dialogue systems ambiguous questions that require
clarification. We show that current language models rarely ask users to clarify
ambiguous questions and instead provide incorrect answers. To address this, we
introduce CLAM: a framework for getting language models to selectively ask for
clarification about ambiguous user questions. In particular, we show that we
can prompt language models to detect whether a given question is ambiguous,
generate an appropriate clarifying question to ask the user, and give a final
answer after receiving clarification. We also show that we can simulate users
by providing language models with privileged information. This lets us
automatically evaluate multi-turn clarification dialogues. Finally, CLAM
significantly improves language models' accuracy on mixed ambiguous and
unambiguous questions relative to SotA.",None,247
3D-CSL: self-supervised 3D context similarity learning for Near-Duplicate Video Retrieval,0.00666453,"In this paper, we introduce 3D-CSL, a compact pipeline for Near-Duplicate
Video Retrieval (NDVR), and explore a novel self-supervised learning strategy
for video similarity learning. Most previous methods only extract video spatial
features from frames separately and then design kinds of complex mechanisms to
learn the temporal correlations among frame features. However, parts of
spatiotemporal dependencies have already been lost. To address this, our 3D-CSL
extracts global spatiotemporal dependencies in videos end-to-end with a 3D
transformer and find a good balance between efficiency and effectiveness by
matching on clip-level. Furthermore, we propose a two-stage self-supervised
similarity learning strategy to optimize the entire network. Firstly, we
propose PredMAE to pretrain the 3D transformer with video prediction task;
Secondly, ShotMix, a novel video-specific augmentation, and FCS loss, a novel
triplet loss, are proposed further promote the similarity learning results. The
experiments on FIVR-200K and CC_WEB_VIDEO demonstrate the superiority and
reliability of our method, which achieves the state-of-the-art performance on
clip-level NDVR.",None,171
An anomaly detection approach for backdoored neural networks: face recognition as a case study,0.00525965,"Backdoor attacks allow an attacker to embed functionality jeopardizing proper
behavior of any algorithm, machine learning or not. This hidden functionality
can remain inactive for normal use of the algorithm until activated by the
attacker. Given how stealthy backdoor attacks are, consequences of these
backdoors could be disastrous if such networks were to be deployed for
applications as critical as border or access control. In this paper, we propose
a novel backdoored network detection method based on the principle of anomaly
detection, involving access to the clean part of the training data and the
trained network. We highlight its promising potential when considering various
triggers, locations and identity pairs, without the need to make any
assumptions on the nature of the backdoor and its setup. We test our method on
a novel dataset of backdoored networks and report detectability results with
perfect scores.",https://gitlab.idiap.ch/bob/bob.paper.backdoors_anomaly_detection.biosig2022,18877
Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on Twitter (BEAR),0.157228,"Text mining and information extraction for the medical domain has focused on
scientific text generated by researchers. However, their direct access to
individual patient experiences or patient-doctor interactions can be limited.
Information provided on social media, e.g., by patients and their relatives,
complements the knowledge in scientific text. It reflects the patient's journey
and their subjective perspective on the process of developing symptoms, being
diagnosed and offered a treatment, being cured or learning to live with a
medical condition. The value of this type of data is therefore twofold:
Firstly, it offers direct access to people's perspectives. Secondly, it might
cover information that is not available elsewhere, including self-treatment or
self-diagnoses. Named entity recognition and relation extraction are methods to
structure information that is available in unstructured text. However, existing
medical social media corpora focused on a comparably small set of entities and
relations and particular domains, rather than putting the patient into the
center of analyses. With this paper we contribute a corpus with a rich set of
annotation layers following the motivation to uncover and model patients'
journeys and experiences in more detail. We label 14 entity classes (incl.
environmental factors, diagnostics, biochemical processes, patients'
quality-of-life descriptions, pathogens, medical conditions, and treatments)
and 20 relation classes (e.g., prevents, influences, interactions, causes) most
of which have not been considered before for social media data. The publicly
available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000
relation annotations. In a corpus analysis we find that over 80 % of documents
contain relevant entities. Over 50 % of tweets express relations which we
consider essential for uncovering patients' narratives about their journeys.",None,-1
"V3GAN: Decomposing Background, Foreground and Motion for Video Generation",0.018876,"Video generation is a challenging task that requires modeling plausible
spatial and temporal dynamics in a video. Inspired by how humans perceive a
video by grouping a scene into moving and stationary components, we propose a
method that decomposes the task of video generation into the synthesis of
foreground, background and motion. Foreground and background together describe
the appearance, whereas motion specifies how the foreground moves in a video
over time. We propose V3GAN, a novel three-branch generative adversarial
network where two branches model foreground and background information, while
the third branch models the temporal information without any supervision. The
foreground branch is augmented with our novel feature-level masking layer that
aids in learning an accurate mask for foreground and background separation. To
encourage motion consistency, we further propose a shuffling loss for the video
discriminator. Extensive quantitative and qualitative analysis on synthetic as
well as real-world benchmark datasets demonstrates that V3GAN outperforms the
state-of-the-art methods by a significant margin.",None,546
WeDef: Weakly Supervised Backdoor Defense for Text Classification,0.05434,"Existing backdoor defense methods are only effective for limited trigger
types. To defend different trigger types at once, we start from the
class-irrelevant nature of the poisoning process and propose a novel weakly
supervised backdoor defense framework WeDef. Recent advances in weak
supervision make it possible to train a reasonably accurate text classifier
using only a small number of user-provided, class-indicative seed words. Such
seed words shall be considered independent of the triggers. Therefore, a weakly
supervised text classifier trained by only the poisoned documents without their
labels will likely have no backdoor. Inspired by this observation, in WeDef, we
define the reliability of samples based on whether the predictions of the weak
classifier agree with their labels in the poisoned training set. We further
improve the results through a two-phase sanitization: (1) iteratively refine
the weak classifier based on the reliable samples and (2) train a binary poison
classifier by distinguishing the most unreliable samples from the most reliable
samples. Finally, we train the sanitized model on the samples that the poison
classifier predicts as benign. Extensive experiments show that WeDefis
effective against popular trigger-based attacks (e.g., words, sentences, and
paraphrases), outperforming existing defense methods.",https://github.com/LeshengJin/WeDef,4518
CasNet: Investigating Channel Robustness for Speech Separation,0.0101415,"Recording channel mismatch between training and testing conditions has been
shown to be a serious problem for speech separation. This situation greatly
reduces the separation performance, and cannot meet the requirement of daily
use. In this study, inheriting the use of our previously constructed TAT-2mix
corpus, we address the channel mismatch problem by proposing a channel-aware
audio separation network (CasNet), a deep learning framework for end-to-end
time-domain speech separation. CasNet is implemented on top of TasNet. Channel
embedding (characterizing channel information in a mixture of multiple
utterances) generated by Channel Encoder is introduced into the separation
module by the FiLM technique. Through two training strategies, we explore two
roles that channel embedding may play: 1) a real-life noise disturbance, making
the model more robust, or 2) a guide, instructing the separation model to
retain the desired channel information. Experimental results on TAT-2mix show
that CasNet trained with both training strategies outperforms the TasNet
baseline, which does not use channel embeddings.",https://github.com/Sinica-SLAM/CasNet,8197
Distribution-based Emotion Recognition in Conversation,0.00105675,"Automatic emotion recognition in conversation (ERC) is crucial for
emotion-aware conversational artificial intelligence. This paper proposes a
distribution-based framework that formulates ERC as a sequence-to-sequence
problem for emotion distribution estimation. The inherent ambiguity of emotions
and the subjectivity of human perception lead to disagreements in emotion
labels, which is handled naturally in our framework from the perspective of
uncertainty estimation in emotion distributions. A Bayesian training loss is
introduced to improve the uncertainty estimation by conditioning each emotional
state on an utterance-specific Dirichlet prior distribution. Experimental
results on the IEMOCAP dataset show that ERC outperformed the
single-utterance-based system, and the proposed distribution-based ERC methods
have not only better classification accuracy, but also show improved
uncertainty estimation.",None,3441
Towards Realistic Underwater Dataset Generation and Color Restoration,0.0102387,"Recovery of true color from underwater images is an ill-posed problem. This
is because the wide-band attenuation coefficients for the RGB color channels
depend on object range, reflectance, etc. which are difficult to model. Also,
there is backscattering due to suspended particles in water. Thus, most
existing deep-learning based color restoration methods, which are trained on
synthetic underwater datasets, do not perform well on real underwater data.
This can be attributed to the fact that synthetic data cannot accurately
represent real conditions. To address this issue, we use an image to image
translation network to bridge the gap between the synthetic and real domains by
translating images from synthetic underwater domain to real underwater domain.
Using this multimodal domain adaptation technique, we create a dataset that can
capture a diverse array of underwater conditions. We then train a simple but
effective CNN based network on our domain adapted dataset to perform color
restoration. Code and pre-trained models can be accessed at
https://github.com/nehamjain10/TRUDGCR",https://github.com/nehamjain10/TRUDGCRCCSCONCEPTS,1848
Cross-Camera View-Overlap Recognition,0.00687131,"We propose a decentralised view-overlap recognition framework that operates
across freely moving cameras without the need of a reference 3D map. Each
camera independently extracts, aggregates into a hierarchical structure, and
shares feature-point descriptors over time. A view overlap is recognised by
view-matching and geometric validation to discard wrongly matched views. The
proposed framework is generic and can be used with different descriptors. We
conduct the experiments on publicly available sequences as well as new
sequences we collected with hand-held cameras. We show that Oriented FAST and
Rotated BRIEF (ORB) features with Bags of Binary Words within the proposed
framework lead to higher precision and a higher or similar accuracy compared to
NetVLAD, RootSIFT, and SuperGlue.",None,212
Explaining Image Classifiers Using Contrastive Counterfactuals in Generative Latent Spaces,0.0263522,"Despite their high accuracies, modern complex image classifiers cannot be
trusted for sensitive tasks due to their unknown decision-making process and
potential biases. Counterfactual explanations are very effective in providing
transparency for these black-box algorithms. Nevertheless, generating
counterfactuals that can have a consistent impact on classifier outputs and yet
expose interpretable feature changes is a very challenging task. We introduce a
novel method to generate causal and yet interpretable counterfactual
explanations for image classifiers using pretrained generative models without
any re-training or conditioning. The generative models in this technique are
not bound to be trained on the same data as the target classifier. We use this
framework to obtain contrastive and causal sufficiency and necessity scores as
global explanations for black-box classifiers. On the task of face attribute
classification, we show how different attributes influence the classifier
output by providing both causal and contrastive feature attributions, and the
corresponding counterfactual images.",None,41910
DATE: Dual Assignment for End-to-End Fully Convolutional Object Detection,0.0443286,"Fully convolutional detectors discard the one-to-many assignment and adopt a
one-to-one assigning strategy to achieve end-to-end detection but suffer from
the slow convergence issue. In this paper, we revisit these two assignment
methods and find that bringing one-to-many assignment back to end-to-end fully
convolutional detectors helps with model convergence. Based on this
observation, we propose {\em \textbf{D}ual \textbf{A}ssignment} for end-to-end
fully convolutional de\textbf{TE}ction (DATE). Our method constructs two
branches with one-to-many and one-to-one assignment during training and speeds
up the convergence of the one-to-one assignment branch by providing more
supervision signals. DATE only uses the branch with the one-to-one matching
strategy for model inference, which doesn't bring inference overhead.
Experimental results show that Dual Assignment gives nontrivial improvements
and speeds up model convergence upon OneNet and DeFCN. Code:
https://github.com/YiqunChen1999/date.",https://github.com/YiqunChen1999/date,14428
Sort by Structure: Language Model Ranking as Dependency Probing,0.0178449,"Making an informed choice of pre-trained language model (LM) is critical for
performance, yet environmentally costly, and as such widely underexplored. The
field of Computer Vision has begun to tackle encoder ranking, with promising
forays into Natural Language Processing, however they lack coverage of
linguistic tasks such as structured prediction. We propose probing to rank LMs,
specifically for parsing dependencies in a given language, by measuring the
degree to which labeled trees are recoverable from an LM's contextualized
embeddings. Across 46 typologically and architecturally diverse LM-language
pairs, our probing approach predicts the best LM choice 79% of the time using
orders of magnitude less compute than training a full parser. Within this
study, we identify and analyze one recently proposed decoupled LM - RemBERT -
and find it strikingly contains less inherent dependency information, but often
yields the best parser after full fine-tuning. Without this outlier our
approach identifies the best LM in 89% of cases.",https://personads.me/x/naacl-2022-code,6816
"A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks",0.0169225,"Predictive coding networks are neuroscience-inspired models with roots in
both Bayesian statistics and neuroscience. Training such models, however, is
quite inefficient and unstable. In this work, we show how by simply changing
the temporal scheduling of the update rule for the synaptic weights leads to an
algorithm that is much more efficient and stable than the original one, and has
theoretical guarantees in terms of convergence. The proposed algorithm, that we
call incremental predictive coding (iPC) is also more biologically plausible
than the original one, as it it fully automatic. In an extensive set of
experiments, we show that iPC constantly performs better than the original
formulation on a large number of benchmarks for image classification, as well
as for the training of both conditional and masked language models, in terms of
test accuracy, efficiency, and convergence with respect to a large set of
hyperparameters.",None,12571
A.I. Robustness: a Human-Centered Perspective on Technological Challenges and Opportunities,0.0213387,"Despite the impressive performance of Artificial Intelligence (AI) systems,
their robustness remains elusive and constitutes a key issue that impedes
large-scale adoption. Robustness has been studied in many domains of AI, yet
with different interpretations across domains and contexts. In this work, we
systematically survey the recent progress to provide a reconciled terminology
of concepts around AI robustness. We introduce three taxonomies to organize and
describe the literature both from a fundamental and applied point of view: 1)
robustness by methods and approaches in different phases of the machine
learning pipeline; 2) robustness for specific model architectures, tasks, and
systems; and in addition, 3) robustness assessment methodologies and insights,
particularly the trade-offs with other trustworthiness properties. Finally, we
identify and discuss research gaps and opportunities and give an outlook on the
field. We highlight the central role of humans in evaluating and enhancing AI
robustness, considering the necessary knowledge humans can provide, and discuss
the need for better understanding practices and developing supportive tools in
the future.",None,8667
Traffic Sign Classification Using Deep and Quantum Neural Networks,0.025663,"Quantum Neural Networks (QNNs) are an emerging technology that can be used in
many applications including computer vision. In this paper, we presented a
traffic sign classification system implemented using a hybrid quantum-classical
convolutional neural network. Experiments on the German Traffic Sign
Recognition Benchmark dataset indicate that currently QNN do not outperform
classical DCNN (Deep Convolutuional Neural Networks), yet still provide an
accuracy of over 90% and are a definitely promising solution for advanced
computer vision.",None,743
Zero-Shot Video Captioning with Evolving Pseudo-Tokens,0.281674,"We introduce a zero-shot video captioning method that employs two frozen
networks: the GPT-2 language model and the CLIP image-text matching model. The
matching score is used to steer the language model toward generating a sentence
that has a high average matching score to a subset of the video frames. Unlike
zero-shot image captioning methods, our work considers the entire sentence at
once. This is achieved by optimizing, during the generation process, part of
the prompt from scratch, by modifying the representation of all other tokens in
the prompt, and by repeating the process iteratively, gradually improving the
specificity and comprehensiveness of the generated sentence. Our experiments
show that the generated captions are coherent and display a broad range of
real-world knowledge. Our code is available at:
https://github.com/YoadTew/zero-shot-video-to-text",None,37698
"""The Pedestrian next to the Lamppost"" Adaptive Object Graphs for Better Instantaneous Mapping",0.103607,"Estimating a semantically segmented bird's-eye-view (BEV) map from a single
image has become a popular technique for autonomous control and navigation.
However, they show an increase in localization error with distance from the
camera. While such an increase in error is entirely expected - localization is
harder at distance - much of the drop in performance can be attributed to the
cues used by current texture-based models, in particular, they make heavy use
of object-ground intersections (such as shadows), which become increasingly
sparse and uncertain for distant objects. In this work, we address these
shortcomings in BEV-mapping by learning the spatial relationship between
objects in a scene. We propose a graph neural network which predicts BEV
objects from a monocular image by spatially reasoning about an object within
the context of other objects. Our approach sets a new state-of-the-art in BEV
estimation from monocular images across three large-scale datasets, including a
50% relative improvement for objects on nuScenes.",None,19337
Duality-Induced Regularizer for Semantic Matching Knowledge Graph Embeddings,0.0367605,"Semantic matching models -- which assume that entities with similar semantics
have similar embeddings -- have shown great power in knowledge graph embeddings
(KGE). Many existing semantic matching models use inner products in embedding
spaces to measure the plausibility of triples and quadruples in static and
temporal knowledge graphs. However, vectors that have the same inner products
with another vector can still be orthogonal to each other, which implies that
entities with similar semantics may have dissimilar embeddings. This property
of inner products significantly limits the performance of semantic matching
models. To address this challenge, we propose a novel regularizer -- namely,
DUality-induced RegulArizer (DURA) -- which effectively encourages the entities
with similar semantics to have similar embeddings. The major novelty of DURA is
based on the observation that, for an existing semantic matching KGE model
(primal), there is often another distance based KGE model (dual) closely
associated with it, which can be used as effective constraints for entity
embeddings. Experiments demonstrate that DURA consistently and significantly
improves the performance of state-of-the-art semantic matching models on both
static and temporal knowledge graph benchmarks.",None,25278
Rethinking Implicit Neural Representations for Vision Learners,0.0219789,"Implicit Neural Representations (INRs) are powerful to parameterize
continuous signals in computer vision. However, almost all INRs methods are
limited to low-level tasks, e.g., image/video compression, super-resolution,
and image generation. The questions on how to explore INRs to high-level tasks
and deep networks are still under-explored. Existing INRs methods suffer from
two problems: 1) narrow theoretical definitions of INRs are inapplicable to
high-level tasks; 2) lack of representation capabilities to deep networks.
Motivated by the above facts, we reformulate the definitions of INRs from a
novel perspective and propose an innovative Implicit Neural Representation
Network (INRN), which is the first study of INRs to tackle both low-level and
high-level tasks. Specifically, we present three key designs for basic blocks
in INRN along with two different stacking ways and corresponding loss
functions. Extensive experiments with analysis on both low-level tasks (image
fitting) and high-level vision tasks (image classification, object detection,
instance segmentation) demonstrate the effectiveness of the proposed method.",None,7989
Joint covariate-alignment and concept-alignment: a framework for domain generalization,0.0133565,"In this paper, we propose a novel domain generalization (DG) framework based
on a new upper bound to the risk on the unseen domain. Particularly, our
framework proposes to jointly minimize both the covariate-shift as well as the
concept-shift between the seen domains for a better performance on the unseen
domain. While the proposed approach can be implemented via an arbitrary
combination of covariate-alignment and concept-alignment modules, in this work
we use well-established approaches for distributional alignment namely, Maximum
Mean Discrepancy (MMD) and covariance Alignment (CORAL), and use an Invariant
Risk Minimization (IRM)-based approach for concept alignment. Our numerical
results show that the proposed methods perform as well as or better than the
state-of-the-art for domain generalization on several data sets.",https://github.com/thuan2412/Joint-covariate-alignment-and-concept-alignment-for-domain-generalization,12938
PINCH: An Adversarial Extraction Attack Framework for Deep Learning Models,0.0151029,"Adversarial extraction attacks constitute an insidious threat against Deep
Learning (DL) models in-which an adversary aims to steal the architecture,
parameters, and hyper-parameters of a targeted DL model. Existing extraction
attack literature have observed varying levels of attack success for different
DL models and datasets, yet the underlying cause(s) behind their susceptibility
often remain unclear, and would help facilitate creating secure DL systems. In
this paper we present PINCH: an efficient and automated extraction attack
framework capable of designing, deploying, and analyzing extraction attack
scenarios across heterogeneous hardware platforms. Using PINCH, we perform
extensive experimental evaluation of extraction attacks against 21 model
architectures to explore new extraction attack scenarios and further attack
staging. Our findings show (1) key extraction characteristics whereby
particular model configurations exhibit strong resilience against specific
attacks, (2) even partial extraction success enables further staging for other
adversarial attacks, and (3) equivalent stolen models uncover differences in
expressive power, yet exhibit similar captured knowledge.",None,5686
RedApt: An Adaptor for wav2vec 2 Encoding \\ Faster and Smaller Speech Translation without Quality Compromise,0.0112745,"Pre-trained speech Transformers in speech translation (ST) have facilitated
state-of-the-art (SotA) results; yet, using such encoders is computationally
expensive. To improve this, we present a novel Reducer Adaptor block, RedApt,
that could be seamlessly integrated within any Transformer-based speech
encoding architecture. Integrating the pretrained wav2vec 2 speech encoder with
RedAptbrings 41% speedup, 33% memory reduction with 24% fewer FLOPs at
inference. To our positive surprise, our ST model with RedApt outperforms the
SotA architecture by an average of 0.68 BLEU score on 8 language pairs from
Must-C.",https://github.com/mingzi151/w2v2-st,14601
"Modeling Intention, Emotion and External World in Dialogue Systems",0.0430846,"Intention, emotion and action are important elements in human activities.
Modeling the interaction process between individuals by analyzing the
relationships between these elements is a challenging task. However, previous
work mainly focused on modeling intention and emotion independently, and
neglected of exploring the mutual relationships between intention and emotion.
In this paper, we propose a RelAtion Interaction Network (RAIN), consisting of
Intention Relation Module and Emotion Relation Module, to jointly model mutual
relationships and explicitly integrate historical intention information. The
experiments on the dataset show that our model can take full advantage of the
intention, emotion and action between individuals and achieve a remarkable
improvement over BERT-style baselines. Qualitative analysis verifies the
importance of the mutual interaction between the intention and emotion.",None,269
Arbitrary Point Cloud Upsampling with Spherical Mixture of Gaussians,0.12227,"Generating dense point clouds from sparse raw data benefits downstream 3D
understanding tasks, but existing models are limited to a fixed upsampling
ratio or to a short range of integer values. In this paper, we present
APU-SMOG, a Transformer-based model for Arbitrary Point cloud Upsampling (APU).
The sparse input is firstly mapped to a Spherical Mixture of Gaussians (SMOG)
distribution, from which an arbitrary number of points can be sampled. Then,
these samples are fed as queries to the Transformer decoder, which maps them
back to the target surface. Extensive qualitative and quantitative evaluations
show that APU-SMOG outperforms state-of-the-art fixed-ratio methods, while
effectively enabling upsampling with any scaling factor, including non-integer
values, with a single trained model. The code is available at
https://github.com/apusmog/apusmog/",https://github.com/apusmog/apusmog,9074
JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning,0.0356519,"In online job marketplaces, it is important to establish a well-defined job
title taxonomy for various downstream tasks (e.g., job recommendation, users'
career analysis, and turnover prediction). Job Title Normalization (JTN) is
such a cleaning step to classify user-created non-standard job titles into
normalized ones. However, solving the JTN problem is non-trivial with
challenges: (1) semantic similarity of different job titles, (2) non-normalized
user-created job titles, and (3) large-scale and long-tailed job titles in
real-world applications. To this end, we propose a novel solution, named JAMES,
that constructs three unique embeddings (i.e., graph, contextual, and
syntactic) of a target job title to effectively capture its various traits. We
further propose a multi-aspect co-attention mechanism to attentively combine
these embeddings, and employ neural logical reasoning representations to
collaboratively estimate similarities between messy job titles and normalized
job titles in a reasoning space. To evaluate JAMES, we conduct comprehensive
experiments against ten competing models on a large-scale real-world dataset
with over 350,000 job titles. Our experimental results show that JAMES
significantly outperforms the best baseline by 10.06% in Precision@10 and by
17.52% in NDCG@10, respectively.",None,13066
Lightweight Monocular Depth Estimation with an Edge Guided Network,0.0743767,"Monocular depth estimation is an important task that can be applied to many
robotic applications. Existing methods focus on improving depth estimation
accuracy via training increasingly deeper and wider networks, however these
suffer from large computational complexity. Recent studies found that edge
information are important cues for convolutional neural networks (CNNs) to
estimate depth. Inspired by the above observations, we present a novel
lightweight Edge Guided Depth Estimation Network (EGD-Net) in this study. In
particular, we start out with a lightweight encoder-decoder architecture and
embed an edge guidance branch which takes as input image gradients and
multi-scale feature maps from the backbone to learn the edge attention
features. In order to aggregate the context information and edge attention
features, we design a transformer-based feature aggregation module (TRFA). TRFA
captures the long-range dependencies between the context information and edge
attention features through cross-attention mechanism. We perform extensive
experiments on the NYU depth v2 dataset. Experimental results show that the
proposed method runs about 96 fps on a Nvidia GTX 1080 GPU whilst achieving the
state-of-the-art performance in terms of accuracy.",None,10318
Task-Induced Representation Learning,0.0282793,"In this work, we evaluate the effectiveness of representation learning
approaches for decision making in visually complex environments. Representation
learning is essential for effective reinforcement learning (RL) from
high-dimensional inputs. Unsupervised representation learning approaches based
on reconstruction, prediction or contrastive learning have shown substantial
learning efficiency gains. Yet, they have mostly been evaluated in clean
laboratory or simulated settings. In contrast, real environments are visually
complex and contain substantial amounts of clutter and distractors.
Unsupervised representations will learn to model such distractors, potentially
impairing the agent's learning efficiency. In contrast, an alternative class of
approaches, which we call task-induced representation learning, leverages task
information such as rewards or demonstrations from prior tasks to focus on
task-relevant parts of the scene and ignore distractors. We investigate the
effectiveness of unsupervised and task-induced representation learning
approaches on four visually complex environments, from Distracting DMControl to
the CARLA driving simulator. For both, RL and imitation learning, we find that
representation learning generally improves sample efficiency on unseen tasks
even in visually complex scenes and that task-induced representations can
double learning efficiency compared to unsupervised alternatives. Code is
available at https://clvrai.com/tarp.",https://github.com/xxx,98
Retrieval Based Time Series Forecasting,0.0313139,"Time series data appears in a variety of applications such as smart
transportation and environmental monitoring. One of the fundamental problems
for time series analysis is time series forecasting. Despite the success of
recent deep time series forecasting methods, they require sufficient
observation of historical values to make accurate forecasting. In other words,
the ratio of the output length (or forecasting horizon) to the sum of the input
and output lengths should be low enough (e.g., 0.3). As the ratio increases
(e.g., to 0.8), the uncertainty for the forecasting accuracy increases
significantly. In this paper, we show both theoretically and empirically that
the uncertainty could be effectively reduced by retrieving relevant time series
as references. In the theoretical analysis, we first quantify the uncertainty
and show its connections to the Mean Squared Error (MSE). Then we prove that
models with references are easier to learn than models without references since
the retrieved references could reduce the uncertainty. To empirically
demonstrate the effectiveness of the retrieval based time series forecasting
models, we introduce a simple yet effective two-stage method, called ReTime
consisting of a relational retrieval and a content synthesis. We also show that
ReTime can be easily adapted to the spatial-temporal time series and time
series imputation settings. Finally, we evaluate ReTime on real-world datasets
to demonstrate its effectiveness.",None,80967
Style Matters! Investigating Linguistic Style in Online Communities,0.276514,"Content has historically been the primary lens used to study language in
online communities. This paper instead focuses on the linguistic style of
communities. While we know that individuals have distinguishable styles, here
we ask whether communities have distinguishable styles. Additionally, while
prior work has relied on a narrow definition of style, we employ a broad
definition involving 262 features to analyze the linguistic style of 9 online
communities from 3 social media platforms discussing politics, television and
travel. We find that communities indeed have distinct styles. Also, style is an
excellent predictor of group membership (F-score 0.952 and Accuracy 96.09%).
While on average it is statistically equivalent to predictions using content
alone, it is more resilient to reductions in training data.",https://github.com/pushshift/api,8411
Reinforcement Learning Methods for Wordle: A POMDP/Adaptive Control Approach,0.00998951,"In this paper we address the solution of the popular Wordle puzzle, using new
reinforcement learning methods, which apply more generally to adaptive control
of dynamic systems and to classes of Partially Observable Markov Decision
Process (POMDP) problems. These methods are based on approximation in value
space and the rollout approach, admit a straightforward implementation, and
provide improved performance over various heuristic approaches. For the Wordle
puzzle, they yield on-line solution strategies that are very close to optimal
at relatively modest computational cost. Our methods are viable for more
complex versions of Wordle and related search problems, for which an optimal
strategy would be impossible to compute. They are also applicable to a wide
range of adaptive sequential decision problems that involve an unknown or
frequently changing environment whose parameters are estimated on-line.",None,134250
Long Short-Term Memory for Spatial Encoding in Multi-Agent Path Planning,0.0927892,"Reinforcement learning-based path planning for multi-agent systems of varying
size constitutes a research topic with increasing significance as progress in
domains such as urban air mobility and autonomous aerial vehicles continues.
Reinforcement learning with continuous state and action spaces is used to train
a policy network that accommodates desirable path planning behaviors and can be
used for time-critical applications. A Long Short-Term Memory module is
proposed to encode an unspecified number of states for a varying, indefinite
number of agents. The described training strategies and policy architecture
lead to a guidance that scales to an infinite number of agents and unlimited
physical dimensions, although training takes place at a smaller scale. The
guidance is implemented on a low-cost, off-the-shelf onboard computer. The
feasibility of the proposed approach is validated by presenting flight test
results of up to four drones, autonomously navigating collision-free in a
real-world environment.",None,3267
Misspelling Semantics In Thai,0.135371,"User-generated content is full of misspellings. Rather than being just random
noise, we hypothesise that many misspellings contain hidden semantics that can
be leveraged for language understanding tasks. This paper presents a
fine-grained annotated corpus of misspelling in Thai, together with an analysis
of misspelling intention and its possible semantics to get a better
understanding of the misspelling patterns observed in the corpus. In addition,
we introduce two approaches to incorporate the semantics of misspelling:
Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST).
Experiments on a sentiment analysis task confirm our overall hypothesis:
additional semantics from misspelling can boost the micro F1 score up to
0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",None,21
SSformer: A Lightweight Transformer for Semantic Segmentation,0.0138457,"It is well believed that Transformer performs better in semantic segmentation
compared to convolutional neural networks. Nevertheless, the original Vision
Transformer may lack of inductive biases of local neighborhoods and possess a
high time complexity. Recently, Swin Transformer sets a new record in various
vision tasks by using hierarchical architecture and shifted windows while being
more efficient. However, as Swin Transformer is specifically designed for image
classification, it may achieve suboptimal performance on dense prediction-based
segmentation task. Further, simply combing Swin Transformer with existing
methods would lead to the boost of model size and parameters for the final
segmentation model. In this paper, we rethink the Swin Transformer for semantic
segmentation, and design a lightweight yet effective transformer model, called
SSformer. In this model, considering the inherent hierarchical design of Swin
Transformer, we propose a decoder to aggregate information from different
layers, thus obtaining both local and global attentions. Experimental results
show the proposed SSformer yields comparable mIoU performance with
state-of-the-art models, while maintaining a smaller model size and lower
compute.",https://github.com/shiwt03/SSformer,4373
Batch-efficient EigenDecomposition for Small and Medium Matrices,0.05519,"EigenDecomposition (ED) is at the heart of many computer vision algorithms
and applications. One crucial bottleneck limiting its usage is the expensive
computation cost, particularly for a mini-batch of matrices in the deep neural
networks. In this paper, we propose a QR-based ED method dedicated to the
application scenarios of computer vision. Our proposed method performs the ED
entirely by batched matrix/vector multiplication, which processes all the
matrices simultaneously and thus fully utilizes the power of GPUs. Our
technique is based on the explicit QR iterations by Givens rotation with double
Wilkinson shifts. With several acceleration techniques, the time complexity of
QR iterations is reduced from $O{(}n^5{)}$ to $O{(}n^3{)}$. The numerical test
shows that for small and medium batched matrices (\emph{e.g.,} $dim{<}32$) our
method can be much faster than the Pytorch SVD function. Experimental results
on visual recognition and image generation demonstrate that our methods also
achieve competitive performances.",https://github.com/KingJamesSong/BatchED,42425
Probabilistic and Non-Deterministic Event Data in Process Mining: Embedding Uncertainty in Process Analysis Techniques,0.0919136,"Process mining is a subfield of process science that analyzes event data
collected in databases called event logs. Recently, novel types of event data
have become of interest due to the wide industrial application of process
mining analyses. In this paper, we examine uncertain event data. Such data
contain meta-attributes describing the amount of imprecision tied with
attributes recorded in an event log. We provide examples of uncertain event
data, present the state of the art in regard of uncertainty in process mining,
and illustrate open challenges related to this research direction.",None,472
Leveraging Trajectory Prediction for Pedestrian Video Anomaly Detection,0.0449405,"Video anomaly detection is a core problem in vision. Correctly detecting and
identifying anomalous behaviors in pedestrians from video data will enable
safety-critical applications such as surveillance, activity monitoring, and
human-robot interaction. In this paper, we propose to leverage trajectory
localization and prediction for unsupervised pedestrian anomaly event
detection. Different than previous reconstruction-based approaches, our
proposed framework rely on the prediction errors of normal and abnormal
pedestrian trajectories to detect anomalies spatially and temporally. We
present experimental results on real-world benchmark datasets on varying
timescales and show that our proposed trajectory-predictor-based anomaly
detection pipeline is effective and efficient at identifying anomalous
activities of pedestrians in videos. Code will be made available at
https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection.",https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection,604
Fine-grained Czech News Article Dataset: An Interdisciplinary Approach to Trustworthiness Analysis,0.0156727,"We present the Verifee Dataset: a novel dataset of news articles with
fine-grained trustworthiness annotations. We develop a detailed methodology
that assesses the texts based on their parameters encompassing editorial
transparency, journalist conventions, and objective reporting while penalizing
manipulative techniques. We bring aboard a diverse set of researchers from
social, media, and computer sciences to overcome barriers and limited framing
of this interdisciplinary problem. We collect over $10,000$ unique articles
from almost $60$ Czech online news sources. These are categorized into one of
the $4$ classes across the credibility spectrum we propose, raging from
entirely trustworthy articles all the way to the manipulative ones. We produce
detailed statistics and study trends emerging throughout the set. Lastly, we
fine-tune multiple popular sequence-to-sequence language models using our
dataset on the trustworthiness classification task and report the best testing
F-1 score of $0.52$. We open-source the dataset, annotation methodology, and
annotators' instructions in full length at https://verifee.ai/research to
enable easy build-up work. We believe similar methods can help prevent
disinformation and educate in the realm of media literacy.",https://verifee.ai/research,351
Booster-SHOT: Boosting Stacked Homography Transformations for Multiview Pedestrian Detection with Attention,0.00810099,"Improving multi-view aggregation is integral for multi-view pedestrian
detection, which aims to obtain a bird's-eye-view pedestrian occupancy map from
images captured through a set of calibrated cameras. Inspired by the success of
attention modules for deep neural networks, we first propose a Homography
Attention Module (HAM) which is shown to boost the performance of existing
end-to-end multiview detection approaches by utilizing a novel channel gate and
spatial gate. Additionally, we propose Booster-SHOT, an end-to-end
convolutional approach to multiview pedestrian detection incorporating our
proposed HAM as well as elements from previous approaches such as view-coherent
augmentation or stacked homography transformations. Booster-SHOT achieves 92.9%
and 94.2% for MODA on Wildtrack and MultiviewX respectively, outperforming the
state-of-the-art by 1.4% on Wildtrack and 0.5% on MultiviewX, achieving
state-of-the-art performance overall for standard evaluation metrics used in
multi-view pedestrian detection.",None,188
Online Dynamic Reliability Evaluation of Wind Turbines based on Drone-assisted Monitoring,0.0286378,"The offshore wind energy is increasingly becoming an attractive source of
energy due to having lower environmental impact. Effective operation and
maintenance that ensures the maximum availability of the energy generation
process using offshore facilities and minimal production cost are two key
factors to improve the competitiveness of this energy source over other
traditional sources of energy. Condition monitoring systems are widely used for
health management of offshore wind farms to have improved operation and
maintenance. Reliability of the wind farms are increasingly being evaluated to
aid in the maintenance process and thereby to improve the availability of the
farms. However, much of the reliability analysis is performed offline based on
statistical data. In this article, we propose a drone-assisted monitoring based
method for online reliability evaluation of wind turbines. A blade system of a
wind turbine is used as an illustrative example to demonstrate the proposed
approach.",None,5416
Scan2Part: Fine-grained and Hierarchical Part-level Understanding of Real-World 3D Scans,0.0207504,"We propose Scan2Part, a method to segment individual parts of objects in
real-world, noisy indoor RGB-D scans. To this end, we vary the part hierarchies
of objects in indoor scenes and explore their effect on scene understanding
models. Specifically, we use a sparse U-Net-based architecture that captures
the fine-scale detail of the underlying 3D scan geometry by leveraging a
multi-scale feature hierarchy. In order to train our method, we introduce the
Scan2Part dataset, which is the first large-scale collection providing detailed
semantic labels at the part level in the real-world setting. In total, we
provide 242,081 correspondences between 53,618 PartNet parts of 2,477 ShapeNet
objects and 1,506 ScanNet scenes, at two spatial resolutions of 2 cm$^3$ and 5
cm$^3$. As output, we are able to predict fine-grained per-object part labels,
even when the geometry is coarse or partially missing.",None,6812
A gentle introduction to Quantum Natural Language Processing,0.0261484,"The main goal of this master's thesis is to introduce Quantum Natural
Language Processing (QNLP) in a way understandable by both the NLP engineer and
the quantum computing practitioner. QNLP is a recent application of quantum
computing that aims at representing sentences' meaning as vectors encoded into
quantum computers. To achieve this, the distributional meaning of words is
extended by the compositional meaning of sentences (DisCoCat model) : the
vectors representing words' meanings are composed through the syntactic
structure of the sentence. This is done using an algorithm based on tensor
products. We see that this algorithm is inefficient on classical computers but
scales well using quantum circuits. After exposing the practical details of its
implementation, we go through three use-cases.",None,-1
Feature Refinement to Improve High Resolution Image Inpainting,0.0709817,"In this paper, we address the problem of degradation in inpainting quality of
neural networks operating at high resolutions. Inpainting networks are often
unable to generate globally coherent structures at resolutions higher than
their training set. This is partially attributed to the receptive field
remaining static, despite an increase in image resolution. Although downscaling
the image prior to inpainting produces coherent structure, it inherently lacks
detail present at higher resolutions. To get the best of both worlds, we
optimize the intermediate featuremaps of a network by minimizing a multiscale
consistency loss at inference. This runtime optimization improves the
inpainting results and establishes a new state-of-the-art for high resolution
inpainting. Code is available at:
https://github.com/geomagical/lama-with-refiner/tree/refinement.",https://github.com/geomagical/lama-with-refiner/tree/refinement,49
Practical Network Acceleration with Tiny Sets,0.0290576,"Due to data privacy issues, accelerating networks with tiny training sets has
become a critical need in practice. Previous methods mainly adopt filter-level
pruning to accelerate networks with scarce training samples. In this paper, we
reveal that dropping blocks is a fundamentally superior approach in this
scenario. It enjoys a higher acceleration ratio and results in a better
latency-accuracy performance under the few-shot setting. To choose which blocks
to drop, we propose a new concept namely recoverability to measure the
difficulty of recovering the compressed network. Our recoverability is
efficient and effective for choosing which blocks to drop. Finally, we propose
an algorithm named PRACTISE to accelerate networks using only tiny sets of
training images. PRACTISE outperforms previous methods by a significant margin.
For 22% latency reduction, PRACTISE surpasses previous methods by on average 7%
on ImageNet-1k. It also enjoys high generalization ability, working well under
data-free or out-of-domain data settings, too. Our code is at
https://github.com/DoctorKey/Practise.",https://github.com/DoctorKey/Practise,21492
A Federated Learning-enabled Smart Street Light Monitoring Application: Benefits and Future Challenges,0.0207768,"Data-enabled cities are recently accelerated and enhanced with automated
learning for improved Smart Cities applications. In the context of an Internet
of Things (IoT) ecosystem, the data communication is frequently costly,
inefficient, not scalable and lacks security. Federated Learning (FL) plays a
pivotal role in providing privacy-preserving and communication efficient
Machine Learning (ML) frameworks. In this paper we evaluate the feasibility of
FL in the context of a Smart Cities Street Light Monitoring application. FL is
evaluated against benchmarks of centralised and (fully) personalised machine
learning techniques for the classification task of the lampposts operation.
Incorporating FL in such a scenario shows minimal performance reduction in
terms of the classification task, but huge improvements in the communication
cost and the privacy preserving. These outcomes strengthen FL's viability and
potential for IoT applications.",None,454
Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling,0.116834,"To capture the relationship between samples and labels, conditional
generative models often inherit spurious correlations from the training
dataset. This can result in label-conditional distributions that are imbalanced
with respect to another latent attribute. To mitigate this issue, which we call
spurious causality of conditional generation, we propose a general two-step
strategy. (a) Fairness Intervention (FI): emphasize the minority samples that
are hard to generate due to the spurious correlation in the training dataset.
(b) Corrective Sampling (CS): explicitly filter the generated samples and
ensure that they follow the desired latent attribute distribution. We have
designed the fairness intervention to work for various degrees of supervision
on the spurious attribute, including unsupervised, weakly-supervised, and
semi-supervised scenarios. Our experimental results demonstrate that FICS can
effectively resolve spurious causality of conditional generation across various
datasets.",https://github.com/POSTECH-CVLab/PyTorch-StudioGAN,11542
Accurate and Reliable Methods for 5G UAV Jamming Identification With Calibrated Uncertainty,0.0376915,"Only increasing accuracy without considering uncertainty may negatively
impact Deep Neural Network (DNN) decision-making and decrease its reliability.
This paper proposes five combined preprocessing and post-processing methods for
time-series binary classification problems that simultaneously increase the
accuracy and reliability of DNN outputs applied in a 5G UAV security dataset.
These techniques use DNN outputs as input parameters and process them in
different ways. Two methods use a well-known Machine Learning (ML) algorithm as
a complement, and the other three use only confidence values that the DNN
estimates. We compare seven different metrics, such as the Expected Calibration
Error (ECE), Maximum Calibration Error (MCE), Mean Confidence (MC), Mean
Accuracy (MA), Normalized Negative Log Likelihood (NLL), Brier Score Loss
(BSL), and Reliability Score (RS) and the tradeoffs between them to evaluate
the proposed hybrid algorithms. First, we show that the eXtreme Gradient
Boosting (XGB) classifier might not be reliable for binary classification under
the conditions this work presents. Second, we demonstrate that at least one of
the potential methods can achieve better results than the classification in the
DNN softmax layer. Finally, we show that the prospective methods may improve
accuracy and reliability with better uncertainty calibration based on the
assumption that the RS determines the difference between MC and MA metrics, and
this difference should be zero to increase reliability. For example, Method 3
presents the best RS of 0.65 even when compared to the XGB classifier, which
achieves RS of 7.22.",None,453
An Emotion-guided Approach to Domain Adaptive Fake News Detection using Adversarial Learning,0.0368886,"Recent works on fake news detection have shown the efficacy of using emotions
as a feature for improved performance. However, the cross-domain impact of
emotion-guided features for fake news detection still remains an open problem.
In this work, we propose an emotion-guided, domain-adaptive, multi-task
approach for cross-domain fake news detection, proving the efficacy of
emotion-guided models in cross-domain settings for various datasets.",None,6855
Entity-Conditioned Question Generation for Robust Attention Distribution in Neural Information Retrieval,0.0329518,"We show that supervised neural information retrieval (IR) models are prone to
learning sparse attention patterns over passage tokens, which can result in key
phrases including named entities receiving low attention weights, eventually
leading to model under-performance. Using a novel targeted synthetic data
generation method that identifies poorly attended entities and conditions the
generation episodes on those, we teach neural IR to attend more uniformly and
robustly to all entities in a given passage. On two public IR benchmarks, we
empirically show that the proposed method helps improve both the model's
attention patterns and retrieval performance, including in zero-shot settings.",https://github.com/blender-nlp/EntityConditionedQGen,3216
Ethical Design of Computers: From Semiconductors to IoT and Artificial Intelligence,0.0,"Computing systems are tightly integrated today into our professional, social,
and private lives. An important consequence of this growing ubiquity of
computing is that it can have significant ethical implications of which
computing professionals should take account. In most real-world scenarios, it
is not immediately obvious how particular technical choices during the design
and use of computing systems could be viewed from an ethical perspective. This
article provides a perspective on the ethical challenges within semiconductor
chip design, IoT applications, and the increasing use of artificial
intelligence in the design processes, tools, and hardware-software stacks of
these systems.",None,21094
A Contrastive Objective for Learning Disentangled Representations,0.102877,"Learning representations of images that are invariant to sensitive or
unwanted attributes is important for many tasks including bias removal and
cross domain retrieval. Here, our objective is to learn representations that
are invariant to the domain (sensitive attribute) for which labels are
provided, while being informative over all other image attributes, which are
unlabeled. We present a new approach, proposing a new domain-wise contrastive
objective for ensuring invariant representations. This objective crucially
restricts negative image pairs to be drawn from the same domain, which enforces
domain invariance whereas the standard contrastive objective does not. This
domain-wise objective is insufficient on its own as it suffers from shortcut
solutions resulting in feature suppression. We overcome this issue by a
combination of a reconstruction constraint, image augmentations and
initialization with pre-trained weights. Our analysis shows that the choice of
augmentations is important, and that a misguided choice of augmentations can
harm the invariance and informativeness objectives. In an extensive evaluation,
our method convincingly outperforms the state-of-the-art in terms of
representation invariance, representation informativeness, and training speed.
Furthermore, we find that in some cases our method can achieve excellent
results even without the reconstruction constraint, leading to a much faster
and resource efficient training.",https://github.com/jonkahana/DCoDR,3148
Applied monocular reconstruction of parametric faces with domain engineering,0.0138874,"Many modern online 3D applications and videogames rely on parametric models
of human faces for creating believable avatars. However, manual reproduction of
someone's facial likeness with a parametric model is difficult and
time-consuming. Machine Learning solution for that task is highly desirable but
is also challenging. The paper proposes a novel approach to the so-called
Face-to-Parameters problem (F2P for short), aiming to reconstruct a parametric
face from a single image. The proposed method utilizes synthetic data, domain
decomposition, and domain adaptation for addressing multifaceted challenges in
solving the F2P. The open-sourced codebase illustrates our key observations and
provides means for quantitative evaluation. The presented approach proves
practical in an industrial application; it improves accuracy and allows for
more efficient models training. The techniques have the potential to extend to
other types of parametric models.",None,567
Unified Line and Paragraph Detection by Graph Convolutional Networks,0.0147867,"We formulate the task of detecting lines and paragraphs in a document into a
unified two-level clustering problem. Given a set of text detection boxes that
roughly correspond to words, a text line is a cluster of boxes and a paragraph
is a cluster of lines. These clusters form a two-level tree that represents a
major part of the layout of a document. We use a graph convolutional network to
predict the relations between text detection boxes and then build both levels
of clusters from these predictions. Experimentally, we demonstrate that the
unified approach can be highly efficient while still achieving state-of-the-art
quality for detecting paragraphs in public benchmarks and real-world images.",None,1989
ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations,0.446489,"Context is everything, even in commonsense moral reasoning. Changing contexts
can flip the moral judgment of an action; ""Lying to a friend"" is wrong in
general, but may be morally acceptable if it is intended to protect their life.
  We present ClarifyDelphi, an interactive system that learns to ask
clarification questions (e.g., why did you lie to your friend?) in order to
elicit additional salient contexts of a social or moral situation. We posit
that questions whose potential answers lead to diverging moral judgments are
the most informative. Thus, we propose a reinforcement learning framework with
a defeasibility reward that aims to maximize the divergence between moral
judgments of hypothetical answers to a question. Human evaluation demonstrates
that our system generates more relevant, informative and defeasible questions
compared to competitive baselines. Our work is ultimately inspired by studies
in cognitive science that have investigated the flexibility in moral cognition
(i.e., the diverse contexts in which moral rules can be bent), and we hope that
research in this direction can assist both cognitive and computational
investigations of moral judgments.",https://github.com/allenai/clarifydelphi,46374
"Please, Don't Forget the Difference and the Confidence Interval when Seeking for the State-of-the-Art Status",0.00352352,"This paper argues for the widest possible use of bootstrap confidence
intervals for comparing NLP system performances instead of the state-of-the-art
status (SOTA) and statistical significance testing. Their main benefits are to
draw attention to the difference in performance between two systems and to help
assessing the degree of superiority of one system over another. Two cases
studies, one comparing several systems and the other based on a K-fold
cross-validation procedure, illustrate these benefits. A python module for
obtaining these confidence intervals as well as a second function implementing
the Fisher-Pitman test for paired samples are freely available on PyPi.",https://github.com/Pranav-Goel/Neural_Emotion_Intensity_Prediction,4062
Prompting Large Pre-trained Vision-Language Models For Compositional Concept Learning,0.0346826,"This work explores the zero-shot compositional learning ability of large
pre-trained vision-language models(VLMs) within the prompt-based learning
framework and propose a model (\textit{PromptCompVL}) to solve the compositonal
zero-shot learning (CZSL) problem. \textit{PromptCompVL} makes two design
choices: first, it uses a soft-prompting instead of hard-prompting to inject
learnable parameters to reprogram VLMs for compositional learning. Second, to
address the compositional challenge, it uses the soft-embedding layer to learn
primitive concepts in different combinations. By combining both soft-embedding
and soft-prompting, \textit{PromptCompVL} achieves state-of-the-art performance
on the MIT-States dataset. Furthermore, our proposed model achieves consistent
improvement compared to other CLIP-based methods which shows the effectiveness
of the proposed prompting strategies for CZSL.",None,1473
Self-Knowledge Distillation via Dropout,0.0586329,"To boost the performance, deep neural networks require deeper or wider
network structures that involve massive computational and memory costs. To
alleviate this issue, the self-knowledge distillation method regularizes the
model by distilling the internal knowledge of the model itself. Conventional
self-knowledge distillation methods require additional trainable parameters or
are dependent on the data. In this paper, we propose a simple and effective
self-knowledge distillation method using a dropout (SD-Dropout). SD-Dropout
distills the posterior distributions of multiple models through a dropout
sampling. Our method does not require any additional trainable modules, does
not rely on data, and requires only simple operations. Furthermore, this simple
method can be easily combined with various self-knowledge distillation
approaches. We provide a theoretical and experimental analysis of the effect of
forward and reverse KL-divergences in our work. Extensive experiments on
various vision tasks, i.e., image classification, object detection, and
distribution shift, demonstrate that the proposed method can effectively
improve the generalization of a single network. Further experiments show that
the proposed method also improves calibration performance, adversarial
robustness, and out-of-distribution detection ability.",None,295
Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation,0.106007,"Occlusion poses a great threat to monocular multi-person 3D human pose
estimation due to large variability in terms of the shape, appearance, and
position of occluders. While existing methods try to handle occlusion with pose
priors/constraints, data augmentation, or implicit reasoning, they still fail
to generalize to unseen poses or occlusion cases and may make large mistakes
when multiple people are present. Inspired by the remarkable ability of humans
to infer occluded joints from visible cues, we develop a method to explicitly
model this process that significantly improves bottom-up multi-person human
pose estimation with or without occlusions. First, we split the task into two
subtasks: visible keypoints detection and occluded keypoints reasoning, and
propose a Deeply Supervised Encoder Distillation (DSED) network to solve the
second one. To train our model, we propose a Skeleton-guided human Shape
Fitting (SSF) approach to generate pseudo occlusion labels on the existing
datasets, enabling explicit occlusion reasoning. Experiments show that
explicitly learning from occlusions improves human pose estimation. In
addition, exploiting feature-level information of visible joints allows us to
reason about occluded joints more accurately. Our method outperforms both the
state-of-the-art top-down and bottom-up methods on several benchmarks.",None,114452
Crowdsourcing Relative Rankings of Multi-Word Expressions: Experts versus Non-Experts,0.0509888,"In this study we investigate to which degree experts and non-experts agree on
questions of difficulty in a crowdsourcing experiment. We ask non-experts
(second language learners of Swedish) and two groups of experts (teachers of
Swedish as a second/foreign language and CEFR experts) to rank multi-word
expressions in a crowdsourcing experiment. We find that the resulting rankings
by all the three tested groups correlate to a very high degree, which suggests
that judgments produced in a comparative setting are not influenced by
professional insights into Swedish as a second language.",None,1279
Team VI-I2R Technical Report on EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021,0.0154179,"In this report, we present the technical details of our approach to the
EPIC-KITCHENS-100 Unsupervised Domain Adaptation (UDA) Challenge for Action
Recognition. The EPIC-KITCHENS-100 dataset consists of daily kitchen activities
focusing on the interaction between human hands and their surrounding objects.
It is very challenging to accurately recognize these fine-grained activities,
due to the presence of distracting objects and visually similar action classes,
especially in the unlabelled target domain. Based on an existing method for
video domain adaptation, i.e., TA3N, we propose to learn hand-centric features
by leveraging the hand bounding box information for UDA on fine-grained action
recognition. This helps reduce the distraction from background as well as
facilitate the learning of domain-invariant features. To achieve high quality
hand localization, we adopt an uncertainty-aware domain adaptation network,
i.e., MEAA, to train a domain-adaptive hand detector, which only uses very
limited hand bounding box annotations in the source domain but can generalize
well to the unlabelled target domain. Our submission achieved the 1st place in
terms of top-1 action recognition accuracy, using only RGB and optical flow
modalities as input.",None,4082
A dataset of ant colonies motion trajectories in indoor and outdoor scenes for social cluster behavior study,0.0265708,"Motion and interaction of social insects (such as ants) have been studied by
many researchers to understand the clustering mechanism. Most studies in the
field of ant behavior have only focused on indoor environments, while outdoor
environments are still underexplored. In this paper, we collect 10 videos of
ant colonies from different indoor and outdoor scenes. And we develop an image
sequence marking software named VisualMarkData, which enables us to provide
annotations of ants in the video. In all 5354 frames, the location information
and the identification number of each ant are recorded for a total of 712 ants
and 114112 annotations. Moreover, we provide visual analysis tools to assess
and validate the technical quality and reproducibility of our data. It is hoped
that this dataset will contribute to a deeper exploration on the behavior of
the ant colony.",https://github.com/holmescao/ANTS_marking_and_analysis_tools,907
Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts,0.0893511,"Despite the advances in digital healthcare systems offering curated
structured knowledge, much of the critical information still lies in large
volumes of unlabeled and unstructured clinical texts. These texts, which often
contain protected health information (PHI), are exposed to information
extraction tools for downstream applications, risking patient identification.
Existing works in de-identification rely on using large-scale annotated corpora
in English, which often are not suitable in real-world multilingual settings.
Pre-trained language models (LM) have shown great potential for cross-lingual
transfer in low-resource settings. In this work, we empirically show the
few-shot cross-lingual transfer property of LMs for named entity recognition
(NER) and apply it to solve a low-resource and real-world challenge of
code-mixed (Spanish-Catalan) clinical notes de-identification in the stroke
domain. We annotate a gold evaluation dataset to assess few-shot setting
performance where we only use a few hundred labeled examples for training. Our
model improves the zero-shot F1-score from 73.7% to 91.2% on the gold
evaluation set when adapting Multilingual BERT (mBERT) (Devlin et al., 2019)
from the MEDDOCAN (Marimon et al., 2019) corpus with our few-shot cross-lingual
target corpus. When generalized to an out-of-sample test set, the best model
achieves a human-evaluation F1-score of 97.2%.",https://github.com/suamin/T2NER,196
Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments,0.223418,"Recent work in Vision-and-Language Navigation (VLN) has presented two
environmental paradigms with differing realism -- the standard VLN setting
built on topological environments where navigation is abstracted away, and the
VLN-CE setting where agents must navigate continuous 3D environments using
low-level actions. Despite sharing the high-level task and even the underlying
instruction-path data, performance on VLN-CE lags behind VLN significantly. In
this work, we explore this gap by transferring an agent from the abstract
environment of VLN to the continuous environment of VLN-CE. We find that this
sim-2-sim transfer is highly effective, improving over the prior state of the
art in VLN-CE by +12% success rate. While this demonstrates the potential for
this direction, the transfer does not fully retain the original performance of
the agent in the abstract setting. We present a sequence of experiments to
identify what differences result in performance degradation, providing clear
directions for further improvement.",https://github.com/YicongHong/Recurrent-VLN-BERT,8982
Ethics for Digital Medicine: A Path for Ethical Emerging Medical IoT Design,0.0910585,"The dawn of the digital medicine era, ushered in by increasingly powerful
embedded systems and Internet of Things (IoT) computing devices, is creating
new therapies and biomedical solutions that promise to positively transform our
quality of life. However, the digital medicine revolution also creates
unforeseen and complex ethical, regulatory, and societal issues. In this
article, we reflect on the ethical challenges facing digital medicine. We
discuss the perils of ethical oversights in medical devices, and the role of
professional codes and regulatory oversight towards the ethical design,
deployment, and operation of digital medicine devices that safely and
effectively meet the needs of patients. We advocate for an ensemble approach of
intensive education, programmable ethical behaviors, and ethical analysis
frameworks, to prevent mishaps and sustain ethical innovation, design, and
lifecycle management of emerging digital medicine devices.",None,7012
Design and Development of Rule-based open-domain Question-Answering System on SQuAD v2.0 Dataset,0.0651143,"Human mind is the palace of curious questions that seek answers.
Computational resolution of this challenge is possible through Natural Language
Processing techniques. Statistical techniques like machine learning and deep
learning require a lot of data to train and despite that they fail to tap into
the nuances of language. Such systems usually perform best on close-domain
datasets. We have proposed development of a rule-based open-domain
question-answering system which is capable of answering questions of any domain
from a corresponding context passage. We have used 1000 questions from SQuAD
2.0 dataset for testing the developed system and it gives satisfactory results.
In this paper, we have described the structure of the developed system and have
analyzed the performance.",None,1659
What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured,0.0311228,"Transformer-based models are now predominant in NLP. They outperform
approaches based on static models in many respects. This success has in turn
prompted research that reveals a number of biases in the language models
generated by transformers. In this paper we utilize this research on biases to
investigate to what extent transformer-based language models allow for
extracting knowledge about object relations (X occurs in Y; X consists of Z;
action A involves using X). To this end, we compare contextualized models with
their static counterparts. We make this comparison dependent on the application
of a number of similarity measures and classifiers. Our results are threefold:
Firstly, we show that the models combined with the different similarity
measures differ greatly in terms of the amount of knowledge they allow for
extracting. Secondly, our results suggest that similarity measures perform much
worse than classifier-based approaches. Thirdly, we show that, surprisingly,
static models perform almost as well as contextualized models -- in some cases
even better.",None,3603
Neural Space-filling Curves,0.0327947,"We present Neural Space-filling Curves (SFCs), a data-driven approach to
infer a context-based scan order for a set of images. Linear ordering of pixels
forms the basis for many applications such as video scrambling, compression,
and auto-regressive models that are used in generative modeling for images.
Existing algorithms resort to a fixed scanning algorithm such as Raster scan or
Hilbert scan. Instead, our work learns a spatially coherent linear ordering of
pixels from the dataset of images using a graph-based neural network. The
resulting Neural SFC is optimized for an objective suitable for the downstream
task when the image is traversed along with the scan line order. We show the
advantage of using Neural SFCs in downstream applications such as image
compression. Code and additional results will be made available at
https://hywang66.github.io/publication/neuralsfc.",None,13015
Robust and Accurate -- Compositional Architectures for Randomized Smoothing,0.0385201,"Randomized Smoothing (RS) is considered the state-of-the-art approach to
obtain certifiably robust models for challenging tasks. However, current RS
approaches drastically decrease standard accuracy on unperturbed data, severely
limiting their real-world utility. To address this limitation, we propose a
compositional architecture, ACES, which certifiably decides on a per-sample
basis whether to use a smoothed model yielding predictions with guarantees or a
more accurate standard model without guarantees. This, in contrast to prior
approaches, enables both high standard accuracies and significant provable
robustness. On challenging tasks such as ImageNet, we obtain, e.g., $80.0\%$
natural accuracy and $28.2\%$ certifiable accuracy against $\ell_2$
perturbations with $r=1.0$. We release our code and models at
https://github.com/eth-sri/aces.",https://github.com/eth-sri/aces,14821
Traceable and Authenticable Image Tagging for Fake News Detection,0.00741567,"To prevent fake news images from misleading the public, it is desirable not
only to verify the authenticity of news images but also to trace the source of
fake news, so as to provide a complete forensic chain for reliable fake news
detection. To simultaneously achieve the goals of authenticity verification and
source tracing, we propose a traceable and authenticable image tagging approach
that is based on a design of Decoupled Invertible Neural Network (DINN). The
designed DINN can simultaneously embed the dual-tags, \textit{i.e.},
authenticable tag and traceable tag, into each news image before publishing,
and then separately extract them for authenticity verification and source
tracing. Moreover, to improve the accuracy of dual-tags extraction, we design a
parallel Feature Aware Projection Model (FAPM) to help the DINN preserve
essential tag information. In addition, we define a Distance Metric-Guided
Module (DMGM) that learns asymmetric one-class representations to enable the
dual-tags to achieve different robustness performances under malicious
manipulations. Extensive experiments, on diverse datasets and unseen
manipulations, demonstrate that the proposed tagging approach achieves
excellent performance in the aspects of both authenticity verification and
source tracing for reliable fake news detection and outperforms the prior
works.",None,18428
Generalized Probabilistic U-Net for medical image segementation,0.0268776,"We propose the Generalized Probabilistic U-Net, which extends the
Probabilistic U-Net by allowing more general forms of the Gaussian distribution
as the latent space distribution that can better approximate the uncertainty in
the reference segmentations. We study the effect the choice of latent space
distribution has on capturing the uncertainty in the reference segmentations
using the LIDC-IDRI dataset. We show that the choice of distribution affects
the sample diversity of the predictions and their overlap with respect to the
reference segmentations. For the LIDC-IDRI dataset, we show that using a
mixture of Gaussians results in a statistically significant improvement in the
generalized energy distance (GED) metric with respect to the standard
Probabilistic U-Net. We have made our implementation available at
https://github.com/ishaanb92/GeneralizedProbabilisticUNet",https://github.com/ishaanb92/GeneralizedProbabilisticUNet,5946
Knowledge Transfer from Answer Ranking to Answer Generation,0.0367087,"Recent studies show that Question Answering (QA) based on Answer Sentence
Selection (AS2) can be improved by generating an improved answer from the top-k
ranked answer sentences (termed GenQA). This allows for synthesizing the
information from multiple candidates into a concise, natural-sounding answer.
However, creating large-scale supervised training data for GenQA models is very
challenging. In this paper, we propose to train a GenQA model by transferring
knowledge from a trained AS2 model, to overcome the aforementioned issue.
First, we use an AS2 model to produce a ranking over answer candidates for a
set of questions. Then, we use the top ranked candidate as the generation
target, and the next k top ranked candidates as context for training a GenQA
model. We also propose to use the AS2 model prediction scores for loss
weighting and score-conditioned input/output shaping, to aid the knowledge
transfer. Our evaluation on three public and one large industrial datasets
demonstrates the superiority of our approach over the AS2 baseline, and GenQA
trained using supervised data.",https://github.com/amazon-research/wqa-genqa-knowledge-transfer,14771
Split-U-Net: Preventing Data Leakage in Split Learning for Collaborative Multi-Modal Brain Tumor Segmentation,0.049545,"Split learning (SL) has been proposed to train deep learning models in a
decentralized manner. For decentralized healthcare applications with vertical
data partitioning, SL can be beneficial as it allows institutes with
complementary features or images for a shared set of patients to jointly
develop more robust and generalizable models. In this work, we propose
""Split-U-Net"" and successfully apply SL for collaborative biomedical image
segmentation. Nonetheless, SL requires the exchanging of intermediate
activation maps and gradients to allow training models across different feature
spaces, which might leak data and raise privacy concerns. Therefore, we also
quantify the amount of data leakage in common SL scenarios for biomedical image
segmentation and provide ways to counteract such leakage by applying
appropriate defense strategies.",None,21315
On the Usefulness of the Fit-on-the-Test View on Evaluating Calibration of Classifiers,0.0,"Every uncalibrated classifier has a corresponding true calibration map that
calibrates its confidence. Deviations of this idealistic map from the identity
map reveal miscalibration. Such calibration errors can be reduced with many
post-hoc calibration methods which fit some family of calibration maps on a
validation dataset. In contrast, evaluation of calibration with the expected
calibration error (ECE) on the test set does not explicitly involve fitting.
However, as we demonstrate, ECE can still be viewed as if fitting a family of
functions on the test data. This motivates the fit-on-the-test view on
evaluation: first, approximate a calibration map on the test data, and second,
quantify its distance from the identity. Exploiting this view allows us to
unlock missed opportunities: (1) use the plethora of post-hoc calibration
methods for evaluating calibration; (2) tune the number of bins in ECE with
cross-validation. Furthermore, we introduce: (3) benchmarking on pseudo-real
data where the true calibration map can be estimated very precisely; and (4)
novel calibration and evaluation methods using new calibration map families PL
and PL3.",https://github.com/markus93/fit-on-the-test,4116
Task Grouping for Multilingual Text Recognition,0.0627957,"Most existing OCR methods focus on alphanumeric characters due to the
popularity of English and numbers, as well as their corresponding datasets. On
extending the characters to more languages, recent methods have shown that
training different scripts with different recognition heads can greatly improve
the end-to-end recognition accuracy compared to combining characters from all
languages in the same recognition head. However, we postulate that similarities
between some languages could allow sharing of model parameters and benefit from
joint training. Determining language groupings, however, is not immediately
obvious. To this end, we propose an automatic method for multilingual text
recognition with a task grouping and assignment module using Gumbel-Softmax,
introducing a task grouping loss and weighted recognition loss to allow for
simultaneous training of the models and grouping modules. Experiments on MLT19
lend evidence to our hypothesis that there is a middle ground between combining
every task together and separating every task that achieves a better
configuration of task grouping/separation.",https://github.com/facebookresearch/MultiplexedOCR,18752
License Plate Privacy in Collaborative Visual Analysis of Traffic Scenes,0.0731315,"Traffic scene analysis is important for emerging technologies such as smart
traffic management and autonomous vehicles. However, such analysis also poses
potential privacy threats. For example, a system that can recognize license
plates may construct patterns of behavior of the corresponding vehicles' owners
and use that for various illegal purposes. In this paper we present a system
that enables traffic scene analysis while at the same time preserving license
plate privacy. The system is based on a multi-task model whose latent space is
selectively compressed depending on the amount of information the specific
features carry about analysis tasks and private information. Effectiveness of
the proposed method is illustrated by experiments on the Cityscapes dataset,
for which we also provide license plate annotations.",None,3
Scale-free and Task-agnostic Attack: Generating Photo-realistic Adversarial Patterns with Patch Quilting Generator,0.134751,"\noindent Traditional L_p norm-restricted image attack algorithms suffer from
poor transferability to black box scenarios and poor robustness to defense
algorithms. Recent CNN generator-based attack approaches can synthesize
unrestricted and semantically meaningful entities to the image, which is shown
to be transferable and robust. However, such methods attack images by either
synthesizing local adversarial entities, which are only suitable for attacking
specific contents or performing global attacks, which are only applicable to a
specific image scale. In this paper, we propose a novel Patch Quilting
Generative Adversarial Networks (PQ-GAN) to learn the first scale-free CNN
generator that can be applied to attack images with arbitrary scales for
various computer vision tasks. The principal investigation on transferability
of the generated adversarial examples, robustness to defense frameworks, and
visual quality assessment show that the proposed PQG-based attack framework
outperforms the other nine state-of-the-art adversarial attack approaches when
attacking the neural networks trained on two standard evaluation datasets
(i.e., ImageNet and CityScapes).",https://anonymous.4open.science/r/PQAttack-0781,13692
Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification,0.05937,"Few-shot text classification aims to classify the text under the few-shot
scenario. Most of the previous methods adopt optimization-based meta learning
to obtain task distribution. However, due to the neglect of matching between
the few amount of samples and complicated models, as well as the distinction
between useful and useless task features, these methods suffer from the
overfitting issue. To address this issue, we propose a novel Adaptive
Meta-learner via Gradient Similarity (AMGS) method to improve the model
generalization ability to a new task. Specifically, the proposed AMGS
alleviates the overfitting based on two aspects: (i) acquiring the potential
semantic representation of samples and improving model generalization through
the self-supervised auxiliary task in the inner loop, (ii) leveraging the
adaptive meta-learner via gradient similarity to add constraints on the
gradient obtained by base-learner in the outer loop. Moreover, we make a
systematic analysis of the influence of regularization on the entire framework.
Experimental results on several benchmarks demonstrate that the proposed AMGS
consistently improves few-shot text classification performance compared with
the state-of-the-art optimization-based meta-learning approaches.",https://github.com/Tianyi-Lei,3279
Instance Segmentation of Unlabeled Modalities via Cyclic Segmentation GAN,0.115086,"Instance segmentation for unlabeled imaging modalities is a challenging but
essential task as collecting expert annotation can be expensive and
time-consuming. Existing works segment a new modality by either deploying a
pre-trained model optimized on diverse training data or conducting domain
translation and image segmentation as two independent steps. In this work, we
propose a novel Cyclic Segmentation Generative Adversarial Network (CySGAN)
that conducts image translation and instance segmentation jointly using a
unified framework. Besides the CycleGAN losses for image translation and
supervised losses for the annotated source domain, we introduce additional
self-supervised and segmentation-based adversarial objectives to improve the
model performance by leveraging unlabeled target domain images. We benchmark
our approach on the task of 3D neuronal nuclei segmentation with annotated
electron microscopy (EM) images and unlabeled expansion microscopy (ExM) data.
Our CySGAN outperforms both pretrained generalist models and the baselines that
sequentially conduct image translation and segmentation. Our implementation and
the newly collected, densely annotated ExM nuclei dataset, named NucExM, are
available at https://connectomics-bazaar.github.io/proj/CySGAN/index.html.",None,65775
Development of Automatic Endotracheal Tube and Carina Detection on Portable Supine Chest Radiographs using Artificial Intelligence,0.0868051,"The image quality of portable supine chest radiographs is inherently poor due
to low contrast and high noise. The endotracheal intubation detection requires
the locations of the endotracheal tube (ETT) tip and carina. The goal is to
find the distance between the ETT tip and the carina in chest radiography. To
overcome such a problem, we propose a feature extraction method with Mask
R-CNN. The Mask R-CNN predicts a tube and a tracheal bifurcation in an image.
Then, the feature extraction method is used to find the feature point of the
ETT tip and that of the carina. Therefore, the ETT-carina distance can be
obtained. In our experiments, our results can exceed 96\% in terms of recall
and precision. Moreover, the object error is less than $4.7751\pm 5.3420$ mm,
and the ETT-carina distance errors are less than $5.5432\pm 6.3100$ mm. The
external validation shows that the proposed method is a high-robustness system.
According to the Pearson correlation coefficient, we have a strong correlation
between the board-certified intensivists and our result in terms of ETT-carina
distance.",None,857
Pessimistic Off-Policy Optimization for Learning to Rank,0.00926374,"Off-policy learning is a framework for optimizing policies without deploying
them, using data collected by another policy. In recommender systems, this is
especially challenging due to the imbalance in logged data: some items are
recommended and thus logged more frequently than others. This is further
perpetuated when recommending a list of items, as the action space is
combinatorial. To address this challenge, we study pessimistic off-policy
optimization for learning to rank. The key idea is to compute lower confidence
bounds on parameters of click models and then return the list with the highest
pessimistic estimate of its value. This approach is computationally efficient
and we analyze it. We study its Bayesian and frequentist variants, and overcome
the limitation of unknown prior by incorporating empirical Bayes. To show the
empirical effectiveness of our approach, we compare it to off-policy optimizers
that use inverse propensity scores or neglect uncertainty. Our approach
outperforms all baselines, is robust, and is also general.",None,5108
Consistency driven Sequential Transformers Attention Model for Partially Observable Scenes,0.0450168,"Most hard attention models initially observe a complete scene to locate and
sense informative glimpses, and predict class-label of a scene based on
glimpses. However, in many applications (e.g., aerial imaging), observing an
entire scene is not always feasible due to the limited time and resources
available for acquisition. In this paper, we develop a Sequential Transformers
Attention Model (STAM) that only partially observes a complete image and
predicts informative glimpse locations solely based on past glimpses. We design
our agent using DeiT-distilled and train it with a one-step actor-critic
algorithm. Furthermore, to improve classification performance, we introduce a
novel training objective, which enforces consistency between the class
distribution predicted by a teacher model from a complete image and the class
distribution predicted by our agent using glimpses. When the agent senses only
4% of the total image area, the inclusion of the proposed consistency loss in
our training objective yields 3% and 8% higher accuracy on ImageNet and fMoW
datasets, respectively. Moreover, our agent outperforms previous
state-of-the-art by observing nearly 27% and 42% fewer pixels in glimpses on
ImageNet and fMoW.",https://github.com/samrudhdhirangrej/STAM-Sequential-Transformers-Attention-Model,4005
ACORT: A Compact Object Relation Transformer for Parameter Efficient Image Captioning,0.0724399,"Recent research that applies Transformer-based architectures to image
captioning has resulted in state-of-the-art image captioning performance,
capitalising on the success of Transformers on natural language tasks.
Unfortunately, though these models work well, one major flaw is their large
model sizes. To this end, we present three parameter reduction methods for
image captioning Transformers: Radix Encoding, cross-layer parameter sharing,
and attention parameter sharing. By combining these methods, our proposed ACORT
models have 3.7x to 21.6x fewer parameters than the baseline model without
compromising test performance. Results on the MS-COCO dataset demonstrate that
our ACORT models are competitive against baselines and SOTA approaches, with
CIDEr score >=126. Finally, we present qualitative results and ablation studies
to demonstrate the efficacy of the proposed changes further. Code and
pre-trained models are publicly available at
https://github.com/jiahuei/sparse-image-captioning.",https://github.com/jiahuei/sparse-image-captioning,7891
Self-Supervised Domain Calibration and Uncertainty Estimation for Place Recognition,0.0113976,"Visual place recognition techniques based on deep learning, which have
imposed themselves as the state-of-the-art in recent years, do not generalize
well to environments visually different from the training set. Thus, to achieve
top performance, it is sometimes necessary to fine-tune the networks to the
target environment. To this end, we propose a self-supervised domain
calibration procedure based on robust pose graph optimization from Simultaneous
Localization and Mapping (SLAM) as the supervision signal without requiring GPS
or manual labeling. Moreover, we leverage the procedure to improve uncertainty
estimation for place recognition matches which is important in safety critical
applications. We show that our approach can improve the performance of a
state-of-the-art technique on a target environment dissimilar from its training
set and that we can obtain uncertainty estimates. We believe that this approach
will help practitioners to deploy robust place recognition solutions in
real-world applications. Our code is available publicly:
https://github.com/MISTLab/vpr-calibration-and-uncertainty",https://github.com/MISTLab/vpr-calibration-and-uncertainty,2903
Natural Language Processing for Cognitive Analysis of Emotions,0.00597033,"Emotion analysis in texts suffers from two major limitations: annotated
gold-standard corpora are mostly small and homogeneous, and emotion
identification is often simplified as a sentence-level classification problem.
To address these issues, we introduce a new annotation scheme for exploring
emotions and their causes, along with a new French dataset composed of
autobiographical accounts of an emotional scene. The texts were collected by
applying the Cognitive Analysis of Emotions developed by A. Finkel to help
people improve on their emotion management. The method requires the manual
analysis of an emotional event by a coach trained in Cognitive Analysis. We
present a rule-based approach to automatically annotate emotions and their
semantic roles (e.g. emotion causes) to facilitate the identification of
relevant aspects by the coach. We investigate future directions for emotion
analysis using graph structures.",https://github.com/pandora-intelligence/crosslingual-coreference,6994
Compositional Generalization Requires Compositional Parsers,0.045046,"A rapidly growing body of research on compositional generalization
investigates the ability of a semantic parser to dynamically recombine
linguistic elements seen in training into unseen sequences. We present a
systematic comparison of sequence-to-sequence models and models guided by
compositional principles on the recent COGS corpus (Kim and Linzen, 2020).
Though seq2seq models can perform well on lexical tasks, they perform with
near-zero accuracy on structural generalization tasks that require novel
syntactic structures; this holds true even when they are trained to predict
syntax instead of semantics. In contrast, compositional models achieve
near-perfect accuracy on structural generalization; we present new results
confirming this from the AM parser (Groschwitz et al., 2021). Our findings show
structural generalization is a key measure of compositional generalization and
requires models that are aware of complex structure.",https://github.com/najoungkim/COGS,4148
Debugging using Orthogonal Gradient Descent,0.00334628,"In this report we consider the following problem: Given a trained model that
is partially faulty, can we correct its behaviour without having to train the
model from scratch? In other words, can we ``debug"" neural networks similar to
how we address bugs in our mathematical models and standard computer code. We
base our approach on the hypothesis that debugging can be treated as a two-task
continual learning problem. In particular, we employ a modified version of a
continual learning algorithm called Orthogonal Gradient Descent (OGD) to
demonstrate, via two simple experiments on the MNIST dataset, that we can
in-fact \textit{unlearn} the undesirable behaviour while retaining the general
performance of the model, and we can additionally \textit{relearn} the
appropriate behaviour, both without having to train the model from scratch.",None,12567
Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record,0.0148141,"Objective: Social determinants of health (SDOH) impact health outcomes and
are documented in the electronic health record (EHR) through structured data
and unstructured clinical notes. However, clinical notes often contain more
comprehensive SDOH information, detailing aspects such as status, severity, and
temporality. This work has two primary objectives: i) develop a natural
language processing (NLP) information extraction model to capture detailed SDOH
information and ii) evaluate the information gain achieved by applying the SDOH
extractor to clinical narratives and combining the extracted representations
with existing structured data.
  Materials and Methods: We developed a novel SDOH extractor using a deep
learning entity and relation extraction architecture to characterize SDOH
across various dimensions. In an EHR case study, we applied the SDOH extractor
to a large clinical data set with 225,089 patients and 430,406 notes with
social history sections and compared the extracted SDOH information with
existing structured data.
  Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the
EHR case study, we found extracted SDOH information complements existing
structured data with 32% of homeless patients, 19% of current tobacco users,
and 10% of drug users only having these health risk factors documented in the
clinical narrative.
  Conclusions: Utilizing EHR data to identify SDOH health risk factors and
social needs may improve patient care and outcomes. Semantic representations of
text-encoded SDOH information can augment existing structured data, and this
more comprehensive SDOH representation can assist health systems in identifying
and addressing these social needs.",https://github.com/uw-bionlp/mspert,8863
Knowledge-Grounded Conversational Data Augmentation with Generative Conversational Networks,0.0147488,"While rich, open-domain textual data are generally available and may include
interesting phenomena (humor, sarcasm, empathy, etc.) most are designed for
language processing tasks, and are usually in a non-conversational format. In
this work, we take a step towards automatically generating conversational data
using Generative Conversational Networks, aiming to benefit from the breadth of
available language and knowledge data, and train open domain social
conversational agents. We evaluate our approach on conversations with and
without knowledge on the Topical Chat dataset using automatic metrics and human
evaluators. Our results show that for conversations without knowledge
grounding, GCN can generalize from the seed data, producing novel conversations
that are less relevant but more engaging and for knowledge-grounded
conversations, it can produce more knowledge-focused, fluent, and engaging
conversations. Specifically, we show that for open-domain conversations with
10\% of seed data, our approach performs close to the baseline that uses 100%
of the data, while for knowledge-grounded conversations, it achieves the same
using only 1% of the data, on human ratings of engagingness, fluency, and
relevance.",None,21260
MVP: Robust Multi-View Practice for Driving Action Localization,0.0458675,"Distracted driving causes thousands of deaths per year, and how to apply
deep-learning methods to prevent these tragedies has become a crucial problem.
In Track3 of the 6th AI City Challenge, researchers provide a high-quality
video dataset with densely action annotations. Due to the small data scale and
unclear action boundary, the dataset presents a unique challenge to precisely
localize all the different actions and classify their categories. In this
paper, we make good use of the multi-view synchronization among videos, and
conduct robust Multi-View Practice (MVP) for driving action localization. To
avoid overfitting, we fine-tune SlowFast with Kinetics-700 pre-training as the
feature extractor. Then the features of different views are passed to
ActionFormer to generate candidate action proposals. For precisely localizing
all the actions, we design elaborate post-processing, including model voting,
threshold filtering and duplication removal. The results show that our MVP is
robust for driving action localization, which achieves 28.49% F1-score in the
Track3 test set.",None,1527
Harnessing Artificial Intelligence to Infer Novel Spatial Biomarkers for the Diagnosis of Eosinophilic Esophagitis,0.168528,"Eosinophilic esophagitis (EoE) is a chronic allergic inflammatory condition
of the esophagus associated with elevated esophageal eosinophils. Second only
to gastroesophageal reflux disease, EoE is one of the leading causes of chronic
refractory dysphagia in adults and children. EoE diagnosis requires enumerating
the density of esophageal eosinophils in esophageal biopsies, a somewhat
subjective task that is time-consuming, thus reducing the ability to process
the complex tissue structure. Previous artificial intelligence (AI) approaches
that aimed to improve histology-based diagnosis focused on recapitulating
identification and quantification of the area of maximal eosinophil density.
However, this metric does not account for the distribution of eosinophils or
other histological features, over the whole slide image. Here, we developed an
artificial intelligence platform that infers local and spatial biomarkers based
on semantic segmentation of intact eosinophils and basal zone distributions.
Besides the maximal density of eosinophils (referred to as Peak Eosinophil
Count [PEC]) and a maximal basal zone fraction, we identify two additional
metrics that reflect the distribution of eosinophils and basal zone fractions.
This approach enables a decision support system that predicts EoE activity and
classifies the histological severity of EoE patients. We utilized a cohort that
includes 1066 biopsy slides from 400 subjects to validate the system's
performance and achieved a histological severity classification accuracy of
86.70%, sensitivity of 84.50%, and specificity of 90.09%. Our approach
highlights the importance of systematically analyzing the distribution of
biopsy features over the entire slide and paves the way towards a personalized
decision support system that will assist not only in counting cells but can
also potentially improve diagnosis and provide treatment prediction.",None,73438
Seq2Seq Surrogates of Epidemic Models to Facilitate Bayesian Inference,0.0154675,"Epidemic models are powerful tools in understanding infectious disease.
However, as they increase in size and complexity, they can quickly become
computationally intractable. Recent progress in modelling methodology has shown
that surrogate models can be used to emulate complex epidemic models with a
high-dimensional parameter space. We show that deep sequence-to-sequence
(seq2seq) models can serve as accurate surrogates for complex epidemic models
with sequence based model parameters, effectively replicating seasonal and
long-term transmission dynamics. Once trained, our surrogate can predict
scenarios a several thousand times faster than the original model, making them
ideal for policy exploration. We demonstrate that replacing a traditional
epidemic model with a learned simulator facilitates robust Bayesian inference.",None,88105
Graph Neural Networks and Representation Embedding for Table Extraction in PDF Documents,0.105325,"Tables are widely used in several types of documents since they can bring
important information in a structured way. In scientific papers, tables can sum
up novel discoveries and summarize experimental results, making the research
comparable and easily understandable by scholars. Several methods perform table
analysis working on document images, losing useful information during the
conversion from the PDF files since OCR tools can be prone to recognition
errors, in particular for text inside tables. The main contribution of this
work is to tackle the problem of table extraction, exploiting Graph Neural
Networks. Node features are enriched with suitably designed representation
embeddings. These representations help to better distinguish not only tables
from the other parts of the paper, but also table cells from table headers. We
experimentally evaluated the proposed approach on a new dataset obtained by
merging the information provided in the PubLayNet and PubTables-1M datasets.",None,2625
"Can counterfactual explanations of AI systems' predictions skew lay users' causal intuitions about the world? If so, can we correct for that?",0.135174,"Counterfactual (CF) explanations have been employed as one of the modes of
explainability in explainable AI-both to increase the transparency of AI
systems and to provide recourse. Cognitive science and psychology, however,
have pointed out that people regularly use CFs to express causal relationships.
Most AI systems are only able to capture associations or correlations in data
so interpreting them as casual would not be justified. In this paper, we
present two experiment (total N = 364) exploring the effects of CF explanations
of AI system's predictions on lay people's causal beliefs about the real world.
In Experiment 1 we found that providing CF explanations of an AI system's
predictions does indeed (unjustifiably) affect people's causal beliefs
regarding factors/features the AI uses and that people are more likely to view
them as causal factors in the real world. Inspired by the literature on
misinformation and health warning messaging, Experiment 2 tested whether we can
correct for the unjustified change in causal beliefs. We found that pointing
out that AI systems capture correlations and not necessarily causal
relationships can attenuate the effects of CF explanations on people's causal
beliefs.",https://osf.io/xu7v6/,9000
Hyperplane bounds for neural feature mappings,0.0104288,"Deep learning methods minimise the empirical risk using loss functions such
as the cross entropy loss. When minimising the empirical risk, the
generalisation of the learnt function still depends on the performance on the
training data, the Vapnik-Chervonenkis(VC)-dimension of the function and the
number of training examples. Neural networks have a large number of parameters,
which correlates with their VC-dimension that is typically large but not
infinite, and typically a large number of training instances are needed to
effectively train them.
  In this work, we explore how to optimize feature mappings using neural
network with the intention to reduce the effective VC-dimension of the
hyperplane found in the space generated by the mapping. An interpretation of
the results of this study is that it is possible to define a loss that controls
the VC-dimension of the separating hyperplane. We evaluate this approach and
observe that the performance when using this method improves when the size of
the training set is small.",https://github.com/ajjimeno/nn-hyperplane-bounds,-1
Personalized Game Difficulty Prediction Using Factorization Machines,0.0224337,"The accurate and personalized estimation of task difficulty provides many
opportunities for optimizing user experience. However, user diversity makes
such difficulty estimation hard, in that empirical measurements from some user
sample do not necessarily generalize to others. In this paper, we contribute a
new approach for personalized difficulty estimation of game levels, borrowing
methods from content recommendation. Using factorization machines (FM) on a
large dataset from a commercial puzzle game, we are able to predict difficulty
as the number of attempts a player requires to pass future game levels, based
on observed attempt counts from earlier levels and levels played by others. In
addition to performance and scalability, FMs offer the benefit that the learned
latent variable model can be used to study the characteristics of both players
and game levels that contribute to difficulty. We compare the approach to a
simple non-personalized baseline and a personalized prediction using Random
Forests. Our results suggest that FMs are a promising tool enabling game
designers to both optimize player experience and learn more about their players
and the game.",None,3054
ProspectNet: Weighted Conditional Attention for Future Interaction Modeling in Behavior Prediction,0.0158765,"Behavior prediction plays an important role in integrated autonomous driving
software solutions. In behavior prediction research, interactive behavior
prediction is a less-explored area, compared to single-agent behavior
prediction. Predicting the motion of interactive agents requires initiating
novel mechanisms to capture the joint behaviors of the interactive pairs. In
this work, we formulate the end-to-end joint prediction problem as a sequential
learning process of marginal learning and joint learning of vehicle behaviors.
We propose ProspectNet, a joint learning block that adopts the weighted
attention score to model the mutual influence between interactive agent pairs.
The joint learning block first weighs the multi-modal predicted candidate
trajectories, then updates the ego-agent's embedding via cross attention.
Furthermore, we broadcast the individual future predictions for each
interactive agent into a pair-wise scoring module to select the top $K$
prediction pairs. We show that ProspectNet outperforms the Cartesian product of
two marginal predictions, and achieves comparable performance on the Waymo
Interactive Motion Prediction benchmarks.",None,428
Strong Instance Segmentation Pipeline for MMSports Challenge,0.0106876,"The goal of ACM MMSports2022 DeepSportRadar Instance Segmentation Challenge
is to tackle the segmentation of individual humans including players, coaches
and referees on a basketball court. And the main characteristics of this
challenge are there is a high level of occlusions between players and the
amount of data is quite limited. In order to address these problems, we
designed a strong instance segmentation pipeline. Firstly, we employed a proper
data augmentation strategy for this task mainly including photometric
distortion transform and copy-paste strategy, which can generate more image
instances with a wider distribution. Secondly, we employed a strong
segmentation model, Hybrid Task Cascade based detector on the Swin-Base based
CBNetV2 backbone, and we add MaskIoU head to HTCMaskHead that can simply and
effectively improve the performance of instance segmentation. Finally, the SWA
training strategy was applied to improve the performance further. Experimental
results demonstrate the proposed pipeline can achieve a competitive result on
the DeepSportRadar challenge, with 0.768AP@0.50:0.95 on the challenge set.
Source code is available at
https://github.com/YJingyu/Instanc_Segmentation_Pro.",https://github.com/YJingyu/Instanc_Segmentation_Pro,297
Near-Term Advances in Quantum Natural Language Processing,0.023988,"This paper describes experiments showing that some tasks in natural language
processing (NLP) can already be performed using quantum computers, though so
far only with small datasets.
  We demonstrate various approaches to topic classification. The first uses an
explicit word-based approach, in which word-topic scoring weights are
implemented as fractional rotations of individual qubit, and a new phrase is
classified based on the accumulation of these weights in a scoring qubit using
entangling controlled-NOT gates. This is compared with more scalable quantum
encodings of word embedding vectors, which are used in the computation of
kernel values in a quantum support vector machine: this approach achieved an
average of 62% accuracy on classification tasks involving over 10000 words,
which is the largest such quantum computing experiment to date.
  We describe a quantum probability approach to bigram modeling that can be
applied to sequences of words and formal concepts, investigating a generative
approximation to these distributions using a quantum circuit Born machine, and
an approach to ambiguity resolution in verb-noun composition using single-qubit
rotations for simple nouns and 2-qubit controlled-NOT gates for simple verbs.
  The smaller systems described have been run successfully on physical quantum
computers, and the larger ones have been simulated. We show that statistically
meaningful results can be obtained using real datasets, but this is much more
difficult to predict than with easier artificial language examples used
previously in developing quantum NLP systems.
  Other approaches to quantum NLP are compared, partly with respect to
contemporary issues including informal language, fluency, and truthfulness.",None,4087
Pavementscapes: a large-scale hierarchical image dataset for asphalt pavement damage segmentation,0.0,"Pavement damage segmentation has benefited enormously from deep learning. %
and large-scale datasets. However, few current public datasets limit the
potential exploration of deep learning in the application of pavement damage
segmentation. To address this problem, this study has proposed Pavementscapes,
a large-scale dataset to develop and evaluate methods for pavement damage
segmentation. Pavementscapes is comprised of 4,000 images with a resolution of
$1024 \times 2048$, which have been recorded in the real-world pavement
inspection projects with 15 different pavements. A total of 8,680 damage
instances are manually labeled with six damage classes at the pixel level. The
statistical study gives a thorough investigation and analysis of the proposed
dataset. The numeral experiments propose the top-performing deep neural
networks capable of segmenting pavement damages, which provides the baselines
of the open challenge for pavement inspection. The experiment results also
indicate the existing problems for damage segmentation using deep learning, and
this study provides potential solutions.",https://github.com/tongzheng1992/E-FCN,2235
Computable Artificial General Intelligence,0.0443995,"Artificial general intelligence (AGI) may herald our extinction, according to
AI safety research. Yet claims regarding AGI must rely upon mathematical
formalisms -- theoretical agents we may analyse or attempt to build. AIXI
appears to be the only such formalism supported by proof that its behaviour is
optimal, a consequence of its use of compression as a proxy for intelligence.
Unfortunately, AIXI is incomputable and claims regarding its behaviour highly
subjective. We argue that this is because AIXI formalises cognition as taking
place in isolation from the environment in which goals are pursued (Cartesian
dualism). We propose an alternative, supported by proof and experiment, which
overcomes these problems. Integrating research from cognitive science with AI,
we formalise an enactive model of learning and reasoning to address the problem
of subjectivity. This allows us to formulate a different proxy for
intelligence, called weakness, which addresses the problem of incomputability.
We prove optimal behaviour is attained when weakness is maximised. This proof
is supplemented by experimental results comparing weakness and description
length (the closest analogue to compression possible without reintroducing
subjectivity). Weakness outperforms description length, suggesting it is a
better proxy. Furthermore we show that, if cognition is enactive, then
minimisation of description length is neither necessary nor sufficient to
attain optimal performance, undermining the notion that compression is closely
related to intelligence. However, there remain open questions regarding the
implementation of scale-able AGI. In the short term, these results may be best
utilised to improve the performance of existing systems. For example, our
results explain why Deepmind's Apperception Engine is able to generalise
effectively, and how to replicate that performance by maximising weakness.",None,76
PersDet: Monocular 3D Detection in Perspective Bird's-Eye-View,0.041286,"Currently, detecting 3D objects in Bird's-Eye-View (BEV) is superior to other
3D detectors for autonomous driving and robotics. However, transforming image
features into BEV necessitates special operators to conduct feature sampling.
These operators are not supported on many edge devices, bringing extra
obstacles when deploying detectors. To address this problem, we revisit the
generation of BEV representation and propose detecting objects in perspective
BEV -- a new BEV representation that does not require feature sampling. We
demonstrate that perspective BEV features can likewise enjoy the benefits of
the BEV paradigm. Moreover, the perspective BEV improves detection performance
by addressing issues caused by feature sampling. We propose PersDet for
high-performance object detection in perspective BEV space based on this
discovery. While implementing a simple and memory-efficient structure, PersDet
outperforms existing state-of-the-art monocular methods on the nuScenes
benchmark, reaching 34.6% mAP and 40.8% NDS when using ResNet-50 as the
backbone.",None,-1
CATrans: Context and Affinity Transformer for Few-Shot Segmentation,0.0,"Few-shot segmentation (FSS) aims to segment novel categories given scarce
annotated support images. The crux of FSS is how to aggregate dense
correlations between support and query images for query segmentation while
being robust to the large variations in appearance and context. To this end,
previous Transformer-based methods explore global consensus either on context
similarity or affinity map between support-query pairs. In this work, we
effectively integrate the context and affinity information via the proposed
novel Context and Affinity Transformer (CATrans) in a hierarchical
architecture. Specifically, the Relation-guided Context Transformer (RCT)
propagates context information from support to query images conditioned on more
informative support features. Based on the observation that a huge feature
distinction between support and query pairs brings barriers for context
knowledge transfer, the Relation-guided Affinity Transformer (RAT) measures
attention-aware affinity as auxiliary information for FSS, in which the
self-affinity is responsible for more reliable cross-affinity. We conduct
experiments to demonstrate the effectiveness of the proposed model,
outperforming the state-of-the-art methods.",None,-1
Knowledge Graph Induction enabling Recommending and Trend Analysis: A Corporate Research Community Use Case,0.0506885,"A research division plays an important role of driving innovation in an
organization. Drawing insights, following trends, keeping abreast of new
research, and formulating strategies are increasingly becoming more challenging
for both researchers and executives as the amount of information grows in both
velocity and volume. In this paper we present a use case of how a corporate
research community, IBM Research, utilizes Semantic Web technologies to induce
a unified Knowledge Graph from both structured and textual data obtained by
integrating various applications used by the community related to research
projects, academic papers, datasets, achievements and recognition. In order to
make the Knowledge Graph more accessible to application developers, we
identified a set of common patterns for exploiting the induced knowledge and
exposed them as APIs. Those patterns were born out of user research which
identified the most valuable use cases or user pain points to be alleviated. We
outline two distinct scenarios: recommendation and analytics for business use.
We will discuss these scenarios in detail and provide an empirical evaluation
on entity recommendation specifically. The methodology used and the lessons
learned from this work can be applied to other organizations facing similar
challenges.",None,3252
Diversity Enhanced Table-to-Text Generation via Type Control,0.271898,"Generating natural language statements to convey logical inferences from
tabular data (i.e., Logical NLG) is a process with one input and a variety of
valid outputs. This characteristic underscores the need for a method to produce
a diverse set of valid outputs, presenting different perspectives of the input
data. We propose a simple yet effective diversity-enhancing scheme that builds
upon an inherent property of the statements, their logic-types, by using a
type-controlled table-to-text generation model. We demonstrate, through
extensive automatic and human evaluations over the two publicly available
Logical NLG datasets, that our proposed method both facilitates the ability to
effectively control the generated statement type, and produces results superior
to the strongest baselines in terms of quality and factuality-diversity
trade-off.",None,8174
A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss,0.11342,"For readability assessment, traditional methods mainly employ machine
learning classifiers with hundreds of linguistic features. Although the deep
learning model has become the prominent approach for almost all NLP tasks, it
is less explored for readability assessment. In this paper, we propose a
BERT-based model with feature projection and length-balanced loss (BERT-FP-LBL)
for readability assessment. Specially, we present a new difficulty knowledge
guided semi-supervised method to extract topic features to complement the
traditional linguistic features. From the linguistic features, we employ
projection filtering to extract orthogonal features to supplement BERT
representations. Furthermore, we design a new length-balanced loss to handle
the greatly varying length distribution of data. Our model achieves
state-of-the-art performances on two English benchmark datasets and one dataset
of Chinese textbooks, and also achieves the near-perfect accuracy of 99\% on
one English dataset. Moreover, our proposed model obtains comparable results
with human experts in consistency test.",https://github.com/liwb1219/zhfeat,1295
Individual Topology Structure of Eye Movement Trajectories,0.0157208,"Traditionally, extracting patterns from eye movement data relies on
statistics of different macro-events such as fixations and saccades. This
requires an additional preprocessing step to separate the eye movement
subtypes, often with a number of parameters on which the classification results
depend. Besides that, definitions of such macro events are formulated in
different ways by different researchers.
  We propose an application of a new class of features to the quantitative
analysis of personal eye movement trajectories structure. This new class of
features based on algebraic topology allows extracting patterns from different
modalities of gaze such as time series of coordinates and amplitudes, heatmaps,
and point clouds in a unified way at all scales from micro to macro. We
experimentally demonstrate the competitiveness of the new class of features
with the traditional ones and their significant synergy while being used
together for the person authentication task on the recently published eye
movement trajectories dataset.",None,63
Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar,0.134609,"We introduce NSEdit (neural-symbolic edit), a novel Transformer-based code
repair method. Given only the source code that contains bugs, NSEdit predicts
an editing sequence that can fix the bugs. The edit grammar is formulated as a
regular language, and the Transformer uses it as a neural-symbolic scripting
interface to generate editing programs. We modify the Transformer and add a
pointer network to select the edit locations. An ensemble of rerankers are
trained to re-rank the editing sequences generated by beam search. We fine-tune
the rerankers on the validation set to reduce over-fitting. NSEdit is evaluated
on various code repair datasets and achieved a new state-of-the-art accuracy
($24.04\%$) on the Tufano small dataset of the CodeXGLUE benchmark. NSEdit
performs robustly when programs vary from packages to packages and when buggy
programs are concrete. We conduct detailed analysis on our methods and
demonstrate the effectiveness of each component.",None,13503
Do Large Language Models know what humans know?,0.0373026,"Humans can attribute beliefs to others. However, it is unknown to what extent
this ability results from an innate biological endowment or from experience
accrued through child development, particularly exposure to language describing
others' mental states. We test the viability of the language exposure
hypothesis by assessing whether models exposed to large quantities of human
language display sensitivity to the implied knowledge states of characters in
written passages. In pre-registered analyses, we present a linguistic version
of the False Belief Task to both human participants and a Large Language Model,
GPT-3. Both are sensitive to others' beliefs, but while the language model
significantly exceeds chance behavior, it does not perform as well as the
humans, nor does it explain the full extent of their behavior -- despite being
exposed to more language than a human would in a lifetime. This suggests that
while statistical learning from language exposure may in part explain how
humans develop the ability to reason about the mental states of others, other
mechanisms are also responsible.",None,556
"""Even if ..."" -- Diverse Semifactual Explanations of Reject",0.025127,"Machine learning based decision making systems applied in safety critical
areas require reliable high certainty predictions. For this purpose, the system
can be extended by an reject option which allows the system to reject inputs
where only a prediction with an unacceptably low certainty would be possible.
While being able to reject uncertain samples is important, it is also of
importance to be able to explain why a particular sample was rejected. With the
ongoing rise of eXplainable AI (XAI), a lot of explanation methodologies for
machine learning based systems have been developed -- explaining reject
options, however, is still a novel field where only very little prior work
exists.
  In this work, we propose to explain rejects by semifactual explanations, an
instance of example-based explanation methods, which them self have not been
widely considered in the XAI community yet. We propose a conceptual modeling of
semifactual explanations for arbitrary reject options and empirically evaluate
a specific implementation on a conformal prediction based reject option.",None,12042
Learning Options via Compression,0.0132363,"Identifying statistical regularities in solutions to some tasks in multi-task
reinforcement learning can accelerate the learning of new tasks. Skill learning
offers one way of identifying these regularities by decomposing pre-collected
experiences into a sequence of skills. A popular approach to skill learning is
maximizing the likelihood of the pre-collected experience with latent variable
models, where the latent variables represent the skills. However, there are
often many solutions that maximize the likelihood equally well, including
degenerate solutions. To address this underspecification, we propose a new
objective that combines the maximum likelihood objective with a penalty on the
description length of the skills. This penalty incentivizes the skills to
maximally extract common structures from the experiences. Empirically, our
objective learns skills that solve downstream tasks in fewer samples compared
to skills learned from only maximizing likelihood. Further, while most prior
works in the offline multi-task setting focus on tasks with low-dimensional
observations, our objective can scale to challenging tasks with
high-dimensional image observations.",https://github.com/yidingjiang/love,52102
Anomaly Attribution with Likelihood Compensation,0.0525261,"This paper addresses the task of explaining anomalous predictions of a
black-box regression model. When using a black-box model, such as one to
predict building energy consumption from many sensor measurements, we often
have a situation where some observed samples may significantly deviate from
their prediction. It may be due to a sub-optimal black-box model, or simply
because those samples are outliers. In either case, one would ideally want to
compute a ``responsibility score'' indicative of the extent to which an input
variable is responsible for the anomalous output. In this work, we formalize
this task as a statistical inverse problem: Given model deviation from the
expected value, infer the responsibility score of each of the input variables.
We propose a new method called likelihood compensation (LC), which is founded
on the likelihood principle and computes a correction to each input variable.
To the best of our knowledge, this is the first principled framework that
computes a responsibility score for real valued anomalous model deviations. We
apply our approach to a real-world building energy prediction task and confirm
its utility based on expert feedback.",None,12371
CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations,0.119469,"While Self-Supervised Learning has helped reap the benefit of the scale from
the available unlabeled data, the learning paradigms are continuously being
bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which
uses clustering and an augmentation-based cross-contrastive loss as its
self-supervised objective. Through the clustering module, we scale down the
influence of those negative examples that are highly similar to the positive.
The Cross-Contrastive loss is computed between the encoder output of the
original sample and the quantizer output of its augmentation and vice-versa,
bringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up
to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on
the test-clean and test-other sets, respectively, of LibriSpeech, without the
use of any language model. The proposed method also achieves up to 14.9%
relative WER improvement over the baseline wav2vec 2.0 when fine-tuned on
Switchboard data. We make all our codes publicly available on GitHub.",https://github.com/Speech-Lab-IITM/CCC-wav2vec-2.0,256
Retrieval augmentation of large language models for lay language generation,0.0726202,"Recent lay language generation systems have used Transformer models trained
on a parallel corpus to increase health information accessibility. However, the
applicability of these models is constrained by the limited size and topical
breadth of available corpora. We introduce CELLS, the largest (63k pairs) and
broadest-ranging (12 journals) parallel corpus for lay language generation. The
abstract and the corresponding lay language summary are written by domain
experts, assuring the quality of our dataset. Furthermore, qualitative
evaluation of expert-authored plain language summaries has revealed background
explanation as a key strategy to increase accessibility. Such explanation is
challenging for neural models to generate because it goes beyond simplification
by adding content absent from the source. We derive two specialized paired
corpora from CELLS to address key challenges in lay language generation:
generating background explanations and simplifying the original abstract. We
adopt retrieval-augmented models as an intuitive fit for the task of background
explanation generation, and show improvements in summary quality and simplicity
while maintaining factual correctness. Taken together, this work presents the
first comprehensive study of background explanation for lay language
generation, paving the path for disseminating scientific knowledge to a broader
audience. CELLS is publicly available at:
https://github.com/LinguisticAnomalies/pls_retrieval.",https://github.com/LinguisticAnomalies/pls-retrieval,27504
A Real-Time Fusion Framework for Long-term Visual Localization,0.0902252,"Visual localization is a fundamental task that regresses the 6 Degree Of
Freedom (6DoF) poses with image features in order to serve the high precision
localization requests in many robotics applications. Degenerate conditions like
motion blur, illumination changes and environment variations place great
challenges in this task. Fusion with additional information, such as sequential
information and Inertial Measurement Unit (IMU) inputs, would greatly assist
such problems. In this paper, we present an efficient client-server visual
localization architecture that fuses global and local pose estimations to
realize promising precision and efficiency. We include additional geometry
hints in mapping and global pose regressing modules to improve the measurement
quality. A loosely coupled fusion policy is adopted to leverage the computation
complexity and accuracy. We conduct the evaluations on two typical open-source
benchmarks, 4Seasons and OpenLORIS. Quantitative results prove that our
framework has competitive performance with respect to other state-of-the-art
visual localization solutions.",None,5803
Nested Named Entity Recognition from Medical Texts: An Adaptive Shared Network Architecture with Attentive CRF,0.0103454,"Recognizing useful named entities plays a vital role in medical information
processing, which helps drive the development of medical area research. Deep
learning methods have achieved good results in medical named entity recognition
(NER). However, we find that existing methods face great challenges when
dealing with the nested named entities. In this work, we propose a novel
method, referred to as ASAC, to solve the dilemma caused by the nested
phenomenon, in which the core idea is to model the dependency between different
categories of entity recognition. The proposed method contains two key modules:
the adaptive shared (AS) part and the attentive conditional random field (ACRF)
module. The former part automatically assigns adaptive weights across each task
to achieve optimal recognition accuracy in the multi-layer network. The latter
module employs the attention operation to model the dependency between
different entities. In this way, our model could learn better entity
representations by capturing the implicit distinctions and relationships
between different categories of entities. Extensive experiments on public
datasets verify the effectiveness of our method. Besides, we also perform
ablation analyses to deeply understand our methods.",None,27779
Sequence-aware multimodal page classification of Brazilian legal documents,0.0294521,"The Brazilian Supreme Court receives tens of thousands of cases each
semester. Court employees spend thousands of hours to execute the initial
analysis and classification of those cases -- which takes effort away from
posterior, more complex stages of the case management workflow. In this paper,
we explore multimodal classification of documents from Brazil's Supreme Court.
We train and evaluate our methods on a novel multimodal dataset of 6,510
lawsuits (339,478 pages) with manual annotation assigning each page to one of
six classes. Each lawsuit is an ordered sequence of pages, which are stored
both as an image and as a corresponding text extracted through optical
character recognition. We first train two unimodal classifiers: a ResNet
pre-trained on ImageNet is fine-tuned on the images, and a convolutional
network with filters of multiple kernel sizes is trained from scratch on
document texts. We use them as extractors of visual and textual features, which
are then combined through our proposed Fusion Module. Our Fusion Module can
handle missing textual or visual input by using learned embeddings for missing
data. Moreover, we experiment with bi-directional Long Short-Term Memory
(biLSTM) networks and linear-chain conditional random fields to model the
sequential nature of the pages. The multimodal approaches outperform both
textual and visual classifiers, especially when leveraging the sequential
nature of the pages.",https://github.com/peluz/victor-visual-text,733
Feature Selection for Fault Detection and Prediction based on Event Log Analysis,0.00466555,"Event logs are widely used for anomaly detection and prediction in complex
systems. Existing log-based anomaly detection methods usually consist of four
main steps: log collection, log parsing, feature extraction, and anomaly
detection, wherein the feature extraction step extracts useful features for
anomaly detection by counting log events. For a complex system, such as a
lithography machine consisting of a large number of subsystems, its log may
contain thousands of different events, resulting in abounding extracted
features. However, when anomaly detection is performed at the subsystem level,
analyzing all features becomes expensive and unnecessary. To mitigate this
problem, we develop a feature selection method for log-based anomaly detection
and prediction, largely improving the effectiveness and efficiency.",None,2612
StereoKG: Data-Driven Knowledge Graph Construction for Cultural Knowledge and Stereotypes,0.017786,"Analyzing ethnic or religious bias is important for improving fairness,
accountability, and transparency of natural language processing models.
However, many techniques rely on human-compiled lists of bias terms, which are
expensive to create and are limited in coverage. In this study, we present a
fully data-driven pipeline for generating a knowledge graph (KG) of cultural
knowledge and stereotypes. Our resulting KG covers 5 religious groups and 5
nationalities and can easily be extended to include more entities. Our human
evaluation shows that the majority (59.2%) of non-singleton entries are
coherent and complete stereotypes. We further show that performing intermediate
masked language model training on the verbalized KG leads to a higher level of
cultural awareness in the model and has the potential to increase
classification performance on knowledge-crucial samples on a related task,
i.e., hate speech detection.",https://github.com/uds-lsv/StereoKG,4373
A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation,0.047262,"It is difficult for non-autoregressive translation (NAT) models to capture
the multi-modal distribution of target translations due to their conditional
independence assumption, which is known as the ""multi-modality problem"",
including the lexical multi-modality and the syntactic multi-modality. While
the first one has been well studied, the syntactic multi-modality brings severe
challenge to the standard cross entropy (XE) loss in NAT and is under studied.
In this paper, we conduct a systematic study on the syntactic multi-modality
problem. Specifically, we decompose it into short- and long-range syntactic
multi-modalities and evaluate several recent NAT algorithms with advanced loss
functions on both carefully designed synthesized datasets and real datasets. We
find that the Connectionist Temporal Classification (CTC) loss and the
Order-Agnostic Cross Entropy (OAXE) loss can better handle short- and
long-range syntactic multi-modalities respectively. Furthermore, we take the
best of both and design a new loss function to better handle the complicated
syntactic multi-modality in real-world datasets. To facilitate practical usage,
we provide a guide to use different loss functions for different kinds of
syntactic multi-modality.",https://github.com/kpu/kenlm,62742
Label Anchored Contrastive Learning for Language Understanding,0.0765328,"Contrastive learning (CL) has achieved astonishing progress in computer
vision, speech, and natural language processing fields recently with
self-supervised learning. However, CL approach to the supervised setting is not
fully explored, especially for the natural language understanding
classification task. Intuitively, the class label itself has the intrinsic
ability to perform hard positive/negative mining, which is crucial for CL.
Motivated by this, we propose a novel label anchored contrastive learning
approach (denoted as LaCon) for language understanding. Specifically, three
contrastive objectives are devised, including a multi-head instance-centered
contrastive loss (ICL), a label-centered contrastive loss (LCL), and a label
embedding regularizer (LER). Our approach does not require any specialized
network architecture or any extra data augmentation, thus it can be easily
plugged into existing powerful pre-trained language models. Compared to the
state-of-the-art baselines, LaCon obtains up to 4.1% improvement on the popular
datasets of GLUE and CLUE benchmarks. Besides, LaCon also demonstrates
significant advantages under the few-shot and data imbalance settings, which
obtains up to 9.4% improvement on the FewGLUE and FewCLUE benchmarking tasks.",https://github.com/huggingface/transformers,459
DETR++: Taming Your Multi-Scale Detection Transformer,0.136939,"Convolutional Neural Networks (CNN) have dominated the field of detection
ever since the success of AlexNet in ImageNet classification [12]. With the
sweeping reform of Transformers [27] in natural language processing, Carion et
al. [2] introduce the Transformer-based detection method, i.e., DETR. However,
due to the quadratic complexity in the self-attention mechanism in the
Transformer, DETR is never able to incorporate multi-scale features as
performed in existing CNN-based detectors, leading to inferior results in small
object detection. To mitigate this issue and further improve performance of
DETR, in this work, we investigate different methods to incorporate multi-scale
features and find that a Bi-directional Feature Pyramid (BiFPN) works best with
DETR in further raising the detection precision. With this discovery, we
propose DETR++, a new architecture that improves detection results by 1.9% AP
on MS COCO 2017, 11.5% AP on RICO icon detection, and 9.1% AP on RICO layout
extraction over existing baselines.",None,7581
Synthesizing Programs with Continuous Optimization,0.00434682,"Automatic software generation based on some specification is known as program
synthesis. Most existing approaches formulate program synthesis as a search
problem with discrete parameters. In this paper, we present a novel formulation
of program synthesis as a continuous optimization problem and use a
state-of-the-art evolutionary approach, known as Covariance Matrix Adaptation
Evolution Strategy to solve the problem. We then propose a mapping scheme to
convert the continuous formulation into actual programs. We compare our system,
called GENESYS, with several recent program synthesis techniques (in both
discrete and continuous domains) and show that GENESYS synthesizes more
programs within a fixed time budget than those existing schemes. For example,
for programs of length 10, GENESYS synthesizes 28% more programs than those
existing schemes within the same time budget.",None,2478
Towards 3D Object Detection with 2D Supervision,0.0402705,"The great progress of 3D object detectors relies on large-scale data and 3D
annotations. The annotation cost for 3D bounding boxes is extremely expensive
while the 2D ones are easier and cheaper to collect. In this paper, we
introduce a hybrid training framework, enabling us to learn a visual 3D object
detector with massive 2D (pseudo) labels, even without 3D annotations. To break
through the information bottleneck of 2D clues, we explore a new perspective:
Temporal 2D Supervision. We propose a temporal 2D transformation to bridge the
3D predictions with temporal 2D labels. Two steps, including homography wraping
and 2D box deduction, are taken to transform the 3D predictions into 2D ones
for supervision. Experiments conducted on the nuScenes dataset show strong
results (nearly 90% of its fully-supervised performance) with only 25% 3D
annotations. We hope our findings can provide new insights for using a large
number of 2D annotations for 3D perception.",None,306651
Predictive Crypto-Asset Automated Market Making Architecture for Decentralized Finance using Deep Reinforcement Learning,0.0465139,"The study proposes a quote-driven predictive automated market maker (AMM)
platform with on-chain custody and settlement functions, alongside off-chain
predictive reinforcement learning capabilities to improve liquidity provision
of real-world AMMs. The proposed AMM architecture is an augmentation to the
Uniswap V3, a cryptocurrency AMM protocol, by utilizing a novel market
equilibrium pricing for reduced divergence and slippage loss. Further, the
proposed architecture involves a predictive AMM capability, utilizing a deep
hybrid Long Short-Term Memory (LSTM) and Q-learning reinforcement learning
framework that looks to improve market efficiency through better forecasts of
liquidity concentration ranges, so liquidity starts moving to expected
concentration ranges, prior to asset price movement, so that liquidity
utilization is improved. The augmented protocol framework is expected have
practical real-world implications, by (i) reducing divergence loss for
liquidity providers, (ii) reducing slippage for crypto-asset traders, while
(iii) improving capital efficiency for liquidity provision for the AMM
protocol. To our best knowledge, there are no known protocol or literature that
are proposing similar deep learning-augmented AMM that achieves similar capital
efficiency and loss minimization objectives for practical real-world
applications.",None,74
Fuzzy granular approximation classifier,0.0279899,"In this article, a new Fuzzy Granular Approximation Classifier (FGAC) is
introduced. The classifier is based on the previously introduced concept of the
granular approximation and its multi-class classification case. The classifier
is instance-based and its biggest advantage is its local transparency i.e., the
ability to explain every individual prediction it makes. We first develop the
FGAC for the binary classification case and the multi-class classification case
and we discuss its variation that includes the Ordered Weighted Average (OWA)
operators. Those variations of the FGAC are then empirically compared with
other locally transparent ML methods. At the end, we discuss the transparency
of the FGAC and its advantage over other locally transparent methods. We
conclude that while the FGAC has similar predictive performance to other
locally transparent ML models, its transparency can be superior in certain
cases.",https://github.com/markopalangetic/FGAC_experiments,28553
Prompt-based Text Entailment for Low-Resource Named Entity Recognition,0.0944513,"Pre-trained Language Models (PLMs) have been applied in NLP tasks and achieve
promising results. Nevertheless, the fine-tuning procedure needs labeled data
of the target domain, making it difficult to learn in low-resource and
non-trivial labeled scenarios. To address these challenges, we propose
Prompt-based Text Entailment (PTE) for low-resource named entity recognition,
which better leverages knowledge in the PLMs. We first reformulate named entity
recognition as the text entailment task. The original sentence with entity
type-specific prompts is fed into PLMs to get entailment scores for each
candidate. The entity type with the top score is then selected as final label.
Then, we inject tagging labels into prompts and treat words as basic units
instead of n-gram spans to reduce time complexity in generating candidates by
n-grams enumeration. Experimental results demonstrate that the proposed method
PTE achieves competitive performance on the CoNLL03 dataset, and better than
fine-tuned counterparts on the MIT Movie and Few-NERD dataset in low-resource
settings.",None,8098
Hierarchical Attention Network for Few-Shot Object Detection via Meta-Contrastive Learning,0.0326231,"Few-shot object detection (FSOD) aims to classify and detect few images of
novel categories. Existing meta-learning methods insufficiently exploit
features between support and query images owing to structural limitations. We
propose a hierarchical attention network with sequentially large receptive
fields to fully exploit the query and support images. In addition,
meta-learning does not distinguish the categories well because it determines
whether the support and query images match. In other words, metric-based
learning for classification is ineffective because it does not work directly.
Thus, we propose a contrastive learning method called meta-contrastive
learning, which directly helps achieve the purpose of the meta-learning
strategy. Finally, we establish a new state-of-the-art network, by realizing
significant margins. Our method brings 2.3, 1.0, 1.3, 3.4 and 2.4% AP
improvements for 1-30 shots object detection on COCO dataset. Our code is
available at: https://github.com/infinity7428/hANMCL",https://github.com/infinity7428/hANMCL,12071
Morphological Processing of Low-Resource Languages: Where We Are and What's Next,0.0477708,"Automatic morphological processing can aid downstream natural language
processing applications, especially for low-resource languages, and assist
language documentation efforts for endangered languages. Having long been
multilingual, the field of computational morphology is increasingly moving
towards approaches suitable for languages with minimal or no annotated
resources. First, we survey recent developments in computational morphology
with a focus on low-resource languages. Second, we argue that the field is
ready to tackle the logical next challenge: understanding a language's
morphology from raw text alone. We perform an empirical study on a truly
unsupervised version of the paradigm completion task and show that, while
existing state-of-the-art models bridged by two newly proposed models we devise
perform reasonably, there is still much room for improvement. The stakes are
high: solving this task will increase the language coverage of morphological
resources by a number of magnitudes.",https://github.com/Adamits/tUMPC,3375
Bias-Eliminated Semantic Refinement for Any-Shot Learning,0.134599,"When training samples are scarce, the semantic embedding technique, ie,
describing class labels with attributes, provides a condition to generate
visual features for unseen objects by transferring the knowledge from seen
objects. However, semantic descriptions are usually obtained in an external
paradigm, such as manual annotation, resulting in weak consistency between
descriptions and visual features. In this paper, we refine the coarse-grained
semantic description for any-shot learning tasks, ie, zero-shot learning (ZSL),
generalized zero-shot learning (GZSL), and few-shot learning (FSL). A new
model, namely, the semantic refinement Wasserstein generative adversarial
network (SRWGAN) model, is designed with the proposed multihead representation
and hierarchical alignment techniques. Unlike conventional methods, semantic
refinement is performed with the aim of identifying a bias-eliminated condition
for disjoint-class feature generation and is applicable in both inductive and
transductive settings. We extensively evaluate model performance on six
benchmark datasets and observe state-of-the-art results for any-shot learning;
eg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset
and 82.2% harmonic accuracy for the Oxford Flowers (FLO) dataset in the
standard GZSL setting. Various visualizations are also provided to show the
bias-eliminated generation of SRWGAN. Our code is available.",https://github.com/LiangjunFeng/SRWGAN,8266
Evaluation of Pre-Trained CNN Models for Geographic Fake Image Detection,0.0424933,"Thanks to the remarkable advances in generative adversarial networks (GANs),
it is becoming increasingly easy to generate/manipulate images. The existing
works have mainly focused on deepfake in face images and videos. However, we
are currently witnessing the emergence of fake satellite images, which can be
misleading or even threatening to national security. Consequently, there is an
urgent need to develop detection methods capable of distinguishing between real
and fake satellite images. To advance the field, in this paper, we explore the
suitability of several convolutional neural network (CNN) architectures for
fake satellite image detection. Specifically, we benchmark four CNN models by
conducting extensive experiments to evaluate their performance and robustness
against various image distortions. This work allows the establishment of new
baselines and may be useful for the development of CNN-based methods for fake
satellite image detection.",None,24565
Learning Parameters for a Generalized Vidale-Wolfe Response Model with Flexible Ad Elasticity and Word-of-Mouth,0.219286,"In this research, we investigate a generalized form of Vidale-Wolfe (GVW)
model. One key element of our modeling work is that the GVW model contains two
useful indexes representing advertiser's elasticity and the word-of-mouth (WoM)
effect, respectively. Moreover, we discuss some desirable properties of the GVW
model, and present a deep neural network (DNN)-based estimation method to learn
its parameters. Furthermore, based on three realworld datasets, we conduct
computational experiments to validate the GVW model and identified properties.
In addition, we also discuss potential advantages of the GVW model over
econometric models. The research outcome shows that both the ad elasticity
index and the WoM index have significant influences on advertising responses,
and the GVW model has potential advantages over econometric models of
advertising, in terms of several interesting phenomena drawn from practical
advertising situations. The GVW model and its deep learning-based estimation
method provide a basis to support big data-driven advertising analytics and
decision makings; in the meanwhile, identified properties and experimental
findings of this research illuminate critical managerial insights for
advertisers in various advertising forms.",None,22251
Safe and Robust Experience Sharing for Deterministic Policy Gradient Algorithms,0.0460301,"Learning in high dimensional continuous tasks is challenging, mainly when the
experience replay memory is very limited. We introduce a simple yet effective
experience sharing mechanism for deterministic policies in continuous action
domains for the future off-policy deep reinforcement learning applications in
which the allocated memory for the experience replay buffer is limited. To
overcome the extrapolation error induced by learning from other agents'
experiences, we facilitate our algorithm with a novel off-policy correction
technique without any action probability estimates. We test the effectiveness
of our method in challenging OpenAI Gym continuous control tasks and conclude
that it can achieve a safe experience sharing across multiple agents and
exhibits a robust performance when the replay memory is strictly limited.",https://github.com/baturaysaglam/DASE,644
An IoT Cloud and Big Data Architecture for the Maintenance of Home Appliances,0.0281699,"Billions of interconnected Internet of Things (IoT) sensors and devices
collect tremendous amounts of data from real-world scenarios. Big data is
generating increasing interest in a wide range of industries. Once data is
analyzed through compute-intensive Machine Learning (ML) methods, it can derive
critical business value for organizations. Powerfulplatforms are essential to
handle and process such massive collections of information cost-effectively and
conveniently. This work introduces a distributed and scalable platform
architecture that can be deployed for efficient real-world big data collection
and analytics. The proposed system was tested with a case study for Predictive
Maintenance of Home Appliances, where current and vibration sensors with high
acquisition frequency were connected to washing machines and refrigerators. The
introduced platform was used to collect, store, and analyze the data. The
experimental results demonstrated that the presented system could be
advantageous for tackling real-world IoT scenarios in a cost-effective and
local approach.",None,1367
Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation,0.517173,"Research on Automatic Story Generation (ASG) relies heavily on human and
automatic evaluation. However, there is no consensus on which human evaluation
criteria to use, and no analysis of how well automatic criteria correlate with
them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a
set of 6 orthogonal and comprehensive human criteria, carefully motivated by
the social sciences literature. We also present HANNA, an annotated dataset of
1,056 stories produced by 10 different ASG systems. HANNA allows us to
quantitatively evaluate the correlations of 72 automatic metrics with human
criteria. Our analysis highlights the weaknesses of current metrics for ASG and
allows us to formulate practical recommendations for ASG evaluation.",https://github.com/dig-team/hanna-benchmark-asg,3215
Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts,0.0420878,"Recent works suggest that transformer models are capable of multi-tasking on
diverse NLP tasks and adapting to new tasks efficiently. However, the potential
of these multi-task models may be limited as they use the same set of
parameters for all tasks. In contrast, humans tackle tasks in a more flexible
way, by making proper presumptions on what skills and knowledge are relevant
and executing only the necessary computations. Inspired by this, we propose to
use task-level mixture-of-expert models, which has a collection of transformer
layers (i.e., experts) and a router component that chooses from these experts
dynamically and flexibly. We find that these models help improve the average
performance gain (ARG) metric by 2.6% when adapting to unseen tasks in the
few-shot setting and by 5.6% in the zero-shot generalization setting. Further,
we show that the learned routing decisions partly rediscover human
categorization of NLP tasks -- certain experts are strongly associated with
extractive tasks, some with classification tasks, and some with tasks requiring
world knowledge.",https://github.com/INK-USC/CrossTaskMoE,378
EnDex: Evaluation of Dialogue Engagingness at Scale,0.0140826,"We propose EnDex, the first human-reaction based model to evaluate dialogue
engagingness. EnDex is trained on 80k Reddit-based Engagement Dataset (RED)
curated using a novel distant-supervision framework. Engagingness is a key
measure that captures high-level quality of AI dialogue systems and closely
reflects actual user experience. However, data shortage, plus the abstract and
extensive definition of engagingness makes it challenging to develop an
automatic metric. Our work departs from mainstream approaches that use
synthetic negative examples to train binary classifiers, and instead, proposes
a solution using distant-supervision from human-reaction feedback. To support
the soundness of our EnDex metric, we offer a theoretical foundation for
engagement, an extensive ablation study, and empirical evidence of high
correlation on five engagingness related datasets. We will release code,
off-the-shelf EnDex model, and a large-scale dataset upon paper publication to
facilitate future research.",https://github.com/gxxu-ml/EnDex,8747
Low-resource Neural Machine Translation with Cross-modal Alignment,0.181888,"How to achieve neural machine translation with limited parallel data?
Existing techniques often rely on large-scale monolingual corpora, which is
impractical for some low-resource languages. In this paper, we turn to connect
several low-resource languages to a particular high-resource one by additional
visual modality. Specifically, we propose a cross-modal contrastive learning
method to learn a shared space for all languages, where both a coarse-grained
sentence-level objective and a fine-grained token-level one are introduced.
Experimental results and further analysis show that our method can effectively
learn the cross-modal and cross-lingual alignment with a small amount of
image-text pairs and achieves significant improvements over the text-only
baseline under both zero-shot and few-shot scenarios.",https://github.com/ictnlp/LNMT-CA,6142
Reinforced Imitative Graph Learning for Mobile User Profiling,0.0310722,"Mobile user profiling refers to the efforts of extracting users'
characteristics from mobile activities. In order to capture the dynamic varying
of user characteristics for generating effective user profiling, we propose an
imitation-based mobile user profiling framework. Considering the objective of
teaching an autonomous agent to imitate user mobility based on the user's
profile, the user profile is the most accurate when the agent can perfectly
mimic the user behavior patterns. The profiling framework is formulated into a
reinforcement learning task, where an agent is a next-visit planner, an action
is a POI that a user will visit next, and the state of the environment is a
fused representation of a user and spatial entities. An event in which a user
visits a POI will construct a new state, which helps the agent predict users'
mobility more accurately. In the framework, we introduce a spatial Knowledge
Graph (KG) to characterize the semantics of user visits over connected spatial
entities. Additionally, we develop a mutual-updating strategy to quantify the
state that evolves over time. Along these lines, we develop a reinforcement
imitative graph learning framework for mobile user profiling. Finally, we
conduct extensive experiments to demonstrate the superiority of our approach.",https://github.com/maciejkula/spotlight,39478
Multi-Frames Temporal Abnormal Clues Learning Method for Face Anti-Spoofing,0.0193995,"Face anti-spoofing researches are widely used in face recognition and has
received more attention from industry and academics. In this paper, we propose
the EulerNet, a new temporal feature fusion network in which the differential
filter and residual pyramid are used to extract and amplify abnormal clues from
continuous frames, respectively. A lightweight sample labeling method based on
face landmarks is designed to label large-scale samples at a lower cost and has
better results than other methods such as 3D camera. Finally, we collect 30,000
live and spoofing samples using various mobile ends to create a dataset that
replicates various forms of attacks in a real-world setting. Extensive
experiments on public OULU-NPU show that our algorithm is superior to the state
of art and our solution has already been deployed in real-world systems
servicing millions of users.",None,59
Combining State-of-the-Art Models with Maximal Marginal Relevance for Few-Shot and Zero-Shot Multi-Document Summarization,0.0413975,"In Natural Language Processing, multi-document summarization (MDS) poses many
challenges to researchers above those posed by single-document summarization
(SDS). These challenges include the increased search space and greater
potential for the inclusion of redundant information. While advancements in
deep learning approaches have led to the development of several advanced
language models capable of summarization, the variety of training data specific
to the problem of MDS remains relatively limited. Therefore, MDS approaches
which require little to no pretraining, known as few-shot or zero-shot
applications, respectively, could be beneficial additions to the current set of
tools available in summarization. To explore one possible approach, we devise a
strategy for combining state-of-the-art models' outputs using maximal marginal
relevance (MMR) with a focus on query relevance rather than document diversity.
Our MMR-based approach shows improvement over some aspects of the current
state-of-the-art results in both few-shot and zero-shot MDS applications while
maintaining a state-of-the-art standard of output by all available metrics.",None,5398
Deep Learning Reproducibility and Explainable AI (XAI),0.0,"The nondeterminism of Deep Learning (DL) training algorithms and its
influence on the explainability of neural network (NN) models are investigated
in this work with the help of image classification examples. To discuss the
issue, two convolutional neural networks (CNN) have been trained and their
results compared. The comparison serves the exploration of the feasibility of
creating deterministic, robust DL models and deterministic explainable
artificial intelligence (XAI) in practice. Successes and limitation of all here
carried out efforts are described in detail. The source code of the attained
deterministic models has been listed in this work. Reproducibility is indexed
as a development-phase-component of the Model Governance Framework, proposed by
the EU within their excellence in AI approach. Furthermore, reproducibility is
a requirement for establishing causality for the interpretation of model
results and building of trust towards the overwhelming expansion of AI systems
applications. Problems that have to be solved on the way to reproducibility and
ways to deal with some of them, are examined in this work.",None,-1
Universal Deep GNNs: Rethinking Residual Connection in GNNs from a Path Decomposition Perspective for Preventing the Over-smoothing,0.0222211,"The performance of GNNs degrades as they become deeper due to the
over-smoothing. Among all the attempts to prevent over-smoothing, residual
connection is one of the promising methods due to its simplicity. However,
recent studies have shown that GNNs with residual connections only slightly
slow down the degeneration. The reason why residual connections fail in GNNs is
still unknown. In this paper, we investigate the forward and backward behavior
of GNNs with residual connections from a novel path decomposition perspective.
We find that the recursive aggregation of the median length paths from the
binomial distribution of residual connection paths dominates output
representation, resulting in over-smoothing as GNNs go deeper. Entangled
propagation and weight matrices cause gradient smoothing and prevent GNNs with
residual connections from optimizing to the identity mapping. Based on these
findings, we present a Universal Deep GNNs (UDGNN) framework with cold-start
adaptive residual connections (DRIVE) and feedforward modules. Extensive
experiments demonstrate the effectiveness of our method, which achieves
state-of-the-art results over non-smooth heterophily datasets by simply
stacking standard GNNs.",https://github.com/JC-202/UDGNNs,15017
Unsupervised Domain Adaptation for One-stage Object Detector using Offsets to Bounding Box,0.0230374,"Most existing domain adaptive object detection methods exploit adversarial
feature alignment to adapt the model to a new domain. Recent advances in
adversarial feature alignment strives to reduce the negative effect of
alignment, or negative transfer, that occurs because the distribution of
features varies depending on the category of objects. However, by analyzing the
features of the anchor-free one-stage detector, in this paper, we find that
negative transfer may occur because the feature distribution varies depending
on the regression value for the offset to the bounding box as well as the
category. To obtain domain invariance by addressing this issue, we align the
feature conditioned on the offset value, considering the modality of the
feature distribution. With a very simple and effective conditioning method, we
propose OADA (Offset-Aware Domain Adaptive object detector) that achieves
state-of-the-art performances in various experimental settings. In addition, by
analyzing through singular value decomposition, we find that our model enhances
both discriminability and transferability.",None,4373
Smart Multi-tenant Federated Learning,0.0616644,"Federated learning (FL) is an emerging distributed machine learning method
that empowers in-situ model training on decentralized edge devices. However,
multiple simultaneous training activities could overload resource-constrained
devices. In this work, we propose a smart multi-tenant FL system, MuFL, to
effectively coordinate and execute simultaneous training activities. We first
formalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and
introduce a vanilla multi-tenant FL system that trains activities sequentially
to form baselines. Then, we propose two approaches to optimize multi-tenant FL:
1) activity consolidation merges training activities into one activity with a
multi-task architecture; 2) after training it for rounds, activity splitting
divides it into groups by employing affinities among activities such that
activities within a group have better synergy. Extensive experiments
demonstrate that MuFL outperforms other methods while consuming 40% less
energy. We hope this work will inspire the community to further study and
optimize multi-tenant FL.",https://github.com/tstandley/taskgrouping,20521
Online Auction-Based Incentive Mechanism Design for Horizontal Federated Learning with Budget Constraint,0.0409351,"Federated learning makes it possible for all parties with data isolation to
train the model collaboratively and efficiently while satisfying privacy
protection. To obtain a high-quality model, an incentive mechanism is necessary
to motivate more high-quality workers with data and computing power. The
existing incentive mechanisms are applied in offline scenarios, where the task
publisher collects all bids and selects workers before the task. However, it is
practical that different workers arrive online in different orders before or
during the task. Therefore, we propose a reverse auction-based online incentive
mechanism for horizontal federated learning with budget constraint. Workers
submit bids when they arrive online. The task publisher with a limited budget
leverages the information of the arrived workers to decide on whether to select
the new worker. Theoretical analysis proves that our mechanism satisfies budget
feasibility, computational efficiency, individual rationality, consumer
sovereignty, time truthfulness, and cost truthfulness with a sufficient budget.
The experimental results show that our online mechanism is efficient and can
obtain high-quality models.",None,49
TourBERT: A pretrained language model for the tourism industry,0.527633,"The Bidirectional Encoder Representations from Transformers (BERT) is
currently one of the most important and state-of-the-art models for natural
language. However, it has also been shown that for domain-specific tasks it is
helpful to pretrain BERT on a domain-specific corpus. In this paper, we present
TourBERT, a pretrained language model for tourism. We describe how TourBERT was
developed and evaluated. The evaluations show that TourBERT is outperforming
BERT in all tourism-specific tasks.",None,149
Infrared and visible image fusion based on Multi-State Contextual Hidden Markov Model,0.0324339,"The traditional two-state hidden Markov model divides the high frequency
coefficients only into two states (large and small states). Such scheme is
prone to produce an inaccurate statistical model for the high frequency subband
and reduces the quality of fusion result. In this paper, a fine-grained
multi-state contextual hidden Markov model (MCHMM) is proposed for infrared and
visible image fusion in the non-subsampled Shearlet domain, which takes full
consideration of the strong correlations and level of details of NSST
coefficients. To this end, an accurate soft context variable is designed
correspondingly from the perspective of context correlation. Then, the
statistical features provided by MCHMM are utilized for the fusion of high
frequency subbands. To ensure the visual quality, a fusion strategy based on
the difference in regional energy is proposed as well for lowfrequency
subbands. Experimental results demonstrate that the proposed method can achieve
a superior performance compared with other fusion methods in both subjective
and objective aspects.",None,-1
Computing Abductive Explanations for Boosted Trees,0.125846,"Boosted trees is a dominant ML model, exhibiting high accuracy. However,
boosted trees are hardly intelligible, and this is a problem whenever they are
used in safety-critical applications. Indeed, in such a context, rigorous
explanations of the predictions made are expected. Recent work have shown how
subset-minimal abductive explanations can be derived for boosted trees, using
automated reasoning techniques. However, the generation of such well-founded
explanations is intractable in the general case. To improve the scalability of
their generation, we introduce the notion of tree-specific explanation for a
boosted tree. We show that tree-specific explanations are abductive
explanations that can be computed in polynomial time. We also explain how to
derive a subset-minimal abductive explanation from a tree-specific explanation.
Experiments on various datasets show the computational benefits of leveraging
tree-specific explanations for deriving subset-minimal abductive explanations.",None,-1
Socio-cognitive Optimization of Time-delay Control Problems using Evolutionary Metaheuristics,0.079563,"Metaheuristics are universal optimization algorithms which should be used for
solving difficult problems, unsolvable by classic approaches. In this paper we
aim at constructing novel socio-cognitive metaheuristic based on castes, and
apply several versions of this algorithm to optimization of time-delay system
model. Besides giving the background and the details of the proposed algorithms
we apply them to optimization of selected variants of the problem and discuss
the results.",None,-1
SentBS: Sentence-level Beam Search for Controllable Summarization,0.037171,"A wide range of control perspectives have been explored in controllable text
generation. Structure-controlled summarization is recently proposed as a useful
and interesting research direction. However, current structure-controlling
methods have limited effectiveness in enforcing the desired structure. To
address this limitation, we propose a sentence-level beam search generation
method (SentBS), where evaluation is conducted throughout the generation
process to select suitable sentences for subsequent generations. We experiment
with different combinations of decoding methods to be used as subcomponents by
SentBS and evaluate results on the structure-controlled dataset MReD.
Experiments show that all explored combinations for SentBS can improve the
agreement between the generated text and the desired structure, with the best
method significantly reducing the structural discrepancies suffered by the
existing model, by approximately 68%.",https://github.com/Shen-Chenhui/SentBS,-1
Revisiting initial sets in abstract argumentation,0.00997978,"We revisit the notion of initial sets by Xu and Cayrol, i.e., non-empty
minimal admissible sets in abstract argumentation frameworks. Initial sets are
a simple concept for analysing conflicts in an abstract argumentation framework
and to explain why certain arguments can be accepted. We contribute with new
insights on the structure of initial sets and devise a simple non-deterministic
construction principle for any admissible set, based on iterative selection of
initial sets of the original framework and its induced reducts. In particular,
we characterise many existing admissibility-based semantics via this
construction principle, thus providing a constructive explanation on the
structure of extensions. We also investigate certain problems related to
initial sets with respect to their computational complexity.",None,-1
Graph-based Extractive Explainer for Recommendations,0.161265,"Explanations in a recommender system assist users in making informed
decisions among a set of recommended items. Great research attention has been
devoted to generating natural language explanations to depict how the
recommendations are generated and why the users should pay attention to them.
However, due to different limitations of those solutions, e.g., template-based
or generation-based, it is hard to make the explanations easily perceivable,
reliable and personalized at the same time.
  In this work, we develop a graph attentive neural network model that
seamlessly integrates user, item, attributes, and sentences for
extraction-based explanation. The attributes of items are selected as the
intermediary to facilitate message passing for user-item specific evaluation of
sentence relevance. And to balance individual sentence relevance, overall
attribute coverage, and content redundancy, we solve an integer linear
programming problem to make the final selection of sentences. Extensive
empirical evaluations against a set of state-of-the-art baseline methods on two
benchmark review datasets demonstrated the generation quality of the proposed
solution.",None,-1
3D Moments from Near-Duplicate Photos,0.220065,"We introduce 3D Moments, a new computational photography effect. As input we
take a pair of near-duplicate photos, i.e., photos of moving subjects from
similar viewpoints, common in people's photo collections. As output, we produce
a video that smoothly interpolates the scene motion from the first photo to the
second, while also producing camera motion with parallax that gives a
heightened sense of 3D. To achieve this effect, we represent the scene as a
pair of feature-based layered depth images augmented with scene flow. This
representation enables motion interpolation along with independent control of
the camera viewpoint. Our system produces photorealistic space-time videos with
motion parallax and scene dynamics, while plausibly recovering regions occluded
in the original views. We conduct extensive experiments demonstrating superior
performance over baselines on public datasets and in-the-wild photos. Project
page: https://3d-moments.github.io/",https://3d-moments.github.io/,-1
Explainable Reinforcement Learning via Model Transforms,0.00736046,"Understanding emerging behaviors of reinforcement learning (RL) agents may be
difficult since such agents are often trained in complex environments using
highly complex decision making procedures. This has given rise to a variety of
approaches to explainability in RL that aim to reconcile discrepancies that may
arise between the behavior of an agent and the behavior that is anticipated by
an observer. Most recent approaches have relied either on domain knowledge that
may not always be available, on an analysis of the agent's policy, or on an
analysis of specific elements of the underlying environment, typically modeled
as a Markov Decision Process (MDP). Our key claim is that even if the
underlying model is not fully known (e.g., the transition probabilities have
not been accurately learned) or is not maintained by the agent (i.e., when
using model-free methods), the model can nevertheless be exploited to
automatically generate explanations. For this purpose, we suggest using formal
MDP abstractions and transforms, previously used in the literature for
expediting the search for optimal policies, to automatically produce
explanations. Since such transforms are typically based on a symbolic
representation of the environment, they can provide meaningful explanations for
gaps between the anticipated and actual agent behavior. We formally define the
explainability problem, suggest a class of transforms that can be used for
explaining emergent behaviors, and suggest methods that enable efficient search
for an explanation. We demonstrate the approach on a set of standard
benchmarks.",https://github.com/sarah-keren/RLPE.git,-1
Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for Personality Detection,0.182825,"Predicting personality traits based on online posts has emerged as an
important task in many fields such as social network analysis. One of the
challenges of this task is assembling information from various posts into an
overall profile for each user. While many previous solutions simply concatenate
the posts into a long document and then encode the document by sequential or
hierarchical models, they introduce unwarranted orders for the posts, which may
mislead the models. In this paper, we propose a dynamic deep graph
convolutional network (D-DGCN) to overcome the above limitation. Specifically,
we design a learn-to-connect approach that adopts a dynamic multi-hop structure
instead of a deterministic structure, and combine it with a DGCN module to
automatically learn the connections between posts. The modules of post encoder,
learn-to-connect, and DGCN are jointly trained in an end-to-end manner.
Experimental results on the Kaggle and Pandora datasets show the superior
performance of D-DGCN to state-of-the-art baselines. Our code is available at
https://github.com/djz233/D-DGCN.",https://github.com/djz233/D-DGCN,-1
Adversarial Learning to Reason in an Arbitrary Logic,0.00234891,"Existing approaches to learning to prove theorems focus on particular logics
and datasets. In this work, we propose Monte-Carlo simulations guided by
reinforcement learning that can work in an arbitrarily specified logic, without
any human knowledge or set of problems. Since the algorithm does not need any
training dataset, it is able to learn to work with any logical foundation, even
when there is no body of proofs or even conjectures available. We practically
demonstrate the feasibility of the approach in multiple logical systems. The
approach is stronger than training on randomly generated data but weaker than
the approaches trained on tailored axiom and conjecture sets. It however allows
us to apply machine learning to automated theorem proving for many logics,
where no such attempts have been tried to date, such as intuitionistic logic or
linear logic.",None,-1
A deep scalable neural architecture for soil properties estimation from spectral information,0.00866028,"In this paper we propose an adaptive deep neural architecture for the
prediction of multiple soil characteristics from the analysis of hyperspectral
signatures. The proposed method overcomes the limitations of previous methods
in the state of art: (i) it allows to predict multiple soil variables at once;
(ii) it permits to backtrace the spectral bands that most contribute to the
estimation of a given variable; (iii) it is based on a flexible neural
architecture capable of automatically adapting to the spectral library under
analysis. The proposed architecture is experimented on LUCAS, a large
laboratory dataset and on a dataset achieved by simulating PRISMA hyperspectral
sensor. 'Results, compared with other state-of-the-art methods confirm the
effectiveness of the proposed solution.",https://github.com/dros1986/scalable-cnn-for-soil-properties-estimation,-1
Beyond Prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations,0.053644,"Recent work has demonstrated that pre-trained language models (PLMs) are
zero-shot learners. However, most existing zero-shot methods involve heavy
human engineering or complicated self-training pipelines, hindering their
application to new situations. In this work, we show that zero-shot text
classification can be improved simply by clustering texts in the embedding
spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian
Gaussian Mixture Model after initializing cluster positions and shapes using
class names. Despite its simplicity, this approach achieves superior or
comparable performance on both topic and sentiment classification datasets and
outperforms prior works significantly on unbalanced datasets. We further
explore the applicability of our clustering approach by evaluating it on 14
datasets with more diverse topics, text lengths, and numbers of classes. Our
approach achieves an average of 20% absolute improvement over prompt-based
zero-shot learning. Finally, we compare different PLM embedding spaces and find
that texts are well-clustered by topics even if the PLM is not explicitly
pre-trained to generate meaningful sentence embeddings. This work indicates
that PLM embeddings can categorize texts without task-specific fine-tuning,
thus providing a new way to analyze and utilize their knowledge and zero-shot
learning ability.",https://github.com/fywalter/simptc,-1
Data-Driven Online Interactive Bidding Strategy for Demand Response,0.02367,"Demand response (DR), as one of the important energy resources in the
future's grid, provides the services of peak shaving, enhancing the efficiency
of renewable energy utilization with a short response period, and low cost.
Various categories of DR are established, e.g. automated DR, incentive DR,
emergency DR, and demand bidding. However, with the practical issue of the
unawareness of residential and commercial consumers' utility models, the
researches about demand bidding aggregator involved in the electricity market
are just at the beginning stage. For this issue, the bidding price and bidding
quantity are two required decision variables while considering the
uncertainties due to the market and participants. In this paper, we determine
the bidding and purchasing strategy simultaneously employing the smart meter
data and functions. A two-agent deep deterministic policy gradient method is
developed to optimize the decisions through learning historical bidding
experiences. The online learning further utilizes the daily newest bidding
experience attained to ensure trend tracing and self-adaptation. Two
environment simulators are adopted for testifying the robustness of the model.
The results prove that when facing diverse situations the proposed model can
earn the optimal profit via off/online learning the bidding rules and robustly
making the proper bid.",None,-1
Score-Guided Intermediate Layer Optimization: Fast Langevin Mixing for Inverse Problems,0.247393,"We prove fast mixing and characterize the stationary distribution of the
Langevin Algorithm for inverting random weighted DNN generators. This result
extends the work of Hand and Voroninski from efficient inversion to efficient
posterior sampling. In practice, to allow for increased expressivity, we
propose to do posterior sampling in the latent space of a pre-trained
generative model. To achieve that, we train a score-based model in the latent
space of a StyleGAN-2 and we use it to solve inverse problems. Our framework,
Score-Guided Intermediate Layer Optimization (SGILO), extends prior work by
replacing the sparsity regularization with a generative prior in the
intermediate layer. Experimentally, we obtain significant improvements over the
previous state-of-the-art, especially in the low measurement regime.",None,-1
FedUKD: Federated UNet Model with Knowledge Distillation for Land Use Classification from Satellite and Street Views,0.0105325,"Federated Deep Learning frameworks can be used strategically to monitor Land
Use locally and infer environmental impacts globally. Distributed data from
across the world would be needed to build a global model for Land Use
classification. The need for a Federated approach in this application domain
would be to avoid transfer of data from distributed locations and save network
bandwidth to reduce communication cost. We use a Federated UNet model for
Semantic Segmentation of satellite and street view images. The novelty of the
proposed architecture is the integration of Knowledge Distillation to reduce
communication cost and response time. The accuracy obtained was above 95% and
we also brought in a significant model compression to over 17 times and 62
times for street View and satellite images respectively. Our proposed framework
has the potential to be a game-changer in real-time tracking of climate change
across the planet.",https://anonymous.4open.science/r/FedUKD,-1
Event Tables for Efficient Experience Replay,0.0241444,"Experience replay (ER) is a crucial component of many deep reinforcement
learning (RL) systems. However, uniform sampling from an ER buffer can lead to
slow convergence and unstable asymptotic behaviors. This paper introduces
Stratified Sampling from Event Tables (SSET), which partitions an ER buffer
into Event Tables, each capturing important subsequences of optimal behavior.
We prove a theoretical advantage over the traditional monolithic buffer
approach and combine SSET with an existing prioritized sampling strategy to
further improve learning speed and stability. Empirical results in challenging
MiniGrid domains, benchmark RL environments, and a high-fidelity car racing
simulator demonstrate the advantages and versatility of SSET over existing ER
buffer sampling approaches.",None,-1
OptG: Optimizing Gradient-driven Criteria in Network Sparsity,0.32968,"Network sparsity receives popularity mostly due to its capability to reduce
the network complexity. Extensive studies excavate gradient-driven sparsity.
Typically, these methods are constructed upon premise of weight independence,
which however, is contrary to the fact that weights are mutually influenced.
Thus, their performance remains to be improved. In this paper, we propose to
optimize gradient-driven sparsity (OptG) by solving this independence paradox.
Our motive comes from the recent advances in supermask training which shows
that high-performing sparse subnetworks can be located by simply updating mask
values without modifying any weight. We prove that supermask training is to
accumulate the criteria of gradient-driven sparsity for both removed and
preserved weights, and it can partly solve the independence paradox.
Consequently, OptG integrates supermask training into gradient-driven sparsity,
and a novel supermask optimizer is further proposed to comprehensively mitigate
the independence paradox. Experiments show that OptG can well surpass many
existing state-of-the-art competitors, especially at ultra-high sparsity
levels. Our code is available at \url{https://github.com/zyxxmu/OptG}.",https://github.com/zyxxmu/OptG,-1
Learning Disentangled Representations of Negation and Uncertainty,0.15684,"Negation and uncertainty modeling are long-standing tasks in natural language
processing. Linguistic theory postulates that expressions of negation and
uncertainty are semantically independent from each other and the content they
modify. However, previous works on representation learning do not explicitly
model this independence. We therefore attempt to disentangle the
representations of negation, uncertainty, and content using a Variational
Autoencoder. We find that simply supervising the latent representations results
in good disentanglement, but auxiliary objectives based on adversarial learning
and mutual information minimization can provide additional disentanglement
gains.",https://github.com/jvasilakes/disentanglement-vae,-1
DIGAT: Modeling News Recommendation with Dual-Graph Interaction,0.047045,"News recommendation (NR) is essential for online news services. Existing NR
methods typically adopt a news-user representation learning framework, facing
two potential limitations. First, in news encoder, single candidate news
encoding suffers from an insufficient semantic information problem. Second,
existing graph-based NR methods are promising but lack effective news-user
feature interaction, rendering the graph-based recommendation suboptimal. To
overcome these limitations, we propose dual-interactive graph attention
networks (DIGAT) consisting of news- and user-graph channels. In the news-graph
channel, we enrich the semantics of single candidate news by incorporating the
semantically relevant news information with a semantic-augmented graph (SAG).
In the user-graph channel, multi-level user interests are represented with a
news-topic graph. Most notably, we design a dual-graph interaction process to
perform effective feature interaction between the news and user graphs, which
facilitates accurate news-user representation matching. Experiment results on
the benchmark dataset MIND show that DIGAT outperforms existing news
recommendation methods. Further ablation studies and analyses validate the
effectiveness of (1) semantic-augmented news graph modeling and (2) dual-graph
interaction.",https://github.com/Veason-silverbullet/DIGAT,-1
"Strategy Complexity of Point Payoff, Mean Payoff and Total Payoff Objectives in Countable MDPs",0.0,"We study countably infinite Markov decision processes (MDPs) with real-valued
transition rewards. Every infinite run induces the following sequences of
payoffs: 1. Point payoff (the sequence of directly seen transition rewards), 2.
Mean payoff (the sequence of the sums of all rewards so far, divided by the
number of steps), and 3. Total payoff (the sequence of the sums of all rewards
so far). For each payoff type, the objective is to maximize the probability
that the $\liminf$ is non-negative. We establish the complete picture of the
strategy complexity of these objectives, i.e., how much memory is necessary and
sufficient for $\varepsilon$-optimal (resp. optimal) strategies. Some cases can
be won with memoryless deterministic strategies, while others require a step
counter, a reward counter, or both.",None,-1
CC-Riddle: A Question Answering Dataset of Chinese Character Riddles,0.0104335,"The Chinese character riddle is a unique form of cultural entertainment
specific to the Chinese language. It typically comprises two parts: the riddle
description and the solution. The solution to the riddle is a single character,
while the riddle description primarily describes the glyph of the solution,
occasionally supplemented with its explanation and pronunciation. Solving
Chinese character riddles is a challenging task that demands understanding of
character glyph, general knowledge, and a grasp of figurative language. In this
paper, we construct a \textbf{C}hinese \textbf{C}haracter riddle dataset named
CC-Riddle, which covers the majority of common simplified Chinese characters.
The construction process is a combination of web crawling, language model
generation and manual filtering. In generation stage, we input the Chinese
phonetic alphabet, glyph and meaning of the solution character into the
generation model, which then produces multiple riddle descriptions. The
generated riddles are then manually filtered and the final CC-Riddle dataset is
composed of both human-written riddles and these filtered, generated riddles.
In order to assess the performance of language models on the task of solving
character riddles, we use retrieval-based, generative and multiple-choice QA
strategies to test three language models: BERT, ChatGPT and ChatGLM. The test
results reveal that current language models still struggle to solve Chinese
character riddles. CC-Riddle is publicly available at
\url{https://github.com/pku0xff/CC-Riddle}.",https://github.com/pku0xff/CC-Riddle,-1
M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation,0.173782,"End-to-end speech-to-text translation models are often initialized with
pre-trained speech encoder and pre-trained text decoder. This leads to a
significant training gap between pre-training and fine-tuning, largely due to
the modality differences between speech outputs from the encoder and text
inputs to the decoder. In this work, we aim to bridge the modality gap between
speech and text to improve translation quality. We propose M-Adapter, a novel
Transformer-based module, to adapt speech representations to text. While
shrinking the speech sequence, M-Adapter produces features desired for
speech-to-text translation via modelling global and local dependencies of a
speech sequence. Our experimental results show that our model outperforms a
strong baseline by up to 1 BLEU score on the Must-C En$\rightarrow$DE
dataset.\footnote{Our code is available at
https://github.com/mingzi151/w2v2-st.}",https://github.com/mingzi151/w2v2-st,-1
A Twitter-Driven Deep Learning Mechanism for the Determination of Vehicle Hijacking Spots in Cities,0.0357973,"Vehicle hijacking is one of the leading crimes in many cities. For instance,
in South Africa, drivers must constantly remain vigilant on the road in order
to ensure that they do not become hijacking victims. This work is aimed at
developing a map depicting hijacking spots in a city by using Twitter data.
Tweets, which include the keyword ""hijacking"", are obtained in a designated
city of Cape Town, in this work. In order to extract relevant tweets, these
tweets are analyzed by using the following machine learning techniques: 1) a
Multi-layer Feed-forward Neural Network (MLFNN); 2) Convolutional Neural
Network; and Bidirectional Encoder Representations from Transformers (BERT).
Through training and testing, CNN achieved an accuracy of 99.66%, while MLFNN
and BERT achieve accuracies of 98.99% and 73.99% respectively. In terms of
Recall, Precision and F1-score, CNN also achieved the best results. Therefore,
CNN was used for the identification of relevant tweets. The relevant reports
that it generates are visually presented on a points map of the City of Cape
Town. This work used a small dataset of 426 tweets. In future, the use of
evolutionary computation will be explored for purposes of optimizing the deep
learning models. A mobile application is under development to make this
information usable by the general public.",None,-1
Self-Supervised Losses for One-Class Textual Anomaly Detection,0.140809,"Current deep learning methods for anomaly detection in text rely on
supervisory signals in inliers that may be unobtainable or bespoke
architectures that are difficult to tune. We study a simpler alternative:
fine-tuning Transformers on the inlier data with self-supervised objectives and
using the losses as an anomaly score. Overall, the self-supervision approach
outperforms other methods under various anomaly detection scenarios, improving
the AUROC score on semantic anomalies by 11.6% and on syntactic anomalies by
22.8% on average. Additionally, the optimal objective and resultant learnt
representation depend on the type of downstream anomaly. The separability of
anomalies and inliers signals that a representation is more effective for
detecting semantic anomalies, whilst the presence of narrow feature directions
signals a representation that is effective for detecting syntactic anomalies.",None,-1
Monte Carlo Planning in Hybrid Belief POMDPs,0.00816115,"Real-world problems often require reasoning about hybrid beliefs, over both
discrete and continuous random variables. Yet, such a setting has hardly been
investigated in the context of planning. Moreover, existing online Partially
Observable Markov Decision Processes (POMDPs) solvers do not support hybrid
beliefs directly. In particular, these solvers do not address the added
computational burden due to an increasing number of hypotheses with the
planning horizon, which can grow exponentially. As part of this work, we
present a novel algorithm, Hybrid Belief Monte Carlo Planning (HB-MCP) that
utilizes the Monte Carlo Tree Search (MCTS) algorithm to solve a POMDP while
maintaining a hybrid belief. We illustrate how the upper confidence bound (UCB)
exploration bonus can be leveraged to guide the growth of hypotheses trees
alongside the belief trees. We then evaluate our approach in highly aliased
simulated environments where unresolved data association leads to multi-modal
belief hypotheses.",None,-1
PointInverter: Point Cloud Reconstruction and Editing via a Generative Model with Shape Priors,0.0352937,"In this paper, we propose a new method for mapping a 3D point cloud to the
latent space of a 3D generative adversarial network. Our generative model for
3D point clouds is based on SP-GAN, a state-of-the-art sphere-guided 3D point
cloud generator. We derive an efficient way to encode an input 3D point cloud
to the latent space of the SP-GAN. Our point cloud encoder can resolve the
point ordering issue during inversion, and thus can determine the
correspondences between points in the generated 3D point cloud and those in the
canonical sphere used by the generator. We show that our method outperforms
previous GAN inversion methods for 3D point clouds, achieving state-of-the-art
results both quantitatively and qualitatively. Our code is available at
https://github.com/hkust-vgd/point_inverter.",https://github.com/hkust-vgd/point_inverter,-1
CLINICAL: Targeted Active Learning for Imbalanced Medical Image Classification,0.0562619,"Training deep learning models on medical datasets that perform well for all
classes is a challenging task. It is often the case that a suboptimal
performance is obtained on some classes due to the natural class imbalance
issue that comes with medical data. An effective way to tackle this problem is
by using targeted active learning, where we iteratively add data points to the
training data that belong to the rare classes. However, existing active
learning methods are ineffective in targeting rare classes in medical datasets.
In this work, we propose Clinical (targeted aCtive Learning for ImbalaNced
medICal imAge cLassification) a framework that uses submodular mutual
information functions as acquisition functions to mine critical data points
from rare classes. We apply our framework to a wide-array of medical imaging
datasets on a variety of real-world class imbalance scenarios - namely, binary
imbalance and long-tail imbalance. We show that Clinical outperforms the
state-of-the-art active learning methods by acquiring a diverse set of data
points that belong to the rare classes.",None,-1
Multilingual Multimodal Learning with Machine Translated Text,0.073169,"Most vision-and-language pretraining research focuses on English tasks.
However, the creation of multilingual multimodal evaluation datasets (e.g.
Multi30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality
training data that is both multilingual and multimodal. In this paper, we
investigate whether machine translating English multimodal data can be an
effective proxy for the lack of readily available multilingual data. We call
this framework TD-MML: Translated Data for Multilingual Multimodal Learning,
and it can be applied to any multimodal dataset and model. We apply it to both
pretraining and fine-tuning data with a state-of-the-art model. In order to
prevent models from learning from low-quality translated text, we propose two
metrics for automatically removing such translations from the resulting
datasets. In experiments on five tasks across 20 languages in the IGLUE
benchmark, we show that translated data can provide a useful signal for
multilingual multimodal learning, both at pretraining and fine-tuning.",https://github.com/danoneata/td-mml,-1
CLEAR: Causal Explanations from Attention in Neural Recommenders,0.0450677,"We present CLEAR, a method for learning session-specific causal graphs, in
the possible presence of latent confounders, from attention in pre-trained
attention-based recommenders. These causal graphs describe user behavior,
within the context captured by attention, and can provide a counterfactual
explanation for a recommendation. In essence, these causal graphs allow
answering ""why"" questions uniquely for any specific session. Using empirical
evaluations we show that, compared to naively using attention weights to
explain input-output relations, counterfactual explanations found by CLEAR are
shorter and an alternative recommendation is ranked higher in the original
top-k recommendations.",None,-1
UM6P-CS at SemEval-2022 Task 11: Enhancing Multilingual and Code-Mixed Complex Named Entity Recognition via Pseudo Labels using Multilingual Transformer,0.0198676,"Building real-world complex Named Entity Recognition (NER) systems is a
challenging task. This is due to the complexity and ambiguity of named entities
that appear in various contexts such as short input sentences, emerging
entities, and complex entities. Besides, real-world queries are mostly
malformed, as they can be code-mixed or multilingual, among other scenarios. In
this paper, we introduce our submitted system to the Multilingual Complex Named
Entity Recognition (MultiCoNER) shared task. We approach the complex NER for
multilingual and code-mixed queries, by relying on the contextualized
representation provided by the multilingual Transformer XLM-RoBERTa. In
addition to the CRF-based token classification layer, we incorporate a span
classification loss to recognize named entities spans. Furthermore, we use a
self-training mechanism to generate weakly-annotated data from a large
unlabeled dataset. Our proposed system is ranked 6th and 8th in the
multilingual and code-mixed MultiCoNER's tracks respectively.",None,-1
Stop Filtering: Multi-View Attribute-Enhanced Dialogue Learning,0.0227809,"There is a growing interest in improving the conversational ability of models
by filtering the raw dialogue corpora. Previous filtering strategies usually
rely on a scoring method to assess and discard samples from one perspective,
enabling the model to enhance the corresponding dialogue attributes (e.g.,
consistency) more easily. However, the discarded samples may obtain high scores
in other perspectives and can provide regularization effects on the model
learning, which causes the performance improvement to be sensitive to the
filtering ratio. In this work, we propose a multi-view attribute-enhanced
dialogue learning framework that strengthens the attribute-related features
more robustly and comprehensively. Instead of filtering the raw dataset to
train the model, our framework first pre-trains the model on the raw dataset
and then fine-tunes it through adapters on the selected sub-sets, which also
enhances certain attributes of responses but without suffering from the
problems mentioned above. Considering the variety of the dialogue attribute, we
further design a multi-view enhancement mechanism, including multi-view
selection and inter-view fusion. It groups the high-quality samples from
multiple perspectives, respectively, and enhances different attributes of
responses with the corresponding sample sets and adapters, keeping knowledge
independent and allowing flexible integration. Empirical results and analysis
show that our framework can improve the performance significantly in terms of
enhancing dialogue attributes and fusing view-specific knowledge.",None,-1
Online Detection Of Supply Chain Network Disruptions Using Sequential Change-Point Detection for Hawkes Processes,0.0466921,"In this paper, we attempt to detect an inflection or change-point resulting
from the Covid-19 pandemic on supply chain data received from a large furniture
company. To accomplish this, we utilize a modified CUSUM (Cumulative Sum)
procedure on the company's spatial-temporal order data as well as a GLR
(Generalized Likelihood Ratio) based method. We model the order data using the
Hawkes Process Network, a multi-dimensional self and mutually exciting point
process, by discretizing the spatial data and treating each order as an event
that has a corresponding node and time. We apply the methodologies on the
company's most ordered item on a national scale and perform a deep dive into a
single state. Because the item was ordered infrequently in the state compared
to the nation, this approach allows us to show efficacy upon different degrees
of data sparsity. Furthermore, it showcases use potential across differing
levels of spatial detail.",None,-1
The distribution of syntactic dependency distances,0.0112027,"The syntactic structure of a sentence can be represented as a graph where
vertices are words and edges indicate syntactic dependencies between them. In
this setting, the distance between two syntactically linked words can be
defined as the difference between their positions. Here we want to contribute
to the characterization of the actual distribution of syntactic dependency
distances, and unveil its relationship with short-term memory limitations. We
propose a new double-exponential model in which decay in probability is allowed
to change after a break-point. This transition could mirror the transition from
the processing of words chunks to higher-level structures. We find that a
two-regime model -- where the first regime follows either an exponential or a
power-law decay -- is the most likely one in all 20 languages we considered,
independently of sentence length and annotation style. Moreover, the
break-point is fairly stable across languages and averages values of 4-5 words,
suggesting that the amount of words that can be simultaneously processed
abstracts from the specific language to a high degree. Finally, we give an
account of the relation between the best estimated model and the closeness of
syntactic dependencies, as measured by a recently introduced optimality score.",None,-1
Estimating Social Influence from Observational Data,0.00737052,"We consider the problem of estimating social influence, the effect that a
person's behavior has on the future behavior of their peers. The key challenge
is that shared behavior between friends could be equally explained by influence
or by two other confounding factors: 1) latent traits that caused people to
both become friends and engage in the behavior, and 2) latent preferences for
the behavior. This paper addresses the challenges of estimating social
influence with three contributions. First, we formalize social influence as a
causal effect, one which requires inferences about hypothetical interventions.
Second, we develop Poisson Influence Factorization (PIF), a method for
estimating social influence from observational data. PIF fits probabilistic
factor models to networks and behavior data to infer variables that serve as
substitutes for the confounding latent traits. Third, we develop assumptions
under which PIF recovers estimates of social influence. We empirically study
PIF with semi-synthetic and real data from Last.fm, and conduct a sensitivity
analysis. We find that PIF estimates social influence most accurately compared
to related methods and remains robust under some violations of its assumptions.",https://github.com/blei-lab/poisson-influence-factorization,-1
Keyword Extraction from Short Texts with a Text-To-Text Transfer Transformer,0.093237,"The paper explores the relevance of the Text-To-Text Transfer Transformer
language model (T5) for Polish (plT5) to the task of intrinsic and extrinsic
keyword extraction from short text passages. The evaluation is carried out on
the new Polish Open Science Metadata Corpus (POSMAC), which is released with
this paper: a collection of 216,214 abstracts of scientific publications
compiled in the CURLICAT project. We compare the results obtained by four
different methods, i.e. plT5kw, extremeText, TermoPL, KeyBERT and conclude that
the plT5kw model yields particularly promising results for both frequent and
sparsely represented keywords. Furthermore, a plT5kw keyword generation model
trained on the POSMAC also seems to produce highly useful results in
cross-domain text labelling scenarios. We discuss the performance of the model
on news stories and phone-based dialog transcripts which represent text genres
and domains extrinsic to the dataset of scientific abstracts. Finally, we also
attempt to characterize the challenges of evaluating a text-to-text model on
both intrinsic and extrinsic keyword extraction.",None,-1
System Resilience through Health Monitoring and Reconfiguration,0.0653568,"We demonstrate an end-to-end framework to improve the resilience of man-made
systems to unforeseen events. The framework is based on a physics-based digital
twin model and three modules tasked with real-time fault diagnosis, prognostics
and reconfiguration. The fault diagnosis module uses model-based diagnosis
algorithms to detect and isolate faults and generates interventions in the
system to disambiguate uncertain diagnosis solutions. We scale up the fault
diagnosis algorithm to the required real-time performance through the use of
parallelization and surrogate models of the physics-based digital twin. The
prognostics module tracks the fault progressions and trains the online
degradation models to compute remaining useful life of system components. In
addition, we use the degradation models to assess the impact of the fault
progression on the operational requirements. The reconfiguration module uses
PDDL-based planning endowed with semantic attachments to adjust the system
controls so that the fault impact on the system operation is minimized. We
define a resilience metric and use the example of a fuel system model to
demonstrate how the metric improves with our framework.",None,-1
Urban Scene Semantic Segmentation with Low-Cost Coarse Annotation,0.0622467,"For best performance, today's semantic segmentation methods use large and
carefully labeled datasets, requiring expensive annotation budgets. In this
work, we show that coarse annotation is a low-cost but highly effective
alternative for training semantic segmentation models. Considering the urban
scene segmentation scenario, we leverage cheap coarse annotations for
real-world captured data, as well as synthetic data to train our model and show
competitive performance compared with finely annotated real-world data.
Specifically, we propose a coarse-to-fine self-training framework that
generates pseudo labels for unlabeled regions of the coarsely annotated data,
using synthetic data to improve predictions around the boundaries between
semantic classes, and using cross-domain data augmentation to increase
diversity. Our extensive experimental results on Cityscapes and BDD100k
datasets demonstrate that our method achieves a significantly better
performance vs annotation cost tradeoff, yielding a comparable performance to
fully annotated data with only a small fraction of the annotation budget. Also,
when used as pretraining, our framework performs better compared to the
standard fully supervised setting.",None,-1
A Better Choice: Entire-space Datasets for Aspect Sentiment Triplet Extraction,0.0191404,"Aspect sentiment triplet extraction (ASTE) aims to extract aspect term,
sentiment and opinion term triplets from sentences. Since the initial datasets
used to evaluate models on ASTE had flaws, several studies later corrected the
initial datasets and released new versions of the datasets independently. As a
result, different studies select different versions of datasets to evaluate
their methods, which makes ASTE-related works hard to follow. In this paper, we
analyze the relation between different versions of datasets and suggest that
the entire-space version should be used for ASTE. Besides the sentences
containing triplets and the triplets in the sentences, the entire-space version
additionally includes the sentences without triplets and the aspect terms which
do not belong to any triplets. Hence, the entire-space version is consistent
with real-world scenarios and evaluating models on the entire-space version can
better reflect the models' performance in real-world scenarios. In addition,
experimental results show that evaluating models on non-entire-space datasets
inflates the performance of existing models and models trained on the
entire-space version can obtain better performance.",https://github.com/l294265421/entire-space-aste,-1
PairReranker: Pairwise Reranking for Natural Language Generation,0.00779766,"Pre-trained language models have been successful in natural language
generation (NLG) tasks. While various decoding methods have been employed, they
often produce suboptimal results. We first present an empirical analysis of
three NLG tasks: summarization, machine translation, and constrained text
generation. We found that selecting the best output from the results of
multiple decoding methods can significantly improve performance. To further
improve reranking for NLG tasks, we proposed a novel method,
\textsc{PairReranker}, which uses a single encoder and a pairwise loss function
to jointly encode a source input and a pair of candidates and compare them.
Experiments on three NLG tasks demonstrated the effectiveness and flexibility
of \textsc{PairReranker}, showing strong results, compared with previous
baselines. In addition, our \textsc{PairReranker} can generalize to
significantly improve GPT-3 (text-davinci-003) results (e.g., 24.55\% on
CommonGen and 11.35\% on WMT18 zh-en), even though our rerankers are not
trained with any GPT-3 candidates.",https://inklab.usc.edu/PairReranker,-1
Connecting Neural Response measurements & Computational Models of language: a non-comprehensive guide,0.103494,"Understanding the neural basis of language comprehension in the brain has
been a long-standing goal of various scientific research programs. Recent
advances in language modelling and in neuroimaging methodology promise
potential improvements in both the investigation of language's neurobiology and
in the building of better and more human-like language models. This survey
traces a line from early research linking Event Related Potentials and
complexity measures derived from simple language models to contemporary studies
employing Artificial Neural Network models trained on large corpora in
combination with neural response recordings from multiple modalities using
naturalistic stimuli.",None,-1
A generative grammar of cooking,0.0,"Cooking is a uniquely human endeavor for transforming raw ingredients into
delicious dishes. Over centuries, cultures worldwide have evolved diverse
cooking practices ingrained in their culinary traditions. Recipes, thus, are
cultural capsules that capture culinary knowledge in elaborate cooking
protocols. While simple quantitative models have probed the patterns in recipe
composition and the process of cuisine evolution, unlike other cultural quirks
such as language, the principles of cooking remain hitherto unexplored. The
fundamental rules that drive the act of cooking, shaping recipe composition and
cuisine architecture, are unclear. Here we present a generative grammar of
cooking that captures the underlying culinary logic. By studying an extensive
repository of structured recipes, we identify core concepts and rules that
together forge a combinatorial system for culinary synthesis. Building on the
body of work done in the context of language, the demonstration of a logically
consistent generative framework offers profound insights into the act of
cooking. Given the central role of food in nutrition and lifestyle disorders,
culinary grammar provides leverage to improve public health through dietary
interventions beyond applications for creative pursuits such as novel recipe
generation.",None,-1
Is it all a cluster game? -- Exploring Out-of-Distribution Detection based on Clustering in the Embedding Space,0.0438781,"It is essential for safety-critical applications of deep neural networks to
determine when new inputs are significantly different from the training
distribution. In this paper, we explore this out-of-distribution (OOD)
detection problem for image classification using clusters of semantically
similar embeddings of the training data and exploit the differences in distance
relationships to these clusters between in- and out-of-distribution data. We
study the structure and separation of clusters in the embedding space and find
that supervised contrastive learning leads to well-separated clusters while its
self-supervised counterpart fails to do so. In our extensive analysis of
different training methods, clustering strategies, distance metrics, and
thresholding approaches, we observe that there is no clear winner. The optimal
approach depends on the model architecture and selected datasets for in- and
out-of-distribution. While we could reproduce the outstanding results for
contrastive training on CIFAR-10 as in-distribution data, we find standard
cross-entropy paired with cosine similarity outperforms all contrastive
training methods when training on CIFAR-100 instead. Cross-entropy provides
competitive results as compared to expensive contrastive training methods.",None,-1
Contrastive Learning for Joint Normal Estimation and Point Cloud Filtering,0.00437056,"Point cloud filtering and normal estimation are two fundamental research
problems in the 3D field. Existing methods usually perform normal estimation
and filtering separately and often show sensitivity to noise and/or inability
to preserve sharp geometric features such as corners and edges. In this paper,
we propose a novel deep learning method to jointly estimate normals and filter
point clouds. We first introduce a 3D patch based contrastive learning
framework, with noise corruption as an augmentation, to train a feature encoder
capable of generating faithful representations of point cloud patches while
remaining robust to noise. These representations are consumed by a simple
regression network and supervised by a novel joint loss, simultaneously
estimating point normals and displacements that are used to filter the patch
centers. Experimental results show that our method well supports the two tasks
simultaneously and preserves sharp features and fine details. It generally
outperforms state-of-the-art techniques on both tasks. Our source code is
available at https://github.com/ddsediri/CLJNEPCF.",https://github.com/ddsediri/CLJNEPCF,-1
Local Feature Swapping for Generalization in Reinforcement Learning,0.0422064,"Over the past few years, the acceleration of computing resources and research
in deep learning has led to significant practical successes in a range of
tasks, including in particular in computer vision. Building on these advances,
reinforcement learning has also seen a leap forward with the emergence of
agents capable of making decisions directly from visual observations. Despite
these successes, the over-parametrization of neural architectures leads to
memorization of the data used during training and thus to a lack of
generalization. Reinforcement learning agents based on visual inputs also
suffer from this phenomenon by erroneously correlating rewards with unrelated
visual features such as background elements. To alleviate this problem, we
introduce a new regularization technique consisting of channel-consistent local
permutations (CLOP) of the feature maps. The proposed permutations induce
robustness to spatial correlations and help prevent overfitting behaviors in
RL. We demonstrate, on the OpenAI Procgen Benchmark, that RL agents trained
with the CLOP method exhibit robustness to visual changes and better
generalization properties than agents trained using other state-of-the-art
regularization techniques. We also demonstrate the effectiveness of CLOP as a
general regularization technique in supervised learning.",https://github.com/rraileanu/idaac,-1
TALCS: An Open-Source Mandarin-English Code-Switching Corpus and a Speech Recognition Baseline,0.0802702,"This paper introduces a new corpus of Mandarin-English code-switching speech
recognition--TALCS corpus, suitable for training and evaluating code-switching
speech recognition systems. TALCS corpus is derived from real online one-to-one
English teaching scenes in TAL education group, which contains roughly 587
hours of speech sampled at 16 kHz. To our best knowledge, TALCS corpus is the
largest well labeled Mandarin-English code-switching open source automatic
speech recognition (ASR) dataset in the world. In this paper, we will introduce
the recording procedure in detail, including audio capturing devices and corpus
environments. And the TALCS corpus is freely available for download under the
permissive license1. Using TALCS corpus, we conduct ASR experiments in two
popular speech recognition toolkits to make a baseline system, including ESPnet
and Wenet. The Mixture Error Rate (MER) performance in the two speech
recognition toolkits is compared in TALCS corpus. The experimental results
implies that the quality of audio recordings and transcriptions are promising
and the baseline system is workable.",https://ai.100tal.com/dataset,-1
PIZZA: A Powerful Image-only Zero-Shot Zero-CAD Approach to 6 DoF Tracking,0.0586418,"Estimating the relative pose of a new object without prior knowledge is a
hard problem, while it is an ability very much needed in robotics and Augmented
Reality. We present a method for tracking the 6D motion of objects in RGB video
sequences when neither the training images nor the 3D geometry of the objects
are available. In contrast to previous works, our method can therefore consider
unknown objects in open world instantly, without requiring any prior
information or a specific training phase. We consider two architectures, one
based on two frames, and the other relying on a Transformer Encoder, which can
exploit an arbitrary number of past frames. We train our architectures using
only synthetic renderings with domain randomization. Our results on challenging
datasets are on par with previous works that require much more information
(training images of the target objects, 3D models, and/or depth data). Our
source code is available at https://github.com/nv-nguyen/pizza",https://github.com/nv-nguyen/pizza,-1
Consistency-based Self-supervised Learning for Temporal Anomaly Localization,0.0765198,"This work tackles Weakly Supervised Anomaly detection, in which a predictor
is allowed to learn not only from normal examples but also from a few labeled
anomalies made available during training. In particular, we deal with the
localization of anomalous activities within the video stream: this is a very
challenging scenario, as training examples come only with video-level
annotations (and not frame-level). Several recent works have proposed various
regularization terms to address it i.e. by enforcing sparsity and smoothness
constraints over the weakly-learned frame-level anomaly scores. In this work,
we get inspired by recent advances within the field of self-supervised learning
and ask the model to yield the same scores for different augmentations of the
same video sequence. We show that enforcing such an alignment improves the
performance of the model on XD-Violence.",None,-1
Evaluating Distributional Distortion in Neural Language Modeling,0.0826041,"A fundamental characteristic of natural language is the high rate at which
speakers produce novel expressions. Because of this novelty, a heavy-tail of
rare events accounts for a significant amount of the total probability mass of
distributions in language (Baayen, 2001). Standard language modeling metrics
such as perplexity quantify the performance of language models (LM) in
aggregate. As a result, we have relatively little understanding of whether
neural LMs accurately estimate the probability of sequences in this heavy-tail
of rare events. To address this gap, we develop a controlled evaluation scheme
which uses generative models trained on natural data as artificial languages
from which we can exactly compute sequence probabilities. Training LMs on
generations from these artificial languages, we compare the sequence-level
probability estimates given by LMs to the true probabilities in the target
language. Our experiments reveal that LSTM and Transformer language models (i)
systematically underestimate the probability of sequences drawn from the target
language, and (ii) do so more severely for less-probable sequences.
Investigating where this probability mass went, (iii) we find that LMs tend to
overestimate the probability of ill formed (perturbed) sequences. In addition,
we find that this underestimation behaviour (iv) is weakened, but not
eliminated by greater amounts of training data, and (v) is exacerbated for
target distributions with lower entropy.",None,-1
A new perspective on Digital Twins: Imparting intelligence and agency to entities,0.0418856,"Despite the Digital Twin (DT) concept being in the industry for a long time,
it remains ambiguous, unable to differentiate itself from information models,
general computing, and simulation technologies. Part of this confusion stems
from previous studies overlooking the DT's bidirectional nature, that enables
the shift of agency (delegating control) from humans to physical elements,
something that was not possible with earlier technologies. Thus, we present DTs
in a new light by viewing them as a means of imparting intelligence and agency
to entities, emphasizing that DTs are not just expert-centric tools but are
active systems that extend the capabilities of the entities being twinned. This
new perspective on DTs can help reduce confusion and humanize the concept by
starting discussions about how intelligent a DT should be, and its roles and
responsibilities, as well as setting a long-term direction for DTs.",None,-1
RITA: Boost Driving Simulators with Realistic Interactive Traffic Flow,0.0401843,"High-quality traffic flow generation is the core module in building
simulators for autonomous driving. However, the majority of available
simulators are incapable of replicating traffic patterns that accurately
reflect the various features of real-world data while also simulating
human-like reactive responses to the tested autopilot driving strategies.
Taking one step forward to addressing such a problem, we propose Realistic
Interactive TrAffic flow (RITA) as an integrated component of existing driving
simulators to provide high-quality traffic flow for the evaluation and
optimization of the tested driving strategies. RITA is developed with
consideration of three key features, i.e., fidelity, diversity, and
controllability, and consists of two core modules called RITABackend and
RITAKit. RITABackend is built to support vehicle-wise control and provide
traffic generation models from real-world datasets, while RITAKit is developed
with easy-to-use interfaces for controllable traffic generation via
RITABackend. We demonstrate RITA's capacity to create diversified and
high-fidelity traffic simulations in several highly interactive highway
scenarios. The experimental findings demonstrate that our produced RITA traffic
flows exhibit all three key features, hence enhancing the completeness of
driving strategy evaluation. Moreover, we showcase the possibility for further
improvement of baseline strategies through online fine-tuning with RITA traffic
flows.",None,-1
Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation,0.0350812,"Neural Machine Translation (NMT) has reached a level of maturity to be
recognized as the premier method for the translation between different
languages and aroused interest in different research areas, including software
engineering. A key step to validate the robustness of the NMT models consists
in evaluating the performance of the models on adversarial inputs, i.e., inputs
obtained from the original ones by adding small amounts of perturbation.
However, when dealing with the specific task of the code generation (i.e., the
generation of code starting from a description in natural language), it has not
yet been defined an approach to validate the robustness of the NMT models. In
this work, we address the problem by identifying a set of perturbations and
metrics tailored for the robustness assessment of such models. We present a
preliminary experimental evaluation, showing what type of perturbations affect
the model the most and deriving useful insights for future directions.",None,-1
CINO: A Chinese Minority Pre-trained Language Model,0.7092,"Multilingual pre-trained language models have shown impressive performance on
cross-lingual tasks. It greatly facilitates the applications of natural
language processing on low-resource languages. However, there are still some
languages that the current multilingual models do not perform well on. In this
paper, we propose CINO (Chinese Minority Pre-trained Language Model), a
multilingual pre-trained language model for Chinese minority languages. It
covers Standard Chinese, Yue Chinese, and six other ethnic minority languages.
To evaluate the cross-lingual ability of the multilingual model on ethnic
minority languages, we collect documents from Wikipedia and news websites, and
construct two text classification datasets, WCM (Wiki-Chinese-Minority) and
CMNews (Chinese-Minority-News). We show that CINO notably outperforms the
baselines on various classification tasks. The CINO model and the datasets are
publicly available at http://cino.hfl-rc.com.",http://cino.hfl-rc.com,-1
Language Agnostic Code-Mixing Data Augmentation by Predicting Linguistic Patterns,0.0188526,"In this work, we focus on intrasentential code-mixing and propose several
different Synthetic Code-Mixing (SCM) data augmentation methods that outperform
the baseline on downstream sentiment analysis tasks across various amounts of
labeled gold data. Most importantly, our proposed methods demonstrate that
strategically replacing parts of sentences in the matrix language with a
constant mask significantly improves classification accuracy, motivating
further linguistic insights into the phenomenon of code-mixing. We test our
data augmentation method in a variety of low-resource and cross-lingual
settings, reaching up to a relative improvement of 7.73% on the extremely
scarce English-Malayalam dataset. We conclude that the code-switch pattern in
code-mixing sentences is also important for the model to learn. Finally, we
propose a language-agnostic SCM algorithm that is cheap yet extremely helpful
for low-resource languages.",None,-1
Bridging the Gap between Reality and Ideality of Entity Matching: A Revisiting and Benchmark Re-Construction,0.0683571,"Entity matching (EM) is the most critical step for entity resolution (ER).
While current deep learningbased methods achieve very impressive performance on
standard EM benchmarks, their realworld application performance is much
frustrating. In this paper, we highlight that such the gap between reality and
ideality stems from the unreasonable benchmark construction process, which is
inconsistent with the nature of entity matching and therefore leads to biased
evaluations of current EM approaches. To this end, we build a new EM corpus and
re-construct EM benchmarks to challenge critical assumptions implicit in the
previous benchmark construction process by step-wisely changing the restricted
entities, balanced labels, and single-modal records in previous benchmarks into
open entities, imbalanced labels, and multimodal records in an open
environment. Experimental results demonstrate that the assumptions made in the
previous benchmark construction process are not coincidental with the open
environment, which conceal the main challenges of the task and therefore
significantly overestimate the current progress of entity matching. The
constructed benchmarks and code are publicly released",https://github.com/tshu-w/ember,-1
Controllable Text Generation with Neurally-Decomposed Oracle,0.138016,"We propose a general and efficient framework to control auto-regressive
generation models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained
base language model and a sequence-level boolean oracle function, we propose to
decompose the oracle function into token-level guidance to steer the base model
in text generation. Specifically, the token-level guidance is approximated by a
neural model trained with examples sampled from the base model, demanding no
additional auxiliary labeled data. Based on posterior regularization, we
present the closed-form optimal solution to incorporate the token-level
guidance into the base model for controllable generation. We further provide a
theoretical analysis of how the approximation quality of NADO affects the
controllable generation results. Experiments conducted on two applications: (1)
text generation with lexical constraints and (2) machine translation with
formality control demonstrate that our framework efficiently guides the base
model towards the given oracle while maintaining high generation quality.",https://github.com/MtSomeThree/constrDecoding,-1
Proceedings of the 2022 XCSP3 Competition,0.00677865,"This document represents the proceedings of the 2022 XCSP3 Competition. The
results of this competition of constraint solvers were presented at FLOC
(Federated Logic Conference) 2022 Olympic Games, held in Haifa, Israel from
31th July 2022 to 7th August, 2022.",https://github.com/xcsp3team/ace,-1
Domain-Specific Text Generation for Machine Translation,0.171238,"Preservation of domain knowledge from the source to target is crucial in any
translation workflow. It is common in the translation industry to receive
highly specialized projects, where there is hardly any parallel in-domain data.
In such scenarios where there is insufficient in-domain data to fine-tune
Machine Translation (MT) models, producing translations that are consistent
with the relevant context is challenging. In this work, we propose a novel
approach to domain adaptation leveraging state-of-the-art pretrained language
models (LMs) for domain-specific data augmentation for MT, simulating the
domain characteristics of either (a) a small bilingual dataset, or (b) the
monolingual source text to be translated. Combining this idea with
back-translation, we can generate huge amounts of synthetic bilingual in-domain
data for both use cases. For our investigation, we use the state-of-the-art
Transformer architecture. We employ mixed fine-tuning to train models that
significantly improve translation of in-domain texts. More specifically, in
both scenarios, our proposed methods achieve improvements of approximately 5-6
BLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic
language pairs. Furthermore, the outcome of human evaluation corroborates the
automatic evaluation results.",https://github.com/ymoslem/MT-Preparation,-1
A Feasibility Study on Image Inpainting for Non-cleft Lip Generation from Patients with Cleft Lip,0.036521,"A Cleft lip is a congenital abnormality requiring surgical repair by a
specialist. The surgeon must have extensive experience and theoretical
knowledge to perform surgery, and Artificial Intelligence (AI) method has been
proposed to guide surgeons in improving surgical outcomes. If AI can be used to
predict what a repaired cleft lip would look like, surgeons could use it as an
adjunct to adjust their surgical technique and improve results. To explore the
feasibility of this idea while protecting patient privacy, we propose a deep
learning-based image inpainting method that is capable of covering a cleft lip
and generating a lip and nose without a cleft. Our experiments are conducted on
two real-world cleft lip datasets and are assessed by expert cleft lip surgeons
to demonstrate the feasibility of the proposed method.",https://github.com/ChrisChen1023/NCLG-MT,-1
OTExtSum: Extractive Text Summarisation with Optimal Transport,0.864665,"Extractive text summarisation aims to select salient sentences from a
document to form a short yet informative summary. While learning-based methods
have achieved promising results, they have several limitations, such as
dependence on expensive training and lack of interpretability. Therefore, in
this paper, we propose a novel non-learning-based method by for the first time
formulating text summarisation as an Optimal Transport (OT) problem, namely
Optimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction
is conceptualised as obtaining an optimal summary that minimises the
transportation cost to a given document regarding their semantic distributions.
Such a cost is defined by the Wasserstein distance and used to measure the
summary's semantic coverage of the original document. Comprehensive experiments
on four challenging and widely used datasets - MultiNews, PubMed, BillSum, and
CNN/DM demonstrate that our proposed method outperforms the state-of-the-art
non-learning-based methods and several recent learning-based methods in terms
of the ROUGE metric.",https://github.com/peggypytang/OTExtSum/,-1
HieNet: Bidirectional Hierarchy Framework for Automated ICD Coding,0.295195,"International Classification of Diseases (ICD) is a set of classification
codes for medical records. Automated ICD coding, which assigns unique
International Classification of Diseases codes with each medical record, is
widely used recently for its efficiency and error-prone avoidance. However,
there are challenges that remain such as heterogeneity, label unbalance, and
complex relationships between ICD codes. In this work, we proposed a novel
Bidirectional Hierarchy Framework(HieNet) to address the challenges.
Specifically, a personalized PageRank routine is developed to capture the
co-relation of codes, a bidirectional hierarchy passage encoder to capture the
codes' hierarchical representations, and a progressive predicting method is
then proposed to narrow down the semantic searching space of prediction. We
validate our method on two widely used datasets. Experimental results on two
authoritative public datasets demonstrate that our proposed method boosts
state-of-the-art performance by a large margin.",None,-1
Learn to Adapt for Monocular Depth Estimation,0.0092392,"Monocular depth estimation is one of the fundamental tasks in environmental
perception and has achieved tremendous progress in virtue of deep learning.
However, the performance of trained models tends to degrade or deteriorate when
employed on other new datasets due to the gap between different datasets.
Though some methods utilize domain adaptation technologies to jointly train
different domains and narrow the gap between them, the trained models cannot
generalize to new domains that are not involved in training. To boost the
transferability of depth estimation models, we propose an adversarial depth
estimation task and train the model in the pipeline of meta-learning. Our
proposed adversarial task mitigates the issue of meta-overfitting, since the
network is trained in an adversarial manner and aims to extract domain
invariant representations. In addition, we propose a constraint to impose upon
cross-task depth consistency to compel the depth estimation to be identical in
different adversarial tasks, which improves the performance of our method and
smoothens the training process. Experiments demonstrate that our method adapts
well to new datasets after few training steps during the test procedure.",None,-1
Abstract Interpretation on E-Graphs,0.00738385,"Recent e-graph applications have typically considered concrete semantics of
expressions, where the notion of equivalence stems from concrete interpretation
of expressions. However, equivalences that hold over one interpretation may not
hold in an alternative interpretation. Such an observation can be exploited. We
consider the application of abstract interpretation to e-graphs, and show that
within an e-graph, the lattice meet operation associated with the abstract
domain has a natural interpretation for an e-class, leading to improved
precision in over-approximation. In this extended abstract, we use Interval
Arithmetic (IA) to illustrate this point.",None,-1
BEV-Locator: An End-to-end Visual Semantic Localization Network Using Multi-View Images,0.0979126,"Accurate localization ability is fundamental in autonomous driving.
Traditional visual localization frameworks approach the semantic map-matching
problem with geometric models, which rely on complex parameter tuning and thus
hinder large-scale deployment. In this paper, we propose BEV-Locator: an
end-to-end visual semantic localization neural network using multi-view camera
images. Specifically, a visual BEV (Birds-Eye-View) encoder extracts and
flattens the multi-view images into BEV space. While the semantic map features
are structurally embedded as map queries sequence. Then a cross-model
transformer associates the BEV features and semantic map queries. The
localization information of ego-car is recursively queried out by
cross-attention modules. Finally, the ego pose can be inferred by decoding the
transformer outputs. We evaluate the proposed method in large-scale nuScenes
and Qcraft datasets. The experimental results show that the BEV-locator is
capable to estimate the vehicle poses under versatile scenarios, which
effectively associates the cross-model information from multi-view images and
global semantic maps. The experiments report satisfactory accuracy with mean
absolute errors of 0.052m, 0.135m and 0.251$^\circ$ in lateral, longitudinal
translation and heading angle degree.",None,-1
An Application of Pseudo-Log-Likelihoods to Natural Language Scoring,0.0114502,"Language models built using semi-supervised machine learning on large corpora
of natural language have very quickly enveloped the fields of natural language
generation and understanding. In this paper we apply a zero-shot approach
independently developed by a number of researchers now gaining recognition as a
significant alternative to fine-tuning for evaluation on common sense tasks. A
language model with relatively few parameters and training steps compared to a
more recent language model (T5) can outperform it on a recent large data set
(TimeDial), while displaying robustness in its performance across a similar
class of language tasks. Surprisingly, this result is achieved by using a
hyperparameter-free zero-shot method with the smaller model, compared to
fine-tuning to the larger model. We argue that robustness of the smaller model
ought to be understood in terms of compositionality, in a sense that we draw
from recent literature on a class of similar models. We identify a practical
cost for our method and model: high GPU-time for natural language evaluation.
The zero-shot measurement technique that produces remarkable stability, both
for ALBERT and other BERT variants, is an application of pseudo-log-likelihoods
to masked language models for the relative measurement of probability for
substitution alternatives in forced choice language tasks such as the Winograd
Schema Challenge, Winogrande, and others. One contribution of this paper is to
bring together a number of similar, but independent strands of research. We
produce some absolute state-of-the-art results for common sense reasoning in
binary choice tasks, performing better than any published result in the
literature, including fine-tuned efforts. We show a remarkable consistency of
the model's performance under adversarial settings, which we argue is best
explained by the model's compositionality of representations.",https://anonymous.4open.science/r/NotSoFineTuning-4620/,-1
FRSUM: Towards Faithful Abstractive Summarization via Enhancing Factual Robustness,0.0250855,"Despite being able to generate fluent and grammatical text, current Seq2Seq
summarization models still suffering from the unfaithful generation problem. In
this paper, we study the faithfulness of existing systems from a new
perspective of factual robustness which is the ability to correctly generate
factual information over adversarial unfaithful information. We first measure a
model's factual robustness by its success rate to defend against adversarial
attacks when generating factual information. The factual robustness analysis on
a wide range of current systems shows its good consistency with human judgments
on faithfulness. Inspired by these findings, we propose to improve the
faithfulness of a model by enhancing its factual robustness. Specifically, we
propose a novel training strategy, namely FRSUM, which teaches the model to
defend against both explicit adversarial samples and implicit factual
adversarial perturbations. Extensive automatic and human evaluation results
show that FRSUM consistently improves the faithfulness of various Seq2Seq
models, such as T5, BART.",None,-1
A Socially Assistive Robot using Automated Planning in a Paediatric Clinical Setting,0.0507477,"We present an ongoing project that aims to develop a social robot to help
children cope with painful and distressing medical procedures in a clinical
setting. Our approach uses automated planning as a core component for action
selection in order to generate plans that include physical, sensory, and social
actions for the robot to use when interacting with humans. A key capability of
our system is that the robot's behaviour adapts based on the affective state of
the child patient. The robot must operate in a challenging physical and social
environment where appropriate and safe interaction with children,
parents/caregivers, and healthcare professionals is crucial. In this paper, we
present our system, examine some of the key challenges of the scenario, and
describe how they are addressed by our system.",None,-1
"""Dummy Grandpa, do you know anything?"": Identifying and Characterizing Ad hominem Fallacy Usage in the Wild",0.0429732,"Today, participating in discussions on online forums is extremely commonplace
and these discussions have started rendering a strong influence on the overall
opinion of online users. Naturally, twisting the flow of the argument can have
a strong impact on the minds of naive users, which in the long run might have
socio-political ramifications, for example, winning an election or spreading
targeted misinformation. Thus, these platforms are potentially highly
vulnerable to malicious players who might act individually or as a cohort to
breed fallacious arguments with a motive to sway public opinion. Ad hominem
arguments are one of the most effective forms of such fallacies. Although a
simple fallacy, it is effective enough to sway public debates in offline world
and can be used as a precursor to shutting down the voice of opposition by
slander.
  In this work, we take a first step in shedding light on the usage of ad
hominem fallacies in the wild. First, we build a powerful ad hominem detector
with high accuracy (F1 more than 83%, showing a significant improvement over
prior work), even for datasets for which annotated instances constitute a very
small fraction. We then used our detector on 265k arguments collected from the
online debate forum - CreateDebate. Our crowdsourced surveys validate our
in-the-wild predictions on CreateDebate data (94% match with manual
annotation). Our analysis revealed that a surprising 31.23% of CreateDebate
content contains ad hominem fallacy, and a cohort of highly active users post
significantly more ad hominem to suppress opposing views. Then, our temporal
analysis revealed that ad hominem argument usage increased significantly since
the 2016 US Presidential election, not only for topics like Politics, but also
for Science and Law. We conclude by discussing important implications of our
work to detect and defend against ad hominem fallacies.",None,-1
Learning functional sections in medical conversations: iterative pseudo-labeling and human-in-the-loop approach,0.0131034,"Medical conversations between patients and medical professionals have
implicit functional sections, such as ""history taking"", ""summarization"",
""education"", and ""care plan."" In this work, we are interested in learning to
automatically extract these sections. A direct approach would require
collecting large amounts of expert annotations for this task, which is
inherently costly due to the contextual inter-and-intra variability between
these sections. This paper presents an approach that tackles the problem of
learning to classify medical dialogue into functional sections without
requiring a large number of annotations. Our approach combines pseudo-labeling
and human-in-the-loop. First, we bootstrap using weak supervision with
pseudo-labeling to generate dialogue turn-level pseudo-labels and train a
transformer-based model, which is then applied to individual sentences to
create noisy sentence-level labels. Second, we iteratively refine
sentence-level labels using a cluster-based human-in-the-loop approach. Each
iteration requires only a few dozen annotator decisions. We evaluate the
results on an expert-annotated dataset of 100 dialogues and find that while our
models start with 69.5% accuracy, we can iteratively improve it to 82.5%. The
code used to perform all experiments described in this paper can be found here:
https://github.com/curai/curai-research/tree/main/functional-sections.",https://github.com/curai/curai-research/tree/main/functional-sections.,-1
Prompt-based Generative Approach towards Multi-Hierarchical Medical Dialogue State Tracking,0.0861141,"The medical dialogue system is a promising application that can provide great
convenience for patients. The dialogue state tracking (DST) module in the
medical dialogue system which interprets utterances into the machine-readable
structure for downstream tasks is particularly challenging. Firstly, the states
need to be able to represent compound entities such as symptoms with their body
part or diseases with degrees of severity to provide enough information for
decision support. Secondly, these named entities in the utterance might be
discontinuous and scattered across sentences and speakers. These also make it
difficult to annotate a large corpus which is essential for most methods.
Therefore, we first define a multi-hierarchical state structure. We annotate
and publish a medical dialogue dataset in Chinese. To the best of our
knowledge, there are no publicly available ones before. Then we propose a
Prompt-based Generative Approach which can generate slot values with
multi-hierarchies incrementally using a top-down approach. A dialogue style
prompt is also supplemented to utilize the large unlabeled dialogue corpus to
alleviate the data scarcity problem. The experiments show that our approach
outperforms other DST methods and is rather effective in the scenario with
little data.",None,-1
"Classification of multi-frequency RF signals by extreme learning, using magnetic tunnel junctions as neurons and synapses",0.0780593,"Extracting information from radiofrequency (RF) signals using artificial
neural networks at low energy cost is a critical need for a wide range of
applications from radars to health. These RF inputs are composed of multiples
frequencies. Here we show that magnetic tunnel junctions can process analogue
RF inputs with multiple frequencies in parallel and perform synaptic
operations. Using a backpropagation-free method called extreme learning, we
classify noisy images encoded by RF signals, using experimental data from
magnetic tunnel junctions functioning as both synapses and neurons. We achieve
the same accuracy as an equivalent software neural network. These results are a
key step for embedded radiofrequency artificial intelligence.",None,-1
Decay No More: A Persistent Twitter Dataset for Learning Social Meaning,0.0253884,"With the proliferation of social media, many studies resort to social media
to construct datasets for developing social meaning understanding systems. For
the popular case of Twitter, most researchers distribute tweet IDs without the
actual text contents due to the data distribution policy of the platform. One
issue is that the posts become increasingly inaccessible over time, which leads
to unfair comparisons and a temporal bias in social media research. To
alleviate this challenge of data decay, we leverage a paraphrase model to
propose a new persistent English Twitter dataset for social meaning (PTSM).
PTSM consists of $17$ social meaning datasets in $10$ categories of tasks. We
experiment with two SOTA pre-trained language models and show that our PTSM can
substitute the actual tweets with paraphrases with marginal performance loss.",https://github.com/chiyuzhang94/PTSM,-1
Attentive Dual Stream Siamese U-net for Flood Detection on Multi-temporal Sentinel-1 Data,0.331598,"Due to climate and land-use change, natural disasters such as flooding have
been increasing in recent years. Timely and reliable flood detection and
mapping can help emergency response and disaster management. In this work, we
propose a flood detection network using bi-temporal SAR acquisitions. The
proposed segmentation network has an encoder-decoder architecture with two
Siamese encoders for pre and post-flood images. The network's feature maps are
fused and enhanced using attention blocks to achieve more accurate detection of
the flooded areas. Our proposed network is evaluated on publicly available
Sen1Flood11 benchmark dataset. The network outperformed the existing
state-of-the-art (uni-temporal) flood detection method by 6\% IOU. The
experiments highlight that the combination of bi-temporal SAR data with an
effective network architecture achieves more accurate flood detection than
uni-temporal methods.",None,-1
CoNFies: Controllable Neural Face Avatars,0.0619976,"Neural Radiance Fields (NeRF) are compelling techniques for modeling dynamic
3D scenes from 2D image collections. These volumetric representations would be
well suited for synthesizing novel facial expressions but for two problems.
First, deformable NeRFs are object agnostic and model holistic movement of the
scene: they can replay how the motion changes over time, but they cannot alter
it in an interpretable way. Second, controllable volumetric representations
typically require either time-consuming manual annotations or 3D supervision to
provide semantic meaning to the scene. We propose a controllable neural
representation for face self-portraits (CoNFies), that solves both of these
problems within a common framework, and it can rely on automated processing. We
use automated facial action recognition (AFAR) to characterize facial
expressions as a combination of action units (AU) and their intensities. AUs
provide both the semantic locations and control labels for the system. CoNFies
outperformed competing methods for novel view and expression synthesis in terms
of visual and anatomic fidelity of expressions.",None,-1
Rethinking Round-Trip Translation for Machine Translation Evaluation,0.00968215,"Automatic evaluation on low-resource language translation suffers from a
deficiency of parallel corpora. Round-trip translation could be served as a
clever and straightforward technique to alleviate the requirement of the
parallel evaluation corpus. However, there was an observation of obscure
correlations between the evaluation scores by forward and round-trip
translations in the era of statistical machine translation (SMT). In this
paper, we report the surprising finding that round-trip translation can be used
for automatic evaluation without the references. Firstly, our revisit on the
round-trip translation in SMT evaluation unveils that its long-standing
misunderstanding is essentially caused by copying mechanism. After removing
copying mechanism in SMT, round-trip translation scores can appropriately
reflect the forward translation performance. Then, we demonstrate the
rectification is overdue as round-trip translation could benefit multiple
machine translation evaluation tasks. To be more specific, round-trip
translation could be used i) to predict corresponding forward translation
scores; ii) to improve the performance of the recently advanced quality
estimation model; and iii) to identify adversarial competitors in shared tasks
via cross-system verification.",https://github.com/UKPLab/EasyNMT,-1
What AI can do for horse-racing ?,0.0657076,"Since the 1980s, machine learning has been widely used for horse-racing
predictions, gradually expanding to where algorithms are now playing a huge
role in the betting market. Machine learning has changed the horse-racing
betting market over the last ten years, but main changes are still to come. The
paradigm shift of neural networks (deep learning) may not only improve our
ability to simply predict the outcome of a race, but it will also certainly
shake our entire way of thinking about horse-racing - and maybe more generally
about horses. Since 2012, deep learning provided more and more state-of-the-art
results in computer vision and now statistical learning or game theory. We
describe how the convergence of the three machine learning fields (computer
vision, statistical learning, and game theory) will be game-changers in the
next decade in our ability to predict and understand horse-racing. We consider
that horse-racing is a real world laboratory where we can work on the
animal-human interaction and build a non-anthropocentric Artificial
Intelligence. We believe that this will lead us to understand the horses better
and the interactions between animals and humans in general.",None,-1
VizInspect Pro -- Automated Optical Inspection (AOI) solution,0.0604405,"Traditional vision based Automated Optical Inspection (referred to as AOI in
paper) systems present multiple challenges in factory settings including
inability to scale across multiple product lines, requirement of vendor
programming expertise, little tolerance to variations and lack of cloud
connectivity for aggregated insights. The lack of flexibility in these systems
presents a unique opportunity for a deep learning based AOI system specifically
for factory automation. The proposed solution, VizInspect pro is a generic
computer vision based AOI solution built on top of Leo - An edge AI platform.
Innovative features that overcome challenges of traditional vision systems
include deep learning based image analysis which combines the power of
self-learning with high speed and accuracy, an intuitive user interface to
configure inspection profiles in minutes without ML or vision expertise and the
ability to solve complex inspection challenges while being tolerant to
deviations and unpredictable defects. This solution has been validated by
multiple external enterprise customers with confirmed value propositions. In
this paper we show you how this solution and platform solved problems around
model development, deployment, scaling multiple inferences and visualizations.",None,-1
Abstraction not Memory: BERT and the English Article System,0.127225,"Article prediction is a task that has long defied accurate linguistic
description. As such, this task is ideally suited to evaluate models on their
ability to emulate native-speaker intuition. To this end, we compare the
performance of native English speakers and pre-trained models on the task of
article prediction set up as a three way choice (a/an, the, zero). Our
experiments with BERT show that BERT outperforms humans on this task across all
articles. In particular, BERT is far superior to humans at detecting the zero
article, possibly because we insert them using rules that the deep neural model
can easily pick up. More interestingly, we find that BERT tends to agree more
with annotators than with the corpus when inter-annotator agreement is high but
switches to agreeing more with the corpus as inter-annotator agreement drops.
We contend that this alignment with annotators, despite being trained on the
corpus, suggests that BERT is not memorising article use, but captures a high
level generalisation of article use akin to human intuition.",https://github.com/H-TayyarMadabushi/Abstraction-not-Memory-BERT-and-the-English-Article-System-NAACL-2022,-1
SHREC 2022 Track on Online Detection of Heterogeneous Gestures,0.0820067,"This paper presents the outcomes of a contest organized to evaluate methods
for the online recognition of heterogeneous gestures from sequences of 3D hand
poses. The task is the detection of gestures belonging to a dictionary of 16
classes characterized by different pose and motion features. The dataset
features continuous sequences of hand tracking data where the gestures are
interleaved with non-significant motions. The data have been captured using the
Hololens 2 finger tracking system in a realistic use-case of mixed reality
interaction. The evaluation is based not only on the detection performances but
also on the latency and the false positives, making it possible to understand
the feasibility of practical interaction tools based on the algorithms
proposed. The outcomes of the contest's evaluation demonstrate the necessity of
further research to reduce recognition errors, while the computational cost of
the algorithms proposed is sufficiently low.",None,-1
Computational Metacognition,0.079913,"Computational metacognition represents a cognitive systems perspective on
high-order reasoning in integrated artificial systems that seeks to leverage
ideas from human metacognition and from metareasoning approaches in artificial
intelligence. The key characteristic is to declaratively represent and then
monitor traces of cognitive activity in an intelligent system in order to
manage the performance of cognition itself. Improvements in cognition then lead
to improvements in behavior and thus performance. We illustrate these concepts
with an agent implementation in a cognitive architecture called MIDCA and show
the value of metacognition in problem-solving. The results illustrate how
computational metacognition improves performance by changing cognition through
meta-level goal operations and learning.",https://github.com/COLAB2/midca,-1
WikiMulti: a Corpus for Cross-Lingual Summarization,0.0260199,"Cross-lingual summarization (CLS) is the task to produce a summary in one
particular language for a source document in a different language. We introduce
WikiMulti - a new dataset for cross-lingual summarization based on Wikipedia
articles in 15 languages. As a set of baselines for further studies, we
evaluate the performance of existing cross-lingual abstractive summarization
methods on our dataset. We make our dataset publicly available here:
https://github.com/tikhonovpavel/wikimulti",https://github.com/tikhonovpavel/wikimulti,-1
VRKitchen2.0-IndoorKit: A Tutorial for Augmented Indoor Scene Building in Omniverse,0.0250651,"With the recent progress of simulations by 3D modeling software and game
engines, many researchers have focused on Embodied AI tasks in the virtual
environment. However, the research community lacks a platform that can easily
serve both indoor scene synthesis and model benchmarking with various
algorithms. Meanwhile, computer graphics-related tasks need a toolkit for
implementing advanced synthesizing techniques. To facilitate the study of
indoor scene building methods and their potential robotics applications, we
introduce INDOORKIT: a built-in toolkit for NVIDIA OMNIVERSE that provides
flexible pipelines for indoor scene building, scene randomizing, and animation
controls. Besides, combining Python coding in the animation software INDOORKIT
assists researchers in creating real-time training and controlling avatars and
robotics. The source code for this toolkit is available at
https://github.com/realvcla/VRKitchen2.0-Tutorial, and the tutorial along with
the toolkit is available at https://vrkitchen20-tutorial.readthedocs.io/en/",https://github.com/realvcla/VRKitchen2.0-Tutorial,-1
Solving Bilevel Knapsack Problem using Graph Neural Networks,0.0258778,"The Bilevel Optimization Problem is a hierarchical optimization problem with
two agents, a leader and a follower. The leader make their own decisions first,
and the followers make the best choices accordingly. The leader knows the
information of the followers, and the goal of the problem is to find the
optimal solution by considering the reactions of the followers from the
leader's point of view. For the Bilevel Optimization Problem, there are no
general and efficient algorithms or commercial solvers to get an optimal
solution, and it is very difficult to get a good solution even for a simple
problem. In this paper, we propose a deep learning approach using Graph Neural
Networks to solve the bilevel knapsack problem. We train the model to predict
the leader's solution and use it to transform the hierarchical optimization
problem into a single-level optimization problem to get the solution. Our model
found the feasible solution that was about 500 times faster than the exact
algorithm with $1.7\%$ optimal gap. Also, our model performed well on problems
of different size from the size it was trained on.",None,-1
Enriching Abusive Language Detection with Community Context,0.00926749,"Uses of pejorative expressions can be benign or actively empowering. When
models for abuse detection misclassify these expressions as derogatory, they
inadvertently censor productive conversations held by marginalized groups. One
way to engage with non-dominant perspectives is to add context around
conversations. Previous research has leveraged user- and thread-level features,
but it often neglects the spaces within which productive conversations take
place. Our paper highlights how community context can improve classification
outcomes in abusive language detection. We make two main contributions to this
end. First, we demonstrate that online communities cluster by the nature of
their support towards victims of abuse. Second, we establish how community
context improves accuracy and reduces the false positive rates of
state-of-the-art abusive language classifiers. These findings suggest a
promising direction for context-aware models in abusive language research.",None,-1
Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning,0.0833668,"There is a rising interest in further exploring the zero-shot learning
potential of large pre-trained language models (PLMs). A new paradigm called
data-generation-based zero-shot learning has achieved impressive success. In
this paradigm, the synthesized data from the PLM acts as the carrier of
knowledge, which is used to train a task-specific model with orders of
magnitude fewer parameters than the PLM, achieving both higher performance and
efficiency than prompt-based zero-shot learning methods on PLMs. The main
hurdle of this approach is that the synthesized data from PLM usually contains
a significant portion of low-quality samples. Fitting on such data will greatly
hamper the performance of the task-specific model, making it unreliable for
deployment. Previous methods remedy this issue mainly by filtering synthetic
data using heuristic metrics(e.g., output confidence), or refining the data
with the help of a human expert, which comes with excessive manual tuning or
expensive costs. In this paper, we propose a novel noise-robust re-weighting
framework SunGen to automatically construct high-quality data for zero-shot
classification problems. Our framework features the ability to learn the sample
weights indicating data quality without requiring any human annotation. We
theoretically and empirically verify the ability of our method to help
construct good-quality synthetic datasets. Notably, SunGen-LSTM yields a 9.8%
relative improvement than the baseline on average accuracy across eight
different established text classification tasks.",None,-1
Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models,0.034561,"Relations between words are governed by hierarchical structure rather than
linear ordering. Sequence-to-sequence (seq2seq) models, despite their success
in downstream NLP applications, often fail to generalize in a
hierarchy-sensitive manner when performing syntactic transformations - for
example, transforming declarative sentences into questions. However, syntactic
evaluations of seq2seq models have only observed models that were not
pre-trained on natural language data before being trained to perform syntactic
transformations, in spite of the fact that pre-training has been found to
induce hierarchical linguistic generalizations in language models; in other
words, the syntactic capabilities of seq2seq models may have been greatly
understated. We address this gap using the pre-trained seq2seq models T5 and
BART, as well as their multilingual variants mT5 and mBART. We evaluate whether
they generalize hierarchically on two transformations in two languages:
question formation and passivization in English and German. We find that
pre-trained seq2seq models generalize hierarchically when performing syntactic
transformations, whereas models trained from scratch on syntactic
transformations do not. This result presents evidence for the learnability of
hierarchical syntactic information from non-annotated natural language text
while also demonstrating that seq2seq models are capable of syntactic
generalization, though only after exposure to much more language data than
human learners receive.",https://github.com/sebschu/multilingual-transformations,-1
The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory,0.0102106,"We describe the first steps in the development of an artificial agent focused
on the Brazilian maritime territory, a large region within the South Atlantic
also known as the Blue Amazon. The ""BLue Amazon Brain"" (BLAB) integrates a
number of services aimed at disseminating information about this region and its
importance, functioning as a tool for environmental awareness. The main service
provided by BLAB is a conversational facility that deals with complex questions
about the Blue Amazon, called BLAB-Chat; its central component is a controller
that manages several task-oriented natural language processing modules (e.g.,
question answering and summarizer systems). These modules have access to an
internal data lake as well as to third-party databases. A news reporter
(BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the
BLAB service architecture. In this paper, we describe our current version of
BLAB's architecture (interface, backend, web services, NLP modules, and
resources) and comment on the challenges we have faced so far, such as the lack
of training data and the scattered state of domain information. Solving these
issues presents a considerable challenge in the development of artificial
intelligence for technical domains.",https://github.com/C4AI,-1
CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction,0.0732,"Vision transformer (ViT) has achieved competitive accuracy on a variety of
computer vision applications, but its computational cost impedes the deployment
on resource-limited mobile devices.
  We explore the sparsity in ViT and observe that informative patches and heads
are sufficient for accurate image recognition.
  In this paper, we propose a cascade pruning framework named CP-ViT by
predicting sparsity in ViT models progressively and dynamically to reduce
computational redundancy while minimizing the accuracy loss. Specifically, we
define the cumulative score to reserve the informative patches and heads across
the ViT model for better accuracy. We also propose the dynamic pruning ratio
adjustment technique based on layer-aware attention range. CP-ViT has great
general applicability for practical deployment, which can be applied to a wide
range of ViT models and can achieve superior accuracy with or without
fine-tuning.
  Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various
pre-trained models have demonstrated the effectiveness and efficiency of
CP-ViT. By progressively pruning 50\% patches, our CP-ViT method reduces over
40\% FLOPs while maintaining accuracy loss within 1\%.",None,-1
An Overview of Distant Supervision for Relation Extraction with a Focus on Denoising and Pre-training Methods,0.0537049,"Relation Extraction (RE) is a foundational task of natural language
processing. RE seeks to transform raw, unstructured text into structured
knowledge by identifying relational information between entity pairs found in
text. RE has numerous uses, such as knowledge graph completion, text
summarization, question-answering, and search querying. The history of RE
methods can be roughly organized into four phases: pattern-based RE,
statistical-based RE, neural-based RE, and large language model-based RE. This
survey begins with an overview of a few exemplary works in the earlier phases
of RE, highlighting limitations and shortcomings to contextualize progress.
Next, we review popular benchmarks and critically examine metrics used to
assess RE performance. We then discuss distant supervision, a paradigm that has
shaped the development of modern RE methods. Lastly, we review recent RE works
focusing on denoising and pre-training methods.",None,-1
New wrapper method based on normalized mutual information for dimension reduction and classification of hyperspectral images,0.045995,"Feature selection is one of the most important problems in hyperspectral
images classification. It consists to choose the most informative bands from
the entire set of input datasets and discard the noisy, redundant and
irrelevant ones. In this context, we propose a new wrapper method based on
normalized mutual information (NMI) and error probability (PE) using support
vector machine (SVM) to reduce the dimensionality of the used hyperspectral
images and increase the classification efficiency. The experiments have been
performed on two challenging hyperspectral benchmarks datasets captured by the
NASA's Airborne Visible/Infrared Imaging Spectrometer Sensor (AVIRIS). Several
metrics had been calculated to evaluate the performance of the proposed
algorithm. The obtained results prove that our method can increase the
classification performance and provide an accurate thematic map in comparison
with other reproduced algorithms. This method may be improved for more
classification efficiency. Keywords-Feature selection, hyperspectral images,
classification, wrapper, normalized mutual information, support vector machine.",None,-1
Refining neural network predictions using background knowledge,0.0225432,"Recent work has shown logical background knowledge can be used in learning
systems to compensate for a lack of labeled training data. Many methods work by
creating a loss function that encodes this knowledge. However, often the logic
is discarded after training, even if it is still useful at test time. Instead,
we ensure neural network predictions satisfy the knowledge by refining the
predictions with an extra computation step. We introduce differentiable
refinement functions that find a corrected prediction close to the original
prediction. We study how to effectively and efficiently compute these
refinement functions. Using a new algorithm called Iterative Local Refinement
(ILR), we combine refinement functions to find refined predictions for logical
formulas of any complexity. ILR finds refinements on complex SAT formulas in
significantly fewer iterations and frequently finds solutions where gradient
descent can not. Finally, ILR produces competitive results in the MNIST
addition task.",https://github.com/DanieleAlessandro/IterativeLocalRefinement,-1
Learning crop type mapping from regional label proportions in large-scale SAR and optical imagery,0.0381785,"The application of deep learning algorithms to Earth observation (EO) in
recent years has enabled substantial progress in fields that rely on remotely
sensed data. However, given the data scale in EO, creating large datasets with
pixel-level annotations by experts is expensive and highly time-consuming. In
this context, priors are seen as an attractive way to alleviate the burden of
manual labeling when training deep learning methods for EO. For some
applications, those priors are readily available. Motivated by the great
success of contrastive-learning methods for self-supervised feature
representation learning in many computer-vision tasks, this study proposes an
online deep clustering method using crop label proportions as priors to learn a
sample-level classifier based on government crop-proportion data for a whole
agricultural region. We evaluate the method using two large datasets from two
different agricultural regions in Brazil. Extensive experiments demonstrate
that the method is robust to different data types (synthetic-aperture radar and
optical images), reporting higher accuracy values considering the major crop
types in the target regions. Thus, it can alleviate the burden of large-scale
image annotation in EO applications.",None,-1
End-to-End Speech to Intent Prediction to improve E-commerce Customer Support Voicebot in Hindi and English,0.0417622,"Automation of on-call customer support relies heavily on accurate and
efficient speech-to-intent (S2I) systems. Building such systems using
multi-component pipelines can pose various challenges because they require
large annotated datasets, have higher latency, and have complex deployment.
These pipelines are also prone to compounding errors. To overcome these
challenges, we discuss an end-to-end (E2E) S2I model for customer support
voicebot task in a bilingual setting. We show how we can solve E2E intent
classification by leveraging a pre-trained automatic speech recognition (ASR)
model with slight modification and fine-tuning on small annotated datasets.
Experimental results show that our best E2E model outperforms a conventional
pipeline by a relative ~27% on the F1 score.",None,-1
Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling,0.0279651,"Boundary information is critical for various Chinese language processing
tasks, such as word segmentation, part-of-speech tagging, and named entity
recognition. Previous studies usually resorted to the use of a high-quality
external lexicon, where lexicon items can offer explicit boundary information.
However, to ensure the quality of the lexicon, great human effort is always
necessary, which has been generally ignored. In this work, we suggest
unsupervised statistical boundary information instead, and propose an
architecture to encode the information directly into pre-trained language
models, resulting in Boundary-Aware BERT (BABERT). We apply BABERT for feature
induction of Chinese sequence labeling tasks. Experimental results on ten
benchmarks of Chinese sequence labeling demonstrate that BABERT can provide
consistent improvements on all datasets. In addition, our method can complement
previous supervised lexicon exploration, where further improvements can be
achieved when integrated with external lexicon information.",http://github.com/modelscope/adaseq/examples/babert,-1
Improving Generalization of Deep Neural Network Acoustic Models with Length Perturbation and N-best Based Label Smoothing,0.228815,"We introduce two techniques, length perturbation and n-best based label
smoothing, to improve generalization of deep neural network (DNN) acoustic
models for automatic speech recognition (ASR). Length perturbation is a data
augmentation algorithm that randomly drops and inserts frames of an utterance
to alter the length of the speech feature sequence. N-best based label
smoothing randomly injects noise to ground truth labels during training in
order to avoid overfitting, where the noisy labels are generated from n-best
hypotheses. We evaluate these two techniques extensively on the 300-hour
Switchboard (SWB300) dataset and an in-house 500-hour Japanese (JPN500) dataset
using recurrent neural network transducer (RNNT) acoustic models for ASR. We
show that both techniques improve the generalization of RNNT models
individually and they can also be complementary. In particular, they yield good
improvements over a strong SWB300 baseline and give state-of-art performance on
SWB300 using RNNT models.",None,-1
Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training,0.0369329,"In this paper, we present a cross-modal recipe retrieval framework,
Transformer-based Network for Large Batch Training (TNLBT), which is inspired
by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).
TNLBT aims to accomplish retrieval tasks while generating images from recipe
embeddings. We apply the Hierarchical Transformer-based recipe text encoder,
the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial
network architecture to enable better cross-modal embedding learning for recipe
texts and images. In addition, we use self-supervised learning to exploit the
rich information in the recipe texts having no corresponding images. Since
contrastive learning could benefit from a larger batch size according to the
recent literature on self-supervised learning, we adopt a large batch size
during training and have validated its effectiveness. In the experiments, the
proposed framework significantly outperformed the current state-of-the-art
frameworks in both cross-modal recipe retrieval and image generation tasks on
the benchmark Recipe1M. This is the first work which confirmed the
effectiveness of large batch training on cross-modal recipe embeddings.",https://github.com/mseitzer/pytorch-fid,-1
Ultra-high-resolution unpaired stain transformation via Kernelized Instance Normalization,0.0193312,"While hematoxylin and eosin (H&E) is a standard staining procedure,
immunohistochemistry (IHC) staining further serves as a diagnostic and
prognostic method. However, acquiring special staining results requires
substantial costs.
  Hence, we proposed a strategy for ultra-high-resolution unpaired
image-to-image translation: Kernelized Instance Normalization (KIN), which
preserves local information and successfully achieves seamless stain
transformation with constant GPU memory usage. Given a patch, corresponding
position, and a kernel, KIN computes local statistics using convolution
operation. In addition, KIN can be easily plugged into most currently developed
frameworks without re-training.
  We demonstrate that KIN achieves state-of-the-art stain transformation by
replacing instance normalization (IN) layers with KIN layers in three popular
frameworks and testing on two histopathological datasets. Furthermore, we
manifest the generalizability of KIN with high-resolution natural images.
Finally, human evaluation and several objective metrics are used to compare the
performance of different approaches.
  Overall, this is the first successful study for the ultra-high-resolution
unpaired image-to-image translation with constant space complexity. Code is
available at: https://github.com/Kaminyou/URUST",https://github.com/Kaminyou/URUST,-1
Dog nose print matching with dual global descriptor based on Contrastive Learning,0.0193363,"Recent studies in biometric-based identification tasks have shown that deep
learning methods can achieve better performance. These methods generally
extract the global features as descriptor to represent the original image.
Nonetheless, it does not perform well for biometric identification under
fine-grained tasks. The main reason is that the single image descriptor
contains insufficient information to represent image. In this paper, we present
a dual global descriptor model, which combines multiple global descriptors to
exploit multi level image features. Moreover, we utilize a contrastive loss to
enlarge the distance between image representations of confusing classes. The
proposed framework achieves the top2 on the CVPR2022 Biometrics Workshop Pet
Biometric Challenge. The source code and trained models are publicly available
at: https://github.com/flyingsheepbin/pet-biometrics",https://github.com/ﬂyingsheepbin/pet-biometrics,-1
Improving ProtoNet for Few-Shot Video Object Recognition: Winner of ORBIT Challenge 2022,0.334039,"In this work, we present the winning solution for ORBIT Few-Shot Video Object
Recognition Challenge 2022. Built upon the ProtoNet baseline, the performance
of our method is improved with three effective techniques. These techniques
include the embedding adaptation, the uniform video clip sampler and the
invalid frame detection. In addition, we re-factor and re-implement the
official codebase to encourage modularity, compatibility and improved
performance. Our implementation accelerates the data loading in both training
and testing.",None,-1
A Mixed Integer Programming Approach for Verifying Properties of Binarized Neural Networks,0.0370519,"Many approaches for verifying input-output properties of neural networks have
been proposed recently. However, existing algorithms do not scale well to large
networks. Recent work in the field of model compression studied binarized
neural networks (BNNs), whose parameters and activations are binary. BNNs tend
to exhibit a slight decrease in performance compared to their full-precision
counterparts, but they can be easier to verify. This paper proposes a simple
mixed integer programming formulation for BNN verification that leverages
network structure. We demonstrate our approach by verifying properties of BNNs
trained on the MNIST dataset and an aircraft collision avoidance controller. We
compare the runtime of our approach against state-of-the-art verification
algorithms for full-precision neural networks. The results suggest that the
difficulty of training BNNs might be worth the reduction in runtime achieved by
our verification algorithm.",None,-1
Lexical Generalization Improves with Larger Models and Longer Training,0.0470754,"While fine-tuned language models perform well on many tasks, they were also
shown to rely on superficial surface features such as lexical overlap.
Excessive utilization of such heuristics can lead to failure on challenging
inputs. We analyze the use of lexical overlap heuristics in natural language
inference, paraphrase detection, and reading comprehension (using a novel
contrastive dataset), and find that larger models are much less susceptible to
adopting lexical overlap heuristics. We also find that longer training leads
models to abandon lexical overlap heuristics. Finally, we provide evidence that
the disparity between models size has its source in the pre-trained model",https://github.com/elronbandel/lexical-generalization,-1
DeViT: Deformed Vision Transformers in Video Inpainting,0.0402173,"This paper proposes a novel video inpainting method. We make three main
contributions: First, we extended previous Transformers with patch alignment by
introducing Deformed Patch-based Homography (DePtH), which improves patch-level
feature alignments without additional supervision and benefits challenging
scenes with various deformation. Second, we introduce Mask Pruning-based Patch
Attention (MPPA) to improve patch-wised feature matching by pruning out less
essential features and using saliency map. MPPA enhances matching accuracy
between warped tokens with invalid pixels. Third, we introduce a
Spatial-Temporal weighting Adaptor (STA) module to obtain accurate attention to
spatial-temporal tokens under the guidance of the Deformation Factor learned
from DePtH, especially for videos with agile motions. Experimental results
demonstrate that our method outperforms recent methods qualitatively and
quantitatively and achieves a new state-of-the-art.",None,-1
Sampling Strategies for Static Powergrid Models,0.00173599,"Machine learning and computational intelligence technologies gain more and
more popularity as possible solution for issues related to the power grid. One
of these issues, the power flow calculation, is an iterative method to compute
the voltage magnitudes of the power grid's buses from power values. Machine
learning and, especially, artificial neural networks were successfully used as
surrogates for the power flow calculation. Artificial neural networks highly
rely on the quality and size of the training data, but this aspect of the
process is apparently often neglected in the works we found. However, since the
availability of high quality historical data for power grids is limited, we
propose the Correlation Sampling algorithm. We show that this approach is able
to cover a larger area of the sampling space compared to different random
sampling algorithms from the literature and a copula-based approach, while at
the same time inter-dependencies of the inputs are taken into account, which,
from the other algorithms, only the copula-based approach does.",None,-1
AutoMap: Automatic Medical Code Mapping for Clinical Prediction Model Deployment,0.0450244,"Given a deep learning model trained on data from a source site, how to deploy
the model to a target hospital automatically? How to accommodate heterogeneous
medical coding systems across different hospitals? Standard approaches rely on
existing medical code mapping tools, which have significant practical
limitations.
  To tackle this problem, we propose AutoMap to automatically map the medical
codes across different EHR systems in a coarse-to-fine manner: (1)
Ontology-level Alignment: We leverage the ontology structure to learn a coarse
alignment between the source and target medical coding systems; (2) Code-level
Refinement: We refine the alignment at a fine-grained code level for the
downstream tasks using a teacher-student framework.
  We evaluate AutoMap using several deep learning models with two real-world
EHR datasets: eICU and MIMIC-III. Results show that AutoMap achieves relative
improvements up to 3.9% (AUC-ROC) and 8.7% (AUC-PR) for mortality prediction,
and up to 4.7% (AUC-ROC) and 3.7% (F1) for length-of-stay estimation. Further,
we show that AutoMap can provide accurate mapping across coding systems.
Lastly, we demonstrate that AutoMap can adapt to the two challenging scenarios:
(1) mapping between completely different coding systems and (2) between
completely different hospitals.",None,-1
COVID-19-related Nepali Tweets Classification in a Low Resource Setting,0.0239332,"Billions of people across the globe have been using social media platforms in
their local languages to voice their opinions about the various topics related
to the COVID-19 pandemic. Several organizations, including the World Health
Organization, have developed automated social media analysis tools that
classify COVID-19-related tweets into various topics. However, these tools that
help combat the pandemic are limited to very few languages, making several
countries unable to take their benefit. While multi-lingual or low-resource
language-specific tools are being developed, they still need to expand their
coverage, such as for the Nepali language. In this paper, we identify the eight
most common COVID-19 discussion topics among the Twitter community using the
Nepali language, set up an online platform to automatically gather Nepali
tweets containing the COVID-19-related keywords, classify the tweets into the
eight topics, and visualize the results across the period in a web-based
dashboard. We compare the performance of two state-of-the-art multi-lingual
language models for Nepali tweet classification, one generic (mBERT) and the
other Nepali language family-specific model (MuRIL). Our results show that the
models' relative performance depends on the data size, with MuRIL doing better
for a larger dataset. The annotated data, models, and the web-based dashboard
are open-sourced at https://github.com/naamiinepal/covid-tweet-classification.",https://github.com/naamiinepal/covid-tweet-classification,-1
Context based lemmatizer for Polish language,0.0207285,"Lemmatization is the process of grouping together the inflected forms of a
word so they can be analysed as a single item, identified by the word's lemma,
or dictionary form. In computational linguistics, lemmatisation is the
algorithmic process of determining the lemma of a word based on its intended
meaning. Unlike stemming, lemmatisation depends on correctly identifying the
intended part of speech and meaning of a word in a sentence, as well as within
the larger context surrounding that sentence. As a result, developing efficient
lemmatisation algorithm is the complex task. In recent years it can be observed
that deep learning models used for this task outperform other methods including
machine learning algorithms. In this paper the polish lemmatizer based on
Google T5 model is presented. The training was run with different context
lengths. The model achieves the best results for polish language lemmatisation
process.",None,-1
Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives,0.046665,"In this paper, we investigate the instability in the standard dense retrieval
training, which iterates between model training and hard negative selection
using the being-trained model. We show the catastrophic forgetting phenomena
behind the training instability, where models learn and forget different
negative groups during training iterations. We then propose ANCE-Tele, which
accumulates momentum negatives from past iterations and approximates future
iterations using lookahead negatives, as ""teleportations"" along the time axis
to smooth the learning process. On web search and OpenQA, ANCE-Tele outperforms
previous state-of-the-art systems of similar size, eliminates the dependency on
sparse retrieval negatives, and is competitive among systems using
significantly more (50x) parameters. Our analysis demonstrates that
teleportation negatives reduce catastrophic forgetting and improve convergence
speed for dense retrieval training. Our code is available at
https://github.com/OpenMatch/ANCE-Tele.",https://github.com/OpenMatch/ANCE-Tele,-1
Unsupervised Non-transferable Text Classification,0.0192624,"Training a good deep learning model requires substantial data and computing
resources, which makes the resulting neural model a valuable intellectual
property. To prevent the neural network from being undesirably exploited,
non-transferable learning has been proposed to reduce the model generalization
ability in specific target domains. However, existing approaches require
labeled data for the target domain which can be difficult to obtain.
Furthermore, they do not have the mechanism to still recover the model's
ability to access the target domain. In this paper, we propose a novel
unsupervised non-transferable learning method for the text classification task
that does not require annotated target domain data. We further introduce a
secret key component in our approach for recovering the access to the target
domain, where we design both an explicit and an implicit method for doing so.
Extensive experiments demonstrate the effectiveness of our approach.",https://github.com/ChaosCodes/UNTL,-1
Detecting Context-Aware Deviations in Process Executions,0.0196007,"A deviation detection aims to detect deviating process instances, e.g.,
patients in the healthcare process and products in the manufacturing process. A
business process of an organization is executed in various contextual
situations, e.g., a COVID-19 pandemic in the case of hospitals and a lack of
semiconductor chip shortage in the case of automobile companies. Thus,
context-aware deviation detection is essential to provide relevant insights.
However, existing work 1) does not provide a systematic way of incorporating
various contexts, 2) is tailored to a specific approach without using an
extensive pool of existing deviation detection techniques, and 3) does not
distinguish positive and negative contexts that justify and refute deviation,
respectively. In this work, we provide a framework to bridge the aforementioned
gaps. We have implemented the proposed framework as a web service that can be
extended to various contexts and deviation detection methods. We have evaluated
the effectiveness of the proposed framework by conducting experiments using 255
different contextual scenarios.",https://github.com/janikbenzin/contect,-1
A Flexible Schema-Guided Dialogue Management Framework: From Friendly Peer to Virtual Standardized Cancer Patient,0.0588592,"A schema-guided approach to dialogue management has been shown in recent work
to be effective in creating robust customizable virtual agents capable of
acting as friendly peers or task assistants. However, successful applications
of these methods in open-ended, mixed-initiative domains remain elusive --
particularly within medical domains such as virtual standardized patients,
where such complex interactions are commonplace -- and require more extensive
and flexible dialogue management capabilities than previous systems provide. In
this paper, we describe a general-purpose schema-guided dialogue management
framework used to develop SOPHIE, a virtual standardized cancer patient that
allows a doctor to conveniently practice for interactions with patients. We
conduct a crowdsourced evaluation of conversations between medical students and
SOPHIE. Our agent is judged to produce responses that are natural, emotionally
appropriate, and consistent with her role as a cancer patient. Furthermore, it
significantly outperforms an end-to-end neural model fine-tuned on a human
standardized patient corpus, attesting to the advantages of a schema-guided
approach.",None,-1
ViT-CX: Causal Explanation of Vision Transformers,0.0532335,"Despite the popularity of Vision Transformers (ViTs) and eXplainable AI
(XAI), only a few explanation methods have been designed specially for ViTs
thus far. They mostly use attention weights of the [CLS] token on patch
embeddings and often produce unsatisfactory saliency maps. This paper proposes
a novel method for explaining ViTs called ViT-CX. It is based on patch
embeddings, rather than attentions paid to them, and their causal impacts on
the model output. Other characteristics of ViTs such as causal
overdetermination are also considered in the design of ViT-CX. The empirical
results show that ViT-CX produces more meaningful saliency maps and does a
better job revealing all important evidence for the predictions than previous
methods. The explanation generated by ViT-CX also shows significantly better
faithfulness to the model. The codes and appendix are available at
https://github.com/vaynexie/CausalX-ViT.",https://github.com/vaynexie/CausalX-ViT,-1
AbductionRules: Training Transformers to Explain Unexpected Inputs,0.281833,"Transformers have recently been shown to be capable of reliably performing
logical reasoning over facts and rules expressed in natural language, but
abductive reasoning - inference to the best explanation of an unexpected
observation - has been underexplored despite significant applications to
scientific discovery, common-sense reasoning, and model interpretability.
  We present AbductionRules, a group of natural language datasets designed to
train and test generalisable abduction over natural-language knowledge bases.
We use these datasets to finetune pretrained Transformers and discuss their
performance, finding that our models learned generalisable abductive techniques
but also learned to exploit the structure of our data. Finally, we discuss the
viability of this approach to abductive reasoning and ways in which it may be
improved in future work.",https://github.com/Strong-AI-Lab/AbductionRules/,-1
Code Switched and Code Mixed Speech Recognition for Indic languages,0.0812894,"Training multilingual automatic speech recognition (ASR) systems is
challenging because acoustic and lexical information is typically language
specific. Training multilingual system for Indic languages is even more tougher
due to lack of open source datasets and results on different approaches. We
compare the performance of end to end multilingual speech recognition system to
the performance of monolingual models conditioned on language identification
(LID). The decoding information from a multilingual model is used for language
identification and then combined with monolingual models to get an improvement
of 50% WER across languages. We also propose a similar technique to solve the
Code Switched problem and achieve a WER of 21.77 and 28.27 over Hindi-English
and Bengali-English respectively. Our work talks on how transformer based ASR
especially wav2vec 2.0 can be applied in developing multilingual ASR and code
switched ASR for Indic languages.",https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation,-1
A Realism Metric for Generated LiDAR Point Clouds,0.0318383,"A considerable amount of research is concerned with the generation of
realistic sensor data. LiDAR point clouds are generated by complex simulations
or learned generative models. The generated data is usually exploited to enable
or improve downstream perception algorithms. Two major questions arise from
these procedures: First, how to evaluate the realism of the generated data?
Second, does more realistic data also lead to better perception performance?
This paper addresses both questions and presents a novel metric to quantify the
realism of LiDAR point clouds. Relevant features are learned from real-world
and synthetic point clouds by training on a proxy classification task. In a
series of experiments, we demonstrate the application of our metric to
determine the realism of generated LiDAR data and compare the realism
estimation of our metric to the performance of a segmentation model. We confirm
that our metric provides an indication for the downstream segmentation
performance.",https://github.com/PRBonn/lidar-bonnetal,-1
Improved Touchless Respiratory Rate Sensing,0.0475431,"Recently, remote respiratory rate measurement techniques gained much
attention as they were developed to overcome the limitations of device-based
classical methods and manual counting. Many approaches for RR extraction from
the video stream of the visible light camera were proposed, including the pixel
intensity changes method. In this paper, we propose a new method for 1D profile
creation for pixel intensity changes-based method, which significantly
increases the algorithm's performance. Additional accuracy gain is obtained via
a new method of motion signals grouping presented in this work. We introduce
several changes to the standard pipeline, which enables real-time continuous RR
monitoring and allows applications in the human-computer interaction systems.
Evaluation results on two internal and one public datasets showed 0.7 BPM, 0.6
BPM, and 1.4 BPM MAE, respectively.",None,-1
"Movement Analytics: Current Status, Application to Manufacturing, and Future Prospects from an AI Perspective",0.042057,"Data-driven decision making is becoming an integral part of manufacturing
companies. Data is collected and commonly used to improve efficiency and
produce high quality items for the customers. IoT-based and other forms of
object tracking are an emerging tool for collecting movement data of
objects/entities (e.g. human workers, moving vehicles, trolleys etc.) over
space and time. Movement data can provide valuable insights like process
bottlenecks, resource utilization, effective working time etc. that can be used
for decision making and improving efficiency.
  Turning movement data into valuable information for industrial management and
decision making requires analysis methods. We refer to this process as movement
analytics. The purpose of this document is to review the current state of work
for movement analytics both in manufacturing and more broadly.
  We survey relevant work from both a theoretical perspective and an
application perspective. From the theoretical perspective, we put an emphasis
on useful methods from two research areas: machine learning, and logic-based
knowledge representation. We also review their combinations in view of movement
analytics, and we discuss promising areas for future development and
application. Furthermore, we touch on constraint optimization.
  From an application perspective, we review applications of these methods to
movement analytics in a general sense and across various industries. We also
describe currently available commercial off-the-shelf products for tracking in
manufacturing, and we overview main concepts of digital twins and their
applications.",None,-1
ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual Simultaneous Localization and Mapping,0.0970104,"In this paper, a novel solution is introduced for visual Simultaneous
Localization and Mapping (vSLAM) that is built up of Deep Learning components.
The proposed architecture is a highly modular framework in which each component
offers state of the art results in their respective fields of vision-based deep
learning solutions. The paper shows that with the synergic integration of these
individual building blocks, a functioning and efficient all-through deep neural
(ATDN) vSLAM system can be created. The Embedding Distance Loss function is
introduced and using it the ATDN architecture is trained. The resulting system
managed to achieve 4.4% translation and 0.0176 deg/m rotational error on a
subset of the KITTI dataset. The proposed architecture can be used for
efficient and low-latency autonomous driving (AD) aiding database creation as
well as a basis for autonomous vehicle (AV) control.",None,-1
Anytime Capacity Expansion in Medical Residency Match by Monte Carlo Tree Search,0.103709,"This paper considers the capacity expansion problem in two-sided matchings,
where the policymaker is allowed to allocate some extra seats as well as the
standard seats. In medical residency match, each hospital accepts a limited
number of doctors. Such capacity constraints are typically given in advance.
However, such exogenous constraints can compromise the welfare of the doctors;
some popular hospitals inevitably dismiss some of their favorite doctors.
Meanwhile, it is often the case that the hospitals are also benefited to accept
a few extra doctors. To tackle the problem, we propose an anytime method that
the upper confidence tree searches the space of capacity expansions, each of
which has a resident-optimal stable assignment that the deferred acceptance
method finds. Constructing a good search tree representation significantly
boosts the performance of the proposed method. Our simulation shows that the
proposed method identifies an almost optimal capacity expansion with a
significantly smaller computational budget than exact methods based on
mixed-integer programming.",https://github.com/CyberAgentAILab/uct-capacity-expansion,-1
Semi-Supervised Imitation Learning of Team Policies from Suboptimal Demonstrations,0.0127056,"We present Bayesian Team Imitation Learner (BTIL), an imitation learning
algorithm to model the behavior of teams performing sequential tasks in
Markovian domains. In contrast to existing multi-agent imitation learning
techniques, BTIL explicitly models and infers the time-varying mental states of
team members, thereby enabling learning of decentralized team policies from
demonstrations of suboptimal teamwork. Further, to allow for sample- and
label-efficient policy learning from small datasets, BTIL employs a Bayesian
perspective and is capable of learning from semi-supervised demonstrations. We
demonstrate and benchmark the performance of BTIL on synthetic multi-agent
tasks as well as a novel dataset of human-agent teamwork. Our experiments show
that BTIL can successfully learn team policies from demonstrations despite the
influence of team members' (time-varying and potentially misaligned) mental
states on their behavior.",None,-1
Self-Supervised Pre-training of 3D Point Cloud Networks with Image Data,0.122716,"Reducing the quantity of annotations required for supervised training is
vital when labels are scarce and costly. This reduction is especially important
for semantic segmentation tasks involving 3D datasets that are often
significantly smaller and more challenging to annotate than their image-based
counterparts. Self-supervised pre-training on large unlabelled datasets is one
way to reduce the amount of manual annotations needed. Previous work has
focused on pre-training with point cloud data exclusively; this approach often
requires two or more registered views. In the present work, we combine image
and point cloud modalities, by first learning self-supervised image features
and then using these features to train a 3D model. By incorporating image data,
which is often included in many 3D datasets, our pre-training method only
requires a single scan of a scene. We demonstrate that our pre-training
approach, despite using single scans, achieves comparable performance to other
multi-scan, point cloud-only methods.",None,-1
Handling sign language transcription system with the computer-friendly numerical multilabels,0.0176294,"This paper presents our recent developments in the automatic processing of
sign language corpora using the Hamburg Sign Language Annotation System
(HamNoSys). We designed an automated tool to convert HamNoSys annotations into
numerical labels for defined initial features of body and hand positions. Our
proposed numerical multilabels greatly simplify annotations' structure without
significant loss of gloss meaning. These numerical multilabels can potentially
be used to feed the machine learning models, which would accelerate the
development of vision-based sign language recognition. In addition, this tool
can assist experts in the annotation process and help identify semantic errors.
The code and sample annotations are publicly available at
\url{https://github.com/hearai/parse-hamnosys}.",https://github.com/hearai/parse-hamnosys,-1
Best of Both Worlds Model Selection,0.00739823,"We study the problem of model selection in bandit scenarios in the presence
of nested policy classes, with the goal of obtaining simultaneous adversarial
and stochastic (""best of both worlds"") high-probability regret guarantees. Our
approach requires that each base learner comes with a candidate regret bound
that may or may not hold, while our meta algorithm plays each base learner
according to a schedule that keeps the base learner's candidate regret bounds
balanced until they are detected to violate their guarantees. We develop
careful mis-specification tests specifically designed to blend the above model
selection criterion with the ability to leverage the (potentially benign)
nature of the environment. We recover the model selection guarantees of the
CORRAL algorithm for adversarial environments, but with the additional benefit
of achieving high probability regret bounds, specifically in the case of nested
adversarial linear bandits. More importantly, our model selection results also
hold simultaneously in stochastic environments under gap assumptions. These are
the first theoretical results that achieve best of both world (stochastic and
adversarial) guarantees while performing model selection in (linear) bandit
scenarios.",None,-1
Hybrid Indoor Localization via Reinforcement Learning-based Information Fusion,0.02017,"The paper is motivated by the importance of the Smart Cities (SC) concept for
future management of global urbanization. Among all Internet of Things
(IoT)-based communication technologies, Bluetooth Low Energy (BLE) plays a
vital role in city-wide decision making and services. Extreme fluctuations of
the Received Signal Strength Indicator (RSSI), however, prevent this technology
from being a reliable solution with acceptable accuracy in the dynamic indoor
tracking/localization approaches for ever-changing SC environments. The latest
version of the BLE v.5.1 introduced a better possibility for tracking users by
utilizing the direction finding approaches based on the Angle of Arrival (AoA),
which is more reliable. There are still some fundamental issues remaining to be
addressed. Existing works mainly focus on implementing stand-alone models
overlooking potentials fusion strategies. The paper addresses this gap and
proposes a novel Reinforcement Learning (RL)-based information fusion framework
(RL-IFF) by coupling AoA with RSSI-based particle filtering and Inertial
Measurement Unit (IMU)-based Pedestrian Dead Reckoning (PDR) frameworks. The
proposed RL-IFF solution is evaluated through a comprehensive set of
experiments illustrating superior performance compared to its counterparts.",None,-1
Modeling Context With Linear Attention for Scalable Document-Level Translation,0.0851928,"Document-level machine translation leverages inter-sentence dependencies to
produce more coherent and consistent translations. However, these models,
predominantly based on transformers, are difficult to scale to long documents
as their attention layers have quadratic complexity in the sequence length.
Recent efforts on efficient attention improve scalability, but their effect on
document translation remains unexplored. In this work, we investigate the
efficacy of a recent linear attention model by Peng et al. (2021) on document
translation and augment it with a sentential gate to promote a recency
inductive bias. We evaluate the model on IWSLT 2015 and OpenSubtitles 2018
against the transformer, demonstrating substantially increased decoding speed
on long sequences with similar or better BLEU scores. We show that sentential
gating further improves translation quality on IWSLT.",https://github.com/ZhaofengWu/rfa-doc-mt,-1
On-device Synaptic Memory Consolidation using Fowler-Nordheim Quantum-tunneling,0.154869,"Synaptic memory consolidation has been heralded as one of the key mechanisms
for supporting continual learning in neuromorphic Artificial Intelligence (AI)
systems. Here we report that a Fowler-Nordheim (FN) quantum-tunneling device
can implement synaptic memory consolidation similar to what can be achieved by
algorithmic consolidation models like the cascade and the elastic weight
consolidation (EWC) models. The proposed FN-synapse not only stores the
synaptic weight but also stores the synapse's historical usage statistic on the
device itself. We also show that the operation of the FN-synapse is
near-optimal in terms of the synaptic lifetime and we demonstrate that a
network comprising FN-synapses outperforms a comparable EWC network for a small
benchmark continual learning task. With an energy footprint of femtojoules per
synaptic update, we believe that the proposed FN-synapse provides an
ultra-energy-efficient approach for implementing both synaptic memory
consolidation and persistent learning.",None,-1
skrl: Modular and Flexible Library for Reinforcement Learning,0.0156529,"skrl is an open-source modular library for reinforcement learning written in
Python and designed with a focus on readability, simplicity, and transparency
of algorithm implementations. In addition to supporting environments that use
the traditional interfaces from OpenAI Gym and DeepMind, it provides the
facility to load, configure, and operate NVIDIA Isaac Gym and NVIDIA Omniverse
Isaac Gym environments. Furthermore, it enables the simultaneous training of
several agents with customizable scopes (subsets of environments among all
available ones), which may or may not share resources, in the same run. The
library's documentation can be found at https://skrl.readthedocs.io and its
source code is available on GitHub at https://github.com/Toni-SM/skrl.",https://github.com/Toni-SM/skrl,-1
Graph-Based Active Machine Learning Method for Diverse and Novel Antimicrobial Peptides Generation and Selection,0.00994779,"As antibiotic-resistant bacterial strains are rapidly spreading worldwide,
infections caused by these strains are emerging as a global crisis causing the
death of millions of people every year. Antimicrobial Peptides (AMPs) are one
of the candidates to tackle this problem because of their potential diversity,
and ability to favorably modulate the host immune response. However,
large-scale screening of new AMP candidates is expensive, time-consuming, and
now affordable in developing countries, which need the treatments the most. In
this work, we propose a novel active machine learning-based framework that
statistically minimizes the number of wet-lab experiments needed to design new
AMPs, while ensuring a high diversity and novelty of generated AMPs sequences,
in multi-rounds of wet-lab AMP screening settings. Combining recurrent neural
network models and a graph-based filter (GraphCC), our proposed approach
delivers novel and diverse candidates and demonstrates better performances
according to our defined metrics.",None,-1
Can Foundation Models Talk Causality?,0.030088,"Foundation models are subject to an ongoing heated debate, leaving open the
question of progress towards AGI and dividing the community into two camps: the
ones who see the arguably impressive results as evidence to the scaling
hypothesis, and the others who are worried about the lack of interpretability
and reasoning capabilities. By investigating to which extent causal
representations might be captured by these large scale language models, we make
a humble efforts towards resolving the ongoing philosophical conflicts.",https://github.com/MoritzWillig/causalFM,-1
Network Pruning via Feature Shift Minimization,0.0100905,"Channel pruning is widely used to reduce the complexity of deep network
models. Recent pruning methods usually identify which parts of the network to
discard by proposing a channel importance criterion. However, recent studies
have shown that these criteria do not work well in all conditions. In this
paper, we propose a novel Feature Shift Minimization (FSM) method to compress
CNN models, which evaluates the feature shift by converging the information of
both features and filters. Specifically, we first investigate the compression
efficiency with some prevalent methods in different layer-depths and then
propose the feature shift concept. Then, we introduce an approximation method
to estimate the magnitude of the feature shift, since it is difficult to
compute it directly. Besides, we present a distribution-optimization algorithm
to compensate for the accuracy loss and improve the network compression
efficiency. The proposed method yields state-of-the-art performance on various
benchmark networks and datasets, verified by extensive experiments. Our codes
are available at: https://github.com/lscgx/FSM.",https://github.com/lscgx/FSM,-1
VS-CAM: Vertex Semantic Class Activation Mapping to Interpret Vision Graph Neural Network,0.425036,"Graph convolutional neural network (GCN) has drawn increasing attention and
attained good performance in various computer vision tasks, however, there
lacks a clear interpretation of GCN's inner mechanism. For standard
convolutional neural networks (CNNs), class activation mapping (CAM) methods
are commonly used to visualize the connection between CNN's decision and image
region by generating a heatmap. Nonetheless, such heatmap usually exhibits
semantic-chaos when these CAMs are applied to GCN directly. In this paper, we
proposed a novel visualization method particularly applicable to GCN, Vertex
Semantic Class Activation Mapping (VS-CAM). VS-CAM includes two independent
pipelines to produce a set of semantic-probe maps and a semantic-base map,
respectively. Semantic-probe maps are used to detect the semantic information
from semantic-base map to aggregate a semantic-aware heatmap. Qualitative
results show that VS-CAM can obtain heatmaps where the highlighted regions
match the objects much more precisely than CNN-based CAM. The quantitative
evaluation further demonstrates the superiority of VS-CAM.",None,-1
Semantic Segmentation-Assisted Instance Feature Fusion for Multi-Level 3D Part Instance Segmentation,0.00595751,"Recognizing 3D part instances from a 3D point cloud is crucial for 3D
structure and scene understanding. Several learning-based approaches use
semantic segmentation and instance center prediction as training tasks and fail
to further exploit the inherent relationship between shape semantics and part
instances. In this paper, we present a new method for 3D part instance
segmentation. Our method exploits semantic segmentation to fuse nonlocal
instance features, such as center prediction, and further enhances the fusion
scheme in a multi- and cross-level way. We also propose a semantic region
center prediction task to train and leverage the prediction results to improve
the clustering of instance points. Our method outperforms existing methods with
a large-margin improvement in the PartNet benchmark. We also demonstrate that
our feature fusion scheme can be applied to other existing methods to improve
their performance in indoor scene instance segmentation tasks.",https://isunchy.github.io/projects/3d_instance_segmentation.html,-1
Instance-based Learning for Knowledge Base Completion,0.0620527,"In this paper, we propose a new method for knowledge base completion (KBC):
instance-based learning (IBL). For example, to answer (Jill Biden, lived city,?
), instead of going directly to Washington D.C., our goal is to find Joe Biden,
who has the same lived city as Jill Biden. Through prototype entities, IBL
provides interpretability. We develop theories for modeling prototypes and
combining IBL with translational models. Experiments on various tasks confirmed
the IBL model's effectiveness and interpretability.
  In addition, IBL shed light on the mechanism of rule-based KBC models.
Previous research has generally agreed that rule-based models provide rules
with semantically compatible premises and hypotheses. We challenge this view.
We begin by demonstrating that some logical rules represent {\it instance-based
equivalence} (i.e. prototypes) rather than semantic compatibility. These are
denoted as {\it IBL rules}. Surprisingly, despite occupying only a small
portion of the rule space, IBL rules outperform non-IBL rules in all four
benchmarks. We use a variety of experiments to demonstrate that rule-based
models work because they have the ability to represent instance-based
equivalence via IBL rules. The findings provide new insights of how rule-based
models work and how to interpret their rules.",https://github.com/chenxran/InstanceBasedLearning,-1
Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection,0.289019,"Over the past few years there has been major progress in the field of
synthetic data generation using simulation based techniques. These methods use
high-end graphics engines and physics-based ray-tracing rendering in order to
represent the world in 3D and create highly realistic images. Datagen has
specialized in the generation of high-quality 3D humans, realistic 3D
environments and generation of realistic human motion. This technology has been
developed into a data generation platform which we used for these experiments.
This work demonstrates the use of synthetic photo-realistic in-cabin data to
train a Driver Monitoring System that uses a lightweight neural network to
detect whether the driver's hands are on the wheel. We demonstrate that when
only a small amount of real data is available, synthetic data can be a simple
way to boost performance. Moreover, we adopt the data-centric approach and show
how performing error analysis and generating the missing edge-cases in our
platform boosts performance. This showcases the ability of human-centric
synthetic data to generalize well to the real world, and help train algorithms
in computer vision settings where data from the target domain is scarce or hard
to collect.",None,-1
Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture,0.150194,"Person name capture from human speech is a difficult task in human-machine
conversations. In this paper, we propose a novel approach to capture the person
names from the caller utterances in response to the prompt ""say and spell your
first/last name"". Inspired from work on spell correction, disfluency removal
and text normalization, we propose a lightweight Seq-2-Seq system which
generates a name spell from a varying user input. Our proposed method
outperforms the strong baseline which is based on LM-driven rule-based
approach.",None,-1
Towards Efficient Neural Scene Graphs by Learning Consistency Fields,0.0,"Neural Radiance Fields (NeRF) achieves photo-realistic image rendering from
novel views, and the Neural Scene Graphs (NSG) \cite{ost2021neural} extends it
to dynamic scenes (video) with multiple objects. Nevertheless, computationally
heavy ray marching for every image frame becomes a huge burden. In this paper,
taking advantage of significant redundancy across adjacent frames in videos, we
propose a feature-reusing framework. From the first try of naively reusing the
NSG features, however, we learn that it is crucial to disentangle
object-intrinsic properties consistent across frames from transient ones. Our
proposed method, \textit{Consistency-Field-based NSG (CF-NSG)}, reformulates
neural radiance fields to additionally consider \textit{consistency fields}.
With disentangled representations, CF-NSG takes full advantage of the
feature-reusing scheme and performs an extended degree of scene manipulation in
a more controllable manner. We empirically verify that CF-NSG greatly improves
the inference efficiency by using 85\% less queries than NSG without notable
degradation in rendering quality. Code will be available at:
https://github.com/ldynx/CF-NSG",https://github.com/ldynx/CF-NSG,-1
Graph Neural Network Bandits,0.0364242,"We consider the bandit optimization problem with the reward function defined
over graph-structured data. This problem has important applications in molecule
design and drug discovery, where the reward is naturally invariant to graph
permutations. The key challenges in this setting are scaling to large domains,
and to graphs with many nodes. We resolve these challenges by embedding the
permutation invariance into our model. In particular, we show that graph neural
networks (GNNs) can be used to estimate the reward function, assuming it
resides in the Reproducing Kernel Hilbert Space of a permutation-invariant
additive kernel. By establishing a novel connection between such kernels and
the graph neural tangent kernel (GNTK), we introduce the first GNN confidence
bound and use it to design a phased-elimination algorithm with sublinear
regret. Our regret bound depends on the GNTK's maximum information gain, which
we also provide a bound for. While the reward function depends on all $N$ node
features, our guarantees are independent of the number of graph nodes $N$.
Empirically, our approach exhibits competitive performance and scales well on
graph-structured domains.",None,-1
Self-supervised 3D Semantic Representation Learning for Vision-and-Language Navigation,0.124128,"In the Vision-and-Language Navigation task, the embodied agent follows
linguistic instructions and navigates to a specific goal. It is important in
many practical scenarios and has attracted extensive attention from both
computer vision and robotics communities. However, most existing works only use
RGB images but neglect the 3D semantic information of the scene. To this end,
we develop a novel self-supervised training framework to encode the voxel-level
3D semantic reconstruction into a 3D semantic representation. Specifically, a
region query task is designed as the pretext task, which predicts the presence
or absence of objects of a particular class in a specific 3D region. Then, we
construct an LSTM-based navigation model and train it with the proposed 3D
semantic representations and BERT language features on vision-language pairs.
Experiments show that the proposed approach achieves success rates of 68% and
66% on the validation unseen and test unseen splits of the R2R dataset
respectively, which are superior to most of RGB-based methods utilizing
vision-language transformers.",None,-1
Deep Deformable 3D Caricatures with Learned Shape Control,0.107719,"A 3D caricature is an exaggerated 3D depiction of a human face. The goal of
this paper is to model the variations of 3D caricatures in a compact parameter
space so that we can provide a useful data-driven toolkit for handling 3D
caricature deformations. To achieve the goal, we propose an MLP-based framework
for building a deformable surface model, which takes a latent code and produces
a 3D surface. In the framework, a SIREN MLP models a function that takes a 3D
position on a fixed template surface and returns a 3D displacement vector for
the input position. We create variations of 3D surfaces by learning a
hypernetwork that takes a latent code and produces the parameters of the MLP.
Once learned, our deformable model provides a nice editing space for 3D
caricatures, supporting label-based semantic editing and point-handle-based
deformation, both of which produce highly exaggerated and natural 3D caricature
shapes. We also demonstrate other applications of our deformable model, such as
automatic 3D caricature creation.",https://github.com/ycjungSubhuman/DeepDeformable3DCaricatures,-1
Augment with Care: Contrastive Learning for Combinatorial Problems,0.103755,"Supervised learning can improve the design of state-of-the-art solvers for
combinatorial problems, but labelling large numbers of combinatorial instances
is often impractical due to exponential worst-case complexity. Inspired by the
recent success of contrastive pre-training for images, we conduct a scientific
study of the effect of augmentation design on contrastive pre-training for the
Boolean satisfiability problem. While typical graph contrastive pre-training
uses label-agnostic augmentations, our key insight is that many combinatorial
problems have well-studied invariances, which allow for the design of
label-preserving augmentations. We find that label-preserving augmentations are
critical for the success of contrastive pre-training. We show that our
representations are able to achieve comparable test accuracy to
fully-supervised learning while using only 1% of the labels. We also
demonstrate that our representations are more transferable to larger problems
from unseen domains. Our code is available at
https://github.com/h4duan/contrastive-sat.",https://github.com/h4duan/contrastive-sat,-1
Responsible AI Implementation: A Human-centered Framework for Accelerating the Innovation Process,0.124775,"There is still a significant gap between expectations and the successful
adoption of AI to innovate and improve businesses. Due to the emergence of deep
learning, AI adoption is more complex as it often incorporates big data and the
internet of things, affecting data privacy. Existing frameworks have identified
the need to focus on human-centered design, combining technical and
business/organizational perspectives. However, trust remains a critical issue
that needs to be designed from the beginning. The proposed framework expands
from the human-centered design approach, emphasizing and maintaining the trust
that underpins the process. This paper proposes a theoretical framework for
responsible artificial intelligence (AI) implementation. The proposed framework
emphasizes a synergistic business technology approach for the agile co-creation
process. The aim is to streamline the adoption process of AI to innovate and
improve business by involving all stakeholders throughout the project so that
the AI technology is designed, developed, and deployed in conjunction with
people and not in isolation. The framework presents a fresh viewpoint on
responsible AI implementation based on analytical literature review, conceptual
framework design, and practitioners' mediating expertise. The framework
emphasizes establishing and maintaining trust throughout the human-centered
design and agile development of AI. This human-centered approach is aligned
with and enabled by the privacy by design principle. The creators of the
technology and the end-users are working together to tailor the AI solution
specifically for the business requirements and human characteristics. An
illustrative case study on adopting AI for assisting planning in a hospital
will demonstrate that the proposed framework applies to real-life applications.",None,-1
Detecting the Role of an Entity in Harmful Memes: Techniques and Their Limitations,0.0579402,"Harmful or abusive online content has been increasing over time, raising
concerns for social media platforms, government agencies, and policymakers.
Such harmful or abusive content can have major negative impact on society,
e.g., cyberbullying can lead to suicides, rumors about COVID-19 can cause
vaccine hesitance, promotion of fake cures for COVID-19 can cause health harms
and deaths. The content that is posted and shared online can be textual,
visual, or a combination of both, e.g., in a meme. Here, we describe our
experiments in detecting the roles of the entities (hero, villain, victim) in
harmful memes, which is part of the CONSTRAINT-2022 shared task, as well as our
system for the task. We further provide a comparative analysis of different
experimental settings (i.e., unimodal, multimodal, attention, and
augmentation). For reproducibility, we make our experimental code publicly
available. \url{https://github.com/robi56/harmful_memes_block_fusion}",https://github.com/robi56/harmful_memes_block_fusion,-1
OPERA:Operation-Pivoted Discrete Reasoning over Text,0.107885,"Machine reading comprehension (MRC) that requires discrete reasoning
involving symbolic operations, e.g., addition, sorting, and counting, is a
challenging task. According to this nature, semantic parsing-based methods
predict interpretable but complex logical forms. However, logical form
generation is nontrivial and even a little perturbation in a logical form will
lead to wrong answers. To alleviate this issue, multi-predictor -based methods
are proposed to directly predict different types of answers and achieve
improvements. However, they ignore the utilization of symbolic operations and
encounter a lack of reasoning ability and interpretability. To inherit the
advantages of these two types of methods, we propose OPERA, an
operation-pivoted discrete reasoning framework, where lightweight symbolic
operations (compared with logical forms) as neural modules are utilized to
facilitate the reasoning ability and interpretability. Specifically, operations
are first selected and then softly executed to simulate the answer reasoning
procedure. Extensive experiments on both DROP and RACENum datasets show the
reasoning ability of OPERA. Moreover, further analysis verifies its
interpretability.",https://github.com/JD-AI-Research-NLP/OPERA,-1
UniDU: Towards A Unified Generative Dialogue Understanding Framework,0.362007,"With the development of pre-trained language models, remarkable success has
been witnessed in dialogue understanding (DU). However, current DU approaches
usually employ independent models for each distinct DU task without considering
shared knowledge across different DU tasks. In this paper, we propose a unified
generative dialogue understanding framework, named {\em UniDU}, to achieve
effective information exchange across diverse DU tasks. Here, we reformulate
all DU tasks into a unified prompt-based generative model paradigm. More
importantly, a novel model-agnostic multi-task training strategy (MATS) is
introduced to dynamically adapt the weights of diverse tasks for best knowledge
sharing during training, based on the nature and available data of each task.
Experiments on ten DU datasets covering five fundamental DU tasks show that the
proposed UniDU framework largely outperforms task-specific well-designed
methods on all tasks. MATS also reveals the knowledge-sharing structure of
these tasks. Finally, UniDU obtains promising performance in the unseen
dialogue domain, showing the great potential for generalization.",None,-1
Learning Entity Linking Features for Emerging Entities,0.0147139,"Entity linking (EL) is the process of linking entity mentions appearing in
text with their corresponding entities in a knowledge base. EL features of
entities (e.g., prior probability, relatedness score, and entity embedding) are
usually estimated based on Wikipedia. However, for newly emerging entities
(EEs) which have just been discovered in news, they may still not be included
in Wikipedia yet. As a consequence, it is unable to obtain required EL features
for those EEs from Wikipedia and EL models will always fail to link ambiguous
mentions with those EEs correctly as the absence of their EL features. To deal
with this problem, in this paper we focus on a new task of learning EL features
for emerging entities in a general way. We propose a novel approach called
STAMO to learn high-quality EL features for EEs automatically, which needs just
a small number of labeled documents for each EE collected from the Web, as it
could further leverage the knowledge hidden in the unlabeled data. STAMO is
mainly based on self-training, which makes it flexibly integrated with any EL
feature or EL model, but also makes it easily suffer from the error
reinforcement problem caused by the mislabeled data. Instead of some common
self-training strategies that try to throw the mislabeled data away explicitly,
we regard self-training as a multiple optimization process with respect to the
EL features of EEs, and propose both intra-slot and inter-slot optimizations to
alleviate the error reinforcement problem implicitly. We construct two EL
datasets involving selected EEs to evaluate the quality of obtained EL features
for EEs, and the experimental results show that our approach significantly
outperforms other baseline methods of learning EL features.",https://github.com/stamo4el/STAMO,-1
A Privacy-Preserving Unsupervised Domain Adaptation Framework for Clinical Text Analysis,0.100956,"Unsupervised domain adaptation (UDA) generally aligns the unlabeled target
domain data to the distribution of the source domain to mitigate the
distribution shift problem. The standard UDA requires sharing the source data
with the target, having potential data privacy leaking risks. To protect the
source data's privacy, we first propose to share the source feature
distribution instead of the source data. However, sharing only the source
feature distribution may still suffer from the membership inference attack who
can infer an individual's membership by the black-box access to the source
model. To resolve this privacy issue, we further study the under-explored
problem of privacy-preserving domain adaptation and propose a method with a
novel differential privacy training strategy to protect the source data
privacy. We model the source feature distribution by Gaussian Mixture Models
(GMMs) under the differential privacy setting and send it to the target client
for adaptation. The target client resamples differentially private source
features from GMMs and adapts on target data with several state-of-art UDA
backbones. With our proposed method, the source data provider could avoid
leaking source data privacy during domain adaptation as well as reserve the
utility. To evaluate our proposed method's utility and privacy loss, we apply
our model on a medical report disease label classification task using two noisy
challenging clinical text datasets. The results show that our proposed method
can preserve source data's privacy with a minor performance influence on the
text classification task.",None,-1
Fair Group-Shared Representations with Normalizing Flows,0.0565808,"The issue of fairness in machine learning stems from the fact that historical
data often displays biases against specific groups of people which have been
underprivileged in the recent past, or still are. In this context, one of the
possible approaches is to employ fair representation learning algorithms which
are able to remove biases from data, making groups statistically
indistinguishable. In this paper, we instead develop a fair representation
learning algorithm which is able to map individuals belonging to different
groups in a single group. This is made possible by training a pair of
Normalizing Flow models and constraining them to not remove information about
the ground truth by training a ranking or classification model on top of them.
The overall, ``chained'' model is invertible and has a tractable Jacobian,
which allows to relate together the probability densities for different groups
and ``translate'' individuals from one group to another. We show experimentally
that our methodology is competitive with other fair representation learning
algorithms. Furthermore, our algorithm achieves stronger invariance w.r.t. the
sensitive attribute.",None,-1
3DMM-RF: Convolutional Radiance Fields for 3D Face Modeling,0.028024,"Facial 3D Morphable Models are a main computer vision subject with countless
applications and have been highly optimized in the last two decades. The
tremendous improvements of deep generative networks have created various
possibilities for improving such models and have attracted wide interest.
Moreover, the recent advances in neural radiance fields, are revolutionising
novel-view synthesis of known scenes. In this work, we present a facial 3D
Morphable Model, which exploits both of the above, and can accurately model a
subject's identity, pose and expression and render it in arbitrary
illumination. This is achieved by utilizing a powerful deep style-based
generator to overcome two main weaknesses of neural radiance fields, their
rigidity and rendering speed. We introduce a style-based generative network
that synthesizes in one pass all and only the required rendering samples of a
neural radiance field. We create a vast labelled synthetic dataset of facial
renders, and train the network on these data, so that it can accurately model
and generalize on facial identity, pose and appearance. Finally, we show that
this model can accurately be fit to ""in-the-wild"" facial images of arbitrary
pose and illumination, extract the facial characteristics, and be used to
re-render the face in controllable conditions.",None,-1
Contrastive Learning for Online Semi-Supervised General Continual Learning,0.00872201,"We study Online Continual Learning with missing labels and propose SemiCon, a
new contrastive loss designed for partly labeled data. We demonstrate its
efficiency by devising a memory-based method trained on an unlabeled data
stream, where every data added to memory is labeled using an oracle. Our
approach outperforms existing semi-supervised methods when few labels are
available, and obtain similar results to state-of-the-art supervised methods
while using only 2.6% of labels on Split-CIFAR10 and 10% of labels on
Split-CIFAR100.",https://github.com/Nicolas1203/ossgcl,-1
Explainable Decision Making with Lean and Argumentative Explanations,0.0124642,"It is widely acknowledged that transparency of automated decision making is
crucial for deployability of intelligent systems, and explaining the reasons
why some decisions are ""good"" and some are not is a way to achieving this
transparency. We consider two variants of decision making, where ""good""
decisions amount to alternatives (i) meeting ""most"" goals, and (ii) meeting
""most preferred"" goals. We then define, for each variant and notion of
""goodness"" (corresponding to a number of existing notions in the literature),
explanations in two formats, for justifying the selection of an alternative to
audiences with differing needs and competences: lean explanations, in terms of
goals satisfied and, for some notions of ""goodness"", alternative decisions, and
argumentative explanations, reflecting the decision process leading to the
selection, while corresponding to the lean explanations. To define
argumentative explanations, we use assumption-based argumentation (ABA), a
well-known form of structured argumentation. Specifically, we define ABA
frameworks such that ""good"" decisions are admissible ABA arguments and draw
argumentative explanations from dispute trees sanctioning this admissibility.
Finally, we instantiate our overall framework for explainable decision-making
to accommodate connections between goals and decisions in terms of decision
graphs incorporating defeasible and non-defeasible information.",None,-1
A Deep Reinforcement Learning Blind AI in DareFightingICE,0.101553,"This paper presents a deep reinforcement learning agent (AI) that uses sound
as the input on the DareFightingICE platform at the DareFightingICE Competition
in IEEE CoG 2022. In this work, an AI that only uses sound as the input is
called blind AI. While state-of-the-art AIs rely mostly on visual or structured
observations provided by their environments, learning to play games from only
sound is still new and thus challenging. We propose different approaches to
process audio data and use the Proximal Policy Optimization algorithm for our
blind AI. We also propose to use our blind AI in evaluation of sound designs
submitted to the competition and define two metrics for this task. The
experimental results show the effectiveness of not only our blind AI but also
the proposed two metrics.",None,-1
Life-long Learning for Multilingual Neural Machine Translation with Knowledge Distillation,0.0215853,"A common scenario of Multilingual Neural Machine Translation (MNMT) is that
each translation task arrives in a sequential manner, and the training data of
previous tasks is unavailable. In this scenario, the current methods suffer
heavily from catastrophic forgetting (CF). To alleviate the CF, we investigate
knowledge distillation based life-long learning methods. Specifically, in
one-tomany scenario, we propose a multilingual distillation method to make the
new model (student) jointly learn multilingual output from old model (teacher)
and new task. In many-to one scenario, we find that direct distillation faces
the extreme partial distillation problem, and we propose two different methods
to address it: pseudo input distillation and reverse teacher distillation. The
experimental results on twelve translation tasks show that the proposed methods
can better consolidate the previous knowledge and sharply alleviate the CF.",https://github.com/THUNLP-MT/THUMT,-1
Random Rank: The One and Only Strategyproof and Proportionally Fair Randomized Facility Location Mechanism,0.0179963,"Proportionality is an attractive fairness concept that has been applied to a
range of problems including the facility location problem, a classic problem in
social choice. In our work, we propose a concept called Strong Proportionality,
which ensures that when there are two groups of agents at different locations,
both groups incur the same total cost. We show that although Strong
Proportionality is a well-motivated and basic axiom, there is no deterministic
strategyproof mechanism satisfying the property. We then identify a randomized
mechanism called Random Rank (which uniformly selects a number $k$ between $1$
to $n$ and locates the facility at the $k$'th highest agent location) which
satisfies Strong Proportionality in expectation. Our main theorem characterizes
Random Rank as the unique mechanism that achieves universal truthfulness,
universal anonymity, and Strong Proportionality in expectation among all
randomized mechanisms. Finally, we show via the AverageOrRandomRank mechanism
that even stronger ex-post fairness guarantees can be achieved by weakening
universal truthfulness to strategyproofness in expectation.",None,-1
Speciesist Language and Nonhuman Animal Bias in English Masked Language Models,0.453595,"Various existing studies have analyzed what social biases are inherited by
NLP models. These biases may directly or indirectly harm people, therefore
previous studies have focused only on human attributes. However, until recently
no research on social biases in NLP regarding nonhumans existed. In this paper,
we analyze biases to nonhuman animals, i.e. speciesist bias, inherent in
English Masked Language Models such as BERT. We analyzed speciesist bias
against 46 animal names using template-based and corpus-extracted sentences
containing speciesist (or non-speciesist) language. We found that pre-trained
masked language models tend to associate harmful words with nonhuman animals
and have a bias toward using speciesist language for some nonhuman animal
names. Our code for reproducing the experiments will be made available on
GitHub.",None,-1
Adaptive Risk-Tendency: Nano Drone Navigation in Cluttered Environments with Distributional Reinforcement Learning,0.0579554,"Enabling the capability of assessing risk and making risk-aware decisions is
essential to applying reinforcement learning to safety-critical robots like
drones. In this paper, we investigate a specific case where a nano quadcopter
robot learns to navigate an apriori-unknown cluttered environment under partial
observability. We present a distributional reinforcement learning framework to
generate adaptive risk-tendency policies. Specifically, we propose to use lower
tail conditional variance of the learnt return distribution as intrinsic
uncertainty estimation, and use exponentially weighted average forecasting
(EWAF) to adapt the risk-tendency in accordance with the estimated uncertainty.
In simulation and real-world empirical results, we show that (1) the most
effective risk-tendency vary across states, (2) the agent with adaptive
risk-tendency achieves superior performance compared to risk-neutral policy or
risk-averse policy baselines.",None,-1
Reinforcement Learning Your Way: Agent Characterization through Policy Regularization,0.128295,"The increased complexity of state-of-the-art reinforcement learning (RL)
algorithms have resulted in an opacity that inhibits explainability and
understanding. This has led to the development of several post-hoc
explainability methods that aim to extract information from learned policies
thus aiding explainability. These methods rely on empirical observations of the
policy and thus aim to generalize a characterization of agents' behaviour. In
this study, we have instead developed a method to imbue a characteristic
behaviour into agents' policies through regularization of their objective
functions. Our method guides the agents' behaviour during learning which
results in an intrinsic characterization; it connects the learning process with
model explanation. We provide a formal argument and empirical evidence for the
viability of our method. In future work, we intend to employ it to develop
agents that optimize individual financial customers' investment portfolios
based on their spending personalities.",None,-1
Two-Aspect Information Fusion Model For ABAW4 Multi-task Challenge,0.00492264,"In this paper, we propose the solution to the Multi-Task Learning (MTL)
Challenge of the 4th Affective Behavior Analysis in-the-wild (ABAW)
competition. The task of ABAW is to predict frame-level emotion descriptors
from videos: discrete emotional state; valence and arousal; and action units.
Although researchers have proposed several approaches and achieved promising
results in ABAW, current works in this task rarely consider interactions
between different emotion descriptors. To this end, we propose a novel end to
end architecture to achieve full integration of different types of information.
Experimental results demonstrate the effectiveness of our proposed solution.",None,-1
Quality Diversity Evolutionary Learning of Decision Trees,0.0276889,"Addressing the need for explainable Machine Learning has emerged as one of
the most important research directions in modern Artificial Intelligence (AI).
While the current dominant paradigm in the field is based on black-box models,
typically in the form of (deep) neural networks, these models lack direct
interpretability for human users, i.e., their outcomes (and, even more so,
their inner working) are opaque and hard to understand. This is hindering the
adoption of AI in safety-critical applications, where high interests are at
stake. In these applications, explainable by design models, such as decision
trees, may be more suitable, as they provide interpretability. Recent works
have proposed the hybridization of decision trees and Reinforcement Learning,
to combine the advantages of the two approaches. So far, however, these works
have focused on the optimization of those hybrid models. Here, we apply
MAP-Elites for diversifying hybrid models over a feature space that captures
both the model complexity and its behavioral variability. We apply our method
on two well-known control problems from the OpenAI Gym library, on which we
discuss the ""illumination"" patterns projected by MAP-Elites, comparing its
results against existing similar approaches.",None,-1
Human Mobility Prediction with Causal and Spatial-constrained Multi-task Network,0.061475,"Modeling human mobility helps to understand how people are accessing
resources and physically contacting with each other in cities, and thus
contributes to various applications such as urban planning, epidemic control,
and location-based advertisement. Next location prediction is one decisive task
in individual human mobility modeling and is usually viewed as sequence
modeling, solved with Markov or RNN-based methods. However, the existing models
paid little attention to the logic of individual travel decisions and the
reproducibility of the collective behavior of population. To this end, we
propose a Causal and Spatial-constrained Long and Short-term Learner (CSLSL)
for next location prediction. CSLSL utilizes a causal structure based on
multi-task learning to explicitly model the
""\textit{when$\rightarrow$what$\rightarrow$where}"", a.k.a.
""\textit{time$\rightarrow$activity$\rightarrow$location}"" decision logic. We
next propose a spatial-constrained loss function as an auxiliary task, to
ensure the consistency between the predicted and actual spatial distribution of
travelers' destinations. Moreover, CSLSL adopts modules named Long and
Short-term Capturer (LSC) to learn the transition regularities across different
time spans. Extensive experiments on three real-world datasets show promising
performance improvements of CSLSL over baselines and confirm the effectiveness
of introducing the causality and consistency constraints. The implementation is
available at https://github.com/urbanmobility/CSLSL.",https://github.com/urbanmobility/CSLSL,-1
TCE at Qur'an QA 2022: Arabic Language Question Answering Over Holy Qur'an Using a Post-Processed Ensemble of BERT-based Models,0.0407984,"In recent years, we witnessed great progress in different tasks of natural
language understanding using machine learning. Question answering is one of
these tasks which is used by search engines and social media platforms for
improved user experience. Arabic is the language of the Holy Qur'an; the sacred
text for 1.8 billion people across the world. Arabic is a challenging language
for Natural Language Processing (NLP) due to its complex structures. In this
article, we describe our attempts at OSACT5 Qur'an QA 2022 Shared Task, which
is a question answering challenge on the Holy Qur'an in Arabic. We propose an
ensemble learning model based on Arabic variants of BERT models. In addition,
we perform post-processing to enhance the model predictions. Our system
achieves a Partial Reciprocal Rank (pRR) score of 56.6% on the official test
set.",https://github.com/mohammed-elkomy/quran-qa,-1
Generalized Face Anti-Spoofing via Multi-Task Learning and One-Side Meta Triplet Loss,0.0266565,"With the increasing variations of face presentation attacks, model
generalization becomes an essential challenge for a practical face
anti-spoofing system. This paper presents a generalized face anti-spoofing
framework that consists of three tasks: depth estimation, face parsing, and
live/spoof classification. With the pixel-wise supervision from the face
parsing and depth estimation tasks, the regularized features can better
distinguish spoof faces. While simulating domain shift with meta-learning
techniques, the proposed one-side triplet loss can further improve the
generalization capability by a large margin. Extensive experiments on four
public datasets demonstrate that the proposed framework and training strategies
are more effective than previous works for model generalization to unseen
domains.",None,-1
SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution,0.0177932,"Dynamic convolution achieves better performance for efficient CNNs at the
cost of negligible FLOPs increase. However, the performance increase can not
match the significantly expanded number of parameters, which is the main
bottleneck in real-world applications. Contrastively, mask-based unstructured
pruning obtains a lightweight network by removing redundancy in the heavy
network. In this paper, we propose a new framework, \textbf{Sparse Dynamic
Convolution} (\textsc{SD-Conv}), to naturally integrate these two paths such
that it can inherit the advantage of dynamic mechanism and sparsity. We first
design a binary mask derived from a learnable threshold to prune static
kernels, significantly reducing the parameters and computational cost but
achieving higher performance in Imagenet-1K. We further transfer pretrained
models into a variety of downstream tasks, showing consistently better results
than baselines. We hope our SD-Conv could be an efficient alternative to
conventional dynamic convolutions.",https://github.com/weiaicunzai/pytorch-cifar100,-1
Neural Networks Based on Power Method and Inverse Power Method for Solving Linear Eigenvalue Problems,0.0,"In this article, we propose two kinds of neural networks inspired by power
method and inverse power method to solve linear eigenvalue problems. These
neural networks share similar ideas with traditional methods, in which the
differential operator is realized by automatic differentiation. The
eigenfunction of the eigenvalue problem is learned by the neural network and
the iterative algorithms are implemented by optimizing the specially defined
loss function. The largest positive eigenvalue, smallest eigenvalue and
interior eigenvalues with the given prior knowledge can be solved efficiently.
We examine the applicability and accuracy of our methods in the numerical
experiments in one dimension, two dimensions and higher dimensions. Numerical
results show that accurate eigenvalue and eigenfunction approximations can be
obtained by our methods.",https://github.com/SummerLoveRain/PMNN_IPMNN,-1
Automatic Recognition and Classification of Future Work Sentences from Academic Articles in a Specific Domain,0.109047,"Future work sentences (FWS) are the particular sentences in academic papers
that contain the author's description of their proposed follow-up research
direction. This paper presents methods to automatically extract FWS from
academic papers and classify them according to the different future directions
embodied in the paper's content. FWS recognition methods will enable subsequent
researchers to locate future work sentences more accurately and quickly and
reduce the time and cost of acquiring the corpus. The current work on automatic
identification of future work sentences is relatively small, and the existing
research cannot accurately identify FWS from academic papers, and thus cannot
conduct data mining on a large scale. Furthermore, there are many aspects to
the content of future work, and the subdivision of the content is conducive to
the analysis of specific development directions. In this paper, Nature Language
Processing (NLP) is used as a case study, and FWS are extracted from academic
papers and classified into different types. We manually build an annotated
corpus with six different types of FWS. Then, automatic recognition and
classification of FWS are implemented using machine learning models, and the
performance of these models is compared based on the evaluation metrics. The
results show that the Bernoulli Bayesian model has the best performance in the
automatic recognition task, with the Macro F1 reaching 90.73%, and the SCIBERT
model has the best performance in the automatic classification task, with the
weighted average F1 reaching 72.63%. Finally, we extract keywords from FWS and
gain a deep understanding of the key content described in FWS, and we also
demonstrate that content determination in FWS will be reflected in the
subsequent research work by measuring the similarity between future work
sentences and the abstracts.",https://github.com/xiangyi-njust/FWS,-1
Analysing the effectiveness of a generative model for semi-supervised medical image segmentation,0.0235073,"Image segmentation is important in medical imaging, providing valuable,
quantitative information for clinical decision-making in diagnosis, therapy,
and intervention. The state-of-the-art in automated segmentation remains
supervised learning, employing discriminative models such as U-Net. However,
training these models requires access to large amounts of manually labelled
data which is often difficult to obtain in real medical applications. In such
settings, semi-supervised learning (SSL) attempts to leverage the abundance of
unlabelled data to obtain more robust and reliable models. Recently, generative
models have been proposed for semantic segmentation, as they make an attractive
choice for SSL. Their ability to capture the joint distribution over input
images and output label maps provides a natural way to incorporate information
from unlabelled images. This paper analyses whether deep generative models such
as the SemanticGAN are truly viable alternatives to tackle challenging medical
image segmentation problems. To that end, we thoroughly evaluate the
segmentation performance, robustness, and potential subgroup disparities of
discriminative and generative segmentation methods when applied to large-scale,
publicly available chest X-ray datasets.",None,-1
Automatic Controlling Fish Feeding Machine using Feature Extraction of Nutriment and Ripple Behavior,0.0132291,"Controlling fish feeding machine is challenging problem because experienced
fishermen can adequately control based on assumption. To build robust method
for reasonable application, we propose automatic controlling fish feeding
machine based on computer vision using combination of counting nutriments and
estimating ripple behavior using regression and textural feature, respectively.
To count number of nutriments, we apply object detection and tracking methods
to acknowledge the nutriments moving to sea surface. Recently, object tracking
is active research and challenging problem in computer vision. Unfortunately,
the robust tracking method for multiple small objects with dense and complex
relationships is unsolved problem in aquaculture field with more appearance
creatures. Based on the number of nutriments and ripple behavior, we can
control fish feeding machine which consistently performs well in real
environment. Proposed method presents the agreement for automatic controlling
fish feeding by the activation graphs and textural feature of ripple behavior.
Our tracking method can precisely track the nutriments in next frame comparing
with other methods. Based on computational time, proposed method reaches 3.86
fps while other methods spend lower than 1.93 fps. Quantitative evaluation can
promise that proposed method is valuable for aquaculture fish farm with widely
applied to real environment.",None,-1
iSimLoc: Visual Global Localization for Previously Unseen Environments with Simulated Images,0.044099,"The visual camera is an attractive device in beyond visual line of sight
(B-VLOS) drone operation, since they are low in size, weight, power, and cost,
and can provide redundant modality to GPS failures. However, state-of-the-art
visual localization algorithms are unable to match visual data that have a
significantly different appearance due to illuminations or viewpoints. This
paper presents iSimLoc, a condition/viewpoint consistent hierarchical global
re-localization approach. The place features of iSimLoc can be utilized to
search target images under changing appearances and viewpoints. Additionally,
our hierarchical global re-localization module refines in a coarse-to-fine
manner, allowing iSimLoc to perform a fast and accurate estimation. We evaluate
our method on one dataset with appearance variations and one dataset that
focuses on demonstrating large-scale matching over a long flight in complicated
environments. On our two datasets, iSimLoc achieves 88.7\% and 83.8\%
successful retrieval rates with 1.5s inferencing time, compared to 45.8% and
39.7% using the next best method. These results demonstrate robust localization
in a range of environments.",None,-1
Alleviating Representational Shift for Continual Fine-tuning,0.0188923,"We study a practical setting of continual learning: fine-tuning on a
pre-trained model continually. Previous work has found that, when training on
new tasks, the features (penultimate layer representations) of previous data
will change, called representational shift. Besides the shift of features, we
reveal that the intermediate layers' representational shift (IRS) also matters
since it disrupts batch normalization, which is another crucial cause of
catastrophic forgetting. Motivated by this, we propose ConFiT, a fine-tuning
method incorporating two components, cross-convolution batch normalization
(Xconv BN) and hierarchical fine-tuning. Xconv BN maintains pre-convolution
running means instead of post-convolution, and recovers post-convolution ones
before testing, which corrects the inaccurate estimates of means under IRS.
Hierarchical fine-tuning leverages a multi-stage strategy to fine-tune the
pre-trained network, preventing massive changes in Conv layers and thus
alleviating IRS. Experimental results on four datasets show that our method
remarkably outperforms several state-of-the-art methods with lower storage
overhead.",http://github.com/JieShibo/ConFiT,-1
Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models,0.012962,"Our goal is to develop fine-grained real-image editing methods suitable for
real-world applications. In this paper, we first summarize four requirements
for these methods and propose a novel diffusion-based image editing framework
with pixel-wise guidance that satisfies these requirements. Specifically, we
train pixel-classifiers with a few annotated data and then infer the
segmentation map of a target image. Users then manipulate the map to instruct
how the image will be edited. We utilize a pre-trained diffusion model to
generate edited images aligned with the user's intention with pixel-wise
guidance. The effective combination of proposed guidance and other techniques
enables highly controllable editing with preserving the outside of the edited
area, which results in meeting our requirements. The experimental results
demonstrate that our proposal outperforms the GAN-based method for editing
quality and speed.",https://github.com/sony/pixel-guided-diffusion.git,-1
Exploring Extreme Parameter Compression for Pre-trained Language Models,0.0912006,"Recent work explored the potential of large-scale Transformer-based
pre-trained models, especially Pre-trained Language Models (PLMs) in natural
language processing. This raises many concerns from various perspectives, e.g.,
financial costs and carbon emissions. Compressing PLMs like BERT with
negligible performance loss for faster inference and cheaper deployment has
attracted much attention. In this work, we aim to explore larger compression
ratios for PLMs, among which tensor decomposition is a potential but
under-investigated one. Two decomposition and reconstruction protocols are
further proposed to improve the effectiveness and efficiency during
compression. Our compressed BERT with ${1}/{7}$ parameters in Transformer
layers performs on-par with, sometimes slightly better than the original BERT
in GLUE benchmark. A tiny version achieves $96.7\%$ performance of BERT-base
with $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding
the embedding layer) and $2.7 \times$ faster on inference. To show that the
proposed method is orthogonal to existing compression methods like knowledge
distillation, we also explore the benefit of the proposed method on a distilled
BERT.",None,-1
EAPruning: Evolutionary Pruning for Vision Transformers and CNNs,0.00988404,"Structured pruning greatly eases the deployment of large neural networks in
resource-constrained environments. However, current methods either involve
strong domain expertise, require extra hyperparameter tuning, or are restricted
only to a specific type of network, which prevents pervasive industrial
applications. In this paper, we undertake a simple and effective approach that
can be easily applied to both vision transformers and convolutional neural
networks. Specifically, we consider pruning as an evolution process of
sub-network structures that inherit weights through reconstruction techniques.
We achieve a 50% FLOPS reduction for ResNet50 and MobileNetV1, leading to 1.37x
and 1.34x speedup respectively. For DeiT-Base, we reach nearly 40% FLOPs
reduction and 1.4x speedup. Our code will be made available.",None,-1
External Knowledge Selection with Weighted Negative Sampling in Knowledge-grounded Task-oriented Dialogue Systems,0.0143695,"Constructing a robust dialogue system on spoken conversations bring more
challenge than written conversation. In this respect, DSTC10-Track2-Task2 is
proposed, which aims to build a task-oriented dialogue (TOD) system
incorporating unstructured external knowledge on a spoken conversation,
extending DSTC9-Track1. This paper introduces our system containing four
advanced methods: data construction, weighted negative sampling, post-training,
and style transfer. We first automatically construct a large training data
because DSTC10-Track2 does not release the official training set. For the
knowledge selection task, we propose weighted negative sampling to train the
model more fine-grained manner. We also employ post-training and style transfer
for the response generation task to generate an appropriate response with a
similar style to the target response. In the experiment, we investigate the
effect of weighted negative sampling, post-training, and style transfer. Our
model ranked 7 out of 16 teams in the objective evaluation and 6 in human
evaluation.",https://github.com/hanjanghoon/Weighted NS,-1
Fuzzy Rough Sets Based on Fuzzy Quantification,0.0249391,"One of the weaknesses of classical (fuzzy) rough sets is their sensitivity to
noise, which is particularly undesirable for machine learning applications. One
approach to solve this issue is by making use of fuzzy quantifiers, as done by
the vaguely quantified fuzzy rough set (VQFRS) model. While this idea is
intuitive, the VQFRS model suffers from both theoretical flaws as well as from
suboptimal performance in applications. In this paper, we improve on VQFRS by
introducing fuzzy quantifier-based fuzzy rough sets (FQFRS), an intuitive
generalization of fuzzy rough sets that makes use of general unary and binary
quantification models. We show how several existing models fit in this
generalization as well as how it inspires novel ones. Several binary
quantification models are proposed to be used with FQFRS. We conduct a
theoretical study of their properties, and investigate their potential by
applying them to classification problems. In particular, we highlight Yager's
Weighted Implication-based (YWI) binary quantification model, which induces a
fuzzy rough set model that is both a significant improvement on VQFRS, as well
as a worthy competitor to the popular ordered weighted averaging based fuzzy
rough set (OWAFRS) model.",None,-1
Parameter-Parallel Distributed Variational Quantum Algorithm,0.0234918,"Variational quantum algorithms (VQAs) have emerged as a promising near-term
technique to explore practical quantum advantage on noisy intermediate-scale
quantum (NISQ) devices. However, the inefficient parameter training process due
to the incompatibility with backpropagation and the cost of a large number of
measurements, posing a great challenge to the large-scale development of VQAs.
Here, we propose a parameter-parallel distributed variational quantum algorithm
(PPD-VQA), to accelerate the training process by parameter-parallel training
with multiple quantum processors. To maintain the high performance of PPD-VQA
in the realistic noise scenarios, a alternate training strategy is proposed to
alleviate the acceleration attenuation caused by noise differences among
multiple quantum processors, which is an unavoidable common problem of
distributed VQA. Besides, the gradient compression is also employed to overcome
the potential communication bottlenecks. The achieved results suggest that the
PPD-VQA could provide a practical solution for coordinating multiple quantum
processors to handle large-scale real-word applications.",None,-1
Evaluating Byte and Wordpiece Level Models for Massively Multilingual Semantic Parsing,0.00563544,"Token free approaches have been successfully applied to a series of word and
span level tasks. In this work, we compare a byte-level (ByT5) and a wordpiece
based (mT5) sequence to sequence model on the 51 languages of the MASSIVE
multilingual semantic parsing dataset. We examine multiple experimental
settings: (i) zero-shot, (ii) full gold data and (iii) zero-shot with synthetic
data. By leveraging a state-of-the-art label projection method for machine
translated examples, we are able to reduce the gap in exact match accuracy to
only 5 points with respect to a model trained on gold data from all the
languages. We additionally provide insights on the cross-lingual transfer of
ByT5 and show how the model compares with respect to mT5 across all parameter
sizes.",None,-1
AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities,0.053685,"In this paper, we present an end-to-end joint entity and relation extraction
approach based on transformer-based language models. We apply the model to the
task of linking mathematical symbols to their descriptions in LaTeX documents.
In contrast to existing approaches, which perform entity and relation
extraction in sequence, our system incorporates information from relation
extraction into entity extraction. This means that the system can be trained
even on data sets where only a subset of all valid entity spans is annotated.
We provide an extensive evaluation of the proposed system and its strengths and
weaknesses. Our approach, which can be scaled dynamically in computational
complexity at inference time, produces predictions with high precision and
reaches 3rd place in the leaderboard of SemEval-2022 Task 12. For inputs in the
domain of physics and math, it achieves high relation extraction macro F1
scores of 95.43% and 79.17%, respectively. The code used for training and
evaluating our models is available at: https://github.com/nicpopovic/RE1st",https://github.com/davidsbatista/NER-Evaluation,-1
FisheyeDistill: Self-Supervised Monocular Depth Estimation with Ordinal Distillation for Fisheye Cameras,0.0949775,"In this paper, we deal with the problem of monocular depth estimation for
fisheye cameras in a self-supervised manner. A known issue of self-supervised
depth estimation is that it suffers in low-light/over-exposure conditions and
in large homogeneous regions. To tackle this issue, we propose a novel ordinal
distillation loss that distills the ordinal information from a large teacher
model. Such a teacher model, since having been trained on a large amount of
diverse data, can capture the depth ordering information well, but lacks in
preserving accurate scene geometry. Combined with self-supervised losses, we
show that our model can not only generate reasonable depth maps in challenging
environments but also better recover the scene geometry. We further leverage
the fisheye cameras of an AR-Glasses device to collect an indoor dataset to
facilitate evaluation.",None,-1
Open Vocabulary Extreme Classification Using Generative Models,0.402445,"The extreme multi-label classification (XMC) task aims at tagging content
with a subset of labels from an extremely large label set. The label vocabulary
is typically defined in advance by domain experts and assumed to capture all
necessary tags. However in real world scenarios this label set, although large,
is often incomplete and experts frequently need to refine it. To develop
systems that simplify this process, we introduce the task of open vocabulary
XMC (OXMC): given a piece of content, predict a set of labels, some of which
may be outside of the known tag set. Hence, in addition to not having training
data for some labels - as is the case in zero-shot classification - models need
to invent some labels on-the-fly. We propose GROOV, a fine-tuned seq2seq model
for OXMC that generates the set of labels as a flat sequence and is trained
using a novel loss independent of predicted label order. We show the efficacy
of the approach, experimenting with popular XMC datasets for which GROOV is
able to predict meaningful labels outside the given vocabulary while performing
on par with state-of-the-art solutions for known labels.",None,-1
Relation-Aware Language-Graph Transformer for Question Answering,0.0992345,"Question Answering (QA) is a task that entails reasoning over natural
language contexts, and many relevant works augment language models (LMs) with
graph neural networks (GNNs) to encode the Knowledge Graph (KG) information.
However, most existing GNN-based modules for QA do not take advantage of rich
relational information of KGs and depend on limited information interaction
between the LM and the KG. To address these issues, we propose Question
Answering Transformer (QAT), which is designed to jointly reason over language
and graphs with respect to entity relations in a unified manner. Specifically,
QAT constructs Meta-Path tokens, which learn relation-centric embeddings based
on diverse structural and semantic relations. Then, our Relation-Aware
Self-Attention module comprehensively integrates different modalities via the
Cross-Modal Relative Position Bias, which guides information exchange between
relevant entites of different modalities. We validate the effectiveness of QAT
on commonsense question answering datasets like CommonsenseQA and OpenBookQA,
and on a medical question answering dataset, MedQA-USMLE. On all the datasets,
our method achieves state-of-the-art performance. Our code is available at
http://github.com/mlvlab/QAT.",None,-1
Delving Deeper into Cross-lingual Visual Question Answering,0.265901,"Visual question answering (VQA) is one of the crucial vision-and-language
tasks. Yet, existing VQA research has mostly focused on the English language,
due to a lack of suitable evaluation resources. Previous work on cross-lingual
VQA has reported poor zero-shot transfer performance of current multilingual
multimodal Transformers with large gaps to monolingual performance, without any
deeper analysis. In this work, we delve deeper into the different aspects of
cross-lingual VQA, aiming to understand the impact of 1) modeling methods and
choices, including architecture, inductive bias, fine-tuning; 2) learning
biases: including question types and modality biases in cross-lingual setups.
The key results of our analysis are: 1) We show that simple modifications to
the standard training setup can substantially reduce the transfer gap to
monolingual English performance, yielding +10 accuracy points over existing
methods. 2) We analyze cross-lingual VQA across different question types of
varying complexity for different multilingual multimodal Transformers, and
identify question types that are the most difficult to improve on. 3) We
provide an analysis of modality biases present in training data and models,
revealing why zero-shot performance gaps remain for certain question types and
languages.",https://github.com/UKPLab/eacl2023-xlingvqa,-1
Equilibrium Aggregation: Encoding Sets via Optimization,0.0260899,"Processing sets or other unordered, potentially variable-sized inputs in
neural networks is usually handled by aggregating a number of input tensors
into a single representation. While a number of aggregation methods already
exist from simple sum pooling to multi-head attention, they are limited in
their representational power both from theoretical and empirical perspectives.
On the search of a principally more powerful aggregation strategy, we propose
an optimization-based method called Equilibrium Aggregation. We show that many
existing aggregation methods can be recovered as special cases of Equilibrium
Aggregation and that it is provably more efficient in some important cases.
Equilibrium Aggregation can be used as a drop-in replacement in many existing
architectures and applications. We validate its efficiency on three different
tasks: median estimation, class counting, and molecular property prediction. In
all experiments, Equilibrium Aggregation achieves higher performance than the
other aggregation techniques we test.",http://github.com/google/jax,-1
Sustainable AI Processing at the Edge,0.160221,"Edge computing is a popular target for accelerating machine learning
algorithms supporting mobile devices without requiring the communication
latencies to handle them in the cloud. Edge deployments of machine learning
primarily consider traditional concerns such as SWaP constraints (Size, Weight,
and Power) for their installations. However, such metrics are not entirely
sufficient to consider environmental impacts from computing given the
significant contributions from embodied energy and carbon. In this paper we
explore the tradeoffs of convolutional neural network acceleration engines for
both inference and on-line training. In particular, we explore the use of
processing-in-memory (PIM) approaches, mobile GPU accelerators, and recently
released FPGAs, and compare them with novel Racetrack memory PIM. Replacing
PIM-enabled DDR3 with Racetrack memory PIM can recover its embodied energy as
quickly as 1 year. For high activity ratios, mobile GPUs can be more
sustainable but have higher embodied energy to overcome compared to PIM-enabled
Racetrack memory.",None,-1
Deep Probabilistic Graph Matching,0.0239558,"Most previous learning-based graph matching algorithms solve the
\textit{quadratic assignment problem} (QAP) by dropping one or more of the
matching constraints and adopting a relaxed assignment solver to obtain
sub-optimal correspondences. Such relaxation may actually weaken the original
graph matching problem, and in turn hurt the matching performance. In this
paper we propose a deep learning-based graph matching framework that works for
the original QAP without compromising on the matching constraints. In
particular, we design an affinity-assignment prediction network to jointly
learn the pairwise affinity and estimate the node assignments, and we then
develop a differentiable solver inspired by the probabilistic perspective of
the pairwise affinities. Aiming to obtain better matching results, the
probabilistic solver refines the estimated assignments in an iterative manner
to impose both discrete and one-to-one matching constraints. The proposed
method is evaluated on three popularly tested benchmarks (Pascal VOC, Willow
Object and SPair-71k), and it outperforms all previous state-of-the-arts on all
benchmarks.",https://github.com/Thinklab-SJTU/ThinkMatch,-1
Explaining Classifications to Non Experts: An XAI User Study of Post Hoc Explanations for a Classifier When People Lack Expertise,0.0548924,"Very few eXplainable AI (XAI) studies consider how users understanding of
explanations might change depending on whether they know more or less about the
to be explained domain (i.e., whether they differ in their expertise). Yet,
expertise is a critical facet of most high stakes, human decision making (e.g.,
understanding how a trainee doctor differs from an experienced consultant).
Accordingly, this paper reports a novel, user study (N=96) on how peoples
expertise in a domain affects their understanding of post-hoc explanations by
example for a deep-learning, black box classifier. The results show that
peoples understanding of explanations for correct and incorrect classifications
changes dramatically, on several dimensions (e.g., response times, perceptions
of correctness and helpfulness), when the image-based domain considered is
familiar (i.e., MNIST) as opposed to unfamiliar (i.e., Kannada MNIST). The
wider implications of these new findings for XAI strategies are discussed.",None,-1
SporeAgent: Reinforced Scene-level Plausibility for Object Pose Refinement,0.041979,"Observational noise, inaccurate segmentation and ambiguity due to symmetry
and occlusion lead to inaccurate object pose estimates. While depth- and
RGB-based pose refinement approaches increase the accuracy of the resulting
pose estimates, they are susceptible to ambiguity in the observation as they
consider visual alignment. We propose to leverage the fact that we often
observe static, rigid scenes. Thus, the objects therein need to be under
physically plausible poses. We show that considering plausibility reduces
ambiguity and, in consequence, allows poses to be more accurately predicted in
cluttered environments. To this end, we extend a recent RL-based registration
approach towards iterative refinement of object poses. Experiments on the
LINEMOD and YCB-VIDEO datasets demonstrate the state-of-the-art performance of
our depth-based refinement approach.",https://github.com/dornik/sporeagent,-1
Analogical Math Word Problems Solving with Enhanced Problem-Solution Association,0.270317,"Math word problem (MWP) solving is an important task in question answering
which requires human-like reasoning ability. Analogical reasoning has long been
used in mathematical education, as it enables students to apply common
relational structures of mathematical situations to solve new problems. In this
paper, we propose to build a novel MWP solver by leveraging analogical MWPs,
which advance the solver's generalization ability across different kinds of
MWPs. The key idea, named analogy identification, is to associate the
analogical MWP pairs in a latent space, i.e., encoding an MWP close to another
analogical MWP, while moving away from the non-analogical ones. Moreover, a
solution discriminator is integrated into the MWP solver to enhance the
association between the representations of MWPs and their true solutions. The
evaluation results verify that our proposed analogical learning strategy
promotes the performance of MWP-BERT on Math23k over the state-of-the-art model
Generate2Rank, with 5 times fewer parameters in the encoder. We also find that
our model has a stronger generalization ability in solving difficult MWPs due
to the analogical learning from easy MWPs.",https://github.com/ShichaoSun/math_seq2tree,-1
Toward More Effective Human Evaluation for Machine Translation,0.0112255,"Improvements in text generation technologies such as machine translation have
necessitated more costly and time-consuming human evaluation procedures to
ensure an accurate signal. We investigate a simple way to reduce cost by
reducing the number of text segments that must be annotated in order to
accurately predict a score for a complete test set. Using a sampling approach,
we demonstrate that information from document membership and automatic metrics
can help improve estimates compared to a pure random sampling baseline. We
achieve gains of up to 20% in average absolute error by leveraging stratified
sampling and control variates. Our techniques can improve estimates made from a
fixed annotation budget, are easy to implement, and can be applied to any
problem with structure similar to the one we study.",None,-1
Sentiment Analysis with Deep Learning Models: A Comparative Study on a Decade of Sinhala Language Facebook Data,0.0201593,"The relationship between Facebook posts and the corresponding reaction
feature is an interesting subject to explore and understand. To achieve this
end, we test state-of-the-art Sinhala sentiment analysis models against a data
set containing a decade worth of Sinhala posts with millions of reactions. For
the purpose of establishing benchmarks and with the goal of identifying the
best model for Sinhala sentiment analysis, we also test, on the same data set
configuration, other deep learning models catered for sentiment analysis. In
this study we report that the 3 layer Bidirectional LSTM model achieves an F1
score of 84.58% for Sinhala sentiment analysis, surpassing the current
state-of-the-art model; Capsule B, which only manages to get an F1 score of
82.04%. Further, since all the deep learning models show F1 scores above 75% we
conclude that it is safe to claim that Facebook reactions are suitable to
predict the sentiment of a text.",None,-1
Muscle Vision: Real Time Keypoint Based Pose Classification of Physical Exercises,0.0156002,"Recent advances in machine learning technology have enabled highly portable
and performant models for many common tasks, especially in image recognition.
One emerging field, 3D human pose recognition extrapolated from video, has now
advanced to the point of enabling real-time software applications with robust
enough output to support downstream machine learning tasks. In this work we
propose a new machine learning pipeline and web interface that performs human
pose recognition on a live video feed to detect when common exercises are
performed and classify them accordingly. We present a model interface capable
of webcam input with live display of classification results. Our main
contributions include a keypoint and time series based lightweight approach for
classifying a selected set of fitness exercises and a web-based software
application for obtaining and visualizing the results in real time.",None,-1
NSNet: A General Neural Probabilistic Framework for Satisfiability Problems,0.0038291,"We present the Neural Satisfiability Network (NSNet), a general neural
framework that models satisfiability problems as probabilistic inference and
meanwhile exhibits proper explainability. Inspired by the Belief Propagation
(BP), NSNet uses a novel graph neural network (GNN) to parameterize BP in the
latent space, where its hidden representations maintain the same probabilistic
interpretation as BP. NSNet can be flexibly configured to solve both SAT and
#SAT problems by applying different learning objectives. For SAT, instead of
directly predicting a satisfying assignment, NSNet performs marginal inference
among all satisfying solutions, which we empirically find is more feasible for
neural networks to learn. With the estimated marginals, a satisfying assignment
can be efficiently generated by rounding and executing a stochastic local
search. For #SAT, NSNet performs approximate model counting by learning the
Bethe approximation of the partition function. Our evaluations show that NSNet
achieves competitive results in terms of inference accuracy and time efficiency
on multiple SAT and #SAT datasets.",https://github.com/zhaoyu-li/NSNet,-1
Bi-Link: Bridging Inductive Link Predictions from Text via Contrastive Learning of Transformers and Prompts,0.00218767,"Inductive knowledge graph completion requires models to comprehend the
underlying semantics and logic patterns of relations. With the advance of
pretrained language models, recent research have designed transformers for link
prediction tasks. However, empirical studies show that linearizing triples
affects the learning of relational patterns, such as inversion and symmetry. In
this paper, we propose Bi-Link, a contrastive learning framework with
probabilistic syntax prompts for link predictions. Using grammatical knowledge
of BERT, we efficiently search for relational prompts according to learnt
syntactical patterns that generalize to large knowledge graphs. To better
express symmetric relations, we design a symmetric link prediction model,
establishing bidirectional linking between forward prediction and backward
prediction. This bidirectional linking accommodates flexible self-ensemble
strategies at test time. In our experiments, Bi-Link outperforms recent
baselines on link prediction datasets (WN18RR, FB15K-237, and Wikidata5M).
Furthermore, we construct Zeshel-Ind as an in-domain inductive entity linking
the environment to evaluate Bi-Link. The experimental results demonstrate that
our method yields robust representations which can generalize under domain
shift.",https://anonymous.4open.science/r/Bi-Link-2277/,-1
Error Correction in ASR using Sequence-to-Sequence Models,0.00715552,"Post-editing in Automatic Speech Recognition (ASR) entails automatically
correcting common and systematic errors produced by the ASR system. The outputs
of an ASR system are largely prone to phonetic and spelling errors. In this
paper, we propose to use a powerful pre-trained sequence-to-sequence model,
BART, further adaptively trained to serve as a denoising model, to correct
errors of such types. The adaptive training is performed on an augmented
dataset obtained by synthetically inducing errors as well as by incorporating
actual errors from an existing ASR system. We also propose a simple approach to
rescore the outputs using word level alignments. Experimental results on
accented speech data demonstrate that our strategy effectively rectifies a
significant number of ASR errors and produces improved WER results when
compared against a competitive baseline. We also highlight a negative result
obtained on the related grammatical error correction task in Hindi language
showing the limitation in capturing wider context by our proposed model.",None,-1
MetaASSIST: Robust Dialogue State Tracking with Meta Learning,0.0169556,"Existing dialogue datasets contain lots of noise in their state annotations.
Such noise can hurt model training and ultimately lead to poor generalization
performance. A general framework named ASSIST has recently been proposed to
train robust dialogue state tracking (DST) models. It introduces an auxiliary
model to generate pseudo labels for the noisy training set. These pseudo labels
are combined with vanilla labels by a common fixed weighting parameter to train
the primary DST model. Notwithstanding the improvements of ASSIST on DST,
tuning the weighting parameter is challenging. Moreover, a single parameter
shared by all slots and all instances may be suboptimal. To overcome these
limitations, we propose a meta learning-based framework MetaASSIST to
adaptively learn the weighting parameter. Specifically, we propose three
schemes with varying degrees of flexibility, ranging from slot-wise to both
slot-wise and instance-wise, to convert the weighting parameter into learnable
functions. These functions are trained in a meta-learning manner by taking the
validation set as meta data. Experimental results demonstrate that all three
schemes can achieve competitive performance. Most impressively, we achieve a
state-of-the-art joint goal accuracy of 80.10% on MultiWOZ 2.4.",https://github.com/smartyfh/,-1
Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words,0.0623039,"Prompt-based fine-tuning for pre-trained models has proven effective for many
natural language processing tasks under few-shot settings in general domain.
However, tuning with prompt in biomedical domain has not been investigated
thoroughly. Biomedical words are often rare in general domain, but quite
ubiquitous in biomedical contexts, which dramatically deteriorates the
performance of pre-trained models on downstream biomedical applications even
after fine-tuning, especially in low-resource scenarios. We propose a simple
yet effective approach to helping models learn rare biomedical words during
tuning with prompt. Experimental results show that our method can achieve up to
6% improvement in biomedical natural language inference task without any extra
parameters or training steps using few-shot vanilla prompt settings.",https://github.com/s65b40/prompt_n_paraphrase,-1
Self-Supervised Learning of Image Scale and Orientation,0.192521,"We study the problem of learning to assign a characteristic pose, i.e., scale
and orientation, for an image region of interest. Despite its apparent
simplicity, the problem is non-trivial; it is hard to obtain a large-scale set
of image regions with explicit pose annotations that a model directly learns
from. To tackle the issue, we propose a self-supervised learning framework with
a histogram alignment technique. It generates pairs of image patches by random
rescaling/rotating and then train an estimator to predict their
scale/orientation values so that their relative difference is consistent with
the rescaling/rotating used. The estimator learns to predict a non-parametric
histogram distribution of scale/orientation without any supervision.
Experiments show that it significantly outperforms previous methods in
scale/orientation estimation and also improves image matching and 6 DoF camera
pose estimation by incorporating our patch poses into a matching process.",https://github.com/bluedream1121/self-sca-ori,-1
Reconciling Security and Communication Efficiency in Federated Learning,0.0310028,"Cross-device Federated Learning is an increasingly popular machine learning
setting to train a model by leveraging a large population of client devices
with high privacy and security guarantees. However, communication efficiency
remains a major bottleneck when scaling federated learning to production
environments, particularly due to bandwidth constraints during uplink
communication. In this paper, we formalize and address the problem of
compressing client-to-server model updates under the Secure Aggregation
primitive, a core component of Federated Learning pipelines that allows the
server to aggregate the client updates without accessing them individually. In
particular, we adapt standard scalar quantization and pruning methods to Secure
Aggregation and propose Secure Indexing, a variant of Secure Aggregation that
supports quantization for extreme compression. We establish state-of-the-art
results on LEAF benchmarks in a secure Federated Learning setup with up to
40$\times$ compression in uplink communication with no meaningful loss in
utility compared to uncompressed baselines.",https://github.com/facebookresearch/SecureFLCompression,-1
What are the best systems? New perspectives on NLP Benchmarking,0.170983,"In Machine Learning, a benchmark refers to an ensemble of datasets associated
with one or multiple metrics together with a way to aggregate different systems
performances. They are instrumental in (i) assessing the progress of new
methods along different axes and (ii) selecting the best systems for practical
use. This is particularly the case for NLP with the development of large
pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a
variety of tasks. While the community mainly focused on developing new datasets
and metrics, there has been little interest in the aggregation procedure, which
is often reduced to a simple average over various performance measures.
However, this procedure can be problematic when the metrics are on a different
scale, which may lead to spurious conclusions. This paper proposes a new
procedure to rank systems based on their performance across different tasks.
Motivated by the social choice theory, the final system ordering is obtained
through aggregating the rankings induced by each task and is theoretically
grounded. We conduct extensive numerical experiments (on over 270k scores) to
assess the soundness of our approach both on synthetic and real scores (e.g.
GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method
yields different conclusions on state-of-the-art systems than the
mean-aggregation procedure while being both more reliable and robust.",None,-1
Towards PAC Multi-Object Detection and Tracking,0.0180033,"Accurately detecting and tracking multi-objects is important for
safety-critical applications such as autonomous navigation. However, it remains
challenging to provide guarantees on the performance of state-of-the-art
techniques based on deep learning. We consider a strategy known as conformal
prediction, which predicts sets of labels instead of a single label; in the
classification and regression settings, these algorithms can guarantee that the
true label lies within the prediction set with high probability. Building on
these ideas, we propose multi-object detection and tracking algorithms that
come with probably approximately correct (PAC) guarantees. They do so by
constructing both a prediction set around each object detection as well as
around the set of edge transitions; given an object, the detection prediction
set contains its true bounding box with high probability, and the edge
prediction set contains its true transition across frames with high
probability. We empirically demonstrate that our method can detect and track
objects with PAC guarantees on the COCO and MOT-17 datasets.",None,-1
Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation,0.0600847,"We introduce Action-GPT, a plug-and-play framework for incorporating Large
Language Models (LLMs) into text-based action generation models. Action phrases
in current motion capture datasets contain minimal and to-the-point
information. By carefully crafting prompts for LLMs, we generate richer and
fine-grained descriptions of the action. We show that utilizing these detailed
descriptions instead of the original action phrases leads to better alignment
of text and motion spaces. We introduce a generic approach compatible with
stochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion
models. In addition, the approach enables multiple text descriptions to be
utilized. Our experiments show (i) noticeable qualitative and quantitative
improvement in the quality of synthesized motions, (ii) benefits of utilizing
multiple LLM-generated descriptions, (iii) suitability of the prompt function,
and (iv) zero-shot generation capabilities of the proposed approach. Project
page: https://actiongpt.github.io",https://actiongpt.github.io,-1
Layer Adaptive Deep Neural Networks for Out-of-distribution Detection,0.0294703,"During the forward pass of Deep Neural Networks (DNNs), inputs gradually
transformed from low-level features to high-level conceptual labels. While
features at different layers could summarize the important factors of the
inputs at varying levels, modern out-of-distribution (OOD) detection methods
mostly focus on utilizing their ending layer features. In this paper, we
proposed a novel layer-adaptive OOD detection framework (LA-OOD) for DNNs that
can fully utilize the intermediate layers' outputs. Specifically, instead of
training a unified OOD detector at a fixed ending layer, we train multiple
One-Class SVM OOD detectors simultaneously at the intermediate layers to
exploit the full spectrum characteristics encoded at varying depths of DNNs. We
develop a simple yet effective layer-adaptive policy to identify the best layer
for detecting each potential OOD example. LA-OOD can be applied to any existing
DNNs and does not require access to OOD samples during the training. Using
three DNNs of varying depth and architectures, our experiments demonstrate that
LA-OOD is robust against OODs of varying complexity and can outperform
state-of-the-art competitors by a large margin on some real-world datasets.",https://github.com/haoliangwang86/LA-OOD,-1
Efficiently Tuned Parameters are Task Embeddings,0.0128429,"Intermediate-task transfer can benefit a wide range of NLP tasks with
properly selected source datasets. However, it is computationally infeasible to
experiment with all intermediate transfer combinations, making choosing a
useful source task a challenging problem. In this paper, we anticipate that
task-specific parameters updated in parameter-efficient tuning methods are
likely to encode task-specific information. Therefore, such parameters can be
predictive for inter-task transferability. Thus, we propose to exploit these
efficiently tuned parameters as off-the-shelf task embeddings for the efficient
selection of source datasets for intermediate-task transfer. We experiment with
11 text classification tasks and 11 question answering tasks. Experimental
results show that our approach can consistently outperform existing inter-task
transferability prediction methods while being conceptually simple and
computationally efficient. Our analysis also reveals that the ability of
efficiently tuned parameters on transferability prediction is disentangled with
their in-task performance. This allows us to use parameters from early
checkpoints as task embeddings to further improve efficiency.",https://github.com/JetRunner/TuPaTE,-1
Self-Configuring nnU-Nets Detect Clouds in Satellite Images,0.0216949,"Cloud detection is a pivotal satellite image pre-processing step that can be
performed both on the ground and on board a satellite to tag useful images. In
the latter case, it can help to reduce the amount of data to downlink by
pruning the cloudy areas, or to make a satellite more autonomous through
data-driven acquisition re-scheduling of the cloudy areas. We approach this
important task with nnU-Nets, a self-reconfigurable framework able to perform
meta-learning of a segmentation network over various datasets. Our experiments,
performed over Sentinel-2 and Landsat-8 multispectral images revealed that
nnU-Nets deliver state-of-the-art cloud segmentation performance without any
manual design. Our approach was ranked within the top 7% best solutions (across
847 participating teams) in the On Cloud N: Cloud Cover Detection Challenge,
where we reached the Jaccard index of 0.882 over more than 10k unseen
Sentinel-2 image patches (the winners obtained 0.897, whereas the baseline
U-Net with the ResNet-34 backbone used as an encoder: 0.817, and the classic
Sentinel-2 image thresholding: 0.652).",https://gitlab.com/jnalepa/nnUNets_for_clouds,-1
Multilingual Communication System with Deaf Individuals Utilizing Natural and Visual Languages,0.0252238,"According to the World Federation of the Deaf, more than two hundred sign
languages exist. Therefore, it is challenging to understand deaf individuals,
even proficient sign language users, resulting in a barrier between the deaf
community and the rest of society. To bridge this language barrier, we propose
a novel multilingual communication system, namely MUGCAT, to improve the
communication efficiency of sign language users. By converting recognized
specific hand gestures into expressive pictures, which is universal usage and
language independence, our MUGCAT system significantly helps deaf people convey
their thoughts. To overcome the limitation of sign language usage, which is
mostly impossible to translate into complete sentences for ordinary people, we
propose to reconstruct meaningful sentences from the incomplete translation of
sign language. We also measure the semantic similarity of generated sentences
with fragmented recognized hand gestures to keep the original meaning.
Experimental results show that the proposed system can work in a real-time
manner and synthesize exquisite stunning illustrations and meaningful sentences
from a few hand gestures of sign language. This proves that our MUGCAT has
promising potential in assisting deaf communication.",None,-1
Self-supervised AutoFlow,0.431763,"Recently, AutoFlow has shown promising results on learning a training set for
optical flow, but requires ground truth labels in the target domain to compute
its search metric. Observing a strong correlation between the ground truth
search metric and self-supervised losses, we introduce self-supervised AutoFlow
to handle real-world videos without ground truth labels. Using self-supervised
loss as the search metric, our self-supervised AutoFlow performs on par with
AutoFlow on Sintel and KITTI where ground truth is available, and performs
better on the real-world DAVIS dataset. We further explore using
self-supervised AutoFlow in the (semi-)supervised setting and obtain
competitive results against the state of the art.",None,-1
Are Sample-Efficient NLP Models More Robust?,0.0176124,"Recent results in image classification and extractive question answering have
observed that pre-trained models trained on less in-distribution data have
better out-of-distribution performance. However, it is unclear how broadly
these trends hold. We conduct a large empirical study across three tasks, three
broadly-applicable modeling interventions (increasing model size, using a
different adaptation method, and pre-training on more data), and 14 diverse
datasets to investigate the relationship between sample efficiency (amount of
data needed to reach a given ID accuracy) and robustness (how models fare on
OOD evaluation). We find that higher sample efficiency is only correlated with
better average OOD robustness on some modeling interventions and tasks, but not
others. On individual datasets, models with lower sample efficiency can even be
more robust. These results suggest that general-purpose methods for improving
sample efficiency are unlikely to yield universal OOD robustness improvements,
since such improvements are highly dataset- and task-dependent. Even in an era
of large, multi-purpose pretrained models, task-specific decisions may often be
necessary for OOD generalization.",None,-1
Adversarial and Random Transformations for Robust Domain Adaptation and Generalization,0.0122999,"Data augmentation has been widely used to improve generalization in training
deep neural networks. Recent works show that using worst-case transformations
or adversarial augmentation strategies can significantly improve the accuracy
and robustness. However, due to the non-differentiable properties of image
transformations, searching algorithms such as reinforcement learning or
evolution strategy have to be applied, which are not computationally practical
for large scale problems. In this work, we show that by simply applying
consistency training with random data augmentation, state-of-the-art results on
domain adaptation (DA) and generalization (DG) can be obtained. To further
improve the accuracy and robustness with adversarial examples, we propose a
differentiable adversarial data augmentation method based on spatial
transformer networks (STN). The combined adversarial and random transformations
based method outperforms the state-of-the-art on multiple DA and DG benchmark
datasets. Besides, the proposed method shows desirable robustness to
corruption, which is also validated on commonly used datasets.",None,-1
Hierarchical Conditional Variational Autoencoder Based Acoustic Anomaly Detection,0.0753947,"This paper aims to develop an acoustic signal-based unsupervised anomaly
detection method for automatic machine monitoring. Existing approaches such as
deep autoencoder (DAE), variational autoencoder (VAE), conditional variational
autoencoder (CVAE) etc. have limited representation capabilities in the latent
space and, hence, poor anomaly detection performance. Different models have to
be trained for each different kind of machines to accurately perform the
anomaly detection task. To solve this issue, we propose a new method named as
hierarchical conditional variational autoencoder (HCVAE). This method utilizes
available taxonomic hierarchical knowledge about industrial facility to refine
the latent space representation. This knowledge helps model to improve the
anomaly detection performance as well. We demonstrated the generalization
capability of a single HCVAE model for different types of machines by using
appropriate conditions. Additionally, to show the practicability of the
proposed approach, (i) we evaluated HCVAE model on different domain and (ii) we
checked the effect of partial hierarchical knowledge. Our results show that
HCVAE method validates both of these points, and it outperforms the baseline
system on anomaly detection task by utmost 15 % on the AUC score metric.",None,-1
Sampling-based techniques for designing school boundaries,0.0886114,"Recently, an increasing number of researchers, especially in the realm of
political redistricting, have proposed sampling-based techniques to generate a
subset of plans from the vast space of districting plans. These techniques have
been increasingly adopted by U.S. courts of law and independent commissions as
a tool for identifying partisan gerrymanders. Motivated by these recent
developments, we develop a set of similar sampling techniques for designing
school boundaries based on the flip proposal. Note that the flip proposal here
refers to the change in the districting plan by a single assignment. These
sampling-based techniques serve a dual purpose. They can be used as a baseline
for comparing redistricting algorithms based on local search. Additionally,
these techniques can help to infer the problem characteristics that may be
further used for developing efficient redistricting methods. We empirically
touch on both these aspects in regards to the problem of school redistricting.",https://github.com/subhodipbiswas/SamplingbasedSchoolRedistricting,-1
Solvability of orbit-finite systems of linear equations,0.0338074,"We study orbit-finite systems of linear equations, in the setting of sets
with atoms. Our principal contribution is a decision procedure for solvability
of such systems. The procedure works for every field (and even commutative
ring) under mild effectiveness assumptions, and reduces a given orbit-finite
system to a number of finite ones: exponentially many in general, but
polynomially many when atom dimension of input systems is fixed. Towards
obtaining the procedure we push further the theory of vector spaces generated
by orbit-finite sets, and show that each such vector space admits an
orbit-finite basis. This fundamental property is a key tool in our development,
but should be also of wider interest.",None,-1
Structure Extraction in Task-Oriented Dialogues with Slot Clustering,0.130759,"Extracting structure information from dialogue data can help us better
understand user and system behaviors. In task-oriented dialogues, dialogue
structure has often been considered as transition graphs among dialogue states.
However, annotating dialogue states manually is expensive and time-consuming.
In this paper, we propose a simple yet effective approach for structure
extraction in task-oriented dialogues. We first detect and cluster possible
slot tokens with a pre-trained model to approximate dialogue ontology for a
target domain. Then we track the status of each identified token group and
derive a state transition structure. Empirical results show that our approach
outperforms unsupervised baseline models by far in dialogue structure
extraction. In addition, we show that data augmentation based on extracted
structures enriches the surface formats of training data and can achieve a
significant performance boost in dialogue response generation.",https://github.com/salesforce/dialog-flow-extraction,-1
PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations,0.018393,"While self-supervised speech representation learning (SSL) models serve a
variety of downstream tasks, these models have been observed to overfit to the
domain from which the unlabelled data originates. To alleviate this issue, we
propose PADA (Pruning Assisted Domain Adaptation) and zero out redundant
weights from models pre-trained on large amounts of out-of-domain (OOD) data.
Intuitively, this helps to make space for the target-domain ASR finetuning. The
redundant weights can be identified through various pruning strategies which
have been discussed in detail as a part of this work. Specifically, we
investigate the effect of the recently discovered Task-Agnostic and Task-Aware
pruning on PADA and propose a new pruning paradigm based on the latter, which
we call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial
pruning mask from a well fine-tuned OOD model, which makes it starkly different
from the rest of the pruning strategies discussed in the paper. Our proposed
CD-TAW methodology achieves up to 20.6% relative WER improvement over our
baseline when fine-tuned on a 2-hour subset of Switchboard data without
language model (LM) decoding. Furthermore, we conduct a detailed analysis to
highlight the key design choices of our proposed method.",https://github.com/Speech-Lab-IITM/PADA,-1
Tensor-based Sequential Learning via Hankel Matrix Representation for Next Item Recommendations,0.013315,"Self-attentive transformer models have recently been shown to solve the next
item recommendation task very efficiently. The learned attention weights
capture sequential dynamics in user behavior and generalize well. Motivated by
the special structure of learned parameter space, we question if it is possible
to mimic it with an alternative and more lightweight approach. We develop a new
tensor factorization-based model that ingrains the structural knowledge about
sequential data within the learning process. We demonstrate how certain
properties of a self-attention network can be reproduced with our approach
based on special Hankel matrix representation. The resulting model has a
shallow linear architecture and compares competitively to its neural
counterpart.",https://github.com/recspert/SATF,-1
Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models,0.0575633,"With a growing number of BERTology work analyzing different components of
pre-trained language models, we extend this line of research through an
in-depth analysis of discourse information in pre-trained and fine-tuned
language models. We move beyond prior work along three dimensions: First, we
describe a novel approach to infer discourse structures from arbitrarily long
documents. Second, we propose a new type of analysis to explore where and how
accurately intrinsic discourse is captured in the BERT and BART models.
Finally, we assess how similar the generated structures are to a variety of
baselines as well as their distribution within and between models.",https://github.com/Wendy-Xiao/summ_guided_disco_parser,-1
Strong-TransCenter: Improved Multi-Object Tracking based on Transformers with Dense Representations,0.0699113,"Transformer networks have been a focus of research in many fields in recent
years, being able to surpass the state-of-the-art performance in different
computer vision tasks. A few attempts have been made to apply this method to
the task of Multiple Object Tracking (MOT), among those the state-of-the-art
was TransCenter, a transformer-based MOT architecture with dense object queries
for accurately tracking all the objects while keeping reasonable runtime.
TransCenter is the first center-based transformer framework for MOT, and is
also among the first to show the benefits of using transformer-based
architectures for MOT. In this paper we show an improvement to this tracker
using post processing mechanism based in the Track-by-Detection paradigm:
motion model estimation using Kalman filter and target Re-identification using
an embedding network. Our new tracker shows significant improvements in the
IDF1 and HOTA metrics and comparable results on the MOTA metric (70.9%, 59.8%
and 75.8% respectively) on the MOTChallenge MOT17 test dataset and improvement
on all 3 metrics (67.5%, 56.3% and 73.0%) on the MOT20 test dataset. Our
tracker is currently ranked first among transformer-based trackers in these
datasets. The code is publicly available at:
https://github.com/amitgalor18/STC_Tracker",https://github.com/amitgalor18/STC_Tracker,-1
Interpretable Fault Diagnosis of Rolling Element Bearings with Temporal Logic Neural Network,0.0645263,"Machine learning-based methods have achieved successful applications in
machinery fault diagnosis. However, the main limitation that exists for these
methods is that they operate as a black box and are generally not
interpretable. This paper proposes a novel neural network structure, called
temporal logic neural network (TLNN), in which the neurons of the network are
logic propositions. More importantly, the network can be described and
interpreted as a weighted signal temporal logic. TLNN not only keeps the nice
properties of traditional neuron networks but also provides a formal
interpretation of itself with formal language. Experiments with real datasets
show the proposed neural network can obtain highly accurate fault diagnosis
results with good computation efficiency. Additionally, the embedded formal
language of the neuron network can provide explanations about the decision
process, thus achieve interpretable fault diagnosis.",https://github.com/datagangchen/TLNN,-1
Learning Semantics for Visual Place Recognition through Multi-Scale Attention,0.0937462,"In this paper we address the task of visual place recognition (VPR), where
the goal is to retrieve the correct GPS coordinates of a given query image
against a huge geotagged gallery. While recent works have shown that building
descriptors incorporating semantic and appearance information is beneficial,
current state-of-the-art methods opt for a top down definition of the
significant semantic content. Here we present the first VPR algorithm that
learns robust global embeddings from both visual appearance and semantic
content of the data, with the segmentation process being dynamically guided by
the recognition of places through a multi-scale attention module. Experiments
on various scenarios validate this new approach and demonstrate its performance
against state-of-the-art methods. Finally, we propose the first synthetic-world
dataset suited for both place recognition and segmentation tasks.",None,-1
Chaining Simultaneous Thoughts for Numerical Reasoning,0.144442,"Given that rich information is hidden behind ubiquitous numbers in text,
numerical reasoning over text should be an essential skill of AI systems. To
derive precise equations to solve numerical reasoning problems, previous work
focused on modeling the structures of equations, and has proposed various
structured decoders. Though structure modeling proves to be effective, these
structured decoders construct a single equation in a pre-defined autoregressive
order, potentially placing an unnecessary restriction on how a model should
grasp the reasoning process. Intuitively, humans may have numerous pieces of
thoughts popping up in no pre-defined order; thoughts are not limited to the
problem at hand, and can even be concerned with other related problems. By
comparing diverse thoughts and chaining relevant pieces, humans are less prone
to errors. In this paper, we take this inspiration and propose CANTOR, a
numerical reasoner that models reasoning steps using a directed acyclic graph
where we produce diverse reasoning steps simultaneously without pre-defined
decoding dependencies, and compare and chain relevant ones to reach a solution.
Extensive experiments demonstrated the effectiveness of CANTOR under both
fully-supervised and weakly-supervised settings.",https://github.com/motrom/fastmurty,-1
What do navigation agents learn about their environment?,0.108089,"Today's state of the art visual navigation agents typically consist of large
deep learning models trained end to end. Such models offer little to no
interpretability about the learned skills or the actions of the agent taken in
response to its environment. While past works have explored interpreting deep
learning models, little attention has been devoted to interpreting embodied AI
systems, which often involve reasoning about the structure of the environment,
target characteristics and the outcome of one's actions. In this paper, we
introduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal
and Object Goal navigation agents. We use iSEE to probe the dynamic
representations produced by these agents for the presence of information about
the agent as well as the environment. We demonstrate interesting insights about
navigation agents using iSEE, including the ability to encode reachable
locations (to avoid obstacles), visibility of the target, progress from the
initial spawn location as well as the dramatic effect on the behaviors of
agents when we mask out critical individual neurons. The code is available at:
https://github.com/allenai/iSEE",https://github.com/allenai/iSEE,-1
Can Visual Context Improve Automatic Speech Recognition for an Embodied Agent?,0.00599055,"The usage of automatic speech recognition (ASR) systems are becoming
omnipresent ranging from personal assistant to chatbots, home, and industrial
automation systems, etc. Modern robots are also equipped with ASR capabilities
for interacting with humans as speech is the most natural interaction modality.
However, ASR in robots faces additional challenges as compared to a personal
assistant. Being an embodied agent, a robot must recognize the physical
entities around it and therefore reliably recognize the speech containing the
description of such entities. However, current ASR systems are often unable to
do so due to limitations in ASR training, such as generic datasets and
open-vocabulary modeling. Also, adverse conditions during inference, such as
noise, accented, and far-field speech makes the transcription inaccurate. In
this work, we present a method to incorporate a robot's visual information into
an ASR system and improve the recognition of a spoken utterance containing a
visible entity. Specifically, we propose a new decoder biasing technique to
incorporate the visual context while ensuring the ASR output does not degrade
for incorrect context. We achieve a 59% relative reduction in WER from an
unmodified ASR system.",https://github.com/mozilla/TTS,-1
From One to Many: Dynamic Cross Attention Networks for LiDAR and Camera Fusion,0.102522,"LiDAR and cameras are two complementary sensors for 3D perception in
autonomous driving. LiDAR point clouds have accurate spatial and geometry
information, while RGB images provide textural and color data for context
reasoning. To exploit LiDAR and cameras jointly, existing fusion methods tend
to align each 3D point to only one projected image pixel based on calibration,
namely one-to-one mapping. However, the performance of these approaches highly
relies on the calibration quality, which is sensitive to the temporal and
spatial synchronization of sensors. Therefore, we propose a Dynamic Cross
Attention (DCA) module with a novel one-to-many cross-modality mapping that
learns multiple offsets from the initial projection towards the neighborhood
and thus develops tolerance to calibration error. Moreover, a \textit{dynamic
query enhancement} is proposed to perceive the model-independent calibration,
which further strengthens DCA's tolerance to the initial misalignment. The
whole fusion architecture named Dynamic Cross Attention Network (DCAN) exploits
multi-level image features and adapts to multiple representations of point
clouds, which allows DCA to serve as a plug-in fusion module. Extensive
experiments on nuScenes and KITTI prove DCA's effectiveness. The proposed DCAN
outperforms state-of-the-art methods on the nuScenes detection challenge.",https://github.com/open-mmlab/mmdetection3d,-1
LibertyMFD: A Lexicon to Assess the Moral Foundation of Liberty,0.0,"Quantifying the moral narratives expressed in the user-generated text, news,
or public discourses is fundamental for understanding individuals' concerns and
viewpoints and preventing violent protests and social polarisation. The Moral
Foundation Theory (MFT) was developed to operationalise morality in a
five-dimensional scale system. Recent developments of the theory urged for the
introduction of a new foundation, the Liberty Foundation. Being only recently
added to the theory, there are no available linguistic resources to assess
whether liberty is present in text corpora. Given its importance to current
social issues such as the vaccination debate, we propose two data-driven
approaches, deriving two candidate lexicons generated based on aligned
documents from online news sources with different worldviews. After extensive
experimentation, we contribute to the research community a novel lexicon that
assesses the liberty moral foundation in the way individuals with contrasting
viewpoints express themselves through written text. The LibertyMFD dictionary
can be a valuable tool for policymakers to understand diverse viewpoints on
controversial social issues such as vaccination, abortion, or even uprisings,
as they happen and on a large scale.",None,-1
Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation,0.0442042,"We introduce a novel setup for low-resource task-oriented semantic parsing
which incorporates several constraints that may arise in real-world scenarios:
(1) lack of similar datasets/models from a related domain, (2) inability to
sample useful logical forms directly from a grammar, and (3) privacy
requirements for unlabeled natural utterances. Our goal is to improve a
low-resource semantic parser using utterances collected through user
interactions. In this highly challenging but realistic setting, we investigate
data augmentation approaches involving generating a set of structured canonical
utterances corresponding to logical forms, before simulating corresponding
natural language and filtering the resulting pairs. We find that such
approaches are effective despite our restrictive setup: in a low-resource
setting on the complex SMCalFlow calendaring dataset (Andreas et al., 2020), we
observe 33% relative improvement over a non-data-augmented baseline in top-1
match.",None,-1
Automated speech tools for helping communities process restricted-access corpora for language revival efforts,0.0842129,"Many archival recordings of speech from endangered languages remain
unannotated and inaccessible to community members and language learning
programs. One bottleneck is the time-intensive nature of annotation. An even
narrower bottleneck occurs for recordings with access constraints, such as
language that must be vetted or filtered by authorised community members before
annotation can begin. We propose a privacy-preserving workflow to widen both
bottlenecks for recordings where speech in the endangered language is
intermixed with a more widely-used language such as English for meta-linguistic
commentary and questions (e.g. What is the word for 'tree'?). We integrate
voice activity detection (VAD), spoken language identification (SLI), and
automatic speech recognition (ASR) to transcribe the metalinguistic content,
which an authorised person can quickly scan to triage recordings that can be
annotated by people with lower levels of access. We report work-in-progress
processing 136 hours archival audio containing a mix of English and Muruwari.
Our collaborative work with the Muruwari custodian of the archival materials
show that this workflow reduces metalanguage transcription time by 20% even
given only minimal amounts of annotated training data: 10 utterances per
language for SLI and for ASR at most 39 minutes, and possibly as little as 39
seconds.",https://github.com/CoEDL/vad-sli-asr,-1
SMT-based Weighted Model Integration with Structure Awareness,0.0900831,"Weighted Model Integration (WMI) is a popular formalism aimed at unifying
approaches for probabilistic inference in hybrid domains, involving logical and
algebraic constraints. Despite a considerable amount of recent work, allowing
WMI algorithms to scale with the complexity of the hybrid problem is still a
challenge. In this paper we highlight some substantial limitations of existing
state-of-the-art solutions, and develop an algorithm that combines SMT-based
enumeration, an efficient technique in formal verification, with an effective
encoding of the problem structure. This allows our algorithm to avoid
generating redundant models, resulting in substantial computational savings. An
extensive experimental evaluation on both synthetic and real-world datasets
confirms the advantage of the proposed solution over existing alternatives.",https://github.com/unitn-sml/wmi-pa,-1
Cross-lingual Word Embeddings in Hyperbolic Space,0.0400964,"Cross-lingual word embeddings can be applied to several natural language
processing applications across multiple languages. Unlike prior works that use
word embeddings based on the Euclidean space, this short paper presents a
simple and effective cross-lingual Word2Vec model that adapts to the Poincar\'e
ball model of hyperbolic space to learn unsupervised cross-lingual word
representations from a German-English parallel corpus. It has been shown that
hyperbolic embeddings can capture and preserve hierarchical relationships. We
evaluate the model on both hypernymy and analogy tasks. The proposed model
achieves comparable performance with the vanilla Word2Vec model on the
cross-lingual analogy task, the hypernymy task shows that the cross-lingual
Poincar\'e Word2Vec model can capture latent hierarchical structure from free
text across languages, which are absent from the Euclidean-based Word2Vec
representations. Our results show that by preserving the latent hierarchical
information, hyperbolic spaces can offer better representations for
cross-lingual embeddings.",None,-1
Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling,0.0478545,"Generative Spoken Language Modeling research focuses on optimizing speech
Language Models (LMs) using raw audio recordings without accessing any textual
supervision. Such speech LMs usually operate over discrete units obtained from
quantizing internal representations of self-supervised models. Although such
units show impressive modeling results, their robustness capabilities have not
been extensively investigated. This work focuses on improving the robustness of
discrete input representations for generative spoken language modeling. First,
we formally define how to measure the robustness of such representations to
various signal variations that do not alter the spoken information (e.g.,
time-stretch). Next, we empirically demonstrate how current state-of-the-art
representation models lack robustness to such variations. To overcome this, we
propose an effective and efficient method to learn robust discrete speech
representation for generative spoken language modeling. The proposed approach
is based on applying a set of signal transformations to the speech signal and
optimizing the model using an iterative pseudo-labeling scheme. Our method
significantly improves over the evaluated baselines when considering encoding
and modeling metrics. We additionally evaluate our method on the
speech-to-speech translation task, considering Spanish-English and
French-English translations, and show the proposed approach outperforms the
evaluated baselines.",None,-1
Structural Prior Guided Generative Adversarial Transformers for Low-Light Image Enhancement,0.0675501,"We propose an effective Structural Prior guided Generative Adversarial
Transformer (SPGAT) to solve low-light image enhancement. Our SPGAT mainly
contains a generator with two discriminators and a structural prior estimator
(SPE). The generator is based on a U-shaped Transformer which is used to
explore non-local information for better clear image restoration. The SPE is
used to explore useful structures from images to guide the generator for better
structural detail estimation. To generate more realistic images, we develop a
new structural prior guided adversarial learning method by building the skip
connections between the generator and discriminators so that the discriminators
can better discriminate between real and fake features. Finally, we propose a
parallel windows-based Swin Transformer block to aggregate different level
hierarchical features for high-quality image restoration. Experimental results
demonstrate that the proposed SPGAT performs favorably against recent
state-of-the-art methods on both synthetic and real-world datasets.",None,-1
Physics-Constrained Backdoor Attacks on Power System Fault Localization,0.0582151,"The advances in deep learning (DL) techniques have the potential to deliver
transformative technological breakthroughs to numerous complex tasks in modern
power systems that suffer from increasing uncertainty and nonlinearity.
However, the vulnerability of DL has yet to be thoroughly explored in power
system tasks under various physical constraints. This work, for the first time,
proposes a novel physics-constrained backdoor poisoning attack, which embeds
the undetectable attack signal into the learned model and only performs the
attack when it encounters the corresponding signal. The paper illustrates the
proposed attack on the real-time fault line localization application.
Furthermore, the simulation results on the 68-bus power system demonstrate that
DL-based fault line localization methods are not robust to our proposed attack,
indicating that backdoor poisoning attacks pose real threats to DL
implementations in power systems. The proposed attack pipeline can be easily
generalized to other power system tasks.",None,-1
Latent Space Unsupervised Semantic Segmentation,0.0542236,"The development of compact and energy-efficient wearable sensors has led to
an increase in the availability of biosignals. To analyze these continuously
recorded, and often multidimensional, time series at scale, being able to
conduct meaningful unsupervised data segmentation is an auspicious target. A
common way to achieve this is to identify change-points within the time series
as the segmentation basis. However, traditional change-point detection
algorithms often come with drawbacks, limiting their real-world applicability.
Notably, they generally rely on the complete time series to be available and
thus cannot be used for real-time applications. Another common limitation is
that they poorly (or cannot) handle the segmentation of multidimensional time
series. Consequently, the main contribution of this work is to propose a novel
unsupervised segmentation algorithm for multidimensional time series named
Latent Space Unsupervised Semantic Segmentation (LS-USS), which was designed to
work easily with both online and batch data. When comparing LS-USS against
other state-of-the-art change-point detection algorithms on a variety of
real-world datasets, in both the offline and real-time setting, LS-USS
systematically achieves on par or better performances.",None,-1
What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation,0.278736,"Accurate automatic evaluation metrics for open-domain dialogs are in high
demand. Existing model-based metrics for system response evaluation are trained
on human annotated data, which is cumbersome to collect. In this work, we
propose to use information that can be automatically extracted from the next
user utterance, such as its sentiment or whether the user explicitly ends the
conversation, as a proxy to measure the quality of the previous system
response. This allows us to train on a massive set of dialogs with weak
supervision, without requiring manual system turn quality annotations.
Experiments show that our model is comparable to models trained on human
annotated data. Furthermore, our model generalizes across both spoken and
written open-domain dialog corpora collected from real and paid users.",https://github.com/exe1023/DialEvalMetrics,-1
On the link between conscious function and general intelligence in humans and machines,0.145843,"In popular media, there is often a connection drawn between the advent of
awareness in artificial agents and those same agents simultaneously achieving
human or superhuman level intelligence. In this work, we explore the validity
and potential application of this seemingly intuitive link between
consciousness and intelligence. We do so by examining the cognitive abilities
associated with three contemporary theories of conscious function: Global
Workspace Theory (GWT), Information Generation Theory (IGT), and Attention
Schema Theory (AST). We find that all three theories specifically relate
conscious function to some aspect of domain-general intelligence in humans.
With this insight, we turn to the field of Artificial Intelligence (AI) and
find that, while still far from demonstrating general intelligence, many
state-of-the-art deep learning methods have begun to incorporate key aspects of
each of the three functional theories. Having identified this trend, we use the
motivating example of mental time travel in humans to propose ways in which
insights from each of the three theories may be combined into a single unified
and implementable model. Given that it is made possible by cognitive abilities
underlying each of the three functional theories, artificial agents capable of
mental time travel would not only possess greater general intelligence than
current approaches, but also be more consistent with our current understanding
of the functional role of consciousness in humans, thus making it a promising
near-term goal for AI research.",None,-1
The Metaverse Data Deluge: What Can We Do About It?,0.0812921,"In the Metaverse, the physical space and the virtual space co-exist, and
interact simultaneously. While the physical space is virtually enhanced with
information, the virtual space is continuously refreshed with real-time,
real-world information. To allow users to process and manipulate information
seamlessly between the real and digital spaces, novel technologies must be
developed. These include smart interfaces, new augmented realities, efficient
storage and data management and dissemination techniques. In this paper, we
first discuss some promising co-space applications. These applications offer
opportunities that neither of the spaces can realize on its own. We then
discuss challenges. Finally, we discuss and envision what are likely to be
required from the database and system perspectives.",None,-1
Object Manipulation via Visual Target Localization,0.0206012,"Object manipulation is a critical skill required for Embodied AI agents
interacting with the world around them. Training agents to manipulate objects,
poses many challenges. These include occlusion of the target object by the
agent's arm, noisy object detection and localization, and the target frequently
going out of view as the agent moves around in the scene. We propose
Manipulation via Visual Object Location Estimation (m-VOLE), an approach that
explores the environment in search for target objects, computes their 3D
coordinates once they are located, and then continues to estimate their 3D
locations even when the objects are not visible, thus robustly aiding the task
of manipulating these objects throughout the episode. Our evaluations show a
massive 3x improvement in success rate over a model that has access to the same
sensory suite but is trained without the object location estimator, and our
analysis shows that our agent is robust to noise in depth perception and agent
localization. Importantly, our proposed approach relaxes several assumptions
about idealized localization and perception that are commonly employed by
recent works in embodied AI -- an important step towards training agents for
object manipulation in the real world.",None,-1
Systematic Evaluation of Predictive Fairness,0.0403865,"Mitigating bias in training on biased datasets is an important open problem.
Several techniques have been proposed, however the typical evaluation regime is
very limited, considering very narrow data conditions. For instance, the effect
of target class imbalance and stereotyping is under-studied. To address this
gap, we examine the performance of various debiasing methods across multiple
tasks, spanning binary classification (Twitter sentiment), multi-class
classification (profession prediction), and regression (valence prediction).
Through extensive experimentation, we find that data conditions have a strong
influence on relative model performance, and that general conclusions cannot be
drawn about method efficacy when evaluating only on standard datasets, as is
current practice in fairness research.",https://github.com/HanXudong/Systematic_Evaluation_of_Predictive_Fairness,-1
Do Language Models Learn Position-Role Mappings?,0.0746104,"How is knowledge of position-role mappings in natural language learned? We
explore this question in a computational setting, testing whether a variety of
well-performing pertained language models (BERT, RoBERTa, and DistilBERT)
exhibit knowledge of these mappings, and whether this knowledge persists across
alternations in syntactic, structural, and lexical alternations. In Experiment
1, we show that these neural models do indeed recognize distinctions between
theme and recipient roles in ditransitive constructions, and that these
distinct patterns are shared across construction type. We strengthen this
finding in Experiment 2 by showing that fine-tuning these language models on
novel theme- and recipient-like tokens in one paradigm allows the models to
make correct predictions about their placement in other paradigms, suggesting
that the knowledge of these mappings is shared rather than independently
learned. We do, however, observe some limitations of this generalization when
tasks involve constructions with novel ditransitive verbs, hinting at a degree
of lexical specificity which underlies model performance.",None,-1
Action Conditioned Tactile Prediction: a case study on slip prediction,0.36801,"Tactile predictive models can be useful across several robotic manipulation
tasks, e.g. robotic pushing, robotic grasping, slip avoidance, and in-hand
manipulation. However, available tactile prediction models are mostly studied
for image-based tactile sensors and there is no comparison study indicating the
best performing models. In this paper, we presented two novel data-driven
action-conditioned models for predicting tactile signals during real-world
physical robot interaction tasks (1) action condition tactile prediction and
(2) action conditioned tactile-video prediction models. We use a magnetic-based
tactile sensor that is challenging to analyse and test state-of-the-art
predictive models and the only existing bespoke tactile prediction model. We
compare the performance of these models with those of our proposed models. We
perform the comparison study using our novel tactile enabled dataset containing
51,000 tactile frames of a real-world robotic manipulation task with 11
flat-surfaced household objects. Our experimental results demonstrate the
superiority of our proposed tactile prediction models in terms of qualitative,
quantitative and slip prediction scores.",https://github.com/imanlab/action-conditioned-tactile-prediction,-1
Quantification of emotions in decision making,0.294678,"The problem of quantification of emotions in the choice between alternatives
is considered. The alternatives are evaluated in a dual manner. From one side,
they are characterized by rational features defining the utility of each
alternative. From the other side, the choice is affected by emotions labeling
the alternatives as attractive or repulsive, pleasant or unpleasant. A decision
maker needs to make a choice taking into account both these features, the
utility of alternatives and their attractiveness. The notion of utility is
based on rational grounds, while the notion of attractiveness is vague and
rather is based on irrational feelings. A general method, allowing for the
quantification of the choice combining rational and emotional features is
described. Despite that emotions seem to avoid precise quantification, their
quantitative evaluation is possible at the aggregate level. The analysis of a
series of empirical data demonstrates the efficiency of the approach, including
the realistic behavioral problems that cannot be treated by the standard
expected utility theory.",None,-1
Word-Embeddings Distinguish Denominal and Root-Derived Verbs in Semitic,0.0309975,"Proponents of the Distributed Morphology framework have posited the existence
of two levels of morphological word formation: a lower one, leading to loose
input-output semantic relationships; and an upper one, leading to tight
input-output semantic relationships. In this work, we propose to test the
validity of this assumption in the context of Hebrew word embeddings. If the
two-level hypothesis is borne out, we expect state-of-the-art Hebrew word
embeddings to encode (1) a noun, (2) a denominal derived from it (via an
upper-level operation), and (3) a verb related to the noun (via a lower-level
operation on the noun's root), in such a way that the denominal (2) should be
closer in the embedding space to the noun (1) than the related verb (3) is to
the same noun (1). We report that this hypothesis is verified by four embedding
models of Hebrew: fastText, GloVe, Word2Vec and AlephBERT. This suggests that
word embedding models are able to capture complex and fine-grained semantic
properties that are morphologically motivated.",None,-1
SaiT: Sparse Vision Transformers through Adaptive Token Pruning,0.0788293,"While vision transformers have achieved impressive results, effectively and
efficiently accelerating these models can further boost performances. In this
work, we propose a dense/sparse training framework to obtain a unified model,
enabling weight sharing across various token densities. Thus one model offers a
range of accuracy and throughput tradeoffs for different applications. Besides,
we introduce adaptive token pruning to optimize the patch token sparsity based
on the input image. In addition, we investigate knowledge distillation to
enhance token selection capability in early transformer modules. Sparse
adaptive image Transformer (SaiT) offers varying levels of model acceleration
by merely changing the token sparsity on the fly. Specifically, SaiT reduces
the computation complexity (FLOPs) by 39% - 43% and increases the throughput by
67% - 91% with less than 0.5% accuracy loss for various vision transformer
models. Meanwhile, the same model also provides the zero accuracy drop option
by skipping the sparsification step. SaiT achieves better accuracy and
computation tradeoffs than state-of-the-art transformer and convolutional
models.",https://github.com/rwightman/pytorch-image-models,-1
Complexity of Representations in Deep Learning,0.0,"Deep neural networks use multiple layers of functions to map an object
represented by an input vector progressively to different representations, and
with sufficient training, eventually to a single score for each class that is
the output of the final decision function. Ideally, in this output space, the
objects of different classes achieve maximum separation. Motivated by the need
to better understand the inner working of a deep neural network, we analyze the
effectiveness of the learned representations in separating the classes from a
data complexity perspective. Using a simple complexity measure, a popular
benchmarking task, and a well-known architecture design, we show how the data
complexity evolves through the network, how it changes during training, and how
it is impacted by the network design and the availability of training samples.
We discuss the implications of the observations and the potentials for further
studies.",https://github.com/osmr/imgclsmob/tree/master/pytorch,-1
COVIBOT: A Smart Chatbot for Assistance and E-Awareness during COVID-19 Pandemic,0.0,"The coronavirus pandemic has spread over the past two years in our highly
connected and information-dense society. Nonetheless, disseminating accurate
and up-to-date information on the spread of this pandemic remains a challenge.
In this context, opting for a solution based on conversational artificial
intelligence, also known under the name of the chatbot, is proving to be an
unavoidable solution, especially since it has already shown its effectiveness
in fighting the coronavirus crisis in several countries. This work proposes to
design and implement a smart chatbot on the theme of COVID-19, called COVIBOT,
which will be useful in the context of Saudi Arabia. COVIBOT is a
generative-based contextual chatbot, which is built using machine learning APIs
that are offered by the cloud-based Azure Cognitive Services. Two versions of
COVIBOT are offered: English and Arabic versions. Use cases of COVIBOT are
tested and validated using a scenario-based approach.",None,-1
Tutorial on Course-of-Action (COA) Attack Search Methods in Computer Networks,0.0182667,"In the literature of modern network security research, deriving effective and
efficient course-of-action (COA) attach search methods are of interests in
industry and academia. As the network size grows, the traditional COA attack
search methods can suffer from the limitations to computing and communication
resources. Therefore, various methods have been developed to solve these
problems, and reinforcement learning (RL)-based intelligent algorithms are one
of the most effective solutions. Therefore, we review the RL-based COA attack
search methods for network attack scenarios in terms of the trends and their
contrib",None,-1
A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications,0.0607698,"Automatic text generation based on neural language models has achieved
performance levels that make the generated text almost indistinguishable from
those written by humans. Despite the value that text generation can have in
various applications, it can also be employed for malicious tasks. The
diffusion of such practices represent a threat to the quality of academic
publishing. To address these problems, we propose in this paper two datasets
comprised of artificially generated research content: a completely synthetic
dataset and a partial text substitution dataset. In the first case, the content
is completely generated by the GPT-2 model after a short prompt extracted from
original papers. The partial or hybrid dataset is created by replacing several
sentences of abstracts with sentences that are generated by the Arxiv-NLP
model. We evaluate the quality of the datasets comparing the generated texts to
aligned original texts using fluency metrics such as BLEU and ROUGE. The more
natural the artificial texts seem, the more difficult they are to detect and
the better is the benchmark. We also evaluate the difficulty of the task of
distinguishing original from generated text by using state-of-the-art
classification models.",https://github.com/strib/scigen.git,-1
Temporal Point Cloud Completion with Pose Disturbance,0.203824,"Point clouds collected by real-world sensors are always unaligned and sparse,
which makes it hard to reconstruct the complete shape of object from a single
frame of data. In this work, we manage to provide complete point clouds from
sparse input with pose disturbance by limited translation and rotation. We also
use temporal information to enhance the completion model, refining the output
with a sequence of inputs. With the help of gated recovery units(GRU) and
attention mechanisms as temporal units, we propose a point cloud completion
framework that accepts a sequence of unaligned and sparse inputs, and outputs
consistent and aligned point clouds. Our network performs in an online manner
and presents a refined point cloud for each frame, which enables it to be
integrated into any SLAM or reconstruction pipeline. As far as we know, our
framework is the first to utilize temporal information and ensure temporal
consistency with limited transformation. Through experiments in ShapeNet and
KITTI, we prove that our framework is effective in both synthetic and
real-world datasets.",None,-1
Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22,0.10179,"This paper describes the system developed at the Universitat Polit\`ecnica de
Catalunya for the Workshop on Machine Translation 2022 Sign Language
Translation Task, in particular, for the sign-to-text direction. We use a
Transformer model implemented with the Fairseq modeling toolkit. We have
experimented with the vocabulary size, data augmentation techniques and
pretraining the model with the PHOENIX-14T dataset. Our system obtains 0.50
BLEU score for the test set, improving the organizers' baseline by 0.38 BLEU.
We remark the poor results for both the baseline and our system, and thus, the
unreliability of our findings.",https://github.com/bricksdont/sign-sockeye-baselines,-1
SegTAD: Precise Temporal Action Detection via Semantic Segmentation,0.108167,"Temporal action detection (TAD) is an important yet challenging task in video
analysis. Most existing works draw inspiration from image object detection and
tend to reformulate it as a proposal generation - classification problem.
However, there are two caveats with this paradigm. First, proposals are not
equipped with annotated labels, which have to be empirically compiled, thus the
information in the annotations is not necessarily precisely employed in the
model training process. Second, there are large variations in the temporal
scale of actions, and neglecting this fact may lead to deficient representation
in the video features. To address these issues and precisely model temporal
action detection, we formulate the task of temporal action detection in a novel
perspective of semantic segmentation. Owing to the 1-dimensional property of
TAD, we are able to convert the coarse-grained detection annotations to
fine-grained semantic segmentation annotations for free. We take advantage of
them to provide precise supervision so as to mitigate the impact induced by the
imprecise proposal labels. We propose an end-to-end framework SegTAD composed
of a 1D semantic segmentation network (1D-SSN) and a proposal detection network
(PDN).",None,-1
Low Complexity Convolutional Neural Networks for Equalization in Optical Fiber Transmission,0.0483907,"A convolutional neural network is proposed to mitigate fiber transmission
effects, achieving a five-fold reduction in trainable parameters compared to
alternative equalizers, and 3.5 dB improvement in MSE compared to DBP with
comparable complexity.",None,-1
"Massively Multilingual ASR on 70 Languages: Tokenization, Architecture, and Generalization Capabilities",0.0277657,"End-to-end multilingual ASR has become more appealing because of several
reasons such as simplifying the training and deployment process and positive
performance transfer from high-resource to low-resource languages. However,
scaling up the number of languages, total hours, and number of unique tokens is
not a trivial task. This paper explores large-scale multilingual ASR models on
70 languages. We inspect two architectures: (1) Shared embedding and output and
(2) Multiple embedding and output model. In the shared model experiments, we
show the importance of tokenization strategy across different languages. Later,
we use our optimal tokenization strategy to train multiple embedding and output
model to further improve our result. Our multilingual ASR achieves 13.9%-15.6%
average WER relative improvement compared to monolingual models. We show that
our multilingual ASR generalizes well on an unseen dataset and domain,
achieving 9.5% and 7.5% WER on Multilingual Librispeech (MLS) with zero-shot
and finetuning, respectively.",https://github.com/facebookresearch/fairscale,-1
Language-free Training for Zero-shot Video Grounding,0.208012,"Given an untrimmed video and a language query depicting a specific temporal
moment in the video, video grounding aims to localize the time interval by
understanding the text and video simultaneously. One of the most challenging
issues is an extremely time- and cost-consuming annotation collection,
including video captions in a natural language form and their corresponding
temporal regions. In this paper, we present a simple yet novel training
framework for video grounding in the zero-shot setting, which learns a network
with only video data without any annotation. Inspired by the recent
language-free paradigm, i.e. training without language data, we train the
network without compelling the generation of fake (pseudo) text queries into a
natural language form. Specifically, we propose a method for learning a video
grounding model by selecting a temporal interval as a hypothetical correct
answer and considering the visual feature selected by our method in the
interval as a language feature, with the help of the well-aligned
visual-language space of CLIP. Extensive experiments demonstrate the prominence
of our language-free training framework, outperforming the existing zero-shot
video grounding method and even several weakly-supervised approaches with large
margins on two standard datasets.",None,-1
SOCIOFILLMORE: A Tool for Discovering Perspectives,0.0577741,"SOCIOFILLMORE is a multilingual tool which helps to bring to the fore the
focus or the perspective that a text expresses in depicting an event. Our tool,
whose rationale we also support through a large collection of human judgements,
is theoretically grounded on frame semantics and cognitive linguistics, and
implemented using the LOME frame semantic parser. We describe SOCIOFILLMORE's
development and functionalities, show how non-NLP researchers can easily
interact with the tool, and present some example case studies which are already
incorporated in the system, together with the kind of analysis that can be
visualised.",None,-1
Patch-NetVLAD+: Learned patch descriptor and weighted matching strategy for place recognition,0.0516005,"Visual Place Recognition (VPR) in areas with similar scenes such as urban or
indoor scenarios is a major challenge. Existing VPR methods using global
descriptors have difficulty capturing local specific regions (LSR) in the scene
and are therefore prone to localization confusion in such scenarios. As a
result, finding the LSR that are critical for location recognition becomes key.
To address this challenge, we introduced Patch-NetVLAD+, which was inspired by
patch-based VPR researches. Our method proposed a fine-tuning strategy with
triplet loss to make NetVLAD suitable for extracting patch-level descriptors.
Moreover, unlike existing methods that treat all patches in an image equally,
our method extracts patches of LSR, which present less frequently throughout
the dataset, and makes them play an important role in VPR by assigning proper
weights to them. Experiments on Pittsburgh30k and Tokyo247 datasets show that
our approach achieved up to 6.35\% performance improvement than existing
patch-based methods.",None,-1
A General Purpose Neural Architecture for Geospatial Systems,0.012008,"Geospatial Information Systems are used by researchers and Humanitarian
Assistance and Disaster Response (HADR) practitioners to support a wide variety
of important applications. However, collaboration between these actors is
difficult due to the heterogeneous nature of geospatial data modalities (e.g.,
multi-spectral images of various resolutions, timeseries, weather data) and
diversity of tasks (e.g., regression of human activity indicators or detecting
forest fires). In this work, we present a roadmap towards the construction of a
general-purpose neural architecture (GPNA) with a geospatial inductive bias,
pre-trained on large amounts of unlabelled earth observation data in a
self-supervised manner. We envision how such a model may facilitate cooperation
between members of the community. We show preliminary results on the first step
of the roadmap, where we instantiate an architecture that can process a wide
variety of geospatial data modalities and demonstrate that it can achieve
competitive performance with domain-specific architectures on tasks relating to
the U.N.'s Sustainable Development Goals.",None,-1
CAISE: Conversational Agent for Image Search and Editing,0.06493,"Demand for image editing has been increasing as users' desire for expression
is also increasing. However, for most users, image editing tools are not easy
to use since the tools require certain expertise in photo effects and have
complex interfaces. Hence, users might need someone to help edit their images,
but having a personal dedicated human assistant for every user is impossible to
scale. For that reason, an automated assistant system for image editing is
desirable. Additionally, users want more image sources for diverse image
editing works, and integrating an image search functionality into the editing
tool is a potential remedy for this demand. Thus, we propose a dataset of an
automated Conversational Agent for Image Search and Editing (CAISE). To our
knowledge, this is the first dataset that provides conversational image search
and editing annotations, where the agent holds a grounded conversation with
users and helps them to search and edit images according to their requests. To
build such a system, we first collect image search and editing conversations
between pairs of annotators. The assistant-annotators are equipped with a
customized image search and editing tool to address the requests from the
user-annotators. The functions that the assistant-annotators conduct with the
tool are recorded as executable commands, allowing the trained system to be
useful for real-world application execution. We also introduce a
generator-extractor baseline model for this task, which can adaptively select
the source of the next token (i.e., from the vocabulary or from textual/visual
contexts) for the executable command. This serves as a strong starting point
while still leaving a large human-machine performance gap for useful future
work. Our code and dataset are publicly available at:
https://github.com/hyounghk/CAISE",https://github.com/hyounghk/CAISE,-1
Toward Understanding Bias Correlations for Mitigation in NLP,0.027505,"Natural Language Processing (NLP) models have been found discriminative
against groups of different social identities such as gender and race. With the
negative consequences of these undesired biases, researchers have responded
with unprecedented effort and proposed promising approaches for bias
mitigation. In spite of considerable practical importance, current algorithmic
fairness literature lacks an in-depth understanding of the relations between
different forms of biases. Social bias is complex by nature. Numerous studies
in social psychology identify the ""generalized prejudice"", i.e., generalized
devaluing sentiments across different groups. For example, people who devalue
ethnic minorities are also likely to devalue women and gays. Therefore, this
work aims to provide a first systematic study toward understanding bias
correlations in mitigation. In particular, we examine bias mitigation in two
common NLP tasks -- toxicity detection and word embeddings -- on three social
identities, i.e., race, gender, and religion. Our findings suggest that biases
are correlated and present scenarios in which independent debiasing approaches
dominant in current literature may be insufficient. We further investigate
whether jointly mitigating correlated biases is more desired than independent
and individual debiasing. Lastly, we shed light on the inherent issue of
debiasing-accuracy trade-off in bias mitigation. This study serves to motivate
future research on joint bias mitigation that accounts for correlated biases.",https://github.com/ogencoglu/fair_cyberbullying_detection,-1
"SpeechNet: Weakly Supervised, End-to-End Speech Recognition at Industrial Scale",0.0170993,"End-to-end automatic speech recognition systems represent the state of the
art, but they rely on thousands of hours of manually annotated speech for
training, as well as heavyweight computation for inference. Of course, this
impedes commercialization since most companies lack vast human and
computational resources. In this paper, we explore training and deploying an
ASR system in the label-scarce, compute-limited setting. To reduce human labor,
we use a third-party ASR system as a weak supervision source, supplemented with
labeling functions derived from implicit user feedback. To accelerate
inference, we propose to route production-time queries across a pool of CUDA
graphs of varying input lengths, the distribution of which best matches the
traffic's. Compared to our third-party ASR, we achieve a relative improvement
in word-error rate of 8% and a speedup of 600%. Our system, called SpeechNet,
currently serves 12 million queries per day on our voice-enabled smart
television. To our knowledge, this is the first time a large-scale,
Wav2vec-based deployment has been described in the academic literature.",None,-1
Meta-X$_{NLG}$: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation,0.168014,"Recently, the NLP community has witnessed a rapid advancement in multilingual
and cross-lingual transfer research where the supervision is transferred from
high-resource languages (HRLs) to low-resource languages (LRLs). However, the
cross-lingual transfer is not uniform across languages, particularly in the
zero-shot setting. Towards this goal, one promising research direction is to
learn shareable structures across multiple tasks with limited annotated data.
The downstream multilingual applications may benefit from such a learning setup
as most of the languages across the globe are low-resource and share some
structures with other languages. In this paper, we propose a novel
meta-learning framework (called Meta-X$_{NLG}$) to learn shareable structures
from typologically diverse languages based on meta-learning and language
clustering. This is a step towards uniform cross-lingual transfer for unseen
languages. We first cluster the languages based on language representations and
identify the centroid language of each cluster. Then, a meta-learning algorithm
is trained with all centroid languages and evaluated on the other languages in
the zero-shot setting. We demonstrate the effectiveness of this modeling on two
NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular
datasets and 30 typologically diverse languages. Consistent improvements over
strong baselines demonstrate the efficacy of the proposed framework. The
careful design of the model makes this end-to-end NLG setup less vulnerable to
the accidental translation problem, which is a prominent concern in zero-shot
cross-lingual NLG tasks.",https://github.com/kaushal0494/Meta_XNLG,-1
A new band selection approach based on information theory and support vector machine for hyperspectral images reduction and classification,0.0867067,"The high dimensionality of hyperspectral images consisting of several bands
often imposes a big computational challenge for image processing. Therefore,
spectral band selection is an essential step for removing the irrelevant, noisy
and redundant bands. Consequently increasing the classification accuracy.
However, identification of useful bands from hundreds or even thousands of
related bands is a nontrivial task. This paper aims at identifying a small set
of highly discriminative bands, for improving computational speed and
prediction accuracy. Hence, we proposed a new strategy based on joint mutual
information to measure the statistical dependence and correlation between the
selected bands and evaluate the relative utility of each one to classification.
The proposed filter approach is compared to an effective reproduced filters
based on mutual information. Simulations results on the hyperpectral image HSI
AVIRIS 92AV3C using the SVM classifier have shown that the effective proposed
algorithm outperforms the reproduced filters strategy performance.
  Keywords-Hyperspectral images, Classification, band Selection, Joint Mutual
Information, dimensionality reduction ,correlation, SVM.",None,-1
Masked Vision-Language Transformer in Fashion,0.0,"We present a masked vision-language transformer (MVLT) for fashion-specific
multi-modal representation. Technically, we simply utilize vision transformer
architecture for replacing the BERT in the pre-training model, making MVLT the
first end-to-end framework for the fashion domain. Besides, we designed masked
image reconstruction (MIR) for a fine-grained understanding of fashion. MVLT is
an extensible and convenient architecture that admits raw multi-modal inputs
without extra pre-processing models (e.g., ResNet), implicitly modeling the
vision-language alignments. More importantly, MVLT can easily generalize to
various matching and generative tasks. Experimental results show obvious
improvements in retrieval (rank@5: 17%) and recognition (accuracy: 3%) tasks
over the Fashion-Gen 2018 winner Kaleido-BERT. Code is made available at
https://github.com/GewelsJI/MVLT.",https://github.com/GewelsJI/MVLT,-1
Conditional entropy minimization principle for learning domain invariant representation features,0.029658,"Invariance-principle-based methods such as Invariant Risk Minimization (IRM),
have recently emerged as promising approaches for Domain Generalization (DG).
Despite promising theory, such approaches fail in common classification tasks
due to the mixing of true invariant features and spurious invariant features.
To address this, we propose a framework based on the conditional entropy
minimization (CEM) principle to filter-out the spurious invariant features
leading to a new algorithm with a better generalization capability. We show
that our proposed approach is closely related to the well-known Information
Bottleneck (IB) framework and prove that under certain assumptions, entropy
minimization can exactly recover the true invariant features. Our approach
provides competitive classification accuracy compared to recent
theoretically-principled state-of-the-art alternatives across several DG
datasets.",https://github.com/ahujak/IB-IRM,-1
Continual Learning Based on OOD Detection and Task Masking,0.514738,"Existing continual learning techniques focus on either task incremental
learning (TIL) or class incremental learning (CIL) problem, but not both. CIL
and TIL differ mainly in that the task-id is provided for each test sample
during testing for TIL, but not provided for CIL. Continual learning methods
intended for one problem have limitations on the other problem. This paper
proposes a novel unified approach based on out-of-distribution (OOD) detection
and task masking, called CLOM, to solve both problems. The key novelty is that
each task is trained as an OOD detection model rather than a traditional
supervised learning model, and a task mask is trained to protect each task to
prevent forgetting. Our evaluation shows that CLOM outperforms existing
state-of-the-art baselines by large margins. The average TIL/CIL accuracy of
CLOM over six experiments is 87.6/67.9% while that of the best baselines is
only 82.4/55.0%.",https://github.com/k-gyuhak/CLOM,-1
ELSR: Extreme Low-Power Super Resolution Network For Mobile Devices,0.365264,"With the popularity of mobile devices, e.g., smartphone and wearable devices,
lighter and faster model is crucial for the application of video super
resolution. However, most previous lightweight models tend to concentrate on
reducing lantency of model inference on desktop GPU, which may be not energy
efficient in current mobile devices. In this paper, we proposed Extreme
Low-Power Super Resolution (ELSR) network which only consumes a small amount of
energy in mobile devices. Pretraining and finetuning methods are applied to
boost the performance of the extremely tiny model. Extensive experiments show
that our method achieves a excellent balance between restoration quality and
power consumption. Finally, we achieve a competitive score of 90.9 with PSNR
27.34 dB and power 0.09 W/30FPS on the target MediaTek Dimensity 9000
plantform, ranking 1st place in the Mobile AI & AIM 2022 Real-Time Video
Super-Resolution Challenge.",None,-1
Point Cloud Quality Assessment using 3D Saliency Maps,0.00603659,"Point cloud quality assessment (PCQA) has become an appealing research field
in recent days. Considering the importance of saliency detection in quality
assessment, we propose an effective full-reference PCQA metric which makes the
first attempt to utilize the saliency information to facilitate quality
prediction, called point cloud quality assessment using 3D saliency maps
(PQSM). Specifically, we first propose a projection-based point cloud saliency
map generation method, in which depth information is introduced to better
reflect the geometric characteristics of point clouds. Then, we construct point
cloud local neighborhoods to derive three structural descriptors to indicate
the geometry, color and saliency discrepancies. Finally, a saliency-based
pooling strategy is proposed to generate the final quality score. Extensive
experiments are performed on four independent PCQA databases. The results
demonstrate that the proposed PQSM shows competitive performances compared to
multiple state-of-the-art PCQA metrics.",None,-1
A Critical Analysis of Image-based Camera Pose Estimation Techniques,0.0605817,"Camera, and associated with its objects within the field of view,
localization could benefit many computer vision fields, such as autonomous
driving, robot navigation, and augmented reality (AR). In this survey, we first
introduce specific application areas and the evaluation metrics for camera
localization pose according to different sub-tasks (learning-based 2D-2D task,
feature-based 2D-3D task, and 3D-3D task). Then, we review common methods for
structure-based camera pose estimation approaches, absolute pose regression and
relative pose regression approaches by critically modelling the methods to
inspire further improvements in their algorithms such as loss functions, neural
network structures. Furthermore, we summarise what are the popular datasets
used for camera localization and compare the quantitative and qualitative
results of these methods with detailed performance metrics. Finally, we discuss
future research possibilities and applications.",None,-1
MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer for Autonomous Driving,0.0338743,"3D object detection is a significant task for autonomous driving. Recently
with the progress of vision transformers, the 2D object detection problem is
being treated with the set-to-set loss. Inspired by these approaches on 2D
object detection and an approach for multi-view 3D object detection DETR3D, we
propose MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer architecture to
fuse image and LiDAR features to improve the detection accuracy. Our end-to-end
single-stage, anchor-free and NMS-free network takes in multi-view images and
LiDAR point clouds and predicts 3D bounding boxes. Firstly, we link the object
queries learnt from data to the image and LiDAR features using a novel
MSF3DDETR cross-attention block. Secondly, the object queries interacts with
each other in multi-head self-attention block. Finally, MSF3DDETR block is
repeated for $L$ number of times to refine the object queries. The MSF3DDETR
network is trained end-to-end on the nuScenes dataset using Hungarian algorithm
based bipartite matching and set-to-set loss inspired by DETR. We present both
quantitative and qualitative results which are competitive to the
state-of-the-art approaches.",None,-1
Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models,0.0388142,"Zero-shot cross-lingual transfer learning has been shown to be highly
challenging for tasks involving a lot of linguistic specificities or when a
cultural gap is present between languages, such as in hate speech detection. In
this paper, we highlight this limitation for hate speech detection in several
domains and languages using strict experimental settings. Then, we propose to
train on multilingual auxiliary tasks -- sentiment analysis, named entity
recognition, and tasks relying on syntactic information -- to improve zero-shot
transfer of hate speech detection models across languages. We show how hate
speech detection models benefit from a cross-lingual knowledge proxy brought by
auxiliary tasks fine-tuning and highlight these tasks' positive impact on
bridging the hate speech linguistic and cultural gap between languages.",https://github.com/ArijRB/Multilingual-Auxiliary-Tasks-Training-Bridging-the-Gap-between-Languages-for-Zero-Shot-Transfer-of-/,-1
Is Surprisal in Issue Trackers Actionable?,0.0,"Background. From information theory, surprisal is a measurement of how
unexpected an event is. Statistical language models provide a probabilistic
approximation of natural languages, and because surprisal is constructed with
the probability of an event occuring, it is therefore possible to determine the
surprisal associated with English sentences. The issues and pull requests of
software repository issue trackers give insight into the development process
and likely contain the surprising events of this process.
  Objective. Prior works have identified that unusual events in software
repositories are of interest to developers, and use simple code metrics-based
methods for detecting them. In this study we will propose a new method for
unusual event detection in software repositories using surprisal. With the
ability to find surprising issues and pull requests, we intend to further
analyse them to determine if they actually hold importance in a repository, or
if they pose a significant challenge to address. If it is possible to find bad
surprises early, or before they cause additional troubles, it is plausible that
effort, cost and time will be saved as a result.
  Method. After extracting the issues and pull requests from 5000 of the most
popular software repositories on GitHub, we will train a language model to
represent these issues. We will measure their perceived importance in the
repository, measure their resolution difficulty using several analogues,
measure the surprisal of each, and finally generate inferential statistics to
describe any correlations.",None,-1
AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models,0.766493,"Knowledge distillation (KD) methods compress large models into smaller
students with manually-designed student architectures given pre-specified
computational cost. This requires several trials to find a viable student, and
further repeating the process for each student or computational budget change.
We use Neural Architecture Search (NAS) to automatically distill several
compressed students with variable cost from a large model. Current works train
a single SuperLM consisting of millions of subnetworks with weight-sharing,
resulting in interference between subnetworks of different sizes. Our framework
AutoDistil addresses above challenges with the following steps: (a)
Incorporates inductive bias and heuristics to partition Transformer search
space into K compact sub-spaces (K=3 for typical student sizes of base, small
and tiny); (b) Trains one SuperLM for each sub-space using task-agnostic
objective (e.g., self-attention distillation) with weight-sharing of students;
(c) Lightweight search for the optimal student without re-training. Fully
task-agnostic training and search allow students to be reused for fine-tuning
on any downstream task. Experiments on GLUE benchmark against state-of-the-art
KD and NAS methods demonstrate AutoDistil to outperform leading compression
techniques with upto 2.7x reduction in computational cost and negligible loss
in task performance.",None,-1
Cross-Lingual Phrase Retrieval,0.104589,"Cross-lingual retrieval aims to retrieve relevant text across languages.
Current methods typically achieve cross-lingual retrieval by learning
language-agnostic text representations in word or sentence level. However, how
to learn phrase representations for cross-lingual phrase retrieval is still an
open problem. In this paper, we propose XPR, a cross-lingual phrase retriever
that extracts phrase representations from unlabeled example sentences.
Moreover, we create a large-scale cross-lingual phrase retrieval dataset, which
contains 65K bilingual phrase pairs and 4.2M example sentences in 8
English-centric language pairs. Experimental results show that XPR outperforms
state-of-the-art baselines which utilize word-level or sentence-level
representations. XPR also shows impressive zero-shot transferability that
enables the model to perform retrieval in an unseen language pair during
training. Our dataset, code, and trained models are publicly available at
www.github.com/cwszz/XPR/.",https://github.com/cwszz/XPR/,-1
Incorporating Causal Analysis into Diversified and Logical Response Generation,0.00711675,"Although the Conditional Variational AutoEncoder (CVAE) model can generate
more diversified responses than the traditional Seq2Seq model, the responses
often have low relevance with the input words or are illogical with the
question. A causal analysis is carried out to study the reasons behind, and a
methodology of searching for the mediators and mitigating the confounding bias
in dialogues is provided. Specifically, we propose to predict the mediators to
preserve relevant information and auto-regressively incorporate the mediators
into generating process. Besides, a dynamic topic graph guided conditional
variational autoencoder (TGG-CVAE) model is utilized to complement the semantic
space and reduce the confounding bias in responses. Extensive experiments
demonstrate that the proposed model is able to generate both relevant and
informative responses, and outperforms the state-of-the-art in terms of
automatic metrics and human evaluations.",None,-1
"Textual Stylistic Variation: Choices, Genres and Individuals",0.0779498,"This chapter argues for more informed target metrics for the statistical
processing of stylistic variation in text collections. Much as operationalised
relevance proved a useful goal to strive for in information retrieval, research
in textual stylistics, whether application oriented or philologically inclined,
needs goals formulated in terms of pertinence, relevance, and utility - notions
that agree with reader experience of text. Differences readers are aware of are
mostly based on utility - not on textual characteristics per se. Mostly,
readers report stylistic differences in terms of genres. Genres, while vague
and undefined, are well-established and talked about: very early on, readers
learn to distinguish genres. This chapter discusses variation given by genre,
and contrasts it to variation occasioned by individual choice.",None,-1
An approach to robust ICP initialization,0.0238112,"In this note, we propose an approach to initialize the Iterative Closest
Point (ICP) algorithm to match unlabelled point clouds related by rigid
transformations. The method is based on matching the ellipsoids defined by the
points' covariance matrices and then testing the various principal half-axes
matchings that differ by elements of a finite reflection group. We derive
bounds on the robustness of our approach to noise and numerical experiments
confirm our theoretical findings.",None,-1
TemporalUV: Capturing Loose Clothing with Temporally Coherent UV Coordinates,0.249708,"We propose a novel approach to generate temporally coherent UV coordinates
for loose clothing. Our method is not constrained by human body outlines and
can capture loose garments and hair. We implemented a differentiable pipeline
to learn UV mapping between a sequence of RGB inputs and textures via UV
coordinates. Instead of treating the UV coordinates of each frame separately,
our data generation approach connects all UV coordinates via feature matching
for temporal stability. Subsequently, a generative model is trained to balance
the spatial quality and temporal stability. It is driven by supervised and
unsupervised losses in both UV and image spaces. Our experiments show that the
trained models output high-quality UV coordinates and generalize to new poses.
Once a sequence of UV coordinates has been inferred by our model, it can be
used to flexibly synthesize new looks and modified visual styles. Compared to
existing methods, our approach reduces the computational workload to animate
new outfits by several orders of magnitude.",None,-1
A taxonomy of explanations to support Explainability-by-Design,0.00379153,"As automated decision-making solutions are increasingly applied to all
aspects of everyday life, capabilities to generate meaningful explanations for
a variety of stakeholders (i.e., decision-makers, recipients of decisions,
auditors, regulators...) become crucial. In this paper, we present a taxonomy
of explanations that was developed as part of a holistic
'Explainability-by-Design' approach for the purposes of the project PLEAD. The
taxonomy was built with a view to produce explanations for a wide range of
requirements stemming from a variety of regulatory frameworks or policies set
at the organizational level either to translate high-level compliance
requirements or to meet business needs. The taxonomy comprises nine dimensions.
It is used as a stand-alone classifier of explanations conceived as detective
controls, in order to aid supportive automated compliance strategies. A
machinereadable format of the taxonomy is provided in the form of a light
ontology and the benefits of starting the Explainability-by-Design journey with
such a taxonomy are demonstrated through a series of examples.",None,-1
Cost Volume Pyramid Network with Multi-strategies Range Searching for Multi-view Stereo,0.0587361,"Multi-view stereo is an important research task in computer vision while
still keeping challenging. In recent years, deep learning-based methods have
shown superior performance on this task. Cost volume pyramid network-based
methods which progressively refine depth map in coarse-to-fine manner, have
yielded promising results while consuming less memory. However, these methods
fail to take fully consideration of the characteristics of the cost volumes in
each stage, leading to adopt similar range search strategies for each cost
volume stage. In this work, we present a novel cost volume pyramid based
network with different searching strategies for multi-view stereo. By choosing
different depth range sampling strategies and applying adaptive unimodal
filtering, we are able to obtain more accurate depth estimation in low
resolution stages and iteratively upsample depth map to arbitrary resolution.
We conducted extensive experiments on both DTU and BlendedMVS datasets, and
results show that our method outperforms most state-of-the-art methods.",https://github.com/SibylGao/MSCVP-MVSNet.git,-1
Robustness Implies Generalization via Data-Dependent Generalization Bounds,0.0,"This paper proves that robustness implies generalization via data-dependent
generalization bounds. As a result, robustness and generalization are shown to
be connected closely in a data-dependent manner. Our bounds improve previous
bounds in two directions, to solve an open problem that has seen little
development since 2010. The first is to reduce the dependence on the covering
number. The second is to remove the dependence on the hypothesis space. We
present several examples, including ones for lasso and deep learning, in which
our bounds are provably preferable. The experiments on real-world data and
theoretical models demonstrate near-exponential improvements in various
situations. To achieve these improvements, we do not require additional
assumptions on the unknown distribution; instead, we only incorporate an
observable and computable property of the training samples. A key technical
innovation is an improved concentration bound for multinomial random variables
that is of independent interest beyond robustness and generalization.",None,-1
A Deep Learning Anomaly Detection Method in Textual Data,0.0152726,"In this article, we propose using deep learning and transformer architectures
combined with classical machine learning algorithms to detect and identify text
anomalies in texts. Deep learning model provides a very crucial context
information about the textual data which all textual context are converted to a
numerical representation. We used multiple machine learning methods such as
Sentence Transformers, Auto Encoders, Logistic Regression and Distance
calculation methods to predict anomalies. The method are tested on the texts
data and we used syntactic data from different source injected into the
original text as anomalies or use them as target. Different methods and
algorithm are explained in the field of outlier detection and the results of
the best technique is presented. These results suggest that our algorithm could
potentially reduce false positive rates compared with other anomaly detection
methods that we are testing.",None,-1
Deep reinforced active learning for multi-class image classification,0.0423068,"High accuracy medical image classification can be limited by the costs of
acquiring more data as well as the time and expertise needed to label existing
images. In this paper, we apply active learning to medical image
classification, a method which aims to maximise model performance on a minimal
subset from a larger pool of data. We present a new active learning framework,
based on deep reinforcement learning, to learn an active learning query
strategy to label images based on predictions from a convolutional neural
network. Our framework modifies the deep-Q network formulation, allowing us to
pick data based additionally on geometric arguments in the latent space of the
classifier, allowing for high accuracy multi-class classification in a
batch-based active learning setting, enabling the agent to label datapoints
that are both diverse and about which it is most uncertain. We apply our
framework to two medical imaging datasets and compare with standard query
strategies as well as the most recent reinforcement learning based active
learning approach for image classification.",https://github.com/cosmic-cortex/modAL,-1
Small Batch Sizes Improve Training of Low-Resource Neural MT,0.016753,"We study the role of an essential hyper-parameter that governs the training
of Transformers for neural machine translation in a low-resource setting: the
batch size. Using theoretical insights and experimental evidence, we argue
against the widespread belief that batch size should be set as large as allowed
by the memory of the GPUs. We show that in a low-resource setting, a smaller
batch size leads to higher scores in a shorter training time, and argue that
this is due to better regularization of the gradients during training.",https://github.com/google/sentencepiece,-1
Humans disagree with the IoU for measuring object detector localization error,0.0714625,"The localization quality of automatic object detectors is typically evaluated
by the Intersection over Union (IoU) score. In this work, we show that humans
have a different view on localization quality. To evaluate this, we conduct a
survey with more than 70 participants. Results show that for localization
errors with the exact same IoU score, humans might not consider that these
errors are equal, and express a preference. Our work is the first to evaluate
IoU with humans and makes it clear that relying on IoU scores alone to evaluate
localization errors might not be sufficient.",https://github.com/ombretta/humans_vs_IoU,-1
MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification,0.0243074,"This article presents a dataset of 10,917 news articles with hierarchical
news categories collected between 1 January 2019 and 31 December 2019. We
manually labeled the articles based on a hierarchical taxonomy with 17
first-level and 109 second-level categories. This dataset can be used to train
machine learning models for automatically classifying news articles by topic.
This dataset can be helpful for researchers working on news structuring,
classification, and predicting future events based on released news.",https://github.com/alinapetukhova/mn-ds-news-classiﬁcation,-1
PREF: Predictability Regularized Neural Motion Fields,0.0167775,"Knowing the 3D motions in a dynamic scene is essential to many vision
applications. Recent progress is mainly focused on estimating the activity of
some specific elements like humans. In this paper, we leverage a neural motion
field for estimating the motion of all points in a multiview setting. Modeling
the motion from a dynamic scene with multiview data is challenging due to the
ambiguities in points of similar color and points with time-varying color. We
propose to regularize the estimated motion to be predictable. If the motion
from previous frames is known, then the motion in the near future should be
predictable. Therefore, we introduce a predictability regularization by first
conditioning the estimated motion on latent embeddings, then by adopting a
predictor network to enforce predictability on the embeddings. The proposed
framework PREF (Predictability REgularized Fields) achieves on par or better
results than state-of-the-art neural motion field-based dynamic scene
representation methods, while requiring no prior knowledge of the scene.",None,-1
Momentum Diminishes the Effect of Spectral Bias in Physics-Informed Neural Networks,0.073023,"Physics-informed neural network (PINN) algorithms have shown promising
results in solving a wide range of problems involving partial differential
equations (PDEs). However, they often fail to converge to desirable solutions
when the target function contains high-frequency features, due to a phenomenon
known as spectral bias. In the present work, we exploit neural tangent kernels
(NTKs) to investigate the training dynamics of PINNs evolving under stochastic
gradient descent with momentum (SGDM). This demonstrates SGDM significantly
reduces the effect of spectral bias. We have also examined why training a model
via the Adam optimizer can accelerate the convergence while reducing the
spectral bias. Moreover, our numerical experiments have confirmed that
wide-enough networks using SGDM still converge to desirable solutions, even in
the presence of high-frequency features. In fact, we show that the width of a
network plays a critical role in convergence.",None,-1
Aspect-based Analysis of Advertising Appeals for Search Engine Advertising,0.0120162,"Writing an ad text that attracts people and persuades them to click or act is
essential for the success of search engine advertising. Therefore, ad creators
must consider various aspects of advertising appeals (A$^3$) such as the price,
product features, and quality. However, products and services exhibit unique
effective A$^3$ for different industries. In this work, we focus on exploring
the effective A$^3$ for different industries with the aim of assisting the ad
creation process. To this end, we created a dataset of advertising appeals and
used an existing model that detects various aspects for ad texts. Our
experiments demonstrated that different industries have their own effective
A$^3$ and that the identification of the A$^3$ contributes to the estimation of
advertising performance.",None,-1
Tailoring Self-Supervision for Supervised Learning,0.0277754,"Recently, it is shown that deploying a proper self-supervision is a
prospective way to enhance the performance of supervised learning. Yet, the
benefits of self-supervision are not fully exploited as previous pretext tasks
are specialized for unsupervised representation learning. To this end, we begin
by presenting three desirable properties for such auxiliary tasks to assist the
supervised objective. First, the tasks need to guide the model to learn rich
features. Second, the transformations involved in the self-supervision should
not significantly alter the training distribution. Third, the tasks are
preferred to be light and generic for high applicability to prior arts.
Subsequently, to show how existing pretext tasks can fulfill these and be
tailored for supervised learning, we propose a simple auxiliary
self-supervision task, predicting localizable rotation (LoRot). Our exhaustive
experiments validate the merits of LoRot as a pretext task tailored for
supervised learning in terms of robustness and generalization capability. Our
code is available at https://github.com/wjun0830/Localizable-Rotation.",https://github.com/wjun0830/Localizable-Rotation,-1
Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension,0.0666145,"Procedural Multimodal Documents (PMDs) organize textual instructions and
corresponding images step by step. Comprehending PMDs and inducing their
representations for the downstream reasoning tasks is designated as Procedural
MultiModal Machine Comprehension (M3C). In this study, we approach Procedural
M3C at a fine-grained level (compared with existing explorations at a document
or sentence level), that is, entity. With delicate consideration, we model
entity both in its temporal and cross-modal relation and propose a novel
Temporal-Modal Entity Graph (TMEG). Specifically, graph structure is formulated
to capture textual and visual entities and trace their temporal-modal
evolution. In addition, a graph aggregation module is introduced to conduct
graph encoding and reasoning. Comprehensive experiments across three Procedural
M3C tasks are conducted on a traditional dataset RecipeQA and our new dataset
CraftQA, which can better evaluate the generalization of TMEG.",None,-1
Unimodal and Multimodal Representation Training for Relation Extraction,0.015642,"Multimodal integration of text, layout and visual information has achieved
SOTA results in visually rich document understanding (VrDU) tasks, including
relation extraction (RE). However, despite its importance, evaluation of the
relative predictive capacity of these modalities is less prevalent. Here, we
demonstrate the value of shared representations for RE tasks by conducting
experiments in which each data type is iteratively excluded during training. In
addition, text and layout data are evaluated in isolation. While a bimodal text
and layout approach performs best (F1=0.684), we show that text is the most
important single predictor of entity relations. Additionally, layout geometry
is highly predictive and may even be a feasible unimodal approach. Despite
being less effective, we highlight circumstances where visual information can
bolster performance. In total, our results demonstrate the efficacy of training
joint representations for RE.",https://github.com/doc-analysis/XFUND,-1
Keyword Extraction in Scientific Documents,0.0570335,"The scientific publication output grows exponentially. Therefore, it is
increasingly challenging to keep track of trends and changes. Understanding
scientific documents is an important step in downstream tasks such as knowledge
graph building, text mining, and discipline classification. In this workshop,
we provide a better understanding of keyword and keyphrase extraction from the
abstract of scientific publications.",None,-1
Unsupervised Sign Language Phoneme Clustering using HamNoSys Notation,0.0676933,"Traditionally, sign language resources have been collected in controlled
settings for specific tasks involving supervised sign classification or
linguistic studies accompanied by specific annotation type. To date, very few
who explored signing videos found online on social media platforms as well as
the use of unsupervised methods applied to such resources. Due to the fact that
the field is striving to achieve acceptable model performance on the data that
differs from that seen during training calls for more diversity in sign
language data, stepping away from the data obtained in controlled laboratory
settings. Moreover, since the sign language data collection and annotation
carries large overheads, it is desirable to accelerate the annotation process.
Considering the aforementioned tendencies, this paper takes the side of
harvesting online data in a pursuit for automatically generating and annotating
sign language corpora through phoneme clustering.",None,-1
Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue,0.151356,"Embodied dialogue instruction following requires an agent to complete a
complex sequence of tasks from a natural language exchange. The recent
introduction of benchmarks (Padmakumar et al., 2022) raises the question of how
best to train and evaluate models for this multi-turn, multi-agent,
long-horizon task. This paper contributes to that conversation, by arguing that
imitation learning (IL) and related low-level metrics are actually misleading
and do not align with the goals of embodied dialogue research and may hinder
progress. We provide empirical comparisons of metrics, analysis of three
models, and make suggestions for how the field might best progress. First, we
observe that models trained with IL take spurious actions during evaluation.
Second, we find that existing models fail to ground query utterances, which are
essential for task completion. Third, we argue evaluation should focus on
higher-level semantic goals.",https://github.com/soyeonm/TEACh_FILM,-1
A lightweight Transformer-based model for fish landmark detection,0.0139507,"Transformer-based models, such as the Vision Transformer (ViT), can
outperform onvolutional Neural Networks (CNNs) in some vision tasks when there
is sufficient training data. However, (CNNs) have a strong and useful inductive
bias for vision tasks (i.e. translation equivariance and locality). In this
work, we developed a novel model architecture that we call a Mobile fish
landmark detection network (MFLD-net). We have made this model using
convolution operations based on ViT (i.e. Patch embeddings, Multi-Layer
Perceptrons). MFLD-net can achieve competitive or better results in low data
regimes while being lightweight and therefore suitable for embedded and mobile
devices. Furthermore, we show that MFLD-net can achieve keypoint (landmark)
estimation accuracies on-par or even better than some of the state-of-the-art
(CNNs) on a fish image dataset. Additionally, unlike ViT, MFLD-net does not
need a pre-trained model and can generalise well when trained on a small
dataset. We provide quantitative and qualitative results that demonstrate the
model's generalisation capabilities. This work will provide a foundation for
future efforts in developing mobile, but efficient fish monitoring systems and
devices.",None,-1
Improving abstractive summarization with energy-based re-ranking,0.0624546,"Current abstractive summarization systems present important weaknesses which
prevent their deployment in real-world applications, such as the omission of
relevant information and the generation of factual inconsistencies (also known
as hallucinations). At the same time, automatic evaluation metrics such as CTC
scores have been recently proposed that exhibit a higher correlation with human
judgments than traditional lexical-overlap metrics such as ROUGE. In this work,
we intend to close the loop by leveraging the recent advances in summarization
metrics to create quality-aware abstractive summarizers. Namely, we propose an
energy-based model that learns to re-rank summaries according to one or a
combination of these metrics. We experiment using several metrics to train our
energy-based re-ranker and show that it consistently improves the scores
achieved by the predicted summaries. Nonetheless, human evaluation results show
that the re-ranking approach should be used with care for highly abstractive
summaries, as the available metrics are not yet sufficiently reliable for this
purpose.",https://github.com/Priberam/SummEBR,-1
Norm of Word Embedding Encodes Information Gain,0.0101156,"Distributed representations of words encode lexical semantic information, but
what type of information is encoded and how? Focusing on the skip-gram with
negative-sampling method, we found that the squared norm of static word
embedding encodes the information gain conveyed by the word; the information
gain is defined by the Kullback-Leibler divergence of the co-occurrence
distribution of the word to the unigram distribution. Our findings are
explained by the theoretical framework of the exponential family of probability
distributions and confirmed through precise experiments that remove spurious
correlations arising from word frequency. This theory also extends to
contextualized word embeddings in language models or any neural networks with
the softmax output layer. We also demonstrate that both the KL divergence and
the squared norm of embedding provide a useful metric of the informativeness of
a word in tasks such as keyword extraction, proper-noun discrimination, and
hypernym discrimination.",None,-1
Target-Guided Open-Domain Conversation Planning,0.121731,"Prior studies addressing target-oriented conversational tasks lack a crucial
notion that has been intensively studied in the context of goal-oriented
artificial intelligence agents, namely, planning. In this study, we propose the
task of Target-Guided Open-Domain Conversation Planning (TGCP) task to evaluate
whether neural conversational agents have goal-oriented conversation planning
abilities. Using the TGCP task, we investigate the conversation planning
abilities of existing retrieval models and recent strong generative models. The
experimental results reveal the challenges facing current technology.",https://github.com/y-kishinami/TGCP,-1
COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation,0.027543,"As language models become increasingly integrated into our digital lives,
Personalized Text Generation (PTG) has emerged as a pivotal component with a
wide range of applications. However, the bias inherent in user written text,
often used for PTG model training, can inadvertently associate different levels
of linguistic quality with users' protected attributes. The model can inherit
the bias and perpetuate inequality in generating text w.r.t. users' protected
attributes, leading to unfair treatment when serving users. In this work, we
investigate fairness of PTG in the context of personalized explanation
generation for recommendations. We first discuss the biases in generated
explanations and their fairness implications. To promote fairness, we introduce
a general framework to achieve measure-specific counterfactual fairness in
explanation generation. Extensive experiments and human evaluations demonstrate
the effectiveness of our method.",None,-1
Understanding Long Documents with Different Position-Aware Attentions,0.199636,"Despite several successes in document understanding, the practical task for
long document understanding is largely under-explored due to several challenges
in computation and how to efficiently absorb long multimodal input. Most
current transformer-based approaches only deal with short documents and employ
solely textual information for attention due to its prohibitive computation and
memory limit. To address those issues in long document understanding, we
explore different approaches in handling 1D and new 2D position-aware attention
with essentially shortened context. Experimental results show that our proposed
models have advantages for this task based on various evaluation metrics.
Furthermore, our model makes changes only to the attention and thus can be
easily adapted to any transformer-based architecture.",https://github.com/applicaai/kleister-nda,-1
Huqariq: A Multilingual Speech Corpus of Native Languages of Peru for Speech Recognition,0.0264486,"The Huqariq corpus is a multilingual collection of speech from native
Peruvian languages. The transcribed corpus is intended for the research and
development of speech technologies to preserve endangered languages in Peru.
Huqariq is primarily designed for the development of automatic speech
recognition, language identification and text-to-speech tools. In order to
achieve corpus collection sustainably, we employ the crowdsourcing methodology.
Huqariq includes four native languages of Peru, and it is expected that by the
end of the year 2022, it can reach up to 20 native languages out of the 48
native languages in Peru. The corpus has 220 hours of transcribed audio
recorded by more than 500 volunteers, making it the largest speech corpus for
native languages in Peru. In order to verify the quality of the corpus, we
present speech recognition experiments using 220 hours of fully transcribed
audio.",None,-1
ResNeRF: Geometry-Guided Residual Neural Radiance Field for Indoor Scene Novel View Synthesis,0.0670413,"We represent the ResNeRF, a novel geometry-guided two-stage framework for
indoor scene novel view synthesis. Be aware of that a good geometry would
greatly boost the performance of novel view synthesis, and to avoid the
geometry ambiguity issue, we propose to characterize the density distribution
of the scene based on a base density estimated from scene geometry and a
residual density parameterized by the geometry. In the first stage, we focus on
geometry reconstruction based on SDF representation, which would lead to a good
geometry surface of the scene and also a sharp density. In the second stage,
the residual density is learned based on the SDF learned in the first stage for
encoding more details about the appearance. In this way, our method can better
learn the density distribution with the geometry prior for high-fidelity novel
view synthesis while preserving the 3D structures. Experiments on large-scale
indoor scenes with many less-observed and textureless areas show that with the
good 3D surface, our method achieves state-of-the-art performance for novel
view synthesis.",None,-1
INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold,0.126674,"Binary Neural Networks (BNNs) have emerged as a promising solution for
reducing the memory footprint and compute costs of deep neural networks, but
they suffer from quality degradation due to the lack of freedom as activations
and weights are constrained to the binary values. To compensate for the
accuracy drop, we propose a novel BNN design called Binary Neural Network with
INSTAnce-aware threshold (INSTA-BNN), which controls the quantization threshold
dynamically in an input-dependent or instance-aware manner. According to our
observation, higher-order statistics can be a representative metric to estimate
the characteristics of the input distribution. INSTA-BNN is designed to adjust
the threshold dynamically considering various information, including
higher-order statistics, but it is also optimized judiciously to realize
minimal overhead on a real device. Our extensive study shows that INSTA-BNN
outperforms the baseline by 3.0% and 2.8% on the ImageNet classification task
with comparable computing cost, achieving 68.5% and 72.2% top-1 accuracy on
ResNet-18 and MobileNetV1 based models, respectively.",None,-1
Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask,0.0700979,"Reading is integral to everyday life, and yet learning to read is a struggle
for many young learners. During lessons, teachers can use comprehension
questions to increase engagement, test reading skills, and improve retention.
Historically such questions were written by skilled teachers, but recently
language models have been used to generate comprehension questions. However,
many existing Question Generation (QG) systems focus on generating literal
questions from the text, and have no way to control the type of the generated
question. In this paper, we study QG for reading comprehension where
inferential questions are critical and extractive techniques cannot be used. We
propose a two-step model (HTA-WTA) that takes advantage of previous datasets,
and can generate questions for a specific targeted comprehension skill. We
propose a new reading comprehension dataset that contains questions annotated
with story-based reading comprehension skills (SBRCS), allowing for a more
complete reader assessment. Across several experiments, our results show that
HTA-WTA outperforms multiple strong baselines on this new dataset. We show that
the HTA-WTA model tests for strong SCRS by asking deep inferential questions.",https://github.com/seanie12/neural-question-generation,-1
ULDGNN: A Fragmented UI Layer Detector Based on Graph Neural Networks,0.00841263,"While some work attempt to generate front-end code intelligently from UI
screenshots, it may be more convenient to utilize UI design drafts in Sketch
which is a popular UI design software, because we can access multimodal UI
information directly such as layers type, position, size, and visual images.
However, fragmented layers could degrade the code quality without being merged
into a whole part if all of them are involved in the code generation. In this
paper, we propose a pipeline to merge fragmented layers automatically. We first
construct a graph representation for the layer tree of a UI draft and detect
all fragmented layers based on the visual features and graph neural networks.
Then a rule-based algorithm is designed to merge fragmented layers. Through
experiments on a newly constructed dataset, our approach can retrieve most
fragmented layers in UI design drafts, and achieve 87% accuracy in the
detection task, and the post-processing algorithm is developed to cluster
associative layers under simple and general circumstances.",None,-1
Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning,0.00847642,"The rapidly-developing intersection of machine learning (ML) with high-energy
physics (HEP) presents both opportunities and challenges to our community. Far
beyond applications of standard ML tools to HEP problems, genuinely new and
potentially revolutionary approaches are being developed by a generation of
talent literate in both fields. There is an urgent need to support the needs of
the interdisciplinary community driving these developments, including funding
dedicated research at the intersection of the two fields, investing in
high-performance computing at universities and tailoring allocation policies to
support this work, developing of community tools and standards, and providing
education and career paths for young researchers attracted by the intellectual
vitality of machine learning for high energy physics.",None,-1
FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction,0.0119359,"This paper presents FAMIE, a comprehensive and efficient active learning (AL)
toolkit for multilingual information extraction. FAMIE is designed to address a
fundamental problem in existing AL frameworks where annotators need to wait for
a long time between annotation batches due to the time-consuming nature of
model training and data selection at each AL iteration. This hinders the
engagement, productivity, and efficiency of annotators. Based on the idea of
using a small proxy network for fast data selection, we introduce a novel
knowledge distillation mechanism to synchronize the proxy network with the main
large model (i.e., BERT-based) to ensure the appropriateness of the selected
annotation examples for the main model. Our AL framework can support multiple
languages. The experiments demonstrate the advantages of FAMIE in terms of
competitive performance and time efficiency for sequence labeling with AL. We
publicly release our code (\url{https://github.com/nlp-uoregon/famie}) and demo
website (\url{http://nlp.uoregon.edu:9000/}). A demo video for FAMIE is
provided at: \url{https://youtu.be/I2i8n_jAyrY}.",https://github.com/nlp-uoregon/famie,-1
COEM: Cross-Modal Embedding for MetaCell Identification,0.64459,"Metacells are disjoint and homogeneous groups of single-cell profiles,
representing discrete and highly granular cell states. Existing metacell
algorithms tend to use only one modality to infer metacells, even though
single-cell multi-omics datasets profile multiple molecular modalities within
the same cell. Here, we present \textbf{C}ross-M\textbf{O}dal
\textbf{E}mbedding for \textbf{M}etaCell Identification (COEM), which utilizes
an embedded space leveraging the information of both scATAC-seq and scRNA-seq
to perform aggregation, balancing the trade-off between fine resolution and
sufficient sequencing coverage. COEM outperforms the state-of-the-art method
SEACells by efficiently identifying accurate and well-separated metacells
across datasets with continuous and discrete cell types. Furthermore, COEM
significantly improves peak-to-gene association analyses, and facilitates
complex gene regulatory inference tasks.",None,-1
Hybrid Reinforced Medical Report Generation with M-Linear Attention and Repetition Penalty,0.0567815,"To reduce doctors' workload, deep-learning-based automatic medical report
generation has recently attracted more and more research efforts, where deep
convolutional neural networks (CNNs) are employed to encode the input images,
and recurrent neural networks (RNNs) are used to decode the visual features
into medical reports automatically. However, these state-of-the-art methods
mainly suffer from three shortcomings: (i) incomprehensive optimization, (ii)
low-order and unidimensional attention mechanisms, and (iii) repeated
generation. In this article, we propose a hybrid reinforced medical report
generation method with m-linear attention and repetition penalty mechanism
(HReMRG-MR) to overcome these problems. Specifically, a hybrid reward with
different weights is employed to remedy the limitations of single-metric-based
rewards. We also propose a search algorithm with linear complexity to
approximate the best weight combination. Furthermore, we use m-linear attention
modules to explore high-order feature interactions and to achieve multi-modal
reasoning, while a repetition penalty applies penalties to repeated terms
during the model's training process. Extensive experimental studies on two
public datasets show that HReMRG-MR greatly outperforms the state-of-the-art
baselines in terms of all metrics. We also conducted a series of ablation
experiments to prove the effectiveness of all our proposed components. We also
performed a reward search toy experiment to give evidence that our proposed
search approach can significantly reduce the search time while approximating
the best performance.",None,-1
Point Cloud Generation with Continuous Conditioning,0.105104,"Generative models can be used to synthesize 3D objects of high quality and
diversity. However, there is typically no control over the properties of the
generated object.This paper proposes a novel generative adversarial network
(GAN) setup that generates 3D point cloud shapes conditioned on a continuous
parameter. In an exemplary application, we use this to guide the generative
process to create a 3D object with a custom-fit shape. We formulate this
generation process in a multi-task setting by using the concept of auxiliary
classifier GANs. Further, we propose to sample the generator label input for
training from a kernel density estimation (KDE) of the dataset. Our ablations
show that this leads to significant performance increase in regions with few
samples. Extensive quantitative and qualitative experiments show that we gain
explicit control over the object dimensions while maintaining good generation
quality and diversity.",https://github.com/seowok/TreeGAN,-1
REKnow: Enhanced Knowledge for Joint Entity and Relation Extraction,0.950213,"Relation extraction is an important but challenging task that aims to extract
all hidden relational facts from the text. With the development of deep
language models, relation extraction methods have achieved good performance on
various benchmarks. However, we observe two shortcomings of previous methods:
first, there is no unified framework that works well under various relation
extraction settings; second, effectively utilizing external knowledge as
background information is absent. In this work, we propose a knowledge-enhanced
generative model to mitigate these two issues. Our generative model is a
unified framework to sequentially generate relational triplets under various
relation extraction settings and explicitly utilizes relevant knowledge from
Knowledge Graph (KG) to resolve ambiguities. Our model achieves superior
performance on multiple benchmarks and settings, including WebNLG, NYT10, and
TACRED.",https://github.com/wikimedia/pywikibot,-1
Attention-based Feature Compression for CNN Inference Offloading in Edge Computing,0.111237,"This paper studies the computational offloading of CNN inference in
device-edge co-inference systems. Inspired by the emerging paradigm semantic
communication, we propose a novel autoencoder-based CNN architecture (AECNN),
for effective feature extraction at end-device. We design a feature compression
module based on the channel attention method in CNN, to compress the
intermediate data by selecting the most important features. To further reduce
communication overhead, we can use entropy encoding to remove the statistical
redundancy in the compressed data. At the receiver, we design a lightweight
decoder to reconstruct the intermediate data through learning from the received
compressed data to improve accuracy. To fasten the convergence, we use a
step-by-step approach to train the neural networks obtained based on ResNet-50
architecture. Experimental results show that AECNN can compress the
intermediate data by more than 256x with only about 4% accuracy loss, which
outperforms the state-of-the-art work, BottleNet++. Compared to offloading
inference task directly to edge server, AECNN can complete inference task
earlier, in particular, under poor wireless channel condition, which highlights
the effectiveness of AECNN in guaranteeing higher accuracy within time
constraint.",None,-1
Summarizing a virtual robot's past actions in natural language,0.0935875,"We propose and demonstrate the task of giving natural language summaries of
the actions of a robotic agent in a virtual environment. We explain why such a
task is important, what makes it difficult, and discuss how it might be
addressed. To encourage others to work on this, we show how a popular existing
dataset that matches robot actions with natural language descriptions designed
for an instruction following task can be repurposed to serve as a training
ground for robot action summarization work. We propose and test several methods
of learning to generate such summaries, starting from either egocentric video
frames of the robot taking actions or intermediate text representations of the
actions used by an automatic planner. We provide quantitative and qualitative
evaluations of our results, which can serve as a baseline for future work.",https://github.com/askforalfred/alfred,-1
Automatic Severity Classification of Dysarthric speech by using Self-supervised Model with Multi-task Learning,0.0426828,"Automatic assessment of dysarthric speech is essential for sustained
treatments and rehabilitation. However, obtaining atypical speech is
challenging, often leading to data scarcity issues. To tackle the problem, we
propose a novel automatic severity assessment method for dysarthric speech,
using the self-supervised model in conjunction with multi-task learning.
Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity
classification and auxiliary automatic speech recognition (ASR). For the
baseline experiments, we employ hand-crafted acoustic features and machine
learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean
dysarthric speech QoLT database, our model outperforms the traditional baseline
methods, with a relative percentage increase of 1.25% for F1-score. In
addition, the proposed model surpasses the model trained without ASR head,
achieving 10.61% relative percentage improvements. Furthermore, we present how
multi-task learning affects the severity classification performance by
analyzing the latent representations and regularization effect.",https://github.com/juice500ml/dysarthria-mtl,654
A Reference Data Model for Process-Related User Interaction Logs,0.1161,"User interaction (UI) logs are high-resolution event logs that record
low-level activities performed by a user during the execution of a task in an
information system. Each event in a UI log corresponds to a single interaction
between the user and the interface, such as clicking a button or entering a
string into a text field. UI logs are used for purposes like task mining or
robotic process automation (RPA), but each study and tool relies on a different
conceptualization and implementation of the elements and attributes that
constitute user interactions. This lack of standardization makes it difficult
to integrate UI logs from different sources and to combine tools for UI data
collection with downstream analytics or automation solutions. To address this,
we propose a universally applicable reference data model for process-related UI
logs. Based on a review of scientific literature and industry solutions, this
model includes the core attributes of UI logs, but remains flexible with regard
to the scope, level of abstraction, and case notion. We provide an
implementation of the model as an extension to the XES interchange standard for
event logs and demonstrate its practical applicability in a real-life RPA
scenario.",None,1102
Atypical lexical abbreviations identification in Russian medical texts,0.00780499,"Abbreviation is a method of word formation that aims to construct the
shortened term from the first letters of the initial phrase. Implicit
abbreviations frequently cause the comprehension difficulties for unprepared
readers. In this paper, we propose an efficient ML-based algorithm which allows
to identify the abbreviations in Russian texts. The method achieves ROC AUC
score 0.926 and F1 score 0.706 which are confirmed as competitive in comparison
with the baselines. Along with the pipeline, we also establish first to our
knowledge Russian dataset that is relevant for the desired task.",https://github.com/aberdichevskaya/abbreviation-identification,-1
Semiconductor Defect Pattern Classification by Self-Proliferation-and-Attention Neural Network,0.0740846,"Semiconductor manufacturing is on the cusp of a revolution: the Internet of
Things (IoT). With IoT we can connect all the equipment and feed information
back to the factory so that quality issues can be detected. In this situation,
more and more edge devices are used in wafer inspection equipment. This edge
device must have the ability to quickly detect defects. Therefore, how to
develop a high-efficiency architecture for automatic defect classification to
be suitable for edge devices is the primary task. In this paper, we present a
novel architecture that can perform defect classification in a more efficient
way. The first function is self-proliferation, using a series of linear
transformations to generate more feature maps at a cheaper cost. The second
function is self-attention, capturing the long-range dependencies of feature
map by the channel-wise and spatial-wise attention mechanism. We named this
method as self-proliferation-and-attention neural network. This method has been
successfully applied to various defect pattern classification tasks. Compared
with other latest methods, SP&A-Net has higher accuracy and lower computation
cost in many defect inspection tasks.",https://github.com/Yfyangd/SPA,7483
"""Covid vaccine is against Covid but Oxford vaccine is made at Oxford!"" Semantic Interpretation of Proper Noun Compounds",0.0241587,"Proper noun compounds, e.g., ""Covid vaccine"", convey information in a
succinct manner (a ""Covid vaccine"" is a ""vaccine that immunizes against the
Covid disease""). These are commonly used in short-form domains, such as news
headlines, but are largely ignored in information-seeking applications. To
address this limitation, we release a new manually annotated dataset, ProNCI,
consisting of 22.5K proper noun compounds along with their free-form semantic
interpretations. ProNCI is 60 times larger than prior noun compound datasets
and also includes non-compositional examples, which have not been previously
explored. We experiment with various neural models for automatically generating
the semantic interpretations from proper noun compounds, ranging from few-shot
prompting to supervised learning, with varying degrees of knowledge about the
constituent nouns. We find that adding targeted knowledge, particularly about
the common noun, results in performance gains of upto 2.8%. Finally, we
integrate our model generated interpretations with an existing Open IE system
and observe an 7.5% increase in yield at a precision of 85%. The dataset and
code are available at https://github.com/dair-iitd/pronci.",https://github.com/dair-iitd/pronci,3100
Socially Intelligent Genetic Agents for the Emergence of Explicit Norms,0.0441994,"Norms help regulate a society. Norms may be explicit (represented in
structured form) or implicit. We address the emergence of explicit norms by
developing agents who provide and reason about explanations for norm violations
in deciding sanctions and identifying alternative norms. These agents use a
genetic algorithm to produce norms and reinforcement learning to learn the
values of these norms. We find that applying explanations leads to norms that
provide better cohesion and goal satisfaction for the agents. Our results are
stable for societies with differing attitudes of generosity.",https://github.com/niravajmeri/,31895
RWT-SLAM: Robust Visual SLAM for Highly Weak-textured Environments,0.0210986,"As a fundamental task for intelligent robots, visual SLAM has made great
progress over the past decades. However, robust SLAM under highly weak-textured
environments still remains very challenging. In this paper, we propose a novel
visual SLAM system named RWT-SLAM to tackle this problem. We modify LoFTR
network which is able to produce dense point matching under low-textured scenes
to generate feature descriptors. To integrate the new features into the popular
ORB-SLAM framework, we develop feature masks to filter out the unreliable
features and employ KNN strategy to strengthen the matching robustness. We also
retrained visual vocabulary upon new descriptors for efficient loop closing.
The resulting RWT-SLAM is tested in various public datasets such as TUM and
OpenLORIS, as well as our own data. The results shows very promising
performance under highly weak-textured environments.",None,1155
Benchmark datasets driving artificial intelligence development fail to capture the needs of medical professionals,0.05609,"Publicly accessible benchmarks that allow for assessing and comparing model
performances are important drivers of progress in artificial intelligence (AI).
While recent advances in AI capabilities hold the potential to transform
medical practice by assisting and augmenting the cognitive processes of
healthcare professionals, the coverage of clinically relevant tasks by AI
benchmarks is largely unclear. Furthermore, there is a lack of systematized
meta-information that allows clinical AI researchers to quickly determine
accessibility, scope, content and other characteristics of datasets and
benchmark datasets relevant to the clinical domain.
  To address these issues, we curated and released a comprehensive catalogue of
datasets and benchmarks pertaining to the broad domain of clinical and
biomedical natural language processing (NLP), based on a systematic review of
literature and online resources. A total of 450 NLP datasets were manually
systematized and annotated with rich metadata, such as targeted tasks, clinical
applicability, data types, performance metrics, accessibility and licensing
information, and availability of data splits. We then compared tasks covered by
AI benchmark datasets with relevant tasks that medical practitioners reported
as highly desirable targets for automation in a previous empirical study.
  Our analysis indicates that AI benchmarks of direct clinical relevance are
scarce and fail to cover most work activities that clinicians want to see
addressed. In particular, tasks associated with routine documentation and
patient data administration workflows are not represented despite significant
associated workloads. Thus, currently available AI benchmarks are improperly
aligned with desired targets for AI automation in clinical settings, and novel
benchmarks should be created to fill these gaps.",None,12
Exploiting Global and Local Hierarchies for Hierarchical Text Classification,0.128213,"Hierarchical text classification aims to leverage label hierarchy in
multi-label text classification. Existing methods encode label hierarchy in a
global view, where label hierarchy is treated as the static hierarchical
structure containing all labels. Since global hierarchy is static and
irrelevant to text samples, it makes these methods hard to exploit hierarchical
information. Contrary to global hierarchy, local hierarchy as a structured
labels hierarchy corresponding to each text sample. It is dynamic and relevant
to text samples, which is ignored in previous methods. To exploit global and
local hierarchies,we propose Hierarchy-guided BERT with Global and Local
hierarchies (HBGL), which utilizes the large-scale parameters and prior
language knowledge of BERT to model both global and local
hierarchies.Moreover,HBGL avoids the intentional fusion of semantic and
hierarchical modules by directly modeling semantic and hierarchical information
with BERT.Compared with the state-of-the-art method HGCLR,our method achieves
significant improvement on three benchmark datasets.",http://github.com/kongds/,15937
Label-Driven Denoising Framework for Multi-Label Few-Shot Aspect Category Detection,0.139308,"Multi-Label Few-Shot Aspect Category Detection (FS-ACD) is a new sub-task of
aspect-based sentiment analysis, which aims to detect aspect categories
accurately with limited training instances. Recently, dominant works use the
prototypical network to accomplish this task, and employ the attention
mechanism to extract keywords of aspect category from the sentences to produce
the prototype for each aspect. However, they still suffer from serious noise
problems: (1) due to lack of sufficient supervised data, the previous methods
easily catch noisy words irrelevant to the current aspect category, which
largely affects the quality of the generated prototype; (2) the
semantically-close aspect categories usually generate similar prototypes, which
are mutually noisy and confuse the classifier seriously. In this paper, we
resort to the label information of each aspect to tackle the above problems,
along with proposing a novel Label-Driven Denoising Framework (LDF). Extensive
experimental results show that our framework achieves better performance than
other state-of-the-art methods.",https://github.com/1429904852/LDF,4048
Task-specific Compression for Multi-task Language Models using Attribution-based Pruning,0.022262,"Multi-task language models show outstanding performance for various natural
language understanding tasks with only a single model. However, these language
models utilize an unnecessarily large number of model parameters, even when
used only for a specific task. This paper proposes a novel training-free
compression method for multi-task language models using a pruning method.
Specifically, we use an attribution method to determine which neurons are
essential for performing a specific task. We task-specifically prune
unimportant neurons and leave only task-specific parameters. Furthermore, we
extend our method to be applicable in low-resource and unsupervised settings.
Since our compression method is training-free, it uses few computing resources
and does not destroy the pre-trained knowledge of language models. Experimental
results on the six widely-used datasets show that our proposed pruning method
significantly outperforms baseline pruning methods. In addition, we demonstrate
that our method preserves performance even in an unseen domain setting.",None,4966
Multi-View Dreaming: Multi-View World Model with Contrastive Learning,0.041571,"In this paper, we propose Multi-View Dreaming, a novel reinforcement learning
agent for integrated recognition and control from multi-view observations by
extending Dreaming. Most current reinforcement learning method assumes a
single-view observation space, and this imposes limitations on the observed
data, such as lack of spatial information and occlusions. This makes obtaining
ideal observational information from the environment difficult and is a
bottleneck for real-world robotics applications. In this paper, we use
contrastive learning to train a shared latent space between different
viewpoints, and show how the Products of Experts approach can be used to
integrate and control the probability distributions of latent states for
multiple viewpoints. We also propose Multi-View DreamingV2, a variant of
Multi-View Dreaming that uses a categorical distribution to model the latent
state instead of the Gaussian distribution. Experiments show that the proposed
method outperforms simple extensions of existing methods in a realistic robot
control task.",None,3802
Dual Progressive Transformations for Weakly Supervised Semantic Segmentation,0.0930372,"Weakly supervised semantic segmentation (WSSS), which aims to mine the object
regions by merely using class-level labels, is a challenging task in computer
vision. The current state-of-the-art CNN-based methods usually adopt
Class-Activation-Maps (CAMs) to highlight the potential areas of the object,
however, they may suffer from the part-activated issues. To this end, we try an
early attempt to explore the global feature attention mechanism of vision
transformer in WSSS task. However, since the transformer lacks the inductive
bias as in CNN models, it can not boost the performance directly and may yield
the over-activated problems. To tackle these drawbacks, we propose a
Convolutional Neural Networks Refined Transformer (CRT) to mine a globally
complete and locally accurate class activation maps in this paper. To validate
the effectiveness of our proposed method, extensive experiments are conducted
on PASCAL VOC 2012 and CUB-200-2011 datasets. Experimental evaluations show
that our proposed CRT achieves the new state-of-the-art performance on both the
weakly supervised semantic segmentation task the weakly supervised object
localization task, which outperform others by a large margin.",https://github.com/huodongjian0603/crt,5685
HyperNST: Hyper-Networks for Neural Style Transfer,0.0400282,"We present HyperNST; a neural style transfer (NST) technique for the artistic
stylization of images, based on Hyper-networks and the StyleGAN2 architecture.
Our contribution is a novel method for inducing style transfer parameterized by
a metric space, pre-trained for style-based visual search (SBVS). We show for
the first time that such space may be used to drive NST, enabling the
application and interpolation of styles from an SBVS system. The technical
contribution is a hyper-network that predicts weight updates to a StyleGAN2
pre-trained over a diverse gamut of artistic content (portraits), tailoring the
style parameterization on a per-region basis using a semantic map of the facial
regions. We show HyperNST to exceed state of the art in content preservation
for our stylized content while retaining good style transfer performance.",None,33989
Standing on the Shoulders of Giant Frozen Language Models,0.177088,"Huge pretrained language models (LMs) have demonstrated surprisingly good
zero-shot capabilities on a wide variety of tasks. This gives rise to the
appealing vision of a single, versatile model with a wide range of
functionalities across disparate applications. However, current leading
techniques for leveraging a ""frozen"" LM -- i.e., leaving its weights untouched
-- still often underperform fine-tuning approaches which modify these weights
in a task-dependent way. Those, in turn, suffer forgetfulness and compromise
versatility, suggesting a tradeoff between performance and versatility. The
main message of this paper is that current frozen-model techniques such as
prompt tuning are only the tip of the iceberg, and more powerful methods for
leveraging frozen LMs can do just as well as fine tuning in challenging domains
without sacrificing the underlying model's versatility. To demonstrate this, we
introduce three novel methods for leveraging frozen models: input-dependent
prompt tuning, frozen readers, and recursive LMs, each of which vastly improves
on current frozen-model approaches. Indeed, some of our methods even outperform
fine-tuning approaches in domains currently dominated by the latter. The
computational cost of each method is higher than that of existing frozen model
methods, but still negligible relative to a single pass through a huge frozen
LM. Each of these methods constitutes a meaningful contribution in its own
right, but by presenting these contributions together we aim to convince the
reader of a broader message that goes beyond the details of any given method:
that frozen models have untapped potential and that fine-tuning is often
unnecessary.",None,27124
NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs,0.0511849,"Complex query answering (CQA) is an essential task for multi-hop and logical
reasoning on knowledge graphs (KGs). Currently, most approaches are limited to
queries among binary relational facts and pay less attention to n-ary facts
(n>=2) containing more than two entities, which are more prevalent in the real
world. Moreover, previous CQA methods can only make predictions for a few given
types of queries and cannot be flexibly extended to more complex logical
queries, which significantly limits their applications. To overcome these
challenges, in this work, we propose a novel N-ary Query Embedding (NQE) model
for CQA over hyper-relational knowledge graphs (HKGs), which include massive
n-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder and
fuzzy logic theory to satisfy all n-ary FOL queries, including existential
quantifiers, conjunction, disjunction, and negation. We also propose a parallel
processing algorithm that can train or predict arbitrary n-ary FOL queries in a
single batch, regardless of the kind of each query, with good flexibility and
extensibility. In addition, we generate a new CQA dataset WD50K-NFOL, including
diverse n-ary FOL queries over WD50K. Experimental results on WD50K-NFOL and
other standard CQA datasets show that NQE is the state-of-the-art CQA method
over HKGs with good generalization capability. Our code and dataset are
publicly available.",https://github.com/LHRLAB/NQE,2442
Towards customizable reinforcement learning agents: Enabling preference specification through online vocabulary expansion,0.138389,"There is a growing interest in developing automated agents that can work
alongside humans. In addition to completing the assigned task, such an agent
will undoubtedly be expected to behave in a manner that is preferred by the
human. This requires the human to communicate their preferences to the agent.
To achieve this, the current approaches either require the users to specify the
reward function or the preference is interactively learned from queries that
ask the user to compare behavior. The former approach can be challenging if the
internal representation used by the agent is inscrutable to the human while the
latter is unnecessarily cumbersome for the user if their preference can be
specified more easily in symbolic terms. In this work, we propose PRESCA
(PREference Specification through Concept Acquisition), a system that allows
users to specify their preferences in terms of concepts that they understand.
PRESCA maintains a set of such concepts in a shared vocabulary. If the relevant
concept is not in the shared vocabulary, then it is learned. To make learning a
new concept more feedback efficient, PRESCA leverages causal associations
between the target concept and concepts that are already known. In addition, we
use a novel data augmentation approach to further reduce required feedback. We
evaluate PRESCA by using it on a Minecraft environment and show that it can
effectively align the agent with the user's preference.",None,17546
Defending From Physically-Realizable Adversarial Attacks Through Internal Over-Activation Analysis,0.0831078,"This work presents Z-Mask, a robust and effective strategy to improve the
adversarial robustness of convolutional networks against physically-realizable
adversarial attacks. The presented defense relies on specific Z-score analysis
performed on the internal network features to detect and mask the pixels
corresponding to adversarial objects in the input image. To this end, spatially
contiguous activations are examined in shallow and deep layers to suggest
potential adversarial regions. Such proposals are then aggregated through a
multi-thresholding mechanism. The effectiveness of Z-Mask is evaluated with an
extensive set of experiments carried out on models for both semantic
segmentation and object detection. The evaluation is performed with both
digital patches added to the input images and printed patches positioned in the
real world. The obtained results confirm that Z-Mask outperforms the
state-of-the-art methods in terms of both detection accuracy and overall
performance of the networks under attack. Additional experiments showed that
Z-Mask is also robust against possible defense-aware attacks.",None,24075
Motion Prediction via Joint Dependency Modeling in Phase Space,0.0941315,"Motion prediction is a classic problem in computer vision, which aims at
forecasting future motion given the observed pose sequence. Various deep
learning models have been proposed, achieving state-of-the-art performance on
motion prediction. However, existing methods typically focus on modeling
temporal dynamics in the pose space. Unfortunately, the complicated and high
dimensionality nature of human motion brings inherent challenges for dynamic
context capturing. Therefore, we move away from the conventional pose based
representation and present a novel approach employing a phase space trajectory
representation of individual joints. Moreover, current methods tend to only
consider the dependencies between physically connected joints. In this paper,
we introduce a novel convolutional neural model to effectively leverage
explicit prior knowledge of motion anatomy, and simultaneously capture both
spatial and temporal information of joint trajectory dynamics. We then propose
a global optimization module that learns the implicit relationships between
individual joint features.
  Empirically, our method is evaluated on large-scale 3D human motion benchmark
datasets (i.e., Human3.6M, CMU MoCap). These results demonstrate that our
method sets the new state-of-the-art on the benchmark datasets. Our code will
be available at https://github.com/Pose-Group/TEID.",https://github.com/Pose-Group/TEID,3258
"For Learning in Symmetric Teams, Local Optima are Global Nash Equilibria",0.01963,"Although it has been known since the 1970s that a globally optimal strategy
profile in a common-payoff game is a Nash equilibrium, global optimality is a
strict requirement that limits the result's applicability. In this work, we
show that any locally optimal symmetric strategy profile is also a (global)
Nash equilibrium. Furthermore, we show that this result is robust to
perturbations to the common payoff and to the local optimum. Applied to machine
learning, our result provides a global guarantee for any gradient method that
finds a local optimum in symmetric strategy space. While this result indicates
stability to unilateral deviation, we nevertheless identify broad classes of
games where mixed local optima are unstable under joint, asymmetric deviations.
We analyze the prevalence of instability by running learning algorithms in a
suite of symmetric games, and we conclude by discussing the applicability of
our results to multi-agent RL, cooperative inverse RL, and decentralized
POMDPs.",https://github.com/scottemmons/coordination,117510
Will Large-scale Generative Models Corrupt Future Datasets?,0.194836,"Recently proposed large-scale text-to-image generative models such as
DALL$\cdot$E 2, Midjourney, and StableDiffusion can generate high-quality and
realistic images from users' prompts. Not limited to the research community,
ordinary Internet users enjoy these generative models, and consequently, a
tremendous amount of generated images have been shared on the Internet.
Meanwhile, today's success of deep learning in the computer vision field owes a
lot to images collected from the Internet. These trends lead us to a research
question: ""\textbf{will such generated images impact the quality of future
datasets and the performance of computer vision models positively or
negatively?}"" This paper empirically answers this question by simulating
contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using
a state-of-the-art generative model and evaluate models trained with
""contaminated"" datasets on various tasks, including image classification and
image generation. Throughout experiments, we conclude that generated images
negatively affect downstream performance, while the significance depends on
tasks and the amount of generated images. The generated datasets and the codes
for experiments will be publicly released for future research. Generated
datasets and source codes are available from
\url{https://github.com/moskomule/dataset-contamination}.",https://github.com/moskomule/dataset-contamination,694
Rethinking Audio-visual Synchronization for Active Speaker Detection,0.106856,"Active speaker detection (ASD) systems are important modules for analyzing
multi-talker conversations. They aim to detect which speakers or none are
talking in a visual scene at any given time. Existing research on ASD does not
agree on the definition of active speakers. We clarify the definition in this
work and require synchronization between the audio and visual speaking
activities. This clarification of definition is motivated by our extensive
experiments, through which we discover that existing ASD methods fail in
modeling the audio-visual synchronization and often classify unsynchronized
videos as active speaking. To address this problem, we propose a cross-modal
contrastive learning strategy and apply positional encoding in attention
modules for supervised ASD models to leverage the synchronization cue.
Experimental results suggest that our model can successfully detect
unsynchronized speaking as not speaking, addressing the limitation of current
models.",https://github.com/urkax/SyncTalkNet,30069
Towards Disentangled Speech Representations,0.0529856,"The careful construction of audio representations has become a dominant
feature in the design of approaches to many speech tasks. Increasingly, such
approaches have emphasized ""disentanglement"", where a representation contains
only parts of the speech signal relevant to transcription while discarding
irrelevant information. In this paper, we construct a representation learning
task based on joint modeling of ASR and TTS, and seek to learn a representation
of audio that disentangles that part of the speech signal that is relevant to
transcription from that part which is not. We present empirical evidence that
successfully finding such a representation is tied to the randomness inherent
in training. We then make the observation that these desired, disentangled
solutions to the optimization problem possess unique statistical properties.
Finally, we show that enforcing these properties during training improves WER
by 24.5% relative on average for our joint modeling task. These observations
motivate a novel approach to learning effective audio representations.",None,150035
Unsupervised Cross-Modality Domain Adaptation for Vestibular Schwannoma Segmentation and Koos Grade Prediction based on Semi-Supervised Contrastive Learning,0.121909,"Domain adaptation has been widely adopted to transfer styles across
multi-vendors and multi-centers, as well as to complement the missing
modalities. In this challenge, we proposed an unsupervised domain adaptation
framework for cross-modality vestibular schwannoma (VS) and cochlea
segmentation and Koos grade prediction. We learn the shared representation from
both ceT1 and hrT2 images and recover another modality from the latent
representation, and we also utilize proxy tasks of VS segmentation and brain
parcellation to restrict the consistency of image structures in domain
adaptation. After generating missing modalities, the nnU-Net model is utilized
for VS and cochlea segmentation, while a semi-supervised contrastive learning
pre-train approach is employed to improve the model performance for Koos grade
prediction. On CrossMoDA validation phase Leaderboard, our method received rank
4 in task1 with a mean Dice score of 0.8394 and rank 2 in task2 with
Macro-Average Mean Square Error of 0.3941. Our code is available at
https://github.com/fiy2W/cmda2022.superpolymerization.",https://github.com/fiy2W/cmda2022.superpolymerization,13797
Implicit Two-Tower Policies,0.106533,"We present a new class of structured reinforcement learning
policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are
chosen based on the attention scores of their learnable latent representations
with those of the input states. By explicitly disentangling action from state
processing in the policy stack, we achieve two main goals: substantial
computational gains and better performance. Our architectures are compatible
with both: discrete and continuous action spaces. By conducting tests on 15
environments from OpenAI Gym and DeepMind Control Suite, we show that
ITT-architectures are particularly suited for blackbox/evolutionary
optimization and the corresponding policy training algorithms outperform their
vanilla unstructured implicit counterparts as well as commonly used explicit
policies. We complement our analysis by showing how techniques such as hashing
and lazy tower updates, critically relying on the two-tower structure of ITTs,
can be applied to obtain additional computational improvements.",https://anonymous.4open.science/r/itt-9881/README.md,17269
Goal Recognition as a Deep Learning Task: the GRNet Approach,0.0224485,"In automated planning, recognising the goal of an agent from a trace of
observations is an important task with many applications. The state-of-the-art
approaches to goal recognition rely on the application of planning techniques,
which requires a model of the domain actions and of the initial domain state
(written, e.g., in PDDL). We study an alternative approach where goal
recognition is formulated as a classification task addressed by machine
learning. Our approach, called GRNet, is primarily aimed at making goal
recognition more accurate as well as faster by learning how to solve it in a
given domain. Given a planning domain specified by a set of propositions and a
set of action names, the goal classification instances in the domain are solved
by a Recurrent Neural Network (RNN). A run of the RNN processes a trace of
observed actions to compute how likely it is that each domain proposition is
part of the agent's goal, for the problem instance under considerations. These
predictions are then aggregated to choose one of the candidate goals. The only
information required as input of the trained RNN is a trace of action labels,
each one indicating just the name of an observed action. An experimental
analysis confirms that \our achieves good performance in terms of both goal
classification accuracy and runtime, obtaining better performance w.r.t. a
state-of-the-art goal recognition system over the considered benchmarks.",None,3062
Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?,0.0734519,"The use of Deep Learning and Computer Vision in the Cultural Heritage domain
is becoming highly relevant in the last few years with lots of applications
about audio smart guides, interactive museums and augmented reality. All these
technologies require lots of data to work effectively and be useful for the
user. In the context of artworks, such data is annotated by experts in an
expensive and time consuming process. In particular, for each artwork, an image
of the artwork and a description sheet have to be collected in order to perform
common tasks like Visual Question Answering. In this paper we propose a method
for Visual Question Answering that allows to generate at runtime a description
sheet that can be used for answering both visual and contextual questions about
the artwork, avoiding completely the image and the annotation process. For this
purpose, we investigate on the use of GPT-3 for generating descriptions for
artworks analyzing the quality of generated descriptions through captioning
metrics. Finally we evaluate the performance for Visual Question Answering and
captioning tasks.",None,656
Discrete Factorial Representations as an Abstraction for Goal Conditioned Reinforcement Learning,0.0142688,"Goal-conditioned reinforcement learning (RL) is a promising direction for
training agents that are capable of solving multiple tasks and reach a diverse
set of objectives. How to \textit{specify} and \textit{ground} these goals in
such a way that we can both reliably reach goals during training as well as
generalize to new goals during evaluation remains an open area of research.
Defining goals in the space of noisy and high-dimensional sensory inputs poses
a challenge for training goal-conditioned agents, or even for generalization to
novel goals. We propose to address this by learning factorial representations
of goals and processing the resulting representation via a discretization
bottleneck, for coarser goal specification, through an approach we call DGRL.
We show that applying a discretizing bottleneck can improve performance in
goal-conditioned RL setups, by experimentally evaluating this method on tasks
ranging from maze environments to complex robotic navigation and manipulation.
Additionally, we prove a theorem lower-bounding the expected return on
out-of-distribution goals, while still allowing for specifying goals with
expressive combinatorial structure.",None,794065
SAT: Self-adaptive training for fashion compatibility prediction,0.0487522,"This paper presents a self-adaptive training (SAT) model for fashion
compatibility prediction. It focuses on the learning of some hard items, such
as those that share similar color, texture, and pattern features but are
considered incompatible due to the aesthetics or temporal shifts. Specifically,
we first design a method to define hard outfits and a difficulty score (DS) is
defined and assigned to each outfit based on the difficulty in recommending an
item for it. Then, we propose a self-adaptive triplet loss (SATL), where the DS
of the outfit is considered. Finally, we propose a very simple conditional
similarity network combining the proposed SATL to achieve the learning of hard
items in the fashion compatibility prediction. Experiments on the publicly
available Polyvore Outfits and Polyvore Outfits-D datasets demonstrate our
SAT's effectiveness in fashion compatibility prediction. Besides, our SATL can
be easily extended to other conditional similarity networks to improve their
performance.",None,7956
DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing,0.0917354,"In the field of representation learning on knowledge graphs (KGs), a
hyper-relational fact consists of a main triple and several auxiliary
attribute-value descriptions, which is considered more comprehensive and
specific than a triple-based fact. However, currently available
hyper-relational KG embedding methods in a single view are limited in
application because they weaken the hierarchical structure that represents the
affiliation between entities. To overcome this limitation, we propose a
dual-view hyper-relational KG structure (DH-KG) that contains a
hyper-relational instance view for entities and a hyper-relational ontology
view for concepts that are abstracted hierarchically from the entities. This
paper defines link prediction and entity typing tasks on DH-KG for the first
time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and
HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding
model based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms
baseline models on DH-KG, according to experimental results. Finally, we
provide an example of how this technology can be used to treat hypertension.
Our model and new datasets are publicly available.",https://github.com/LHRLAB/DHGE,1107
Learning to Detect Mobile Objects from LiDAR Scans Without Labels,0.253891,"Current 3D object detectors for autonomous driving are almost entirely
trained on human-annotated data. Although of high quality, the generation of
such data is laborious and costly, restricting them to a few specific locations
and object types. This paper proposes an alternative approach entirely based on
unlabeled data, which can be collected cheaply and in abundance almost
everywhere on earth. Our approach leverages several simple common sense
heuristics to create an initial set of approximate seed labels. For example,
relevant traffic participants are generally not persistent across multiple
traversals of the same route, do not fly, and are never under ground. We
demonstrate that these seed labels are highly effective to bootstrap a
surprisingly accurate detector through repeated self-training without a single
human annotated label.",https://github.com/YurongYou/MODEST,46951
A Low-Shot Object Counting Network With Iterative Prototype Adaptation,0.353208,"We consider low-shot counting of arbitrary semantic categories in the image
using only few annotated exemplars (few-shot) or no exemplars (no-shot). The
standard few-shot pipeline follows extraction of appearance queries from
exemplars and matching them with image features to infer the object counts.
Existing methods extract queries by feature pooling which neglects the shape
information (e.g., size and aspect) and leads to a reduced object localization
accuracy and count estimates. We propose a Low-shot Object Counting network
with iterative prototype Adaptation (LOCA). Our main contribution is the new
object prototype extraction module, which iteratively fuses the exemplar shape
and appearance information with image features. The module is easily adapted to
zero-shot scenarios, enabling LOCA to cover the entire spectrum of low-shot
counting problems. LOCA outperforms all recent state-of-the-art methods on
FSC147 benchmark by 20-30% in RMSE on one-shot and few-shot and achieves
state-of-the-art on zero-shot scenarios, while demonstrating better
generalization capabilities.",https://github.com/djukicn/loca,11587
On three types of $L$-fuzzy $β$-covering-based rough sets,0.0371745,"In this paper, we mainly construct three types of $L$-fuzzy
$\beta$-covering-based rough set models and study the axiom sets, matrix
representations and interdependency of these three pairs of $L$-fuzzy
$\beta$-covering-based rough approximation operators. Firstly, we propose three
pairs of $L$-fuzzy $\beta$-covering-based rough approximation operators by
introducing the concepts such as $\beta$-degree of intersection and
$\beta$-subsethood degree, which are generalizations of degree of intersection
and subsethood degree, respectively. And then, the axiom set for each of these
$L$-fuzzy $\beta$-covering-based rough approximation operator is investigated.
Thirdly, we give the matrix representations of three types of $L$-fuzzy
$\beta$-covering-based rough approximation operators, which make it valid to
calculate the $L$-fuzzy $\beta$-covering-based lower and upper rough
approximation operators through operations on matrices. Finally, the
interdependency of the three pairs of rough approximation operators based on
$L$-fuzzy $\beta$-covering is studied by using the notion of reducible elements
and independent elements. In other words, we present the necessary and
sufficient conditions under which two $L$-fuzzy $\beta$-coverings can generate
the same lower and upper rough approximation operations.",None,7646
Subword Segmental Language Modelling for Nguni Languages,0.0498461,"Subwords have become the standard units of text in NLP, enabling efficient
open-vocabulary models. With algorithms like byte-pair encoding (BPE), subword
segmentation is viewed as a preprocessing step applied to the corpus before
training. This can lead to sub-optimal segmentations for low-resource languages
with complex morphologies. We propose a subword segmental language model (SSLM)
that learns how to segment words while being trained for autoregressive
language modelling. By unifying subword segmentation and language modelling,
our model learns subwords that optimise LM performance. We train our model on
the 4 Nguni languages of South Africa. These are low-resource agglutinative
languages, so subword information is critical. As an LM, SSLM outperforms
existing approaches such as BPE-based models on average across the 4 languages.
Furthermore, it outperforms standard subword segmenters on unsupervised
morphological segmentation. We also train our model as a word-level sequence
model, resulting in an unsupervised morphological segmenter that outperforms
existing methods by a large margin for all 4 languages. Our results show that
learning subword segmentation is an effective alternative to existing subword
segmenters, enabling the model to discover morpheme-like subwords that improve
its LM capabilities.",https://github.com/francois-meyer/subword-segmental-lm,3337
Better Quality Estimation for Low Resource Corpus Mining,0.0271242,"Quality Estimation (QE) models have the potential to change how we evaluate
and maybe even train machine translation models. However, these models still
lack the robustness to achieve general adoption. We show that State-of-the-art
QE models, when tested in a Parallel Corpus Mining (PCM) setting, perform
unexpectedly bad due to a lack of robustness to out-of-domain examples. We
propose a combination of multitask training, data augmentation and contrastive
learning to achieve better and more robust QE performance. We show that our
method improves QE performance significantly in the MLQE challenge and the
robustness of QE models when tested in the Parallel Corpus Mining setup. We
increase the accuracy in PCM by more than 0.80, making it on par with
state-of-the-art PCM methods that use millions of sentence pairs to train their
models. In comparison, we use a thousand times less data, 7K parallel sentences
in total, and propose a novel low resource PCM method.",https://github.com/facebookresearch/LASER,2587
Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models,0.111532,"Massively Multilingual Transformer based Language Models have been observed
to be surprisingly effective on zero-shot transfer across languages, though the
performance varies from language to language depending on the pivot language(s)
used for fine-tuning. In this work, we build upon some of the existing
techniques for predicting the zero-shot performance on a task, by modeling it
as a multi-task learning problem. We jointly train predictive models for
different tasks which helps us build more accurate predictors for tasks where
we have test data in very few languages to measure the actual performance of
the model. Our approach also lends us the ability to perform a much more robust
feature selection and identify a common set of features that influence
zero-shot performance across a variety of tasks.",https://github.com/hichamjanati/mutar,4767
Taylor Genetic Programming for Symbolic Regression,0.301101,"Genetic programming (GP) is a commonly used approach to solve symbolic
regression (SR) problems. Compared with the machine learning or deep learning
methods that depend on the pre-defined model and the training dataset for
solving SR problems, GP is more focused on finding the solution in a search
space. Although GP has good performance on large-scale benchmarks, it randomly
transforms individuals to search results without taking advantage of the
characteristics of the dataset. So, the search process of GP is usually slow,
and the final results could be unstable.To guide GP by these characteristics,
we propose a new method for SR, called Taylor genetic programming (TaylorGP)
(Code and appendix at https://kgae-cup.github.io/TaylorGP/). TaylorGP leverages
a Taylor polynomial to approximate the symbolic equation that fits the dataset.
It also utilizes the Taylor polynomial to extract the features of the symbolic
equation: low order polynomial discrimination, variable separability, boundary,
monotonic, and parity. GP is enhanced by these Taylor polynomial techniques.
Experiments are conducted on three kinds of benchmarks: classical SR, machine
learning, and physics. The experimental results show that TaylorGP not only has
higher accuracy than the nine baseline methods, but also is faster in finding
stable results.",https://kgae-cup.github.io/TaylorGP/,2208
Pre-trained Token-replaced Detection Model as Few-shot Learner,0.0639986,"Pre-trained masked language models have demonstrated remarkable ability as
few-shot learners. In this paper, as an alternative, we propose a novel
approach to few-shot learning with pre-trained token-replaced detection models
like ELECTRA. In this approach, we reformulate a classification or a regression
task as a token-replaced detection problem. Specifically, we first define a
template and label description words for each task and put them into the input
to form a natural language prompt. Then, we employ the pre-trained
token-replaced detection model to predict which label description word is the
most original (i.e., least replaced) among all label description words in the
prompt. A systematic evaluation on 16 datasets demonstrates that our approach
outperforms few-shot learners with pre-trained masked language models in both
one-sentence and two-sentence learning tasks.",https://github.com/cjfarmer/TRD_FSL,5176
On the Relationship Between Variational Inference and Auto-Associative Memory,0.00610859,"In this article, we propose a variational inference formulation of
auto-associative memories, allowing us to combine perceptual inference and
memory retrieval into the same mathematical framework. In this formulation, the
prior probability distribution onto latent representations is made memory
dependent, thus pulling the inference process towards previously stored
representations. We then study how different neural network approaches to
variational inference can be applied in this framework. We compare methods
relying on amortized inference such as Variational Auto Encoders and methods
relying on iterative inference such as Predictive Coding and suggest combining
both approaches to design new auto-associative memory models. We evaluate the
obtained algorithms on the CIFAR10 and CLEVR image datasets and compare them
with other associative memory models such as Hopfield Networks, End-to-End
Memory Networks and Neural Turing Machines.",https://github.com/sino7/predictive_coding_associative_memories,657
Fine-grained Contrastive Learning for Relation Extraction,0.0684873,"Recent relation extraction (RE) works have shown encouraging improvements by
conducting contrastive learning on silver labels generated by distant
supervision before fine-tuning on gold labels. Existing methods typically
assume all these silver labels are accurate and treat them equally; however,
distant supervision is inevitably noisy -- some silver labels are more reliable
than others. In this paper, we propose fine-grained contrastive learning
(FineCL) for RE, which leverages fine-grained information about which silver
labels are and are not noisy to improve the quality of learned relationship
representations for RE. We first assess the quality of silver labels via a
simple and automatic approach we call ""learning order denoising,"" where we
train a language model to learn these relations and record the order of learned
training instances. We show that learning order largely corresponds to label
accuracy -- early-learned silver labels have, on average, more accurate labels
than later-learned silver labels. Then, during pre-training, we increase the
weights of accurate labels within a novel contrastive learning objective.
Experiments on several RE benchmarks show that FineCL makes consistent and
significant performance gains over state-of-the-art methods.",https://github.com/wphogan/finecl,4518
CD-FSOD: A Benchmark for Cross-domain Few-shot Object Detection,0.0530354,"In this paper, we propose a study of the cross-domain few-shot object
detection (CD-FSOD) benchmark, consisting of image data from a diverse data
domain. On the proposed benchmark, we evaluate state-of-art FSOD approaches,
including meta-learning FSOD approaches and fine-tuning FSOD approaches. The
results show that these methods tend to fall, and even underperform the naive
fine-tuning model. We analyze the reasons for their failure and introduce a
strong baseline that uses a mutually-beneficial manner to alleviate the
overfitting problem. Our approach is remarkably superior to existing approaches
by significant margins (2.0\% on average) on the proposed benchmark. Our code
is available at \url{https://github.com/FSOD/CD-FSOD}.",https://github.com/FSOD/CD-FSOD,30
NEEDED: Introducing Hierarchical Transformer to Eye Diseases Diagnosis,0.0667801,"With the development of natural language processing techniques(NLP),
automatic diagnosis of eye diseases using ophthalmology electronic medical
records (OEMR) has become possible. It aims to evaluate the condition of both
eyes of a patient respectively, and we formulate it as a particular multi-label
classification task in this paper. Although there are a few related studies in
other diseases, automatic diagnosis of eye diseases exhibits unique
characteristics. First, descriptions of both eyes are mixed up in OEMR
documents, with both free text and templated asymptomatic descriptions,
resulting in sparsity and clutter of information. Second, OEMR documents
contain multiple parts of descriptions and have long document lengths. Third,
it is critical to provide explainability to the disease diagnosis model. To
overcome those challenges, we present an effective automatic eye disease
diagnosis framework, NEEDED. In this framework, a preprocessing module is
integrated to improve the density and quality of information. Then, we design a
hierarchical transformer structure for learning the contextualized
representations of each sentence in the OEMR document. For the diagnosis part,
we propose an attention-based predictor that enables traceable diagnosis by
obtaining disease-specific information. Experiments on the real dataset and
comparison with several baseline models show the advantage and explainability
of our framework.",https://github.com/coco11563/,1662
Evaluation Beyond Task Performance: Analyzing Concepts in AlphaZero in Hex,0.00528461,"AlphaZero, an approach to reinforcement learning that couples neural networks
and Monte Carlo tree search (MCTS), has produced state-of-the-art strategies
for traditional board games like chess, Go, shogi, and Hex. While researchers
and game commentators have suggested that AlphaZero uses concepts that humans
consider important, it is unclear how these concepts are captured in the
network. We investigate AlphaZero's internal representations in the game of Hex
using two evaluation techniques from natural language processing (NLP): model
probing and behavioral tests. In doing so, we introduce new evaluation tools to
the RL community and illustrate how evaluations other than task performance can
be used to provide a more complete picture of a model's strengths and
weaknesses. Our analyses in the game of Hex reveal interesting patterns and
generate some testable hypotheses about how such models learn in general. For
example, we find that MCTS discovers concepts before the neural network learns
to encode them. We also find that concepts related to short-term end-game
planning are best encoded in the final layers of the model, whereas concepts
related to long-term planning are encoded in the middle layers of the model.",https://github.com/jzf2101/alphatology,10423
Does Video Compression Impact Tracking Accuracy?,0.0170341,"Everyone ""knows"" that compressing a video will degrade the accuracy of object
tracking. Yet, a literature search on this topic reveals that there is very
little documented evidence for this presumed fact. Part of the reason is that,
until recently, there were no object tracking datasets for uncompressed video,
which made studying the effects of compression on tracking accuracy difficult.
In this paper, using a recently published dataset that contains tracking
annotations for uncompressed videos, we examined the degradation of tracking
accuracy due to video compression using rigorous statistical methods.
Specifically, we examined the impact of quantization parameter (QP) and motion
search range (MSR) on Multiple Object Tracking Accuracy (MOTA). The results
show that QP impacts MOTA at the 95% confidence level, while there is
insufficient evidence to claim that MSR impacts MOTA. Moreover, regression
analysis allows us to derive a quantitative relationship between MOTA and QP
for the specific tracker used in the experiments.",https://github.com/ultralytics/yolov3,231
METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals,0.0651485,"We present an efficient method of pretraining large-scale autoencoding
language models using training signals generated by an auxiliary model.
Originated in ELECTRA, this training strategy has demonstrated
sample-efficiency to pretrain models at the scale of hundreds of millions of
parameters. In this work, we conduct a comprehensive empirical study, and
propose a recipe, namely ""Model generated dEnoising TRaining Objective""
(METRO), which incorporates some of the best modeling techniques developed
recently to speed up, stabilize, and enhance pretrained language models without
compromising model effectiveness. The resultant models, METRO-LM, consisting of
up to 5.4 billion parameters, achieve new state-of-the-art on the GLUE,
SuperGLUE, and SQuAD benchmarks. More importantly, METRO-LM are efficient in
that they often outperform previous large models with significantly smaller
model sizes and lower pretraining cost.",https://github.com/namisan/mt-dnn/tree/master/experiments/superglue,82331
Hierarchical Compositional Representations for Few-shot Action Recognition,0.0200738,"Recently action recognition has received more and more attention for its
comprehensive and practical applications in intelligent surveillance and
human-computer interaction. However, few-shot action recognition has not been
well explored and remains challenging because of data scarcity. In this paper,
we propose a novel hierarchical compositional representations (HCR) learning
approach for few-shot action recognition. Specifically, we divide a complicated
action into several sub-actions by carefully designed hierarchical clustering
and further decompose the sub-actions into more fine-grained spatially
attentional sub-actions (SAS-actions). Although there exist large differences
between base classes and novel classes, they can share similar patterns in
sub-actions or SAS-actions. Furthermore, we adopt the Earth Mover's Distance in
the transportation problem to measure the similarity between video samples in
terms of sub-action representations. It computes the optimal matching flows
between sub-actions as distance metric, which is favorable for comparing
fine-grained patterns. Extensive experiments show our method achieves the
state-of-the-art results on HMDB51, UCF101 and Kinetics datasets.",None,38355
Low-complexity CNNs for Acoustic Scene Classification,0.108447,"This paper presents a low-complexity framework for acoustic scene
classification (ASC). Most of the frameworks designed for ASC use convolutional
neural networks (CNNs) due to their learning ability and improved performance
compared to hand-engineered features. However, CNNs are resource hungry due to
their large size and high computational complexity. Therefore, CNNs are
difficult to deploy on resource constrained devices. This paper addresses the
problem of reducing the computational complexity and memory requirement in
CNNs. We propose a low-complexity CNN architecture, and apply pruning and
quantization to further reduce the parameters and memory. We then propose an
ensemble framework that combines various low-complexity CNNs to improve the
overall performance. An experimental evaluation of the proposed framework is
performed on the publicly available DCASE 2022 Task 1 that focuses on ASC. The
proposed ensemble framework has approximately 60K parameters, requires 19M
multiply-accumulate operations and improves the performance by approximately
2-4 percentage points compared to the DCASE 2022 Task 1 baseline network.",None,17789
That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation with Switch-memory,0.12289,"The evolution of language follows the rule of gradual change. Grammar,
vocabulary, and lexical semantic shifts take place over time, resulting in a
diachronic linguistic gap. As such, a considerable amount of texts are written
in languages of different eras, which creates obstacles for natural language
processing tasks, such as word segmentation and machine translation. Although
the Chinese language has a long history, previous Chinese natural language
processing research has primarily focused on tasks within a specific era.
Therefore, we propose a cross-era learning framework for Chinese word
segmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to
incorporate era-specific linguistic knowledge. Experiments on four corpora from
different eras show that the performance of each corpus significantly improves.
Further analyses also demonstrate that the SM can effectively integrate the
knowledge of the eras into the neural network.",http://github.com/jiayan/Jiayan/,1697
IoV Scenario: Implementation of a Bandwidth Aware Algorithm in Wireless Network Communication Mode,0.344612,"The wireless network communication mode represented by the Internet of
vehicles (IoV) has been widely used. However, due to the limitations of
traditional network architecture, resource scheduling in wireless network
environment is still facing great challenges. This paper focuses on the
allocation of bandwidth resources in the virtual network environment. This
paper proposes a bandwidth aware multi domain virtual network embedding
algorithm (BA-VNE). The algorithm is mainly aimed at the problem that users
need a lot of bandwidth in wireless communication mode, and solves the problem
of bandwidth resource allocation from the perspective of virtual network
embedding (VNE). In order to improve the performance of the algorithm, we
introduce particle swarm optimization (PSO) algorithm to optimize the
performance of the algorithm. In order to verify the effectiveness of the
algorithm, we have carried out simulation experiments from link bandwidth,
mapping cost and virtual network request (VNR) acceptance rate. The final
results show that the proposed algorithm is better than other representative
algorithms in the above indicators.",None,74924
Geometric Features Informed Multi-person Human-object Interaction Recognition in Videos,0.0613587,"Human-Object Interaction (HOI) recognition in videos is important for
analyzing human activity. Most existing work focusing on visual features
usually suffer from occlusion in the real-world scenarios. Such a problem will
be further complicated when multiple people and objects are involved in HOIs.
Consider that geometric features such as human pose and object position provide
meaningful information to understand HOIs, we argue to combine the benefits of
both visual and geometric features in HOI recognition, and propose a novel
Two-level Geometric feature-informed Graph Convolutional Network (2G-GCN). The
geometric-level graph models the interdependency between geometric features of
humans and objects, while the fusion-level graph further fuses them with visual
features of humans and objects. To demonstrate the novelty and effectiveness of
our method in challenging scenarios, we propose a new multi-person HOI dataset
(MPHOI-72). Extensive experiments on MPHOI-72 (multi-person HOI), CAD-120
(single-human HOI) and Bimanual Actions (two-hand HOI) datasets demonstrate our
superior performance compared to state-of-the-arts.",https://github.com/tanqiu98/2G-GCN,6272
DiffDreamer: Towards Consistent Unsupervised Single-view Scene Extrapolation with Conditional Diffusion Models,0.0441653,"Scene extrapolation -- the idea of generating novel views by flying into a
given image -- is a promising, yet challenging task. For each predicted frame,
a joint inpainting and 3D refinement problem has to be solved, which is ill
posed and includes a high level of ambiguity. Moreover, training data for
long-range scenes is difficult to obtain and usually lacks sufficient views to
infer accurate camera poses. We introduce DiffDreamer, an unsupervised
framework capable of synthesizing novel views depicting a long camera
trajectory while training solely on internet-collected images of nature scenes.
Utilizing the stochastic nature of the guided denoising steps, we train the
diffusion models to refine projected RGBD images but condition the denoising
steps on multiple past and future frames for inference. We demonstrate that
image-conditioned diffusion models can effectively perform long-range scene
extrapolation while preserving consistency significantly better than prior
GAN-based methods. DiffDreamer is a powerful and efficient solution for scene
extrapolation, producing impressive results despite limited supervision.
Project page: https://primecai.github.io/diffdreamer.",https://primecai.github.io/diffdreamer,220689
Cross-Lingual Knowledge Transfer for Clinical Phenotyping,0.0576786,"Clinical phenotyping enables the automatic extraction of clinical conditions
from patient records, which can be beneficial to doctors and clinics worldwide.
However, current state-of-the-art models are mostly applicable to clinical
notes written in English. We therefore investigate cross-lingual knowledge
transfer strategies to execute this task for clinics that do not use the
English language and have a small amount of in-domain data available. We
evaluate these strategies for a Greek and a Spanish clinic leveraging clinical
notes from different clinical domains such as cardiology, oncology and the ICU.
Our results reveal two strategies that outperform the state-of-the-art:
Translation-based methods in combination with domain-specific encoders and
cross-lingual encoders plus adapters. We find that these strategies perform
especially well for classifying rare phenotypes and we advise on which method
to prefer in which situation. Our results show that using multilingual data
overall improves clinical phenotyping models and can compensate for data
sparseness.",https://github.com/neuron1682/cross-lingual-phenotype-prediction,20322
Unsupervised Syntactically Controlled Paraphrase Generation with Abstract Meaning Representations,0.13895,"Syntactically controlled paraphrase generation has become an emerging
research direction in recent years. Most existing approaches require annotated
paraphrase pairs for training and are thus costly to extend to new domains.
Unsupervised approaches, on the other hand, do not need paraphrase pairs but
suffer from relatively poor performance in terms of syntactic control and
quality of generated paraphrases. In this paper, we demonstrate that leveraging
Abstract Meaning Representations (AMR) can greatly improve the performance of
unsupervised syntactically controlled paraphrase generation. Our proposed
model, AMR-enhanced Paraphrase Generator (AMRPG), separately encodes the AMR
graph and the constituency parse of the input sentence into two disentangled
semantic and syntactic embeddings. A decoder is then learned to reconstruct the
input sentence from the semantic and syntactic embeddings. Our experiments show
that AMRPG generates more accurate syntactically controlled paraphrases, both
quantitatively and qualitatively, compared to the existing unsupervised
approaches. We also demonstrate that the paraphrases generated by AMRPG can be
used for data augmentation to improve the robustness of NLP models.",https://github.com/bjascob/amrlib-models,34559
Matching Tweets With Applicable Fact-Checks Across Languages,0.16573,"An important challenge for news fact-checking is the effective dissemination
of existing fact-checks. This in turn brings the need for reliable methods to
detect previously fact-checked claims. In this paper, we focus on automatically
finding existing fact-checks for claims made in social media posts (tweets). We
conduct both classification and retrieval experiments, in monolingual (English
only), multilingual (Spanish, Portuguese), and cross-lingual (Hindi-English)
settings using multilingual transformer models such as XLM-RoBERTa and
multilingual embeddings such as LaBSE and SBERT. We present promising results
for ""match"" classification (86% average accuracy) in four language pairs. We
also find that a BM25 baseline outperforms or is on par with state-of-the-art
multilingual embedding models for the retrieval task during our monolingual
experiments. We highlight and discuss NLP challenges while addressing this
problem in different languages, and we introduce a novel curated dataset of
fact-checks and corresponding tweets for future research.",None,43162
Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours,0.0145099,"Text classification can be useful in many real-world scenarios, saving a lot
of time for end users. However, building a custom classifier typically requires
coding skills and ML knowledge, which poses a significant barrier for many
potential users. To lift this barrier, we introduce Label Sleuth, a free open
source system for labeling and creating text classifiers. This system is unique
for (a) being a no-code system, making NLP accessible to non-experts, (b)
guiding users through the entire labeling process until they obtain a custom
classifier, making the process efficient -- from cold start to classifier in a
few hours, and (c) being open for configuration and extension by developers. By
open sourcing Label Sleuth we hope to build a community of users and developers
that will broaden the utilization of NLP models.",https://github.com/heartexlabs/label-studio,9733
AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation,0.149081,"Crowdsourced dialogue corpora are usually limited in scale and topic coverage
due to the expensive cost of data curation. This would hinder the
generalization of downstream dialogue models to open-domain topics. In this
work, we leverage large language models for dialogue augmentation in the task
of emotional support conversation (ESC). By treating dialogue augmentation as a
dialogue completion task, we prompt a fine-tuned language model to complete
full dialogues from available dialogue posts of various topics, which are then
postprocessed based on heuristics. Applying this approach, we construct AugESC,
an augmented dataset for the ESC task, which largely extends the scale and
topic coverage of the crowdsourced ESConv corpus. Through comprehensive human
evaluation, we demonstrate that our approach is superior to strong baselines of
dialogue augmentation and that AugESC has comparable dialogue quality to the
crowdsourced corpus. We also conduct human interactive evaluation and prove
that post-training on AugESC improves downstream dialogue models'
generalization ability to open-domain topics. These results suggest the utility
of AugESC and highlight the potential of large language models in improving
data-scarce dialogue generation tasks.",https://github.com/thu-coai/AugESC,39147
A comparison of several AI techniques for authorship attribution on Romanian texts,0.0573299,"Determining the author of a text is a difficult task. Here we compare
multiple AI techniques for classifying literary texts written by multiple
authors by taking into account a limited number of speech parts (prepositions,
adverbs, and conjunctions). We also introduce a new dataset composed of texts
written in the Romanian language on which we have run the algorithms. The
compared methods are Artificial Neural Networks, Support Vector Machines, Multi
Expression Programming, Decision Trees with C5.0, and k-Nearest Neighbour.
Numerical experiments show, first of all, that the problem is difficult, but
some algorithms are able to generate decent errors on the test set.",https://github.com/sanda-avram/ROST-source-code,2427
Self-supervised Image Clustering from Multiple Incomplete Views via Constrastive Complementary Generation,0.130021,"Incomplete Multi-View Clustering aims to enhance clustering performance by
using data from multiple modalities. Despite the fact that several approaches
for studying this issue have been proposed, the following drawbacks still
persist: 1) It's difficult to learn latent representations that account for
complementarity yet consistency without using label information; 2) and thus
fails to take full advantage of the hidden information in incomplete data
results in suboptimal clustering performance when complete data is scarce. In
this paper, we propose Contrastive Incomplete Multi-View Image Clustering with
Generative Adversarial Networks (CIMIC-GAN), which uses GAN to fill in
incomplete data and uses double contrastive learning to learn consistency on
complete and incomplete data. More specifically, considering diversity and
complementary information among multiple modalities, we incorporate
autoencoding representation of complete and incomplete data into double
contrastive learning to achieve learning consistency. Integrating GANs into the
autoencoding process can not only take full advantage of new features of
incomplete data, but also better generalize the model in the presence of high
data missing rates. Experiments conducted on \textcolor{black}{four}
extensively-used datasets show that CIMIC-GAN outperforms state-of-the-art
incomplete multi-View clustering methods.",None,7737
Active Self-Training for Weakly Supervised 3D Scene Semantic Segmentation,0.155596,"Since the preparation of labeled data for training semantic segmentation
networks of point clouds is a time-consuming process, weakly supervised
approaches have been introduced to learn from only a small fraction of data.
These methods are typically based on learning with contrastive losses while
automatically deriving per-point pseudo-labels from a sparse set of
user-annotated labels. In this paper, our key observation is that the selection
of what samples to annotate is as important as how these samples are used for
training. Thus, we introduce a method for weakly supervised segmentation of 3D
scenes that combines self-training with active learning. The active learning
selects points for annotation that likely result in performance improvements to
the trained model, while the self-training makes efficient use of the
user-provided labels for learning the model. We demonstrate that our approach
leads to an effective method that provides improvements in scene segmentation
over previous works and baselines, while requiring only a small number of user
annotations.",None,6439
Seeking Diverse Reasoning Logic: Controlled Equation Expression Generation for Solving Math Word Problems,0.029869,"To solve Math Word Problems, human students leverage diverse reasoning logic
that reaches different possible equation solutions. However, the mainstream
sequence-to-sequence approach of automatic solvers aims to decode a fixed
solution equation supervised by human annotation. In this paper, we propose a
controlled equation generation solver by leveraging a set of control codes to
guide the model to consider certain reasoning logic and decode the
corresponding equations expressions transformed from the human reference. The
empirical results suggest that our method universally improves the performance
on single-unknown (Math23K) and multiple-unknown (DRAW1K, HMWP) benchmarks,
with substantial improvements up to 13.2% accuracy on the challenging
multiple-unknown datasets.",https://github.com/yiyunya/CTRL-MWP,7897
Continual Learning with Recursive Gradient Optimization,0.271006,"Learning multiple tasks sequentially without forgetting previous knowledge,
called Continual Learning(CL), remains a long-standing challenge for neural
networks. Most existing methods rely on additional network capacity or data
replay. In contrast, we introduce a novel approach which we refer to as
Recursive Gradient Optimization(RGO). RGO is composed of an iteratively updated
optimizer that modifies the gradient to minimize forgetting without data replay
and a virtual Feature Encoding Layer(FEL) that represents different long-term
structures with only task descriptors. Experiments demonstrate that RGO has
significantly better performance on popular continual classification benchmarks
when compared to the baselines and achieves new state-of-the-art performance on
20-split-CIFAR100(82.22%) and 20-split-miniImageNet(72.63%). With higher
average accuracy than Single-Task Learning(STL), this method is flexible and
reliable to provide continual learning capabilities for learning models that
rely on gradient descent.",None,-1
Features Fusion Framework for Multimodal Irregular Time-series Events,0.132188,"Some data from multiple sources can be modeled as multimodal time-series
events which have different sampling frequencies, data compositions, temporal
relations and characteristics. Different types of events have complex nonlinear
relationships, and the time of each event is irregular. Neither the classical
Recurrent Neural Network (RNN) model nor the current state-of-the-art
Transformer model can deal with these features well. In this paper, a features
fusion framework for multimodal irregular time-series events is proposed based
on the Long Short-Term Memory networks (LSTM). Firstly, the complex features
are extracted according to the irregular patterns of different events.
Secondly, the nonlinear correlation and complex temporal dependencies
relationship between complex features are captured and fused into a tensor.
Finally, a feature gate are used to control the access frequency of different
tensors. Extensive experiments on MIMIC-III dataset demonstrate that the
proposed framework significantly outperforms to the existing methods in terms
of AUC (the area under Receiver Operating Characteristic curve) and AP (Average
Precision).",None,22
How to Fine-Tune Vision Models with SGD,0.699011,"SGD and AdamW are the two most used optimizers for fine-tuning large neural
networks in computer vision. When the two methods perform the same, SGD is
preferable because it uses less memory (12 bytes/parameter with momentum and 8
bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite
of downstream tasks, especially those with distribution shifts, we find that
fine-tuning with AdamW performs substantially better than SGD on modern Vision
Transformer and ConvNeXt models. We find that large gaps in performance between
SGD and AdamW occur when the fine-tuning gradients in the first ""embedding""
layer are much larger than in the rest of the model. Our analysis suggests an
easy fix that works consistently across datasets and models: freezing the
embedding layer (less than 1% of the parameters) leads to SGD with or without
momentum performing slightly better than AdamW while using less memory (e.g.,
on ViT-L, SGD uses 33% less GPU memory). Our insights result in
state-of-the-art accuracies on five popular distribution shift benchmarks:
WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet.",None,4318
Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically Ambiguous Settings for Low Resource Languages,0.0449765,"We leverage pre-trained language models to solve the task of complex NER for
two low-resource languages: Chinese and Spanish. We use the technique of Whole
Word Masking(WWM) to boost the performance of masked language modeling
objective on large and unsupervised corpora. We experiment with multiple neural
network architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on
top of a fine-tuned BERT layer. All our models outperform the baseline by a
significant margin and our best performing model obtains a competitive position
on the evaluation leaderboard for the blind test set.",https://github.com/AmitPandey-Research/Complex_NER,1943
Zero-shot Blind Image Denoising via Implicit Neural Representations,0.0606132,"Recent denoising algorithms based on the ""blind-spot"" strategy show
impressive blind image denoising performances, without utilizing any external
dataset. While the methods excel in recovering highly contaminated images, we
observe that such algorithms are often less effective under a low-noise or real
noise regime. To address this gap, we propose an alternative denoising strategy
that leverages the architectural inductive bias of implicit neural
representations (INRs), based on our two findings: (1) INR tends to fit the
low-frequency clean image signal faster than the high-frequency noise, and (2)
INR layers that are closer to the output play more critical roles in fitting
higher-frequency parts. Building on these observations, we propose a denoising
algorithm that maximizes the innate denoising capability of INRs by penalizing
the growth of deeper layer weights. We show that our method outperforms
existing zero-shot denoising methods under an extensive set of low-noise or
real-noise scenarios.",None,11542
CONDA: Continual Unsupervised Domain Adaptation Learning in Visual Perception for Self-Driving Cars,0.0329088,"Although unsupervised domain adaptation methods have achieved remarkable
performance in semantic scene segmentation in visual perception for
self-driving cars, these approaches remain impractical in real-world use cases.
In practice, the segmentation models may encounter new data that have not been
seen yet. Also, the previous data training of segmentation models may be
inaccessible due to privacy problems. Therefore, to address these problems, in
this work, we propose a Continual Unsupervised Domain Adaptation (CONDA)
approach that allows the model to continuously learn and adapt with respect to
the presence of the new data. Moreover, our proposed approach is designed
without the requirement of accessing previous training data. To avoid the
catastrophic forgetting problem and maintain the performance of the
segmentation models, we present a novel Bijective Maximum Likelihood loss to
impose the constraint of predicted segmentation distribution shifts. The
experimental results on the benchmark of continual unsupervised domain
adaptation have shown the advanced performance of the proposed CONDA method.",None,4806
CoHS-CQG: Context and History Selection for Conversational Question Generation,0.0804786,"Conversational question generation (CQG) serves as a vital task for machines
to assist humans, such as interactive reading comprehension, through
conversations. Compared to traditional single-turn question generation (SQG),
CQG is more challenging in the sense that the generated question is required
not only to be meaningful, but also to align with the occurred conversation
history. While previous studies mainly focus on how to model the flow and
alignment of the conversation, there has been no thorough study to date on
which parts of the context and history are necessary for the model. We argue
that shortening the context and history is crucial as it can help the model to
optimise more on the conversational alignment property. To this end, we propose
CoHS-CQG, a two-stage CQG framework, which adopts a CoHS module to shorten the
context and history of the input. In particular, CoHS selects contiguous
sentences and history turns according to their relevance scores by a top-p
strategy. Our model achieves state-of-the-art performances on CoQA in both the
answer-aware and answer-unaware settings.",https://github.com/dxlong2000/CoHS-CQG,11730
What Makes Good Contrastive Learning on Small-Scale Wearable-based Tasks?,0.0512041,"Self-supervised learning establishes a new paradigm of learning
representations with much fewer or even no label annotations. Recently there
has been remarkable progress on large-scale contrastive learning models which
require substantial computing resources, yet such models are not practically
optimal for small-scale tasks. To fill the gap, we aim to study contrastive
learning on the wearable-based activity recognition task. Specifically, we
conduct an in-depth study of contrastive learning from both algorithmic-level
and task-level perspectives. For algorithmic-level analysis, we decompose
contrastive models into several key components and conduct rigorous
experimental evaluations to better understand the efficacy and rationale behind
contrastive learning. More importantly, for task-level analysis, we show that
the wearable-based signals bring unique challenges and opportunities to
existing contrastive models, which cannot be readily solved by existing
algorithms. Our thorough empirical studies suggest important practices and shed
light on future research challenges. In the meantime, this paper presents an
open-source PyTorch library \texttt{CL-HAR}, which can serve as a practical
tool for researchers. The library is highly modularized and easy to use, which
opens up avenues for exploring novel contrastive models quickly in the future.",https://github.com/Tian0426/CL-HAR,36321
Placing Human Animations into 3D Scenes by Learning Interaction- and Geometry-Driven Keyframes,0.10509,"We present a novel method for placing a 3D human animation into a 3D scene
while maintaining any human-scene interactions in the animation. We use the
notion of computing the most important meshes in the animation for the
interaction with the scene, which we call ""keyframes."" These keyframes allow us
to better optimize the placement of the animation into the scene such that
interactions in the animations (standing, laying, sitting, etc.) match the
affordances of the scene (e.g., standing on the floor or laying in a bed). We
compare our method, which we call PAAK, with prior approaches, including POSA,
PROX ground truth, and a motion synthesis method, and highlight the benefits of
our method with a perceptual study. Human raters preferred our PAAK method over
the PROX ground truth data 64.6\% of the time. Additionally, in direct
comparisons, the raters preferred PAAK over competing methods including 61.5\%
compared to POSA.",https://gamma.umd.edu/paak/,60988
Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering,0.0312951,"While both extractive and generative readers have been successfully applied
to the Question Answering (QA) task, little attention has been paid toward the
systematic comparison of them. Characterizing the strengths and weaknesses of
the two readers is crucial not only for making a more informed reader selection
in practice but also for developing a deeper understanding to foster further
research on improving readers in a principled manner. Motivated by this goal,
we make the first attempt to systematically study the comparison of extractive
and generative readers for question answering. To be aligned with the
state-of-the-art, we explore nine transformer-based large pre-trained language
models (PrLMs) as backbone architectures. Furthermore, we organize our findings
under two main categories: (1) keeping the architecture invariant, and (2)
varying the underlying PrLMs. Among several interesting findings, it is
important to highlight that (1) the generative readers perform better in long
context QA, (2) the extractive readers perform better in short context while
also showing better out-of-domain generalization, and (3) the encoder of
encoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and
outperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also
study the effect of multi-task learning on the two types of readers varying the
underlying PrLMs and perform qualitative and quantitative diagnosis to provide
further insights into future directions in modeling better readers.",None,14059
Bio-inspired Min-Nets Improve the Performance and Robustness of Deep Networks,0.115396,"Min-Nets are inspired by end-stopped cortical cells with units that output
the minimum of two learned filters. We insert such Min-units into
state-of-the-art deep networks, such as the popular ResNet and DenseNet, and
show that the resulting Min-Nets perform better on the Cifar-10 benchmark.
Moreover, we show that Min-Nets are more robust against JPEG compression
artifacts. We argue that the minimum operation is the simplest way of
implementing an AND operation on pairs of filters and that such AND operations
introduce a bias that is appropriate given the statistics of natural images.",https://github.com/pgruening/bio_inspired_min_nets_improve_the_performance_and_robustness_of_deep_networks,7477
Hypergraph Convolutional Networks for Weakly-Supervised Semantic Segmentation,0.0,"Semantic segmentation is a fundamental topic in computer vision. Several deep
learning methods have been proposed for semantic segmentation with outstanding
results. However, these models require a lot of densely annotated images. To
address this problem, we propose a new algorithm that uses HyperGraph
Convolutional Networks for Weakly-supervised Semantic Segmentation
(HyperGCN-WSS). Our algorithm constructs spatial and k-Nearest Neighbor (k-NN)
graphs from the images in the dataset to generate the hypergraphs. Then, we
train a specialized HyperGraph Convolutional Network (HyperGCN) architecture
using some weak signals. The outputs of the HyperGCN are denominated
pseudo-labels, which are later used to train a DeepLab model for semantic
segmentation. HyperGCN-WSS is evaluated on the PASCAL VOC 2012 dataset for
semantic segmentation, using scribbles or clicks as weak signals. Our algorithm
shows competitive performance against previous methods.",None,9488
Lempel-Ziv Networks,0.011211,"Sequence processing has long been a central area of machine learning
research. Recurrent neural nets have been successful in processing sequences
for a number of tasks; however, they are known to be both ineffective and
computationally expensive when applied to very long sequences.
Compression-based methods have demonstrated more robustness when processing
such sequences -- in particular, an approach pairing the Lempel-Ziv Jaccard
Distance (LZJD) with the k-Nearest Neighbor algorithm has shown promise on long
sequence problems (up to $T=200,000,000$ steps) involving malware
classification. Unfortunately, use of LZJD is limited to discrete domains. To
extend the benefits of LZJD to a continuous domain, we investigate the
effectiveness of a deep-learning analog of the algorithm, the Lempel-Ziv
Network. While we achieve successful proof of concept, we are unable to improve
meaningfully on the performance of a standard LSTM across a variety of datasets
and sequence processing tasks. In addition to presenting this negative result,
our work highlights the problem of sub-par baseline tuning in newer research
areas.",None,10329
Improving Robustness of Convolutional Neural Networks Using Element-Wise Activation Scaling,0.0116389,"Recent works reveal that re-calibrating the intermediate activation of
adversarial examples can improve the adversarial robustness of a CNN model. The
state of the arts [Baiet al., 2021] and [Yanet al., 2021] explores this feature
at the channel level, i.e. the activation of a channel is uniformly scaled by a
factor. In this paper, we investigate the intermediate activation manipulation
at a more fine-grained level. Instead of uniformly scaling the activation, we
individually adjust each element within an activation and thus propose
Element-Wise Activation Scaling, dubbed EWAS, to improve CNNs' adversarial
robustness. Experimental results on ResNet-18 and WideResNet with CIFAR10 and
SVHN show that EWAS significantly improves the robustness accuracy. Especially
for ResNet18 on CIFAR10, EWAS increases the adversarial accuracy by 37.65% to
82.35% against C&W attack. EWAS is simple yet very effective in terms of
improving robustness. The codes are anonymously available at
https://anonymous.4open.science/r/EWAS-DD64.",https://anonymous.4open.science/r/EWAS-DD64,-1
Efficient Data-Plane Memory Scheduling for In-Network Aggregation,0.0285274,"As the scale of distributed training grows, communication becomes a
bottleneck. To accelerate the communication, recent works introduce In-Network
Aggregation (INA), which moves the gradients summation into network
middle-boxes, e.g., programmable switches to reduce the traffic volume.
However, switch memory is scarce compared to the volume of gradients
transmitted in distributed training. Although literature applies methods like
pool-based streaming or dynamic sharing to tackle the mismatch, switch memory
is still a potential performance bottleneck. Furthermore, we observe the
under-utilization of switch memory due to the synchronization requirement for
aggregator deallocation in recent works. To improve the switch memory
utilization, we propose ESA, an $\underline{E}$fficient Switch Memory
$\underline{S}$cheduler for In-Network $\underline{A}$ggregation. At its cores,
ESA enforces the preemptive aggregator allocation primitive and introduces
priority scheduling at the data-plane, which improves the switch memory
utilization and average job completion time (JCT). Experiments show that ESA
can improve the average JCT by up to $1.35\times$.",https://github.com/in-ATP/ATP,35836
A Reinforcement Learning Approach for Electric Vehicle Routing Problem with Vehicle-to-Grid Supply,0.18992,"The use of electric vehicles (EV) in the last mile is appealing from both
sustainability and operational cost perspectives. In addition to the inherent
cost efficiency of EVs, selling energy back to the grid during peak grid
demand, is a potential source of additional revenue to a fleet operator. To
achieve this, EVs have to be at specific locations (discharge points) during
specific points in time (peak period), even while meeting their core purpose of
delivering goods to customers. In this work, we consider the problem of EV
routing with constraints on loading capacity; time window; vehicle-to-grid
energy supply (CEVRPTW-D); which not only satisfy multiple system objectives,
but also scale efficiently to large problem sizes involving hundreds of
customers and discharge stations. We present QuikRouteFinder that uses
reinforcement learning (RL) for EV routing to overcome these challenges. Using
Solomon datasets, results from RL are compared against exact formulations based
on mixed-integer linear program (MILP) and genetic algorithm (GA)
metaheuristics. On an average, the results show that RL is 24 times faster than
MILP and GA, while being close in quality (within 20%) to the optimal.",None,1244
Over-the-Air Computation over Balanced Numerals,0.0649398,"In this study, a digital over-the-air computation (OAC) scheme for achieving
continuous-valued gradient aggregation is proposed. It is shown that the
average of a set of real-valued parameters can be calculated approximately by
using the average of the corresponding numerals, where the numerals are
obtained based on a balanced number system. By using this property, the
proposed scheme encodes the local gradients into a set of numerals. It then
determines the positions of the activated orthogonal frequency division
multiplexing (OFDM) subcarriers by using the values of the numerals. To
eliminate the need for a precise sample-level time synchronization, channel
estimation overhead, and power instabilities due to the channel inversion, the
proposed scheme also uses a non-coherent receiver at the edge server (ES) and
does not utilize a pre-equalization at the edge devices (EDs). Finally, the
theoretical mean squared error (MSE) performance of the proposed scheme is
derived and its performance for federated edge learning (FEEL) is demonstrated.",None,3783
Counterfactual Data Augmentation improves Factuality of Abstractive Summarization,0.0502801,"Abstractive summarization systems based on pretrained language models often
generate coherent but factually inconsistent sentences. In this paper, we
present a counterfactual data augmentation approach where we augment data with
perturbed summaries that increase the training data diversity. Specifically, we
present three augmentation approaches based on replacing (i) entities from
other and the same category and (ii) nouns with their corresponding WordNet
hypernyms. We show that augmenting the training data with our approach improves
the factual correctness of summaries without significantly affecting the ROUGE
score. We show that in two commonly used summarization datasets (CNN/Dailymail
and XSum), we improve the factual correctness by about 2.5 points on average",None,61938
SATformer: Transformer-Based UNSAT Core Learning,0.0904707,"This paper introduces SATformer, a novel Transformer-based approach for the
Boolean Satisfiability (SAT) problem. Rather than solving the problem directly,
SATformer approaches the problem from the opposite direction by focusing on
unsatisfiability. Specifically, it models clause interactions to identify any
unsatisfiable sub-problems. Using a graph neural network, we convert clauses
into clause embeddings and employ a hierarchical Transformer-based model to
understand clause correlation. SATformer is trained through a multi-task
learning approach, using the single-bit satisfiability result and the minimal
unsatisfiable core (MUC) for UNSAT problems as clause supervision. As an
end-to-end learning-based satisfiability classifier, the performance of
SATformer surpasses that of NeuroSAT significantly. Furthermore, we integrate
the clause predictions made by SATformer into modern heuristic-based SAT
solvers and validate our approach with a logic equivalence checking task.
Experimental results show that our SATformer can decrease the runtime of
existing solvers by an average of 21.33%.",None,8741
Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study,0.147878,"This paper presents an empirical study to build relation extraction systems
in low-resource settings. Based upon recent pre-trained language models, we
comprehensively investigate three schemes to evaluate the performance in
low-resource settings: (i) different types of prompt-based methods with
few-shot labeled data; (ii) diverse balancing methods to address the
long-tailed distribution issue; (iii) data augmentation technologies and
self-training to generate more labeled in-domain data. We create a benchmark
with 8 relation extraction (RE) datasets covering different languages, domains
and contexts and perform extensive comparisons over the proposed schemes with
combinations. Our experiments illustrate: (i) Though prompt-based tuning is
beneficial in low-resource RE, there is still much potential for improvement,
especially in extracting relations from cross-sentence contexts with multiple
relational triples; (ii) Balancing methods are not always helpful for RE with
long-tailed distribution; (iii) Data augmentation complements existing
baselines and can bring much performance gain, while self-training may not
consistently achieve advancement to low-resource RE. Code and datasets are in
https://github.com/zjunlp/LREBench.",https://github.com/zjunlp/LREBench,16796
MACSum: Controllable Summarization with Mixed Attributes,0.141442,"Controllable summarization allows users to generate customized summaries with
specified attributes. However, due to the lack of designated annotations of
controlled summaries, existing works have to craft pseudo datasets by adapting
generic summarization benchmarks. Furthermore, most research focuses on
controlling single attributes individually (e.g., a short summary or a highly
abstractive summary) rather than controlling a mix of attributes together
(e.g., a short and highly abstractive summary). In this paper, we propose
MACSum, the first human-annotated summarization dataset for controlling mixed
attributes. It contains source texts from two domains, news articles and
dialogues, with human-annotated summaries controlled by five designed
attributes (Length, Extractiveness, Specificity, Topic, and Speaker). We
propose two simple and effective parameter-efficient approaches for the new
task of mixed controllable summarization based on hard prompt tuning and soft
prefix tuning. Results and analysis demonstrate that hard prompt models yield
the best performance on all metrics and human evaluations. However,
mixed-attribute control is still challenging for summarization tasks. Our
dataset and code are available at https://github.com/psunlpgroup/MACSum.",https://github.com/psunlpgroup/MACSum,35424
Divert More Attention to Vision-Language Tracking,0.108333,"Relying on Transformer for complex visual feature learning, object tracking
has witnessed the new standard for state-of-the-arts (SOTAs). However, this
advancement accompanies by larger training data and longer training period,
making tracking increasingly expensive. In this paper, we demonstrate that the
Transformer-reliance is not necessary and the pure ConvNets are still
competitive and even better yet more economical and friendly in achieving SOTA
tracking. Our solution is to unleash the power of multimodal vision-language
(VL) tracking, simply using ConvNets. The essence lies in learning novel
unified-adaptive VL representations with our modality mixer (ModaMixer) and
asymmetrical ConvNet search. We show that our unified-adaptive VL
representation, learned purely with the ConvNets, is a simple yet strong
alternative to Transformer visual features, by unbelievably improving a
CNN-based Siamese tracker by 14.5% in SUC on challenging LaSOT (50.7% > 65.2%),
even outperforming several Transformer-based SOTA trackers. Besides empirical
results, we theoretically analyze our approach to evidence its effectiveness.
By revealing the potential of VL representation, we expect the community to
divert more attention to VL tracking and hope to open more possibilities for
future tracking beyond Transformer. Code and models will be released at
https://github.com/JudasDie/SOTS.",https://github.com/JudasDie/SOTS,5901
Measuring Harmful Representations in Scandinavian Language Models,0.0143979,"Scandinavian countries are perceived as role-models when it comes to gender
equality. With the advent of pre-trained language models and their widespread
usage, we investigate to what extent gender-based harmful and toxic content
exist in selected Scandinavian language models. We examine nine models,
covering Danish, Swedish, and Norwegian, by manually creating template-based
sentences and probing the models for completion. We evaluate the completions
using two methods for measuring harmful and toxic completions and provide a
thorough analysis of the results. We show that Scandinavian pre-trained
language models contain harmful and gender-based stereotypes with similar
values across all languages. This finding goes against the general expectations
related to gender equality in Scandinavian countries and shows the possible
problematic outcomes of using such models in real-world settings.",None,2551
A BERT-based Deep Learning Approach for Reputation Analysis in Social Media,0.0283484,"Social media has become an essential part of the modern lifestyle, with its
usage being highly prevalent. This has resulted in unprecedented amounts of
data generated from users in social media, such as users' attitudes, opinions,
interests, purchases, and activities across various aspects of their lives.
Therefore, in a world of social media, where its power has shifted to users,
actions taken by companies and public figures are subject to constantly being
under scrutiny by influential global audiences. As a result, reputation
management in social media has become essential as companies and public figures
need to maintain their reputation to preserve their reputation capital.
However, domain experts still face the challenge of lacking appropriate
solutions to automate reliable online reputation analysis. To tackle this
challenge, we proposed a novel reputation analysis approach based on the
popular language model BERT (Bidirectional Encoder Representations from
Transformers). The proposed approach was evaluated on the reputational polarity
task using RepLab 2013 dataset. Compared to previous works, we achieved 5.8%
improvement in accuracy, 26.9% improvement in balanced accuracy, and 21.8%
improvement in terms of F-score.",None,12055
ILSGAN: Independent Layer Synthesis for Unsupervised Foreground-Background Segmentation,0.0338453,"Unsupervised foreground-background segmentation aims at extracting salient
objects from cluttered backgrounds, where Generative Adversarial Network (GAN)
approaches, especially layered GANs, show great promise. However, without human
annotations, they are typically prone to produce foreground and background
layers with non-negligible semantic and visual confusion, dubbed ""information
leakage"", resulting in notable degeneration of the generated segmentation mask.
To alleviate this issue, we propose a simple-yet-effective explicit layer
independence modeling approach, termed Independent Layer Synthesis GAN
(ILSGAN), pursuing independent foreground-background layer generation by
encouraging their discrepancy. Specifically, it targets minimizing the mutual
information between visible and invisible regions of the foreground and
background to spur interlayer independence. Through in-depth theoretical and
experimental analyses, we justify that explicit layer independence modeling is
critical to suppressing information leakage and contributes to impressive
segmentation performance gains. Also, our ILSGAN achieves strong
state-of-the-art generation quality and segmentation performance on complex
real-world data. Code is available at: https://github.com/qrzou/ILSGAN",https://github.com/qrzou/ILSGAN,6360
"Decentralized, Communication- and Coordination-free Learning in Structured Matching Markets",0.0318173,"We study the problem of online learning in competitive settings in the
context of two-sided matching markets. In particular, one side of the market,
the agents, must learn about their preferences over the other side, the firms,
through repeated interaction while competing with other agents for successful
matches. We propose a class of decentralized, communication- and
coordination-free algorithms that agents can use to reach to their stable match
in structured matching markets. In contrast to prior works, the proposed
algorithms make decisions based solely on an agent's own history of play and
requires no foreknowledge of the firms' preferences. Our algorithms are
constructed by splitting up the statistical problem of learning one's
preferences, from noisy observations, from the problem of competing for firms.
We show that under realistic structural assumptions on the underlying
preferences of the agents and firms, the proposed algorithms incur a regret
which grows at most logarithmically in the time horizon. Our results show that,
in the case of matching markets, competition need not drastically affect the
performance of decentralized, communication and coordination free online
learning algorithms.",None,910
Detection of Fights in Videos: A Comparison Study of Anomaly Detection and Action Recognition,0.0265739,"Detection of fights is an important surveillance application in videos. Most
existing methods use supervised binary action recognition. Since frame-level
annotations are very hard to get for anomaly detection, weakly supervised
learning using multiple instance learning is widely used. This paper explores
the detection of fights in videos as one special type of anomaly detection and
as binary action recognition. We use the UBI-Fight and NTU-CCTV-Fight datasets
for most of the study since they have frame-level annotations. We find that the
anomaly detection has similar or even better performance than the action
recognition. Furthermore, we study to use anomaly detection as a toolbox to
generate training datasets for action recognition in an iterative way
conditioned on the performance of the anomaly detection. Experiment results
should show that we achieve state-of-the-art performance on three fight
detection datasets.",None,-1
Discrete Tree Flows via Tree-Structured Permutations,0.00841392,"While normalizing flows for continuous data have been extensively researched,
flows for discrete data have only recently been explored. These prior models,
however, suffer from limitations that are distinct from those of continuous
flows. Most notably, discrete flow-based models cannot be straightforwardly
optimized with conventional deep learning methods because gradients of discrete
functions are undefined or zero. Previous works approximate pseudo-gradients of
the discrete functions but do not solve the problem on a fundamental level. In
addition to that, backpropagation can be computationally burdensome compared to
alternative discrete algorithms such as decision tree algorithms. Our approach
seeks to reduce computational burden and remove the need for pseudo-gradients
by developing a discrete flow based on decision trees -- building upon the
success of efficient tree-based methods for classification and regression for
discrete data. We first define a tree-structured permutation (TSP) that
compactly encodes a permutation of discrete data where the inverse is easy to
compute; thus, we can efficiently compute the density value and sample new
data. We then propose a decision tree algorithm to build TSPs that learns the
tree structure and permutations at each node via novel criteria. We empirically
demonstrate the feasibility of our method on multiple datasets.",https://github.com/TrentBrick/PyTorchDiscreteFlows,1228
Leveraging Graph-based Cross-modal Information Fusion for Neural Sign Language Translation,0.112966,"Sign Language (SL), as the mother tongue of the deaf community, is a special
visual language that most hearing people cannot understand. In recent years,
neural Sign Language Translation (SLT), as a possible way for bridging
communication gap between the deaf and the hearing people, has attracted
widespread academic attention. We found that the current mainstream end-to-end
neural SLT models, which tries to learning language knowledge in a weakly
supervised manner, could not mine enough semantic information under the
condition of low data resources. Therefore, we propose to introduce additional
word-level semantic knowledge of sign language linguistics to assist in
improving current end-to-end neural SLT models. Concretely, we propose a novel
neural SLT model with multi-modal feature fusion based on the dynamic graph, in
which the cross-modal information, i.e. text and video, is first assembled as a
dynamic graph according to their correlation, and then the graph is processed
by a multi-modal graph encoder to generate the multi-modal embeddings for
further usage in the subsequent neural translation models. To the best of our
knowledge, we are the first to introduce graph neural networks, for fusing
multi-modal information, into neural sign language translation models.
Moreover, we conducted experiments on a publicly available popular SLT dataset
RWTH-PHOENIX-Weather-2014T. and the quantitative experiments show that our
method can improve the model.",None,70015
Learning Facial Liveness Representation for Domain Generalized Face Anti-spoofing,0.0164092,"Face anti-spoofing (FAS) aims at distinguishing face spoof attacks from the
authentic ones, which is typically approached by learning proper models for
performing the associated classification task. In practice, one would expect
such models to be generalized to FAS in different image domains. Moreover, it
is not practical to assume that the type of spoof attacks would be known in
advance. In this paper, we propose a deep learning model for addressing the
aforementioned domain-generalized face anti-spoofing task. In particular, our
proposed network is able to disentangle facial liveness representation from the
irrelevant ones (i.e., facial content and image domain features). The resulting
liveness representation exhibits sufficient domain invariant properties, and
thus it can be applied for performing domain-generalized FAS. In our
experiments, we conduct experiments on five benchmark datasets with various
settings, and we verify that our model performs favorably against
state-of-the-art approaches in identifying novel types of spoof attacks in
unseen image domains.",None,9487
From Multi-agent to Multi-robot: A Scalable Training and Evaluation Platform for Multi-robot Reinforcement Learning,0.154587,"Multi-agent reinforcement learning (MARL) has been gaining extensive
attention from academia and industries in the past few decades. One of the
fundamental problems in MARL is how to evaluate different approaches
comprehensively. Most existing MARL methods are evaluated in either video games
or simplistic simulated scenarios. It remains unknown how these methods perform
in real-world scenarios, especially multi-robot systems. This paper introduces
a scalable emulation platform for multi-robot reinforcement learning (MRRL)
called SMART to meet this need. Precisely, SMART consists of two components: 1)
a simulation environment that provides a variety of complex interaction
scenarios for training and 2) a real-world multi-robot system for realistic
performance evaluation. Besides, SMART offers agent-environment APIs that are
plug-and-play for algorithm implementation. To illustrate the practicality of
our platform, we conduct a case study on the cooperative driving lane change
scenario. Building off the case study, we summarize several unique challenges
of MRRL, which are rarely considered previously. Finally, we open-source the
simulation environments, associated benchmark tasks, and state-of-the-art
baselines to encourage and empower MRRL research.",None,27290
Selective Residual M-Net for Real Image Denoising,0.617815,"Image restoration is a low-level vision task which is to restore degraded
images to noise-free images. With the success of deep neural networks, the
convolutional neural networks surpass the traditional restoration methods and
become the mainstream in the computer vision area. To advance the performanceof
denoising algorithms, we propose a blind real image denoising network (SRMNet)
by employing a hierarchical architecture improved from U-Net. Specifically, we
use a selective kernel with residual block on the hierarchical structure called
M-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet
has competitive performance results on two synthetic and two real-world noisy
datasets in terms of quantitative metrics and visual quality. The source code
and pretrained model are available at
https://github.com/TentativeGitHub/SRMNet.",https://github.com/TentativeGitHub/SRMNet,1572
Common Knowledge of Abstract Groups,0.0418996,"Epistemic logics typically talk about knowledge of individual agents or
groups of explicitly listed agents. Often, however, one wishes to express
knowledge of groups of agents specified by a given property, as in `it is
common knowledge among economists'. We introduce such a logic of common
knowledge, which we term abstract-group epistemic logic (AGEL). That is, AGEL
features a common knowledge operator for groups of agents given by concepts in
a separate agent logic that we keep generic, with one possible agent logic
being ALC. We show that AGEL is EXPTIME-complete, with the lower bound
established by reduction from standard group epistemic logic, and the upper
bound by a satisfiability-preserving embedding into the full $\mu$-calculus.
Further main results include a finite model property (not enjoyed by the full
$\mu$-calculus) and a complete axiomatization.",None,3419
Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control,0.012757,"We present Free-HeadGAN, a person-generic neural talking head synthesis
system. We show that modeling faces with sparse 3D facial landmarks are
sufficient for achieving state-of-the-art generative performance, without
relying on strong statistical priors of the face, such as 3D Morphable Models.
Apart from 3D pose and facial expressions, our method is capable of fully
transferring the eye gaze, from a driving actor to a source identity. Our
complete pipeline consists of three components: a canonical 3D key-point
estimator that regresses 3D pose and expression-related deformations, a gaze
estimation network and a generator that is built upon the architecture of
HeadGAN. We further experiment with an extension of our generator to
accommodate few-shot learning using an attention mechanism, in case more than
one source images are available. Compared to the latest models for reenactment
and motion transfer, our system achieves higher photo-realism combined with
superior identity preservation, while offering explicit gaze control.",None,34367
Back to the Roots: Reconstructing Large and Complex Cranial Defects using an Image-based Statistical Shape Model,0.0136326,"Designing implants for large and complex cranial defects is a challenging
task, even for professional designers. Current efforts on automating the design
process focused mainly on convolutional neural networks (CNN), which have
produced state-of-the-art results on reconstructing synthetic defects. However,
existing CNN-based methods have been difficult to translate to clinical
practice in cranioplasty, as their performance on complex and irregular cranial
defects remains unsatisfactory. In this paper, a statistical shape model (SSM)
built directly on the segmentation masks of the skulls is presented. We
evaluate the SSM on several cranial implant design tasks, and the results show
that, while the SSM performs suboptimally on synthetic defects compared to
CNN-based approaches, it is capable of reconstructing large and complex defects
with only minor manual corrections. The quality of the resulting implants is
examined and assured by experienced neurosurgeons. In contrast, CNN-based
approaches, even with massive data augmentation, fail or produce
less-than-satisfactory implants for these cases. Codes are publicly available
at https://github.com/Jianningli/ssm",https://github.com/Jianningli/ssm,4894
Enhancing Deformable Convolution based Video Frame Interpolation with Coarse-to-fine 3D CNN,0.105286,"This paper presents a new deformable convolution-based video frame
interpolation (VFI) method, using a coarse to fine 3D CNN to enhance the
multi-flow prediction. This model first extracts spatio-temporal features at
multiple scales using a 3D CNN, and estimates multi-flows using these features
in a coarse-to-fine manner. The estimated multi-flows are then used to warp the
original input frames as well as context maps, and the warped results are fused
by a synthesis network to produce the final output. This VFI approach has been
fully evaluated against 12 state-of-the-art VFI methods on three commonly used
test databases. The results evidently show the effectiveness of the proposed
method, which offers superior interpolation performance over other state of the
art algorithms, with PSNR gains up to 0.19dB.",https://danier97.github.io/EDC,5466
Easing Automatic Neurorehabilitation via Classification and Smoothness Analysis,0.0640635,"Assessing the quality of movements for post-stroke patients during the
rehabilitation phase is vital given that there is no standard stroke
rehabilitation plan for all the patients. In fact, it depends basically on the
patient's functional independence and its progress along the rehabilitation
sessions. To tackle this challenge and make neurorehabilitation more agile, we
propose an automatic assessment pipeline that starts by recognizing patients'
movements by means of a shallow deep learning architecture, then measuring the
movement quality using jerk measure and related measures. A particularity of
this work is that the dataset used is clinically relevant, since it represents
movements inspired from Fugl-Meyer a well common upper-limb clinical stroke
assessment scale for stroke patients. We show that it is possible to detect the
contrast between healthy and patients movements in terms of smoothness, besides
achieving conclusions about the patients' progress during the rehabilitation
sessions that correspond to the clinicians' findings about each case.",None,4712
Device-friendly Guava fruit and leaf disease detection using deep learning,0.0351877,"This work presents a deep learning-based plant disease diagnostic system
using images of fruits and leaves. Five state-of-the-art convolutional neural
networks (CNN) have been employed for implementing the system. Hitherto model
accuracy has been the focus for such applications and model optimization has
not been accounted for the model to be applicable to end-user devices. Two
model quantization techniques such as float16 and dynamic range quantization
have been applied to the five state-of-the-art CNN architectures. The study
shows that the quantized GoogleNet model achieved the size of 0.143 MB with an
accuracy of 97%, which is the best candidate model considering the size
criterion. The EfficientNet model achieved the size of 4.2MB with an accuracy
of 99%, which is the best model considering the performance criterion. The
source codes are available at
https://github.com/CompostieAI/Guava-disease-detection.",https://github.com/CompostieAI/Guava-disease-detection,4298
Dialogue Meaning Representation for Task-Oriented Dialogue Systems,0.026946,"Dialogue meaning representation formulates natural language utterance
semantics in their conversational context in an explicit and machine-readable
form. Previous work typically follows the intent-slot framework, which is easy
for annotation yet limited in scalability for complex linguistic expressions. A
line of works alleviates the representation issue by introducing hierarchical
structures but challenging to express complex compositional semantics, such as
negation and coreference. We propose Dialogue Meaning Representation (DMR), a
pliable and easily extendable representation for task-oriented dialogue. Our
representation contains a set of nodes and edges to represent rich
compositional semantics. Moreover, we propose an inheritance hierarchy
mechanism focusing on domain extensibility. Additionally, we annotated
DMR-FastFood, a multi-turn dialogue dataset with more than 70k utterances, with
DMR. We propose two evaluation tasks to evaluate different dialogue models and
a novel coreference resolution model GNNCoref for the graph-based coreference
resolution task. Experiments show that DMR can be parsed well with pre-trained
Seq2Seq models, and GNNCoref outperforms the baseline models by a large margin.",https://github.com/amazon-research/dialogue-meaning-representation,20692
Copiloting Autonomous Multi-Robot Missions: A Game-inspired Supervisory Control Interface,0.0927735,"Real-world deployment of new technology and capabilities can be daunting. The
recent DARPA Subterranean (SubT) Challenge, for instance, aimed at the
advancement of robotic platforms and autonomy capabilities in three one-year
development pushes. While multi-agent systems are traditionally deployed in
controlled and structured environments that allow for controlled testing (e.g.,
warehouses), the SubT challenge targeted various types of unknown underground
environments that imposed the risk of robot loss in the case of failure. In
this work, we introduce a video game-inspired interface, an autonomous mission
assistant, and test and deploy these using a heterogeneous multi-agent system
in challenging environments. This work leads to improved human-supervisory
control for a multi-agent system reducing overhead from application switching,
task planning, execution, and verification while increasing available
exploration time with this human-autonomy teaming platform.",None,4549
Super-resolution 3D Human Shape from a Single Low-Resolution Image,0.0896018,"We propose a novel framework to reconstruct super-resolution human shape from
a single low-resolution input image. The approach overcomes limitations of
existing approaches that reconstruct 3D human shape from a single image, which
require high-resolution images together with auxiliary data such as surface
normal or a parametric model to reconstruct high-detail shape. The proposed
framework represents the reconstructed shape with a high-detail implicit
function. Analogous to the objective of 2D image super-resolution, the approach
learns the mapping from a low-resolution shape to its high-resolution
counterpart and it is applied to reconstruct 3D shape detail from
low-resolution images. The approach is trained end-to-end employing a novel
loss function which estimates the information lost between a low and
high-resolution representation of the same 3D surface shape. Evaluation for
single image reconstruction of clothed people demonstrates that our method
achieves high-detail surface reconstruction from low-resolution images without
auxiliary data. Extensive experiments show that the proposed approach can
estimate super-resolution human geometries with a significantly higher level of
detail than that obtained with previous approaches when applied to
low-resolution images.",None,13308
Multi-channel Attentive Graph Convolutional Network With Sentiment Fusion For Multimodal Sentiment Analysis,0.102171,"Nowadays, with the explosive growth of multimodal reviews on social media
platforms, multimodal sentiment analysis has recently gained popularity because
of its high relevance to these social media posts. Although most previous
studies design various fusion frameworks for learning an interactive
representation of multiple modalities, they fail to incorporate sentimental
knowledge into inter-modality learning. This paper proposes a Multi-channel
Attentive Graph Convolutional Network (MAGCN), consisting of two main
components: cross-modality interactive learning and sentimental feature fusion.
For cross-modality interactive learning, we exploit the self-attention
mechanism combined with densely connected graph convolutional networks to learn
inter-modality dynamics. For sentimental feature fusion, we utilize multi-head
self-attention to merge sentimental knowledge into inter-modality feature
representations. Extensive experiments are conducted on three widely-used
datasets. The experimental results demonstrate that the proposed model achieves
competitive performance on accuracy and F1 scores compared to several
state-of-the-art approaches.",None,3594
An Intermediate-level Attack Framework on The Basis of Linear Regression,0.0618903,"This paper substantially extends our work published at ECCV, in which an
intermediate-level attack was proposed to improve the transferability of some
baseline adversarial examples. Specifically, we advocate a framework in which a
direct linear mapping from the intermediate-level discrepancies (between
adversarial features and benign features) to prediction loss of the adversarial
example is established. By delving deep into the core components of such a
framework, we show that 1) a variety of linear regression models can all be
considered in order to establish the mapping, 2) the magnitude of the finally
obtained intermediate-level adversarial discrepancy is correlated with the
transferability, 3) further boost of the performance can be achieved by
performing multiple runs of the baseline attack with random initialization. In
addition, by leveraging these findings, we achieve new state-of-the-arts on
transfer-based $\ell_\infty$ and $\ell_2$ attacks. Our code is publicly
available at https://github.com/qizhangli/ila-plus-plus-lr.",https://github.com/qizhangli/ila-plus-plus-lr,60691
AnyMorph: Learning Transferable Polices By Inferring Agent Morphology,0.148696,"The prototypical approach to reinforcement learning involves training
policies tailored to a particular agent from scratch for every new morphology.
Recent work aims to eliminate the re-training of policies by investigating
whether a morphology-agnostic policy, trained on a diverse set of agents with
similar task objectives, can be transferred to new agents with unseen
morphologies without re-training. This is a challenging problem that required
previous approaches to use hand-designed descriptions of the new agent's
morphology. Instead of hand-designing this description, we propose a
data-driven method that learns a representation of morphology directly from the
reinforcement learning objective. Ours is the first reinforcement learning
algorithm that can train a policy to generalize to new agent morphologies
without requiring a description of the agent's morphology in advance. We
evaluate our approach on the standard benchmark for agent-agnostic control, and
improve over the current state of the art in zero-shot generalization to new
agents. Importantly, our method attains good performance without an explicit
description of morphology.",None,715
State Space Closure: Revisiting Endless Online Level Generation via Reinforcement Learning,0.0188439,"In this paper, we revisit endless online level generation with the recently
proposed experience-driven procedural content generation via reinforcement
learning (EDRL) framework. Inspired by an observation that EDRL tends to
generate recurrent patterns, we formulate a notion of state space closure which
makes any stochastic state appeared possibly in an infinite-horizon online
generation process can be found within a finite-horizon. Through theoretical
analysis, we find that even though state space closure arises a concern about
diversity, it generalises EDRL trained with a finite-horizon to the
infinite-horizon scenario without deterioration of content quality. Moreover,
we verify the quality and the diversity of contents generated by EDRL via
empirical studies, on the widely used Super Mario Bros. benchmark. Experimental
results reveal that the diversity of levels generated by EDRL is limited due to
the state space closure, whereas their quality does not deteriorate in a
horizon which is longer than the one specified in the training. Concluding our
outcomes and analysis, future work on endless online level generation via
reinforcement learning should address the issue of diversity while assuring the
occurrence of state space closure and quality.",https://github.com/SUSTechGameAI/MFEDRL,1829
Frontiers and Exact Learning of ELI Queries under DL-Lite Ontologies,0.977392,"We study ELI queries (ELIQs) in the presence of ontologies formulated in the
description logic DL-Lite. For the dialect DL-LiteH, we show that ELIQs have a
frontier (set of least general generalizations) that is of polynomial size and
can be computed in polynomial time. In the dialect DL-LiteF, in contrast,
frontiers may be infinite. We identify a natural syntactic restriction that
enables the same positive results as for DL-LiteH. We use out results on
frontiers to show that ELIQs are learnable in polynomial time in the presence
of a DL-LiteH / restricted DL-LiteF ontology in Angluin's framework of exact
learning with only membership queries.",None,14546
Deep Hyperspectral-Depth Reconstruction Using Single Color-Dot Projection,0.0434247,"Depth reconstruction and hyperspectral reflectance reconstruction are two
active research topics in computer vision and image processing. Conventionally,
these two topics have been studied separately using independent imaging setups
and there is no existing method which can acquire depth and spectral
reflectance simultaneously in one shot without using special hardware. In this
paper, we propose a novel single-shot hyperspectral-depth reconstruction method
using an off-the-shelf RGB camera and projector. Our method is based on a
single color-dot projection, which simultaneously acts as structured light for
depth reconstruction and spatially-varying color illuminations for
hyperspectral reflectance reconstruction. To jointly reconstruct the depth and
the hyperspectral reflectance from a single color-dot image, we propose a novel
end-to-end network architecture that effectively incorporates a geometric
color-dot pattern loss and a photometric hyperspectral reflectance loss.
Through the experiments, we demonstrate that our hyperspectral-depth
reconstruction method outperforms the combination of an existing
state-of-the-art single-shot hyperspectral reflectance reconstruction method
and depth reconstruction method.",http://www.ok.sc.e.titech.ac.jp/res/DHD/,15690
Test-Time Training for Graph Neural Networks,0.0176096,"Graph Neural Networks (GNNs) have made tremendous progress in the graph
classification task. However, a performance gap between the training set and
the test set has often been noticed. To bridge such gap, in this work we
introduce the first test-time training framework for GNNs to enhance the model
generalization capacity for the graph classification task. In particular, we
design a novel test-time training strategy with self-supervised learning to
adjust the GNN model for each test graph sample. Experiments on the benchmark
datasets have demonstrated the effectiveness of the proposed framework,
especially when there are distribution shifts between training set and test
set. We have also conducted exploratory studies and theoretical analysis to
gain deeper understandings on the rationality of the design of the proposed
graph test time training framework (GT3).",None,60225
In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models,0.617843,"Given the success with in-context learning of large pre-trained language
models, we introduce in-context learning distillation to transfer in-context
few-shot learning ability from large models to smaller models. We propose to
combine in-context learning objectives with language modeling objectives to
distill both the ability to read in-context examples and task knowledge to the
smaller models. We perform in-context learning distillation under two different
few-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask
In-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask
few-shot learning but also requires more computation than Meta-ICT. Our method
shows consistent improvements for both Meta-ICT and Multitask-ICT on two
benchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal
that in-context learning objectives and language modeling objectives are
complementary under the Multitask-ICT paradigm. In-context learning objectives
achieve the best performance when combined with language modeling objectives.",None,30354
Multi-task Active Learning for Pre-trained Transformer-based Models,0.179688,"Multi-task learning, in which several tasks are jointly learned by a single
model, allows NLP models to share information from multiple annotations and may
facilitate better predictions when the tasks are inter-related. This technique,
however, requires annotating the same text with multiple annotation schemes
which may be costly and laborious. Active learning (AL) has been demonstrated
to optimize annotation processes by iteratively selecting unlabeled examples
whose annotation is most valuable for the NLP model. Yet, multi-task active
learning (MT-AL) has not been applied to state-of-the-art pre-trained
Transformer-based NLP models. This paper aims to close this gap. We explore
various multi-task selection criteria in three realistic multi-task scenarios,
reflecting different relations between the participating tasks, and demonstrate
the effectiveness of multi-task compared to single-task selection. Our results
suggest that MT-AL can be effectively used in order to minimize annotation
efforts for multi-task NLP models.",https://github.com/rotmanguy/MTAL,7661
Deep Speech Based End-to-End Automated Speech Recognition (ASR) for Indian-English Accents,0.186141,"Automated Speech Recognition (ASR) is an interdisciplinary application of
computer science and linguistics that enable us to derive the transcription
from the uttered speech waveform. It finds several applications in Military
like High-performance fighter aircraft, helicopters, air-traffic controller.
Other than military speech recognition is used in healthcare, persons with
disabilities and many more. ASR has been an active research area. Several
models and algorithms for speech to text (STT) have been proposed. One of the
most recent is Mozilla Deep Speech, it is based on the Deep Speech research
paper by Baidu. Deep Speech is a state-of-art speech recognition system is
developed using end-to-end deep learning, it is trained using well-optimized
Recurrent Neural Network (RNN) training system utilizing multiple Graphical
Processing Units (GPUs). This training is mostly done using American-English
accent datasets, which results in poor generalizability to other English
accents. India is a land of vast diversity. This can even be seen in the
speech, there are several English accents which vary from state to state. In
this work, we have used transfer learning approach using most recent Deep
Speech model i.e., deepspeech-0.9.3 to develop an end-to-end speech recognition
system for Indian-English accents. This work utilizes fine-tuning and data
argumentation to further optimize and improve the Deep Speech ASR system. Indic
TTS data of Indian-English accents is used for transfer learning and
fine-tuning the pre-trained Deep Speech model. A general comparison is made
among the untrained model, our trained model and other available speech
recognition services for Indian-English Accents.",None,15
Planes vs. Chairs: Category-guided 3D shape learning without any 3D cues,0.0234267,"We present a novel 3D shape reconstruction method which learns to predict an
implicit 3D shape representation from a single RGB image. Our approach uses a
set of single-view images of multiple object categories without viewpoint
annotation, forcing the model to learn across multiple object categories
without 3D supervision. To facilitate learning with such minimal supervision,
we use category labels to guide shape learning with a novel categorical metric
learning approach. We also utilize adversarial and viewpoint regularization
techniques to further disentangle the effects of viewpoint and shape. We obtain
the first results for large-scale (more than 50 categories) single-viewpoint
shape prediction using a single model without any 3D cues. We are also the
first to examine and quantify the benefit of class information in single-view
supervised 3D shape reconstruction. Our method achieves superior performance
over state-of-the-art methods on ShapeNet-13, ShapeNet-55 and Pascal3D+.",https://github.com/chenhsuanlin/signed-distance-SRN,11842
Check-worthy Claim Detection across Topics for Automated Fact-checking,0.0226938,"An important component of an automated fact-checking system is the claim
check-worthiness detection system, which ranks sentences by prioritising them
based on their need to be checked. Despite a body of research tackling the
task, previous research has overlooked the challenging nature of identifying
check-worthy claims across different topics. In this paper, we assess and
quantify the challenge of detecting check-worthy claims for new, unseen topics.
After highlighting the problem, we propose the AraCWA model to mitigate the
performance deterioration when detecting check-worthy claims across topics. The
AraCWA model enables boosting the performance for new topics by incorporating
two components for few-shot learning and data augmentation. Using a publicly
available dataset of Arabic tweets consisting of 14 different topics, we
demonstrate that our proposed data augmentation strategy achieves substantial
improvements across topics overall, where the extent of the improvement varies
across topics. Further, we analyse the semantic similarities between topics,
suggesting that the similarity metric could be used as a proxy to determine the
difficulty level of an unseen topic prior to undertaking the task of labelling
the underlying sentences.",None,7490
Domain-Generalized Textured Surface Anomaly Detection,0.0629535,"Anomaly detection aims to identify abnormal data that deviates from the
normal ones, while typically requiring a sufficient amount of normal data to
train the model for performing this task. Despite the success of recent anomaly
detection methods, performing anomaly detection in an unseen domain remain a
challenging task. In this paper, we address the task of domain-generalized
textured surface anomaly detection. By observing normal and abnormal surface
data across multiple source domains, our model is expected to be generalized to
an unseen textured surface of interest, in which only a small number of normal
data can be observed during testing. Although with only image-level labels
observed in the training data, our patch-based meta-learning model exhibits
promising generalization ability: not only can it generalize to unseen image
domains, but it can also localize abnormal regions in the query image. Our
experiments verify that our model performs favorably against state-of-the-art
anomaly detection and domain generalization approaches in various settings.",None,9487
Attention Distraction: Watermark Removal Through Continual Learning with Selective Forgetting,0.0345016,"Fine-tuning attacks are effective in removing the embedded watermarks in deep
learning models. However, when the source data is unavailable, it is
challenging to just erase the watermark without jeopardizing the model
performance. In this context, we introduce Attention Distraction (AD), a novel
source data-free watermark removal attack, to make the model selectively forget
the embedded watermarks by customizing continual learning. In particular, AD
first anchors the model's attention on the main task using some unlabeled data.
Then, through continual learning, a small number of \textit{lures} (randomly
selected natural images) that are assigned a new label distract the model's
attention away from the watermarks. Experimental results from different
datasets and networks corroborate that AD can thoroughly remove the watermark
with a small resource budget without compromising the model's performance on
the main task, which outperforms the state-of-the-art works.",None,14329
Generation-Augmented Query Expansion For Code Retrieval,0.225686,"Pre-trained language models have achieved promising success in code retrieval
tasks, where a natural language documentation query is given to find the most
relevant existing code snippet. However, existing models focus only on
optimizing the documentation code pairs by embedding them into latent space,
without the association of external knowledge. In this paper, we propose a
generation-augmented query expansion framework. Inspired by the human retrieval
process - sketching an answer before searching, in this work, we utilize the
powerful code generation model to benefit the code retrieval task.
Specifically, we demonstrate that rather than merely retrieving the target code
snippet according to the documentation query, it would be helpful to augment
the documentation query with its generation counterpart - generated code
snippets from the code generation model. To the best of our knowledge, this is
the first attempt that leverages the code generation model to enhance the code
retrieval task. We achieve new state-of-the-art results on the CodeSearchNet
benchmark and surpass the baselines significantly.",https://github.com/kingoflolz/mesh-transformer-jax,19975
Policy Optimization over General State and Action Spaces,0.0365223,"Reinforcement learning (RL) problems over general state and action spaces are
notoriously challenging. In contrast to the tableau setting, one can not
enumerate all the states and then iteratively update the policies for each
state. This prevents the application of many well-studied RL methods especially
those with provable convergence guarantees. In this paper, we first present a
substantial generalization of the recently developed policy mirror descent
method to deal with general state and action spaces. We introduce new
approaches to incorporate function approximation into this method, so that we
do not need to use explicit policy parameterization at all. Moreover, we
present a novel policy dual averaging method for which possibly simpler
function approximation techniques can be applied. We establish linear
convergence rate to global optimality or sublinear convergence to stationarity
for these methods applied to solve different classes of RL problems under exact
policy evaluation. We then define proper notions of the approximation errors
for policy evaluation and investigate their impact on the convergence of these
methods applied to general-state RL problems with either finite-action or
continuous-action spaces. To the best of our knowledge, the development of
these algorithmic frameworks as well as their convergence analysis appear to be
new in the literature.",None,11470
Stochastic analysis of the Elo rating algorithm in round-robin tournaments,0.049866,"The Elo algorithm, renowned for its simplicity, is widely used for rating in
sports tournaments and other applications. However, despite its widespread use,
a detailed understanding of the convergence characteristics of the Elo
algorithm is still lacking. Aiming to fill this gap, this paper presents a
comprehensive (stochastic) analysis of the Elo algorithm, considering
round-robin tournaments. Specifically, analytical expressions are derived
describing the evolution of the skills and performance metrics. Then, taking
into account the relationship between the behavior of the algorithm and the
step-size value, which is a hyperparameter that can be controlled, design
guidelines and discussions about the performance of the algorithm are provided.
Experimental results are shown confirming the accuracy of the analysis and
illustrating the applicability of the theoretical findings using real-world
data obtained from SuperLega, the Italian volleyball league.",https://github.com/dangpzanco/elo-rating,2617
The Naughtyformer: A Transformer Understands Offensive Humor,0.0488934,"Jokes are intentionally written to be funny, but not all jokes are created
the same. Some jokes may be fit for a classroom of kindergarteners, but others
are best reserved for a more mature audience. While recent work has shown
impressive results on humor detection in text, here we instead investigate the
more nuanced task of detecting humor subtypes, especially of the less innocent
variety. To that end, we introduce a novel jokes dataset filtered from Reddit
and solve the subtype classification task using a finetuned Transformer dubbed
the Naughtyformer. Moreover, we show that our model is significantly better at
detecting offensiveness in jokes compared to state-of-the-art methods.",None,1716
Continual Predictive Learning from Videos,0.0168061,"Predictive learning ideally builds the world model of physical processes in
one or more given environments. Typical setups assume that we can collect data
from all environments at all times. In practice, however, different prediction
tasks may arrive sequentially so that the environments may change persistently
throughout the training procedure. Can we develop predictive learning
algorithms that can deal with more realistic, non-stationary physical
environments? In this paper, we study a new continual learning problem in the
context of video prediction, and observe that most existing methods suffer from
severe catastrophic forgetting in this setup. To tackle this problem, we
propose the continual predictive learning (CPL) approach, which learns a
mixture world model via predictive experience replay and performs test-time
adaptation with non-parametric task inference. We construct two new benchmarks
based on RoboNet and KTH, in which different tasks correspond to different
physical robotic environments or human actions. Our approach is shown to
effectively mitigate forgetting and remarkably outperform the na\""ive
combinations of previous art in video prediction and continual learning.",None,30629
AI Art in Architecture,0.0,"Recent diffusion-based AI art platforms are able to create impressive images
from simple text descriptions. This makes them powerful tools for concept
design in any discipline that requires creativity in visual design tasks. This
is also true for early stages of architectural design with multiple stages of
ideation, sketching and modelling. In this paper, we investigate how applicable
diffusion-based models already are to these tasks. We research the
applicability of the platforms Midjourney, DALL-E 2 and StableDiffusion to a
series of common use cases in architectural design to determine which are
already solvable or might soon be. We also analyze how they are already being
used by analyzing a data set of 40 million Midjourney queries with NLP methods
to extract common usage patterns. With this insights we derived a workflow to
interior and exterior design that combines the strengths of the individual
platforms.",None,3533
Vector Quantized Semantic Communication System,0.180754,"Although analog semantic communication systems have received considerable
attention in the literature, there is less work on digital semantic
communication systems. In this paper, we develop a deep learning (DL)-enabled
vector quantized (VQ) semantic communication system for image transmission,
named VQ-DeepSC. Specifically, we propose a convolutional neural network
(CNN)-based transceiver to extract multi-scale semantic features of images and
introduce multi-scale semantic embedding spaces to perform semantic feature
quantization, rendering the data compatible with digital communication systems.
Furthermore, we employ adversarial training to improve the quality of received
images by introducing a PatchGAN discriminator. Experimental results
demonstrate that the proposed VQ-DeepSC is more robustness than BPG in digital
communication systems and has comparable MS-SSIM performance to the DeepJSCC
method.",None,10261
Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems,0.0405732,"Structure from motion using uncalibrated multi-camera systems is a
challenging task. This paper proposes a bundle adjustment solution that
implements a baseline constraint respecting that these cameras are static to
each other. We assume these cameras are mounted on a mobile platform,
uncalibrated, and coarsely synchronized. To this end, we propose the baseline
constraint that is formulated for the scenario in which the cameras have
overlapping views. The constraint is incorporated in the bundle adjustment
solution to keep the relative motion of different cameras static. Experiments
were conducted using video frames of two collocated GoPro cameras mounted on a
vehicle with no system calibration. These two cameras were placed capturing
overlapping contents. We performed our bundle adjustment using the proposed
constraint and then produced 3D dense point clouds. Evaluations were performed
by comparing these dense point clouds against LiDAR reference data. We showed
that, as compared to traditional bundle adjustment, our proposed method
achieved an improvement of 29.38%.",None,12062
Bilingual Synchronization: Restoring Translational Relationships with Editing Operations,0.031569,"Machine Translation (MT) is usually viewed as a one-shot process that
generates the target language equivalent of some source text from scratch. We
consider here a more general setting which assumes an initial target sequence,
that must be transformed into a valid translation of the source, thereby
restoring parallelism between source and target. For this bilingual
synchronization task, we consider several architectures (both autoregressive
and non-autoregressive) and training regimes, and experiment with multiple
practical settings such as simulated interactive MT, translating with
Translation Memory (TM) and TM cleaning. Our results suggest that one single
generic edit-based system, once fine-tuned, can compare with, or even
outperform, dedicated systems specifically trained for these tasks.",None,7732
Unsupervised Lifelong Person Re-identification via Contrastive Rehearsal,0.0194788,"Existing unsupervised person re-identification (ReID) methods focus on
adapting a model trained on a source domain to a fixed target domain. However,
an adapted ReID model usually only works well on a certain target domain, but
can hardly memorize the source domain knowledge and generalize to upcoming
unseen data. In this paper, we propose unsupervised lifelong person ReID, which
focuses on continuously conducting unsupervised domain adaptation on new
domains without forgetting the knowledge learnt from old domains. To tackle
unsupervised lifelong ReID, we conduct a contrastive rehearsal on a small
number of stored old samples while sequentially adapting to new domains. We
further set an image-to-image similarity constraint between old and new models
to regularize the model updates in a way that suits old knowledge. We
sequentially train our model on several large-scale datasets in an unsupervised
manner and test it on all seen domains as well as several unseen domains to
validate the generalizability of our method. Our proposed unsupervised lifelong
method achieves strong generalizability, which significantly outperforms
previous lifelong methods on both seen and unseen domains. Code will be made
available at https://github.com/chenhao2345/UCR.",https://github.com/chenhao2345/UCR,12010
Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks,0.864665,"Representation learning on networks aims to derive a meaningful vector
representation for each node, thereby facilitating downstream tasks such as
link prediction, node classification, and node clustering. In heterogeneous
text-rich networks, this task is more challenging due to (1) presence or
absence of text: Some nodes are associated with rich textual information, while
others are not; (2) diversity of types: Nodes and edges of multiple types form
a heterogeneous network structure. As pretrained language models (PLMs) have
demonstrated their effectiveness in obtaining widely generalizable text
representations, a substantial amount of effort has been made to incorporate
PLMs into representation learning on text-rich networks. However, few of them
can jointly consider heterogeneous structure (network) information as well as
rich textual semantic information of each node effectively. In this paper, we
propose Heterformer, a Heterogeneous Network-Empowered Transformer that
performs contextualized text encoding and heterogeneous structure encoding in a
unified model. Specifically, we inject heterogeneous structure information into
each Transformer layer when encoding node texts. Meanwhile, Heterformer is
capable of characterizing node/edge type heterogeneity and encoding nodes with
or without texts. We conduct comprehensive experiments on three tasks (i.e.,
link prediction, node classification, and node clustering) on three large-scale
datasets from different domains, where Heterformer outperforms competitive
baselines significantly and consistently.",https://github.com/PeterGriﬃnJin/Heterformer,255449
Fault-Tolerant Offline Multi-Agent Path Planning,0.0927892,"We study a novel graph path planning problem for multiple agents that may
crash at runtime, and block part of the workspace. In our setting, agents can
detect neighboring crashed agents, and change followed paths at runtime. The
objective is then to prepare a set of paths and switching rules for each agent,
ensuring that all correct agents reach their destinations without collisions or
deadlocks, despite unforeseen crashes of other agents. Such planning is
attractive to build reliable multi-robot systems. We present problem
formalization, theoretical analysis such as computational complexities, and how
to solve this offline planning problem.",None,-1
Image-based Automatic Dial Meter Reading in Unconstrained Scenarios,0.320399,"The replacement of analog meters with smart meters is costly, laborious, and
far from complete in developing countries. The Energy Company of Parana (Copel)
(Brazil) performs more than 4 million meter readings (almost entirely of
non-smart devices) per month, and we estimate that 850 thousand of them are
from dial meters. Therefore, an image-based automatic reading system can reduce
human errors, create a proof of reading, and enable the customers to perform
the reading themselves through a mobile application. We propose novel
approaches for Automatic Dial Meter Reading (ADMR) and introduce a new dataset
for ADMR in unconstrained scenarios, called UFPR-ADMR-v2. Our best-performing
method combines YOLOv4 with a novel regression approach (AngReg), and explores
several postprocessing techniques. Compared to previous works, it decreased the
Mean Absolute Error (MAE) from 1,343 to 129 and achieved a meter recognition
rate (MRR) of 98.90% -- with an error tolerance of 1 Kilowatt-hour (kWh).",None,6178
ArcAid: Analysis of Archaeological Artifacts using Drawings,0.0562783,"Archaeology is an intriguing domain for computer vision. It suffers not only
from shortage in (labeled) data, but also from highly-challenging data, which
is often extremely abraded and damaged. This paper proposes a novel
semi-supervised model for classification and retrieval of images of
archaeological artifacts. This model utilizes unique data that exists in the
domain -- manual drawings made by special artists. These are used during
training to implicitly transfer the domain knowledge from the drawings to their
corresponding images, improving their classification results. We show that
while learning how to classify, our model also learns how to generate drawings
of the artifacts, an important documentation task, which is currently performed
manually. Last but not least, we collected a new dataset of stamp-seals of the
Southern Levant. Our code and dataset are publicly available.",https://github.com/offry/Arc-Aid,13977
Multi-sensor large-scale dataset for multi-view 3D reconstruction,0.119322,"We present a new multi-sensor dataset for multi-view 3D surface
reconstruction. It includes registered RGB and depth data from sensors of
different resolutions and modalities: smartphones, Intel RealSense, Microsoft
Kinect, industrial cameras, and structured-light scanner. The scenes are
selected to emphasize a diverse set of material properties challenging for
existing algorithms. We provide around 1.4 million images of 107 different
scenes acquired from 100 viewing directions under 14 lighting conditions. We
expect our dataset will be useful for evaluation and training of 3D
reconstruction algorithms and for related tasks. The dataset is available at
skoltech3d.appliedai.tech.",None,15328
Data-driven End-to-end Learning of Pole Placement Control for Nonlinear Dynamics via Koopman Invariant Subspaces,0.025636,"We propose a data-driven method for controlling the frequency and convergence
rate of black-box nonlinear dynamical systems based on the Koopman operator
theory. With the proposed method, a policy network is trained such that the
eigenvalues of a Koopman operator of controlled dynamics are close to the
target eigenvalues. The policy network consists of a neural network to find a
Koopman invariant subspace, and a pole placement module to adjust the
eigenvalues of the Koopman operator. Since the policy network is
differentiable, we can train it in an end-to-end fashion using reinforcement
learning. We demonstrate that the proposed method achieves better performance
than model-free reinforcement learning and model-based control with system
identification.",None,4892
Semi-supervised Object Detection via Virtual Category Learning,0.0807466,"Due to the costliness of labelled data in real-world applications,
semi-supervised object detectors, underpinned by pseudo labelling, are
appealing. However, handling confusing samples is nontrivial: discarding
valuable confusing samples would compromise the model generalisation while
using them for training would exacerbate the confirmation bias issue caused by
inevitable mislabelling. To solve this problem, this paper proposes to use
confusing samples proactively without label correction. Specifically, a virtual
category (VC) is assigned to each confusing sample such that they can safely
contribute to the model optimisation even without a concrete label. It is
attributed to specifying the embedding distance between the training sample and
the virtual category as the lower bound of the inter-class distance. Moreover,
we also modify the localisation loss to allow high-quality boundaries for
location regression. Extensive experiments demonstrate that the proposed VC
learning significantly surpasses the state-of-the-art, especially with small
amounts of available labels.",https://github.com/GeoffreyChen777/VC,-1
Lahjoita puhetta -- a large-scale corpus of spoken Finnish with some benchmarks,0.540019,"The Donate Speech campaign has so far succeeded in gathering approximately
3600 hours of ordinary, colloquial Finnish speech into the Lahjoita puhetta
(Donate Speech) corpus. The corpus includes over twenty thousand speakers from
all the regions of Finland and from all age brackets. The primary goals of the
collection were to create a representative, large-scale resource to study
spontaneous spoken Finnish and to accelerate the development of language
technology and speech-based services. In this paper, we present the collection
process and the collected corpus, and showcase its versatility through multiple
use cases. The evaluated use cases include: automatic speech recognition of
spontaneous speech, detection of age, gender, dialect and topic and metadata
analysis. We provide benchmarks for the use cases, as well down loadable,
trained baseline systems with open-source code for reproducibility. One further
use case is to verify the metadata and transcripts given in this corpus itself,
and to suggest artificial metadata and transcripts for the part of the corpus
where it is missing.",None,-1
Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions,0.13271,"We explore zero-shot adaptation, where a general-domain model has access to
customer or domain specific parallel data at inference time, but not during
training. We build on the idea of Retrieval Augmented Translation (RAT) where
top-k in-domain fuzzy matches are found for the source sentence, and
target-language translations of those fuzzy-matched sentences are provided to
the translation model at inference time. We propose a novel architecture to
control interactions between a source sentence and the top-k fuzzy
target-language matches, and compare it to architectures from prior work. We
conduct experiments in two language pairs (En-De and En-Fr) by training models
on WMT data and testing them with five and seven multi-domain datasets,
respectively. Our approach consistently outperforms the alternative
architectures, improving BLEU across language pair, domain, and number k of
fuzzy matches.",https://github.com/elastic/elasticsearch-py,-1
TorchMD-NET: Equivariant Transformers for Neural Network based Molecular Potentials,0.206391,"The prediction of quantum mechanical properties is historically plagued by a
trade-off between accuracy and speed. Machine learning potentials have
previously shown great success in this domain, reaching increasingly better
accuracy while maintaining computational efficiency comparable with classical
force fields. In this work we propose TorchMD-NET, a novel equivariant
transformer (ET) architecture, outperforming state-of-the-art on MD17, ANI-1,
and many QM9 targets in both accuracy and computational efficiency. Through an
extensive attention weight analysis, we gain valuable insights into the black
box predictor and show differences in the learned representation of conformers
versus conformations sampled from molecular dynamics or normal modes.
Furthermore, we highlight the importance of datasets including off-equilibrium
conformations for the evaluation of molecular potentials.",None,-1
Hearing voices at the National Library -- a speech corpus and acoustic model for the Swedish language,0.079097,"This paper explains our work in developing new acoustic models for automated
speech recognition (ASR) at KBLab, the infrastructure for data-driven research
at the National Library of Sweden (KB). We evaluate different approaches for a
viable speech-to-text pipeline for audiovisual resources in Swedish, using the
wav2vec 2.0 architecture in combination with speech corpuses created from KB's
collections. These approaches include pretraining an acoustic model for Swedish
from the ground up, and fine-tuning existing monolingual and multilingual
models. The collections-based corpuses we use have been sampled from millions
of hours of speech, with a conscious attempt to balance regional dialects to
produce a more representative, and thus more democratic, model. The acoustic
model this enabled, ""VoxRex"", outperforms existing models for Swedish ASR. We
also evaluate combining this model with various pretrained language models,
which further enhanced performance. We conclude by highlighting the potential
of such technology for cultural heritage institutions with vast collections of
previously unlabelled audiovisual data. Our models are released for further
exploration and research here: https://huggingface.co/KBLab.",https://huggingface.co/KBLab,-1
Pushing the limits of fairness impossibility: Who's the fairest of them all?,0.0731101,"The impossibility theorem of fairness is a foundational result in the
algorithmic fairness literature. It states that outside of special cases, one
cannot exactly and simultaneously satisfy all three common and intuitive
definitions of fairness - demographic parity, equalized odds, and predictive
rate parity. This result has driven most works to focus on solutions for one or
two of the metrics. Rather than follow suit, in this paper we present a
framework that pushes the limits of the impossibility theorem in order to
satisfy all three metrics to the best extent possible. We develop an
integer-programming based approach that can yield a certifiably optimal
post-processing method for simultaneously satisfying multiple fairness criteria
under small violations. We show experiments demonstrating that our
post-processor can improve fairness across the different definitions
simultaneously with minimal model performance reduction. We also discuss
applications of our framework for model selection and fairness explainability,
thereby attempting to answer the question: who's the fairest of them all?",None,-1
Learning State-Aware Visual Representations from Audible Interactions,0.402428,"We propose a self-supervised algorithm to learn representations from
egocentric video data. Recently, significant efforts have been made to capture
humans interacting with their own environments as they go about their daily
activities. In result, several large egocentric datasets of interaction-rich
multi-modal data have emerged. However, learning representations from videos
can be challenging. First, given the uncurated nature of long-form continuous
videos, learning effective representations require focusing on moments in time
when interactions take place. Second, visual representations of daily
activities should be sensitive to changes in the state of the environment.
However, current successful multi-modal learning frameworks encourage
representation invariance over time. To address these challenges, we leverage
audio signals to identify moments of likely interactions which are conducive to
better learning. We also propose a novel self-supervised objective that learns
from audible state changes caused by interactions. We validate these
contributions extensively on two large-scale egocentric datasets,
EPIC-Kitchens-100 and the recently released Ego4D, and show improvements on
several downstream tasks, including action recognition, long-term action
anticipation, and object state change classification.",https://github.com/HimangiM/RepLAI,-1
3D Equivariant Graph Implicit Functions,0.247297,"In recent years, neural implicit representations have made remarkable
progress in modeling of 3D shapes with arbitrary topology. In this work, we
address two key limitations of such representations, in failing to capture
local 3D geometric fine details, and to learn from and generalize to shapes
with unseen 3D transformations. To this end, we introduce a novel family of
graph implicit functions with equivariant layers that facilitates modeling fine
local details and guaranteed robustness to various groups of geometric
transformations, through local $k$-NN graph embeddings with sparse point set
observations at multiple resolutions. Our method improves over the existing
rotation-equivariant implicit function from 0.69 to 0.89 (IoU) on the ShapeNet
reconstruction task. We also show that our equivariant implicit function can be
extended to other types of similarity transformations and generalizes to unseen
translations and scaling.",None,-1
"Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data",0.0584064,"Advances in deep learning for human activity recognition have been relatively
limited due to the lack of large labelled datasets. In this study, we leverage
self-supervised learning techniques on the UK-Biobank activity tracker
dataset--the largest of its kind to date--containing more than 700,000
person-days of unlabelled wearable sensor data. Our resulting activity
recognition model consistently outperformed strong baselines across seven
benchmark datasets, with an F1 relative improvement of 2.5%-100% (median
18.4%), the largest improvements occurring in the smaller datasets. In contrast
to previous studies, our results generalise across external datasets, devices,
and environments. Our open-source model will help researchers and developers to
build customisable and generalisable activity classifiers with high
performance.",https://github.com/OxWearables/ssl-wearables,-1
Data Augmentation with Paraphrase Generation and Entity Extraction for Multimodal Dialogue System,0.233649,"Contextually aware intelligent agents are often required to understand the
users and their surroundings in real-time. Our goal is to build Artificial
Intelligence (AI) systems that can assist children in their learning process.
Within such complex frameworks, Spoken Dialogue Systems (SDS) are crucial
building blocks to handle efficient task-oriented communication with children
in game-based learning settings. We are working towards a multimodal dialogue
system for younger kids learning basic math concepts. Our focus is on improving
the Natural Language Understanding (NLU) module of the task-oriented SDS
pipeline with limited datasets. This work explores the potential benefits of
data augmentation with paraphrase generation for the NLU models trained on
small task-specific datasets. We also investigate the effects of extracting
entities for conceivably further data expansion. We have shown that
paraphrasing with model-in-the-loop (MITL) strategies using small seed data is
a promising approach yielding improved performance results for the Intent
Recognition task.",None,-1
Towards Boosting the Open-Domain Chatbot with Human Feedback,0.0574009,"Many open-domain dialogue models pre-trained with social media comments can
generate coherent replies but have difficulties producing engaging responses
when interacting with real users. This phenomenon might mainly result from the
deficiency of annotated human-human conversations and the misalignment with
human preference. In this paper, we propose a novel and efficient approach
Diamante to boost the open-domain chatbot, where two kinds of human feedback
(including explicit demonstration and implicit preference) are collected and
leveraged. By asking annotators to select or amend the model-generated
candidate responses, Diamante efficiently collects the human demonstrated
responses and constructs a Chinese chit-chat dataset. To enhance the alignment
with human preference, Diamante leverages the implicit preference in the data
collection process and introduces the generation-evaluation joint training.
Comprehensive experiments indicate that the Diamante dataset and joint training
paradigm can significantly boost the performance of Chinese pre-trained
dialogue models.",https://github.com/PaddlePaddle/Knover/tree/develop/projects/Diamante,-1
FSCNN: A Fast Sparse Convolution Neural Network Inference System,0.0144875,"Convolution neural networks (CNNs) have achieved remarkable success, but
typically accompany high computation cost and numerous redundant weight
parameters. To reduce the FLOPs, structure pruning is a popular approach to
remove the entire hidden structures via introducing coarse-grained sparsity.
Meanwhile, plentiful pruning works leverage fine-grained sparsity instead
(sparsity are randomly distributed), whereas their sparse models lack special
designed computing library for potential speedup. In this technical report, we
study and present an efficient convolution neural network inference system to
accelerate its forward pass by utilizing the fine-grained sparsity of
compressed CNNs. Our developed FSCNN is established based on a set of
specialized designed sparse data structures, operators and associated
algorithms. Experimentally, we validate that FSCNN outperforms standard deep
learning library PyTorch on popular CNN architectures such as VGG16 if
sufficiently high sparsity exhibits. However, due to the contiguity issue of
sparse operators, FSCNN is typically not comparable with highly optimized dense
operator. Therefore, coarse-grained (structured) sparsity is our recommendation
for generic model compression.",None,-1
Flexible Sampling for Long-tailed Skin Lesion Classification,0.0144632,"Most of the medical tasks naturally exhibit a long-tailed distribution due to
the complex patient-level conditions and the existence of rare diseases.
Existing long-tailed learning methods usually treat each class equally to
re-balance the long-tailed distribution. However, considering that some
challenging classes may present diverse intra-class distributions, re-balancing
all classes equally may lead to a significant performance drop. To address
this, in this paper, we propose a curriculum learning-based framework called
Flexible Sampling for the long-tailed skin lesion classification task.
Specifically, we initially sample a subset of training data as anchor points
based on the individual class prototypes. Then, these anchor points are used to
pre-train an inference model to evaluate the per-class learning difficulty.
Finally, we use a curriculum sampling module to dynamically query new samples
from the rest training samples with the learning difficulty-aware sampling
probability. We evaluated our model against several state-of-the-art methods on
the ISIC dataset. The results with two long-tailed settings have demonstrated
the superiority of our proposed training strategy, which achieves a new
benchmark for long-tailed skin lesion classification.",None,-1
Model-Free Reinforcement Learning for Symbolic Automata-encoded Objectives,0.0941909,"Reinforcement learning (RL) is a popular approach for robotic path planning
in uncertain environments. However, the control policies trained for an RL
agent crucially depend on user-defined, state-based reward functions. Poorly
designed rewards can lead to policies that do get maximal rewards but fail to
satisfy desired task objectives or are unsafe. There are several examples of
the use of formal languages such as temporal logics and automata to specify
high-level task specifications for robots (in lieu of Markovian rewards).
Recent efforts have focused on inferring state-based rewards from formal
specifications; here, the goal is to provide (probabilistic) guarantees that
the policy learned using RL (with the inferred rewards) satisfies the
high-level formal specification. A key drawback of several of these techniques
is that the rewards that they infer are sparse: the agent receives positive
rewards only upon completion of the task and no rewards otherwise. This
naturally leads to poor convergence properties and high variance during RL. In
this work, we propose using formal specifications in the form of symbolic
automata: these serve as a generalization of both bounded-time temporal
logic-based specifications as well as automata. Furthermore, our use of
symbolic automata allows us to define non-sparse potential-based rewards which
empirically shape the reward surface, leading to better convergence during RL.
We also show that our potential-based rewarding strategy still allows us to
obtain the policy that maximizes the satisfaction of the given specification.",None,-1
A Context-Aware Feature Fusion Framework for Punctuation Restoration,0.0225962,"To accomplish the punctuation restoration task, most existing approaches
focused on leveraging extra information (e.g., part-of-speech tags) or
addressing the class imbalance problem. Recent works have widely applied the
transformer-based language models and significantly improved their
effectiveness. To the best of our knowledge, an inherent issue has remained
neglected: the attention of individual heads in the transformer will be diluted
or powerless while feeding the long non-punctuation utterances. Since those
previous contexts, not the followings, are comparatively more valuable to the
current position, it's hard to achieve a good balance by independent attention.
In this paper, we propose a novel Feature Fusion framework based on two-type
Attentions (FFA) to alleviate the shortage. It introduces a two-stream
architecture. One module involves interaction between attention heads to
encourage the communication, and another masked attention module captures the
dependent feature representation. Then, it aggregates two feature embeddings to
fuse information and enhances context-awareness. The experiments on the popular
benchmark dataset IWSLT demonstrate that our approach is effective. Without
additional data, it obtains comparable performance to the current
state-of-the-art models.",https://github.com/Young1993/ffa,-1
Spacecraft Pose Estimation Based on Unsupervised Domain Adaptation and on a 3D-Guided Loss Combination,0.0220509,"Spacecraft pose estimation is a key task to enable space missions in which
two spacecrafts must navigate around each other. Current state-of-the-art
algorithms for pose estimation employ data-driven techniques. However, there is
an absence of real training data for spacecraft imaged in space conditions due
to the costs and difficulties associated with the space environment. This has
motivated the introduction of 3D data simulators, solving the issue of data
availability but introducing a large gap between the training (source) and test
(target) domains. We explore a method that incorporates 3D structure into the
spacecraft pose estimation pipeline to provide robustness to intensity domain
shift and we present an algorithm for unsupervised domain adaptation with
robust pseudo-labelling. Our solution has ranked second in the two categories
of the 2021 Pose Estimation Challenge organised by the European Space Agency
and the Stanford University, achieving the lowest average error over the two
categories.",https://github.com/JotaBravo/spacecraft-uda,-1
Learning Discriminative Representations and Decision Boundaries for Open Intent Detection,0.100167,"Open intent detection is a significant problem in natural language
understanding, which aims to identify the unseen open intent while ensuring
known intent identification performance. However, current methods face two
major challenges. Firstly, they struggle to learn friendly representations to
detect the open intent with prior knowledge of only known intents. Secondly,
there is a lack of an effective approach to obtaining specific and compact
decision boundaries for known intents. To address these issues, this paper
presents an original framework called DA-ADB, which successively learns
distance-aware intent representations and adaptive decision boundaries for open
intent detection. Specifically, we first leverage distance information to
enhance the distinguishing capability of the intent representations. Then, we
design a novel loss function to obtain appropriate decision boundaries by
balancing both empirical and open space risks. Extensive experiments
demonstrate the effectiveness of the proposed distance-aware and boundary
learning strategies. Compared to state-of-the-art methods, our framework
achieves substantial improvements on three benchmark datasets. Furthermore, it
yields robust performance with varying proportions of labeled data and known
categories.",https://github.com/thuiar/TEXTOIR,-1
Parallel Augmentation and Dual Enhancement for Occluded Person Re-identification,0.014145,"Occluded person re-identification (Re-ID), the task of searching for the same
person's images in occluded environments, has attracted lots of attention in
the past decades. Recent approaches concentrate on improving performance on
occluded data by data/feature augmentation or using extra models to predict
occlusions. However, they ignore the imbalance problem in this task and can not
fully utilize the information from the training data. To alleviate these two
issues, we propose a simple yet effective method with Parallel Augmentation and
Dual Enhancement (PADE), which is robust on both occluded and non-occluded data
and does not require any auxiliary clues. First, we design a parallel
augmentation mechanism (PAM) to generate more suitable occluded data to
mitigate the negative effects of unbalanced data. Second, we propose the global
and local dual enhancement strategy (DES) to promote the context information
and details. Experimental results on three widely used occluded datasets and
two non-occluded datasets validate the effectiveness of our method. The code is
available at
https://github.com/littleprince1121/PADE_Parallel_Augmentation_and_Dual_Enhancement_for_Occluded_Person_ReID",None,-1
Detect Hate Speech in Unseen Domains using Multi-Task Learning: A Case Study of Political Public Figures,0.00737487,"Automatic identification of hateful and abusive content is vital in combating
the spread of harmful online content and its damaging effects. Most existing
works evaluate models by examining the generalization error on train-test
splits on hate speech datasets. These datasets often differ in their
definitions and labeling criteria, leading to poor model performance when
predicting across new domains and datasets. In this work, we propose a new
Multi-task Learning (MTL) pipeline that utilizes MTL to train simultaneously
across multiple hate speech datasets to construct a more encompassing
classification model. We simulate evaluation on new previously unseen datasets
by adopting a leave-one-out scheme in which we omit a target dataset from
training and jointly train on the other datasets. Our results consistently
outperform a large sample of existing work. We show strong results when
examining generalization error in train-test splits and substantial
improvements when predicting on previously unseen datasets. Furthermore, we
assemble a novel dataset, dubbed PubFigs, focusing on the problematic speech of
American Public Political Figures. We automatically detect problematic speech
in the $305,235$ tweets in PubFigs, and we uncover insights into the posting
behaviors of public figures.",None,-1
Recovering Sign Bits of DCT Coefficients in Digital Images as an Optimization Problem,0.109841,"Recovering unknown, missing, damaged, distorted, or lost information in DCT
coefficients is a common task in multiple applications of digital image
processing, including image compression, selective image encryption, and image
communication. This paper investigates the recovery of sign bits in DCT
coefficients of digital images, by proposing two different approximation
methods to solve a mixed integer linear programming (MILP) problem, which is
NP-hard in general. One method is a relaxation of the MILP problem to a linear
programming (LP) problem, and the other splits the original MILP problem into
some smaller MILP problems and an LP problem. We considered how the proposed
methods can be applied to JPEG-encoded images and conducted extensive
experiments to validate their performances. The experimental results showed
that the proposed methods outperformed other existing methods by a substantial
margin, both according to objective quality metrics and our subjective
evaluation.",https://github.com/ChengqingLi/DCT_SBR,-1
SOTIF Entropy: Online SOTIF Risk Quantification and Mitigation for Autonomous Driving,0.14918,"Autonomous driving confronts great challenges in complex traffic scenarios,
where the risk of Safety of the Intended Functionality (SOTIF) can be triggered
by the dynamic operational environment and system insufficiencies. The SOTIF
risk is reflected not only intuitively in the collision risk with objects
outside the autonomous vehicles (AVs), but also inherently in the performance
limitation risk of the implemented algorithms themselves. How to minimize the
SOTIF risk for autonomous driving is currently a critical, difficult, and
unresolved issue. Therefore, this paper proposes the ""Self-Surveillance and
Self-Adaption System"" as a systematic approach to online minimize the SOTIF
risk, which aims to provide a systematic solution for monitoring,
quantification, and mitigation of inherent and external risks. The core of this
system is the risk monitoring of the implemented artificial intelligence
algorithms within the AV. As a demonstration of the Self-Surveillance and
Self-Adaption System, the risk monitoring of the perception algorithm, i.e.,
YOLOv5 is highlighted. Moreover, the inherent perception algorithm risk and
external collision risk are jointly quantified via SOTIF entropy, which is then
propagated downstream to the decision-making module and mitigated. Finally,
several challenging scenarios are demonstrated, and the Hardware-in-the-Loop
experiments are conducted to verify the efficiency and effectiveness of the
system. The results demonstrate that the Self-Surveillance and Self-Adaption
System enables dependable online monitoring, quantification, and mitigation of
SOTIF risk in real-time critical traffic environments.",https://github.com/MarkDana/RealtimeConeDetection,-1
Learning MAX-SAT from Contextual Examples for Combinatorial Optimisation,0.203551,"Combinatorial optimisation problems are ubiquitous in artificial
intelligence. Designing the underlying models, however, requires substantial
expertise, which is a limiting factor in practice. The models typically consist
of hard and soft constraints, or combine hard constraints with an objective
function. We introduce a novel setting for learning combinatorial optimisation
problems from contextual examples. These positive and negative examples show -
in a particular context - whether the solutions are good enough or not. We
develop our framework using the MAX-SAT formalism as it is simple yet powerful
setting having these features. We study the learnability of MAX-SAT models. Our
theoretical results show that high-quality MAX-SAT models can be learned from
contextual examples in the realisable and agnostic settings, as long as the
data satisfies an intuitive ""representativeness"" condition. We also contribute
two implementations based on our theoretical results: one leverages ideas from
syntax-guided synthesis while the other makes use of stochastic local search
techniques. The two implementations are evaluated by recovering synthetic and
benchmark models from contextual examples. The experimental results support our
theoretical analysis, showing that MAX-SAT models can be learned from
contextual examples. Among the two implementations, the stochastic local search
learner scales much better than the syntax-guided implementation while
providing comparable or better models.",https://github.com/mohitKULeuven/HassleWithLocalSearch,-1
Clinical Prompt Learning with Frozen Language Models,0.0774517,"Prompt learning is a new paradigm in the Natural Language Processing (NLP)
field which has shown impressive performance on a number of natural language
tasks with common benchmarking text datasets in full, few-shot, and zero-shot
train-evaluation setups. Recently, it has even been observed that large but
frozen pre-trained language models (PLMs) with prompt learning outperform
smaller but fine-tuned models. However, as with many recent NLP trends, the
performance of even the largest PLMs such as GPT-3 do not perform well on
specialized domains (e.g. medical text), and the common practice to achieve
State of the Art (SoTA) results still consists of pre-training and fine-tuning
the PLMs on downstream tasks. The reliance on fine-tuning large PLMs is
problematic in clinical settings where data is often held in non-GPU
environments, and more resource efficient methods of training specialized
domain models is crucial. We investigated the viability of prompt learning on
clinically meaningful decision tasks and directly compared with more
traditional fine-tuning methods. Results are partially in line with the prompt
learning literature, with prompt learning able to match or improve on
traditional fine-tuning with substantially fewer trainable parameters and
requiring less training data. We argue that prompt learning therefore provides
lower computational resource costs applicable to clinical settings, that can
serve as an alternative to fine-tuning ever increasing in size PLMs.
Complementary code to reproduce experiments presented in this work can be found
at: https://github.com/NtaylorOX/Public_Clinical_Prompt.",None,-1
Calibrate and Refine! A Novel and Agile Framework for ASR-error Robust Intent Detection,0.0487527,"The past ten years have witnessed the rapid development of text-based intent
detection, whose benchmark performances have already been taken to a remarkable
level by deep learning techniques. However, automatic speech recognition (ASR)
errors are inevitable in real-world applications due to the environment noise,
unique speech patterns and etc, leading to sharp performance drop in
state-of-the-art text-based intent detection models. Essentially, this
phenomenon is caused by the semantic drift brought by ASR errors and most
existing works tend to focus on designing new model structures to reduce its
impact, which is at the expense of versatility and flexibility. Different from
previous one-piece model, in this paper, we propose a novel and agile framework
called CR-ID for ASR error robust intent detection with two plug-and-play
modules, namely semantic drift calibration module (SDCM) and phonemic
refinement module (PRM), which are both model-agnostic and thus could be easily
integrated to any existing intent detection models without modifying their
structures. Experimental results on SNIPS dataset show that, our proposed CR-ID
framework achieves competitive performance and outperform all the baseline
methods on ASR outputs, which verifies that CR-ID can effectively alleviate the
semantic drift caused by ASR errors.",https://github.com/MiuLab/SpokenVec,-1
Autonomous Mobile Clinics: Empowering Affordable Anywhere Anytime Healthcare Access,0.0649241,"We are facing a global healthcare crisis today as the healthcare cost is ever
climbing, but with the aging population, government fiscal revenue is ever
dropping. To create a more efficient and effective healthcare system, three
technical challenges immediately present themselves: healthcare access,
healthcare equity, and healthcare efficiency. An autonomous mobile clinic
solves the healthcare access problem by bringing healthcare services to the
patient by the order of the patient's fingertips. Nevertheless, to enable a
universal autonomous mobile clinic network, a three-stage technical roadmap
needs to be achieved: In stage one, we focus on solving the inequity challenge
in the existing healthcare system by combining autonomous mobility and
telemedicine. In stage two, we develop an AI doctor for primary care, which we
foster from infancy to adulthood with clean healthcare data. With the AI
doctor, we can solve the inefficiency problem. In stage three, after we have
proven that the autonomous mobile clinic network can truly solve the target
clinical use cases, we shall open up the platform for all medical verticals,
thus enabling universal healthcare through this whole new system.",None,-1
Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition,0.0455565,"Continual Learning for Named Entity Recognition (CL-NER) aims to learn a
growing number of entity types over time from a stream of data. However, simply
learning Other-Class in the same way as new entity types amplifies the
catastrophic forgetting and leads to a substantial performance drop. The main
cause behind this is that Other-Class samples usually contain old entity types,
and the old knowledge in these Other-Class samples is not preserved properly.
Thanks to the causal inference, we identify that the forgetting is caused by
the missing causal effect from the old data. To this end, we propose a unified
causal framework to retrieve the causality from both new entity types and
Other-Class. Furthermore, we apply curriculum learning to mitigate the impact
of label noise and introduce a self-adaptive weight for balancing the causal
effects between new entity types and Other-Class. Experimental results on three
benchmark datasets show that our method outperforms the state-of-the-art method
by a large margin. Moreover, our method can be combined with the existing
state-of-the-art methods to improve the performance in CL-NER",https://github.com/zzz47zzz/CFNER,-1
K-level Reasoning for Zero-Shot Coordination in Hanabi,0.11141,"The standard problem setting in cooperative multi-agent settings is self-play
(SP), where the goal is to train a team of agents that works well together.
However, optimal SP policies commonly contain arbitrary conventions
(""handshakes"") and are not compatible with other, independently trained agents
or humans. This latter desiderata was recently formalized by Hu et al. 2020 as
the zero-shot coordination (ZSC) setting and partially addressed with their
Other-Play (OP) algorithm, which showed improved ZSC and human-AI performance
in the card game Hanabi. OP assumes access to the symmetries of the environment
and prevents agents from breaking these in a mutually incompatible way during
training. However, as the authors point out, discovering symmetries for a given
environment is a computationally hard problem. Instead, we show that through a
simple adaption of k-level reasoning (KLR) Costa Gomes et al. 2006,
synchronously training all levels, we can obtain competitive ZSC and ad-hoc
teamplay performance in Hanabi, including when paired with a human-like proxy
bot. We also introduce a new method, synchronous-k-level reasoning with a best
response (SyKLRBR), which further improves performance on our synchronous KLR
by co-training a best response.",None,-1
A hybrid quantum image edge detector for the NISQ era,0.0445732,"Edges are image locations where the gray value intensity changes suddenly.
They are among the most important features to understand and segment an image.
Edge detection is a standard task in digital image processing, solved for
example using filtering techniques. However, the amount of data to be processed
grows rapidly and pushes even supercomputers to their limits. Quantum computing
promises exponentially lower memory usage in terms of the number of qubits
compared to the number of classical bits. In this paper, we propose a hybrid
method for quantum edge detection based on the idea of a quantum artificial
neuron. Our method can be practically implemented on quantum computers,
especially on those of the current noisy intermediate-scale quantum era. We
compare six variants of the method to reduce the number of circuits and thus
the time required for the quantum edge detection. Taking advantage of the
scalability of our method, we can practically detect edges in images
considerably larger than reached before.",None,-1
Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition,0.140588,"Language model fusion helps smart assistants recognize words which are rare
in acoustic data but abundant in text-only corpora (typed search logs).
However, such corpora have properties that hinder downstream performance,
including being (1) too large, (2) beset with domain-mismatched content, and
(3) heavy-headed rather than heavy-tailed (excessively many duplicate search
queries such as ""weather""). We show that three simple strategies for selecting
language modeling data can dramatically improve rare-word recognition without
harming overall performance. First, to address the heavy-headedness, we
downsample the data according to a soft log function, which tunably reduces
high frequency (head) sentences. Second, to encourage rare-word exposure, we
explicitly filter for words rare in the acoustic data. Finally, we tackle
domain-mismatch via perplexity-based contrastive selection, filtering for
examples matched to the target domain. We down-select a large corpus of web
search queries by a factor of 53x and achieve better LM perplexities than
without down-selection. When shallow-fused with a state-of-the-art, production
speech engine, our LM achieves WER reductions of up to 24% relative on
rare-word sentences (without changing overall WER) compared to a baseline LM
trained on the raw corpus. These gains are further validated through favorable
side-by-side evaluations on live voice search traffic.",None,-1
Generalizability of Adversarial Robustness Under Distribution Shifts,0.0127137,"Recent progress in empirical and certified robustness promises to deliver
reliable and deployable Deep Neural Networks (DNNs). Despite that success, most
existing evaluations of DNN robustness have been done on images sampled from
the same distribution on which the model was trained. However, in the real
world, DNNs may be deployed in dynamic environments that exhibit significant
distribution shifts. In this work, we take a first step towards thoroughly
investigating the interplay between empirical and certified adversarial
robustness on one hand and domain generalization on another. To do so, we train
robust models on multiple domains and evaluate their accuracy and robustness on
an unseen domain. We observe that: (1) both empirical and certified robustness
generalize to unseen domains, and (2) the level of generalizability does not
correlate well with input visual similarity, measured by the FID between source
and target domains. We also extend our study to cover a real-world medical
application, in which adversarial augmentation significantly boosts the
generalization of robustness with minimal effect on clean data accuracy.",None,-1
Mukayese: Turkish NLP Strikes Back,0.118667,"Having sufficient resources for language X lifts it from the under-resourced
languages class, but not necessarily from the under-researched class. In this
paper, we address the problem of the absence of organized benchmarks in the
Turkish language. We demonstrate that languages such as Turkish are left behind
the state-of-the-art in NLP applications. As a solution, we present Mukayese, a
set of NLP benchmarks for the Turkish language that contains several NLP tasks.
We work on one or more datasets for each benchmark and present two or more
baselines. Moreover, we present four new benchmarking datasets in Turkish for
language modeling, sentence segmentation, and spell checking. All datasets and
baselines are available under: https://github.com/alisafaya/mukayese",https://github.com/alisafaya/mukayese,-1
Tackling Online One-Class Incremental Learning by Removing Negative Contrasts,0.00816375,"Recent work studies the supervised online continual learning setting where a
learner receives a stream of data whose class distribution changes over time.
Distinct from other continual learning settings the learner is presented new
samples only once and must distinguish between all seen classes. A number of
successful methods in this setting focus on storing and replaying a subset of
samples alongside incoming data in a computationally efficient manner. One
recent proposal ER-AML achieved strong performance in this setting by applying
an asymmetric loss based on contrastive learning to the incoming data and
replayed data. However, a key ingredient of the proposed method is avoiding
contrasts between incoming data and stored data, which makes it impractical for
the setting where only one new class is introduced in each phase of the stream.
In this work we adapt a recently proposed approach (\textit{BYOL}) from
self-supervised learning to the supervised learning setting, unlocking the
constraint on contrasts. We then show that supplementing this with additional
regularization on class prototypes yields a new method that achieves strong
performance in the one-class incremental learning setting and is competitive
with the top performing methods in the multi-class incremental setting.",None,-1
Do LSTMs See Gender? Probing the Ability of LSTMs to Learn Abstract Syntactic Rules,0.0484405,"LSTMs trained on next-word prediction can accurately perform linguistic tasks
that require tracking long-distance syntactic dependencies. Notably, model
accuracy approaches human performance on number agreement tasks (Gulordava et
al., 2018). However, we do not have a mechanistic understanding of how LSTMs
perform such linguistic tasks. Do LSTMs learn abstract grammatical rules, or do
they rely on simple heuristics? Here, we test gender agreement in French which
requires tracking both hierarchical syntactic structures and the inherent
gender of lexical units. Our model is able to reliably predict long-distance
gender agreement in two subject-predicate contexts: noun-adjective and
noun-passive-verb agreement. The model showed more inaccuracies on plural noun
phrases with gender attractors compared to singular cases, suggesting a
reliance on clues from gendered articles for agreement. Overall, our study
highlights key ways in which LSTMs deviate from human behaviour and questions
whether LSTMs genuinely learn abstract syntactic rules and categories. We
propose using gender agreement as a useful probe to investigate the underlying
mechanisms, internal representations, and linguistic capabilities of LSTM
language models.",None,-1
Continual Variational Autoencoder Learning via Online Cooperative Memorization,0.0879991,"Due to their inference, data representation and reconstruction properties,
Variational Autoencoders (VAE) have been successfully used in continual
learning classification tasks. However, their ability to generate images with
specifications corresponding to the classes and databases learned during
Continual Learning (CL) is not well understood and catastrophic forgetting
remains a significant challenge. In this paper, we firstly analyze the
forgetting behaviour of VAEs by developing a new theoretical framework that
formulates CL as a dynamic optimal transport problem. This framework proves
approximate bounds to the data likelihood without requiring the task
information and explains how the prior knowledge is lost during the training
process. We then propose a novel memory buffering approach, namely the Online
Cooperative Memorization (OCM) framework, which consists of a Short-Term Memory
(STM) that continually stores recent samples to provide future information for
the model, and a Long-Term Memory (LTM) aiming to preserve a wide diversity of
samples. The proposed OCM transfers certain samples from STM to LTM according
to the information diversity selection criterion without requiring any
supervised signals. The OCM framework is then combined with a dynamic VAE
expansion mixture network for further enhancing its performance.",https://github.com/dtuzi123/OVAE,-1
Robust Deep Learning for Autonomous Driving,0.0424739,"The last decade's research in artificial intelligence had a significant
impact on the advance of autonomous driving. Yet, safety remains a major
concern when it comes to deploying such systems in high-risk environments. The
objective of this thesis is to develop methodological tools which provide
reliable uncertainty estimates for deep neural networks. First, we introduce a
new criterion to reliably estimate model confidence: the true class probability
(TCP). We show that TCP offers better properties for failure prediction than
current uncertainty measures. Since the true class is by essence unknown at
test time, we propose to learn TCP criterion from data with an auxiliary model,
introducing a specific learning scheme adapted to this context. The relevance
of the proposed approach is validated on image classification and semantic
segmentation datasets. Then, we extend our learned confidence approach to the
task of domain adaptation where it improves the selection of pseudo-labels in
self-training methods. Finally, we tackle the challenge of jointly detecting
misclassification and out-of-distributions samples by introducing a new
uncertainty measure based on evidential models and defined on the simplex.",https://github.com/valeoai/ConfidNet,-1
Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization,0.219227,"Commonsense question answering (CQA) aims to test if models can answer
questions regarding commonsense knowledge that everyone knows. Prior works that
incorporate external knowledge bases have shown promising results, but
knowledge bases are expensive to construct and are often limited to a fixed set
of relations. In this paper, we instead focus on better utilizing the
\textit{implicit knowledge} stored in pre-trained language models. While
researchers have found that the knowledge embedded in pre-trained language
models can be extracted by having them fill in the blanks of carefully designed
prompts for relation extraction and text classification, it remains unclear if
we can adopt this paradigm in CQA where the inputs and outputs take much more
flexible forms. To this end, we investigate four translation methods that can
translate natural questions into cloze-style sentences to better solicit
commonsense knowledge from language models, including a syntactic-based model,
an unsupervised neural model, and two supervised neural models. In addition, to
combine the different translation methods, we propose to encourage consistency
among model predictions on different translated questions with unlabeled data.
We demonstrate the effectiveness of our methods on three CQA datasets in
zero-shot settings. We show that our methods are complementary to a knowledge
base improved model, and combining them can lead to state-of-the-art zero-shot
performance. Analyses also reveal distinct characteristics of the different
cloze translation methods and provide insights on why combining them can lead
to great improvements.",https://github.com/PlusLabNLP/zero-shot-cqa,-1
Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking,0.0778225,"Current natural language interaction for self-tracking tools largely depends
on bespoke implementation optimized for a specific tracking theme and data
format, which is neither generalizable nor scalable to a tremendous design
space of self-tracking. However, training machine learning models in the
context of self-tracking is challenging due to the wide variety of tracking
topics and data formats. In this paper, we propose a novel NLP task for
self-tracking that extracts close- and open-ended information from a
retrospective activity log described as a plain text, and a domain-agnostic,
GPT-3-based NLU framework that performs this task. The framework augments the
prompt using synthetic samples to transform the task into 10-shot learning, to
address a cold-start problem in bootstrapping a new tracking topic. Our
preliminary evaluation suggests that our approach significantly outperforms the
baseline QA models. Going further, we discuss future application domains toward
which the NLP and HCI researchers can collaborate.",None,-1
EventGraph: Event Extraction as Semantic Graph Parsing,0.313955,"Event extraction involves the detection and extraction of both the event
triggers and corresponding event arguments. Existing systems often decompose
event extraction into multiple subtasks, without considering their possible
interactions. In this paper, we propose EventGraph, a joint framework for event
extraction, which encodes events as graphs. We represent event triggers and
arguments as nodes in a semantic graph. Event extraction therefore becomes a
graph parsing problem, which provides the following advantages: 1) performing
event detection and argument extraction jointly; 2) detecting and extracting
multiple events from a piece of text; and 3) capturing the complicated
interaction between event arguments and triggers. Experimental results on
ACE2005 show that our model is competitive to state-of-the-art systems and has
substantially improved the results on argument extraction. Additionally, we
create two new datasets from ACE2005 where we keep the entire text spans for
event arguments, instead of just the head word(s). Our code and models are
released as open-source.",https://github.com/huiling-y/EventGraph,-1
Deep neural networks for fine-grained surveillance of overdose mortality,0.127959,"Surveillance of drug overdose deaths relies on death certificates for
identification of the substances that caused death. Drugs and drug classes can
be identified through the International Classification of Diseases, 10th
Revision (ICD-10) codes present on death certificates. However, ICD-10 codes do
not always provide high levels of specificity in drug identification. To
achieve more fine-grained identification of substances on a death certificate,
the free-text cause of death section, completed by the medical certifier, must
be analyzed. Current methods for analyzing free-text death certificates rely
solely on look-up tables for identifying specific substances, which must be
frequently updated and maintained. To improve identification of drugs on death
certificates, a deep learning named-entity recognition model was developed,
which achieved an F1-score of 99.13%. This model can identify new drug
misspellings and novel substances that are not present on current surveillance
look-up tables, enhancing the surveillance of drug overdose deaths.",https://github.com/pjward5656/DC_flair,-1
NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles,0.0129939,"In this paper, we present the fifth installment of the NELA-GT datasets,
NELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between
January 1st, 2022 and December 31st, 2022. Just as in past releases of the
dataset, NELA-GT-2022 includes outlet-level veracity labels from Media
Bias/Fact Check and tweets embedded in collected news articles. The
NELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H",https://github.com/MELALab/nela-gt,-1
Perturbation Learning Based Anomaly Detection,0.0650518,"This paper presents a simple yet effective method for anomaly detection. The
main idea is to learn small perturbations to perturb normal data and learn a
classifier to classify the normal data and the perturbed data into two
different classes. The perturbator and classifier are jointly learned using
deep neural networks. Importantly, the perturbations should be as small as
possible but the classifier is still able to recognize the perturbed data from
unperturbed data. Therefore, the perturbed data are regarded as abnormal data
and the classifier provides a decision boundary between the normal data and
abnormal data, although the training data do not include any abnormal data.
Compared with the state-of-the-art of anomaly detection, our method does not
require any assumption about the shape (e.g. hypersphere) of the decision
boundary and has fewer hyper-parameters to determine. Empirical studies on
benchmark datasets verify the effectiveness and superiority of our method.",None,-1
Beyond Plain Toxic: Detection of Inappropriate Statements on Flammable Topics for the Russian Language,0.0153678,"Toxicity on the Internet, such as hate speech, offenses towards particular
users or groups of people, or the use of obscene words, is an acknowledged
problem. However, there also exist other types of inappropriate messages which
are usually not viewed as toxic, e.g. as they do not contain explicit offences.
Such messages can contain covered toxicity or generalizations, incite harmful
actions (crime, suicide, drug use), provoke ""heated"" discussions. Such messages
are often related to particular sensitive topics, e.g. on politics, sexual
minorities, social injustice which more often than other topics, e.g. cars or
computing, yield toxic emotional reactions. At the same time, clearly not all
messages within such flammable topics are inappropriate.
  Towards this end, in this work, we present two text collections labelled
according to binary notion of inapropriateness and a multinomial notion of
sensitive topic. Assuming that the notion of inappropriateness is common among
people of the same culture, we base our approach on human intuitive
understanding of what is not acceptable and harmful. To objectivise the notion
of inappropriateness, we define it in a data-driven way though crowdsourcing.
Namely we run a large-scale annotation study asking workers if a given chatbot
textual statement could harm reputation of a company created it. Acceptably
high values of inter-annotator agreement suggest that the notion of
inappropriateness exists and can be uniformly understood by different people.
To define the notion of sensitive topics in an objective way we use on
guidelines suggested commonly by specialists of legal and PR department of a
large public company as potentially harmful.",None,-1
Towards Tracing Factual Knowledge in Language Models Back to the Training Data,0.0614466,"Language models (LMs) have been shown to memorize a great deal of factual
knowledge contained in their training data. But when an LM generates an
assertion, it is often difficult to determine where it learned this information
and whether it is true. In this paper, we propose the problem of fact tracing:
identifying which training examples taught an LM to generate a particular
factual assertion. Prior work on training data attribution (TDA) may offer
effective tools for identifying such examples, known as ""proponents"". We
present the first quantitative benchmark to evaluate this. We compare two
popular families of TDA methods -- gradient-based and embedding-based -- and
find that much headroom remains. For example, both methods have lower
proponent-retrieval precision than an information retrieval baseline (BM25)
that does not have access to the LM at all. We identify key challenges that may
be necessary for further improvement such as overcoming the problem of gradient
saturation, and also show how several nuanced implementation details of
existing neural TDA methods can significantly improve overall fact tracing
performance.",https://github.com/ekinakyurek/influence,-1
Using Large Language Models to Generate Engaging Captions for Data Visualizations,0.0449548,"Creating compelling captions for data visualizations has been a longstanding
challenge. Visualization researchers are typically untrained in journalistic
reporting and hence the captions that are placed below data visualizations tend
to be not overly engaging and rather just stick to basic observations about the
data. In this work we explore the opportunities offered by the newly emerging
crop of large language models (LLM) which use sophisticated deep learning
technology to produce human-like prose. We ask, can these powerful software
devices be purposed to produce engaging captions for generic data
visualizations like a scatterplot. It turns out that the key challenge lies in
designing the most effective prompt for the LLM, a task called prompt
engineering. We report on first experiments using the popular LLM GPT-3 and
deliver some promising results.",None,-1
Exposure Correction Model to Enhance Image Quality,0.0332694,"Exposure errors in an image cause a degradation in the contrast and low
visibility in the content. In this paper, we address this problem and propose
an end-to-end exposure correction model in order to handle both under- and
overexposure errors with a single model. Our model contains an image encoder,
consecutive residual blocks, and image decoder to synthesize the corrected
image. We utilize perceptual loss, feature matching loss, and multi-scale
discriminator to increase the quality of the generated image as well as to make
the training more stable. The experimental results indicate the effectiveness
of proposed model. We achieve the state-of-the-art result on a large-scale
exposure dataset. Besides, we investigate the effect of exposure setting of the
image on the portrait matting task. We find that under- and overexposed images
cause severe degradation in the performance of the portrait matting models. We
show that after applying exposure correction with the proposed model, the
portrait matting quality increases significantly.
https://github.com/yamand16/ExposureCorrection",https://github.com/yamand16/ExposureCorrection,-1
Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents,0.0495418,"We argue that disentangling content selection from the budget used to cover
salient content improves the performance and applicability of abstractive
summarizers. Our method, FactorSum, does this disentanglement by factorizing
summarization into two steps through an energy function: (1) generation of
abstractive summary views; (2) combination of these views into a final summary,
following a budget and content guidance. This guidance may come from different
sources, including from an advisor model such as BART or BigBird, or in oracle
mode -- from the reference. This factorization achieves significantly higher
ROUGE scores on multiple benchmarks for long document summarization, namely
PubMed, arXiv, and GovReport. Most notably, our model is effective for domain
adaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1
score on arXiv, which indicates a strong performance due to more flexible
budget adaptation and content selection less dependent on domain-specific
textual structure.",https://github.com/thefonseca/factorsum,-1
Compositional Generalisation with Structured Reordering and Fertility Layers,0.167307,"Seq2seq models have been shown to struggle with compositional generalisation,
i.e. generalising to new and potentially more complex structures than seen
during training. Taking inspiration from grammar-based models that excel at
compositional generalisation, we present a flexible end-to-end differentiable
neural model that composes two structural operations: a fertility step, which
we introduce in this work, and a reordering step based on previous work (Wang
et al., 2021). To ensure differentiability, we use the expected value of each
step. Our model outperforms seq2seq models by a wide margin on challenging
compositional splits of realistic semantic parsing tasks that require
generalisation to longer examples. It also compares favourably to other models
targeting compositional generalisation.",https://github.com/namednil/f-then-r,-1
3DMODT: Attention-Guided Affinities for Joint Detection & Tracking in 3D Point Clouds,0.0465333,"We propose a method for joint detection and tracking of multiple objects in
3D point clouds, a task conventionally treated as a two-step process comprising
object detection followed by data association. Our method embeds both steps
into a single end-to-end trainable network eliminating the dependency on
external object detectors. Our model exploits temporal information employing
multiple frames to detect objects and track them in a single network, thereby
making it a utilitarian formulation for real-world scenarios. Computing
affinity matrix by employing features similarity across consecutive point cloud
scans forms an integral part of visual tracking. We propose an attention-based
refinement module to refine the affinity matrix by suppressing erroneous
correspondences. The module is designed to capture the global context in
affinity matrix by employing self-attention within each affinity matrix and
cross-attention across a pair of affinity matrices. Unlike competing
approaches, our network does not require complex post-processing algorithms,
and processes raw LiDAR frames to directly output tracking results. We
demonstrate the effectiveness of our method on the three tracking benchmarks:
JRDB, Waymo, and KITTI. Experimental evaluations indicate the ability of our
model to generalize well across datasets.",None,-1
TTTFlow: Unsupervised Test-Time Training with Normalizing Flow,0.135259,"A major problem of deep neural networks for image classification is their
vulnerability to domain changes at test-time. Recent methods have proposed to
address this problem with test-time training (TTT), where a two-branch model is
trained to learn a main classification task and also a self-supervised task
used to perform test-time adaptation. However, these techniques require
defining a proxy task specific to the target application. To tackle this
limitation, we propose TTTFlow: a Y-shaped architecture using an unsupervised
head based on Normalizing Flows to learn the normal distribution of latent
features and detect domain shifts in test examples. At inference, keeping the
unsupervised head fixed, we adapt the model to domain-shifted examples by
maximizing the log likelihood of the Normalizing Flow. Our results show that
our method can significantly improve the accuracy with respect to previous
works.",https://github.com/GustavoVargasHakim/TTTFlow.git,-1
On the Role of Field of View for Occlusion Removal with Airborne Optical Sectioning,0.0810318,"Occlusion caused by vegetation is an essential problem for remote sensing
applications in areas, such as search and rescue, wildfire detection, wildlife
observation, surveillance, border control, and others. Airborne Optical
Sectioning (AOS) is an optical, wavelength-independent synthetic aperture
imaging technique that supports computational occlusion removal in real-time.
It can be applied with manned or unmanned aircrafts, such as drones. In this
article, we demonstrate a relationship between forest density and field of view
(FOV) of applied imaging systems. This finding was made with the help of a
simulated procedural forest model which offers the consideration of more
realistic occlusion properties than our previous statistical model. While AOS
has been explored with automatic and autonomous research prototypes in the
past, we present a free AOS integration for DJI systems. It enables bluelight
organizations and others to use and explore AOS with compatible, manually
operated, off-the-shelf drones. The (digitally cropped) default FOV for this
implementation was chosen based on our new finding.",https://github.com/tensorware/aos-simulation,-1
"Improving Depression estimation from facial videos with face alignment, training optimization and scheduling",0.0360217,"Deep learning models have shown promising results in recognizing depressive
states using video-based facial expressions. While successful models typically
leverage using 3D-CNNs or video distillation techniques, the different use of
pretraining, data augmentation, preprocessing, and optimization techniques
across experiments makes it difficult to make fair architectural comparisons.
We propose instead to enhance two simple models based on ResNet-50 that use
only static spatial information by using two specific face alignment methods
and improved data augmentation, optimization, and scheduling techniques. Our
extensive experiments on benchmark datasets obtain similar results to
sophisticated spatio-temporal models for single streams, while the score-level
fusion of two different streams outperforms state-of-the-art methods. Our
findings suggest that specific modifications in the preprocessing and training
process result in noticeable differences in the performance of the models and
could hide the actual originally attributed to the use of different neural
network architectures.",None,-1
Fusing Frame and Event Vision for High-speed Optical Flow for Edge Application,0.0379038,"Optical flow computation with frame-based cameras provides high accuracy but
the speed is limited either by the model size of the algorithm or by the frame
rate of the camera. This makes it inadequate for high-speed applications. Event
cameras provide continuous asynchronous event streams overcoming the frame-rate
limitation. However, the algorithms for processing the data either borrow frame
like setup limiting the speed or suffer from lower accuracy. We fuse the
complementary accuracy and speed advantages of the frame and event-based
pipelines to provide high-speed optical flow while maintaining a low error
rate. Our bio-mimetic network is validated with the MVSEC dataset showing 19%
error degradation at 4x speed up. We then demonstrate the system with a
high-speed drone flight scenario where a high-speed event camera computes the
flow even before the optical camera sees the drone making it suited for
applications like tracking and segmentation. This work shows the fundamental
trade-offs in frame-based processing may be overcome by fusing data from other
modalities.",None,-1
A Psychological Theory of Explainability,0.00729494,"The goal of explainable Artificial Intelligence (XAI) is to generate
human-interpretable explanations, but there are no computationally precise
theories of how humans interpret AI generated explanations. The lack of theory
means that validation of XAI must be done empirically, on a case-by-case basis,
which prevents systematic theory-building in XAI. We propose a psychological
theory of how humans draw conclusions from saliency maps, the most common form
of XAI explanation, which for the first time allows for precise prediction of
explainee inference conditioned on explanation. Our theory posits that absent
explanation humans expect the AI to make similar decisions to themselves, and
that they interpret an explanation by comparison to the explanations they
themselves would give. Comparison is formalized via Shepard's universal law of
generalization in a similarity space, a classic theory from cognitive science.
A pre-registered user study on AI image classifications with saliency map
explanations demonstrate that our theory quantitatively matches participants'
predictions of the AI.",None,-1
A Temporal-Pattern Backdoor Attack to Deep Reinforcement Learning,0.235829,"Deep reinforcement learning (DRL) has made significant achievements in many
real-world applications. But these real-world applications typically can only
provide partial observations for making decisions due to occlusions and noisy
sensors. However, partial state observability can be used to hide malicious
behaviors for backdoors. In this paper, we explore the sequential nature of DRL
and propose a novel temporal-pattern backdoor attack to DRL, whose trigger is a
set of temporal constraints on a sequence of observations rather than a single
observation, and effect can be kept in a controllable duration rather than in
the instant. We validate our proposed backdoor attack to a typical job
scheduling task in cloud computing. Numerous experimental results show that our
backdoor can achieve excellent effectiveness, stealthiness, and sustainability.
Our backdoor's average clean data accuracy and attack success rate can reach
97.8% and 97.5%, respectively.",https://github.com/EboYu/DRLBackdoor,-1
Semantic features of object concepts generated with GPT-3,0.0278394,"Semantic features have been playing a central role in investigating the
nature of our conceptual representations. Yet the enormous time and effort
required to empirically sample and norm features from human raters has
restricted their use to a limited set of manually curated concepts. Given
recent promising developments with transformer-based language models, here we
asked whether it was possible to use such models to automatically generate
meaningful lists of properties for arbitrary object concepts and whether these
models would produce features similar to those found in humans. To this end, we
probed a GPT-3 model to generate semantic features for 1,854 objects and
compared automatically-generated features to existing human feature norms.
GPT-3 generated many more features than humans, yet showed a similar
distribution in the types of generated features. Generated feature norms
rivaled human norms in predicting similarity, relatedness, and category
membership, while variance partitioning demonstrated that these predictions
were driven by similar variance in humans and GPT-3. Together, these results
highlight the potential of large language models to capture important facets of
human knowledge and yield a new approach for automatically generating
interpretable feature sets, thus drastically expanding the potential use of
semantic features in psychological and linguistic studies.",https://github.com/ViCCo-Group/semantic-features-gpt-3,-1
Measuring Inconsistency in Declarative Process Specifications,0.0866597,"We address the problem of measuring inconsistency in declarative process
specifications, with an emphasis on linear temporal logic on fixed traces
(LTLff). As we will show, existing inconsistency measures for classical logic
cannot provide a meaningful assessment of inconsistency in LTL in general, as
they cannot adequately handle the temporal operators. We therefore propose a
novel paraconsistent semantics as a framework for inconsistency measurement. We
then present two new inconsistency measures based on these semantics and show
that they satisfy important desirable properties. We show how these measures
can be applied to declarative process models and investigate the computational
complexity of the introduced approach.",None,-1
On Mitigating Hard Clusters for Face Clustering,0.055154,"Face clustering is a promising way to scale up face recognition systems using
large-scale unlabeled face images. It remains challenging to identify small or
sparse face image clusters that we call hard clusters, which is caused by the
heterogeneity, \ie, high variations in size and sparsity, of the clusters.
Consequently, the conventional way of using a uniform threshold (to identify
clusters) often leads to a terrible misclassification for the samples that
should belong to hard clusters. We tackle this problem by leveraging the
neighborhood information of samples and inferring the cluster memberships (of
samples) in a probabilistic way. We introduce two novel modules,
Neighborhood-Diffusion-based Density (NDDe) and Transition-Probability-based
Distance (TPDi), based on which we can simply apply the standard Density Peak
Clustering algorithm with a uniform threshold. Our experiments on multiple
benchmarks show that each module contributes to the final performance of our
method, and by incorporating them into other advanced face clustering methods,
these two modules can boost the performance of these methods to a new
state-of-the-art. Code is available at:
https://github.com/echoanran/On-Mitigating-Hard-Clusters.",https://github.com/echoanran/On-Mitigating-Hard-Clusters,-1
FixMatchSeg: Fixing FixMatch for Semi-Supervised Semantic Segmentation,0.0783292,"Supervised deep learning methods for semantic medical image segmentation are
getting increasingly popular in the past few years.However, in resource
constrained settings, getting large number of annotated images is very
difficult as it mostly requires experts, is expensive and
time-consuming.Semi-supervised segmentation can be an attractive solution where
a very few labeled images are used along with a large number of unlabeled ones.
While the gap between supervised and semi-supervised methods have been
dramatically reduced for classification problems in the past couple of years,
there still remains a larger gap in segmentation methods. In this work, we
adapt a state-of-the-art semi-supervised classification method FixMatch to
semantic segmentation task, introducing FixMatchSeg. FixMatchSeg is evaluated
in four different publicly available datasets of different anatomy and
different modality: cardiac ultrasound, chest X-ray, retinal fundus image, and
skin images. When there are few labels, we show that FixMatchSeg performs on
par with strong supervised baselines.",https://github.com/qubvel/segmentation_models.pytorch,-1
Optimizing Fiducial Marker Placement for Improved Visual Localization,0.0230314,"Adding fiducial markers to a scene is a well-known strategy for making visual
localization algorithms more robust. Traditionally, these marker locations are
selected by humans who are familiar with visual localization techniques. This
paper explores the problem of automatic marker placement within a scene.
Specifically, given a predetermined set of markers and a scene model, we
compute optimized marker positions within the scene that can improve accuracy
in visual localization. Our main contribution is a novel framework for modeling
camera localizability that incorporates both natural scene features and
artificial fiducial markers added to the scene. We present optimized marker
placement (OMP), a greedy algorithm that is based on the camera localizability
framework. We have also designed a simulation framework for testing marker
placement algorithms on 3D models and images generated from synthetic scenes.
We have evaluated OMP within this testbed and demonstrate an improvement in the
localization rate by up to 20 percent on four different scenes.",https://github.com/doublestrong/OMP,-1
Encryption and encoding of facial images into quick response and high capacity color 2d code for biometric passport security system,0.0300982,"In this thesis, a multimodal biometric, secure encrypted data and encrypted
biometric encoded into the QR code-based biometric-passport authentication
method is proposed for national security applications. Firstly, using the
Extended Profile - Local Binary Patterns (EP-LBP), a Canny edge detector, and
the Scale Invariant Feature Transform (SIFT) algorithm with Image File
Information (IMFINFO) process, the facial mark size recognition is initially
achieved. Secondly, by using the Active Shape Model (ASM) into Active
Appearance Model (AAM) to follow the hand and infusion the hand geometry
characteristics for verification and identification, hand geometry recognition
is achieved. Thirdly, the encrypted biometric passport information that is
publicly accessible is encoded into the QR code and inserted into the
electronic passport to improve protection. Further, Personal information and
biometric data are encrypted by applying the Advanced Encryption Standard (AES)
and the Secure Hash Algorithm (SHA) 256 algorithm. It will enhance the
biometric passport security system.",None,-1
E2PN: Efficient SE(3)-Equivariant Point Network,0.0964818,"This paper proposes a convolution structure for learning SE(3)-equivariant
features from 3D point clouds. It can be viewed as an equivariant version of
kernel point convolutions (KPConv), a widely used convolution form to process
point cloud data. Compared with existing equivariant networks, our design is
simple, lightweight, fast, and easy to be integrated with existing
task-specific point cloud learning pipelines. We achieve these desirable
properties by combining group convolutions and quotient representations.
Specifically, we discretize SO(3) to finite groups for their simplicity while
using SO(2) as the stabilizer subgroup to form spherical quotient feature
fields to save computations. We also propose a permutation layer to recover
SO(3) features from spherical features to preserve the capacity to distinguish
rotations. Experiments show that our method achieves comparable or superior
performance in various tasks, including object classification, pose estimation,
and keypoint-matching, while consuming much less memory and running faster than
existing work. The proposed method can foster the development of equivariant
models for real-world applications based on point clouds.",https://github.com/minghanz/E2PN,-1
The COVID That Wasn't: Counterfactual Journalism Using GPT,0.0158641,"In this paper, we explore the use of large language models to assess human
interpretations of real world events. To do so, we use a language model trained
prior to 2020 to artificially generate news articles concerning COVID-19 given
the headlines of actual articles written during the pandemic. We then compare
stylistic qualities of our artificially generated corpus with a news corpus, in
this case 5,082 articles produced by CBC News between January 23 and May 5,
2020. We find our artificially generated articles exhibits a considerably more
negative attitude towards COVID and a significantly lower reliance on
geopolitical framing. Our methods and results hold importance for researchers
seeking to simulate large scale cultural processes via recent breakthroughs in
text generation.",None,-1
Feature Overcorrelation in Deep Graph Neural Networks: A New Perspective,0.0167307,"Recent years have witnessed remarkable success achieved by graph neural
networks (GNNs) in many real-world applications such as recommendation and drug
discovery. Despite the success, oversmoothing has been identified as one of the
key issues which limit the performance of deep GNNs. It indicates that the
learned node representations are highly indistinguishable due to the stacked
aggregators. In this paper, we propose a new perspective to look at the
performance degradation of deep GNNs, i.e., feature overcorrelation. Through
empirical and theoretical study on this matter, we demonstrate the existence of
feature overcorrelation in deeper GNNs and reveal potential reasons leading to
this issue. To reduce the feature correlation, we propose a general framework
DeCorr which can encourage GNNs to encode less redundant information. Extensive
experiments have demonstrated that DeCorr can help enable deeper GNNs and is
complementary to existing techniques tackling the oversmoothing issue.",https://github.com/ChandlerBang/DeCorr,-1
Improving Few-Shot Part Segmentation using Coarse Supervision,0.0379066,"A significant bottleneck in training deep networks for part segmentation is
the cost of obtaining detailed annotations. We propose a framework to exploit
coarse labels such as figure-ground masks and keypoint locations that are
readily available for some categories to improve part segmentation models. A
key challenge is that these annotations were collected for different tasks and
with different labeling styles and cannot be readily mapped to the part labels.
To this end, we propose to jointly learn the dependencies between labeling
styles and the part segmentation model, allowing us to utilize supervision from
diverse labels. To evaluate our approach we develop a benchmark on the
Caltech-UCSD birds and OID Aircraft dataset. Our approach outperforms baselines
based on multi-task learning, semi-supervised learning, and competitive methods
relying on loss functions manually designed to exploit sparse-supervision.",None,-1
Black-Box Attack against GAN-Generated Image Detector with Contrastive Perturbation,0.0200531,"Visually realistic GAN-generated facial images raise obvious concerns on
potential misuse. Many effective forensic algorithms have been developed to
detect such synthetic images in recent years. It is significant to assess the
vulnerability of such forensic detectors against adversarial attacks. In this
paper, we propose a new black-box attack method against GAN-generated image
detectors. A novel contrastive learning strategy is adopted to train the
encoder-decoder network based anti-forensic model under a contrastive loss
function. GAN images and their simulated real counterparts are constructed as
positive and negative samples, respectively. Leveraging on the trained attack
model, imperceptible contrastive perturbation could be applied to input
synthetic images for removing GAN fingerprint to some extent. As such, existing
GAN-generated image detectors are expected to be deceived. Extensive
experimental results verify that the proposed attack effectively reduces the
accuracy of three state-of-the-art detectors on six popular GANs. High visual
quality of the attacked images is also achieved. The source code will be
available at https://github.com/ZXMMD/BAttGAND.",https://github.com/ZXMMD/BAttGAND,-1
Robust-by-Design Classification via Unitary-Gradient Neural Networks,0.0122798,"The use of neural networks in safety-critical systems requires safe and
robust models, due to the existence of adversarial attacks. Knowing the minimal
adversarial perturbation of any input x, or, equivalently, knowing the distance
of x from the classification boundary, allows evaluating the classification
robustness, providing certifiable predictions. Unfortunately, state-of-the-art
techniques for computing such a distance are computationally expensive and
hence not suited for online applications. This work proposes a novel family of
classifiers, namely Signed Distance Classifiers (SDCs), that, from a
theoretical perspective, directly output the exact distance of x from the
classification boundary, rather than a probability score (e.g., SoftMax). SDCs
represent a family of robust-by-design classifiers. To practically address the
theoretical requirements of a SDC, a novel network architecture named
Unitary-Gradient Neural Network is presented. Experimental results show that
the proposed architecture approximates a signed distance classifier, hence
allowing an online certifiable classification of x at the cost of a single
inference.",None,-1
VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification,0.243268,"Multimodal learning from document data has achieved great success lately as
it allows to pre-train semantically meaningful features as a prior into a
learnable downstream task. In this paper, we approach the document
classification problem by learning cross-modal representations through language
and vision cues, considering intra- and inter-modality relationships. Instead
of merging features from different modalities into a joint representation
space, the proposed method exploits high-level interactions and learns relevant
semantic information from effective attention flows within and across
modalities. The proposed learning objective is devised between intra- and
inter-modality alignment tasks, where the similarity distribution per task is
computed by contracting positive sample pairs while simultaneously contrasting
negative ones in the joint representation space}. Extensive experiments on
public document classification datasets demonstrate the effectiveness and the
generality of our model on low-scale and large-scale datasets.",https://github.com/tesseract-ocr/tesseract,-1
Towards Uniform Point Distribution in Feature-preserving Point Cloud Filtering,0.209351,"As a popular representation of 3D data, point cloud may contain noise and
need to be filtered before use. Existing point cloud filtering methods either
cannot preserve sharp features or result in uneven point distribution in the
filtered output. To address this problem, this paper introduces a point cloud
filtering method that considers both point distribution and feature
preservation during filtering. The key idea is to incorporate a repulsion term
with a data term in energy minimization. The repulsion term is responsible for
the point distribution, while the data term is to approximate the noisy
surfaces while preserving the geometric features. This method is capable of
handling models with fine-scale features and sharp features. Extensive
experiments show that our method yields better results with a more uniform
point distribution ($5.8\times10^{-5}$ Chamfer Distance on average) in seconds.",None,-1
Storehouse: a Reinforcement Learning Environment for Optimizing Warehouse Management,0.0441457,"Warehouse Management Systems have been evolving and improving thanks to new
Data Intelligence techniques. However, many current optimizations have been
applied to specific cases or are in great need of manual interaction. Here is
where Reinforcement Learning techniques come into play, providing
automatization and adaptability to current optimization policies. In this
paper, we present Storehouse, a customizable environment that generalizes the
definition of warehouse simulations for Reinforcement Learning. We also
validate this environment against state-of-the-art reinforcement learning
algorithms and compare these results to human and random policies.",https://github.com/JulenCestero/storehouse,-1
Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems,0.0128577,"Stochastic optimization has found wide applications in minimizing objective
functions in machine learning, which motivates a lot of theoretical studies to
understand its practical success. Most of existing studies focus on the
convergence of optimization errors, while the generalization analysis of
stochastic optimization is much lagging behind. This is especially the case for
nonconvex and nonsmooth problems often encountered in practice. In this paper,
we initialize a systematic stability and generalization analysis of stochastic
optimization on nonconvex and nonsmooth problems. We introduce novel
algorithmic stability measures and establish their quantitative connection on
the gap between population gradients and empirical gradients, which is then
further extended to study the gap between the Moreau envelope of the empirical
risk and that of the population risk. To our knowledge, these quantitative
connection between stability and generalization in terms of either gradients or
Moreau envelopes have not been studied in the literature. We introduce a class
of sampling-determined algorithms, for which we develop bounds for three
stability measures. Finally, we apply these discussions to derive error bounds
for stochastic gradient descent and its adaptive variant, where we show how to
achieve an implicit regularization by tuning the step sizes and the number of
iterations.",None,-1
TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters,0.137991,"Topic taxonomies, which represent the latent topic (or category) structure of
document collections, provide valuable knowledge of contents in many
applications such as web search and information filtering. Recently, several
unsupervised methods have been developed to automatically construct the topic
taxonomy from a text corpus, but it is challenging to generate the desired
taxonomy without any prior knowledge. In this paper, we study how to leverage
the partial (or incomplete) information about the topic structure as guidance
to find out the complete topic taxonomy. We propose a novel framework for topic
taxonomy completion, named TaxoCom, which recursively expands the topic
taxonomy by discovering novel sub-topic clusters of terms and documents. To
effectively identify novel topics within a hierarchical topic structure,
TaxoCom devises its embedding and clustering techniques to be closely-linked
with each other: (i) locally discriminative embedding optimizes the text
embedding space to be discriminative among known (i.e., given) sub-topics, and
(ii) novelty adaptive clustering assigns terms into either one of the known
sub-topics or novel sub-topics. Our comprehensive experiments on two real-world
datasets demonstrate that TaxoCom not only generates the high-quality topic
taxonomy in terms of term coherency and topic coverage but also outperforms all
other baselines for a downstream task.",https://github.com/joewandy/hlda,-1
Optical Flow Training under Limited Label Budget via Active Learning,0.019029,"Supervised training of optical flow predictors generally yields better
accuracy than unsupervised training. However, the improved performance comes at
an often high annotation cost. Semi-supervised training trades off accuracy
against annotation cost. We use a simple yet effective semi-supervised training
method to show that even a small fraction of labels can improve flow accuracy
by a significant margin over unsupervised training. In addition, we propose
active learning methods based on simple heuristics to further reduce the number
of labels required to achieve the same target accuracy. Our experiments on both
synthetic and real optical flow datasets show that our semi-supervised networks
generally need around 50% of the labels to achieve close to full-label
accuracy, and only around 20% with active learning on Sintel. We also analyze
and show insights on the factors that may influence active learning
performance. Code is available at
https://github.com/duke-vision/optical-flow-active-learning-release.",https://github.com/duke-vision/optical-flow-active-learning-release,-1
On the expressive power of message-passing neural networks as global feature map transformers,0.0958993,"We investigate the power of message-passing neural networks (MPNNs) in their
capacity to transform the numerical features stored in the nodes of their input
graphs. Our focus is on global expressive power, uniformly over all input
graphs, or over graphs of bounded degree with features from a bounded domain.
Accordingly, we introduce the notion of a global feature map transformer
(GFMT). As a yardstick for expressiveness, we use a basic language for GFMTs,
which we call MPLang. Every MPNN can be expressed in MPLang, and our results
clarify to which extent the converse inclusion holds. We consider exact versus
approximate expressiveness; the use of arbitrary activation functions; and the
case where only the ReLU activation function is allowed.",None,-1
Jacobian Computation for Cumulative B-Splines on SE(3) and Application to Continuous-Time Object Tracking,0.0693218,"In this paper we propose a method that estimates the $SE(3)$ continuous
trajectories (orientation and translation) of the dynamic rigid objects present
in a scene, from multiple RGB-D views. Specifically, we fit the object
trajectories to cumulative B-Splines curves, which allow us to interpolate, at
any intermediate time stamp, not only their poses but also their linear and
angular velocities and accelerations. Additionally, we derive in this work the
analytical $SE(3)$ Jacobians needed by the optimization, being applicable to
any other approach that uses this type of curves. To the best of our knowledge
this is the first work that proposes 6-DoF continuous-time object tracking,
which we endorse with significant computational cost reduction thanks to our
analytical derivations. We evaluate our proposal in synthetic data and in a
public benchmark, showing competitive results in localization and significant
improvements in velocity estimation in comparison to discrete-time approaches.",None,-1
Shared Coupling-bridge for Weakly Supervised Local Feature Learning,0.0446303,"Sparse local feature extraction is usually believed to be of important
significance in typical vision tasks such as simultaneous localization and
mapping, image matching and 3D reconstruction. At present, it still has some
deficiencies needing further improvement, mainly including the discrimination
power of extracted local descriptors, the localization accuracy of detected
keypoints, and the efficiency of local feature learning. This paper focuses on
promoting the currently popular sparse local feature learning with camera pose
supervision. Therefore, it pertinently proposes a Shared Coupling-bridge scheme
with four light-weight yet effective improvements for weakly-supervised local
feature (SCFeat) learning. It mainly contains: i) the
\emph{Feature-Fusion-ResUNet Backbone} (F2R-Backbone) for local descriptors
learning, ii) a shared coupling-bridge normalization to improve the decoupling
training of description network and detection network, iii) an improved
detection network with peakiness measurement to detect keypoints and iv) the
fundamental matrix error as a reward factor to further optimize feature
detection training. Extensive experiments prove that our SCFeat improvement is
effective. It could often obtain a state-of-the-art performance on classic
image matching and visual localization. In terms of 3D reconstruction, it could
still achieve competitive results. For sharing and communication, our source
codes are available at https://github.com/sunjiayuanro/SCFeat.git.",https://github.com/sunjiayuanro/SCFeat.git,-1
SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding,0.256158,"In this paper, we investigate how to achieve better visual grounding with
modern vision-language transformers, and propose a simple yet powerful
Selective Retraining (SiRi) mechanism for this challenging task. Particularly,
SiRi conveys a significant principle to the research of visual grounding, i.e.,
a better initialized vision-language encoder would help the model converge to a
better local minimum, advancing the performance accordingly. In specific, we
continually update the parameters of the encoder as the training goes on, while
periodically re-initialize rest of the parameters to compel the model to be
better optimized based on an enhanced encoder. SiRi can significantly
outperform previous approaches on three popular benchmarks. Specifically, our
method achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the
state-of-the-art approaches (training from scratch) by more than 10.21%.
Additionally, we reveal that SiRi performs surprisingly superior even with
limited training data. We also extend it to transformer-based visual grounding
models and other vision-language tasks to verify the validity.",https://github.com/qumengxue/siri-vg.git,-1
Grammar Detection for Sentiment Analysis through Improved Viterbi Algorithm,0.0305068,"Grammar Detection, also referred to as Parts of Speech Tagging of raw text,
is considered an underlying building block of the various Natural Language
Processing pipelines like named entity recognition, question answering, and
sentiment analysis. In short, forgiven a sentence, Parts of Speech tagging is
the task of specifying and tagging each word of a sentence with nouns, verbs,
adjectives, adverbs, and more. Sentiment Analysis may well be a procedure
accustomed to determining if a given sentence's emotional tone is neutral,
positive or negative. To assign polarity scores to the thesis or entities
within phrase, in-text analysis and analytics, machine learning and natural
language processing, approaches are incorporated. This Sentiment Analysis using
POS tagger helps us urge a summary of the broader public over a specific topic.
For this, we are using the Viterbi algorithm, Hidden Markov Model, Constraint
based Viterbi algorithm for POS tagging. By comparing the accuracies, we select
the foremost accurate result of the model for Sentiment Analysis for
determining the character of the sentence.",None,-1
Automatic Depression Detection: An Emotional Audio-Textual Corpus and a GRU/BiLSTM-based Model,0.627124,"Depression is a global mental health problem, the worst case of which can
lead to suicide. An automatic depression detection system provides great help
in facilitating depression self-assessment and improving diagnostic accuracy.
In this work, we propose a novel depression detection approach utilizing speech
characteristics and linguistic contents from participants' interviews. In
addition, we establish an Emotional Audio-Textual Depression Corpus
(EATD-Corpus) which contains audios and extracted transcripts of responses from
depressed and non-depressed volunteers. To the best of our knowledge,
EATD-Corpus is the first and only public depression dataset that contains audio
and text data in Chinese. Evaluated on two depression datasets, the proposed
method achieves the state-of-the-art performances. The outperforming results
demonstrate the effectiveness and generalization ability of the proposed
method. The source code and EATD-Corpus are available at
https://github.com/speechandlanguageprocessing/ICASSP2022-Depression.",https://github.com/speechandlanguageprocessing/ICASSP2022-Depression,-1
Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation,0.320553,"Attribute-based Controlled Text Generation (CTG) refers to generating
sentences that satisfy desirable attributes (e.g., emotions and topics).
Existing works often utilize fine-tuning or resort to extra attribute
classifiers, yet suffer from storage and inference time increases. To address
these concerns, we explore attribute-based CTG in a prompt-based manner. In
short, the proposed Tailor represents each attribute as a pre-trained
continuous vector (i.e., single-attribute prompt) and guides the generation of
a fixed PLM switch to a pre-specified attribute. We experimentally find that
these prompts can be simply concatenated as a whole to multi-attribute CTG
without any re-training, yet raises problems of fluency decrease and position
sensitivity. To this end, Tailor provides a multi-attribute prompt mask and a
re-indexing position-ids sequence to bridge the gap between the training (one
prompt for each task) and testing stage (concatenating more than one prompt).
To further enhance such single-attribute prompt combinations, Tailor also
introduces a trainable prompt connector, which can be concatenated with any two
single-attribute prompts to multi-attribute text generation. Experiments on 11
attribute-specific generation tasks demonstrate strong performances of Tailor
on both single-attribute and multi-attribute CTG, with 0.08\% training
parameters of a GPT-2.",https://github.com/uber-research/PPLM,-1
MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages,0.138425,"We present the results of the Workshop on Multilingual Information Access
(MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question
answering (QA) systems in 16 typologically diverse languages. In this task, we
adapted two large-scale cross-lingual open-retrieval QA datasets in 14
typologically diverse languages, and newly annotated open-retrieval QA data in
2 underrepresented languages: Tagalog and Tamil. Four teams submitted their
systems. The best system leveraging iteratively mined diverse negative examples
and larger pretrained models achieves 32.2 F1, outperforming our baseline by
4.5 points. The second best system uses entity-aware contextualized
representations for document retrieval, and achieves significant improvements
in Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores.",https://github.com/mia-workshop/MIA-Shared-Task-2022,-1
A-Eye: Driving with the Eyes of AI for Corner Case Generation,0.139499,"The overall goal of this work is to enrich training data for automated
driving with so called corner cases. In road traffic, corner cases are
critical, rare and unusual situations that challenge the perception by AI
algorithms. For this purpose, we present the design of a test rig to generate
synthetic corner cases using a human-in-the-loop approach. For the test rig, a
real-time semantic segmentation network is trained and integrated into the
driving simulation software CARLA in such a way that a human can drive on the
network's prediction. In addition, a second person gets to see the same scene
from the original CARLA output and is supposed to intervene with the help of a
second control unit as soon as the semantic driver shows dangerous driving
behavior. Interventions potentially indicate poor recognition of a critical
scene by the segmentation network and then represents a corner case. In our
experiments, we show that targeted enrichment of training data with corner
cases leads to improvements in pedestrian detection in safety relevant episodes
in road traffic.",None,-1
Vector Quantized Image-to-Image Translation,0.0423941,"Current image-to-image translation methods formulate the task with
conditional generation models, leading to learning only the recolorization or
regional changes as being constrained by the rich structural information
provided by the conditional contexts. In this work, we propose introducing the
vector quantization technique into the image-to-image translation framework.
The vector quantized content representation can facilitate not only the
translation, but also the unconditional distribution shared among different
domains. Meanwhile, along with the disentangled style representation, the
proposed method further enables the capability of image extension with
flexibility in both intra- and inter-domains. Qualitative and quantitative
experiments demonstrate that our framework achieves comparable performance to
the state-of-the-art image-to-image translation and image extension methods.
Compared to methods for individual tasks, the proposed method, as a unified
framework, unleashes applications combining image-to-image translation,
unconditional generation, and image extension altogether. For example, it
provides style variability for image generation and extension, and equips
image-to-image translation with further extension capabilities.",https://cyj407.github.io/VQ-I2I/,-1
Deep Reinforcement Learning for Multi-class Imbalanced Training,0.028321,"With the rapid growth of memory and computing power, datasets are becoming
increasingly complex and imbalanced. This is especially severe in the context
of clinical data, where there may be one rare event for many cases in the
majority class. We introduce an imbalanced classification framework, based on
reinforcement learning, for training extremely imbalanced data sets, and extend
it for use in multi-class settings. We combine dueling and double deep
Q-learning architectures, and formulate a custom reward function and
episode-training procedure, specifically with the added capability of handling
multi-class imbalanced training. Using real-world clinical case studies, we
demonstrate that our proposed framework outperforms current state-of-the-art
imbalanced learning methods, achieving more fair and balanced classification,
while also significantly improving the prediction of minority classes.",None,-1
Streamable Neural Fields,0.0234773,"Neural fields have emerged as a new data representation paradigm and have
shown remarkable success in various signal representations. Since they preserve
signals in their network parameters, the data transfer by sending and receiving
the entire model parameters prevents this emerging technology from being used
in many practical scenarios. We propose streamable neural fields, a single
model that consists of executable sub-networks of various widths. The proposed
architectural and training techniques enable a single network to be streamable
over time and reconstruct different qualities and parts of signals. For
example, a smaller sub-network produces smooth and low-frequency signals, while
a larger sub-network can represent fine details. Experimental results have
shown the effectiveness of our method in various domains, such as 2D images,
videos, and 3D signed distance functions. Finally, we demonstrate that our
proposed method improves training stability, by exploiting parameter sharing.",https://github.com/jwcho5576/streamable_nf,-1
Deep Learning-based Facial Appearance Simulation Driven by Surgically Planned Craniomaxillofacial Bony Movement,0.0229279,"Simulating facial appearance change following bony movement is a critical
step in orthognathic surgical planning for patients with jaw deformities.
Conventional biomechanics-based methods such as the finite-element method (FEM)
are labor intensive and computationally inefficient. Deep learning-based
approaches can be promising alternatives due to their high computational
efficiency and strong modeling capability. However, the existing deep
learning-based method ignores the physical correspondence between facial soft
tissue and bony segments and thus is significantly less accurate compared to
FEM. In this work, we propose an Attentive Correspondence assisted Movement
Transformation network (ACMT-Net) to estimate the facial appearance by
transforming the bony movement to facial soft tissue through a point-to-point
attentive correspondence matrix. Experimental results on patients with jaw
deformity show that our proposed method can achieve comparable facial change
prediction accuracy compared with the state-of-the-art FEM-based approach with
significantly improved computational efficiency.",None,-1
xCloth: Extracting Template-free Textured 3D Clothes from a Monocular Image,0.104426,"Existing approaches for 3D garment reconstruction either assume a predefined
template for the garment geometry (restricting them to fixed clothing styles)
or yield vertex colored meshes (lacking high-frequency textural details). Our
novel framework co-learns geometric and semantic information of garment surface
from the input monocular image for template-free textured 3D garment
digitization. More specifically, we propose to extend PeeledHuman
representation to predict the pixel-aligned, layered depth and semantic maps to
extract 3D garments. The layered representation is further exploited to UV
parametrize the arbitrary surface of the extracted garment without any human
intervention to form a UV atlas. The texture is then imparted on the UV atlas
in a hybrid fashion by first projecting pixels from the input image to UV space
for the visible region, followed by inpainting the occluded regions. Thus, we
are able to digitize arbitrarily loose clothing styles while retaining
high-frequency textural details from a monocular image. We achieve
high-fidelity 3D garment reconstruction results on three publicly available
datasets and generalization on internet images.",None,-1
Channel Pruned YOLOv5-based Deep Learning Approach for Rapid and Accurate Outdoor Obstacles Detection,0.042631,"One-stage algorithm have been widely used in target detection systems that
need to be trained with massive data. Most of them perform well both in
real-time and accuracy. However, due to their convolutional structure, they
need more computing power and greater memory consumption. Hence, we applied
pruning strategy to target detection networks to reduce the number of
parameters and the size of model. To demonstrate the practicality of the
pruning method, we select the YOLOv5 model for experiments and provide a data
set of outdoor obstacles to show the effect of model. In this specific data
set, in the best circumstances, the volume of the network model is reduced by
49.7% compared with the original model, and the reasoning time is reduced by
52.5%. Meanwhile, it also uses data processing methods to compensate for the
drop in accuracy caused by pruning.",None,-1
Dual-former: Hybrid Self-attention Transformer for Efficient Image Restoration,0.0737297,"Recently, image restoration transformers have achieved comparable performance
with previous state-of-the-art CNNs. However, how to efficiently leverage such
architectures remains an open problem. In this work, we present Dual-former
whose critical insight is to combine the powerful global modeling ability of
self-attention modules and the local modeling ability of convolutions in an
overall architecture. With convolution-based Local Feature Extraction modules
equipped in the encoder and the decoder, we only adopt a novel Hybrid
Transformer Block in the latent layer to model the long-distance dependence in
spatial dimensions and handle the uneven distribution between channels. Such a
design eliminates the substantial computational complexity in previous image
restoration transformers and achieves superior performance on multiple image
restoration tasks. Experiments demonstrate that Dual-former achieves a 1.91dB
gain over the state-of-the-art MAXIM method on the Indoor dataset for single
image dehazing while consuming only 4.2% GFLOPs as MAXIM. For single image
deraining, it exceeds the SOTA method by 0.1dB PSNR on the average results of
five datasets with only 21.5% GFLOPs. Dual-former also substantially surpasses
the latest desnowing method on various datasets, with fewer parameters.",None,-1
Open Challenges in Musical Metacreation,0.168475,"Musical Metacreation tries to obtain creative behaviors from computers
algorithms composing music. In this paper I briefly analyze how this field
evolved from algorithmic composition to be focused on the search for
creativity, and I point out some issues in pursuing this goal. Finally, I argue
that hybridization of algorithms can be a useful direction for research.",None,-1
The solution set of fuzzy relation equations with addition-min composition,0.0482548,"This paper deals with the resolutions of fuzzy relation equations with
addition-min composition. When the fuzzy relation equations have a solution, we
first propose an algorithm to find all minimal solutions of the fuzzy relation
equations and also supply an algorithm to find all maximal solutions of the
fuzzy relation equations, which will be illustrated, respectively, by numeral
examples. Then we prove that every solution of the fuzzy relation equations is
between a minimal solution and a maximal one, so that we describe the solution
set of the fuzzy relation equations completely.",None,-1
AiTLAS: Artificial Intelligence Toolbox for Earth Observation,0.0605564,"The AiTLAS toolbox (Artificial Intelligence Toolbox for Earth Observation)
includes state-of-the-art machine learning methods for exploratory and
predictive analysis of satellite imagery as well as repository of AI-ready
Earth Observation (EO) datasets. It can be easily applied for a variety of
Earth Observation tasks, such as land use and cover classification, crop type
prediction, localization of specific objects (semantic segmentation), etc. The
main goal of AiTLAS is to facilitate better usability and adoption of novel AI
methods (and models) by EO experts, while offering easy access and standardized
format of EO datasets to AI experts which further allows benchmarking of
various existing and novel AI methods tailored for EO data.",https://github.com/biasvariancelabs/aitlas,-1
Discovering Salient Neurons in Deep NLP Models,0.0539453,"While a lot of work has been done in understanding representations learned
within deep NLP models and what knowledge they capture, little attention has
been paid towards individual neurons. We present a technique called as
Linguistic Correlation Analysis to extract salient neurons in the model, with
respect to any extrinsic property - with the goal of understanding how such a
knowledge is preserved within neurons. We carry out a fine-grained analysis to
answer the following questions: (i) can we identify subsets of neurons in the
network that capture specific linguistic properties? (ii) how localized or
distributed neurons are across the network? iii) how redundantly is the
information preserved? iv) how fine-tuning pre-trained models towards
downstream NLP tasks, impacts the learned linguistic knowledge? iv) how do
architectures vary in learning different linguistic properties? Our
data-driven, quantitative analysis illuminates interesting findings: (i) we
found small subsets of neurons that can predict different linguistic tasks, ii)
with neurons capturing basic lexical information (such as suffixation)
localized in lower most layers, iii) while those learning complex concepts
(such as syntactic role) predominantly in middle and higher layers, iii) that
salient linguistic neurons are relocated from higher to lower layers during
transfer learning, as the network preserve the higher layers for task specific
information, iv) we found interesting differences across pre-trained models,
with respect to how linguistic information is preserved within, and v) we found
that concept exhibit similar neuron distribution across different languages in
the multilingual transformer models. Our code is publicly available as part of
the NeuroX toolkit.",None,-1
Robust Multi-Object Tracking by Marginal Inference,0.0809328,"Multi-object tracking in videos requires to solve a fundamental problem of
one-to-one assignment between objects in adjacent frames. Most methods address
the problem by first discarding impossible pairs whose feature distances are
larger than a threshold, followed by linking objects using Hungarian algorithm
to minimize the overall distance. However, we find that the distribution of the
distances computed from Re-ID features may vary significantly for different
videos. So there isn't a single optimal threshold which allows us to safely
discard impossible pairs. To address the problem, we present an efficient
approach to compute a marginal probability for each pair of objects in real
time. The marginal probability can be regarded as a normalized distance which
is significantly more stable than the original feature distance. As a result,
we can use a single threshold for all videos. The approach is general and can
be applied to the existing trackers to obtain about one point improvement in
terms of IDF1 metric. It achieves competitive results on MOT17 and MOT20
benchmarks. In addition, the computed probability is more interpretable which
facilitates subsequent post-processing operations.",None,-1
Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models,0.578328,"We explore the idea of compressing the prompts used to condition language
models, and show that compressed prompts can retain a substantive amount of
information about the original prompt. For severely compressed prompts, while
fine-grained information is lost, abstract information and general sentiments
can be retained with surprisingly few parameters, which can be useful in the
context of decode-time algorithms for controllability and toxicity reduction.
We explore contrastive conditioning to steer language model generation towards
desirable text and away from undesirable text, and find that some complex
prompts can be effectively compressed into a single token to guide generation.
We also show that compressed prompts are largely compositional, and can be
constructed such that they can be used to control independent aspects of
generated text.",https://github.com/BYU-PCCL/prompt-compression-contrastive-coding,-1
ImPosing: Implicit Pose Encoding for Efficient Visual Localization,0.00585047,"We propose a novel learning-based formulation for visual localization of
vehicles that can operate in real-time in city-scale environments. Visual
localization algorithms determine the position and orientation from which an
image has been captured, using a set of geo-referenced images or a 3D scene
representation. Our new localization paradigm, named Implicit Pose Encoding
(ImPosing), embeds images and camera poses into a common latent representation
with 2 separate neural networks, such that we can compute a similarity score
for each image-pose pair. By evaluating candidates through the latent space in
a hierarchical manner, the camera position and orientation are not directly
regressed but incrementally refined. Very large environments force competitors
to store gigabytes of map data, whereas our method is very compact
independently of the reference database size. In this paper, we describe how to
effectively optimize our learned modules, how to combine them to achieve
real-time localization, and demonstrate results on diverse large scale
scenarios that significantly outperform prior work in accuracy and
computational efficiency.",https://github.com/Nanne/pytorch-NetVlad,-1
Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion,0.0617698,"Deep 3D point cloud models are sensitive to adversarial attacks, which poses
threats to safety-critical applications such as autonomous driving. Robust
training and defend-by-denoising are typical strategies for defending
adversarial perturbations. However, they either induce massive computational
overhead or rely heavily upon specified priors, limiting generalized robustness
against attacks of all kinds. To remedy it, this paper introduces a novel
distortion-aware defense framework that can rebuild the pristine data
distribution with a tailored intensity estimator and a diffusion model. To
perform distortion-aware forward diffusion, we design a distortion estimation
algorithm that is obtained by summing the distance of each point to the
best-fitting plane of its local neighboring points, which is based on the
observation of the local spatial properties of the adversarial point cloud. By
iterative diffusion and reverse denoising, the perturbed point cloud under
various distortions can be restored back to a clean distribution. This approach
enables effective defense against adaptive attacks with varying noise budgets,
enhancing the robustness of existing 3D deep recognition models.",None,-1
Practical Phase Retrieval Using Double Deep Image Priors,0.0952199,"Phase retrieval (PR) concerns the recovery of complex phases from complex
magnitudes. We identify the connection between the difficulty level and the
number and variety of symmetries in PR problems. We focus on the most difficult
far-field PR (FFPR), and propose a novel method using double deep image priors.
In realistic evaluation, our method outperforms all competing methods by large
margins. As a single-instance method, our method requires no training data and
minimal hyperparameter tuning, and hence enjoys good practicality.",None,-1
Scalar is Not Enough: Vectorization-based Unbiased Learning to Rank,0.00874385,"Unbiased learning to rank (ULTR) aims to train an unbiased ranking model from
biased user click logs. Most of the current ULTR methods are based on the
examination hypothesis (EH), which assumes that the click probability can be
factorized into two scalar functions, one related to ranking features and the
other related to bias factors. Unfortunately, the interactions among features,
bias factors and clicks are complicated in practice, and usually cannot be
factorized in this independent way. Fitting click data with EH could lead to
model misspecification and bring the approximation error.
  In this paper, we propose a vector-based EH and formulate the click
probability as a dot product of two vector functions. This solution is complete
due to its universality in fitting arbitrary click functions. Based on it, we
propose a novel model named Vectorization to adaptively learn the relevance
embeddings and sort documents by projecting embeddings onto a base vector.
Extensive experiments show that our method significantly outperforms the
state-of-the-art ULTR methods on complex real clicks as well as simple
simulated clicks.",https://github.com/Keytoyze/Vectorization,-1
Learning English with Peppa Pig,0.170098,"Recent computational models of the acquisition of spoken language via
grounding in perception exploit associations between the spoken and visual
modalities and learn to represent speech and visual data in a joint vector
space. A major unresolved issue from the point of ecological validity is the
training data, typically consisting of images or videos paired with spoken
descriptions of what is depicted. Such a setup guarantees an unrealistically
strong correlation between speech and the visual data. In the real world the
coupling between the linguistic and the visual modality is loose, and often
confounded by correlations with non-semantic aspects of the speech signal. Here
we address this shortcoming by using a dataset based on the children's cartoon
Peppa Pig. We train a simple bi-modal architecture on the portion of the data
consisting of dialog between characters, and evaluate on segments containing
descriptive narrations. Despite the weak and confounded signal in this training
data our model succeeds at learning aspects of the visual semantics of spoken
language.",https://github.com/gchrupala/peppa,-1
Por Qué Não Utiliser Alla Språk? Mixed Training with Gradient Optimization in Few-Shot Cross-Lingual Transfer,0.208634,"The current state-of-the-art for few-shot cross-lingual transfer learning
first trains on abundant labeled data in the source language and then
fine-tunes with a few examples on the target language, termed target-adapting.
Though this has been demonstrated to work on a variety of tasks, in this paper
we show some deficiencies of this approach and propose a one-step mixed
training method that trains on both source and target data with
\textit{stochastic gradient surgery}, a novel gradient-level optimization.
Unlike the previous studies that focus on one language at a time when
target-adapting, we use one model to handle all target languages simultaneously
to avoid excessively language-specific models. Moreover, we discuss the
unreality of utilizing large target development sets for model selection in
previous literature. We further show that our method is both development-free
for target languages, and is also able to escape from overfitting issues. We
conduct a large-scale experiment on 4 diverse NLP tasks across up to 48
languages. Our proposed method achieves state-of-the-art performance on all
tasks and outperforms target-adapting by a large margin, especially for
languages that are linguistically distant from the source language, e.g., 7.36%
F1 absolute gain on average for the NER task, up to 17.60% on Punjabi.",https://github.com/fe1ixxu/Mixed-Gradient-Few-Shot,-1
Font Shape-to-Impression Translation,0.0344389,"Different fonts have different impressions, such as elegant, scary, and cool.
This paper tackles part-based shape-impression analysis based on the
Transformer architecture, which is able to handle the correlation among local
parts by its self-attention mechanism. This ability will reveal how
combinations of local parts realize a specific impression of a font. The
versatility of Transformer allows us to realize two very different approaches
for the analysis, i.e., multi-label classification and translation. A
quantitative evaluation shows that our Transformer-based approaches estimate
the font impressions from a set of local parts more accurately than other
approaches. A qualitative evaluation then indicates the important local parts
for a specific impression.",None,-1
Generalised Implicit Neural Representations,0.0519401,"We consider the problem of learning implicit neural representations (INRs)
for signals on non-Euclidean domains. In the Euclidean case, INRs are trained
on a discrete sampling of a signal over a regular lattice. Here, we assume that
the continuous signal exists on some unknown topological space from which we
sample a discrete graph. In the absence of a coordinate system to identify the
sampled nodes, we propose approximating their location with a spectral
embedding of the graph. This allows us to train INRs without knowing the
underlying continuous domain, which is the case for most graph signals in
nature, while also making the INRs independent of any choice of coordinate
system. We show experiments with our method on various real-world signals on
non-Euclidean domains.",https://github.com/danielegrattarola/GINR,-1
Meta-Learning Fast Weight Language Models,0.0554116,"Dynamic evaluation of language models (LMs) adapts model parameters at test
time using gradient information from previous tokens and substantially improves
LM performance. However, it requires over 3x more compute than standard
inference. We present Fast Weight Layers (FWLs), a neural component that
provides the benefits of dynamic evaluation much more efficiently by expressing
gradient updates as linear attention. A key improvement over dynamic evaluation
is that FWLs can also be applied at training time so the model learns to make
good use of gradient updates. FWLs can easily be added on top of existing
transformer models, require relatively little extra compute or memory to run,
and significantly improve language modeling perplexity.",https://github.com/google-research/fwl,-1
Prompt-driven efficient Open-set Semi-supervised Learning,0.0364922,"Open-set semi-supervised learning (OSSL) has attracted growing interest,
which investigates a more practical scenario where out-of-distribution (OOD)
samples are only contained in unlabeled data. Existing OSSL methods like
OpenMatch learn an OOD detector to identify outliers, which often update all
modal parameters (i.e., full fine-tuning) to propagate class information from
labeled data to unlabeled ones. Currently, prompt learning has been developed
to bridge gaps between pre-training and fine-tuning, which shows higher
computational efficiency in several downstream tasks. In this paper, we propose
a prompt-driven efficient OSSL framework, called OpenPrompt, which can
propagate class information from labeled to unlabeled data with only a small
number of trainable parameters. We propose a prompt-driven joint space learning
mechanism to detect OOD data by maximizing the distribution gap between ID and
OOD samples in unlabeled data, thereby our method enables the outliers to be
detected in a new way. The experimental results on three public datasets show
that OpenPrompt outperforms state-of-the-art methods with less than 1% of
trainable parameters. More importantly, OpenPrompt achieves a 4% improvement in
terms of AUROC on outlier detection over a fully supervised model on CIFAR10.",None,-1
Entailment Semantics Can Be Extracted from an Ideal Language Model,0.185341,"Language models are often trained on text alone, without additional
grounding. There is debate as to how much of natural language semantics can be
inferred from such a procedure. We prove that entailment judgments between
sentences can be extracted from an ideal language model that has perfectly
learned its target distribution, assuming the training sentences are generated
by Gricean agents, i.e., agents who follow fundamental principles of
communication from the linguistic theory of pragmatics. We also show entailment
judgments can be decoded from the predictions of a language model trained on
such Gricean data. Our results reveal a pathway for understanding the semantic
information encoded in unlabeled linguistic data and a potential framework for
extracting semantics from language models.",https://github.com/viking-sudo-rm/,-1
Reinforced Question Rewriting for Conversational Question Answering,0.207995,"Conversational Question Answering (CQA) aims to answer questions contained
within dialogues, which are not easily interpretable without context.
Developing a model to rewrite conversational questions into self-contained ones
is an emerging solution in industry settings as it allows using existing
single-turn QA systems to avoid training a CQA model from scratch. Previous
work trains rewriting models using human rewrites as supervision. However, such
objectives are disconnected with QA models and therefore more human-like
rewrites do not guarantee better QA performance. In this paper we propose using
QA feedback to supervise the rewriting model with reinforcement learning.
Experiments show that our approach can effectively improve QA performance over
baselines for both extractive and retrieval QA. Furthermore, human evaluation
shows that our method can generate more accurate and detailed rewrites when
compared to human annotations.",None,-1
Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective,0.0774653,"Unsupervised video domain adaptation is a practical yet challenging task. In
this work, for the first time, we tackle it from a disentanglement view. Our
key idea is to handle the spatial and temporal domain divergence separately
through disentanglement. Specifically, we consider the generation of
cross-domain videos from two sets of latent factors, one encoding the static
information and another encoding the dynamic information. A Transfer Sequential
VAE (TranSVAE) framework is then developed to model such generation. To better
serve for adaptation, we propose several objectives to constrain the latent
factors. With these constraints, the spatial divergence can be readily removed
by disentangling the static domain-specific information out, and the temporal
divergence is further reduced from both frame- and video-levels through
adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and
Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE
compared with several state-of-the-art approaches. Code is publicly available.",https://github.com/ldkong1205/TranSVAE,-1
Contrastive Representation Learning for Gaze Estimation,0.138989,"Self-supervised learning (SSL) has become prevalent for learning
representations in computer vision. Notably, SSL exploits contrastive learning
to encourage visual representations to be invariant under various image
transformations. The task of gaze estimation, on the other hand, demands not
just invariance to various appearances but also equivariance to the geometric
transformations. In this work, we propose a simple contrastive representation
learning framework for gaze estimation, named Gaze Contrastive Learning
(GazeCLR). GazeCLR exploits multi-view data to promote equivariance and relies
on selected data augmentation techniques that do not alter gaze directions for
invariance learning. Our experiments demonstrate the effectiveness of GazeCLR
for several settings of the gaze estimation task. Particularly, our results
show that GazeCLR improves the performance of cross-domain gaze estimation and
yields as high as 17.2% relative improvement. Moreover, the GazeCLR framework
is competitive with state-of-the-art representation learning methods for
few-shot evaluation. The code and pre-trained models are available at
https://github.com/jswati31/gazeclr.",None,-1
Vakyansh: ASR Toolkit for Low Resource Indic languages,0.0118745,"We present Vakyansh, an end to end toolkit for Speech Recognition in Indic
languages. India is home to almost 121 languages and around 125 crore speakers.
Yet most of the languages are low resource in terms of data and pretrained
models. Through Vakyansh, we introduce automatic data pipelines for data
creation, model training, model evaluation and deployment. We create 14,000
hours of speech data in 23 Indic languages and train wav2vec 2.0 based
pretrained models. These pretrained models are then finetuned to create state
of the art speech recognition models for 18 Indic languages which are followed
by language models and punctuation restoration models. We open source all these
resources with a mission that this will inspire the speech community to develop
speech first applications using our ASR models in Indic languages.",https://github.com/Open-Speech-EkStep/vakyansh-models,-1
Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise,0.189091,"In this paper, we introduce a novel dIffusion language modEl pre-training
framework for text generation, which we call GENIE. GENIE is a large-scale
pretrained diffusion language model that consists of an encoder and a
diffusion-based decoder, which can generate text by gradually transforming a
random noise sequence into a coherent text sequence. To pre-train GENIE on a
large-scale language corpus, we design a new continuous paragraph denoise
objective, which encourages the diffusion-decoder to reconstruct a clean text
paragraph from a corrupted version, while preserving the semantic and syntactic
coherence. We evaluate GENIE on four downstream text generation benchmarks,
namely XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results
show that GENIE achieves comparable performance with the state-of-the-art
autoregressive models on these benchmarks, and generates more diverse text
samples. The code and models of GENIE are available at
https://github.com/microsoft/ProphetNet/tree/master/GENIE.",https://github.com/microsoft/ProphetNet/tree/master/GENIE,-1
Topic-Aware Response Generation in Task-Oriented Dialogue with Unstructured Knowledge Access,0.0481948,"To alleviate the problem of structured databases' limited coverage, recent
task-oriented dialogue systems incorporate external unstructured knowledge to
guide the generation of system responses. However, these usually use word or
sentence level similarities to detect the relevant knowledge context, which
only partially capture the topical level relevance. In this paper, we examine
how to better integrate topical information in knowledge grounded task-oriented
dialogue and propose ``Topic-Aware Response Generation'' (TARG), an end-to-end
response generation model. TARG incorporates multiple topic-aware attention
mechanisms to derive the importance weighting scheme over dialogue utterances
and external knowledge sources towards a better understanding of the dialogue
history. Experimental results indicate that TARG achieves state-of-the-art
performance in knowledge selection and response generation, outperforming
previous state-of-the-art by 3.2, 3.6, and 4.2 points in EM, F1 and BLEU-4
respectively on Doc2Dial, and performing comparably with previous work on
DSTC9; both being knowledge-grounded task-oriented dialogue datasets.",None,-1
Adversarial Examples for Good: Adversarial Examples Guided Imbalanced Learning,0.277374,"Adversarial examples are inputs for machine learning models that have been
designed by attackers to cause the model to make mistakes. In this paper, we
demonstrate that adversarial examples can also be utilized for good to improve
the performance of imbalanced learning. We provide a new perspective on how to
deal with imbalanced data: adjust the biased decision boundary by training with
Guiding Adversarial Examples (GAEs). Our method can effectively increase the
accuracy of minority classes while sacrificing little accuracy on majority
classes. We empirically show, on several benchmark datasets, our proposed
method is comparable to the state-of-the-art method. To our best knowledge, we
are the first to deal with imbalanced learning with adversarial examples.",None,-1
Unsupervised Multi-Granularity Summarization,0.0805175,"Text summarization is a user-preference based task, i.e., for one document,
users often have different priorities for summary. As a key aspect of
customization in summarization, granularity is used to measure the semantic
coverage between the summary and source document. However, developing systems
that can generate summaries with customizable semantic coverage is still an
under-explored topic. In this paper, we propose the first unsupervised
multi-granularity summarization framework, GranuSum. We take events as the
basic semantic units of the source documents and propose to rank these events
by their salience. We also develop a model to summarize input documents with
given events as anchors and hints. By inputting different numbers of events,
GranuSum is capable of producing multi-granular summaries in an unsupervised
manner. Meanwhile, we annotate a new benchmark GranuDUC that contains multiple
summaries at different granularities for each document cluster. Experimental
results confirm the substantial superiority of GranuSum on multi-granularity
summarization over strong baselines. Further, by exploiting the event
information, GranuSum also exhibits state-of-the-art performance under the
conventional unsupervised abstractive setting. Dataset for this paper can be
found at: https://github.com/maszhongming/GranuDUC",https://github.com/yzjiao/Open-vocabulary-event-extraction,-1
AutoAttention: Automatic Field Pair Selection for Attention in User Behavior Modeling,0.698806,"In Click-through rate (CTR) prediction models, a user's interest is usually
represented as a fixed-length vector based on her history behaviors. Recently,
several methods are proposed to learn an attentive weight for each user
behavior and conduct weighted sum pooling. However, these methods only manually
select several fields from the target item side as the query to interact with
the behaviors, neglecting the other target item fields, as well as user and
context fields. Directly including all these fields in the attention may
introduce noise and deteriorate the performance. In this paper, we propose a
novel model named AutoAttention, which includes all item/user/context side
fields as the query, and assigns a learnable weight for each field pair between
behavior fields and query fields. Pruning on these field pairs via these
learnable weights lead to automatic field pair selection, so as to identify and
remove noisy field pairs. Though including more fields, the computation cost of
AutoAttention is still low due to using a simple attention function and field
pair selection. Extensive experiments on the public dataset and Tencent's
production dataset demonstrate the effectiveness of the proposed approach.",https://github.com/waydrow/AutoAttention,-1
Dialect-robust Evaluation of Generated Text,0.199774,"Evaluation metrics that are not robust to dialect variation make it
impossible to tell how well systems perform for many groups of users, and can
even penalize systems for producing text in lower-resource dialects. However,
currently, there exists no way to quantify how metrics respond to change in the
dialect of a generated utterance. We thus formalize dialect robustness and
dialect awareness as goals for NLG evaluation metrics. We introduce a suite of
methods and corresponding statistical tests one can use to assess metrics in
light of the two goals. Applying the suite to current state-of-the-art metrics,
we demonstrate that they are not dialect-robust and that semantic perturbations
frequently lead to smaller decreases in a metric than the introduction of
dialect features. As a first step to overcome this limitation, we propose a
training schema, NANO, which introduces regional and language information to
the pretraining process of a metric. We demonstrate that NANO provides a
size-efficient way for models to improve the dialect robustness while
simultaneously improving their performance on the standard metric benchmark.",https://github.com/google-research/bleurt,-1
On Linking Level Segments,0.264245,"An increasingly common area of study in procedural content generation is the
creation of level segments: short pieces that can be used to form larger
levels. Previous work has used basic concatenation to form these larger levels.
However, even if the segments themselves are completable and well-formed,
concatenation can fail to produce levels that are completable and can cause
broken in-game structures (e.g. malformed pipes in Mario). We show this with
three tile-based games: a side-scrolling platformer, a vertical platformer, and
a top-down roguelike. Additionally, we present a Markov chain and a tree search
algorithm that finds a link between two level segments, which uses filters to
ensure completability and unbroken in-game structures in the linked segments.
We further show that these links work well for multi-segment levels. We find
that this method reliably finds links between segments and is customizable to
meet a designer's needs.",https://github.com/bi3mer/LinkingLevelSegments,-1
Self-Supervised Leaf Segmentation under Complex Lighting Conditions,0.342774,"As an essential prerequisite task in image-based plant phenotyping, leaf
segmentation has garnered increasing attention in recent years. While
self-supervised learning is emerging as an effective alternative to various
computer vision tasks, its adaptation for image-based plant phenotyping remains
rather unexplored. In this work, we present a self-supervised leaf segmentation
framework consisting of a self-supervised semantic segmentation model, a
color-based leaf segmentation algorithm, and a self-supervised color correction
model. The self-supervised semantic segmentation model groups the semantically
similar pixels by iteratively referring to the self-contained information,
allowing the pixels of the same semantic object to be jointly considered by the
color-based leaf segmentation algorithm for identifying the leaf regions.
Additionally, we propose to use a self-supervised color correction model for
images taken under complex illumination conditions. Experimental results on
datasets of different plant species demonstrate the potential of the proposed
self-supervised framework in achieving effective and generalizable leaf
segmentation.",https://github.com/lxfhfut/Self-Supervised-Leaf-Segmentation,-1
MCTNet: A Multi-Scale CNN-Transformer Network for Change Detection in Optical Remote Sensing Images,0.0413399,"For the task of change detection (CD) in remote sensing images, deep
convolution neural networks (CNNs)-based methods have recently aggregated
transformer modules to improve the capability of global feature extraction.
However, they suffer degraded CD performance on small changed areas due to the
simple single-scale integration of deep CNNs and transformer modules. To
address this issue, we propose a hybrid network based on multi-scale
CNN-transformer structure, termed MCTNet, where the multi-scale global and
local information are exploited to enhance the robustness of the CD performance
on changed areas with different sizes. Especially, we design the ConvTrans
block to adaptively aggregate global features from transformer modules and
local features from CNN layers, which provides abundant global-local features
with different scales. Experimental results demonstrate that our MCTNet
achieves better detection performance than existing state-of-the-art CD
methods.",None,-1
SmoothNets: Optimizing CNN architecture design for differentially private deep learning,0.0487676,"The arguably most widely employed algorithm to train deep neural networks
with Differential Privacy is DPSGD, which requires clipping and noising of
per-sample gradients. This introduces a reduction in model utility compared to
non-private training. Empirically, it can be observed that this accuracy
degradation is strongly dependent on the model architecture. We investigated
this phenomenon and, by combining components which exhibit good individual
performance, distilled a new model architecture termed SmoothNet, which is
characterised by increased robustness to the challenges of DP-SGD training.
Experimentally, we benchmark SmoothNet against standard architectures on two
benchmark datasets and observe that our architecture outperforms others,
reaching an accuracy of 73.5\% on CIFAR-10 at $\varepsilon=7.0$ and 69.2\% at
$\varepsilon=7.0$ on ImageNette, a state-of-the-art result compared to prior
architectural modifications for DP.",https://github.com/fastai/imagenette,-1
On the focusing of thermal images,0.315678,"In this paper we present a new thermographic image database suitable for the
analysis of automatic focus measures. This database consists of 8 different
sets of scenes, where each scene contains one image for 96 different focus
positions. Using this database we evaluate the usefulness of six focus measures
with the goal to determine the optimal focus position. Experimental results
reveal that an accurate automatic detection of optimal focus position is
possible, even with a low computational burden. We also present an acquisition
tool able to help the acquisition of thermal images. To the best of our
knowledge, this is the first study about automatic focus of thermal images.",None,-1
Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography,0.274827,"Although robust PCA has been increasingly adopted to extract vessels from
X-ray coronary angiography (XCA) images, challenging problems such as
inefficient vessel-sparsity modelling, noisy and dynamic background artefacts,
and high computational cost still remain unsolved. Therefore, we propose a
novel robust PCA unrolling network with sparse feature selection for
super-resolution XCA vessel imaging. Being embedded within a patch-wise
spatiotemporal super-resolution framework that is built upon a pooling layer
and a convolutional long short-term memory network, the proposed network can
not only gradually prune complex vessel-like artefacts and noisy backgrounds in
XCA during network training but also iteratively learn and select the
high-level spatiotemporal semantic information of moving contrast agents
flowing in the XCA-imaged vessels. The experimental results show that the
proposed method significantly outperforms state-of-the-art methods, especially
in the imaging of the vessel network and its distal vessels, by restoring the
intensity and geometry profiles of heterogeneous vessels against complex and
dynamic backgrounds.",None,-1
FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation,0.292841,"Recent model-based reference-free metrics for open-domain dialogue evaluation
exhibit promising correlations with human judgment. However, they either
perform turn-level evaluation or look at a single dialogue quality dimension.
One would expect a good evaluation metric to assess multiple quality dimensions
at the dialogue level. To this end, we are motivated to propose a
multi-dimensional dialogue-level metric, which consists of three sub-metrics
with each targeting a specific dimension. The sub-metrics are trained with
novel self-supervised objectives and exhibit strong correlations with human
judgment for their respective dimensions. Moreover, we explore two approaches
to combine the sub-metrics: metric ensemble and multitask learning. Both
approaches yield a holistic metric that significantly outperforms individual
sub-metrics. Compared to the existing state-of-the-art metric, the combined
metrics achieve around 16% relative improvement on average across three
high-quality dialogue-level evaluation benchmarks.",https://github.com/e0397123/FineD-Eval,-1
Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation,0.13953,"The principal task in supervised neural machine translation (NMT) is to learn
to generate target sentences conditioned on the source inputs from a set of
parallel sentence pairs, and thus produce a model capable of generalizing to
unseen instances. However, it is commonly observed that the generalization
performance of the model is highly influenced by the amount of parallel data
used in training. Although data augmentation is widely used to enrich the
training data, conventional methods with discrete manipulations fail to
generate diverse and faithful training samples. In this paper, we present a
novel data augmentation paradigm termed Continuous Semantic Augmentation
(CsaNMT), which augments each training instance with an adjacency semantic
region that could cover adequate variants of literal expression under the same
meaning. We conduct extensive experiments on both rich-resource and
low-resource settings involving various language pairs, including WMT14
English-{German,French}, NIST Chinese-English and multiple low-resource IWSLT
translation tasks. The provided empirical evidences show that CsaNMT sets a new
level of performance among existing augmentation techniques, improving on the
state-of-the-art by a large margin. The core codes are contained in Appendix E.",https://github.com/pemywei/csanmt,-1
Computing Programs for Generalized Planning as Heuristic Search,0.0260868,"Although heuristic search is one of the most successful approaches to
classical planning, this planning paradigm does not apply straightforwardly to
Generalized Planning (GP). This paper adapts the planning as heuristic search
paradigm to the particularities of GP, and presents the first native heuristic
search approach to GP. First, the paper defines a program-based solution space
for GP that is independent of the number of planning instances in a GP problem,
and the size of these instances. Second, the paper defines the BFGP algorithm
for GP, that implements a best-first search in our program-based solution
space, and that is guided by different evaluation and heuristic functions.",None,-1
CrackSeg9k: A Collection and Benchmark for Crack Segmentation Datasets and Frameworks,0.524288,"The detection of cracks is a crucial task in monitoring structural health and
ensuring structural safety. The manual process of crack detection is
time-consuming and subjective to the inspectors. Several researchers have tried
tackling this problem using traditional Image Processing or learning-based
techniques. However, their scope of work is limited to detecting cracks on a
single type of surface (walls, pavements, glass, etc.). The metrics used to
evaluate these methods are also varied across the literature, making it
challenging to compare techniques. This paper addresses these problems by
combining previously available datasets and unifying the annotations by
tackling the inherent problems within each dataset, such as noise and
distortions. We also present a pipeline that combines Image Processing and Deep
Learning models. Finally, we benchmark the results of proposed models on these
metrics on our new dataset and compare them with state-of-the-art models in the
literature.",None,-1
AUC Maximization for Low-Resource Named Entity Recognition,0.0373597,"Current work in named entity recognition (NER) uses either cross entropy (CE)
or conditional random fields (CRF) as the objective/loss functions to optimize
the underlying NER model. Both of these traditional objective functions for the
NER problem generally produce adequate performance when the data distribution
is balanced and there are sufficient annotated training examples. But since NER
is inherently an imbalanced tagging problem, the model performance under the
low-resource settings could suffer using these standard objective functions.
Based on recent advances in area under the ROC curve (AUC) maximization, we
propose to optimize the NER model by maximizing the AUC score. We give evidence
that by simply combining two binary-classifiers that maximize the AUC score,
significant performance improvement over traditional loss functions is achieved
under low-resource NER settings. We also conduct extensive experiments to
demonstrate the advantages of our method under the low-resource and
highly-imbalanced data distribution settings. To the best of our knowledge,
this is the first work that brings AUC maximization to the NER setting.
Furthermore, we show that our method is agnostic to different types of NER
embeddings, models and domains. The code to replicate this work will be
provided upon request.",https://github.com/dngu0061/NER-AUC-2T,-1
Image Search with Text Feedback by Additive Attention Compositional Learning,0.155086,"Effective image retrieval with text feedback stands to impact a range of
real-world applications, such as e-commerce. Given a source image and text
feedback that describes the desired modifications to that image, the goal is to
retrieve the target images that resemble the source yet satisfy the given
modifications by composing a multi-modal (image-text) query. We propose a novel
solution to this problem, Additive Attention Compositional Learning (AACL),
that uses a multi-modal transformer-based architecture and effectively models
the image-text contexts. Specifically, we propose a novel image-text
composition module based on additive attention that can be seamlessly plugged
into deep neural networks. We also introduce a new challenging benchmark
derived from the Shopping100k dataset. AACL is evaluated on three large-scale
datasets (FashionIQ, Fashion200k, and Shopping100k), each with strong
baselines. Extensive experiments show that AACL achieves new state-of-the-art
results on all three datasets.",None,-1
Asking the Right Questions in Low Resource Template Extraction,0.0570454,"Information Extraction (IE) researchers are mapping tasks to Question
Answering (QA) in order to leverage existing large QA resources, and thereby
improve data efficiency. Especially in template extraction (TE), mapping an
ontology to a set of questions can be more time-efficient than collecting
labeled examples. We ask whether end users of TE systems can design these
questions, and whether it is beneficial to involve an NLP practitioner in the
process. We compare questions to other ways of phrasing natural language
prompts for TE. We propose a novel model to perform TE with prompts, and find
it benefits from questions over other styles of prompts, and that they do not
require an NLP background to author.",None,-1
MoDi: Unconditional Motion Synthesis from Diverse Data,0.246144,"The emergence of neural networks has revolutionized the field of motion
synthesis. Yet, learning to unconditionally synthesize motions from a given
distribution remains challenging, especially when the motions are highly
diverse. In this work, we present MoDi -- a generative model trained in an
unsupervised setting from an extremely diverse, unstructured and unlabeled
dataset. During inference, MoDi can synthesize high-quality, diverse motions.
Despite the lack of any structure in the dataset, our model yields a
well-behaved and highly structured latent space, which can be semantically
clustered, constituting a strong motion prior that facilitates various
applications including semantic editing and crowd simulation. In addition, we
present an encoder that inverts real motions into MoDi's natural motion
manifold, issuing solutions to various ill-posed challenges such as completion
from prefix and spatial editing. Our qualitative and quantitative experiments
achieve state-of-the-art results that outperform recent SOTA techniques. Code
and trained models are available at https://sigal-raab.github.io/MoDi.",https://sigal-raab.github.io/MoDi,-1
Policy Architectures for Compositional Generalization in Control,0.166318,"Many tasks in control, robotics, and planning can be specified using desired
goal configurations for various entities in the environment. Learning
goal-conditioned policies is a natural paradigm to solve such tasks. However,
current approaches struggle to learn and generalize as task complexity
increases, such as variations in number of environment entities or compositions
of goals. In this work, we introduce a framework for modeling entity-based
compositional structure in tasks, and create suitable policy designs that can
leverage this structure. Our policies, which utilize architectures like Deep
Sets and Self Attention, are flexible and can be trained end-to-end without
requiring any action primitives. When trained using standard reinforcement and
imitation learning methods on a suite of simulated robot manipulation tasks, we
find that these architectures achieve significantly higher success rates with
less data. We also find these architectures enable broader and compositional
generalization, producing policies that extrapolate to different numbers of
entities than seen in training, and stitch together (i.e. compose) learned
skills in novel ways. Videos of the results can be found at
https://sites.google.com/view/comp-gen-rl.",https://github.com/TianhongDai/hindsight-experience-replay,-1
LobsDICE: Offline Learning from Observation via Stationary Distribution Correction Estimation,0.0638299,"We consider the problem of learning from observation (LfO), in which the
agent aims to mimic the expert's behavior from the state-only demonstrations by
experts. We additionally assume that the agent cannot interact with the
environment but has access to the action-labeled transition data collected by
some agents with unknown qualities. This offline setting for LfO is appealing
in many real-world scenarios where the ground-truth expert actions are
inaccessible and the arbitrary environment interactions are costly or risky. In
this paper, we present LobsDICE, an offline LfO algorithm that learns to
imitate the expert policy via optimization in the space of stationary
distributions. Our algorithm solves a single convex minimization problem, which
minimizes the divergence between the two state-transition distributions induced
by the expert and the agent policy. Through an extensive set of offline LfO
tasks, we show that LobsDICE outperforms strong baseline methods.",https://github.com/secury/optidice,-1
RBC: Rectifying the Biased Context in Continual Semantic Segmentation,0.0473581,"Recent years have witnessed a great development of Convolutional Neural
Networks in semantic segmentation, where all classes of training images are
simultaneously available. In practice, new images are usually made available in
a consecutive manner, leading to a problem called Continual Semantic
Segmentation (CSS). Typically, CSS faces the forgetting problem since previous
training images are unavailable, and the semantic shift problem of the
background class. Considering the semantic segmentation as a context-dependent
pixel-level classification task, we explore CSS from a new perspective of
context analysis in this paper. We observe that the context of old-class pixels
in the new images is much more biased on new classes than that in the old
images, which can sharply aggravate the old-class forgetting and new-class
overfitting. To tackle the obstacle, we propose a biased-context-rectified CSS
framework with a context-rectified image-duplet learning scheme and a
biased-context-insensitive consistency loss. Furthermore, we propose an
adaptive re-weighting class-balanced learning strategy for the biased class
distribution. Our approach outperforms state-of-the-art methods by a large
margin in existing CSS scenarios.",https://github.com/arthurdouillard/CVPR2021,-1
Unsupervised Learning of Temporal Abstractions with Slot-based Transformers,0.041824,"The discovery of reusable sub-routines simplifies decision-making and
planning in complex reinforcement learning problems. Previous approaches
propose to learn such temporal abstractions in a purely unsupervised fashion
through observing state-action trajectories gathered from executing a policy.
However, a current limitation is that they process each trajectory in an
entirely sequential manner, which prevents them from revising earlier decisions
about sub-routine boundary points in light of new incoming information. In this
work we propose SloTTAr, a fully parallel approach that integrates sequence
processing Transformers with a Slot Attention module and adaptive computation
for learning about the number of such sub-routines in an unsupervised fashion.
We demonstrate how SloTTAr is capable of outperforming strong baselines in
terms of boundary point discovery, even for sequences containing variable
amounts of sub-routines, while being up to 7x faster to train on existing
benchmarks.",https://github.com/agopal42/slottar,-1
Measuring Cognitive Workload Using Multimodal Sensors,0.153401,"This study aims to identify a set of indicators to estimate cognitive
workload using a multimodal sensing approach and machine learning. A set of
three cognitive tests were conducted to induce cognitive workload in twelve
participants at two levels of task difficulty (Easy and Hard). Four sensors
were used to measure the participants' physiological change, including,
Electrocardiogram (ECG), electrodermal activity (EDA), respiration (RESP), and
blood oxygen saturation (SpO2). To understand the perceived cognitive workload,
NASA-TLX was used after each test and analysed using Chi-Square test. Three
well-know classifiers (LDA, SVM, and DT) were trained and tested independently
using the physiological data. The statistical analysis showed that
participants' perceived cognitive workload was significantly different
(p<0.001) between the tests, which demonstrated the validity of the
experimental conditions to induce different cognitive levels. Classification
results showed that a fusion of ECG and EDA presented good discriminating power
(acc=0.74) for cognitive workload detection. This study provides preliminary
results in the identification of a possible set of indicators of cognitive
workload. Future work needs to be carried out to validate the indicators using
more realistic scenarios and with a larger population.",None,-1
Indian Commercial Truck License Plate Detection and Recognition for Weighbridge Automation,0.08508,"Detection and recognition of a licence plate is important when automating
weighbridge services. While many large databases are available for Latin and
Chinese alphanumeric license plates, data for Indian License Plates is
inadequate. In particular, databases of Indian commercial truck license plates
are inadequate, despite the fact that commercial vehicle license plate
recognition plays a profound role in terms of logistics management and
weighbridge automation. Moreover, models to recognise license plates are not
effectively able to generalise to such data due to its challenging nature, and
due to the abundant frequency of handwritten license plates, leading to the
usage of diverse font styles. Thus, a database and effective models to
recognise and detect such license plates are crucial. This paper provides a
database on commercial truck license plates, and using state-of-the-art models
in real-time object Detection: You Only Look Once Version 7, and SceneText
Recognition: Permuted Autoregressive Sequence Models, our method outperforms
the other cited references where the maximum accuracy obtained was less than
90%, while we have achieved 95.82% accuracy in our algorithm implementation on
the presented challenging license plate dataset. Index Terms- Automatic License
Plate Recognition, character recognition, license plate detection, vision
transformer.",None,-1
SpA-Former: Transformer image shadow detection and removal via spatial attention,0.451937,"In this paper, we propose an end-to-end SpA-Former to recover a shadow-free
image from a single shaded image. Unlike traditional methods that require two
steps for shadow detection and then shadow removal, the SpA-Former unifies
these steps into one, which is a one-stage network capable of directly learning
the mapping function between shadows and no shadows, it does not require a
separate shadow detection. Thus, SpA-former is adaptable to real image
de-shadowing for shadows projected on different semantic regions. SpA-Former
consists of transformer layer and a series of joint Fourier transform residual
blocks and two-wheel joint spatial attention. The network in this paper is able
to handle the task while achieving a very fast processing efficiency.
  Our code is relased on
https://github.com/zhangbaijin/SpA-Former-shadow-removal",https://github.com/zhangbaijin/SpA-Former-shadow-removal,-1
SNeS: Learning Probably Symmetric Neural Surfaces from Incomplete Data,0.111281,"We present a method for the accurate 3D reconstruction of partly-symmetric
objects. We build on the strengths of recent advances in neural reconstruction
and rendering such as Neural Radiance Fields (NeRF). A major shortcoming of
such approaches is that they fail to reconstruct any part of the object which
is not clearly visible in the training image, which is often the case for
in-the-wild images and videos. When evidence is lacking, structural priors such
as symmetry can be used to complete the missing information. However,
exploiting such priors in neural rendering is highly non-trivial: while
geometry and non-reflective materials may be symmetric, shadows and reflections
from the ambient scene are not symmetric in general. To address this, we apply
a soft symmetry constraint to the 3D geometry and material properties, having
factored appearance into lighting, albedo colour and reflectivity. We evaluate
our method on the recently introduced CO3D dataset, focusing on the car
category due to the challenge of reconstructing highly-reflective materials. We
show that it can reconstruct unobserved regions with high fidelity and render
high-quality novel view images.",None,-1
McQueen: a Benchmark for Multimodal Conversational Query Rewrite,0.417651,"The task of query rewrite aims to convert an in-context query to its
fully-specified version where ellipsis and coreference are completed and
referred-back according to the history context. Although much progress has been
made, less efforts have been paid to real scenario conversations that involve
drawing information from more than one modalities. In this paper, we propose
the task of multimodal conversational query rewrite (McQR), which performs
query rewrite under the multimodal visual conversation setting. We collect a
large-scale dataset named McQueen based on manual annotation, which contains
15k visual conversations and over 80k queries where each one is associated with
a fully-specified rewrite version. In addition, for entities appearing in the
rewrite, we provide the corresponding image box annotation. We then use the
McQueen dataset to benchmark a state-of-the-art method for effectively tackling
the McQR task, which is based on a multimodal pre-trained model with pointer
generator. Extensive experiments are performed to demonstrate the effectiveness
of our model on this task\footnote{The dataset and code of this paper are both
available in \url{https://github.com/yfyuan01/MQR}",None,-1
A Novel Self-Knowledge Distillation Approach with Siamese Representation Learning for Action Recognition,0.0297184,"Knowledge distillation is an effective transfer of knowledge from a heavy
network (teacher) to a small network (student) to boost students' performance.
Self-knowledge distillation, the special case of knowledge distillation, has
been proposed to remove the large teacher network training process while
preserving the student's performance. This paper introduces a novel
Self-knowledge distillation approach via Siamese representation learning, which
minimizes the difference between two representation vectors of the two
different views from a given sample. Our proposed method, SKD-SRL, utilizes
both soft label distillation and the similarity of representation vectors.
Therefore, SKD-SRL can generate more consistent predictions and representations
in various views of the same data point. Our benchmark has been evaluated on
various standard datasets. The experimental results have shown that SKD-SRL
significantly improves the accuracy compared to existing supervised learning
and knowledge distillation methods regardless of the networks.",None,-1
Affective Behavior Analysis using Action Unit Relation Graph and Multi-task Cross Attention,0.0381792,"Facial behavior analysis is a broad topic with various categories such as
facial emotion recognition, age, and gender recognition. Many studies focus on
individual tasks while the multi-task learning approach is still an open
research issue and requires more research. In this paper, we present our
solution and experiment result for the Multi-Task Learning challenge of the
Affective Behavior Analysis in-the-wild competition. The challenge is a
combination of three tasks: action unit detection, facial expression
recognition, and valance-arousal estimation. To address this challenge, we
introduce a cross-attentive module to improve multi-task learning performance.
Additionally, a facial graph is applied to capture the association among action
units. As a result, we achieve the evaluation measure of 128.8 on the
validation data provided by the organizers, which outperforms the baseline
result of 30.",None,-1
Wireless Ad Hoc Federated Learning: A Fully Distributed Cooperative Machine Learning,0.508806,"Privacy-sensitive data is stored in autonomous vehicles, smart devices, or
sensor nodes that can move around with making opportunistic contact with each
other. Federation among such nodes was mainly discussed in the context of
federated learning with a centralized mechanism in many works. However, because
of multi-vendor issues, those nodes do not want to rely on a specific server
operated by a third party for this purpose. In this paper, we propose a
wireless ad hoc federated learning (WAFL) -- a fully distributed cooperative
machine learning organized by the nodes physically nearby. WAFL can develop
generalized models from Non-IID datasets stored in distributed nodes locally by
exchanging and aggregating them with each other over opportunistic node-to-node
contacts. In our benchmark-based evaluation with various opportunistic
networks, WAFL has achieved higher accuracy of 94.8-96.3% than the
self-training case of 84.7%. All our evaluation results show that WAFL can
train and converge the model parameters from highly-partitioned Non-IID
datasets over opportunistic networks without any centralized mechanisms.",None,-1
On Label Granularity and Object Localization,0.167957,"Weakly supervised object localization (WSOL) aims to learn representations
that encode object location using only image-level category labels. However,
many objects can be labeled at different levels of granularity. Is it an
animal, a bird, or a great horned owl? Which image-level labels should we use?
In this paper we study the role of label granularity in WSOL. To facilitate
this investigation we introduce iNatLoc500, a new large-scale fine-grained
benchmark dataset for WSOL. Surprisingly, we find that choosing the right
training label granularity provides a much larger performance boost than
choosing the best WSOL algorithm. We also show that changing the label
granularity can significantly improve data efficiency.",https://github.com/tensorflow/models/blob/65407126c5adc216d606d360429fe12ed3c3f187/research/object_detection/configs/tf2/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.config,-1
Building an Endangered Language Resource in the Classroom: Universal Dependencies for Kakataibo,0.0363114,"In this paper, we launch a new Universal Dependencies treebank for an
endangered language from Amazonia: Kakataibo, a Panoan language spoken in Peru.
We first discuss the collaborative methodology implemented, which proved
effective to create a treebank in the context of a Computational Linguistic
course for undergraduates. Then, we describe the general details of the
treebank and the language-specific considerations implemented for the proposed
annotation. We finally conduct some experiments on part-of-speech tagging and
syntactic dependency parsing. We focus on monolingual and transfer learning
settings, where we study the impact of a Shipibo-Konibo treebank, another
Panoan language resource.",https://github.com/yzhangcs/parser,-1
Programming molecular systems to emulate a learning spiking neuron,0.0266397,"Hebbian theory seeks to explain how the neurons in the brain adapt to
stimuli, to enable learning. An interesting feature of Hebbian learning is that
it is an unsupervised method and as such, does not require feedback, making it
suitable in contexts where systems have to learn autonomously. This paper
explores how molecular systems can be designed to show such proto-intelligent
behaviours, and proposes the first chemical reaction network (CRN) that can
exhibit autonomous Hebbian learning across arbitrarily many input channels. The
system emulates a spiking neuron, and we demonstrate that it can learn
statistical biases of incoming inputs. The basic CRN is a minimal,
thermodynamically plausible set of micro-reversible chemical equations that can
be analysed with respect to their energy requirements. However, to explore how
such chemical systems might be engineered de novo, we also propose an extended
version based on enzyme-driven compartmentalised reactions. Finally, we also
show how a purely DNA system, built upon the paradigm of DNA strand
displacement, can realise neuronal dynamics. Our analysis provides a compelling
blueprint for exploring autonomous learning in biological settings, bringing us
closer to realising real synthetic biological intelligence.",None,-1
T4DT: Tensorizing Time for Learning Temporal 3D Visual Data,0.104452,"Unlike 2D raster images, there is no single dominant representation for 3D
visual data processing. Different formats like point clouds, meshes, or
implicit functions each have their strengths and weaknesses. Still, grid
representations such as signed distance functions have attractive properties
also in 3D. In particular, they offer constant-time random access and are
eminently suitable for modern machine learning. Unfortunately, the storage size
of a grid grows exponentially with its dimension. Hence they often exceed
memory limits even at moderate resolution. This work proposes using low-rank
tensor formats, including the Tucker, tensor train, and quantics tensor train
decompositions, to compress time-varying 3D data. Our method iteratively
computes, voxelizes, and compresses each frame's truncated signed distance
function and applies tensor rank truncation to condense all frames into a
single, compressed tensor that represents the entire 4D scene. We show that
low-rank tensor compression is extremely compact to store and query
time-varying signed distance functions. It significantly reduces the memory
footprint of 4D scenes while remarkably preserving their geometric quality.
Unlike existing, iterative learning-based approaches like DeepSDF and NeRF, our
method uses a closed-form algorithm with theoretical guarantees.",https://github.com/prs-eth/T4DT,-1
Deep Feature Fusion via Graph Convolutional Network for Intracranial Artery Labeling,0.156003,"Intracranial arteries are critical blood vessels that supply the brain with
oxygenated blood. Intracranial artery labels provide valuable guidance and
navigation to numerous clinical applications and disease diagnoses. Various
machine learning algorithms have been carried out for automation in the
anatomical labeling of cerebral arteries. However, the task remains challenging
because of the high complexity and variations of intracranial arteries. This
study investigates a novel graph convolutional neural network with deep feature
fusion for cerebral artery labeling. We introduce stacked graph convolutions in
an encoder-core-decoder architecture, extracting high-level representations
from graph nodes and their neighbors. Furthermore, we efficiently aggregate
intermediate features from different hierarchies to enhance the proposed
model's representation capability and labeling performance. We perform
extensive experiments on public datasets, in which the results prove the
superiority of our approach over baselines by a clear margin.",None,-1
Domain-Agnostic Prior for Transfer Semantic Segmentation,0.201728,"Unsupervised domain adaptation (UDA) is an important topic in the computer
vision community. The key difficulty lies in defining a common property between
the source and target domains so that the source-domain features can align with
the target-domain semantics. In this paper, we present a simple and effective
mechanism that regularizes cross-domain representation learning with a
domain-agnostic prior (DAP) that constrains the features extracted from source
and target domains to align with a domain-agnostic space. In practice, this is
easily implemented as an extra loss term that requires a little extra costs. In
the standard evaluation protocol of transferring synthesized data to real data,
we validate the effectiveness of different types of DAP, especially that
borrowed from a text embedding model that shows favorable performance beyond
the state-of-the-art UDA approaches in terms of segmentation accuracy. Our
research reveals that UDA benefits much from better proxies, possibly from
other data modalities.",None,-1
Hyperbolic Relevance Matching for Neural Keyphrase Extraction,0.189627,"Keyphrase extraction is a fundamental task in natural language processing and
information retrieval that aims to extract a set of phrases with important
information from a source document. Identifying important keyphrase is the
central component of the keyphrase extraction task, and its main challenge is
how to represent information comprehensively and discriminate importance
accurately. In this paper, to address these issues, we design a new hyperbolic
matching model (HyperMatch) to represent phrases and documents in the same
hyperbolic space and explicitly estimate the phrase-document relevance via the
Poincar\'e distance as the important score of each phrase. Specifically, to
capture the hierarchical syntactic and semantic structure information,
HyperMatch takes advantage of the hidden representations in multiple layers of
RoBERTa and integrates them as the word embeddings via an adaptive mixing
layer. Meanwhile, considering the hierarchical structure hidden in the
document, HyperMatch embeds both phrases and documents in the same hyperbolic
space via a hyperbolic phrase encoder and a hyperbolic document encoder. This
strategy can further enhance the estimation of phrase-document relevance due to
the good properties of hyperbolic space. In this setting, the keyphrase
extraction can be taken as a matching problem and effectively implemented by
minimizing a hyperbolic margin-based triplet loss. Extensive experiments are
conducted on six benchmarks and demonstrate that HyperMatch outperforms the
state-of-the-art baselines.",https://github.com/MySong7NLPer/HyperMatch,-1
A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment Analysis,0.0389585,"Recently, some span-based methods have achieved encouraging performances for
joint aspect-sentiment analysis, which first extract aspects (aspect
extraction) by detecting aspect boundaries and then classify the span-level
sentiments (sentiment classification). However, most existing approaches either
sequentially extract task-specific features, leading to insufficient feature
interactions, or they encode aspect features and sentiment features in a
parallel manner, implying that feature representation in each task is largely
independent of each other except for input sharing. Both of them ignore the
internal correlations between the aspect extraction and sentiment
classification. To solve this problem, we novelly propose a hierarchical
interactive network (HI-ASA) to model two-way interactions between two tasks
appropriately, where the hierarchical interactions involve two steps:
shallow-level interaction and deep-level interaction. First, we utilize
cross-stitch mechanism to combine the different task-specific features
selectively as the input to ensure proper two-way interactions. Second, the
mutual information technique is applied to mutually constrain learning between
two tasks in the output layer, thus the aspect input and the sentiment input
are capable of encoding features of the other task via backpropagation.
Extensive experiments on three real-world datasets demonstrate HI-ASA's
superiority over baselines.",https://github.com/cwei01/HI-ASA,-1
FEAT: Face Editing with Attention,0.796232,"Employing the latent space of pretrained generators has recently been shown
to be an effective means for GAN-based face manipulation. The success of this
approach heavily relies on the innate disentanglement of the latent space axes
of the generator. However, face manipulation often intends to affect local
regions only, while common generators do not tend to have the necessary spatial
disentanglement. In this paper, we build on the StyleGAN generator, and present
a method that explicitly encourages face manipulation to focus on the intended
regions by incorporating learned attention maps. During the generation of the
edited image, the attention map serves as a mask that guides a blending between
the original features and the modified ones. The guidance for the latent space
edits is achieved by employing CLIP, which has recently been shown to be
effective for text-driven edits. We perform extensive experiments and show that
our method can perform disentangled and controllable face manipulations based
on text descriptions by attending to the relevant regions only. Both
qualitative and quantitative experimental results demonstrate the superiority
of our method for facial region editing over alternative methods.",None,-1
A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing,0.20331,"A key algorithm for understanding the world is material segmentation, which
assigns a label (metal, glass, etc.) to each pixel. We find that a model
trained on existing data underperforms in some settings and propose to address
this with a large-scale dataset of 3.2 million dense segments on 44,560 indoor
and outdoor images, which is 23x more segments than existing data. Our data
covers a more diverse set of scenes, objects, viewpoints and materials, and
contains a more fair distribution of skin types. We show that a model trained
on our data outperforms a state-of-the-art model across datasets and
viewpoints. We propose a large-scale scene parsing benchmark and baseline of
0.729 per-pixel accuracy, 0.585 mean class accuracy and 0.420 mean IoU across
46 materials.",https://github.com/apple/ml-dms-dataset,-1
Graph Neural Networks: a bibliometrics overview,0.0291359,"Recently, graph neural networks have become a hot topic in machine learning
community. This paper presents a Scopus based bibliometric overview of the GNNs
research since 2004, when GNN papers were first published. The study aims to
evaluate GNN research trend, both quantitatively and qualitatively. We provide
the trend of research, distribution of subjects, active and influential authors
and institutions, sources of publications, most cited documents, and hot
topics. Our investigations reveal that the most frequent subject categories in
this field are computer science, engineering, telecommunications, linguistics,
operations research and management science, information science and library
science, business and economics, automation and control systems, robotics, and
social sciences. In addition, the most active source of GNN publications is
Lecture Notes in Computer Science. The most prolific or impactful institutions
are found in the United States, China, and Canada. We also provide must read
papers and future directions. Finally, the application of graph convolutional
networks and attention mechanism are now among hot topics of GNN research.",None,-1
Independent Components of Word Embeddings Represent Semantic Features,0.00707443,"Independent Component Analysis (ICA) is an algorithm originally developed for
finding separate sources in a mixed signal, such as a recording of multiple
people in the same room speaking at the same time. It has also been used to
find linguistic features in distributional representations. In this paper, we
used ICA to analyze words embeddings. We have found that ICA can be used to
find semantic features of the words and these features can easily be combined
to search for words that satisfy the combination. We show that only some of the
independent components represent such features, but those that do are stable
with regard to random initialization of the algorithm.",None,-1
TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval,0.0869496,"Entity retrieval--retrieving information about entity mentions in a query--is
a key step in open-domain tasks, such as question answering or fact checking.
However, state-of-the-art entity retrievers struggle to retrieve rare entities
for ambiguous mentions due to biases towards popular entities. Incorporating
knowledge graph types during training could help overcome popularity biases,
but there are several challenges: (1) existing type-based retrieval methods
require mention boundaries as input, but open-domain tasks run on unstructured
text, (2) type-based methods should not compromise overall performance, and (3)
type-based methods should be robust to noisy and missing types. In this work,
we introduce TABi, a method to jointly train bi-encoders on knowledge graph
types and unstructured text for entity retrieval for open-domain tasks. TABi
leverages a type-enforced contrastive loss to encourage entities and queries of
similar types to be close in the embedding space. TABi improves retrieval of
rare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining
strong overall retrieval performance on open-domain tasks in the KILT benchmark
compared to state-of-the-art retrievers. TABi is also robust to incomplete type
systems, improving rare entity retrieval over baselines with only 5% type
coverage of the training dataset. We make our code publicly available at
https://github.com/HazyResearch/tabi.",None,-1
SwinUNet3D -- A Hierarchical Architecture for Deep Traffic Prediction using Shifted Window Transformers,0.0734312,"Traffic forecasting is an important element of mobility management, an
important key that drives the logistics industry. Over the years, lots of work
have been done in Traffic forecasting using time series as well as
spatiotemporal dynamic forecasting. In this paper, we explore the use of vision
transformer in a UNet setting. We completely remove all convolution-based
building blocks in UNet, while using 3D shifted window transformer in both
encoder and decoder branches. In addition, we experiment with the use of
feature mixing just before patch encoding to control the inter-relationship of
the feature while avoiding contraction of the depth dimension of our
spatiotemporal input. The proposed network is tested on the data provided by
Traffic Map Movie Forecasting Challenge 2021(Traffic4cast2021), held in the
competition track of Neural Information Processing Systems (NeurIPS).
Traffic4cast2021 task is to predict an hour (6 frames) of traffic conditions
(volume and average speed)from one hour of given traffic state (12 frames
averaged in 5 minutes time span). Source code is available online at
https://github.com/bojesomo/Traffic4Cast2021-SwinUNet3D.",https://github.com/bojesomo/Trafﬁc4Cast2021-SwinUNet3D,-1
Why Deep Surgical Models Fail?: Revisiting Surgical Action Triplet Recognition through the Lens of Robustness,0.0937585,"Surgical action triplet recognition provides a better understanding of the
surgical scene. This task is of high relevance as it provides the surgeon with
context-aware support and safety. The current go-to strategy for improving
performance is the development of new network mechanisms. However, the
performance of current state-of-the-art techniques is substantially lower than
other surgical tasks. Why is this happening? This is the question that we
address in this work. We present the first study to understand the failure of
existing deep learning models through the lens of robustness and
explainability. Firstly, we study current existing models under weak and strong
$\delta-$perturbations via an adversarial optimisation scheme. We then analyse
the failure modes via feature based explanations. Our study reveals that the
key to improving performance and increasing reliability is in the core and
spurious attributes. Our work opens the door to more trustworthy and reliable
deep learning models in surgical data science.",None,-1
Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding,0.162849,"Unsupervised pre-training on millions of digital-born or scanned documents
has shown promising advances in visual document understanding~(VDU). While
various vision-language pre-training objectives are studied in existing
solutions, the document textline, as an intrinsic granularity in VDU, has
seldom been explored so far. A document textline usually contains words that
are spatially and semantically correlated, which can be easily obtained from
OCR engines. In this paper, we propose Wukong-Reader, trained with new
pre-training objectives to leverage the structural knowledge nested in document
textlines. We introduce textline-region contrastive learning to achieve
fine-grained alignment between the visual regions and texts of document
textlines. Furthermore, masked region modeling and textline-grid matching are
also designed to enhance the visual and layout representations of textlines.
Experiments show that our Wukong-Reader has superior performance on various VDU
tasks such as information extraction. The fine-grained alignment over textlines
also empowers Wukong-Reader with promising localization ability.",None,-1
Self-distillation Augmented Masked Autoencoders for Histopathological Image Classification,0.348833,"Self-supervised learning (SSL) has drawn increasing attention in
histopathological image analysis in recent years. Compared to contrastive
learning which is troubled with the false negative problem, i.e., semantically
similar images are selected as negative samples, masked autoencoders (MAE)
building SSL from a generative paradigm is probably a more appropriate
pre-training. In this paper, we introduce MAE and verify the effect of visible
patches for histopathological image understanding. Moreover, a novel SD-MAE
model is proposed to enable a self-distillation augmented MAE. Besides the
reconstruction loss on masked image patches, SD-MAE further imposes the
self-distillation loss on visible patches to enhance the representational
capacity of the encoder located shallow layer. We apply SD-MAE to
histopathological image classification, cell segmentation and object detection.
Experiments demonstrate that SD-MAE shows highly competitive performance when
compared with other SSL methods in these tasks.",None,-1
Knowledge Equivalence in Digital Twins of Intelligent Systems,0.0283319,"A digital twin contains up-to-date data-driven models of the physical world
being studied and can use simulation to optimise the physical world. However,
the analysis made by the digital twin is valid and reliable only when the model
is equivalent to the physical world. Maintaining such an equivalent model is
challenging, especially when the physical systems being modelled are
intelligent and autonomous. The paper focuses in particular on digital twin
models of intelligent systems where the systems are knowledge-aware but with
limited capability. The digital twin improves the acting of the physical system
at a meta-level by accumulating more knowledge in the simulated environment.
The modelling of such an intelligent physical system requires replicating the
knowledge-awareness capability in the virtual space. Novel equivalence
maintaining techniques are needed, especially in synchronising the knowledge
between the model and the physical system. This paper proposes the notion of
knowledge equivalence and an equivalence maintaining approach by knowledge
comparison and updates. A quantitative analysis of the proposed approach
confirms that compared to state equivalence, knowledge equivalence maintenance
can tolerate deviation thus reducing unnecessary updates and achieve more
Pareto efficient solutions for the trade-off between update overhead and
simulation reliability.",None,-1
DProQ: A Gated-Graph Transformer for Protein Complex Structure Assessment,0.0509451,"Proteins interact to form complexes to carry out essential biological
functions. Computational methods have been developed to predict the structures
of protein complexes. However, an important challenge in protein complex
structure prediction is to estimate the quality of predicted protein complex
structures without any knowledge of the corresponding native structures. Such
estimations can then be used to select high-quality predicted complex
structures to facilitate biomedical research such as protein function analysis
and drug discovery. We challenge this significant task with DProQ, which
introduces a gated neighborhood-modulating Graph Transformer (GGT) designed to
predict the quality of 3D protein complex structures. Notably, we incorporate
node and edge gates within a novel Graph Transformer framework to control
information flow during graph message passing. We train and evaluate DProQ on
four newly-developed datasets that we make publicly available in this work. Our
rigorous experiments demonstrate that DProQ achieves state-of-the-art
performance in ranking protein complex structures.",https://github.com/BioinfoMachineLearning/DProQ,-1
TSM: Measuring the Enticement of Honeyfiles with Natural Language Processing,0.0919002,"Honeyfile deployment is a useful breach detection method in cyber deception
that can also inform defenders about the intent and interests of intruders and
malicious insiders. A key property of a honeyfile, enticement, is the extent to
which the file can attract an intruder to interact with it. We introduce a
novel metric, Topic Semantic Matching (TSM), which uses topic modelling to
represent files in the repository and semantic matching in an embedding vector
space to compare honeyfile text and topic words robustly. We also present a
honeyfile corpus created with different Natural Language Processing (NLP)
methods. Experiments show that TSM is effective in inter-corpus comparisons and
is a promising tool to measure the enticement of honeyfiles. TSM is the first
measure to use NLP techniques to quantify the enticement of honeyfile content
that compares the essential topical content of local contexts to honeyfiles and
is robust to paraphrasing.",https://github.com/RoelTim/tsm-honeyfile-nlp-enticement,-1
MENLI: Robust Evaluation Metrics from Natural Language Inference,0.0362155,"Recently proposed BERT-based evaluation metrics for text generation perform
well on standard benchmarks but are vulnerable to adversarial attacks, e.g.,
relating to information correctness. We argue that this stems (in part) from
the fact that they are models of semantic similarity. In contrast, we develop
evaluation metrics based on Natural Language Inference (NLI), which we deem a
more appropriate modeling. We design a preference-based adversarial attack
framework and show that our NLI based metrics are much more robust to the
attacks than the recent BERT-based metrics. On standard benchmarks, our NLI
based metrics outperform existing summarization metrics, but perform below SOTA
MT metrics. However, when combining existing metrics with our NLI metrics, we
obtain both higher adversarial robustness (15%-30%) and higher quality metrics
as measured on standard benchmarks (+5% to 30%).",http://github.com/cyr19/MENLI,-1
CTM -- A Model for Large-Scale Multi-View Tweet Topic Classification,0.196195,"Automatically associating social media posts with topics is an important
prerequisite for effective search and recommendation on many social media
platforms. However, topic classification of such posts is quite challenging
because of (a) a large topic space (b) short text with weak topical cues, and
(c) multiple topic associations per post. In contrast to most prior work which
only focuses on post classification into a small number of topics ($10$-$20$),
we consider the task of large-scale topic classification in the context of
Twitter where the topic space is $10$ times larger with potentially multiple
topic associations per Tweet. We address the challenges above by proposing a
novel neural model, CTM that (a) supports a large topic space of $300$ topics
and (b) takes a holistic approach to tweet content modeling -- leveraging
multi-modal content, author context, and deeper semantic cues in the Tweet. Our
method offers an effective way to classify Tweets into topics at scale by
yielding superior performance to other approaches (a relative lift of
$\mathbf{20}\%$ in median average precision score) and has been successfully
deployed in production at Twitter.",None,-1
GeoUDF: Surface Reconstruction from 3D Point Clouds via Geometry-guided Distance Representation,0.0276087,"We present a learning-based method, namely GeoUDF,to tackle the long-standing
and challenging problem of reconstructing a discrete surface from a sparse
point cloud.To be specific, we propose a geometry-guided learning method for
UDF and its gradient estimation that explicitly formulates the unsigned
distance of a query point as the learnable affine averaging of its distances to
the tangent planes of neighboring points on the surface. Besides,we model the
local geometric structure of the input point clouds by explicitly learning a
quadratic polynomial for each point. This not only facilitates upsampling the
input sparse point cloud but also naturally induces unoriented normal, which
further augments UDF estimation. Finally, to extract triangle meshes from the
predicted UDF we propose a customized edge-based marching cube module. We
conduct extensive experiments and ablation studies to demonstrate the
significant advantages of our method over state-of-the-art methods in terms of
reconstruction accuracy, efficiency, and generality. The source code is
publicly available at https://github.com/rsy6318/GeoUDF.",https://github.com/rsy6318/GeoUDF,-1
"You can't pick your neighbors, or can you? When and how to rely on retrieval in the $k$NN-LM",0.207184,"Retrieval-enhanced language models (LMs), which condition their predictions
on text retrieved from large external datastores, have recently shown
significant perplexity improvements compared to standard LMs. One such
approach, the $k$NN-LM, interpolates any existing LM's predictions with the
output of a $k$-nearest neighbors model and requires no additional training. In
this paper, we explore the importance of lexical and semantic matching in the
context of items retrieved by $k$NN-LM. We find two trends: (1) the presence of
large overlapping $n$-grams between the datastore and evaluation set plays an
important factor in strong performance, even when the datastore is derived from
the training data; and (2) the $k$NN-LM is most beneficial when retrieved items
have high semantic similarity with the query. Based on our analysis, we define
a new formulation of the $k$NN-LM that uses retrieval quality to assign the
interpolation coefficient. We empirically measure the effectiveness of our
approach on two English language modeling datasets, Wikitext-103 and PG-19. Our
re-formulation of the $k$NN-LM is beneficial in both cases, and leads to nearly
4% improvement in perplexity on the Wikitext-103 test set.",https://github.com/iesl/knnlm-retrieval-quality,-1
Visual Concepts Tokenization,0.0390648,"Obtaining the human-like perception ability of abstracting visual concepts
from concrete pixels has always been a fundamental and important target in
machine learning research fields such as disentangled representation learning
and scene decomposition. Towards this goal, we propose an unsupervised
transformer-based Visual Concepts Tokenization framework, dubbed VCT, to
perceive an image into a set of disentangled visual concept tokens, with each
concept token responding to one type of independent visual concept.
Particularly, to obtain these concept tokens, we only use cross-attention to
extract visual information from the image tokens layer by layer without
self-attention between concept tokens, preventing information leakage across
concept tokens. We further propose a Concept Disentangling Loss to facilitate
that different concept tokens represent independent visual concepts. The
cross-attention and disentangling loss play the role of induction and mutual
exclusion for the concept tokens, respectively. Extensive experiments on
several popular datasets verify the effectiveness of VCT on the tasks of
disentangled representation learning and scene decomposition. VCT achieves the
state of the art results by a large margin.",https://github.com/thomasmry/VCT,-1
DeepGen: Diverse Search Ad Generation and Real-Time Customization,0.950213,"We present DeepGen, a system deployed at web scale for automatically creating
sponsored search advertisements (ads) for BingAds customers. We leverage
state-of-the-art natural language generation (NLG) models to generate fluent
ads from advertiser's web pages in an abstractive fashion and solve practical
issues such as factuality and inference speed. In addition, our system creates
a customized ad in real-time in response to the user's search query, therefore
highlighting different aspects of the same product based on what the user is
looking for. To achieve this, our system generates a diverse choice of smaller
pieces of the ad ahead of time and, at query time, selects the most relevant
ones to be stitched into a complete ad. We improve generation diversity by
training a controllable NLG model to generate multiple ads for the same web
page highlighting different selling points. Our system design further improves
diversity horizontally by first running an ensemble of generation models
trained with different objectives and then using a diversity sampling algorithm
to pick a diverse subset of generation results for online selection.
Experimental results show the effectiveness of our proposed system design. Our
system is currently deployed in production, serving ${\sim}4\%$ of global ads
served in Bing.",None,-1
In-BoXBART: Get Instructions into Biomedical Multi-Task Learning,0.121071,"Single-task models have proven pivotal in solving specific tasks; however,
they have limitations in real-world applications where multi-tasking is
necessary and domain shifts are exhibited. Recently, instructional prompts have
shown significant improvement towards multi-task generalization; however, the
effect of instructional prompts and Multi-Task Learning (MTL) has not been
systematically studied in the biomedical domain. Motivated by this, this paper
explores the impact of instructional prompts for biomedical MTL. We introduce
the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X)
various categories. Using this meta-dataset, we propose a unified model termed
In-BoXBART, that can jointly learn all tasks of the BoX without any
task-specific modules. To the best of our knowledge, this is the first attempt
to propose a unified model in the biomedical domain and use instructions to
achieve generalization across several biomedical tasks. Experimental results
indicate that the proposed model: 1) outperforms the single-task baseline by
~3% and multi-task (without instruction) baseline by ~18% on an average, and 2)
shows ~23% improvement compared to the single-task baseline in few-shot
learning (i.e., 32 instances per task) on an average. Our analysis indicates
that there is significant room for improvement across tasks in the BoX,
implying the scope for future research direction.",https://github.com/Mihir3009/In-BoXBART,-1
Graph Coloring with Physics-Inspired Graph Neural Networks,0.333077,"We show how graph neural networks can be used to solve the canonical graph
coloring problem. We frame graph coloring as a multi-class node classification
problem and utilize an unsupervised training strategy based on the statistical
physics Potts model. Generalizations to other multi-class problems such as
community detection, data clustering, and the minimum clique cover problem are
straightforward. We provide numerical benchmark results and illustrate our
approach with an end-to-end application for a real-world scheduling use case
within a comprehensive encode-process-decode framework. Our optimization
approach performs on par or outperforms existing solvers, with the ability to
scale to problems with millions of variables.",https://github.com/amazon-research/gcp-with-gnns-example,-1
Impact of Adversarial Training on Robustness and Generalizability of Language Models,0.0128457,"Adversarial training is widely acknowledged as the most effective defense
against adversarial attacks. However, it is also well established that
achieving both robustness and generalization in adversarially trained models
involves a trade-off. The goal of this work is to provide an in depth
comparison of different approaches for adversarial training in language models.
Specifically, we study the effect of pre-training data augmentation as well as
training time input perturbations vs. embedding space perturbations on the
robustness and generalization of transformer-based language models. Our
findings suggest that better robustness can be achieved by pre-training data
augmentation or by training with input space perturbation. However, training
with embedding space perturbation significantly improves generalization. A
linguistic correlation analysis of neurons of the learned models reveals that
the improved generalization is due to 'more specialized' neurons. To the best
of our knowledge, this is the first work to carry out a deep qualitative
analysis of different methods of generating adversarial examples in adversarial
training of language models.",None,-1
Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation,0.0558002,"Large Language Models (LLMs) have in recent years demonstrated impressive
prowess in natural language generation. A common practice to improve generation
diversity is to sample multiple outputs from the model. However, there lacks a
simple and robust way of selecting the best output from these stochastic
samples. As a case study framed in the context of question generation, we
propose two prompt-based approaches to selecting high-quality questions from a
set of LLM-generated candidates. Our method works under the constraints of 1) a
black-box (non-modifiable) question generation model and 2) lack of access to
human-annotated references -- both of which are realistic limitations for
real-world deployment of LLMs. With automatic as well as human evaluations, we
empirically demonstrate that our approach can effectively select questions of
higher qualities than greedy generation.",https://github.com/kingoflolz/mesh-transformer-jax,-1
Maze Learning using a Hyperdimensional Predictive Processing Cognitive Architecture,0.118469,"We present the COGnitive Neural GENerative system (CogNGen), a cognitive
architecture that combines two neurobiologically-plausible, computational
models: predictive processing and hyperdimensional/vector-symbolic models. We
draw inspiration from architectures such as ACT-R and Spaun/Nengo. CogNGen is
in broad agreement with these, providing a level of detail between ACT-R's
high-level symbolic description of human cognition and Spaun's low-level
neurobiological description, furthermore creating the groundwork for designing
agents that learn continually from diverse tasks and model human performance at
larger scales than what is possible with current systems. We test CogNGen on
four maze-learning tasks, including those that test memory and planning, and
find that CogNGen matches performance of deep reinforcement learning models and
exceeds on a task designed to test memory.",None,-1
Adapting the Exploration Rate for Value-of-Information-Based Reinforcement Learning,0.0322313,"In this paper, we consider the problem of adjusting the exploration rate when
using value-of-information-based exploration. We do this by converting the
value-of-information optimization into a problem of finding equilibria of a
flow for a changing exploration rate. We then develop an efficient
path-following scheme for converging to these equilibria and hence uncovering
optimal action-selection policies. Under this scheme, the exploration rate is
automatically adapted according to the agent's experiences. Global convergence
is theoretically assured.
  We first evaluate our exploration-rate adaptation on the Nintendo GameBoy
games Centipede and Millipede. We demonstrate aspects of the search process,
like that it yields a hierarchy of state abstractions. We also show that our
approach returns better policies in fewer episodes than conventional search
strategies relying on heuristic, annealing-based exploration-rate adjustments.
We then illustrate that these trends hold for deep, value-of-information-based
agents that learn to play ten simple games and over forty more complicated
games for the Nintendo GameBoy system. Performance either near or well above
the level of human play is observed.",None,-1
What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders,0.196193,"The last years have witnessed the emergence of a promising self-supervised
learning strategy, referred to as masked autoencoding. However, there is a lack
of theoretical understanding of how masking matters on graph autoencoders
(GAEs). In this work, we present masked graph autoencoder (MaskGAE), a
self-supervised learning framework for graph-structured data. Different from
standard GAEs, MaskGAE adopts masked graph modeling (MGM) as a principled
pretext task - masking a portion of edges and attempting to reconstruct the
missing part with partially visible, unmasked graph structure. To understand
whether MGM can help GAEs learn better representations, we provide both
theoretical and empirical evidence to comprehensively justify the benefits of
this pretext task. Theoretically, we establish close connections between GAEs
and contrastive learning, showing that MGM significantly improves the
self-supervised learning scheme of GAEs. Empirically, we conduct extensive
experiments on a variety of graph benchmarks, demonstrating the superiority of
MaskGAE over several state-of-the-arts on both link prediction and node
classification tasks.",https://github.com/EdisonLeeeee/MaskGAE,-1
Coalescing Global and Local Information for Procedural Text Understanding,0.0515823,"Procedural text understanding is a challenging language reasoning task that
requires models to track entity states across the development of a narrative. A
complete procedural understanding solution should combine three core aspects:
local and global views of the inputs, and global view of outputs. Prior methods
considered a subset of these aspects, resulting in either low precision or low
recall. In this paper, we propose Coalescing Global and Local Information
(CGLI), a new model that builds entity- and timestep-aware input
representations (local input) considering the whole context (global input), and
we jointly model the entity states with a structured prediction objective
(global output). Thus, CGLI simultaneously optimizes for both precision and
recall. We extend CGLI with additional output layers and integrate it into a
story reasoning framework. Extensive experiments on a popular procedural text
understanding dataset show that our model achieves state-of-the-art results;
experiments on a story reasoning benchmark show the positive impact of our
model on downstream reasoning.",https://github.com/Mayer123/CGLI,-1
Unseen Object Instance Segmentation with Fully Test-time RGB-D Embeddings Adaptation,0.288035,"Segmenting unseen objects is a crucial ability for the robot since it may
encounter new environments during the operation. Recently, a popular solution
is leveraging RGB-D features of large-scale synthetic data and directly
applying the model to unseen real-world scenarios. However, the domain shift
caused by the sim2real gap is inevitable, posing a crucial challenge to the
segmentation model. In this paper, we emphasize the adaptation process across
sim2real domains and model it as a learning problem on the BatchNorm parameters
of a simulation-trained model. Specifically, we propose a novel non-parametric
entropy objective, which formulates the learning objective for the test-time
adaptation in an open-world manner. Then, a cross-modality knowledge
distillation objective is further designed to encourage the test-time knowledge
transfer for feature enhancement. Our approach can be efficiently implemented
with only test images, without requiring annotations or revisiting the
large-scale synthetic training data. Besides significant time savings, the
proposed method consistently improves segmentation results on the overlap and
boundary metrics, achieving state-of-the-art performance on unseen object
instance segmentation.",None,-1
The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding,0.146645,"In a real-world dialogue system, generated text must be truthful and
informative while remaining fluent and adhering to a prescribed style.
Satisfying these constraints simultaneously is difficult for the two
predominant paradigms in language generation: neural language modeling and
rule-based generation. We describe a hybrid architecture for dialogue response
generation that combines the strengths of both paradigms. The first component
of this architecture is a rule-based content selection model defined using a
new formal framework called dataflow transduction, which uses declarative rules
to transduce a dialogue agent's actions and their results (represented as
dataflow graphs) into context-free grammars representing the space of
contextually acceptable responses. The second component is a constrained
decoding procedure that uses these grammars to constrain the output of a neural
language model, which selects fluent utterances. Our experiments show that this
system outperforms both rule-based and learned approaches in human evaluations
of fluency, relevance, and truthfulness.",https://github.com/microsoft/dataflow2text,-1
On the Generalization of BasicVSR++ to Video Deblurring and Denoising,0.284399,"The exploitation of long-term information has been a long-standing problem in
video restoration. The recent BasicVSR and BasicVSR++ have shown remarkable
performance in video super-resolution through long-term propagation and
effective alignment. Their success has led to a question of whether they can be
transferred to different video restoration tasks. In this work, we extend
BasicVSR++ to a generic framework for video restoration tasks. In tasks where
inputs and outputs possess identical spatial size, the input resolution is
reduced by strided convolutions to maintain efficiency. With only minimal
changes from BasicVSR++, the proposed framework achieves compelling performance
with great efficiency in various video restoration tasks including video
deblurring and denoising. Notably, BasicVSR++ achieves comparable performance
to Transformer-based approaches with up to 79% of parameter reduction and 44x
speedup. The promising results demonstrate the importance of propagation and
alignment in video restoration tasks beyond just video super-resolution. Code
and models are available at https://github.com/ckkelvinchan/BasicVSR_PlusPlus.",https://github.com/ckkelvinchan/BasicVSR_PlusPlus,-1
CMNEROne at SemEval-2022 Task 11: Code-Mixed Named Entity Recognition by leveraging multilingual data,0.136365,"Identifying named entities is, in general, a practical and challenging task
in the field of Natural Language Processing. Named Entity Recognition on the
code-mixed text is further challenging due to the linguistic complexity
resulting from the nature of the mixing. This paper addresses the submission of
team CMNEROne to the SEMEVAL 2022 shared task 11 MultiCoNER. The Code-mixed NER
task aimed to identify named entities on the code-mixed dataset. Our work
consists of Named Entity Recognition (NER) on the code-mixed dataset by
leveraging the multilingual data. We achieved a weighted average F1 score of
0.7044, i.e., 6% greater than the baseline.",https://github.com/scrapinghub/python-crfsuite,-1
How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image,0.018055,"This study investigates how fake news uses a thumbnail for a news article
with a focus on whether a news article's thumbnail represents the news content
correctly. A news article shared with an irrelevant thumbnail can mislead
readers into having a wrong impression of the issue, especially in social media
environments where users are less likely to click the link and consume the
entire content. We propose to capture the degree of semantic incongruity in the
multimodal relation by using the pretrained CLIP representation. From a
source-level analysis, we found that fake news employs a more incongruous image
to the main content than general news. Going further, we attempted to detect
news articles with image-text incongruity. Evaluation experiments suggest that
CLIP-based methods can successfully detect news articles in which the thumbnail
is semantically irrelevant to news text. This study contributes to the research
by providing a novel view on tackling online fake news and misinformation. Code
and datasets are available at
https://github.com/ssu-humane/fake-news-thumbnail.",https://github.com/su-humane/fake-news-thumbnail,-1
Generating Natural Language Proofs with Verifier-Guided Search,0.378094,"Reasoning over natural language is a challenging problem in NLP. In this
work, we focus on proof generation: Given a hypothesis and a set of supporting
facts, the model generates a proof tree indicating how to derive the hypothesis
from supporting facts. Compared to generating the entire proof in one shot,
stepwise generation can better exploit the compositionality and generalize to
longer proofs but has achieved limited success on real-world data. Existing
stepwise methods struggle to generate proof steps that are both logically valid
and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps
given the hypothesis. In this paper, we present a novel stepwise method,
NLProofS (Natural Language Proof Search), which learns to generate relevant
steps conditioning on the hypothesis. At the core of our approach, we train an
independent verifier to check the validity of the proof steps to prevent
hallucination. Instead of generating steps greedily, we search for proofs
maximizing a global proof score judged by the verifier. NLProofS achieves
state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it
improves the correctness of predicted proofs from 27.7% to 33.3% in the
distractor setting of EntailmentBank, demonstrating the effectiveness of
NLProofS in generating challenging human-authored proofs.",None,-1
A study on cross-corpus speech emotion recognition and data augmentation,0.126733,"Models that can handle a wide range of speakers and acoustic conditions are
essential in speech emotion recognition (SER). Often, these models tend to show
mixed results when presented with speakers or acoustic conditions that were not
visible during training. This paper investigates the impact of cross-corpus
data complementation and data augmentation on the performance of SER models in
matched (test-set from same corpus) and mismatched (test-set from different
corpus) conditions. Investigations using six emotional speech corpora that
include single and multiple speakers as well as variations in emotion style
(acted, elicited, natural) and recording conditions are presented. Observations
show that, as expected, models trained on single corpora perform best in
matched conditions while performance decreases between 10-40% in mismatched
conditions, depending on corpus specific features. Models trained on mixed
corpora can be more stable in mismatched contexts, and the performance
reductions range from 1 to 8% when compared with single corpus models in
matched conditions. Data augmentation yields additional gains up to 4% and seem
to benefit mismatched conditions more than matched ones.",https://github.com/ludwig-ai/ludwig,-1
Consistent Style Transfer,0.0935844,"Recently, attentional arbitrary style transfer methods have been proposed to
achieve fine-grained results, which manipulates the point-wise similarity
between content and style features for stylization. However, the attention
mechanism based on feature points ignores the feature multi-manifold
distribution, where each feature manifold corresponds to a semantic region in
the image. Consequently, a uniform content semantic region is rendered by
highly different patterns from various style semantic regions, producing
inconsistent stylization results with visual artifacts. We proposed the
progressive attentional manifold alignment (PAMA) to alleviate this problem,
which repeatedly applies attention operations and space-aware interpolations.
The attention operation rearranges style features dynamically according to the
spatial distribution of content features. This makes the content and style
manifolds correspond on the feature map. Then the space-aware interpolation
adaptively interpolates between the corresponding content and style manifolds
to increase their similarity. By gradually aligning the content manifolds to
style manifolds, the proposed PAMA achieves state-of-the-art performance while
avoiding the inconsistency of semantic regions. Codes are available at
https://github.com/computer-vision2022/PAMA.",https://github.com/computer-vision2022/PAMA,-1
Pre-training Language Models with Deterministic Factual Knowledge,0.0293546,"Previous works show that Pre-trained Language Models (PLMs) can capture
factual knowledge. However, some analyses reveal that PLMs fail to perform it
robustly, e.g., being sensitive to the changes of prompts when extracting
factual knowledge. To mitigate this issue, we propose to let PLMs learn the
deterministic relationship between the remaining context and the masked
content. The deterministic relationship ensures that the masked factual content
can be deterministically inferable based on the existing clues in the context.
That would provide more stable patterns for PLMs to capture factual knowledge
than randomly masking. Two pre-training tasks are further introduced to
motivate PLMs to rely on the deterministic relationship when filling masks.
Specifically, we use an external Knowledge Base (KB) to identify deterministic
relationships and continuously pre-train PLMs with the proposed methods. The
factual knowledge probing experiments indicate that the continuously
pre-trained PLMs achieve better robustness in factual knowledge capturing.
Further experiments on question-answering datasets show that trying to learn a
deterministic relationship with the proposed methods can also help other
knowledge-intensive tasks.",https://github.com/informagi/REL,-1
A description of Turkish Discourse Bank 1.2 and an examination of common dependencies in Turkish discourse,0.0273446,"We describe Turkish Discourse Bank 1.2, the latest version of a discourse
corpus annotated for explicitly or implicitly conveyed discourse relations,
their constitutive units, and senses in the Penn Discourse Treebank style. We
present an evaluation of the recently added tokens and examine three commonly
occurring dependency patterns that hold among the constitutive units of a pair
of adjacent discourse relations, namely, shared arguments, full embedding and
partial containment of a discourse relation. We present three major findings:
(a) implicitly conveyed relations occur more often than explicitly conveyed
relations in the data; (b) it is much more common for two adjacent implicit
discourse relations to share an argument than for two adjacent explicit
relations to do so; (c) both full embedding and partial containment of
discourse relations are pervasive in the corpus, which can be partly due to
subordinator connectives whose preposed subordinate clause tends to be selected
together with the matrix clause rather than being selected alone. Finally, we
briefly discuss the implications of our findings for Turkish discourse parsing.",https://github.com/disrpt/sharedtask2019/tree/master/data/tur.pdtb.tdb,-1
HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images,0.101834,"In this paper, we present the results of the HSE-NN team in the 4th
competition on Affective Behavior Analysis in-the-wild (ABAW). The novel
multi-task EfficientNet model is trained for simultaneous recognition of facial
expressions and prediction of valence and arousal on static photos. The
resulting MT-EmotiEffNet extracts visual features that are fed into simple
feed-forward neural networks in the multi-task learning challenge. We obtain
performance measure 1.3 on the validation set, which is significantly greater
when compared to either performance of baseline (0.3) or existing models that
are trained only on the s-Aff-Wild2 database. In the learning from synthetic
data challenge, the quality of the original synthetic training set is increased
by using the super-resolution techniques, such as Real-ESRGAN. Next, the
MT-EmotiEffNet is fine-tuned on the new training set. The final prediction is a
simple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our
average validation F1 score is 18% greater than the baseline convolutional
neural network.",https://github.com/HSE-asavchenko/face-emotion-recognition/blob/main/src/ABAW,-1
Combined CNN Transformer Encoder for Enhanced Fine-grained Human Action Recognition,0.0301587,"Fine-grained action recognition is a challenging task in computer vision. As
fine-grained datasets have small inter-class variations in spatial and temporal
space, fine-grained action recognition model requires good temporal reasoning
and discrimination of attribute action semantics. Leveraging on CNN's ability
in capturing high level spatial-temporal feature representations and
Transformer's modeling efficiency in capturing latent semantics and global
dependencies, we investigate two frameworks that combine CNN vision backbone
and Transformer Encoder to enhance fine-grained action recognition: 1) a
vision-based encoder to learn latent temporal semantics, and 2) a multi-modal
video-text cross encoder to exploit additional text input and learn cross
association between visual and text semantics. Our experimental results show
that both our Transformer encoder frameworks effectively learn latent temporal
semantics and cross-modality association, with improved recognition performance
over CNN vision model. We achieve new state-of-the-art performance on the
FineGym benchmark dataset for both proposed architectures.",None,-1
FL Games: A federated learning framework for distribution shifts,0.151028,"Federated learning aims to train predictive models for data that is
distributed across clients, under the orchestration of a server. However,
participating clients typically each hold data from a different distribution,
whereby predictive models with strong in-distribution generalization can fail
catastrophically on unseen domains. In this work, we argue that in order to
generalize better across non-i.i.d. clients, it is imperative to only learn
correlations that are stable and invariant across domains. We propose FL Games,
a game-theoretic framework for federated learning for learning causal features
that are invariant across clients. While training to achieve the Nash
equilibrium, the traditional best response strategy suffers from high-frequency
oscillations. We demonstrate that FL Games effectively resolves this challenge
and exhibits smooth performance curves. Further, FL Games scales well in the
number of clients, requires significantly fewer communication rounds, and is
agnostic to device heterogeneity. Through empirical evaluation, we demonstrate
that FL Games achieves high out-of-distribution performance on various
benchmarks.",None,-1
An End-to-End Dialogue Summarization System for Sales Calls,0.187388,"Summarizing sales calls is a routine task performed manually by salespeople.
We present a production system which combines generative models fine-tuned for
customer-agent setting, with a human-in-the-loop user experience for an
interactive summary curation process. We address challenging aspects of
dialogue summarization task in a real-world setting including long input
dialogues, content validation, lack of labeled data and quality evaluation. We
show how GPT-3 can be leveraged as an offline data labeler to handle training
data scarcity and accommodate privacy constraints in an industrial setting.
Experiments show significant improvements by our models in tackling the
summarization and content validation tasks on public datasets.",https://github.com/google-research/google-research/tree/master/rouge,-1
System Network Analytics: Evolution and Stable Rules of a State Series,0.0219311,"System Evolution Analytics on a system that evolves is a challenge because it
makes a State Series SS = {S1, S2... SN} (i.e., a set of states ordered by
time) with several inter-connected entities changing over time. We present
stability characteristics of interesting evolution rules occurring in multiple
states. We defined an evolution rule with its stability as the fraction of
states in which the rule is interesting. Extensively, we defined stable rule as
the evolution rule having stability that exceeds a given threshold minimum
stability (minStab). We also defined persistence metric, a quantitative measure
of persistent entity-connections. We explain this with an approach and
algorithm for System Network Analytics (SysNet-Analytics), which uses minStab
to retrieve Network Evolution Rules (NERs) and Stable NERs (SNERs). The
retrieved information is used to calculate a proposed System Network
Persistence (SNP) metric. This work is automated as a SysNet-Analytics Tool to
demonstrate application on real world systems including: software system,
natural-language system, retail market system, and IMDb system. We quantified
stability and persistence of entity-connections in a system state series. This
results in evolution information, which helps in system evolution analytics
based on knowledge discovery and data mining.",None,-1
Fairness Increases Adversarial Vulnerability,0.0378866,"The remarkable performance of deep learning models and their applications in
consequential domains (e.g., facial recognition) introduces important
challenges at the intersection of equity and security. Fairness and robustness
are two desired notions often required in learning models. Fairness ensures
that models do not disproportionately harm (or benefit) some groups over
others, while robustness measures the models' resilience against small input
perturbations.
  This paper shows the existence of a dichotomy between fairness and
robustness, and analyzes when achieving fairness decreases the model robustness
to adversarial samples. The reported analysis sheds light on the factors
causing such contrasting behavior, suggesting that distance to the decision
boundary across groups as a key explainer for this behavior. Extensive
experiments on non-linear models and different architectures validate the
theoretical findings in multiple vision domains. Finally, the paper proposes a
simple, yet effective, solution to construct models achieving good tradeoffs
between fairness and robustness.",None,-1
Towards Generalizable and Robust Text-to-SQL Parsing,0.209627,"Text-to-SQL parsing tackles the problem of mapping natural language questions
to executable SQL queries. In practice, text-to-SQL parsers often encounter
various challenging scenarios, requiring them to be generalizable and robust.
While most existing work addresses a particular generalization or robustness
challenge, we aim to study it in a more comprehensive manner. In specific, we
believe that text-to-SQL parsers should be (1) generalizable at three levels of
generalization, namely i.i.d., zero-shot, and compositional, and (2) robust
against input perturbations. To enhance these capabilities of the parser, we
propose a novel TKK framework consisting of Task decomposition, Knowledge
acquisition, and Knowledge composition to learn text-to-SQL parsing in stages.
By dividing the learning process into multiple stages, our framework improves
the parser's ability to acquire general SQL knowledge instead of capturing
spurious patterns, making it more generalizable and robust. Experimental
results under various generalization and robustness settings show that our
framework is effective in all scenarios and achieves state-of-the-art
performance on the Spider, SParC, and CoSQL datasets. Code can be found at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/tkk.",https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/tkk,-1
Out-Of-Distribution Detection In Unsupervised Continual Learning,0.0858309,"Unsupervised continual learning aims to learn new tasks incrementally without
requiring human annotations. However, most existing methods, especially those
targeted on image classification, only work in a simplified scenario by
assuming all new data belong to new tasks, which is not realistic if the class
labels are not provided. Therefore, to perform unsupervised continual learning
in real life applications, an out-of-distribution detector is required at
beginning to identify whether each new data corresponds to a new task or
already learned tasks, which still remains under-explored yet. In this work, we
formulate the problem for Out-of-distribution Detection in Unsupervised
Continual Learning (OOD-UCL) with the corresponding evaluation protocol. In
addition, we propose a novel OOD detection method by correcting the output bias
at first and then enhancing the output confidence for in-distribution data
based on task discriminativeness, which can be applied directly without
modifying the learning procedures and objectives of continual learning. Our
method is evaluated on CIFAR-100 dataset by following the proposed evaluation
protocol and we show improved performance compared with existing OOD detection
methods under the unsupervised continual learning scenario.",None,-1
Personalized Federated Learning for Multi-task Fault Diagnosis of Rotating Machinery,0.0291445,"Intelligent fault diagnosis is essential to safe operation of machinery.
However, due to scarce fault samples and data heterogeneity in field machinery,
deep learning based diagnosis methods are prone to over-fitting with poor
generalization ability. To solve the problem, this paper proposes a
personalized federated learning framework, enabling multi-task fault diagnosis
method across multiple factories in a privacypreserving manner. Firstly,
rotating machines from different factories with similar vibration feature data
are categorized into machine groups using a federated clustering method. Then,
a multi-task deep learning model based on convolutional neural network is
constructed to diagnose the multiple faults of machinery with heterogeneous
information fusion. Finally, a personalized federated learning framework is
proposed to solve data heterogeneity across different machines using adaptive
hierarchical aggregation strategy. The case study on collected data from real
machines verifies the effectiveness of the proposed framework. The result shows
that the diagnosis accuracy could be improved significantly using the proposed
personalized federated learning, especially for those machines with scarce
fault samples.",None,-1
Can domain adaptation make object recognition work for everyone?,0.0289809,"Despite the rapid progress in deep visual recognition, modern computer vision
datasets significantly overrepresent the developed world and models trained on
such datasets underperform on images from unseen geographies. We investigate
the effectiveness of unsupervised domain adaptation (UDA) of such models across
geographies at closing this performance gap. To do so, we first curate two
shifts from existing datasets to study the Geographical DA problem, and
discover new challenges beyond data distribution shift: context shift, wherein
object surroundings may change significantly across geographies, and
subpopulation shift, wherein the intra-category distributions may shift. We
demonstrate the inefficacy of standard DA methods at Geographical DA,
highlighting the need for specialized geographical adaptation solutions to
address the challenge of making object recognition work for everyone.",None,-1
EISeg: An Efficient Interactive Segmentation Tool based on PaddlePaddle,0.0679411,"In recent years, the rapid development of deep learning has brought great
advancements to image and video segmentation methods based on neural networks.
However, to unleash the full potential of such models, large numbers of
high-quality annotated images are necessary for model training. Currently, many
widely used open-source image segmentation software relies heavily on manual
annotation which is tedious and time-consuming. In this work, we introduce
EISeg, an Efficient Interactive SEGmentation annotation tool that can
drastically improve image segmentation annotation efficiency, generating highly
accurate segmentation masks with only a few clicks. We also provide various
domain-specific models for remote sensing, medical imaging, industrial quality
inspections, human segmentation, and temporal aware models for video
segmentation. The source code for our algorithm and user interface are
available at: https://github.com/PaddlePaddle/PaddleSeg.",https://github.com/PaddlePaddle/PaddleSeg,-1
Understanding Adversarial Robustness of Vision Transformers via Cauchy Problem,0.0950662,"Recent research on the robustness of deep learning has shown that Vision
Transformers (ViTs) surpass the Convolutional Neural Networks (CNNs) under some
perturbations, e.g., natural corruption, adversarial attacks, etc. Some papers
argue that the superior robustness of ViT comes from the segmentation of its
input images; others say that the Multi-head Self-Attention (MSA) is the key to
preserving the robustness. In this paper, we aim to introduce a principled and
unified theoretical framework to investigate such an argument on ViT's
robustness. We first theoretically prove that, unlike Transformers in Natural
Language Processing, ViTs are Lipschitz continuous. Then we theoretically
analyze the adversarial robustness of ViTs from the perspective of the Cauchy
Problem, via which we can quantify how the robustness propagates through
layers. We demonstrate that the first and last layers are the critical factors
to affect the robustness of ViTs. Furthermore, based on our theory, we
empirically show that unlike the claims from existing research, MSA only
contributes to the adversarial robustness of ViTs under weak adversarial
attacks, e.g., FGSM, and surprisingly, MSA actually comprises the model's
adversarial robustness under stronger attacks, e.g., PGD attacks.",None,-1
What Do We Maximize in Self-Supervised Learning?,0.411248,"In this paper, we examine self-supervised learning methods, particularly
VICReg, to provide an information-theoretical understanding of their
construction. As a first step, we demonstrate how information-theoretic
quantities can be obtained for a deterministic network, offering a possible
alternative to prior work that relies on stochastic models. This enables us to
demonstrate how VICReg can be (re)discovered from first principles and its
assumptions about data distribution. Furthermore, we empirically demonstrate
the validity of our assumptions, confirming our novel understanding of VICReg.
Finally, we believe that the derivation and insights we obtain can be
generalized to many other SSL methods, opening new avenues for theoretical and
practical understanding of SSL and transfer learning.",None,-1
KSS-ICP: Point Cloud Registration based on Kendall Shape Space,0.0420648,"Point cloud registration is a popular topic which has been widely used in 3D
model reconstruction, location, and retrieval. In this paper, we propose a new
registration method, KSS-ICP, to address the rigid registration task in Kendall
shape space (KSS) with Iterative Closest Point (ICP). The KSS is a quotient
space that removes influences of translations, scales, and rotations for shape
feature-based analysis. Such influences can be concluded as the similarity
transformations that do not change the shape feature. The point cloud
representation in KSS is invariant to similarity transformations. We utilize
such property to design the KSS-ICP for point cloud registration. To tackle the
difficulty to achieve the KSS representation in general, the proposed KSS-ICP
formulates a practical solution that does not require complex feature analysis,
data training, and optimization. With a simple implementation, KSS-ICP achieves
more accurate registration from point clouds. It is robust to similarity
transformation, non-uniform density, noise, and defective parts. Experiments
show that KSS-ICP has better performance than the state of the art.",None,-1
Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds,0.830527,"Despite impressive successes, deep reinforcement learning (RL) systems still
fall short of human performance on generalization to new tasks and environments
that differ from their training. As a benchmark tailored for studying RL
generalization, we introduce Avalon, a set of tasks in which embodied agents in
highly diverse procedural 3D worlds must survive by navigating terrain, hunting
or gathering food, and avoiding hazards. Avalon is unique among existing RL
benchmarks in that the reward function, world dynamics, and action space are
the same for every task, with tasks differentiated solely by altering the
environment; its 20 tasks, ranging in complexity from eat and throw to hunt and
navigate, each create worlds in which the agent must perform specific skills in
order to survive. This setup enables investigations of generalization within
tasks, between tasks, and to compositional tasks that require combining skills
learned from previous tasks. Avalon includes a highly efficient simulator, a
library of baselines, and a benchmark with scoring metrics evaluated against
hundreds of hours of human performance, all of which are open-source and
publicly available. We find that standard RL baselines make progress on most
tasks but are still far from human performance, suggesting Avalon is
challenging enough to advance the quest for generalizable RL.",https://generallyintelligent.com/avalon,-1
Boosting Natural Language Generation from Instructions with Meta-Learning,0.033702,"Recent work has shown that language models (LMs) trained with multi-task
\textit{instructional learning} (MTIL) can solve diverse NLP tasks in zero- and
few-shot settings with improved performance compared to prompt tuning. MTIL
illustrates that LMs can extract and use information about the task from
instructions beyond the surface patterns of the inputs and outputs. This
suggests that meta-learning may further enhance the utilization of instructions
for effective task transfer. In this paper we investigate whether meta-learning
applied to MTIL can further improve generalization to unseen tasks in a
zero-shot setting. Specifically, we propose to adapt meta-learning to MTIL in
three directions: 1) Model Agnostic Meta Learning (MAML), 2) Hyper-Network
(HNet) based adaptation to generate task specific parameters conditioned on
instructions, and 3) an approach combining HNet and MAML. Through extensive
experiments on the large scale Natural Instructions V2 dataset, we show that
our proposed approaches significantly improve over strong baselines in
zero-shot settings. In particular, meta-learning improves the effectiveness of
instructions and is most impactful when the test tasks are strictly zero-shot
(i.e. no similar tasks in the training set) and are ""hard"" for LMs,
illustrating the potential of meta-learning for MTIL for out-of-distribution
tasks.",https://github.com/nicola-decao/KnowledgeEditor,-1
An Explainable Regression Framework for Predicting Remaining Useful Life of Machines,0.0197276,"Prediction of a machine's Remaining Useful Life (RUL) is one of the key tasks
in predictive maintenance. The task is treated as a regression problem where
Machine Learning (ML) algorithms are used to predict the RUL of machine
components. These ML algorithms are generally used as a black box with a total
focus on the performance without identifying the potential causes behind the
algorithms' decisions and their working mechanism. We believe, the performance
(in terms of Mean Squared Error (MSE), etc.,) alone is not enough to build the
trust of the stakeholders in ML prediction rather more insights on the causes
behind the predictions are needed. To this aim, in this paper, we explore the
potential of Explainable AI (XAI) techniques by proposing an explainable
regression framework for the prediction of machines' RUL. We also evaluate
several ML algorithms including classical and Neural Networks (NNs) based
solutions for the task. For the explanations, we rely on two model agnostic XAI
methods namely Local Interpretable Model-Agnostic Explanations (LIME) and
Shapley Additive Explanations (SHAP). We believe, this work will provide a
baseline for future research in the domain.",None,-1
Structure-Aware Flow Generation for Human Body Reshaping,0.0226035,"Body reshaping is an important procedure in portrait photo retouching. Due to
the complicated structure and multifarious appearance of human bodies, existing
methods either fall back on the 3D domain via body morphable model or resort to
keypoint-based image deformation, leading to inefficiency and unsatisfied
visual quality. In this paper, we address these limitations by formulating an
end-to-end flow generation architecture under the guidance of body structural
priors, including skeletons and Part Affinity Fields, and achieve
unprecedentedly controllable performance under arbitrary poses and garments. A
compositional attention mechanism is introduced for capturing both visual
perceptual correlations and structural associations of the human body to
reinforce the manipulation consistency among related parts. For a comprehensive
evaluation, we construct the first large-scale body reshaping dataset, namely
BR-5K, which contains 5,000 portrait photos as well as professionally retouched
targets. Extensive experiments demonstrate that our approach significantly
outperforms existing state-of-the-art methods in terms of visual performance,
controllability, and efficiency. The dataset is available at our website:
https://github.com/JianqiangRen/FlowBasedBodyReshaping.",https://github.com/JianqiangRen/FlowBasedBodyReshaping,-1
ON-DEMAND-FL: A Dynamic and Efficient Multi-Criteria Federated Learning Client Deployment Scheme,0.653756,"In this paper, we increase the availability and integration of devices in the
learning process to enhance the convergence of federated learning (FL) models.
To address the issue of having all the data in one location, federated
learning, which maintains the ability to learn over decentralized data sets,
combines privacy and technology. Until the model converges, the server combines
the updated weights obtained from each dataset over a number of rounds. The
majority of the literature suggested client selection techniques to accelerate
convergence and boost accuracy. However, none of the existing proposals have
focused on the flexibility to deploy and select clients as needed, wherever and
whenever that may be. Due to the extremely dynamic surroundings, some devices
are actually not available to serve as clients in FL, which affects the
availability of data for learning and the applicability of the existing
solution for client selection. In this paper, we address the aforementioned
limitations by introducing an On-Demand-FL, a client deployment approach for
FL, offering more volume and heterogeneity of data in the learning process. We
make use of the containerization technology such as Docker to build efficient
environments using IoT and mobile devices serving as volunteers. Furthermore,
Kubernetes is used for orchestration. The Genetic algorithm (GA) is used to
solve the multi-objective optimization problem due to its evolutionary
strategy. The performed experiments using the Mobile Data Challenge (MDC)
dataset and the Localfed framework illustrate the relevance of the proposed
approach and the efficiency of the on-the-fly deployment of clients whenever
and wherever needed with less discarded rounds and more available data.",None,-1
Surrogate Gradient Spiking Neural Networks as Encoders for Large Vocabulary Continuous Speech Recognition,0.0217921,"Compared to conventional artificial neurons that produce dense and
real-valued responses, biologically-inspired spiking neurons transmit sparse
and binary information, which can also lead to energy-efficient
implementations. Recent research has shown that spiking neural networks can be
trained like standard recurrent neural networks using the surrogate gradient
method. They have shown promising results on speech command recognition tasks.
Using the same technique, we show that they are scalable to large vocabulary
continuous speech recognition, where they are capable of replacing LSTMs in the
encoder with only minor loss of performance. This suggests that they may be
applicable to more involved sequence-to-sequence tasks. Moreover, in contrast
to their recurrent non-spiking counterparts, they show robustness to exploding
gradient problems without the need to use gates.",None,-1
Panini-Net: GAN Prior Based Degradation-Aware Feature Interpolation for Face Restoration,0.18614,"Emerging high-quality face restoration (FR) methods often utilize pre-trained
GAN models (\textit{i.e.}, StyleGAN2) as GAN Prior. However, these methods
usually struggle to balance realness and fidelity when facing various
degradation levels. Besides, there is still a noticeable visual quality gap
compared with pre-trained GAN models. In this paper, we propose a novel GAN
Prior based degradation-aware feature interpolation network, dubbed Panini-Net,
for FR tasks by explicitly learning the abstract representations to distinguish
various degradations. Specifically, an unsupervised degradation representation
learning (UDRL) strategy is first developed to extract degradation
representations (DR) of the input degraded images. Then, a degradation-aware
feature interpolation (DAFI) module is proposed to dynamically fuse the two
types of informative features (\textit{i.e.}, features from input images and
features from GAN Prior) with flexible adaption to various degradations based
on DR. Ablation studies reveal the working mechanism of DAFI and its potential
for editable FR. Extensive experiments demonstrate that our Panini-Net achieves
state-of-the-art performance for multi-degradation face restoration and face
super-resolution. The source code is available at
https://github.com/jianzhangcs/panini.",https://github.com/jianzhangcs/panini,-1
Examining the Differential Risk from High-level Artificial Intelligence and the Question of Control,0.0228569,"Artificial Intelligence (AI) is one of the most transformative technologies
of the 21st century. The extent and scope of future AI capabilities remain a
key uncertainty, with widespread disagreement on timelines and potential
impacts. As nations and technology companies race toward greater complexity and
autonomy in AI systems, there are concerns over the extent of integration and
oversight of opaque AI decision processes. This is especially true in the
subfield of machine learning (ML), where systems learn to optimize objectives
without human assistance. Objectives can be imperfectly specified or executed
in an unexpected or potentially harmful way. This becomes more concerning as
systems increase in power and autonomy, where an abrupt capability jump could
result in unexpected shifts in power dynamics or even catastrophic failures.
This study presents a hierarchical complex systems framework to model AI risk
and provide a template for alternative futures analysis. Survey data were
collected from domain experts in the public and private sectors to classify AI
impact and likelihood. The results show increased uncertainty over the powerful
AI agent scenario, confidence in multiagent environments, and increased concern
over AI alignment failures and influence-seeking behavior.",None,-1
AuxMix: Semi-Supervised Learning with Unconstrained Unlabeled Data,0.070173,"Semi-supervised learning (SSL) has seen great strides when labeled data is
scarce but unlabeled data is abundant. Critically, most recent work assume that
such unlabeled data is drawn from the same distribution as the labeled data. In
this work, we show that state-of-the-art SSL algorithms suffer a degradation in
performance in the presence of unlabeled auxiliary data that does not
necessarily possess the same class distribution as the labeled set. We term
this problem as Auxiliary-SSL and propose AuxMix, an algorithm that leverages
self-supervised learning tasks to learn generic features in order to mask
auxiliary data that are not semantically similar to the labeled set. We also
propose to regularize learning by maximizing the predicted entropy for
dissimilar auxiliary samples. We show an improvement of 5% over existing
baselines on a ResNet-50 model when trained on CIFAR10 dataset with 4k labeled
samples and all unlabeled data is drawn from the Tiny-ImageNet dataset. We
report competitive results on several datasets and conduct ablation studies.",None,-1
DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation,0.377627,"Deep learning approaches achieve prominent success in 3D semantic
segmentation. However, collecting densely annotated real-world 3D datasets is
extremely time-consuming and expensive. Training models on synthetic data and
generalizing on real-world scenarios becomes an appealing alternative, but
unfortunately suffers from notorious domain shifts. In this work, we propose a
Data-Oriented Domain Adaptation (DODA) framework to mitigate pattern and
context gaps caused by different sensing mechanisms and layout placements
across domains. Our DODA encompasses virtual scan simulation to imitate
real-world point cloud patterns and tail-aware cuboid mixing to alleviate the
interior context gap with a cuboid-based intermediate domain. The first
unsupervised sim-to-real adaptation benchmark on 3D indoor semantic
segmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular
Unsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA
approaches by over 13% on both 3D-FRONT -> ScanNet and 3D-FRONT -> S3DIS. Code
is available at https://github.com/CVMI-Lab/DODA.",https://github.com/CVMI-Lab/DODA,-1
Multi-Armed Bandits in Brain-Computer Interfaces,0.0356956,"The multi-armed bandit (MAB) problem models a decision-maker that optimizes
its actions based on current and acquired new knowledge to maximize its reward.
This type of online decision is prominent in many procedures of Brain-Computer
Interfaces (BCIs) and MAB has previously been used to investigate, e.g., what
mental commands to use to optimize BCI performance. However, MAB optimization
in the context of BCI is still relatively unexplored, even though it has the
potential to improve BCI performance during both calibration and real-time
implementation. Therefore, this review aims to further introduce MABs to the
BCI community. The review includes a background on MAB problems and standard
solution methods, and interpretations related to BCI systems. Moreover, it
includes state-of-the-art concepts of MAB in BCI and suggestions for future
research.",https://github.com/SMPyBandits/SMPyBandits,-1
Feature-Level Debiased Natural Language Understanding,0.03703,"Natural language understanding (NLU) models often rely on dataset biases
rather than intended task-relevant features to achieve high performance on
specific datasets. As a result, these models perform poorly on datasets outside
the training distribution. Some recent studies address this issue by reducing
the weights of biased samples during the training process. However, these
methods still encode biased latent features in representations and neglect the
dynamic nature of bias, which hinders model prediction. We propose an NLU
debiasing method, named debiasing contrastive learning (DCT), to simultaneously
alleviate the above problems based on contrastive learning. We devise a
debiasing, positive sampling strategy to mitigate biased latent features by
selecting the least similar biased positive samples. We also propose a dynamic
negative sampling strategy to capture the dynamic influence of biases by
employing a bias-only model to dynamically select the most similar biased
negative samples. We conduct experiments on three NLU benchmark datasets.
Experimental results show that DCT outperforms state-of-the-art baselines on
out-of-distribution datasets while maintaining in-distribution performance. We
also verify that DCT can reduce biased latent features from the model's
representation.",https://github.com/youganglyu/DCT,-1
Through a fair looking-glass: mitigating bias in image datasets,0.182899,"With the recent growth in computer vision applications, the question of how
fair and unbiased they are has yet to be explored. There is abundant evidence
that the bias present in training data is reflected in the models, or even
amplified. Many previous methods for image dataset de-biasing, including models
based on augmenting datasets, are computationally expensive to implement. In
this study, we present a fast and effective model to de-bias an image dataset
through reconstruction and minimizing the statistical dependence between
intended variables. Our architecture includes a U-net to reconstruct images,
combined with a pre-trained classifier which penalizes the statistical
dependence between target attribute and the protected attribute. We evaluate
our proposed model on CelebA dataset, compare the results with a
state-of-the-art de-biasing method, and show that the model achieves a
promising fairness-accuracy combination.",None,-1
A semantic web approach to uplift decentralized household energy data,0.0096617,"In a decentralized household energy system comprised of various devices such
as home appliances, electric vehicles, and solar panels, end-users are able to
dig deeper into the system's details and further achieve energy sustainability
if they are presented with data on the electric energy consumption and
production at the granularity of the device. However, many databases in this
field are siloed from other domains, including solely information pertaining to
energy. This may result in the loss of information (e.g. weather) on each
device's energy use. Meanwhile, a large number of these datasets have been
extensively used in computational modeling techniques such as machine learning
models. While such computational approaches achieve great accuracy and
performance by concentrating only on a local view of datasets, model
reliability cannot be guaranteed since such models are very vulnerable to data
input fluctuations when information omission is taken into account. This
article tackles the data isolation issue in the field of smart energy systems
by examining Semantic Web methods on top of a household energy system. We offer
an ontology-based approach for managing decentralized data at the device-level
resolution in a system. As a consequence, the scope of the data associated with
each device may easily be expanded in an interoperable manner throughout the
Web, and additional information, such as weather, can be obtained from the Web,
provided that the data is organized according to W3C standards.",https://github.com/futaoo/semantic-energy,-1
Enhanced Deep Animation Video Interpolation,0.0284295,"Existing learning-based frame interpolation algorithms extract consecutive
frames from high-speed natural videos to train the model. Compared to natural
videos, cartoon videos are usually in a low frame rate. Besides, the motion
between consecutive cartoon frames is typically nonlinear, which breaks the
linear motion assumption of interpolation algorithms. Thus, it is unsuitable
for generating a training set directly from cartoon videos. For better adapting
frame interpolation algorithms from nature video to animation video, we present
AutoFI, a simple and effective method to automatically render training data for
deep animation video interpolation. AutoFI takes a layered architecture to
render synthetic data, which ensures the assumption of linear motion.
Experimental results show that AutoFI performs favorably in training both DAIN
and ANIN. However, most frame interpolation algorithms will still fail in
error-prone areas, such as fast motion or large occlusion. Besides AutoFI, we
also propose a plug-and-play sketch-based post-processing module, named SktFI,
to refine the final results using user-provided sketches manually. With AutoFI
and SktFI, the interpolated animation frames show high perceptual quality.",https://github.com/laomao0/AutoSktFI,-1
Alignment-Uniformity aware Representation Learning for Zero-shot Video Classification,0.136379,"Most methods tackle zero-shot video classification by aligning
visual-semantic representations within seen classes, which limits
generalization to unseen classes. To enhance model generalizability, this paper
presents an end-to-end framework that preserves alignment and uniformity
properties for representations on both seen and unseen classes. Specifically,
we formulate a supervised contrastive loss to simultaneously align
visual-semantic features (i.e., alignment) and encourage the learned features
to distribute uniformly (i.e., uniformity). Unlike existing methods that only
consider the alignment, we propose uniformity to preserve maximal-info of
existing features, which improves the probability that unobserved features fall
around observed data. Further, we synthesize features of unseen classes by
proposing a class generator that interpolates and extrapolates the features of
seen classes. Besides, we introduce two metrics, closeness and dispersion, to
quantify the two properties and serve as new measurements of model
generalizability. Experiments show that our method significantly outperforms
SoTA by relative improvements of 28.1% on UCF101 and 27.0% on HMDB51. Code is
available.",https://github.com/ShipuLoveMili/CVPR2022-AURL,-1
"REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research",0.00141441,"Transparency around limitations can improve the scientific rigor of research,
help ensure appropriate interpretation of research findings, and make research
claims more credible. Despite these benefits, the machine learning (ML)
research community lacks well-developed norms around disclosing and discussing
limitations. To address this gap, we conduct an iterative design process with
30 ML and ML-adjacent researchers to develop and test REAL ML, a set of guided
activities to help ML researchers recognize, explore, and articulate the
limitations of their research. Using a three-stage interview and survey study,
we identify ML researchers' perceptions of limitations, as well as the
challenges they face when recognizing, exploring, and articulating limitations.
We develop REAL ML to address some of these practical challenges, and highlight
additional cultural challenges that will require broader shifts in community
norms to address. We hope our study and REAL ML help move the ML research
community toward more active and appropriate engagement with limitations.",https://github.com/jesmith14/REAL-ML,-1
Large vocabulary speech recognition for languages of Africa: multilingual modeling and self-supervised learning,0.0584604,"Almost none of the 2,000+ languages spoken in Africa have widely available
automatic speech recognition systems, and the required data is also only
available for a few languages. We have experimented with two techniques which
may provide pathways to large vocabulary speech recognition for African
languages: multilingual modeling and self-supervised learning. We gathered
available open source data and collected data for 15 languages, and trained
experimental models using these techniques. Our results show that pooling the
small amounts of data available in multilingual end-to-end models, and
pre-training on unsupervised data can help improve speech recognition quality
for many African languages.",None,-1
OTPose: Occlusion-Aware Transformer for Pose Estimation in Sparsely-Labeled Videos,0.120634,"Although many approaches for multi-human pose estimation in videos have shown
profound results, they require densely annotated data which entails excessive
man labor. Furthermore, there exists occlusion and motion blur that inevitably
lead to poor estimation performance. To address these problems, we propose a
method that leverages an attention mask for occluded joints and encodes
temporal dependency between frames using transformers. First, our framework
composes different combinations of sparsely annotated frames that denote the
track of the overall joint movement. We propose an occlusion attention mask
from these combinations that enable encoding occlusion-aware heatmaps as a
semi-supervised task. Second, the proposed temporal encoder employs transformer
architecture to effectively aggregate the temporal relationship and
keypoint-wise attention from each time step and accurately refines the target
frame's final pose estimation. We achieve state-of-the-art pose estimation
results for PoseTrack2017 and PoseTrack2018 datasets and demonstrate the
robustness of our approach to occlusion and motion blur in sparsely annotated
video data.",None,-1
A Multi-label Continual Learning Framework to Scale Deep Learning Approaches for Packaging Equipment Monitoring,0.047126,"Continual Learning aims to learn from a stream of tasks, being able to
remember at the same time both new and old tasks. While many approaches were
proposed for single-class classification, multi-label classification in the
continual scenario remains a challenging problem. For the first time, we study
multi-label classification in the Domain Incremental Learning scenario.
Moreover, we propose an efficient approach that has a logarithmic complexity
with regard to the number of tasks, and can be applied also in the Class
Incremental Learning scenario. We validate our approach on a real-world
multi-label Alarm Forecasting problem from the packaging industry. For the sake
of reproducibility, the dataset and the code used for the experiments are
publicly available.",https://github.com/dallepezze/bat-ocdm,-1
SmartFPS: Neural Network based Wireless-inertial fusion positioning system,0.0548616,"The current fusion positioning systems are mainly based on filtering
algorithms, such as Kalman filtering or particle filtering. However, the system
complexity of practical application scenarios is often very high, such as noise
modeling in pedestrian inertial navigation systems, or environmental noise
modeling in fingerprint matching and localization algorithms. To solve this
problem, this paper proposes a fusion positioning system based on deep learning
and proposes a transfer learning strategy for improving the performance of
neural network models for samples with different distributions. The results
show that in the whole floor scenario, the average positioning accuracy of the
fusion network is 0.506m. The experiment results of transfer learning show that
the estimation accuracy of the inertial navigation positioning step size and
rotation angle of different pedestrians can be improved by 53.3% on average,
the Bluetooth positioning accuracy of different devices can be improved by
33.4%, and the fusion can be improved by 31.6%.",None,-1
RV4JaCa -- Runtime Verification for Multi-Agent Systems,0.119187,"This paper presents a Runtime Verification (RV) approach for Multi-Agent
Systems (MAS) using the JaCaMo framework. Our objective is to bring a layer of
security to the MAS. This layer is capable of controlling events during the
execution of the system without needing a specific implementation in the
behaviour of each agent to recognise the events. MAS have been used in the
context of hybrid intelligence. This use requires communication between
software agents and human beings. In some cases, communication takes place via
natural language dialogues. However, this kind of communication brings us to a
concern related to controlling the flow of dialogue so that agents can prevent
any change in the topic of discussion that could impair their reasoning. We
demonstrate the implementation of a monitor that aims to control this dialogue
flow in a MAS that communicates with the user through natural language to aid
decision-making in hospital bed allocation.",https://github.com/DeboraEngelmann/RV4JaCa,-1
Unfolding Local Growth Rate Estimates for (Almost) Perfect Adversarial Detection,0.0296808,"Convolutional neural networks (CNN) define the state-of-the-art solution on
many perceptual tasks. However, current CNN approaches largely remain
vulnerable against adversarial perturbations of the input that have been
crafted specifically to fool the system while being quasi-imperceptible to the
human eye. In recent years, various approaches have been proposed to defend
CNNs against such attacks, for example by model hardening or by adding explicit
defence mechanisms. Thereby, a small ""detector"" is included in the network and
trained on the binary classification task of distinguishing genuine data from
data containing adversarial perturbations. In this work, we propose a simple
and light-weight detector, which leverages recent findings on the relation
between networks' local intrinsic dimensionality (LID) and adversarial attacks.
Based on a re-interpretation of the LID measure and several simple adaptations,
we surpass the state-of-the-art on adversarial detection by a significant
margin and reach almost perfect results in terms of F1-score for several
networks and datasets. Sources available at:
https://github.com/adverML/multiLID",https://github.com/adverML/multiLID,-1
Measuring Data,0.00750929,"We identify the task of measuring data to quantitatively characterize the
composition of machine learning data and datasets. Similar to an object's
height, width, and volume, data measurements quantify different attributes of
data along common dimensions that support comparison. Several lines of research
have proposed what we refer to as measurements, with differing terminology; we
bring some of this work together, particularly in fields of computer vision and
language, and build from it to motivate measuring data as a critical component
of responsible AI development. Measuring data aids in systematically building
and analyzing machine learning (ML) data towards specific goals and gaining
better control of what modern ML systems will learn. We conclude with a
discussion of the many avenues of future work, the limitations of data
measurements, and how to leverage these measurement approaches in research and
practice.",None,-1
minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models,0.61318,"We present minicons, an open source library that provides a standard API for
researchers interested in conducting behavioral and representational analyses
of transformer-based language models (LMs). Specifically, minicons enables
researchers to apply analysis methods at two levels: (1) at the prediction
level -- by providing functions to efficiently extract word/sentence level
probabilities; and (2) at the representational level -- by also facilitating
efficient extraction of word/phrase level vectors from one or more layers. In
this paper, we describe the library and apply it to two motivating case
studies: One focusing on the learning dynamics of the BERT architecture on
relative grammatical judgments, and the other on benchmarking 23 different LMs
on zero-shot abductive reasoning. minicons is available at
https://github.com/kanishkamisra/minicons",https://github.com/kanishkamisra/minicons,-1
Bringing NURC/SP to Digital Life: the Role of Open-source Automatic Speech Recognition Models,0.0122489,"The NURC Project that started in 1969 to study the cultured linguistic urban
norm spoken in five Brazilian capitals, was responsible for compiling a large
corpus for each capital. The digitized NURC/SP comprises 375 inquiries in 334
hours of recordings taken in S\~ao Paulo capital. Although 47 inquiries have
transcripts, there was no alignment between the audio-transcription, and 328
inquiries were not transcribed. This article presents an evaluation and error
analysis of three automatic speech recognition models trained with spontaneous
speech in Portuguese and one model trained with prepared speech. The evaluation
allowed us to choose the best model, using WER and CER metrics, in a manually
aligned sample of NURC/SP, to automatically transcribe 284 hours.",https://github.com/nilc-nlp/nurc-sp,-1
Simple and Effective Knowledge-Driven Query Expansion for QA-Based Product Attribute Extraction,0.104001,"A key challenge in attribute value extraction (AVE) from e-commerce sites is
how to handle a large number of attributes for diverse products. Although this
challenge is partially addressed by a question answering (QA) approach which
finds a value in product data for a given query (attribute), it does not work
effectively for rare and ambiguous queries. We thus propose simple
knowledge-driven query expansion based on possible answers (values) of a query
(attribute) for QA-based AVE. We retrieve values of a query (attribute) from
the training data to expand the query. We train a model with two tricks,
knowledge dropout and knowledge token mixing, which mimic the imperfection of
the value knowledge in testing. Experimental results on our cleaned version of
AliExpress dataset show that our method improves the performance of AVE (+6.08
macro F1), especially for rare and ambiguous attributes (+7.82 and +6.86 macro
F1, respectively).",https://github.com/lanmanok/ACL19_Scaling_Up_Open_Tagging,-1
A Generative Approach for Script Event Prediction via Contrastive Fine-tuning,0.0306442,"Script event prediction aims to predict the subsequent event given the
context. This requires the capability to infer the correlations between events.
Recent works have attempted to improve event correlation reasoning by using
pretrained language models and incorporating external knowledge~(e.g.,
discourse relations). Though promising results have been achieved, some
challenges still remain. First, the pretrained language models adopted by
current works ignore event-level knowledge, resulting in an inability to
capture the correlations between events well. Second, modeling correlations
between events with discourse relations is limited because it can only capture
explicit correlations between events with discourse markers, and cannot capture
many implicit correlations. To this end, we propose a novel generative approach
for this task, in which a pretrained language model is fine-tuned with an
event-centric pretraining objective and predicts the next event within a
generative paradigm. Specifically, we first introduce a novel event-level blank
infilling strategy as the learning objective to inject event-level knowledge
into the pretrained language model, and then design a likelihood-based
contrastive loss for fine-tuning the generative model. Instead of using an
additional prediction layer, we perform prediction by using sequence
likelihoods generated by the generative model. Our approach models correlations
between events in a soft way without any external knowledge. The
likelihood-based prediction eliminates the need to use additional networks to
make predictions and is somewhat interpretable since it scores each word in the
event. Experimental results on the multi-choice narrative cloze~(MCNC) task
demonstrate that our approach achieves better results than other
state-of-the-art baselines. Our code will be available at
https://github.com/zhufq00/mcnc.",https://github.com/zhufq00/mcnc,-1
Multi-Faceted Distillation of Base-Novel Commonality for Few-shot Object Detection,0.0979819,"Most of existing methods for few-shot object detection follow the fine-tuning
paradigm, which potentially assumes that the class-agnostic generalizable
knowledge can be learned and transferred implicitly from base classes with
abundant samples to novel classes with limited samples via such a two-stage
training strategy. However, it is not necessarily true since the object
detector can hardly distinguish between class-agnostic knowledge and
class-specific knowledge automatically without explicit modeling. In this work
we propose to learn three types of class-agnostic commonalities between base
and novel classes explicitly: recognition-related semantic commonalities,
localization-related semantic commonalities and distribution commonalities. We
design a unified distillation framework based on a memory bank, which is able
to perform distillation of all three types of commonalities jointly and
efficiently. Extensive experiments demonstrate that our method can be readily
integrated into most of existing fine-tuning based methods and consistently
improve the performance by a large margin.",https://github.com/WuShuang1998/MFDC,-1
EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing,0.864665,"The success of Pre-Trained Models (PTMs) has reshaped the development of
Natural Language Processing (NLP). Yet, it is not easy to obtain
high-performing models and deploy them online for industrial practitioners. To
bridge this gap, EasyNLP is designed to make it easy to build NLP applications,
which supports a comprehensive suite of NLP algorithms. It further features
knowledge-enhanced pre-training, knowledge distillation and few-shot learning
functionalities for large-scale PTMs, and provides a unified framework of model
training, inference and deployment for real-world applications. Currently,
EasyNLP has powered over ten business units within Alibaba Group and is
seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.
The source code of our EasyNLP toolkit is released at GitHub
(https://github.com/alibaba/EasyNLP).",https://github.com/alibaba/EasyNLP,-1
"NELLIE: A Neuro-Symbolic Inference Engine for Grounded, Compositional, and Explainable Reasoning",0.0330035,"Our goal is a modern approach to answering questions via systematic reasoning
where answers are supported by human interpretable proof trees grounded in an
NL corpus of authoritative facts. Such a system would help alleviate the
challenges of interpretability and hallucination with modern LMs, and the lack
of grounding of current explanation methods (e.g., Chain-of-Thought). This
paper proposes a new take on Prolog-based inference engines, where we replace
handcrafted rules with a combination of neural language modeling, guided
generation, and semiparametric dense retrieval. Our implementation, NELLIE, is
the first system to demonstrate fully interpretable, end-to-end grounded QA as
entailment tree proof search, going beyond earlier work explaining
known-to-be-true facts from text. In experiments, NELLIE outperforms a
similar-sized state-of-the-art reasoner [Tafjord et al., 2022] while producing
knowledge-grounded explanations. We also find NELLIE can exploit both
semi-structured and NL text corpora to guide reasoning. Together these suggest
a new way to jointly reap the benefits of both modern neural methods and
traditional symbolic reasoning.",https://github.com/JHU-CLSP/NELLIE,-1
Machine Learning-Based User Scheduling in Integrated Satellite-HAPS-Ground Networks,0.0222447,"Integrated space-air-ground networks promise to offer a valuable solution
space for empowering the sixth generation of communication networks (6G),
particularly in the context of connecting the unconnected and ultraconnecting
the connected. Such digital inclusion thrive makes resource management
problems, especially those accounting for load-balancing considerations, of
particular interest. The conventional model-based optimization methods,
however, often fail to meet the real-time processing and quality-of-service
needs, due to the high heterogeneity of the space-air-ground networks, and the
typical complexity of the classical algorithms. Given the premises of
artificial intelligence at automating wireless networks design and the
large-scale heterogeneity of non-terrestrial networks, this paper focuses on
showcasing the prospects of machine learning in the context of user scheduling
in integrated space-air-ground communications. The paper first overviews the
most relevant state-of-the art in the context of machine learning applications
to the resource allocation problems, with a dedicated attention to
space-air-ground networks. The paper then proposes, and shows the benefit of,
one specific use case that uses ensembling deep neural networks for optimizing
the user scheduling policies in integrated space-high altitude platform station
(HAPS)-ground networks. Finally, the paper sheds light on the challenges and
open issues that promise to spur the integration of machine learning in
space-air-ground networks, namely, online HAPS power adaptation, learning-based
channel sensing, data-driven multi-HAPSs resource management, and intelligent
flying taxis-empowered systems.",None,-1
Modeling the Lighting in Scenes as Style for Auto White-Balance Correction,0.106011,"Style may refer to different concepts (e.g. painting style, hairstyle,
texture, color, filter, etc.) depending on how the feature space is formed. In
this work, we propose a novel idea of interpreting the lighting in the single-
and multi-illuminant scenes as the concept of style. To verify this idea, we
introduce an enhanced auto white-balance (AWB) method that models the lighting
in single- and mixed-illuminant scenes as the style factor. Our AWB method does
not require any illumination estimation step, yet contains a network learning
to generate the weighting maps of the images with different WB settings.
Proposed network utilizes the style information, extracted from the scene by a
multi-head style extraction module. AWB correction is completed after blending
these weighting maps and the scene. Experiments on single- and mixed-illuminant
datasets demonstrate that our proposed method achieves promising correction
results when compared to the recent works. This shows that the lighting in the
scenes with multiple illuminations can be modeled by the concept of style.
Source code and trained models are available on
https://github.com/birdortyedi/lighting-as-style-awb-correction.",https://github.com/birdortyedi/lighting-as-style-awb-correction,-1
Introducing the Welsh Text Summarisation Dataset and Baseline Systems,0.0165178,"Welsh is an official language in Wales and is spoken by an estimated 884,300
people (29.2% of the population of Wales). Despite this status and estimated
increase in speaker numbers since the last (2011) census, Welsh remains a
minority language undergoing revitalization and promotion by Welsh Government
and relevant stakeholders. As part of the effort to increase the availability
of Welsh digital technology, this paper introduces the first Welsh
summarisation dataset, which we provide freely for research purposes to help
advance the work on Welsh text summarization. The dataset was created by Welsh
speakers by manually summarising Welsh Wikipedia articles. In addition, the
paper discusses the implementation and evaluation of different summarisation
systems for Welsh. The summarization systems and results will serve as
benchmarks for the development of summarises in other minority language
contexts.",https://github.com/UCREL/welsh-summarization-dataset,-1
NTULM: Enriching Social Media Text Representations with Non-Textual Units,0.0267977,"On social media, additional context is often present in the form of
annotations and meta-data such as the post's author, mentions, Hashtags, and
hyperlinks. We refer to these annotations as Non-Textual Units (NTUs). We posit
that NTUs provide social context beyond their textual semantics and leveraging
these units can enrich social media text representations. In this work we
construct an NTU-centric social heterogeneous network to co-embed NTUs. We then
principally integrate these NTU embeddings into a large pretrained language
model by fine-tuning with these additional units. This adds context to noisy
short-text social media. Experiments show that utilizing NTU-augmented text
representations significantly outperforms existing text-only baselines by 2-5\%
relative points on many downstream tasks highlighting the importance of context
to social media NLP. We also highlight that including NTU context into the
initial layers of language model alongside text is better than using it after
the text embedding is generated. Our work leads to the generation of holistic
general purpose social media content embedding.",None,-1
Auction-Based Ex-Post-Payment Incentive Mechanism Design for Horizontal Federated Learning with Reputation and Contribution Measurement,0.184942,"Federated learning trains models across devices with distributed data, while
protecting the privacy and obtaining a model similar to that of centralized ML.
A large number of workers with data and computing power are the foundation of
federal learning. However, the inevitable costs prevent self-interested workers
from serving for free. Moreover, due to data isolation, task publishers lack
effective methods to select, evaluate and pay reliable workers with
high-quality data. Therefore, we design an auction-based incentive mechanism
for horizontal federated learning with reputation and contribution measurement.
By designing a reasonable method of measuring contribution, we establish the
reputation of workers, which is easy to decline and difficult to improve.
Through reverse auctions, workers bid for tasks, and the task publisher selects
workers combining reputation and bid price. With the budget constraint, winning
workers are paid based on performance. We proved that our mechanism satisfies
the individual rationality of the honest worker, budget feasibility,
truthfulness, and computational efficiency.",None,-1
Categorizing Semantic Representations for Neural Machine Translation,0.00830696,"Modern neural machine translation (NMT) models have achieved competitive
performance in standard benchmarks. However, they have recently been shown to
suffer limitation in compositional generalization, failing to effectively learn
the translation of atoms (e.g., words) and their semantic composition (e.g.,
modification) from seen compounds (e.g., phrases), and thus suffering from
significantly weakened translation performance on unseen compounds during
inference. We address this issue by introducing categorization to the source
contextualized representations. The main idea is to enhance generalization by
reducing sparsity and overfitting, which is achieved by finding prototypes of
token representations over the training set and integrating their embeddings
into the source encoding. Experiments on a dedicated MT dataset (i.e.,
CoGnition) show that our method reduces compositional generalization error
rates by 24\% error reduction. In addition, our conceptually simple method
gives consistently better results than the Transformer baseline on a range of
general MT datasets.",https://github.com/ARIES-LM/CatMT4CG.git,-1
Cold Posteriors through PAC-Bayes,0.0382804,"We investigate the cold posterior effect through the lens of PAC-Bayes
generalization bounds. We argue that in the non-asymptotic setting, when the
number of training samples is (relatively) small, discussions of the cold
posterior effect should take into account that approximate Bayesian inference
does not readily provide guarantees of performance on out-of-sample data.
Instead, out-of-sample error is better described through a generalization
bound. In this context, we explore the connections between the ELBO objective
from variational inference and the PAC-Bayes objectives. We note that, while
the ELBO and PAC-Bayes objectives are similar, the latter objectives naturally
contain a temperature parameter $\lambda$ which is not restricted to be
$\lambda=1$. For both regression and classification tasks, in the case of
isotropic Laplace approximations to the posterior, we show how this
PAC-Bayesian interpretation of the temperature parameter captures the cold
posterior effect.",None,-1
Quantum Motion Segmentation,0.027735,"Motion segmentation is a challenging problem that seeks to identify
independent motions in two or several input images. This paper introduces the
first algorithm for motion segmentation that relies on adiabatic quantum
optimization of the objective function. The proposed method achieves on-par
performance with the state of the art on problem instances which can be mapped
to modern quantum annealers.",None,-1
Self-Supervised Representation Learning With MUlti-Segmental Informational Coding (MUSIC),0.0381399,"Self-supervised representation learning maps high-dimensional data into a
meaningful embedding space, where samples of similar semantic contents are
close to each other. Most of the recent representation learning methods
maximize cosine similarity or minimize the distance between the embedding
features of different views from the same sample usually on the $l2$ normalized
unit hypersphere. To prevent the trivial solutions that all samples have the
same embedding feature, various techniques have been developed, such as
contrastive learning, stop gradient, variance and covariance regularization,
etc. In this study, we propose MUlti-Segmental Informational Coding (MUSIC) for
self-supervised representation learning. MUSIC divides the embedding feature
into multiple segments that discriminatively partition samples into different
semantic clusters and different segments focus on different partition
principles. Information theory measurements are directly used to optimize MUSIC
and theoretically guarantee trivial solutions are avoided. MUSIC does not
depend on commonly used techniques, such as memory bank or large batches,
asymmetry networks, gradient stopping, momentum weight updating, etc, making
the training framework flexible. Our experiments demonstrate that MUSIC
achieves better results than most related Barlow Twins and VICReg methods on
ImageNet classification with linear probing, and requires neither deep
projectors nor large feature dimensions. Code will be made available.",None,-1
Soundness of Data-Aware Processes with Arithmetic Conditions,0.111081,"Data-aware processes represent and integrate structural and behavioural
constraints in a single model, and are thus increasingly investigated in
business process management and information systems engineering. In this
spectrum, Data Petri nets (DPNs) have gained increasing popularity thanks to
their ability to balance simplicity with expressiveness. The interplay of data
and control-flow makes checking the correctness of such models, specifically
the well-known property of soundness, crucial and challenging. A major
shortcoming of previous approaches for checking soundness of DPNs is that they
consider data conditions without arithmetic, an essential feature when dealing
with real-world, concrete applications. In this paper, we attack this open
problem by providing a foundational and operational framework for assessing
soundness of DPNs enriched with arithmetic data conditions. The framework comes
with a proof-of-concept implementation that, instead of relying on ad-hoc
techniques, employs off-the-shelf established SMT technologies. The
implementation is validated on a collection of examples from the literature,
and on synthetic variants constructed from such examples.",None,-1
IMPaSh: A Novel Domain-shift Resistant Representation for Colorectal Cancer Tissue Classification,0.864665,"The appearance of histopathology images depends on tissue type, staining and
digitization procedure. These vary from source to source and are the potential
causes for domain-shift problems. Owing to this problem, despite the great
success of deep learning models in computational pathology, a model trained on
a specific domain may still perform sub-optimally when we apply them to another
domain. To overcome this, we propose a new augmentation called PatchShuffling
and a novel self-supervised contrastive learning framework named IMPaSh for
pre-training deep learning models. Using these, we obtained a ResNet50 encoder
that can extract image representation resistant to domain-shift. We compared
our derived representation against those acquired based on other
domain-generalization techniques by using them for the cross-domain
classification of colorectal tissue images. We show that the proposed method
outperforms other traditional histology domain-adaptation and state-of-the-art
self-supervised learning methods. Code is available at:
https://github.com/trinhvg/IMPash .",https://github.com/trinhvg/IMPash,-1
PercentMatch: Percentile-based Dynamic Thresholding for Multi-Label Semi-Supervised Classification,0.864665,"While much of recent study in semi-supervised learning (SSL) has achieved
strong performance on single-label classification problems, an equally
important yet underexplored problem is how to leverage the advantage of
unlabeled data in multi-label classification tasks. To extend the success of
SSL to multi-label classification, we first analyze with illustrative examples
to get some intuition about the extra challenges exist in multi-label
classification. Based on the analysis, we then propose PercentMatch, a
percentile-based threshold adjusting scheme, to dynamically alter the score
thresholds of positive and negative pseudo-labels for each class during the
training, as well as dynamic unlabeled loss weights that further reduces noise
from early-stage unlabeled predictions. Without loss of simplicity, we achieve
strong performance on Pascal VOC2007 and MS-COCO datasets when compared to
recent SSL methods.",None,-1
Domain-aware Self-supervised Pre-training for Label-Efficient Meme Analysis,0.00577594,"Existing self-supervised learning strategies are constrained to either a
limited set of objectives or generic downstream tasks that predominantly target
uni-modal applications. This has isolated progress for imperative multi-modal
applications that are diverse in terms of complexity and domain-affinity, such
as meme analysis. Here, we introduce two self-supervised pre-training methods,
namely Ext-PIE-Net and MM-SimCLR that (i) employ off-the-shelf multi-modal
hate-speech data during pre-training and (ii) perform self-supervised learning
by incorporating multiple specialized pretext tasks, effectively catering to
the required complex multi-modal representation learning for meme analysis. We
experiment with different self-supervision strategies, including potential
variants that could help learn rich cross-modality representations and evaluate
using popular linear probing on the Hateful Memes task. The proposed solutions
strongly compete with the fully supervised baseline via label-efficient
training while distinctly outperforming them on all three tasks of the Memotion
challenge with 0.18%, 23.64%, and 0.93% performance gain, respectively.
Further, we demonstrate the generalizability of the proposed solutions by
reporting competitive performance on the HarMeme task. Finally, we empirically
establish the quality of the learned representations by analyzing task-specific
learning, using fewer labeled training samples, and arguing that the complexity
of the self-supervision strategy and downstream task at hand are correlated.
Our efforts highlight the requirement of better multi-modal self-supervision
methods involving specialized pretext tasks for efficient fine-tuning and
generalizable performance.",None,-1
Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding,0.060119,"Compositional generalization is a troubling blind spot for neural language
models. Recent efforts have presented techniques for improving a model's
ability to encode novel combinations of known inputs, but less work has focused
on generating novel combinations of known outputs. Here we focus on this latter
""decode-side"" form of generalization in the context of gSCAN, a synthetic
benchmark for compositional generalization in grounded language understanding.
We present Recursive Decoding (RD), a novel procedure for training and using
seq2seq models, targeted towards decode-side generalization. Rather than
generating an entire output sequence in one pass, models are trained to predict
one token at a time. Inputs (i.e., the external gSCAN environment) are then
incrementally updated based on predicted tokens, and re-encoded for the next
decoder time step. RD thus decomposes a complex, out-of-distribution sequence
generation task into a series of incremental predictions that each resemble
what the model has already seen during training. RD yields dramatic improvement
on two previously neglected generalization tasks in gSCAN. We provide analyses
to elucidate these gains over failure of a baseline, and then discuss
implications for generalization in naturalistic grounded language
understanding, and seq2seq more generally.",None,-1
Safe Policy Improvement Approaches on Discrete Markov Decision Processes,0.0519021,"Safe Policy Improvement (SPI) aims at provable guarantees that a learned
policy is at least approximately as good as a given baseline policy. Building
on SPI with Soft Baseline Bootstrapping (Soft-SPIBB) by Nadjahi et al., we
identify theoretical issues in their approach, provide a corrected theory, and
derive a new algorithm that is provably safe on finite Markov Decision
Processes (MDP). Additionally, we provide a heuristic algorithm that exhibits
the best performance among many state of the art SPI algorithms on two
different benchmarks. Furthermore, we introduce a taxonomy of SPI algorithms
and empirically show an interesting property of two classes of SPI algorithms:
while the mean performance of algorithms that incorporate the uncertainty as a
penalty on the action-value is higher, actively restricting the set of policies
more consistently produces good policies and is, thus, safer.",None,-1
Class-Incremental Learning via Knowledge Amalgamation,0.0163377,"Catastrophic forgetting has been a significant problem hindering the
deployment of deep learning algorithms in the continual learning setting.
Numerous methods have been proposed to address the catastrophic forgetting
problem where an agent loses its generalization power of old tasks while
learning new tasks. We put forward an alternative strategy to handle the
catastrophic forgetting with knowledge amalgamation (CFA), which learns a
student network from multiple heterogeneous teacher models specializing in
previous tasks and can be applied to current offline methods. The knowledge
amalgamation process is carried out in a single-head manner with only a
selected number of memorized samples and no annotations. The teachers and
students do not need to share the same network structure, allowing
heterogeneous tasks to be adapted to a compact or sparse data representation.
We compare our method with competitive baselines from different strategies,
demonstrating our approach's advantages.",https://github.com/Ivsucram/CFA,-1
Causal Discovery for Fairness,0.154549,"It is crucial to consider the social and ethical consequences of AI and ML
based decisions for the safe and acceptable use of these emerging technologies.
Fairness, in particular, guarantees that the ML decisions do not result in
discrimination against individuals or minorities. Identifying and measuring
reliably fairness/discrimination is better achieved using causality which
considers the causal relation, beyond mere association, between the sensitive
attribute (e.g. gender, race, religion, etc.) and the decision (e.g. job
hiring, loan granting, etc.). The big impediment to the use of causality to
address fairness, however, is the unavailability of the causal model (typically
represented as a causal graph). Existing causal approaches to fairness in the
literature do not address this problem and assume that the causal model is
available. In this paper, we do not make such assumption and we review the
major algorithms to discover causal relations from observable data. This study
focuses on causal discovery and its impact on fairness. In particular, we show
how different causal discovery approaches may result in different causal models
and, most importantly, how even slight differences between causal models can
have significant impact on fairness/discrimination conclusions. These results
are consolidated by empirical analysis using synthetic and standard fairness
benchmark datasets. The main goal of this study is to highlight the importance
of the causal discovery step to appropriately address fairness using causality.",None,-1
NeurMiPs: Neural Mixture of Planar Experts for View Synthesis,0.980607,"We present Neural Mixtures of Planar Experts (NeurMiPs), a novel planar-based
scene representation for modeling geometry and appearance. NeurMiPs leverages a
collection of local planar experts in 3D space as the scene representation.
Each planar expert consists of the parameters of the local rectangular shape
representing geometry and a neural radiance field modeling the color and
opacity. We render novel views by calculating ray-plane intersections and
composite output colors and densities at intersected points to the image.
NeurMiPs blends the efficiency of explicit mesh rendering and flexibility of
the neural radiance field. Experiments demonstrate superior performance and
speed of our proposed method, compared to other 3D representations in novel
view synthesis.",https://zhihao-lin.github.io/neurmips/,-1
BALF: Simple and Efficient Blur Aware Local Feature Detector,0.133288,"Local feature detection is a key ingredient of many image processing and
computer vision applications, such as visual odometry and localization. Most
existing algorithms focus on feature detection from a sharp image. They would
thus have degraded performance once the image is blurred, which could happen
easily under low-lighting conditions. To address this issue, we propose a
simple yet both efficient and effective keypoint detection method that is able
to accurately localize the salient keypoints in a blurred image. Our method
takes advantages of a novel multi-layer perceptron (MLP) based architecture
that significantly improve the detection repeatability for a blurred image. The
network is also light-weight and able to run in real-time, which enables its
deployment for time-constrained applications. Extensive experimental results
demonstrate that our detector is able to improve the detection repeatability
with blurred images, while keeping comparable performance as existing
state-of-the-art detectors for sharp images.",None,-1
Direct parsing to sentiment graphs,0.232799,"This paper demonstrates how a graph-based semantic parser can be applied to
the task of structured sentiment analysis, directly predicting sentiment graphs
from text. We advance the state of the art on 4 out of 5 standard benchmark
sets. We release the source code, models and predictions.",https://github.com/jerbarnes/direct_parsing_to_sent_graph,-1
Object-based active inference,0.0,"The world consists of objects: distinct entities possessing independent
properties and dynamics. For agents to interact with the world intelligently,
they must translate sensory inputs into the bound-together features that
describe each object. These object-based representations form a natural basis
for planning behavior. Active inference (AIF) is an influential unifying
account of perception and action, but existing AIF models have not leveraged
this important inductive bias. To remedy this, we introduce 'object-based
active inference' (OBAI), marrying AIF with recent deep object-based neural
networks. OBAI represents distinct objects with separate variational beliefs,
and uses selective attention to route inputs to their corresponding object
slots. Object representations are endowed with independent action-based
dynamics. The dynamics and generative model are learned from experience with a
simple environment (active multi-dSprites). We show that OBAI learns to
correctly segment the action-perturbed objects from video input, and to
manipulate these objects towards arbitrary goals.",None,-1
Collective Relevance Labeling for Passage Retrieval,0.0963019,"Deep learning for Information Retrieval (IR) requires a large amount of
high-quality query-document relevance labels, but such labels are inherently
sparse. Label smoothing redistributes some observed probability mass over
unobserved instances, often uniformly, uninformed of the true distribution. In
contrast, we propose knowledge distillation for informed labeling, without
incurring high computation overheads at evaluation time. Our contribution is
designing a simple but efficient teacher model which utilizes collective
knowledge, to outperform state-of-the-arts distilled from a more complex
teacher model. Specifically, we train up to x8 faster than the state-of-the-art
teacher, while distilling the rankings better. Our code is publicly available
at https://github.com/jihyukkim-nlp/CollectiveKD",https://github.com/jihyukkim-nlp/CollectiveKD,-1
Universal Adaptive Data Augmentation,0.0632893,"Existing automatic data augmentation (DA) methods either ignore updating DA's
parameters according to the target model's state during training or adopt
update strategies that are not effective enough. In this work, we design a
novel data augmentation strategy called ""Universal Adaptive Data Augmentation""
(UADA). Different from existing methods, UADA would adaptively update DA's
parameters according to the target model's gradient information during
training: given a pre-defined set of DA operations, we randomly decide types
and magnitudes of DA operations for every data batch during training, and
adaptively update DA's parameters along the gradient direction of the loss
concerning DA's parameters. In this way, UADA can increase the training loss of
the target networks, and the target networks would learn features from harder
samples to improve the generalization. Moreover, UADA is very general and can
be utilized in numerous tasks, e.g., image classification, semantic
segmentation and object detection. Extensive experiments with various models
are conducted on CIFAR-10, CIFAR-100, ImageNet, tiny-ImageNet, Cityscapes, and
VOC07+12 to prove the significant performance improvements brought by UADA.",None,-1
Improving Long Tailed Document-Level Relation Extraction via Easy Relation Augmentation and Contrastive Learning,0.0531447,"Towards real-world information extraction scenario, research of relation
extraction is advancing to document-level relation extraction(DocRE). Existing
approaches for DocRE aim to extract relation by encoding various information
sources in the long context by novel model architectures. However, the inherent
long-tailed distribution problem of DocRE is overlooked by prior work. We argue
that mitigating the long-tailed distribution problem is crucial for DocRE in
the real-world scenario. Motivated by the long-tailed distribution problem, we
propose an Easy Relation Augmentation(ERA) method for improving DocRE by
enhancing the performance of tailed relations. In addition, we further propose
a novel contrastive learning framework based on our ERA, i.e., ERACL, which can
further improve the model performance on tailed relations and achieve
competitive overall DocRE performance compared to the state-of-arts.",None,-1
Unsupervised Domain Adaptation for COVID-19 Information Service with Contrastive Adversarial Domain Mixup,0.030598,"In the real-world application of COVID-19 misinformation detection, a
fundamental challenge is the lack of the labeled COVID data to enable
supervised end-to-end training of the models, especially at the early stage of
the pandemic. To address this challenge, we propose an unsupervised domain
adaptation framework using contrastive learning and adversarial domain mixup to
transfer the knowledge from an existing source data domain to the target
COVID-19 data domain. In particular, to bridge the gap between the source
domain and the target domain, our method reduces a radial basis function (RBF)
based discrepancy between these two domains. Moreover, we leverage the power of
domain adversarial examples to establish an intermediate domain mixup, where
the latent representations of the input text from both domains could be mixed
during the training process. Extensive experiments on multiple real-world
datasets suggest that our method can effectively adapt misinformation detection
systems to the unseen COVID-19 target domain with significant improvements
compared to the state-of-the-art baselines.",None,-1
Self-Aware Personalized Federated Learning,0.056103,"In the context of personalized federated learning (FL), the critical
challenge is to balance local model improvement and global model tuning when
the personal and global objectives may not be exactly aligned. Inspired by
Bayesian hierarchical models, we develop a self-aware personalized FL method
where each client can automatically balance the training of its local personal
model and the global model that implicitly contributes to other clients'
training. Such a balance is derived from the inter-client and intra-client
uncertainty quantification. A larger inter-client variation implies more
personalization is needed. Correspondingly, our method uses uncertainty-driven
local training steps and aggregation rule instead of conventional local
fine-tuning and sample size-based aggregation. With experimental studies on
synthetic data, Amazon Alexa audio data, and public datasets such as MNIST,
FEMNIST, CIFAR10, and Sent140, we show that our proposed method can achieve
significantly improved personalization performance compared with the existing
counterparts.",https://github.com/CharlieDinh/,-1
Towards a Sentiment-Aware Conversational Agent,0.141821,"In this paper, we propose an end-to-end sentiment-aware conversational agent
based on two models: a reply sentiment prediction model, which leverages the
context of the dialogue to predict an appropriate sentiment for the agent to
express in its reply; and a text generation model, which is conditioned on the
predicted sentiment and the context of the dialogue, to produce a reply that is
both context and sentiment appropriate. Additionally, we propose to use a
sentiment classification model to evaluate the sentiment expressed by the agent
during the development of the model. This allows us to evaluate the agent in an
automatic way. Both automatic and human evaluation results show that explicitly
guiding the text generation model with a pre-defined set of sentences leads to
clear improvements, both regarding the expressed sentiment and the quality of
the generated text.",None,-1
Large Language Models and the Reverse Turing Test,0.514337,"Large Language Models (LLMs) have been transformative. They are pre-trained
foundational models that are self-supervised and can be adapted with fine
tuning to a wide range of natural language tasks, each of which previously
would have required a separate network model. This is one step closer to the
extraordinary versatility of human language. GPT-3 and more recently LaMDA can
carry on dialogs with humans on many topics after minimal priming with a few
examples. However, there has been a wide range of reactions and debate on
whether these LLMs understand what they are saying or exhibit signs of
intelligence. This high variance is exhibited in three interviews with LLMs
reaching wildly different conclusions. A new possibility was uncovered that
could explain this divergence. What appears to be intelligence in LLMs may in
fact be a mirror that reflects the intelligence of the interviewer, a
remarkable twist that could be considered a Reverse Turing Test. If so, then by
studying interviews we may be learning more about the intelligence and beliefs
of the interviewer than the intelligence of the LLMs. As LLMs become more
capable they may transform the way we interact with machines and how they
interact with each other. Increasingly, LLMs are being coupled with
sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A
road map for achieving artificial general autonomy is outlined with seven major
improvements inspired by brain systems. LLMs could be used to uncover new
insights into brain function by downloading brain data during natural
behaviors.",None,-1
Forest and Water Bodies Segmentation Through Satellite Images Using U-Net,0.116686,"Global environment monitoring is a task that requires additional attention in
the contemporary rapid climate change environment. This includes monitoring the
rate of deforestation and areas affected by flooding. Satellite imaging has
greatly helped monitor the earth, and deep learning techniques have helped to
automate this monitoring process. This paper proposes a solution for observing
the area covered by the forest and water. To achieve this task UNet model has
been proposed, which is an image segmentation model. The model achieved a
validation accuracy of 82.55% and 82.92% for the segmentation of areas covered
by forest and water, respectively.",None,-1
Fine-tuning Image Transformers using Learnable Memory,0.0964547,"In this paper we propose augmenting Vision Transformer models with learnable
memory tokens. Our approach allows the model to adapt to new tasks, using few
parameters, while optionally preserving its capabilities on previously learned
tasks. At each layer we introduce a set of learnable embedding vectors that
provide contextual information useful for specific datasets. We call these
""memory tokens"". We show that augmenting a model with just a handful of such
tokens per layer significantly improves accuracy when compared to conventional
head-only fine-tuning, and performs only slightly below the significantly more
expensive full fine-tuning. We then propose an attention-masking approach that
enables extension to new downstream tasks, with a computation reuse. In this
setup in addition to being parameters efficient, models can execute both old
and new tasks as a part of single inference at a small incremental cost.",None,-1
ICANet: A Method of Short Video Emotion Recognition Driven by Multimodal Data,0.0609202,"With the fast development of artificial intelligence and short videos,
emotion recognition in short videos has become one of the most important
research topics in human-computer interaction. At present, most emotion
recognition methods still stay in a single modality. However, in daily life,
human beings will usually disguise their real emotions, which leads to the
problem that the accuracy of single modal emotion recognition is relatively
terrible. Moreover, it is not easy to distinguish similar emotions. Therefore,
we propose a new approach denoted as ICANet to achieve multimodal short video
emotion recognition by employing three different modalities of audio, video and
optical flow, making up for the lack of a single modality and then improving
the accuracy of emotion recognition in short videos. ICANet has a better
accuracy of 80.77% on the IEMOCAP benchmark, exceeding the SOTA methods by
15.89%.",None,-1
ViGAT: Bottom-up event recognition and explanation in video using factorized graph attention network,0.0585283,"In this paper a pure-attention bottom-up approach, called ViGAT, that
utilizes an object detector together with a Vision Transformer (ViT) backbone
network to derive object and frame features, and a head network to process
these features for the task of event recognition and explanation in video, is
proposed. The ViGAT head consists of graph attention network (GAT) blocks
factorized along the spatial and temporal dimensions in order to capture
effectively both local and long-term dependencies between objects or frames.
Moreover, using the weighted in-degrees (WiDs) derived from the adjacency
matrices at the various GAT blocks, we show that the proposed architecture can
identify the most salient objects and frames that explain the decision of the
network. A comprehensive evaluation study is performed, demonstrating that the
proposed approach provides state-of-the-art results on three large, publicly
available video datasets (FCVID, Mini-Kinetics, ActivityNet).",https://github.com/bmezaris/ViGAT,-1
LitMC-BERT: transformer-based multi-label classification of biomedical literature with an application on COVID-19 literature curation,0.0353835,"The rapid growth of biomedical literature poses a significant challenge for
curation and interpretation. This has become more evident during the COVID-19
pandemic. LitCovid, a literature database of COVID-19 related papers in PubMed,
has accumulated over 180,000 articles with millions of accesses. Approximately
10,000 new articles are added to LitCovid every month. A main curation task in
LitCovid is topic annotation where an article is assigned with up to eight
topics, e.g., Treatment and Diagnosis. The annotated topics have been widely
used both in LitCovid (e.g., accounting for ~18% of total uses) and downstream
studies such as network generation. However, it has been a primary curation
bottleneck due to the nature of the task and the rapid literature growth. This
study proposes LITMC-BERT, a transformer-based multi-label classification
method in biomedical literature. It uses a shared transformer backbone for all
the labels while also captures label-specific features and the correlations
between label pairs. We compare LITMC-BERT with three baseline models on two
datasets. Its micro-F1 and instance-based F1 are 5% and 4% higher than the
current best results, respectively, and only requires ~18% of the inference
time than the Binary BERT baseline. The related datasets and models are
available via https://github.com/ncbi/ml-transformer.",https://github.com/ncbi/ml-transformer,-1
"Human Detection of Political Speech Deepfakes across Transcripts, Audio, and Video",0.154757,"Recent advances in technology for hyper-realistic visual and audio effects
provoke the concern that deepfake videos of political speeches will soon be
indistinguishable from authentic video recordings. The conventional wisdom in
communication theory predicts people will fall for fake news more often when
the same version of a story is presented as a video versus text. We conduct 5
pre-registered randomized experiments with 2,215 participants to evaluate how
accurately humans distinguish real political speeches from fabrications across
base rates of misinformation, audio sources, question framings, and media
modalities. We find base rates of misinformation minimally influence
discernment and deepfakes with audio produced by the state-of-the-art
text-to-speech algorithms are harder to discern than the same deepfakes with
voice actor audio. Moreover across all experiments, we find audio and visual
information enables more accurate discernment than text alone: human
discernment relies more on how something is said, the audio-visual cues, than
what is said, the speech content.",https://researchbox.org/1723&PEER_REVIEW_passcode=EGVULE,-1
The 2021 Urdu Fake News Detection Task using Supervised Machine Learning and Feature Combinations,0.0912145,"This paper presents the system description submitted at the FIRE Shared Task:
""The 2021 Fake News Detection in the Urdu Language"". This challenge aims at
automatically identifying Fake news written in Urdu. Our submitted results
ranked fifth in the competition. However, after the result declaration of the
competition, we managed to attain even better results than the submitted
results. The best F1 Macro score achieved by one of our models is 0.6674,
higher than the second-best score in the competition. The result is achieved on
Support Vector Machines (polynomial kernel degree 1) with stopwords removed,
lemmatization applied, and selecting the 20K best features out of 1.557 million
features in total (which were produced by Word n-grams n=1,2,3,4 and Char
n-grams n=2,3,4,5,6). The code is made available for reproducibility.",https://github.com/humsha/UrduFakeDetection2021,-1
Fast Light-Weight Near-Field Photometric Stereo,0.273914,"We introduce the first end-to-end learning-based solution to near-field
Photometric Stereo (PS), where the light sources are close to the object of
interest. This setup is especially useful for reconstructing large immobile
objects. Our method is fast, producing a mesh from 52 512$\times$384 resolution
images in about 1 second on a commodity GPU, thus potentially unlocking several
AR/VR applications. Existing approaches rely on optimization coupled with a
far-field PS network operating on pixels or small patches. Using optimization
makes these approaches slow and memory intensive (requiring 17GB GPU and 27GB
of CPU memory) while using only pixels or patches makes them highly susceptible
to noise and calibration errors. To address these issues, we develop a
recursive multi-resolution scheme to estimate surface normal and depth maps of
the whole image at each step. The predicted depth map at each scale is then
used to estimate `per-pixel lighting' for the next scale. This design makes our
approach almost 45$\times$ faster and 2$^{\circ}$ more accurate (11.3$^{\circ}$
vs. 13.3$^{\circ}$ Mean Angular Error) than the state-of-the-art near-field PS
reconstruction technique, which uses iterative optimization.",None,-1
Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks,0.0428278,"Autonomous agents are able to draw on a wide variety of potential sources of
task knowledge; however current approaches invariably focus on only one or two.
Here we investigate the challenges and impact of exploiting diverse knowledge
sources to learn online, in one-shot, new tasks for a simulated office mobile
robot. The resulting agent, developed in the Soar cognitive architecture, uses
the following sources of domain and task knowledge: interaction with the
environment, task execution and search knowledge, human natural language
instruction, and responses retrieved from a large language model (GPT-3). We
explore the distinct contributions of these knowledge sources and evaluate the
performance of different combinations in terms of learning correct task
knowledge and human workload. Results show that an agent's online integration
of diverse knowledge sources improves one-shot task learning overall, reducing
human feedback needed for rapid and reliable task learning.",None,-1
Relational Future Captioning Model for Explaining Likely Collisions in Daily Tasks,0.0521464,"Domestic service robots that support daily tasks are a promising solution for
elderly or disabled people. It is crucial for domestic service robots to
explain the collision risk before they perform actions. In this paper, our aim
is to generate a caption about a future event. We propose the Relational Future
Captioning Model (RFCM), a crossmodal language generation model for the future
captioning task. The RFCM has the Relational Self-Attention Encoder to extract
the relationships between events more effectively than the conventional
self-attention in transformers. We conducted comparison experiments, and the
results show the RFCM outperforms a baseline method on two datasets.",https://github.com/keio-smilab22/RelationalFutureCaptioningModel,-1
Training Trajectories of Language Models Across Scales,0.262757,"Scaling up language models has led to unprecedented performance gains, but
little is understood about how the training dynamics change as models get
larger. How do language models of different sizes learn during pre-training?
Why do larger language models demonstrate more desirable behaviors? In this
paper, we analyze the intermediate training checkpoints of differently sized
OPT models (Zhang et al.,2022)--from 125M to 175B parameters--on next-token
prediction, sequence-level generation, and downstream tasks. We find that 1) at
a given perplexity and independent of model sizes, a similar subset of training
tokens see the most significant reduction in loss, with the rest stagnating or
showing double-descent behavior; 2) early in training, all models learn to
reduce the perplexity of grammatical sequences that contain hallucinations,
with small models halting at this suboptimal distribution and larger ones
eventually learning to assign these sequences lower probabilities; 3)
perplexity is a strong predictor of in-context learning performance on 74
multiple-choice tasks from BIG-Bench, and this holds independent of the model
size. Together, these results show that perplexity is more predictive of model
behaviors than model size or training computation.",https://github.com/xiamengzhou/training_trajectory_analysis,-1
LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings,0.0295944,"Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph
structure and text-rich entity/relation information. Text-based KG embeddings
can represent entities by encoding descriptions with pre-trained language
models, but no open-sourced library is specifically designed for KGs with PLMs
at present. In this paper, we present LambdaKG, a library for KGE that equips
with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and
supports various tasks (e.g., knowledge graph completion, question answering,
recommendation, and knowledge probing). LambdaKG is publicly open-sourced at
https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at
http://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.",https://github.com/zjunlp/PromptKG/tree/,-1
Proper Reuse of Image Classification Features Improves Object Detection,0.172031,"A common practice in transfer learning is to initialize the downstream model
weights by pre-training on a data-abundant upstream task. In object detection
specifically, the feature backbone is typically initialized with Imagenet
classifier weights and fine-tuned on the object detection task. Recent works
show this is not strictly necessary under longer training regimes and provide
recipes for training the backbone from scratch. We investigate the opposite
direction of this end-to-end training trend: we show that an extreme form of
knowledge preservation -- freezing the classifier-initialized backbone --
consistently improves many different detection models, and leads to
considerable resource savings. We hypothesize and corroborate experimentally
that the remaining detector components capacity and structure is a crucial
factor in leveraging the frozen backbone. Immediate applications of our
findings include performance improvements on hard cases like detection of
long-tail object classes and computational and memory resource savings that
contribute to making the field more accessible to researchers with access to
fewer computational resources.",https://github.com/tensorflow/models/blob/master/official/projects/backbone_reuse/README.md,-1
Are Current Task-oriented Dialogue Systems Able to Satisfy Impolite Users?,0.0450454,"Task-oriented dialogue (TOD) systems have assisted users on many tasks,
including ticket booking and service inquiries. While existing TOD systems have
shown promising performance in serving customer needs, these systems mostly
assume that users would interact with the dialogue agent politely. This
assumption is unrealistic as impatient or frustrated customers may also
interact with TOD systems impolitely. This paper aims to address this research
gap by investigating impolite users' effects on TOD systems. Specifically, we
constructed an impolite dialogue corpus and conducted extensive experiments to
evaluate the state-of-the-art TOD systems on our impolite dialogue corpus. Our
experimental results show that existing TOD systems are unable to handle
impolite user utterances. We also present a data augmentation method to improve
TOD performance in impolite dialogues. Nevertheless, handling impolite
dialogues remains a very challenging research task. We hope by releasing the
impolite dialogue corpus and establishing the benchmark evaluations, more
researchers are encouraged to investigate this new challenging research task.",None,-1
LEBP -- Language Expectation & Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents,0.666676,"People always desire an embodied agent that can perform a task by
understanding language instruction. Moreover, they also want to monitor and
expect agents to understand commands the way they expected. But, how to build
such an embodied agent is still unclear. Recently, people can explore this
problem with the Vision-and-Language Interaction benchmark ALFRED, which
requires an agent to perform complicated daily household tasks following
natural language instructions in unseen scenes. In this paper, we propose LEBP
-- Language Expectation and Binding Policy Module to tackle the ALFRED. The
LEBP contains a two-stream process: 1) It first conducts a language expectation
module to generate an expectation describing how to perform tasks by
understanding the language instruction. The expectation consists of a sequence
of sub-steps for the task (e.g., Pick an apple). The expectation allows people
to access and check the understanding results of instructions before the agent
takes actual actions, in case the task might go wrong. 2) Then, it uses the
binding policy module to bind sub-steps in expectation to actual actions to
specific scenarios. Actual actions include navigation and object manipulation.
Experimental results suggest our approach achieves comparable performance to
currently published SOTA methods and can avoid large decay from seen scenarios
to unseen scenarios.",None,-1
Privileged Attribution Constrained Deep Networks for Facial Expression Recognition,0.301583,"Facial Expression Recognition (FER) is crucial in many research domains
because it enables machines to better understand human behaviours. FER methods
face the problems of relatively small datasets and noisy data that don't allow
classical networks to generalize well. To alleviate these issues, we guide the
model to concentrate on specific facial areas like the eyes, the mouth or the
eyebrows, which we argue are decisive to recognise facial expressions. We
propose the Privileged Attribution Loss (PAL), a method that directs the
attention of the model towards the most salient facial regions by encouraging
its attribution maps to correspond to a heatmap formed by facial landmarks.
Furthermore, we introduce several channel strategies that allow the model to
have more degrees of freedom. The proposed method is independent of the
backbone architecture and doesn't need additional semantic information at test
time. Finally, experimental results show that the proposed PAL method
outperforms current state-of-the-art methods on both RAF-DB and AffectNet.",None,-1
Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers,0.0171486,"Recently, RNN-Transducers have achieved remarkable results on various
automatic speech recognition tasks. However, lattice-free sequence
discriminative training methods, which obtain superior performance in hybrid
models, are rarely investigated in RNN-Transducers. In this work, we propose
three lattice-free training objectives, namely lattice-free maximum mutual
information, lattice-free segment-level minimum Bayes risk, and lattice-free
minimum Bayes risk, which are used for the final posterior output of the
phoneme-based neural transducer with a limited context dependency. Compared to
criteria using N-best lists, lattice-free methods eliminate the decoding step
for hypotheses generation during training, which leads to more efficient
training. Experimental results show that lattice-free methods gain up to 6.5%
relative improvement in word error rate compared to a sequence-level
cross-entropy trained model. Compared to the N-best-list based minimum Bayes
risk objectives, lattice-free methods gain 40% - 70% relative training time
speedup with a small degradation in performance.",None,-1
BIOWISH: Biometric Recognition using Wearable Inertial Sensors detecting Heart Activity,0.0223578,"Wearable devices are increasingly used, thanks to the wide set of
applications that can be deployed exploiting their ability to monitor physical
activity and health-related parameters. Their usage has been recently proposed
to perform biometric recognition, leveraging on the uniqueness of the recorded
traits to generate discriminative identifiers. Most of the studies conducted on
this topic have considered signals derived from cardiac activity, detecting it
mainly using electrical measurements thorugh electrocardiography, or optical
recordings employing photoplethysmography. In this paper we instead propose a
BIOmetric recognition approach using Wearable Inertial Sensors detecting Heart
activity (BIOWISH). In more detail, we investigate the feasibility of
exploiting mechanical measurements obtained through seismocardiography and
gyrocardiography to recognize a person. Several feature extractors and
classifiers, including deep learning techniques relying on transfer learning
and siamese training, are employed to derive distinctive characteristics from
the considered signals, and differentiate between legitimate and impostor
subjects. An multi-session database, comprising acquisitions taken from
subjects performing different activities, is employed to perform experimental
tests simulating a verification system. The obtained results testify that
identifiers derived from measurements of chest vibrations, collected by
wearable inertial sensors, could be employed to guarantee high recognition
performance, even when considering short-time recordings.",None,-1
P-Transformer: Towards Better Document-to-Document Neural Machine Translation,0.0110973,"Directly training a document-to-document (Doc2Doc) neural machine translation
(NMT) via Transformer from scratch, especially on small datasets usually fails
to converge. Our dedicated probing tasks show that 1) both the absolute
position and relative position information gets gradually weakened or even
vanished once it reaches the upper encoder layers, and 2) the vanishing of
absolute position information in encoder output causes the training failure of
Doc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer
(P-Transformer) to enhance both the absolute and relative position information
in both self-attention and cross-attention. Specifically, we integrate absolute
positional information, i.e., position embeddings, into the query-key pairs
both in self-attention and cross-attention through a simple yet effective
addition operation. Moreover, we also integrate relative position encoding in
self-attention. The proposed P-Transformer utilizes sinusoidal position
encoding and does not require any task-specified position embedding, segment
embedding, or attention mechanism. Through the above methods, we build a
Doc2Doc NMT model with P-Transformer, which ingests the source document and
completely generates the target document in a sequence-to-sequence (seq2seq)
way. In addition, P-Transformer can be applied to seq2seq-based
document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)
translation. Extensive experimental results of Doc2Doc NMT show that
P-Transformer significantly outperforms strong baselines on widely-used 9
document-level datasets in 7 language pairs, covering small-, middle-, and
large-scales, and achieves a new state-of-the-art. Experimentation on discourse
phenomena shows that our Doc2Doc NMT models improve the translation quality in
both BLEU and discourse coherence. We make our code available on Github.",None,-1
Twitter-Demographer: A Flow-based Tool to Enrich Twitter Data,0.0130023,"Twitter data have become essential to Natural Language Processing (NLP) and
social science research, driving various scientific discoveries in recent
years. However, the textual data alone are often not enough to conduct studies:
especially social scientists need more variables to perform their analysis and
control for various factors. How we augment this information, such as users'
location, age, or tweet sentiment, has ramifications for anonymity and
reproducibility, and requires dedicated effort. This paper describes
Twitter-Demographer, a simple, flow-based tool to enrich Twitter data with
additional information about tweets and users. Twitter-Demographer is aimed at
NLP practitioners and (computational) social scientists who want to enrich
their datasets with aggregated information, facilitating reproducibility, and
providing algorithmic privacy-by-design measures for pseudo-anonymity. We
discuss our design choices, inspired by the flow-based programming paradigm, to
use black-box components that can easily be chained together and extended. We
also analyze the ethical issues related to the use of this tool, and the
built-in measures to facilitate pseudo-anonymity.",https://github.com/MilaNLProc/twitter-demographer,-1
GUSOT: Green and Unsupervised Single Object Tracking for Long Video Sequences,0.0119375,"Supervised and unsupervised deep trackers that rely on deep learning
technologies are popular in recent years. Yet, they demand high computational
complexity and a high memory cost. A green unsupervised single-object tracker,
called GUSOT, that aims at object tracking for long videos under a
resource-constrained environment is proposed in this work. Built upon a
baseline tracker, UHP-SOT++, which works well for short-term tracking, GUSOT
contains two additional new modules: 1) lost object recovery, and 2)
color-saliency-based shape proposal. They help resolve the tracking loss
problem and offer a more flexible object proposal, respectively. Thus, they
enable GUSOT to achieve higher tracking accuracy in the long run. We conduct
experiments on the large-scale dataset LaSOT with long video sequences, and
show that GUSOT offers a lightweight high-performance tracking solution that
finds applications in mobile and edge computing platforms.",None,-1
Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models,0.0636937,"With many real-world applications of Natural Language Processing (NLP)
comprising of long texts, there has been a rise in NLP benchmarks that measure
the accuracy of models that can handle longer input sequences. However, these
benchmarks do not consider the trade-offs between accuracy, speed, and power
consumption as input sizes or model sizes are varied. In this work, we perform
a systematic study of this accuracy vs. efficiency trade-off on two widely used
long-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during
fine-tuning and inference on four datasets from the SCROLLS benchmark. To study
how this trade-off differs across hyperparameter settings, we compare the
models across four sequence lengths (1024, 2048, 3072, 4096) and two model
sizes (base and large) under a fixed resource budget. We find that LED
consistently achieves better accuracy at lower energy costs than Big Bird. For
summarization, we find that increasing model size is more energy efficient than
increasing sequence length for higher accuracy. However, this comes at the cost
of a large drop in inference speed. For question answering, we find that
smaller models are both more efficient and more accurate due to the larger
training batch sizes possible under a fixed resource budget.",https://github.com/phyllisayk/nlp-efficiency-tradeoff,-1
Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints,0.0211449,"Processing information locked within clinical health records is a challenging
task that remains an active area of research in biomedical NLP. In this work,
we evaluate a broad set of machine learning techniques ranging from simple RNNs
to specialised transformers such as BioBERT on a dataset containing clinical
notes along with a set of annotations indicating whether a sample is
cancer-related or not.
  Furthermore, we specifically employ efficient fine-tuning methods from NLP,
namely, bottleneck adapters and prompt tuning, to adapt the models to our
specialised task. Our evaluations suggest that fine-tuning a frozen BERT model
pre-trained on natural language and with bottleneck adapters outperforms all
other strategies, including full fine-tuning of the specialised BioBERT model.
Based on our findings, we suggest that using bottleneck adapters in
low-resource situations with limited access to labelled data or processing
capacity could be a viable strategy in biomedical text mining. The code used in
the experiments are going to be made available at
https://github.com/omidrohanian/bottleneck-adapters.",https://github.com/omidrohanian/bottleneck-adapters,-1
TVLT: Textless Vision-Language Transformer,0.831402,"In this work, we present the Textless Vision-Language Transformer (TVLT),
where homogeneous transformer blocks take raw visual and audio inputs for
vision-and-language representation learning with minimal modality-specific
design, and do not use text-specific modules such as tokenization or automatic
speech recognition (ASR). TVLT is trained by reconstructing masked patches of
continuous video frames and audio spectrograms (masked autoencoding) and
contrastive modeling to align video and audio. TVLT attains performance
comparable to its text-based counterpart on various multimodal tasks, such as
visual question answering, image retrieval, video retrieval, and multimodal
sentiment analysis, with 28x faster inference speed and only 1/3 of the
parameters. Our findings suggest the possibility of learning compact and
efficient visual-linguistic representations from low-level visual and audio
signals without assuming the prior existence of text. Our code and checkpoints
are available at: https://github.com/zinengtang/TVLT",https://github.com/zinengtang/TVLT,-1
Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers,0.27959,"When a clinician refers a patient for an imaging exam, they include the
reason (e.g. relevant patient history, suspected disease) in the scan request;
this appears as the indication field in the radiology report. The
interpretation and reporting of the image are substantially influenced by this
request text, steering the radiologist to focus on particular aspects of the
image. We use the indication field to drive better image classification, by
taking a transformer network which is unimodally pre-trained on text (BERT) and
fine-tuning it for multimodal classification of a dual image-text input. We
evaluate the method on the MIMIC-CXR dataset, and present ablation studies to
investigate the effect of the indication field on the classification
performance. The experimental results show our approach achieves 87.8 average
micro AUROC, outperforming the state-of-the-art methods for unimodal (84.4) and
multimodal (86.0) classification. Our code is available at
https://github.com/jacenkow/mmbt.",https://github.com/jacenkow/mmbt,-1
HarmoF0: Logarithmic Scale Dilated Convolution For Pitch Estimation,0.276513,"Sounds, especially music, contain various harmonic components scattered in
the frequency dimension. It is difficult for normal convolutional neural
networks to observe these overtones. This paper introduces a multiple rates
dilated causal convolution (MRDC-Conv) method to capture the harmonic structure
in logarithmic scale spectrograms efficiently. The harmonic is helpful for
pitch estimation, which is important for many sound processing applications. We
propose HarmoF0, a fully convolutional network, to evaluate the MRDC-Conv and
other dilated convolutions in pitch estimation. The results show that this
model outperforms the DeepF0, yields state-of-the-art performance in three
datasets, and simultaneously reduces more than 90% parameters. We also find
that it has stronger noise resistance and fewer octave errors. The code and
pre-trained model are available at https://github.com/WX-Wei/HarmoF0.",https://github.com/WX-Wei/HarmoF0,-1
Neural Network Compression of ACAS Xu Early Prototype is Unsafe: Closed-Loop Verification through Quantized State Backreachability,0.044354,"ACAS Xu is an air-to-air collision avoidance system designed for unmanned
aircraft that issues horizontal turn advisories to avoid an intruder aircraft.
Due the use of a large lookup table in the design, a neural network compression
of the policy was proposed. Analysis of this system has spurred a significant
body of research in the formal methods community on neural network
verification. While many powerful methods have been developed, most work
focuses on open-loop properties of the networks, rather than the main point of
the system -- collision avoidance -- which requires closed-loop analysis.
  In this work, we develop a technique to verify a closed-loop approximation of
the system using state quantization and backreachability. We use favorable
assumptions for the analysis -- perfect sensor information, instant following
of advisories, ideal aircraft maneuvers and an intruder that only flies
straight. When the method fails to prove the system is safe, we refine the
quantization parameters until generating counterexamples where the original
(non-quantized) system also has collisions.",https://github.com/stanleybak/quantized_nn_backreach/releases/tag/NFM2022_submitted,-1
Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling,0.142028,"This paper studies multi-task training of retrieval-augmented generation
models for knowledge-intensive tasks. We propose to clean the training set by
utilizing a distinct property of knowledge-intensive generation: The connection
of query-answer pairs to items in the knowledge base. We filter training
examples via a threshold of confidence on the relevance labels, whether a pair
is answerable by the knowledge base or not. We train a single Fusion-in-Decoder
(FiD) generator on seven combined tasks of the KILT benchmark. The experimental
results suggest that our simple yet effective approach substantially improves
competitive baselines on two strongly imbalanced tasks; and shows either
smaller improvements or no significant regression on the remaining tasks.
Furthermore, we demonstrate our multi-task training with relevance label
sampling scales well with increased model capacity and achieves
state-of-the-art results in five out of seven KILT tasks.",None,-1
Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization,0.388926,"Current 3D scene stylization methods transfer textures and colors as styles
using arbitrary style references, lacking meaningful semantic correspondences.
We introduce Reference-Based Non-Photorealistic Radiance Fields (Ref-NPR) to
address this limitation. This controllable method stylizes a 3D scene using
radiance fields with a single stylized 2D view as a reference. We propose a ray
registration process based on the stylized reference view to obtain pseudo-ray
supervision in novel views. Then we exploit semantic correspondences in content
images to fill occluded regions with perceptually similar styles, resulting in
non-photorealistic and continuous novel view sequences. Our experimental
results demonstrate that Ref-NPR outperforms existing scene and video
stylization methods regarding visual quality and semantic correspondence. The
code and data are publicly available on the project page at
https://ref-npr.github.io.",https://ref-npr.github.io,-1
Formalizing the Problem of Side Effect Regularization,0.0122367,"AI objectives are often hard to specify properly. Some approaches tackle this
problem by regularizing the AI's side effects: Agents must weigh off ""how much
of a mess they make"" with an imperfectly specified proxy objective. We propose
a formal criterion for side effect regularization via the assistance game
framework. In these games, the agent solves a partially observable Markov
decision process (POMDP) representing its uncertainty about the objective
function it should optimize. We consider the setting where the true objective
is revealed to the agent at a later time step. We show that this POMDP is
solved by trading off the proxy reward with the agent's ability to achieve a
range of future tasks. We empirically demonstrate the reasonableness of our
problem formalization via ground-truth evaluation in two gridworld
environments.",https://github.com/aseembits93/attainable-utility-preservation,-1
Panchromatic and Multispectral Image Fusion via Alternating Reverse Filtering Network,0.242112,"Panchromatic (PAN) and multi-spectral (MS) image fusion, named
Pan-sharpening, refers to super-resolve the low-resolution (LR) multi-spectral
(MS) images in the spatial domain to generate the expected high-resolution (HR)
MS images, conditioning on the corresponding high-resolution PAN images. In
this paper, we present a simple yet effective \textit{alternating reverse
filtering network} for pan-sharpening. Inspired by the classical reverse
filtering that reverses images to the status before filtering, we formulate
pan-sharpening as an alternately iterative reverse filtering process, which
fuses LR MS and HR MS in an interpretable manner. Different from existing
model-driven methods that require well-designed priors and degradation
assumptions, the reverse filtering process avoids the dependency on pre-defined
exact priors. To guarantee the stability and convergence of the iterative
process via contraction mapping on a metric space, we develop the learnable
multi-scale Gaussian kernel module, instead of using specific filters. We
demonstrate the theoretical feasibility of such formulations. Extensive
experiments on diverse scenes to thoroughly verify the performance of our
method, significantly outperforming the state of the arts.",None,-1
Post-Training Dialogue Summarization using Pseudo-Paraphrasing,0.0617419,"Previous dialogue summarization techniques adapt large language models
pretrained on the narrative text by injecting dialogue-specific features into
the models. These features either require additional knowledge to recognize or
make the resulting models harder to tune. To bridge the format gap between
dialogues and narrative summaries in dialogue summarization tasks, we propose
to post-train pretrained language models (PLMs) to rephrase from dialogue to
narratives. After that, the model is fine-tuned for dialogue summarization as
usual. Comprehensive experiments show that our approach significantly improves
vanilla PLMs on dialogue summarization and outperforms other SOTA models by the
summary quality and implementation costs.",https://github.com/JiaQiSJTU/DialSent-PGG,-1
Piloting Diversity and Inclusion Workshops in Artificial Intelligence and Robotics for Children,0.12566,"In this paper, we present preliminary work from a pilot workshop that aimed
to promote diversity and inclusion for fundamentals of Artificial Intelligence
and Robotics for Children (air4children) in the context of developing
countries. Considering the scarcity of funding and the little to none
availability of specialised professionals to teach AI and robotics in
developing countries, we present resources based on free open-source hardware
and software, open educational resources, and alternative education programs.
That said, the contribution of this work is the pilot workshop of four lessons
that promote diversity and inclusion on teaching AI and Robotics for children
to a small gender-balanced sample of 14 children of an average age of 7.64
years old. We conclude that participant, instructors, coordinators and parents
engaged well in the pilot workshop noting the various challenges of having the
right resources for the workshops in developing countries and posing future
work. The resources to reproduce this work are available at
https://github.com/air4children/hri2022.",https://github.com/air4children/hri2022,-1
GAM(e) changer or not? An evaluation of interpretable machine learning models based on additive model constraints,0.0358694,"The number of information systems (IS) studies dealing with explainable
artificial intelligence (XAI) is currently exploding as the field demands more
transparency about the internal decision logic of machine learning (ML) models.
However, most techniques subsumed under XAI provide post-hoc-analytical
explanations, which have to be considered with caution as they only use
approximations of the underlying ML model. Therefore, our paper investigates a
series of intrinsically interpretable ML models and discusses their suitability
for the IS community. More specifically, our focus is on advanced extensions of
generalized additive models (GAM) in which predictors are modeled independently
in a non-linear way to generate shape functions that can capture arbitrary
patterns but remain fully interpretable. In our study, we evaluate the
prediction qualities of five GAMs as compared to six traditional ML models and
assess their visual outputs for model interpretability. On this basis, we
investigate their merits and limitations and derive design implications for
further improvements.",https://github.com/fau-is/gam_comparison,-1
Low-Resource Multilingual and Zero-Shot Multispeaker TTS,0.253394,"While neural methods for text-to-speech (TTS) have shown great advances in
modeling multiple speakers, even in zero-shot settings, the amount of data
needed for those approaches is generally not feasible for the vast majority of
the world's over 6,000 spoken languages. In this work, we bring together the
tasks of zero-shot voice cloning and multilingual low-resource TTS. Using the
language agnostic meta learning (LAML) procedure and modifications to a TTS
encoder, we show that it is possible for a system to learn speaking a new
language using just 5 minutes of training data while retaining the ability to
infer the voice of even unseen speakers in the newly learned language. We show
the success of our proposed approach in terms of intelligibility, naturalness
and similarity to target speaker using objective metrics as well as human
studies and provide our code and trained models open source.",https://github.com/DigitalPhonetics/,-1
FedQAS: Privacy-aware machine reading comprehension with federated learning,0.126183,"Machine reading comprehension (MRC) of text data is one important task in
Natural Language Understanding. It is a complex NLP problem with a lot of
ongoing research fueled by the release of the Stanford Question Answering
Dataset (SQuAD) and Conversational Question Answering (CoQA). It is considered
to be an effort to teach computers how to ""understand"" a text, and then to be
able to answer questions about it using deep learning. However, until now
large-scale training on private text data and knowledge sharing has been
missing for this NLP task. Hence, we present FedQAS, a privacy-preserving
machine reading system capable of leveraging large-scale private data without
the need to pool those datasets in a central location. The proposed approach
combines transformer models and federated learning technologies. The system is
developed using the FEDn framework and deployed as a proof-of-concept alliance
initiative. FedQAS is flexible, language-agnostic, and allows intuitive
participation and execution of local model training. In addition, we present
the architecture and implementation of the system, as well as provide a
reference evaluation based on the SQUAD dataset, to showcase how it overcomes
data privacy issues and enables knowledge sharing between alliance members in a
Federated learning setting.",https://github.com/aitmlouk/FEDn-client-FedQAS-tf.git,-1
TAS-NIR: A VIS+NIR Dataset for Fine-grained Semantic Segmentation in Unstructured Outdoor Environments,0.0258145,"Vegetation Indices based on paired images of the visible color spectrum (VIS)
and near infrared spectrum (NIR) have been widely used in remote sensing
applications. These vegetation indices are extended for their application in
autonomous driving in unstructured outdoor environments. In this domain we can
combine traditional vegetation indices like the Normalized Difference
Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI) with Convolutional
Neural Networks (CNNs) pre-trained on available VIS datasets. By laying a focus
on learning calibrated CNN outputs, we can provide an approach to fuse known
hand-crafted image features with CNN predictions for different domains as well.
The method is evaluated on a VIS+NIR dataset of semantically annotated images
in unstructured outdoor environments. The dataset is available at
mucar3.de/iros2022-ppniv-tas-nir.",None,-1
Spatio-Temporal Deformable Attention Network for Video Deblurring,0.4301,"The key success factor of the video deblurring methods is to compensate for
the blurry pixels of the mid-frame with the sharp pixels of the adjacent video
frames. Therefore, mainstream methods align the adjacent frames based on the
estimated optical flows and fuse the alignment frames for restoration. However,
these methods sometimes generate unsatisfactory results because they rarely
consider the blur levels of pixels, which may introduce blurry pixels from
video frames. Actually, not all the pixels in the video frames are sharp and
beneficial for deblurring. To address this problem, we propose the
spatio-temporal deformable attention network (STDANet) for video delurring,
which extracts the information of sharp pixels by considering the pixel-wise
blur levels of the video frames. Specifically, STDANet is an encoder-decoder
network combined with the motion estimator and spatio-temporal deformable
attention (STDA) module, where motion estimator predicts coarse optical flows
that are used as base offsets to find the corresponding sharp pixels in STDA
module. Experimental results indicate that the proposed STDANet performs
favorably against state-of-the-art methods on the GoPro, DVD, and BSD datasets.",https://github.com/huicongzhang/STDAN,-1
Parametric Classification for Generalized Category Discovery: A Baseline Study,0.0949015,"Generalized Category Discovery (GCD) aims to discover novel categories in
unlabelled datasets using knowledge learned from labelled samples. Previous
studies argued that parametric classifiers are prone to overfitting to seen
categories, and endorsed using a non-parametric classifier formed with
semi-supervised k-means. However, in this study, we investigate the failure of
parametric classifiers, verify the effectiveness of previous design choices
when high-quality supervision is available, and identify unreliable
pseudo-labels as a key problem. We demonstrate that two prediction biases
exist: the classifier tends to predict seen classes more often, and produces an
imbalanced distribution across seen and novel categories. Based on these
findings, we propose a simple yet effective parametric classification method
that benefits from entropy regularisation, achieves state-of-the-art
performance on multiple GCD benchmarks and shows strong robustness to unknown
class numbers. We hope the investigation and proposed simple framework can
serve as a strong baseline to facilitate future studies in this field. Our code
is available at: https://github.com/CVMI-Lab/SimGCD.",https://github.com/CVMI-Lab/SimGCD,-1
Hyperbolic Deep Reinforcement Learning,0.0226277,"We propose a new class of deep reinforcement learning (RL) algorithms that
model latent representations in hyperbolic space. Sequential decision-making
requires reasoning about the possible future consequences of current behavior.
Consequently, capturing the relationship between key evolving features for a
given task is conducive to recovering effective policies. To this end,
hyperbolic geometry provides deep RL models with a natural basis to precisely
encode this inherently hierarchical information. However, applying existing
methodologies from the hyperbolic deep learning literature leads to fatal
optimization instabilities due to the non-stationarity and variance
characterizing RL gradient estimators. Hence, we design a new general method
that counteracts such optimization challenges and enables stable end-to-end
learning with deep hyperbolic representations. We empirically validate our
framework by applying it to popular on-policy and off-policy RL algorithms on
the Procgen and Atari 100K benchmarks, attaining near universal performance and
generalization benefits. Given its natural fit, we hope future RL research will
consider hyperbolic representations as a standard tool.",sites.google.com/view/hyperbolic-rl,-1
Leveraging QA Datasets to Improve Generative Data Augmentation,0.186335,"The ability of generative language models (GLMs) to generate text has
improved considerably in the last few years, enabling their use for generative
data augmentation. In this work, we propose CONDA, an approach to further
improve GLMs' ability to generate synthetic data by reformulating data
generation as context generation for a given question-answer (QA) pair and
leveraging QA datasets for training context generators. Then, we cast
downstream tasks into the same question answering format and adapt the
fine-tuned context generators to the target task domain. Finally, we use the
fine-tuned GLM to generate relevant contexts, which are in turn used as
synthetic training data for their corresponding tasks. We perform extensive
experiments on multiple classification datasets and demonstrate substantial
improvements in performance for both few- and zero-shot settings. Our analysis
reveals that QA datasets that require high-level reasoning abilities (e.g.,
abstractive and common-sense QA datasets) tend to give the best boost in
performance in both few-shot and zero-shot settings.",https://github.com/dheeraj7596/CONDA,-1
Points2NeRF: Generating Neural Radiance Fields from 3D point cloud,0.198292,"Contemporary registration devices for 3D visual information, such as LIDARs
and various depth cameras, capture data as 3D point clouds. In turn, such
clouds are challenging to be processed due to their size and complexity.
Existing methods address this problem by fitting a mesh to the point cloud and
rendering it instead. This approach, however, leads to the reduced fidelity of
the resulting visualization and misses color information of the objects crucial
in computer graphics applications. In this work, we propose to mitigate this
challenge by representing 3D objects as Neural Radiance Fields (NeRFs). We
leverage a hypernetwork paradigm and train the model to take a 3D point cloud
with the associated color values and return a NeRF network's weights that
reconstruct 3D objects from input 2D images. Our method provides efficient 3D
object representation and offers several advantages over the existing
approaches, including the ability to condition NeRFs and improved
generalization beyond objects seen in training. The latter we also confirmed in
the results of our empirical evaluation.",https://github.com/gmum/points2nerf,-1
Dialogue Strategy Adaptation to New Action Sets Using Multi-dimensional Modelling,0.012176,"A major bottleneck for building statistical spoken dialogue systems for new
domains and applications is the need for large amounts of training data. To
address this problem, we adopt the multi-dimensional approach to dialogue
management and evaluate its potential for transfer learning. Specifically, we
exploit pre-trained task-independent policies to speed up training for an
extended task-specific action set, in which the single summary action for
requesting a slot is replaced by multiple slot-specific request actions. Policy
optimisation and evaluation experiments using an agenda-based user simulator
show that with limited training data, much better performance levels can be
achieved when using the proposed multi-dimensional adaptation method. We
confirm this improvement in a crowd-sourced human user evaluation of our spoken
dialogue system, comparing partially trained policies. The multi-dimensional
system (with adaptation on limited training data in the target scenario)
outperforms the one-dimensional baseline (without adaptation on the same amount
of training data) by 7% perceived success rate.",None,-1
Towards End-to-End Open Conversational Machine Reading,0.0572684,"In open-retrieval conversational machine reading (OR-CMR) task, machines are
required to do multi-turn question answering given dialogue history and a
textual knowledge base. Existing works generally utilize two independent
modules to approach this problem's two successive sub-tasks: first with a
hard-label decision making and second with a question generation aided by
various entailment reasoning methods. Such usual cascaded modeling is
vulnerable to error propagation and prevents the two sub-tasks from being
consistently optimized. In this work, we instead model OR-CMR as a unified
text-to-text task in a fully end-to-end style. Experiments on the OR-ShARC
dataset show the effectiveness of our proposed end-to-end framework on both
sub-tasks by a large margin, achieving new state-of-the-art results. Further
ablation studies support that our framework can generalize to different
backbone models.",None,-1
Video Anomaly Detection via Prediction Network with Enhanced Spatio-Temporal Memory Exchange,0.123262,"Video anomaly detection is a challenging task because most anomalies are
scarce and non-deterministic. Many approaches investigate the reconstruction
difference between normal and abnormal patterns, but neglect that anomalies do
not necessarily correspond to large reconstruction errors. To address this
issue, we design a Convolutional LSTM Auto-Encoder prediction framework with
enhanced spatio-temporal memory exchange using bi-directionalilty and a
higher-order mechanism. The bi-directional structure promotes learning the
temporal regularity through forward and backward predictions. The unique
higher-order mechanism further strengthens spatial information interaction
between the encoder and the decoder. Considering the limited receptive fields
in Convolutional LSTMs, we also introduce an attention module to highlight
informative features for prediction. Anomalies are eventually identified by
comparing the frames with their corresponding predictions. Evaluations on three
popular benchmarks show that our framework outperforms most existing
prediction-based anomaly detection methods.",None,-1
Holistic Sentence Embeddings for Better Out-of-Distribution Detection,0.0231024,"Detecting out-of-distribution (OOD) instances is significant for the safe
deployment of NLP models. Among recent textual OOD detection works based on
pretrained language models (PLMs), distance-based methods have shown superior
performance. However, they estimate sample distance scores in the last-layer
CLS embedding space and thus do not make full use of linguistic information
underlying in PLMs. To address the issue, we propose to boost OOD detection by
deriving more holistic sentence embeddings. On the basis of the observations
that token averaging and layer combination contribute to improving OOD
detection, we propose a simple embedding approach named Avg-Avg, which averages
all token representations from each intermediate layer as the sentence
embedding and significantly surpasses the state-of-the-art on a comprehensive
suite of benchmarks by a 9.33% FAR95 margin. Furthermore, our analysis
demonstrates that it indeed helps preserve general linguistic knowledge in
fine-tuned PLMs and substantially benefits detecting background shifts. The
simple yet effective embedding method can be applied to fine-tuned PLMs with
negligible extra costs, providing a free gain in OOD detection. Our code is
available at https://github.com/lancopku/Avg-Avg.",https://github.com/lancopku/Avg-Avg,-1
Unsupervised Opinion Summarization Using Approximate Geodesics,0.0223801,"Opinion summarization is the task of creating summaries capturing popular
opinions from user reviews. In this paper, we introduce Geodesic Summarizer
(GeoSumm), a novel system to perform unsupervised extractive opinion
summarization. GeoSumm involves an encoder-decoder based representation
learning model, that generates representations of text as a distribution over
latent semantic units. GeoSumm generates these representations by performing
dictionary learning over pre-trained text representations at multiple decoder
layers. We then use these representations to quantify the relevance of review
sentences using a novel approximate geodesic distance based scoring mechanism.
We use the relevance scores to identify popular opinions in order to compose
general and aspect-specific summaries. Our proposed model, GeoSumm, achieves
state-of-the-art performance on three opinion summarization datasets. We
perform additional experiments to analyze the functioning of our model and
showcase the generalization ability of {\X} across different domains.",None,-1
OrthoMAD: Morphing Attack Detection Through Orthogonal Identity Disentanglement,0.0742895,"Morphing attacks are one of the many threats that are constantly affecting
deep face recognition systems. It consists of selecting two faces from
different individuals and fusing them into a final image that contains the
identity information of both. In this work, we propose a novel regularisation
term that takes into account the existent identity information in both and
promotes the creation of two orthogonal latent vectors. We evaluate our
proposed method (OrthoMAD) in five different types of morphing in the FRLL
dataset and evaluate the performance of our model when trained on five distinct
datasets. With a small ResNet-18 as the backbone, we achieve state-of-the-art
results in the majority of the experiments, and competitive results in the
others. The code of this paper will be publicly available.",https://github.com/NetoPedro/OrthoMAD,-1
Tsetlin Machine for Solving Contextual Bandit Problems,0.00614436,"This paper introduces an interpretable contextual bandit algorithm using
Tsetlin Machines, which solves complex pattern recognition tasks using
propositional logic. The proposed bandit learning algorithm relies on
straightforward bit manipulation, thus simplifying computation and
interpretation. We then present a mechanism for performing Thompson sampling
with Tsetlin Machine, given its non-parametric nature. Our empirical analysis
shows that Tsetlin Machine as a base contextual bandit learner outperforms
other popular base learners on eight out of nine datasets. We further analyze
the interpretability of our learner, investigating how arms are selected based
on propositional expressions that model the context.",https://github.com/xxx,-1
Robust Planning for Human-Robot Joint Tasks with Explicit Reasoning on Human Mental State,0.0459381,"We consider the human-aware task planning problem where a human-robot team is
given a shared task with a known objective to achieve. Recent approaches tackle
it by modeling it as a team of independent, rational agents, where the robot
plans for both agents' (shared) tasks. However, the robot knows that humans
cannot be administered like artificial agents, so it emulates and predicts the
human's decisions, actions, and reactions. Based on earlier approaches, we
describe a novel approach to solve such problems, which models and uses
execution-time observability conventions. Abstractly, this modeling is based on
situation assessment, which helps our approach capture the evolution of
individual agents' beliefs and anticipate belief divergences that arise in
practice. It decides if and when belief alignment is needed and achieves it
with communication. These changes improve the solver's performance: (a)
communication is effectively used, and (b) robust for more realistic and
challenging problems.",None,-1
Fuzzy Positive Learning for Semi-supervised Semantic Segmentation,0.0717298,"Semi-supervised learning (SSL) essentially pursues class boundary exploration
with less dependence on human annotations. Although typical attempts focus on
ameliorating the inevitable error-prone pseudo-labeling, we think differently
and resort to exhausting informative semantics from multiple probably correct
candidate labels. In this paper, we introduce Fuzzy Positive Learning (FPL) for
accurate SSL semantic segmentation in a plug-and-play fashion, targeting
adaptively encouraging fuzzy positive predictions and suppressing
highly-probable negatives. Being conceptually simple yet practically effective,
FPL can remarkably alleviate interference from wrong pseudo labels and
progressively achieve clear pixel-level semantic discrimination. Concretely,
our FPL approach consists of two main components, including fuzzy positive
assignment (FPA) to provide an adaptive number of labels for each pixel and
fuzzy positive regularization (FPR) to restrict the predictions of fuzzy
positive categories to be larger than the rest under different perturbations.
Theoretical analysis and extensive experiments on Cityscapes and VOC 2012 with
consistent performance gain justify the superiority of our approach.",None,-1
Factual Error Correction for Abstractive Summaries Using Entity Retrieval,0.0961315,"Despite the recent advancements in abstractive summarization systems
leveraged from large-scale datasets and pre-trained language models, the
factual correctness of the summary is still insufficient. One line of trials to
mitigate this problem is to include a post-editing process that can detect and
correct factual errors in the summary. In building such a post-editing system,
it is strongly required that 1) the process has a high success rate and
interpretability and 2) has a fast running time. Previous approaches focus on
regeneration of the summary using the autoregressive models, which lack
interpretability and require high computing resources. In this paper, we
propose an efficient factual error correction system RFEC based on entities
retrieval post-editing process. RFEC first retrieves the evidence sentences
from the original document by comparing the sentences with the target summary.
This approach greatly reduces the length of text for a system to analyze. Next,
RFEC detects the entity-level errors in the summaries by considering the
evidence sentences and substitutes the wrong entities with the accurate
entities from the evidence sentences. Experimental results show that our
proposed error correction system shows more competitive performance than
baseline methods in correcting the factual errors with a much faster speed.",None,-1
IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding Alignment,0.0852977,"This paper investigates the problem of temporally interpolating dynamic 3D
point clouds with large non-rigid deformation. We formulate the problem as
estimation of point-wise trajectories (i.e., smooth curves) and further reason
that temporal irregularity and under-sampling are two major challenges. To
tackle the challenges, we propose IDEA-Net, an end-to-end deep learning
framework, which disentangles the problem under the assistance of the
explicitly learned temporal consistency. Specifically, we propose a temporal
consistency learning module to align two consecutive point cloud frames
point-wisely, based on which we can employ linear interpolation to obtain
coarse trajectories/in-between frames. To compensate the high-order nonlinear
components of trajectories, we apply aligned feature embeddings that encode
local geometry properties to regress point-wise increments, which are combined
with the coarse estimations. We demonstrate the effectiveness of our method on
various point cloud sequences and observe large improvement over
state-of-the-art methods both quantitatively and visually. Our framework can
bring benefits to 3D motion data acquisition. The source code is publicly
available at https://github.com/ZENGYIMING-EAMON/IDEA-Net.git.",https://github.com/ZENGYIMING-EAMON/IDEA-Net.git,-1
Beyond calibration: estimating the grouping loss of modern neural networks,0.246569,"The ability to ensure that a classifier gives reliable confidence scores is
essential to ensure informed decision-making. To this end, recent work has
focused on miscalibration, i.e., the over or under confidence of model scores.
Yet calibration is not enough: even a perfectly calibrated classifier with the
best possible accuracy can have confidence scores that are far from the true
posterior probabilities. This is due to the grouping loss, created by samples
with the same confidence scores but different true posterior probabilities.
Proper scoring rule theory shows that given the calibration loss, the missing
piece to characterize individual errors is the grouping loss. While there are
many estimators of the calibration loss, none exists for the grouping loss in
standard settings. Here, we propose an estimator to approximate the grouping
loss. We show that modern neural network architectures in vision and NLP
exhibit grouping loss, notably in distribution shifts settings, which
highlights the importance of pre-production validation.",https://github.com/aperezlebel/beyond_calibration,-1
Are Synonym Substitution Attacks Really Synonym Substitution Attacks?,0.0392914,"In this paper, we explore the following question: Are synonym substitution
attacks really synonym substitution attacks (SSAs)? We approach this question
by examining how SSAs replace words in the original sentence and show that
there are still unresolved obstacles that make current SSAs generate invalid
adversarial samples. We reveal that four widely used word substitution methods
generate a large fraction of invalid substitution words that are ungrammatical
or do not preserve the original sentence's semantics. Next, we show that the
semantic and grammatical constraints used in SSAs for detecting invalid word
replacements are highly insufficient in detecting invalid adversarial samples.",https://textattack.readthedocs.io/en/latest/3recipes/models.html,-1
Deep Domain Adaptation for Detecting Bomb Craters in Aerial Images,0.0818557,"The aftermath of air raids can still be seen for decades after the
devastating events. Unexploded ordnance (UXO) is an immense danger to human
life and the environment. Through the assessment of wartime images, experts can
infer the occurrence of a dud. The current manual analysis process is expensive
and time-consuming, thus automated detection of bomb craters by using deep
learning is a promising way to improve the UXO disposal process. However, these
methods require a large amount of manually labeled training data. This work
leverages domain adaptation with moon surface images to address the problem of
automated bomb crater detection with deep learning under the constraint of
limited training data. This paper contributes to both academia and practice (1)
by providing a solution approach for automated bomb crater detection with
limited training data and (2) by demonstrating the usability and associated
challenges of using synthetic images for domain adaptation.",None,-1
Lymphoma segmentation from 3D PET-CT images using a deep evidential network,0.290395,"An automatic evidential segmentation method based on Dempster-Shafer theory
and deep learning is proposed to segment lymphomas from three-dimensional
Positron Emission Tomography (PET) and Computed Tomography (CT) images. The
architecture is composed of a deep feature-extraction module and an evidential
layer. The feature extraction module uses an encoder-decoder framework to
extract semantic feature vectors from 3D inputs. The evidential layer then uses
prototypes in the feature space to compute a belief function at each voxel
quantifying the uncertainty about the presence or absence of a lymphoma at this
location. Two evidential layers are compared, based on different ways of using
distances to prototypes for computing mass functions. The whole model is
trained end-to-end by minimizing the Dice loss function. The proposed
combination of deep feature extraction and evidential segmentation is shown to
outperform the baseline UNet model as well as three other state-of-the-art
models on a dataset of 173 patients.",https://github.com/iWeisskohl,-1
Natural Language Inference with Self-Attention for Veracity Assessment of Pandemic Claims,0.248295,"We present a comprehensive work on automated veracity assessment from dataset
creation to developing novel methods based on Natural Language Inference (NLI),
focusing on misinformation related to the COVID-19 pandemic. We first describe
the construction of the novel PANACEA dataset consisting of heterogeneous
claims on COVID-19 and their respective information sources. The dataset
construction includes work on retrieval techniques and similarity measurements
to ensure a unique set of claims. We then propose novel techniques for
automated veracity assessment based on Natural Language Inference including
graph convolutional networks and attention based approaches. We have carried
out experiments on evidence retrieval and veracity assessment on the dataset
using the proposed techniques and found them competitive with SOTA methods, and
provided a detailed discussion.",None,-1
Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion,0.349541,"Transformer-based pre-trained models like BERT have achieved great progress
on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also
shown general benefits in multiple NLP tasks. However, how to efficiently
integrate dependency prior structure into pre-trained models to better model
complex semantic matching relations is still unsettled. In this paper, we
propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion
\textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency
structure into pre-trained models and adaptively fuses it with semantic
information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a
structure-sensitive paradigm to construct a dependency matrix for calibrating
attention weights. It adopts an adaptive fusion module to integrate the
obtained dependency information and the original semantic signals. Moreover,
DAFA reconstructs the attention calculation flow and provides better
interpretability. By applying it on BERT, our method achieves state-of-the-art
or competitive performance on 10 public datasets, demonstrating the benefits of
adaptively fusing dependency structure in semantic matching task.",None,-1
ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information,0.698806,"ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10
billion web pages affiliated with rich information. Its design was influenced
by the need for a high quality, large scale web corpus to support a range of
academic and industry research, for example, in information systems,
retrieval-augmented AI systems, and model pretraining. Compared with earlier
ClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of
higher-quality, and aligned with the document distributions in commercial web
search. Besides raw HTML, ClueWeb22 includes rich information about the web
pages provided by industry-standard document understanding systems, including
the visual representation of pages rendered by a web browser, parsed HTML
structure information from a neural network parser, and pre-processed cleaned
document text to lower the barrier to entry. Many of these signals have been
widely used in industry but are available to the research community for the
first time at this scale.",https://github.com/microsoft/Efficient-Large-LM-Trainer,-1
Real-time Gesture Animation Generation from Speech for Virtual Human Interaction,0.252812,"We propose a real-time system for synthesizing gestures directly from speech.
Our data-driven approach is based on Generative Adversarial Neural Networks to
model the speech-gesture relationship. We utilize the large amount of speaker
video data available online to train our 3D gesture model. Our model generates
speaker-specific gestures by taking consecutive audio input chunks of two
seconds in length. We animate the predicted gestures on a virtual avatar. We
achieve a delay below three seconds between the time of audio input and gesture
animation. Code and videos are available at
https://github.com/mrebol/Gestures-From-Speech",https://github.com/mrebol/Gestures-From-Speech,-1
CONSISTENT: Open-Ended Question Generation From News Articles,0.0713479,"Recent work on question generation has largely focused on factoid questions
such as who, what, where, when about basic facts. Generating open-ended why,
how, what, etc. questions that require long-form answers have proven more
difficult. To facilitate the generation of open-ended questions, we propose
CONSISTENT, a new end-to-end system for generating open-ended questions that
are answerable from and faithful to the input text. Using news articles as a
trustworthy foundation for experimentation, we demonstrate our model's strength
over several baselines using both automatic and human=based evaluations. We
contribute an evaluation dataset of expert-generated open-ended questions.We
discuss potential downstream applications for news media organizations.",https://github.com/tuhinjubcse/OpenD,-1
VAST: The Valence-Assessing Semantics Test for Contextualizing Language Models,0.312347,"VAST, the Valence-Assessing Semantics Test, is a novel intrinsic evaluation
task for contextualized word embeddings (CWEs). VAST uses valence, the
association of a word with pleasantness, to measure the correspondence of
word-level LM semantics with widely used human judgments, and examines the
effects of contextualization, tokenization, and LM-specific geometry. Because
prior research has found that CWEs from GPT-2 perform poorly on other intrinsic
evaluations, we select GPT-2 as our primary subject, and include results
showing that VAST is useful for 7 other LMs, and can be used in 7 languages.
GPT-2 results show that the semantics of a word incorporate the semantics of
context in layers closer to model output, such that VAST scores diverge between
our contextual settings, ranging from Pearson's rho of .55 to .77 in layer 11.
We also show that multiply tokenized words are not semantically encoded until
layer 8, where they achieve Pearson's rho of .46, indicating the presence of an
encoding process for multiply tokenized words which differs from that of singly
tokenized words, for which rho is highest in layer 0. We find that a few
neurons with values having greater magnitude than the rest mask word-level
semantics in GPT-2's top layer, but that word-level semantics can be recovered
by nullifying non-semantic principal components: Pearson's rho in the top layer
improves from .32 to .76. After isolating semantics, we show the utility of
VAST for understanding LM semantics via improvements over related work on four
word similarity tasks, with a score of .50 on SimLex-999, better than the
previous best of .45 for GPT-2. Finally, we show that 8 of 10 WEAT bias tests,
which compare differences in word embedding associations between groups of
words, exhibit more stereotype-congruent biases after isolating semantics,
indicating that non-semantic structures in LMs also mask biases.",https://github.com/wolferobert3/vast,-1
Policy Optimization with Advantage Regularization for Long-Term Fairness in Decision Systems,0.182684,"Long-term fairness is an important factor of consideration in designing and
deploying learning-based decision systems in high-stake decision-making
contexts. Recent work has proposed the use of Markov Decision Processes (MDPs)
to formulate decision-making with long-term fairness requirements in
dynamically changing environments, and demonstrated major challenges in
directly deploying heuristic and rule-based policies that worked well in static
environments. We show that policy optimization methods from deep reinforcement
learning can be used to find strictly better decision policies that can often
achieve both higher overall utility and less violation of the fairness
requirements, compared to previously-known strategies. In particular, we
propose new methods for imposing fairness requirements in policy optimization
by regularizing the advantage evaluation of different actions. Our proposed
methods make it easy to impose fairness constraints without reward engineering
or sacrificing training efficiency. We perform detailed analyses in three
established case studies, including attention allocation in incident
monitoring, bank loan approval, and vaccine distribution in population
networks.",None,-1
A Two-Stage Efficient 3-D CNN Framework for EEG Based Emotion Recognition,0.0911767,"This paper proposes a novel two-stage framework for emotion recognition using
EEG data that outperforms state-of-the-art models while keeping the model size
small and computationally efficient. The framework consists of two stages; the
first stage involves constructing efficient models named EEGNet, which is
inspired by the state-of-the-art efficient architecture and employs
inverted-residual blocks that contain depthwise separable convolutional layers.
The EEGNet models on both valence and arousal labels achieve the average
classification accuracy of 90%, 96.6%, and 99.5% with only 6.4k, 14k, and 25k
parameters, respectively. In terms of accuracy and storage cost, these models
outperform the previous state-of-the-art result by up to 9%. In the second
stage, we binarize these models to further compress them and deploy them easily
on edge devices. Binary Neural Networks (BNNs) typically degrade model
accuracy. We improve the EEGNet binarized models in this paper by introducing
three novel methods and achieving a 20\% improvement over the baseline binary
models. The proposed binarized EEGNet models achieve accuracies of 81%, 95%,
and 99% with storage costs of 0.11Mbits, 0.28Mbits, and 0.46Mbits,
respectively. Those models help deploy a precise human emotion recognition
system on the edge environment.",None,-1
MEKER: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering,0.0484099,"Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG
embedding contains concise data used in NLP tasks requiring implicit
information about the real world. Furthermore, the size of KGs that may be
useful in actual NLP assignments is enormous, and creating embedding over it
has memory cost issues. We represent KG as a 3rd-order binary tensor and move
beyond the standard CP decomposition by using a data-specific generalized
version of it. The generalization of the standard CP-ALS algorithm allows
obtaining optimization gradients without a backpropagation mechanism. It
reduces the memory needed in training while providing computational benefits.
We propose a MEKER, a memory-efficient KG embedding model, which yields
SOTA-comparable performance on link prediction tasks and KG-based Question
Answering.",https://github.com/askplatypus/wikidata-simplequestions,-1
Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion,0.0616151,"Automatic discourse processing is bottlenecked by data: current discourse
formalisms pose highly demanding annotation tasks involving large taxonomies of
discourse relations, making them inaccessible to lay annotators. This work
instead adopts the linguistic framework of Questions Under Discussion (QUD) for
discourse analysis and seeks to derive QUD structures automatically. QUD views
each sentence as an answer to a question triggered in prior context; thus, we
characterize relationships between sentences as free-form questions, in
contrast to exhaustive fine-grained taxonomies. We develop the
first-of-its-kind QUD parser that derives a dependency structure of questions
over full documents, trained using a large, crowdsourced question-answering
dataset DCQA (Ko et al., 2022). Human evaluation results show that QUD
dependency parsing is possible for language models trained with this
crowdsourced, generalizable annotation scheme. We illustrate how our QUD
structure is distinct from RST trees, and demonstrate the utility of QUD
analysis in the context of document simplification. Our findings show that QUD
parsing is an appealing alternative for automatic discourse processing.",https://github.com/wjko/discourse-qa,-1
FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation,0.616375,"The speaker-follower models have proven to be effective in
vision-and-language navigation, where a speaker model is used to synthesize new
instructions to augment the training data for a follower navigation model.
However, in many of the previous methods, the generated instructions are not
directly trained to optimize the performance of the follower. In this paper, we
present \textsc{foam}, a \textsc{Fo}llower-\textsc{a}ware speaker
\textsc{M}odel that is constantly updated given the follower feedback, so that
the generated instructions can be more suitable to the current learning state
of the follower. Specifically, we optimize the speaker using a bi-level
optimization framework and obtain its training signals by evaluating the
follower on labeled data. Experimental results on the Room-to-Room and
Room-across-Room datasets demonstrate that our methods can outperform strong
baseline models across settings. Analyses also reveal that our generated
instructions are of higher quality than the baselines.",https://github.com/PlusLabNLP/follower_aware_speaker,-1
Bellman Meets Hawkes: Model-Based Reinforcement Learning via Temporal Point Processes,0.161016,"We consider a sequential decision making problem where the agent faces the
environment characterized by the stochastic discrete events and seeks an
optimal intervention policy such that its long-term reward is maximized. This
problem exists ubiquitously in social media, finance and health informatics but
is rarely investigated by the conventional research in reinforcement learning.
To this end, we present a novel framework of the model-based reinforcement
learning where the agent's actions and observations are asynchronous stochastic
discrete events occurring in continuous-time. We model the dynamics of the
environment by Hawkes process with external intervention control term and
develop an algorithm to embed such process in the Bellman equation which guides
the direction of the value gradient. We demonstrate the superiority of our
method in both synthetic simulator and real-world problem.",https://github.com/WilliamBUG/Event-driven-rl/tree/main,-1
Multilingual Persuasion Detection: Video Games as an Invaluable Data Source for NLP,0.020955,"Role-playing games (RPGs) have a considerable amount of text in video game
dialogues. Quite often this text is semi-annotated by the game developers. In
this paper, we extract a multilingual dataset of persuasive dialogue from
several RPGs. We show the viability of this data in building a persuasion
detection system using a natural language processing (NLP) model called BERT.
We believe that video games have a lot of unused potential as a datasource for
a variety of NLP tasks. The code and data described in this paper are available
on Zenodo.",None,-1
Overview of The MediaEval 2022 Predicting Video Memorability Task,0.103565,"This paper describes the 5th edition of the Predicting Video Memorability
Task as part of MediaEval2022. This year we have reorganised and simplified the
task in order to lubricate a greater depth of inquiry. Similar to last year,
two datasets are provided in order to facilitate generalisation, however, this
year we have replaced the TRECVid2019 Video-to-Text dataset with the VideoMem
dataset in order to remedy underlying data quality issues, and to prioritise
short-term memorability prediction by elevating the Memento10k dataset as the
primary dataset. Additionally, a fully fledged electroencephalography
(EEG)-based prediction sub-task is introduced. In this paper, we outline the
core facets of the task and its constituent sub-tasks; describing the datasets,
evaluation metrics, and requirements for participant submissions.",None,-1
Universal Spam Detection using Transfer Learning of BERT Model,0.041136,"Deep learning transformer models become important by training on text data
based on self-attention mechanisms. This manuscript demonstrated a novel
universal spam detection model using pre-trained Google's Bidirectional Encoder
Representations from Transformers (BERT) base uncased models with four datasets
by efficiently classifying ham or spam emails in real-time scenarios. Different
methods for Enron, Spamassain, Lingspam, and Spamtext message classification
datasets, were used to train models individually in which a single model was
obtained with acceptable performance on four datasets. The Universal Spam
Detection Model (USDM) was trained with four datasets and leveraged
hyperparameters from each model. The combined model was finetuned with the same
hyperparameters from these four models separately. When each model using its
corresponding dataset, an F1-score is at and above 0.9 in individual models. An
overall accuracy reached 97%, with an F1 score of 0.96. Research results and
implications were discussed.",None,-1
Exploiting Context Information for Generic Event Boundary Captioning,0.0619096,"Generic Event Boundary Captioning (GEBC) aims to generate three sentences
describing the status change for a given time boundary. Previous methods only
process the information of a single boundary at a time, which lacks utilization
of video context information. To tackle this issue, we design a model that
directly takes the whole video as input and generates captions for all
boundaries parallelly. The model could learn the context information for each
time boundary by modeling the boundary-boundary interactions. Experiments
demonstrate the effectiveness of context information. The proposed method
achieved a 72.84 score on the test set, and we reached the $2^{nd}$ place in
this challenge. Our code is available at:
\url{https://github.com/zjr2000/Context-GEBC}",https://github.com/zjr2000/Context-GEBC,-1
gBuilder: A Scalable Knowledge Graph Construction System for Unstructured Corpus,0.0845033,"We design a user-friendly and scalable knowledge graph construction (KGC)
system for extracting structured knowledge from the unstructured corpus.
Different from existing KGC systems, gBuilder provides a flexible and
user-defined pipeline to embrace the rapid development of IE models. More
built-in template-based or heuristic operators and programmable operators are
available for adapting to data from different domains. Furthermore, we also
design a cloud-based self-adaptive task scheduling for gBuilder to ensure its
scalability on large-scale knowledge graph construction. Experimental
evaluation demonstrates the ability of gBuilder to organize multiple
information extraction models for knowledge graph construction in a uniform
platform, and confirms its high scalability on large-scale KGC tasks.",None,-1
Cross-Enhancement Transformer for Action Segmentation,0.31433,"Temporal convolutions have been the paradigm of choice in action
segmentation, which enhances long-term receptive fields by increasing
convolution layers. However, high layers cause the loss of local information
necessary for frame recognition. To solve the above problem, a novel
encoder-decoder structure is proposed in this paper, called Cross-Enhancement
Transformer. Our approach can be effective learning of temporal structure
representation with interactive self-attention mechanism. Concatenated each
layer convolutional feature maps in encoder with a set of features in decoder
produced via self-attention. Therefore, local and global information are used
in a series of frame actions simultaneously. In addition, a new loss function
is proposed to enhance the training process that penalizes over-segmentation
errors. Experiments show that our framework performs state-of-the-art on three
challenging datasets: 50Salads, Georgia Tech Egocentric Activities and the
Breakfast dataset.",None,-1
IMU2CLIP: Multimodal Contrastive Learning for IMU Motion Sensors from Egocentric Videos and Text,0.175485,"We present IMU2CLIP, a novel pre-training approach to align Inertial
Measurement Unit (IMU) motion sensor recordings with video and text, by
projecting them into the joint representation space of Contrastive
Language-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to
translate human motions (as measured by IMU sensors) into their corresponding
textual descriptions and videos -- while preserving the transitivity across
these modalities.
  We explore several new IMU-based applications that IMU2CLIP enables, such as
motion-based media retrieval and natural language reasoning tasks with motion
data. In addition, we show that IMU2CLIP can significantly improve the
downstream performance when fine-tuned for each application (e.g. activity
recognition), demonstrating the universal usage of IMU2CLIP as a new
pre-trained resource. Our code will be made publicly available.",None,-1
BDA-SketRet: Bi-Level Domain Adaptation for Zero-Shot SBIR,0.0274623,"The efficacy of zero-shot sketch-based image retrieval (ZS-SBIR) models is
governed by two challenges. The immense distributions-gap between the sketches
and the images requires a proper domain alignment. Moreover, the fine-grained
nature of the task and the high intra-class variance of many categories
necessitates a class-wise discriminative mapping among the sketch, image, and
the semantic spaces. Under this premise, we propose BDA-SketRet, a novel
ZS-SBIR framework performing a bi-level domain adaptation for aligning the
spatial and semantic features of the visual data pairs progressively. In order
to highlight the shared features and reduce the effects of any sketch or
image-specific artifacts, we propose a novel symmetric loss function based on
the notion of information bottleneck for aligning the semantic features while a
cross-entropy-based adversarial loss is introduced to align the spatial feature
maps. Finally, our CNN-based model confirms the discriminativeness of the
shared latent space through a novel topology-preserving semantic projection
network. Experimental results on the extended Sketchy, TU-Berlin, and QuickDraw
datasets exhibit sharp improvements over the literature.",None,-1
Visualize Before You Write: Imagination-Guided Open-Ended Text Generation,0.332585,"Recent advances in text-to-image synthesis make it possible to visualize
machine imaginations for a given context. On the other hand, when generating
text, human writers are gifted at creative visualization, which enhances their
writings by forming imaginations as blueprints before putting down the stories
in words. Inspired by such a cognitive process, we ask the natural question of
whether we can endow machines with the same ability to utilize visual
information and construct a general picture of the context to guide text
generation. In this work, we propose iNLG that uses machine-generated images to
guide language models in open-ended text generation. The experiments and
analyses demonstrate the effectiveness of iNLG on open-ended text generation
tasks, including text completion, story generation, and concept-to-text
generation in both few-shot and full-data scenarios. Both automatic metrics and
human evaluations verify that the text snippets generated by our iNLG are
coherent and informative while displaying minor degeneration.",https://github.com/VegB/iNLG,-1
Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space,0.084696,"Learning causal relationships in high-dimensional data (images, videos) is a
hard task, as they are often defined on low dimensional manifolds and must be
extracted from complex signals dominated by appearance, lighting, textures and
also spurious correlations in the data. We present a method for learning
counterfactual reasoning of physical processes in pixel space, which requires
the prediction of the impact of interventions on initial conditions. Going
beyond the identification of structural relationships, we deal with the
challenging problem of forecasting raw video over long horizons. Our method
does not require the knowledge or supervision of any ground truth positions or
other object or scene properties. Our model learns and acts on a suitable
hybrid latent representation based on a combination of dense features, sets of
2D keypoints and an additional latent vector per keypoint. We show that this
better captures the dynamics of physical processes than purely dense or sparse
representations. We introduce a new challenging and carefully designed
counterfactual benchmark for predictions in pixel space and outperform strong
baselines in physics-inspired ML and video prediction.",https://filteredcophy.github.io,-1
Image Augmentation for Satellite Images,0.0239662,"This study proposes the use of generative models (GANs) for augmenting the
EuroSAT dataset for the Land Use and Land Cover (LULC) Classification task. We
used DCGAN and WGAN-GP to generate images for each class in the dataset. We
then explored the effect of augmenting the original dataset by about 10% in
each case on model performance. The choice of GAN architecture seems to have no
apparent effect on the model performance. However, a combination of geometric
augmentation and GAN-generated images improved baseline results. Our study
shows that GANs augmentation can improve the generalizability of deep
classification models on satellite images.",None,-1
Explainable Action Advising for Multi-Agent Reinforcement Learning,0.416594,"Action advising is a knowledge transfer technique for reinforcement learning
based on the teacher-student paradigm. An expert teacher provides advice to a
student during training in order to improve the student's sample efficiency and
policy performance. Such advice is commonly given in the form of state-action
pairs. However, it makes it difficult for the student to reason with and apply
to novel states. We introduce Explainable Action Advising, in which the teacher
provides action advice as well as associated explanations indicating why the
action was chosen. This allows the student to self-reflect on what it has
learned, enabling advice generalization and leading to improved sample
efficiency and learning performance - even in environments where the teacher is
sub-optimal. We empirically show that our framework is effective in both
single-agent and multi-agent scenarios, yielding improved policy returns and
convergence rates when compared to state-of-the-art methods",https://github.com/sophieyueguo/explainable_action_advising,-1
SecureBERT: A Domain-Specific Language Model for Cybersecurity,0.9682,"Natural Language Processing (NLP) has recently gained wide attention in
cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber
automation. Increased connection and automation have revolutionized the world's
economic and cultural infrastructures, while they have introduced risks in
terms of cyber attacks. CTI is information that helps cybersecurity analysts
make intelligent security decisions, that is often delivered in the form of
natural language text, which must be transformed to machine readable format
through an automated procedure before it can be used for automated security
measures.
  This paper proposes SecureBERT, a cybersecurity language model capable of
capturing text connotations in cybersecurity text (e.g., CTI) and therefore
successful in automation for many critical cybersecurity tasks that would
otherwise rely on human expertise and time-consuming manual efforts. SecureBERT
has been trained using a large corpus of cybersecurity text.To make SecureBERT
effective not just in retaining general English understanding, but also when
applied to text with cybersecurity implications, we developed a customized
tokenizer as well as a method to alter pre-trained weights. The SecureBERT is
evaluated using the standard Masked Language Model (MLM) test as well as two
additional standard NLP tasks. Our evaluation studies show that
SecureBERT\footnote{\url{https://github.com/ehsanaghaei/SecureBERT}}
outperforms existing similar models, confirming its capability for solving
crucial NLP tasks in cybersecurity.",https://github.com/ehsanaghaei/SecureBERT,-1
Contextually Enhanced ES-dRNN with Dynamic Attention for Short-Term Load Forecasting,0.0163675,"In this paper, we propose a new short-term load forecasting (STLF) model
based on contextually enhanced hybrid and hierarchical architecture combining
exponential smoothing (ES) and a recurrent neural network (RNN). The model is
composed of two simultaneously trained tracks: the context track and the main
track. The context track introduces additional information to the main track.
It is extracted from representative series and dynamically modulated to adjust
to the individual series forecasted by the main track. The RNN architecture
consists of multiple recurrent layers stacked with hierarchical dilations and
equipped with recently proposed attentive dilated recurrent cells. These cells
enable the model to capture short-term, long-term and seasonal dependencies
across time series as well as to weight dynamically the input information. The
model produces both point forecasts and predictive intervals. The experimental
part of the work performed on 35 forecasting problems shows that the proposed
model outperforms in terms of accuracy its predecessor as well as standard
statistical models and state-of-the-art machine learning models.",https://github.com/slaweks17/ES-adRNN-with-Context,-1
United States Politicians' Tone Became More Negative with 2016 Primary Campaigns,0.0,"There is a widespread belief that the tone of US political language has
become more negative recently, in particular when Donald Trump entered
politics. At the same time, there is disagreement as to whether Trump changed
or merely continued previous trends. To date, data-driven evidence regarding
these questions is scarce, partly due to the difficulty of obtaining a
comprehensive, longitudinal record of politicians' utterances. Here we apply
psycholinguistic tools to a novel, comprehensive corpus of 24 million quotes
from online news attributed to 18,627 US politicians in order to analyze how
the tone of US politicians' language evolved between 2008 and 2020. We show
that, whereas the frequency of negative emotion words had decreased
continuously during Obama's tenure, it suddenly and lastingly increased with
the 2016 primary campaigns, by 1.6 pre-campaign standard deviations, or 8% of
the pre-campaign mean, in a pattern that emerges across parties. The effect
size drops by 40% when omitting Trump's quotes, and by 50% when averaging over
speakers rather than quotes, implying that prominent speakers, and Trump in
particular, have disproportionately, though not exclusively, contributed to the
rise in negative language. This work provides the first large-scale data-driven
evidence of a drastic shift toward a more negative political tone following
Trump's campaign start as a catalyst, with important implications for the
debate about the state of US politics.",None,-1
Self-Supervised Super-Resolution for Multi-Exposure Push-Frame Satellites,0.13852,"Modern Earth observation satellites capture multi-exposure bursts of
push-frame images that can be super-resolved via computational means. In this
work, we propose a super-resolution method for such multi-exposure sequences, a
problem that has received very little attention in the literature. The proposed
method can handle the signal-dependent noise in the inputs, process sequences
of any length, and be robust to inaccuracies in the exposure times.
Furthermore, it can be trained end-to-end with self-supervision, without
requiring ground truth high resolution frames, which makes it especially suited
to handle real data. Central to our method are three key contributions: i) a
base-detail decomposition for handling errors in the exposure times, ii) a
noise-level-aware feature encoding for improved fusion of frames with varying
signal-to-noise ratio and iii) a permutation invariant fusion strategy by
temporal pooling operators. We evaluate the proposed method on synthetic and
real data and show that it outperforms by a significant margin existing
single-exposure approaches that we adapted to the multi-exposure case.",None,-1
Fine Detailed Texture Learning for 3D Meshes with Generative Models,0.0743314,"This paper presents a method to reconstruct high-quality textured 3D models
from both multi-view and single-view images. The reconstruction is posed as an
adaptation problem and is done progressively where in the first stage, we focus
on learning accurate geometry, whereas in the second stage, we focus on
learning the texture with a generative adversarial network. In the generative
learning pipeline, we propose two improvements. First, since the learned
textures should be spatially aligned, we propose an attention mechanism that
relies on the learnable positions of pixels. Secondly, since discriminator
receives aligned texture maps, we augment its input with a learnable embedding
which improves the feedback to the generator. We achieve significant
improvements on multi-view sequences from Tripod dataset as well as on
single-view image datasets, Pascal 3D+ and CUB. We demonstrate that our method
achieves superior 3D textured models compared to the previous works. Please
visit our web-page for 3D visuals.",None,-1
Does Corpus Quality Really Matter for Low-Resource Languages?,0.0823669,"The vast majority of non-English corpora are derived from automatically
filtered versions of CommonCrawl. While prior work has identified major issues
on the quality of these datasets (Kreutzer et al., 2021), it is not clear how
this impacts downstream performance. Taking representation learning in Basque
as a case study, we explore tailored crawling (manually identifying and
scraping websites with high-quality content) as an alternative to filtering
CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque
portion of popular multilingual corpora like CC100 and mC4, yet it has a much
higher quality according to native annotators. For instance, 66% of documents
are rated as high-quality for EusCrawl, in contrast with <33% for both mC4 and
CC100. Nevertheless, we obtain similar results on downstream NLU tasks
regardless of the corpus used for pre-training. Our work suggests that NLU
performance in low-resource languages is not primarily constrained by the
quality of the data, and other factors like corpus size and domain coverage can
play a more important role.",None,-1
Region Aware Video Object Segmentation with Deep Motion Modeling,0.0231584,"Current semi-supervised video object segmentation (VOS) methods usually
leverage the entire features of one frame to predict object masks and update
memory. This introduces significant redundant computations. To reduce
redundancy, we present a Region Aware Video Object Segmentation (RAVOS)
approach that predicts regions of interest (ROIs) for efficient object
segmentation and memory storage. RAVOS includes a fast object motion tracker to
predict their ROIs in the next frame. For efficient segmentation, object
features are extracted according to the ROIs, and an object decoder is designed
for object-level segmentation. For efficient memory storage, we propose motion
path memory to filter out redundant context by memorizing the features within
the motion path of objects between two frames. Besides RAVOS, we also propose a
large-scale dataset, dubbed OVOS, to benchmark the performance of VOS models
under occlusions. Evaluation on DAVIS and YouTube-VOS benchmarks and our new
OVOS dataset show that our method achieves state-of-the-art performance with
significantly faster inference time, e.g., 86.1 J&F at 42 FPS on DAVIS and 84.4
J&F at 23 FPS on YouTube-VOS.",None,-1
A Unified Framework of Medical Information Annotation and Extraction for Chinese Clinical Text,0.0286275,"Medical information extraction consists of a group of natural language
processing (NLP) tasks, which collaboratively convert clinical text to
pre-defined structured formats. Current state-of-the-art (SOTA) NLP models are
highly integrated with deep learning techniques and thus require massive
annotated linguistic data. This study presents an engineering framework of
medical entity recognition, relation extraction and attribute extraction, which
are unified in annotation, modeling and evaluation. Specifically, the
annotation scheme is comprehensive, and compatible between tasks, especially
for the medical relations. The resulted annotated corpus includes 1,200 full
medical records (or 18,039 broken-down documents), and achieves inter-annotator
agreements (IAAs) of 94.53%, 73.73% and 91.98% F 1 scores for the three tasks.
Three task-specific neural network models are developed within a shared
structure, and enhanced by SOTA NLP techniques, i.e., pre-trained language
models. Experimental results show that the system can retrieve medical
entities, relations and attributes with F 1 scores of 93.47%, 67.14% and
90.89%, respectively. This study, in addition to our publicly released
annotation scheme and code, provides solid and practical engineering experience
of developing an integrated medical information extraction system.",https://github.com/syuoni/eznlp,-1
Mixture-Net: Low-Rank Deep Image Prior Inspired by Mixture Models for Spectral Image Recovery,0.0594075,"This paper proposes a non-data-driven deep neural network for spectral image
recovery problems such as denoising, single hyperspectral image
super-resolution, and compressive spectral imaging reconstruction. Unlike
previous methods, the proposed approach, dubbed Mixture-Net, implicitly learns
the prior information through the network. Mixture-Net consists of a deep
generative model whose layers are inspired by the linear and non-linear
low-rank mixture models, where the recovered image is composed of a weighted
sum between the linear and non-linear decomposition. Mixture-Net also provides
a low-rank decomposition interpreted as the spectral image abundances and
endmembers, helpful in achieving remote sensing tasks without running
additional routines. The experiments show the MixtureNet effectiveness
outperforming state-of-the-art methods in recovery quality with the advantage
of architecture interpretability.",None,-1
Finding the optimal human strategy for Wordle using maximum correct letter probabilities and reinforcement learning,0.916967,"Wordle is an online word puzzle game that gained viral popularity in January
2022. The goal is to guess a hidden five letter word. After each guess, the
player gains information about whether the letters they guessed are present in
the word, and whether they are in the correct position. Numerous blogs have
suggested guessing strategies and starting word lists that improve the chance
of winning. Optimized algorithms can win 100% of games within five of the six
allowed trials. However, it is infeasible for human players to use these
algorithms due to an inability to perfectly recall all known 5-letter words and
perform complex calculations that optimize information gain. Here, we present
two different methods for choosing starting words along with a framework for
discovering the optimal human strategy based on reinforcement learning. Human
Wordle players can use the rules we discover to optimize their chance of
winning.",https://github.com/benton-anderson/wordle-opt,-1
PTDE: Personalized Training with Distilled Execution for Multi-Agent Reinforcement Learning,0.0632603,"Centralized Training with Decentralized Execution (CTDE) has emerged as a
widely adopted paradigm in multi-agent reinforcement learning, emphasizing the
utilization of global information for learning an enhanced joint $Q$-function
or centralized critic. In contrast, our investigation delves into harnessing
global information to directly enhance individual $Q$-functions or individual
actors. Notably, we discover that applying identical global information
universally across all agents proves insufficient for optimal performance.
Consequently, we advocate for the customization of global information tailored
to each agent, creating agent-personalized global information to bolster
overall performance. Furthermore, we introduce a novel paradigm named
Personalized Training with Distilled Execution (PTDE), wherein
agent-personalized global information is distilled into the agent's local
information. This distilled information is then utilized during decentralized
execution, resulting in minimal performance degradation. PTDE can be seamlessly
integrated with state-of-the-art algorithms, leading to notable performance
enhancements across diverse benchmarks, including the SMAC benchmark, Google
Research Football (GRF) benchmark, and Learning to Rank (LTR) task.",None,-1
Sequence-Based Extractive Summarisation for Scientific Articles,0.151253,"This paper presents the results of research on supervised extractive text
summarisation for scientific articles. We show that a simple sequential tagging
model based only on the text within a document achieves high results against a
simple classification model. Improvements can be achieved through additional
sentence-level features, though these were minimal. Through further analysis,
we show the potential of the sequential model relying on the structure of the
document depending on the academic discipline which the document is from.",None,-1
Learning of Structurally Unambiguous Probabilistic Grammars,0.041797,"The problem of identifying a probabilistic context free grammar has two
aspects: the first is determining the grammar's topology (the rules of the
grammar) and the second is estimating probabilistic weights for each rule.
Given the hardness results for learning context-free grammars in general, and
probabilistic grammars in particular, most of the literature has concentrated
on the second problem. In this work we address the first problem. We restrict
attention to structurally unambiguous weighted context-free grammars (SUWCFG)
and provide a query learning algorithm for \structurally unambiguous
probabilistic context-free grammars (SUPCFG). We show that SUWCFG can be
represented using \emph{co-linear multiplicity tree automata} (CMTA), and
provide a polynomial learning algorithm that learns CMTAs. We show that the
learned CMTA can be converted into a probabilistic grammar, thus providing a
complete algorithm for learning a structurally unambiguous probabilistic
context free grammar (both the grammar topology and the probabilistic weights)
using structured membership queries and structured equivalence queries. A
summarized version of this work was published at AAAI 21.",None,-1
Realistic Synthetic Social Networks with Graph Neural Networks,0.00631613,"Social network analysis faces profound difficulties in sharing data between
researchers due to privacy and security concerns. A potential remedy to this
issue are synthetic networks, that closely resemble their real counterparts,
but can be freely distributed. generating synthetic networks requires the
creation of network topologies that, in application, function as realistically
as possible. Widely applied models are currently rule-based and can struggle to
reproduce structural dynamics. Lead by recent developments in Graph Neural
Network (GNN) models for network generation we evaluate the potential of GNNs
for synthetic social networks. Our GNN use is specifically within a reasonable
use-case and includes empirical evaluation using Maximum Mean Discrepancy
(MMD). We include social network specific measurements which allow evaluation
of how realistically synthetic networks behave in typical social network
analysis applications.
  We find that the Gated Recurrent Attention Network (GRAN) extends well to
social networks, and in comparison to a benchmark popular rule-based generation
Recursive-MATrix (R-MAT) method, is better able to replicate realistic
structural dynamics. We find that GRAN is more computationally costly than
R-MAT, but is not excessively costly to employ, so would be effective for
researchers seeking to create datasets of synthetic social networks.",None,-1
Lighting (In)consistency of Paint by Text,0.129459,"Whereas generative adversarial networks are capable of synthesizing highly
realistic images of faces, cats, landscapes, or almost any other single
category, paint-by-text synthesis engines can -- from a single text prompt --
synthesize realistic images of seemingly endless categories with arbitrary
configurations and combinations. This powerful technology poses new challenges
to the photo-forensic community. Motivated by the fact that paint by text is
not based on explicit geometric or physical models, and the human visual
system's general insensitivity to lighting inconsistencies, we provide an
initial exploration of the lighting consistency of DALL-E-2 synthesized images
to determine if physics-based forensic analyses will prove fruitful in
detecting this new breed of synthetic media.",None,-1
Is Lip Region-of-Interest Sufficient for Lipreading?,0.14239,"Lip region-of-interest (ROI) is conventionally used for visual input in the
lipreading task. Few works have adopted the entire face as visual input because
lip-excluded parts of the face are usually considered to be redundant and
irrelevant to visual speech recognition. However, faces contain much more
detailed information than lips, such as speakers' head pose, emotion, identity
etc. We argue that such information might benefit visual speech recognition if
a powerful feature extractor employing the entire face is trained. In this
work, we propose to adopt the entire face for lipreading with self-supervised
learning. AV-HuBERT, an audio-visual multi-modal self-supervised learning
framework, was adopted in our experiments. Our experimental results showed that
adopting the entire face achieved 16% relative word error rate (WER) reduction
on the lipreading task, compared with the baseline method using lip as visual
input. Without self-supervised pretraining, the model with face input achieved
a higher WER than that using lip input in the case of limited training data (30
hours), while a slightly lower WER when using large amount of training data
(433 hours).",None,-1
Improving Robustness to Out-of-Distribution Data by Frequency-based Augmentation,0.154647,"Although Convolutional Neural Networks (CNNs) have high accuracy in image
recognition, they are vulnerable to adversarial examples and
out-of-distribution data, and the difference from human recognition has been
pointed out. In order to improve the robustness against out-of-distribution
data, we present a frequency-based data augmentation technique that replaces
the frequency components with other images of the same class. When the training
data are CIFAR10 and the out-of-distribution data are SVHN, the Area Under
Receiver Operating Characteristic (AUROC) curve of the model trained with the
proposed method increases from 89.22\% to 98.15\%, and further increased to
98.59\% when combined with another data augmentation method. Furthermore, we
experimentally demonstrate that the robust model for out-of-distribution data
uses a lot of high-frequency components of the image.",None,-1
Prediction of Seismic Intensity Distributions Using Neural Networks,0.0165275,"The ground motion prediction equation is commonly used to predict the seismic
intensity distribution. However, it is not easy to apply this method to seismic
distributions affected by underground plate structures, which are commonly
known as abnormal seismic distributions. This study proposes a hybrid of
regression and classification approaches using neural networks. The proposed
model treats the distributions as 2-dimensional data like an image. Our method
can accurately predict seismic intensity distributions, even abnormal
distributions.",None,-1
Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem,0.0825292,"Math word problem solver requires both precise relation reasoning about
quantities in the text and reliable generation for the diverse equation.
Current sequence-to-tree or relation extraction methods regard this only from a
fixed view, struggling to simultaneously handle complex semantics and diverse
equations. However, human solving naturally involves two consistent reasoning
views: top-down and bottom-up, just as math equations also can be expressed in
multiple equivalent forms: pre-order and post-order. We propose a multi-view
consistent contrastive learning for a more complete semantics-to-equation
mapping. The entire process is decoupled into two independent but consistent
views: top-down decomposition and bottom-up construction, and the two reasoning
views are aligned in multi-granularity for consistency, enhancing global
generation and precise reasoning. Experiments on multiple datasets across two
languages show our approach significantly outperforms the existing baselines,
especially on complex problems. We also show after consistent alignment,
multi-view can absorb the merits of both views and generate more diverse
results consistent with the mathematical laws.",https://github.com/zwq2018/,2217
IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty,0.343075,"Single image surface normal estimation and depth estimation are closely
related problems as the former can be calculated from the latter. However, the
surface normals computed from the output of depth estimation methods are
significantly less accurate than the surface normals directly estimated by
networks. To reduce such discrepancy, we introduce a novel framework that uses
surface normal and its uncertainty to recurrently refine the predicted
depth-map. The depth of each pixel can be propagated to a query pixel, using
the predicted surface normal as guidance. We thus formulate depth refinement as
a classification of choosing the neighboring pixel to propagate from. Then, by
propagating to sub-pixel points, we upsample the refined, low-resolution
output. The proposed method shows state-of-the-art performance on NYUv2 and
iBims-1 - both in terms of depth and normal. Our refinement module can also be
attached to the existing depth estimation methods to improve their accuracy. We
also show that our framework, only trained for depth estimation, can also be
used for depth completion. The code is available at
https://github.com/baegwangbin/IronDepth.",https://github.com/baegwangbin/IronDepth,68183
Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness,0.355797,"With Artificial intelligence (AI) to aid or automate decision-making
advancing rapidly, a particular concern is its fairness. In order to create
reliable, safe and trustworthy systems through human-centred artificial
intelligence (HCAI) design, recent efforts have produced user interfaces (UIs)
for AI experts to investigate the fairness of AI models. In this work, we
provide a design space exploration that supports not only data scientists but
also domain experts to investigate AI fairness. Using loan applications as an
example, we held a series of workshops with loan officers and data scientists
to elicit their requirements. We instantiated these requirements into FairHIL,
a UI to support human-in-the-loop fairness investigations, and describe how
this UI could be generalized to other use cases. We evaluated FairHIL through a
think-aloud user study. Our work contributes better designs to investigate an
AI model's fairness-and move closer towards responsible AI.",None,758
Learning Visual Explanations for DCNN-Based Image Classifiers Using an Attention Mechanism,0.0101659,"In this paper two new learning-based eXplainable AI (XAI) methods for deep
convolutional neural network (DCNN) image classifiers, called L-CAM-Fm and
L-CAM-Img, are proposed. Both methods use an attention mechanism that is
inserted in the original (frozen) DCNN and is trained to derive class
activation maps (CAMs) from the last convolutional layer's feature maps. During
training, CAMs are applied to the feature maps (L-CAM-Fm) or the input image
(L-CAM-Img) forcing the attention mechanism to learn the image regions
explaining the DCNN's outcome. Experimental evaluation on ImageNet shows that
the proposed methods achieve competitive results while requiring a single
forward pass at the inference stage. Moreover, based on the derived
explanations a comprehensive qualitative analysis is performed providing
valuable insight for understanding the reasons behind classification errors,
including possible dataset biases affecting the trained classifier.",https://github.com/eclique/RISE,6033
In-Vehicle Interface Adaptation to Environment-Induced Cognitive Workload,0.0412657,"Many car accidents are caused by human distractions, including cognitive
distractions. In-vehicle human-machine interfaces (HMIs) have evolved
throughout the years, providing more and more functions. Interaction with the
HMIs can, however, also lead to further distractions and, as a consequence,
accidents. To tackle this problem, we propose using adaptive HMIs that change
according to the mental workload of the driver. In this work, we present the
current status as well as preliminary results of a user study using
naturalistic secondary tasks while driving (i.e., the primary task) that
attempt to understand the effects of one such interface.",None,67
One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones,0.575011,"We study the problem of developing autonomous agents that can follow human
instructions to infer and perform a sequence of actions to complete the
underlying task. Significant progress has been made in recent years, especially
for tasks with short horizons. However, when it comes to long-horizon tasks
with extended sequences of actions, an agent can easily ignore some
instructions or get stuck in the middle of the long instructions and eventually
fail the task. To address this challenge, we propose a model-agnostic
milestone-based task tracker (M-TRACK) to guide the agent and monitor its
progress. Specifically, we propose a milestone builder that tags the
instructions with navigation and interaction milestones which the agent needs
to complete step by step, and a milestone checker that systemically checks the
agent's progress in its current milestone and determines when to proceed to the
next. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and
52% relative improvement in unseen success rate over two competitive base
models.",https://github.com/chanhee-luke/M-Track,8346
A two-step approach to leverage contextual data: speech recognition in air-traffic communications,0.0402931,"Automatic Speech Recognition (ASR), as the assistance of speech communication
between pilots and air-traffic controllers, can significantly reduce the
complexity of the task and increase the reliability of transmitted information.
ASR application can lead to a lower number of incidents caused by
misunderstanding and improve air traffic management (ATM) efficiency.
Evidently, high accuracy predictions, especially, of key information, i.e.,
callsigns and commands, are required to minimize the risk of errors. We prove
that combining the benefits of ASR and Natural Language Processing (NLP)
methods to make use of surveillance data (i.e. additional modality) helps to
considerably improve the recognition of callsigns (named entity). In this
paper, we investigate a two-step callsign boosting approach: (1) at the 1 step
(ASR), weights of probable callsign n-grams are reduced in G.fst and/or in the
decoding FST (lattices), (2) at the 2 step (NLP), callsigns extracted from the
improved recognition outputs with Named Entity Recognition (NER) are correlated
with the surveillance data to select the most suitable one. Boosting callsign
n-grams with the combination of ASR and NLP methods eventually leads up to
53.7% of an absolute, or 60.4% of a relative, improvement in callsign
recognition.",None,10700
Pretrained Domain-Specific Language Model for General Information Retrieval Tasks in the AEC Domain,0.0512436,"As an essential task for the architecture, engineering, and construction
(AEC) industry, information retrieval (IR) from unstructured textual data based
on natural language processing (NLP) is gaining increasing attention. Although
various deep learning (DL) models for IR tasks have been investigated in the
AEC domain, it is still unclear how domain corpora and domain-specific
pretrained DL models can improve performance in various IR tasks. To this end,
this work systematically explores the impacts of domain corpora and various
transfer learning techniques on the performance of DL models for IR tasks and
proposes a pretrained domain-specific language model for the AEC domain. First,
both in-domain and close-domain corpora are developed. Then, two types of
pretrained models, including traditional wording embedding models and
BERT-based models, are pretrained based on various domain corpora and transfer
learning strategies. Finally, several widely used DL models for IR tasks are
further trained and tested based on various configurations and pretrained
models. The result shows that domain corpora have opposite effects on
traditional word embedding models for text classification and named entity
recognition tasks but can further improve the performance of BERT-based models
in all tasks. Meanwhile, BERT-based models dramatically outperform traditional
methods in all IR tasks, with maximum improvements of 5.4% and 10.1% in the F1
score, respectively. This research contributes to the body of knowledge in two
ways: 1) demonstrating the advantages of domain corpora and pretrained DL
models and 2) opening the first domain-specific dataset and pretrained language
model for the AEC domain, to the best of our knowledge. Thus, this work sheds
light on the adoption and application of pretrained models in the AEC domain.",None,1701
An Item Response Theory Framework for Persuasion,0.0,"In this paper, we apply Item Response Theory, popular in education and
political science research, to the analysis of argument persuasiveness in
language. We empirically evaluate the model's performance on three datasets,
including a novel dataset in the area of political advocacy. We show the
advantages of separating these components under several style and content
representations, including evaluating the ability of the speaker embeddings
generated by the model to parallel real-world observations about
persuadability.",https://github.com/akornilo/IRT_Persuasion,266
Joint Learning Content and Degradation Aware Feature for Blind Super-Resolution,0.0413059,"To achieve promising results on blind image super-resolution (SR), some
attempts leveraged the low resolution (LR) images to predict the kernel and
improve the SR performance. However, these Supervised Kernel Prediction (SKP)
methods are impractical due to the unavailable real-world blur kernels.
Although some Unsupervised Degradation Prediction (UDP) methods are proposed to
bypass this problem, the \textit{inconsistency} between degradation embedding
and SR feature is still challenging. By exploring the correlations between
degradation embedding and SR feature, we observe that jointly learning the
content and degradation aware feature is optimal. Based on this observation, a
Content and Degradation aware SR Network dubbed CDSR is proposed. Specifically,
CDSR contains three newly-established modules: (1) a Lightweight Patch-based
Encoder (LPE) is applied to jointly extract content and degradation features;
(2) a Domain Query Attention based module (DQA) is employed to adaptively
reduce the inconsistency; (3) a Codebook-based Space Compress module (CSC) that
can suppress the redundant information. Extensive experiments on several
benchmarks demonstrate that the proposed CDSR outperforms the existing UDP
models and achieves competitive performance on PSNR and SSIM even compared with
the state-of-the-art SKP methods.",None,11238
Deep Model-Based Super-Resolution with Non-uniform Blur,0.055798,"We propose a state-of-the-art method for super-resolution with non-uniform
blur. Single-image super-resolution methods seek to restore a high-resolution
image from blurred, subsampled, and noisy measurements. Despite their
impressive performance, existing techniques usually assume a uniform blur
kernel. Hence, these techniques do not generalize well to the more general case
of non-uniform blur. Instead, in this paper, we address the more realistic and
computationally challenging case of spatially-varying blur. To this end, we
first propose a fast deep plug-and-play algorithm, based on linearized ADMM
splitting techniques, which can solve the super-resolution problem with
spatially-varying blur. Second, we unfold our iterative algorithm into a single
network and train it end-to-end. In this way, we overcome the intricacy of
manually tuning the parameters involved in the optimization scheme. Our
algorithm presents remarkable performance and generalizes well after a single
training to a large family of spatially-varying blur kernels, noise levels and
scale factors.",https://github.com/claroche-r/DMBSR,471
Mathematical Cookbook for Snapshot Compressive Imaging,0.0734141,"The author intends to provide you with a beautiful, elegant, user-friendly
cookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the
cookbook is composed of introduction, conventional optimization, and deep
equilibrium models. The latest releases are strongly recommended! For any other
questions, suggestions, or comments, feel free to email the author.",None,120
Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance,0.306752,"Denoising diffusion probabilistic models (DDPMs) are a recent family of
generative models that achieve state-of-the-art results. In order to obtain
class-conditional generation, it was suggested to guide the diffusion process
by gradients from a time-dependent classifier. While the idea is theoretically
sound, deep learning-based classifiers are infamously susceptible to
gradient-based adversarial attacks. Therefore, while traditional classifiers
may achieve good accuracy scores, their gradients are possibly unreliable and
might hinder the improvement of the generation results. Recent work discovered
that adversarially robust classifiers exhibit gradients that are aligned with
human perception, and these could better guide a generative process towards
semantically meaningful images. We utilize this observation by defining and
training a time-dependent adversarially robust classifier and use it as
guidance for a generative diffusion model. In experiments on the highly
challenging and diverse ImageNet dataset, our scheme introduces significantly
more intelligible intermediate gradients, better alignment with theoretical
findings, as well as improved generation results under several evaluation
metrics. Furthermore, we conduct an opinion survey whose findings indicate that
human raters prefer our method's results.",https://github.com/bahjat-kawar/enhancing-diffusion-robust,82994
Dreamento: an open-source dream engineering toolbox for sleep EEG wearables,0.022995,"We introduce Dreamento (Dream engineering toolbox), an open-source Python
package for dream engineering using sleep electroencephalography (EEG)
wearables. Dreamento main functions are (1) real-time recording, monitoring,
analysis, and sensory stimulation, and (2) offline post-processing of the
resulting data, both in a graphical user interface (GUI). In real-time,
Dreamento is capable of (1) data recording, visualization, and navigation, (2)
power-spectrum analysis, (3) automatic sleep scoring, (4) sensory stimulation
(visual, auditory, tactile), (5) establishing text-to-speech communication, and
(6) managing annotations of automatic and manual events. The offline functions
aid in post-processing the acquired data with features to reformat the wearable
data and integrate it with non-wearable recorded modalities such as
electromyography (EMG). While Dreamento was primarily developed for (lucid)
dreaming studies, its applications can be extended to other areas of sleep
research such as closed-loop auditory stimulation and targeted memory
reactivation.",https://github.com/dreamento/dreamento,7176
WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation,0.683889,"Existing few-shot image generation approaches typically employ fusion-based
strategies, either on the image or the feature level, to produce new images.
However, previous approaches struggle to synthesize high-frequency signals with
fine details, deteriorating the synthesis quality. To address this, we propose
WaveGAN, a frequency-aware model for few-shot image generation. Concretely, we
disentangle encoded features into multiple frequency components and perform
low-frequency skip connections to preserve outline and structural information.
Then we alleviate the generator's struggles of synthesizing fine details by
employing high-frequency skip connections, thus providing informative frequency
information to the generator. Moreover, we utilize a frequency L1-loss on the
generated and real images to further impede frequency information loss.
Extensive experiments demonstrate the effectiveness and advancement of our
method on three datasets. Noticeably, we achieve new state-of-the-art with FID
42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822
respectively on Flower, Animal Faces, and VGGFace. GitHub:
https://github.com/kobeshegu/ECCV2022_WaveGAN",https://github.com/kobeshegu/ECCV2022_WaveGAN,2755
"From Perception to Programs: Regularize, Overparameterize, and Amortize",0.0758003,"Toward combining inductive reasoning with perception abilities, we develop
techniques for neurosymbolic program synthesis where perceptual input is first
parsed by neural nets into a low-dimensional interpretable representation,
which is then processed by a synthesized program. We explore several techniques
for relaxing the problem and jointly learning all modules end-to-end with
gradient descent: multitask learning; amortized inference;
overparameterization; and a differentiable strategy for penalizing lengthy
programs. Collectedly this toolbox improves the stability of gradient-guided
program search, and suggests ways of learning both how to perceive input as
discrete abstractions, and how to symbolically process those abstractions as
programs.",None,1751
Exploring Diversity-based Active Learning for 3D Object Detection in Autonomous Driving,0.0260712,"3D object detection has recently received much attention due to its great
potential in autonomous vehicle (AV). The success of deep learning based object
detectors relies on the availability of large-scale annotated datasets, which
is time-consuming and expensive to compile, especially for 3D bounding box
annotation. In this work, we investigate diversity-based active learning (AL)
as a potential solution to alleviate the annotation burden. Given limited
annotation budget, only the most informative frames and objects are
automatically selected for human to annotate. Technically, we take the
advantage of the multimodal information provided in an AV dataset, and propose
a novel acquisition function that enforces spatial and temporal diversity in
the selected samples. We benchmark the proposed method against other AL
strategies under realistic annotation cost measurement, where the realistic
costs for annotating a frame and a 3D bounding box are both taken into
consideration. We demonstrate the effectiveness of the proposed method on the
nuScenes dataset and show that it outperforms existing AL strategies
significantly.",https://github.com/poodarchu/Det3D,1317
longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks,0.0287271,"Developing methods to adversarially challenge NLP systems is a promising
avenue for improving both model performance and interpretability. Here, we
describe the approach of the team ""longhorns"" on Task 1 of the The First
Workshop on Dynamic Adversarial Data Collection (DADC), which asked teams to
manually fool a model on an Extractive Question Answering task. Our team
finished first, with a model error rate of 62%. We advocate for a systematic,
linguistically informed approach to formulating adversarial questions, and we
describe the results of our pilot experiments, as well as our official
submission.",None,5637
On Improving Cross-dataset Generalization of Deepfake Detectors,0.303505,"Facial manipulation by deep fake has caused major security risks and raised
severe societal concerns. As a countermeasure, a number of deep fake detection
methods have been proposed recently. Most of them model deep fake detection as
a binary classification problem using a backbone convolutional neural network
(CNN) architecture pretrained for the task. These CNN-based methods have
demonstrated very high efficacy in deep fake detection with the Area under the
Curve (AUC) as high as 0.99. However, the performance of these methods degrades
significantly when evaluated across datasets. In this paper, we formulate deep
fake detection as a hybrid combination of supervised and reinforcement learning
(RL) to improve its cross-dataset generalization performance. The proposed
method chooses the top-k augmentations for each test sample by an RL agent in
an image-specific manner. The classification scores, obtained using CNN, of all
the augmentations of each test image are averaged together for final real or
fake classification. Through extensive experimental validation, we demonstrate
the superiority of our method over existing published research in cross-dataset
generalization of deep fake detectors, thus obtaining state-of-the-art
performance.",None,3040
Detecting Driver Drowsiness as an Anomaly Using LSTM Autoencoders,0.0114765,"In this paper, an LSTM autoencoder-based architecture is utilized for
drowsiness detection with ResNet-34 as feature extractor. The problem is
considered as anomaly detection for a single subject; therefore, only the
normal driving representations are learned and it is expected that drowsiness
representations, yielding higher reconstruction losses, are to be distinguished
according to the knowledge of the network. In our study, the confidence levels
of normal and anomaly clips are investigated through the methodology of label
assignment such that training performance of LSTM autoencoder and
interpretation of anomalies encountered during testing are analyzed under
varying confidence rates. Our method is experimented on NTHU-DDD and
benchmarked with a state-of-the-art anomaly detection method for driver
drowsiness. Results show that the proposed model achieves detection rate of
0.8740 area under curve (AUC) and is able to provide significant improvements
on certain scenarios.",None,2076
Keypoint Cascade Voting for Point Cloud Based 6DoF Pose Estimation,0.220729,"We propose a novel keypoint voting 6DoF object pose estimation method, which
takes pure unordered point cloud geometry as input without RGB information. The
proposed cascaded keypoint voting method, called RCVPose3D, is based upon a
novel architecture which separates the task of semantic segmentation from that
of keypoint regression, thereby increasing the effectiveness of both and
improving the ultimate performance. The method also introduces a pairwise
constraint in between different keypoints to the loss function when regressing
the quantity for keypoint estimation, which is shown to be effective, as well
as a novel Voter Confident Score which enhances both the learning and inference
stages. Our proposed RCVPose3D achieves state-of-the-art performance on the
Occlusion LINEMOD (74.5%) and YCB-Video (96.9%) datasets, outperforming
existing pure RGB and RGB-D based methods, as well as being competitive with
RGB plus point cloud methods.",https://github.com/aaronWool/rcvpose3d,3559
Argus++: Robust Real-time Activity Detection for Unconstrained Video Streams with Overlapping Cube Proposals,0.0434839,"Activity detection is one of the attractive computer vision tasks to exploit
the video streams captured by widely installed cameras. Although achieving
impressive performance, conventional activity detection algorithms are usually
designed under certain constraints, such as using trimmed and/or
object-centered video clips as inputs. Therefore, they failed to deal with the
multi-scale multi-instance cases in real-world unconstrained video streams,
which are untrimmed and have large field-of-views. Real-time requirements for
streaming analysis also mark brute force expansion of them unfeasible.
  To overcome these issues, we propose Argus++, a robust real-time activity
detection system for analyzing unconstrained video streams. The design of
Argus++ introduces overlapping spatio-temporal cubes as an intermediate concept
of activity proposals to ensure coverage and completeness of activity detection
through over-sampling. The overall system is optimized for real-time processing
on standalone consumer-level hardware. Extensive experiments on different
surveillance and driving scenarios demonstrated its superior performance in a
series of activity detection benchmarks, including CVPR ActivityNet ActEV 2021,
NIST ActEV SDL UF/KF, TRECVID ActEV 2020/2021, and ICCV ROAD 2021.",None,233
Prompt Consistency for Zero-Shot Task Generalization,0.434231,"One of the most impressive results of recent NLP history is the ability of
pre-trained language models to solve new tasks in a zero-shot setting. To
achieve this, NLP tasks are framed as natural language prompts, generating a
response indicating the predicted output. Nonetheless, the performance in such
settings often lags far behind its supervised counterpart, suggesting a large
space for potential improvement. In this paper, we explore methods to utilize
unlabeled data to improve zero-shot performance. Specifically, we take
advantage of the fact that multiple prompts can be used to specify a single
task, and propose to regularize prompt consistency, encouraging consistent
predictions over this diverse set of prompts. Our method makes it possible to
fine-tune the model either with extra unlabeled training data, or directly on
test input at inference time in an unsupervised manner. In experiments, our
approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,
2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points
in terms of accuracy. The gains are often attained with a small number of
unlabeled examples.",https://github.com/violet-zct/swarm-distillation-zero-shot,33484
DeepStruct: Pretraining of Language Models for Structure Prediction,0.0610388,"We introduce a method for improving the structural understanding abilities of
language models. Unlike previous approaches that finetune the models with
task-specific augmentation, we pretrain language models on a collection of
task-agnostic corpora to generate structures from text. Our structure
pretraining enables zero-shot transfer of the learned knowledge that models
have about the structure tasks. We study the performance of this approach on 28
datasets, spanning 10 structure prediction tasks including open information
extraction, joint entity and relation extraction, named entity recognition,
relation classification, semantic role labeling, event extraction, coreference
resolution, factual probe, intent detection, and dialogue state tracking. We
further enhance the pretraining with the task-specific training sets. We show
that a 10B parameter language model transfers non-trivially to most tasks and
obtains state-of-the-art performance on 21 of 28 datasets that we evaluate.",https://github.com/cgraywang/deepstruct,40265
Qtrade AI at SemEval-2022 Task 11: An Unified Framework for Multilingual NER Task,0.0491661,"This paper describes our system, which placed third in the Multilingual Track
(subtask 11), fourth in the Code-Mixed Track (subtask 12), and seventh in the
Chinese Track (subtask 9) in the SemEval 2022 Task 11: MultiCoNER Multilingual
Complex Named Entity Recognition. Our system's key contributions are as
follows: 1) For multilingual NER tasks, we offer an unified framework with
which one can easily execute single-language or multilingual NER tasks, 2) for
low-resource code-mixed NER task, one can easily enhance his or her dataset
through implementing several simple data augmentation methods and 3) for
Chinese tasks, we propose a model that can capture Chinese lexical semantic,
lexical border, and lexical graph structural information. Finally, our system
achieves macro-f1 scores of 77.66, 84.35, and 74.00 on subtasks 11, 12, and 9,
respectively, during the testing phase.",None,42
A Human-Centric Assessment Framework for AI,0.153976,"With the rise of AI systems in real-world applications comes the need for
reliable and trustworthy AI. An essential aspect of this are explainable AI
systems. However, there is no agreed standard on how explainable AI systems
should be assessed. Inspired by the Turing test, we introduce a human-centric
assessment framework where a leading domain expert accepts or rejects the
solutions of an AI system and another domain expert. By comparing the
acceptance rates of provided solutions, we can assess how the AI system
performs compared to the domain expert, and whether the AI system's
explanations (if provided) are human-understandable. This setup -- comparable
to the Turing test -- can serve as a framework for a wide range of
human-centric AI system assessments. We demonstrate this by presenting two
instantiations: (1) an assessment that measures the classification accuracy of
a system with the option to incorporate label uncertainties; (2) an assessment
where the usefulness of provided explanations is determined in a human-centric
manner.",None,1665
Modern Machine-Learning Predictive Models for Diagnosing Infectious Diseases,0.300479,"Controlling infectious diseases is a major health priority because they can
spread and infect humans, thus evolving into epidemics or pandemics. Therefore,
early detection of infectious diseases is a significant need, and many
researchers have developed models to diagnose them in the early stages. This
paper reviewed research articles for recent machine-learning (ML) algorithms
applied to infectious disease diagnosis. We searched the Web of Science,
ScienceDirect, PubMed, Springer, and IEEE databases from 2015 to 2022,
identified the pros and cons of the reviewed ML models, and discussed the
possible recommendations to advance the studies in this field. We found that
most of the articles used small datasets, and few of them used real-time data.
Our results demonstrated that a suitable ML technique depends on the nature of
the dataset and the desired goal.",None,3070
A Robust Learning Methodology for Uncertainty-aware Scientific Machine Learning models,0.0667601,"Robust learning is an important issue in Scientific Machine Learning (SciML).
There are several works in the literature addressing this topic. However, there
is an increasing demand for methods that can simultaneously consider all the
different uncertainty components involved in SciML model identification. Hence,
this work proposes a comprehensive methodology for uncertainty evaluation of
the SciML that also considers several possible sources of uncertainties
involved in the identification process. The uncertainties considered in the
proposed method are the absence of theory and causal models, the sensitiveness
to data corruption or imperfection, and the computational effort. Therefore, it
was possible to provide an overall strategy for the uncertainty-aware models in
the SciML field. The methodology is validated through a case study, developing
a Soft Sensor for a polymerization reactor. The results demonstrated that the
identified Soft Sensor are robust for uncertainties, corroborating with the
consistency of the proposed approach.",None,712
ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer,0.113318,"The problem of multimodal intent and trajectory prediction for human-driven
vehicles in parking lots is addressed in this paper. Using models designed with
CNN and Transformer networks, we extract temporal-spatial and contextual
information from trajectory history and local bird's eye view (BEV) semantic
images, and generate predictions about intent distribution and future
trajectory sequences. Our methods outperform existing models in accuracy, while
allowing an arbitrary number of modes, encoding complex multi-agent scenarios,
and adapting to different parking maps. To train and evaluate our method, we
present the first public 4K video dataset of human driving in parking lots with
accurate annotation, high frame rate, and rich traffic scenarios.",https://github.com/XuShenLZ/ParkSim/,29242
Mixture of Input-Output Hidden Markov Models for Heterogeneous Disease Progression Modeling,0.0707383,"A particular challenge for disease progression modeling is the heterogeneity
of a disease and its manifestations in the patients. Existing approaches often
assume the presence of a single disease progression characteristics which is
unlikely for neurodegenerative disorders such as Parkinson's disease. In this
paper, we propose a hierarchical time-series model that can discover multiple
disease progression dynamics. The proposed model is an extension of an
input-output hidden Markov model that takes into account the clinical
assessments of patients' health status and prescribed medications. We
illustrate the benefits of our model using a synthetically generated dataset
and a real-world longitudinal dataset for Parkinson's disease.",https://github.com/tahaceritli/mIOHMM,17554
Learning First-Order Symbolic Planning Representations That Are Grounded,0.0818474,"Two main approaches have been developed for learning first-order planning
(action) models from unstructured data: combinatorial approaches that yield
crisp action schemas from the structure of the state space, and deep learning
approaches that produce action schemas from states represented by images. A
benefit of the former approach is that the learned action schemas are similar
to those that can be written by hand; a benefit of the latter is that the
learned representations (predicates) are grounded on the images, and as a
result, new instances can be given in terms of images. In this work, we develop
a new formulation for learning crisp first-order planning models that are
grounded on parsed images, a step to combine the benefits of the two
approaches. Parsed images are assumed to be given in a simple O2D language
(objects in 2D) that involves a small number of unary and binary predicates
like ""left"", ""above"", ""shape"", etc. After learning, new planning instances can
be given in terms of pairs of parsed images, one for the initial situation and
the other for the goal. Learning and planning experiments are reported for
several domains including Blocks, Sokoban, IPC Grid, and Hanoi.",None,12924
Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian,0.236385,"We present the shared task on artificial text detection in Russian, which is
organized as a part of the Dialogue Evaluation initiative, held in 2022. The
shared task dataset includes texts from 14 text generators, i.e., one human
writer and 13 text generative models fine-tuned for one or more of the
following generation tasks: machine translation, paraphrase generation, text
summarization, text simplification. We also consider back-translation and
zero-shot generation approaches. The human-written texts are collected from
publicly available resources across multiple domains. The shared task consists
of two sub-tasks: (i) to determine if a given text is automatically generated
or written by a human; (ii) to identify the author of a given text. The first
task is framed as a binary classification problem. The second task is a
multi-class classification problem. We provide count-based and BERT-based
baselines, along with the human evaluation on the first sub-task. A total of 30
and 8 systems have been submitted to the binary and multi-class sub-tasks,
correspondingly. Most teams outperform the baselines by a wide margin. We
publicly release our codebase, human evaluation results, and other materials in
our GitHub repository (https://github.com/dialogue-evaluation/RuATD).",https://github.com/dialogue-evaluation/RuATD,2091
Multi-Scale Representation Learning on Proteins,0.311931,"Proteins are fundamental biological entities mediating key roles in cellular
function and disease. This paper introduces a multi-scale graph construction of
a protein -- HoloProt -- connecting surface to structure and sequence. The
surface captures coarser details of the protein, while sequence as primary
component and structure -- comprising secondary and tertiary components --
capture finer details. Our graph encoder then learns a multi-scale
representation by allowing each level to integrate the encoding from level(s)
below with the graph at that level. We test the learned representation on
different tasks, (i.) ligand binding affinity (regression), and (ii.) protein
function prediction (classification). On the regression task, contrary to
previous methods, our model performs consistently and reliably across different
dataset splits, outperforming all baselines on most splits. On the
classification task, it achieves a performance close to the top-performing
model while using 10x fewer parameters. To improve the memory efficiency of our
construction, we segment the multiplex protein surface manifold into molecular
superpixels and substitute the surface with these superpixels at little to no
performance loss.",https://github.com/rdkit/rdkit/releases/tag/Release_2016_09_4,925
Training Naturalized Semantic Parsers with Very Little Data,0.0442042,"Semantic parsing is an important NLP problem, particularly for voice
assistants such as Alexa and Google Assistant. State-of-the-art (SOTA) semantic
parsers are seq2seq architectures based on large language models that have been
pretrained on vast amounts of text. To better leverage that pretraining, recent
work has explored a reformulation of semantic parsing whereby the output
sequences are themselves natural language sentences, but in a controlled
fragment of natural language. This approach delivers strong results,
particularly for few-shot semantic parsing, which is of key importance in
practice and the focus of our paper. We push this line of work forward by
introducing an automated methodology that delivers very significant additional
improvements by utilizing modest amounts of unannotated data, which is
typically easy to obtain. Our method is based on a novel synthesis of four
techniques: joint training with auxiliary unsupervised tasks; constrained
decoding; self-training; and paraphrasing. We show that this method delivers
new SOTA few-shot performance on the Overnight dataset, particularly in very
low-resource settings, and very compelling few-shot results on a new semantic
parsing dataset.",https://github.com/amazon-research/pizza-semantic-parsing-,3238
Design of High-Throughput Mixed-Precision CNN Accelerators on FPGA,0.069347,"Convolutional Neural Networks (CNNs) reach high accuracies in various
application domains, but require large amounts of computation and incur costly
data movements. One method to decrease these costs while trading accuracy is
weight and/or activation word-length reduction. Thereby, layer-wise
mixed-precision quantization allows for more efficient results while inflating
the design space. In this work, we present an in-depth quantitative methodology
to efficiently explore the design space considering the limited hardware
resources of a given FPGA. Our holistic exploration approach vertically
traverses the various design entry levels from the architectural down to the
logic level, and laterally covers optimization from processing elements to
dataflow for an efficient mixed-precision CNN accelerator. Our resulting
hardware accelerators implement truly mixed-precision operations that enable
efficient execution of layer-wise and channel-wise quantized CNNs. Mapping
feed-forward and identity-shortcut-connection mixed-precision CNNs result in
competitive accuracy-throughout trade-offs: 245 frames/s with 87.48% Top-5
accuracy for ResNet-18 and 92.9% Top-5 accuracy with 1.13 TOps/s for
ResNet-152, respectively. Thereby, the required memory footprint for parameters
is reduced by 4.9x and 9.4x compared to the respective floating-point baseline.",None,699
Arabic Word-level Readability Visualization for Assisted Text Simplification,0.0454465,"This demo paper presents a Google Docs add-on for automatic Arabic word-level
readability visualization. The add-on includes a lemmatization component that
is connected to a five-level readability lexicon and Arabic WordNet-based
substitution suggestions. The add-on can be used for assessing the reading
difficulty of a text and identifying difficult words as part of the task of
manual text simplification. We make our add-on and its code publicly available.",None,16583
Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation,0.864665,"3D Human body pose and shape estimation within a temporal sequence can be
quite critical for understanding human behavior. Despite the significant
progress in human pose estimation in the recent years, which are often based on
single images or videos, human motion estimation on live stream videos is still
a rarely-touched area considering its special requirements for real-time output
and temporal consistency. To address this problem, we present a temporally
embedded 3D human body pose and shape estimation (TePose) method to improve the
accuracy and temporal consistency of pose estimation in live stream videos.
TePose uses previous predictions as a bridge to feedback the error for better
estimation in the current frame and to learn the correspondence between data
frames and predictions in the history. A multi-scale spatio-temporal graph
convolutional network is presented as the motion discriminator for adversarial
training using datasets without any 3D labeling. We propose a sequential data
loading strategy to meet the special start-to-end data processing requirement
of live stream. We demonstrate the importance of each proposed module with
extensive experiments. The results show the effectiveness of TePose on
widely-used human pose benchmarks with state-of-the-art performance.",https://github.com/ostadabbas/TePose,2464
Smart Speech Segmentation using Acousto-Linguistic Features with look-ahead,0.0433704,"Segmentation for continuous Automatic Speech Recognition (ASR) has
traditionally used silence timeouts or voice activity detectors (VADs), which
are both limited to acoustic features. This segmentation is often overly
aggressive, given that people naturally pause to think as they speak.
Consequently, segmentation happens mid-sentence, hindering both punctuation and
downstream tasks like machine translation for which high-quality segmentation
is critical. Model-based segmentation methods that leverage acoustic features
are powerful, but without an understanding of the language itself, these
approaches are limited. We present a hybrid approach that leverages both
acoustic and language information to improve segmentation. Furthermore, we show
that including one word as a look-ahead boosts segmentation quality. On
average, our models improve segmentation-F0.5 score by 9.8% over baseline. We
show that this approach works for multiple languages. For the downstream task
of machine translation, it improves the translation BLEU score by an average of
1.05 points.",None,753
InFIP: An Explainable DNN Intellectual Property Protection Method based on Intrinsic Features,0.0487837,"Intellectual property (IP) protection for Deep Neural Networks (DNNs) has
raised serious concerns in recent years. Most existing works embed watermarks
in the DNN model for IP protection, which need to modify the model and lack of
interpretability. In this paper, for the first time, we propose an
interpretable intellectual property protection method for DNN based on
explainable artificial intelligence. Compared with existing works, the proposed
method does not modify the DNN model, and the decision of the ownership
verification is interpretable. We extract the intrinsic features of the DNN
model by using Deep Taylor Decomposition. Since the intrinsic feature is
composed of unique interpretation of the model's decision, the intrinsic
feature can be regarded as fingerprint of the model. If the fingerprint of a
suspected model is the same as the original model, the suspected model is
considered as a pirated model. Experimental results demonstrate that the
fingerprints can be successfully used to verify the ownership of the model and
the test accuracy of the model is not affected. Furthermore, the proposed
method is robust to fine-tuning attack, pruning attack, watermark overwriting
attack, and adaptive attack.",None,11673
Gym-saturation: an OpenAI Gym environment for saturation provers,0.0248099,"`gym-saturation` is an OpenAI Gym environment for reinforcement learning (RL)
agents capable of proving theorems. Currently, only theorems written in a
formal language of the Thousands of Problems for Theorem Provers (TPTP) library
in clausal normal form (CNF) are supported. `gym-saturation` implements the
'given clause' algorithm (similar to the one used in Vampire and E Prover).
Being written in Python, `gym-saturation` was inspired by PyRes. In contrast to
the monolithic architecture of a typical Automated Theorem Prover (ATP),
`gym-saturation` gives different agents opportunities to select clauses
themselves and train from their experience. Combined with a particular agent,
`gym-saturation` can work as an ATP. Even with a non trained agent based on
heuristics, `gym-saturation` can find refutations for 688 (of 8257) CNF
problems from TPTP v7.5.0.",None,20
Transformer based Urdu Handwritten Text Optical Character Reader,0.297542,"Extracting Handwritten text is one of the most important components of
digitizing information and making it available for large scale setting.
Handwriting Optical Character Reader (OCR) is a research problem in computer
vision and natural language processing computing, and a lot of work has been
done for English, but unfortunately, very little work has been done for low
resourced languages such as Urdu. Urdu language script is very difficult
because of its cursive nature and change of shape of characters based on it's
relative position, therefore, a need arises to propose a model which can
understand complex features and generalize it for every kind of handwriting
style. In this work, we propose a transformer based Urdu Handwritten text
extraction model. As transformers have been very successful in Natural Language
Understanding task, we explore them further to understand complex Urdu
Handwriting.",None,272
Unifying Approaches in Active Learning and Active Sampling via Fisher Information and Information-Theoretic Quantities,0.0237294,"Recently proposed methods in data subset selection, that is active learning
and active sampling, use Fisher information, Hessians, similarity matrices
based on gradients, and gradient lengths to estimate how informative data is
for a model's training. Are these different approaches connected, and if so,
how? We revisit the fundamentals of Bayesian optimal experiment design and show
that these recently proposed methods can be understood as approximations to
information-theoretic quantities: among them, the mutual information between
predictions and model parameters, known as expected information gain or BALD in
machine learning, and the mutual information between predictions of acquisition
candidates and test samples, known as expected predictive information gain. We
develop a comprehensive set of approximations using Fisher information and
observed information and derive a unified framework that connects seemingly
disparate literature. Although Bayesian methods are often seen as separate from
non-Bayesian ones, the sometimes fuzzy notion of ""informativeness"" expressed in
various non-Bayesian objectives leads to the same couple of information
quantities, which were, in principle, already known by Lindley (1956) and
MacKay (1992).",None,-1
Dead or Murdered? Predicting Responsibility Perception in Femicide News Reports,0.0281695,"Different linguistic expressions can conceptualize the same event from
different viewpoints by emphasizing certain participants over others. Here, we
investigate a case where this has social consequences: how do linguistic
expressions of gender-based violence (GBV) influence who we perceive as
responsible? We build on previous psycholinguistic research in this area and
conduct a large-scale perception survey of GBV descriptions automatically
extracted from a corpus of Italian newspapers. We then train regression models
that predict the salience of GBV participants with respect to different
dimensions of perceived responsibility. Our best model (fine-tuned BERT) shows
solid overall performance, with large differences between dimensions and
participants: salient _focus_ is more predictable than salient _blame_, and
perpetrators' salience is more predictable than victims' salience. Experiments
with ridge regression models using different representations show that features
based on linguistic theory similarly to word-based features. Overall, we show
that different linguistic choices do trigger different perceptions of
responsibility, and that such perceptions can be modelled automatically. This
work can be a core instrument to raise awareness of the consequences of
different perspectivizations in the general public and in news producers alike.",https://gitlab.com/sociofillmore/perceived-perspective-prediction,92
e-CARE: a New Dataset for Exploring Explainable Causal Reasoning,0.311227,"Understanding causality has vital importance for various Natural Language
Processing (NLP) applications. Beyond the labeled instances, conceptual
explanations of the causality can provide deep understanding of the causal
facts to facilitate the causal reasoning process. However, such explanation
information still remains absent in existing causal reasoning resources. In
this paper, we fill this gap by presenting a human-annotated explainable CAusal
REasoning dataset (e-CARE), which contains over 21K causal reasoning questions,
together with natural language formed explanations of the causal questions.
Experimental results show that generating valid explanations for causal facts
still remains especially challenging for the state-of-the-art models, and the
explanation information can be helpful for promoting the accuracy and stability
of causal reasoning models.",https://github.com/Waste-Wood/e-CARE/,21012
A Dynamic Graph Interactive Framework with Label-Semantic Injection for Spoken Language Understanding,0.172606,"Multi-intent detection and slot filling joint models are gaining increasing
traction since they are closer to complicated real-world scenarios. However,
existing approaches (1) focus on identifying implicit correlations between
utterances and one-hot encoded labels in both tasks while ignoring explicit
label characteristics; (2) directly incorporate multi-intent information for
each token, which could lead to incorrect slot prediction due to the
introduction of irrelevant intent. In this paper, we propose a framework termed
DGIF, which first leverages the semantic information of labels to give the
model additional signals and enriched priors. Then, a multi-grain interactive
graph is constructed to model correlations between intents and slots.
Specifically, we propose a novel approach to construct the interactive graph
based on the injection of label semantics, which can automatically update the
graph to better alleviate error propagation. Experimental results show that our
framework significantly outperforms existing approaches, obtaining a relative
improvement of 13.7% over the previous best model on the MixATIS dataset in
overall accuracy.",https://github.com/Zhihong-Zhu/DGIF,7453
Read Top News First: A Document Reordering Approach for Multi-Document News Summarization,0.182287,"A common method for extractive multi-document news summarization is to
re-formulate it as a single-document summarization problem by concatenating all
documents as a single meta-document. However, this method neglects the relative
importance of documents. We propose a simple approach to reorder the documents
according to their relative importance before concatenating and summarizing
them. The reordering makes the salient content easier to learn by the
summarization model. Experiments show that our approach outperforms previous
state-of-the-art methods with more complex architectures.",https://github.com/zhaochaocs/MDS-DR,30354
Black-box Few-shot Knowledge Distillation,0.11866,"Knowledge distillation (KD) is an efficient approach to transfer the
knowledge from a large ""teacher"" network to a smaller ""student"" network.
Traditional KD methods require lots of labeled training samples and a white-box
teacher (parameters are accessible) to train a good student. However, these
resources are not always available in real-world applications. The distillation
process often happens at an external party side where we do not have access to
much data, and the teacher does not disclose its parameters due to security and
privacy concerns. To overcome these challenges, we propose a black-box few-shot
KD method to train the student with few unlabeled training samples and a
black-box teacher. Our main idea is to expand the training set by generating a
diverse set of out-of-distribution synthetic images using MixUp and a
conditional variational auto-encoder. These synthetic images along with their
labels obtained from the teacher are used to train the student. We conduct
extensive experiments to show that our method significantly outperforms recent
SOTA few/zero-shot KD methods on image classification tasks. The code and
models are available at: https://github.com/nphdang/FS-BBT",https://github.com/nphdang/FS-BBT,30421
Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model,0.0654332,"Contextual ASR, which takes a list of bias terms as input along with audio,
has drawn recent interest as ASR use becomes more widespread. We are releasing
contextual biasing lists to accompany the Earnings21 dataset, creating a public
benchmark for this task. We present baseline results on this benchmark using a
pretrained end-to-end ASR model from the WeNet toolkit. We show results for
shallow fusion contextual biasing applied to two different decoding algorithms.
Our baseline results confirm observations that end-to-end models struggle in
particular with words that are rarely or never seen during training, and that
existing shallow fusion techniques do not adequately address this problem. We
propose an alternate spelling prediction model that improves recall of rare
words by 34.7% relative and of out-of-vocabulary words by 97.2% relative,
compared to contextual biasing without alternate spellings. This model is
conceptually similar to ones used in prior work, but is simpler to implement as
it does not rely on either a pronunciation dictionary or an existing
text-to-speech system.",https://github.com/revdotcom/speech-datasets/tree/main/earnings21,143
LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling,0.138399,"Compared with traditional task-irrelevant downsampling methods, task-oriented
neural networks have shown improved performance in point cloud downsampling
range. Recently, Transformer family of networks has shown a more powerful
learning capacity in visual tasks. However, Transformer-based architectures
potentially consume too many resources which are usually worthless for low
overhead task networks in downsampling range. This paper proposes a novel
light-weight Transformer network (LighTN) for task-oriented point cloud
downsampling, as an end-to-end and plug-and-play solution. In LighTN, a
single-head self-correlation module is presented to extract refined global
contextual features, where three projection matrices are simultaneously
eliminated to save resource overhead, and the output of symmetric matrix
satisfies the permutation invariant. Then, we design a novel downsampling loss
function to guide LighTN focuses on critical point cloud regions with more
uniform distribution and prominent points coverage. Furthermore, We introduce a
feed-forward network scaling mechanism to enhance the learnable capacity of
LighTN according to the expand-reduce strategy. The result of extensive
experiments on classification and registration tasks demonstrates LighTN can
achieve state-of-the-art performance with limited resource overhead.",None,14870
TetGAN: A Convolutional Neural Network for Tetrahedral Mesh Generation,0.161524,"We present TetGAN, a convolutional neural network designed to generate
tetrahedral meshes. We represent shapes using an irregular tetrahedral grid
which encodes an occupancy and displacement field. Our formulation enables
defining tetrahedral convolution, pooling, and upsampling operations to
synthesize explicit mesh connectivity with variable topological genus. The
proposed neural network layers learn deep features over each tetrahedron and
learn to extract patterns within spatial regions across multiple scales. We
illustrate the capabilities of our technique to encode tetrahedral meshes into
a semantically meaningful latent-space which can be used for shape editing and
synthesis. Our project page is at https://threedle.github.io/tetGAN/.",https://threedle.github.io/tetGAN/,3863
Hierarchical Semi-Supervised Contrastive Learning for Contamination-Resistant Anomaly Detection,0.0902364,"Anomaly detection aims at identifying deviant samples from the normal data
distribution. Contrastive learning has provided a successful way to sample
representation that enables effective discrimination on anomalies. However,
when contaminated with unlabeled abnormal samples in training set under
semi-supervised settings, current contrastive-based methods generally 1) ignore
the comprehensive relation between training data, leading to suboptimal
performance, and 2) require fine-tuning, resulting in low efficiency. To
address the above two issues, in this paper, we propose a novel hierarchical
semi-supervised contrastive learning (HSCL) framework, for
contamination-resistant anomaly detection. Specifically, HSCL hierarchically
regulates three complementary relations: sample-to-sample, sample-to-prototype,
and normal-to-abnormal relations, enlarging the discrimination between normal
and abnormal samples with a comprehensive exploration of the contaminated data.
Besides, HSCL is an end-to-end learning approach that can efficiently learn
discriminative representations without fine-tuning. HSCL achieves
state-of-the-art performance in multiple scenarios, such as one-class
classification and cross-dataset detection. Extensive ablation studies further
verify the effectiveness of each considered relation. The code is available at
https://github.com/GaoangW/HSCL.",https://github.com/GaoangW/HSCL,35971
Universal Conditional Masked Language Pre-training for Neural Machine Translation,0.279802,"Pre-trained sequence-to-sequence models have significantly improved Neural
Machine Translation (NMT). Different from prior works where pre-trained models
usually adopt an unidirectional decoder, this paper demonstrates that
pre-training a sequence-to-sequence model but with a bidirectional decoder can
produce notable performance gains for both Autoregressive and
Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked
language model pre-trained on large-scale bilingual and monolingual corpora in
many languages. We also introduce two simple but effective methods to enhance
the CeMAT, aligned code-switching & masking and dynamic dual-masking. We
conduct extensive experiments and show that our CeMAT can achieve significant
performance improvement for all scenarios from low- to extremely high-resource
languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on
average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it
can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the
best of our knowledge, this is the first work to pre-train a unified model for
fine-tuning on both NMT tasks. Code, data, and pre-trained models are available
at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT.",https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT,19622
Causal Balancing for Domain Generalization,0.0742411,"While machine learning models rapidly advance the state-of-the-art on various
real-world tasks, out-of-domain (OOD) generalization remains a challenging
problem given the vulnerability of these models to spurious correlations. We
propose a balanced mini-batch sampling strategy to transform a biased data
distribution into a spurious-free balanced distribution, based on the
invariance of the underlying causal mechanisms for the data generation process.
We argue that the Bayes optimal classifiers trained on such balanced
distribution are minimax optimal across a diverse enough environment space. We
also provide an identifiability guarantee of the latent variable model of the
proposed data generation process, when utilizing enough train environments.
Experiments are conducted on DomainBed, demonstrating empirically that our
method obtains the best performance across 20 baselines reported on the
benchmark.",https://github.com/WANGXinyiLinda/causal-balancing-for-domain-generalization,17592
GaitTAKE: Gait Recognition by Temporal Attention and Keypoint-guided Embedding,0.139762,"Gait recognition, which refers to the recognition or identification of a
person based on their body shape and walking styles, derived from video data
captured from a distance, is widely used in crime prevention, forensic
identification, and social security. However, to the best of our knowledge,
most of the existing methods use appearance, posture and temporal feautures
without considering a learned temporal attention mechanism for global and local
information fusion. In this paper, we propose a novel gait recognition
framework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE),
which effectively fuses temporal-attention-based global and local appearance
feature and temporal aggregated human pose feature. Experimental results show
that our proposed method achieves a new SOTA in gait recognition with rank-1
accuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait
dataset; 90.4% accuracy on the OU-MVLP gait dataset.",None,17667
Combining Humor and Sarcasm for Improving Political Parody Detection,0.0989714,"Parody is a figurative device used for mimicking entities for comedic or
critical purposes. Parody is intentionally humorous and often involves sarcasm.
This paper explores jointly modelling these figurative tropes with the goal of
improving performance of political parody detection in tweets. To this end, we
present a multi-encoder model that combines three parallel encoders to enrich
parody-specific representations with humor and sarcasm information. Experiments
on a publicly available data set of political parody tweets demonstrate that
our approach outperforms previous state-of-the-art methods.",https://github.com/iamoscar1/Multi_Encoder_Model_for_Political_Parody_Prediction,-1
Hand-Object Interaction Image Generation,0.0540551,"In this work, we are dedicated to a new task, i.e., hand-object interaction
image generation, which aims to conditionally generate the hand-object image
under the given hand, object and their interaction status. This task is
challenging and research-worthy in many potential application scenarios, such
as AR/VR games and online shopping, etc. To address this problem, we propose a
novel HOGAN framework, which utilizes the expressive model-aware hand-object
representation and leverages its inherent topology to build the unified surface
space. In this space, we explicitly consider the complex self- and mutual
occlusion during interaction. During final image synthesis, we consider
different characteristics of hand and object and generate the target image in a
split-and-combine manner. For evaluation, we build a comprehensive protocol to
access both the fidelity and structure preservation of the generated image.
Extensive experiments on two large-scale datasets, i.e., HO3Dv3 and DexYCB,
demonstrate the effectiveness and superiority of our framework both
quantitatively and qualitatively. The project page is available at
https://play-with-hoi-generation.github.io/.",None,25445
Phrase-level Textual Adversarial Attack with Label Preservation,0.0634167,"Generating high-quality textual adversarial examples is critical for
investigating the pitfalls of natural language processing (NLP) models and
further promoting their robustness. Existing attacks are usually realized
through word-level or sentence-level perturbations, which either limit the
perturbation space or sacrifice fluency and textual quality, both affecting the
attack effectiveness. In this paper, we propose Phrase-Level Textual
Adversarial aTtack (PLAT) that generates adversarial samples through
phrase-level perturbations. PLAT first extracts the vulnerable phrases as
attack targets by a syntactic parser, and then perturbs them by a pre-trained
blank-infilling model. Such flexible perturbation design substantially expands
the search space for more effective attacks without introducing too many
modifications, and meanwhile maintaining the textual fluency and grammaticality
via contextualized generation using surrounding texts. Moreover, we develop a
label-preservation filter leveraging the likelihoods of language models
fine-tuned on each class, rather than textual similarity, to rule out those
perturbations that potentially alter the original class label for humans.
Extensive experiments and human evaluation demonstrate that PLAT has a superior
attack effectiveness as well as a better label consistency than strong
baselines.",https://github.com/Yibin-Lei/PLAT,15597
Controllable Evaluation and Generation of Physical Adversarial Patch on Face Recognition,0.0830101,"Recent studies have revealed the vulnerability of face recognition models
against physical adversarial patches, which raises security concerns about the
deployed face recognition systems. However, it is still challenging to ensure
the reproducibility for most attack algorithms under complex physical
conditions, which leads to the lack of a systematic evaluation of the existing
methods. It is therefore imperative to develop a framework that can enable a
comprehensive evaluation of the vulnerability of face recognition in the
physical world. To this end, we propose to simulate the complex transformations
of faces in the physical world via 3D-face modeling, which serves as a digital
counterpart of physical faces. The generic framework allows us to control
different face variations and physical conditions to conduct reproducible
evaluations comprehensively. With this digital simulator, we further propose a
Face3DAdv method considering the 3D face transformations and realistic physical
variations. Extensive experiments validate that Face3DAdv can significantly
improve the effectiveness of diverse physically realizable adversarial patches
in both simulated and physical environments, against various white-box and
black-box face recognition models.",None,12717
Geometric Graph Representation Learning via Maximizing Rate Reduction,0.0983848,"Learning discriminative node representations benefits various downstream
tasks in graph analysis such as community detection and node classification.
Existing graph representation learning methods (e.g., based on random walk and
contrastive learning) are limited to maximizing the local similarity of
connected nodes. Such pair-wise learning schemes could fail to capture the
global distribution of representations, since it has no explicit constraints on
the global geometric properties of representation space. To this end, we
propose Geometric Graph Representation Learning (G2R) to learn node
representations in an unsupervised manner via maximizing rate reduction. In
this way, G2R maps nodes in distinct groups (implicitly stored in the adjacency
matrix) into different subspaces, while each subspace is compact and different
subspaces are dispersedly distributed. G2R adopts a graph neural network as the
encoder and maximizes the rate reduction with the adjacency matrix.
Furthermore, we theoretically and empirically demonstrate that rate reduction
maximization is equivalent to maximizing the principal angles between different
subspaces. Experiments on real-world datasets show that G2R outperforms various
baselines on node classification and community detection tasks.",None,12156
Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts,0.252805,"Previous work has shown that there exists a scaling law between the size of
Language Models (LMs) and their zero-shot performance on different downstream
NLP tasks. In this work, we show that this phenomenon does not hold when
evaluating large LMs on tasks with negated prompts, but instead shows an
inverse scaling law. We evaluate 9 different tasks with negated prompts on (1)
pretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further
pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with
few-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all
LM types perform worse on negated prompts as they scale and show a huge
performance gap between the human performance when comparing the average score
on both original and negated prompts. By highlighting a critical limitation of
existing LMs and methods, we urge the community to develop new approaches of
developing LMs that actually follow the given instructions. We provide the code
and the datasets to explore negated prompts at
https://github.com/joeljang/negated-prompts-for-llms",None,6400
"Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities",0.151826,"As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection.",None,34651
VeriDark: A Large-Scale Benchmark for Authorship Verification on the Dark Web,0.113647,"The DarkWeb represents a hotbed for illicit activity, where users communicate
on different market forums in order to exchange goods and services. Law
enforcement agencies benefit from forensic tools that perform authorship
analysis, in order to identify and profile users based on their textual
content. However, authorship analysis has been traditionally studied using
corpora featuring literary texts such as fragments from novels or fan fiction,
which may not be suitable in a cybercrime context. Moreover, the few works that
employ authorship analysis tools for cybercrime prevention usually employ
ad-hoc experimental setups and datasets. To address these issues, we release
VeriDark: a benchmark comprised of three large scale authorship verification
datasets and one authorship identification dataset obtained from user activity
from either Dark Web related Reddit communities or popular illicit Dark Web
market forums. We evaluate competitive NLP baselines on the three datasets and
perform an analysis of the predictions to better understand the limitations of
such approaches. We make the datasets and baselines publicly available at
https://github.com/bit-ml/VeriDark",None,7673
Chemotaxis of sea urchin sperm cells through deep reinforcement learning,0.0971203,"By imitating biological microswimmers, microrobots can be designed to
accomplish targeted delivery of cargos and biomedical manipulations at
microscale. However, it is still a great challenge to enable microrobots to
maneuver in a complex environment. Machine learning algorithms offer a tool to
boost mobility and flexibility of a synthetic microswimmer, hence could help us
design truly smart microrobots. In this work, we investigate how a model of sea
urchin sperm cell can self-learn chemotactic motion in a chemoattractant
concentration field. We employ an artificial neural network to act as a
decision-making agent and facilitate the sperm cell to discover efficient
maneuver strategies through a deep reinforcement learning (DRL) algorithm. Our
results show that chemotactic behaviours, very similar to the realistic ones,
can be achieved by the DRL utilizing only limited environmental information. In
most cases, the DRL algorithm discovers more efficient strategies than the
human-devised one. Furthermore, the DRL can even utilize an external
disturbance to facilitate the chemotactic motion if the extra flow information
is also taken into account by the artificial neural network. Our results
provide insights to the chemotactic process of sea urchin sperm cells and also
prepare guidance for the intelligent maneuver of microrobots.",None,1256
Toward More Meaningful Resources for Lower-resourced Languages,0.291206,"In this position paper, we describe our perspective on how meaningful
resources for lower-resourced languages should be developed in connection with
the speakers of those languages. We first examine two massively multilingual
resources in detail. We explore the contents of the names stored in Wikidata
for a few lower-resourced languages and find that many of them are not in fact
in the languages they claim to be and require non-trivial effort to correct. We
discuss quality issues present in WikiAnn and evaluate whether it is a useful
supplement to hand annotated data. We then discuss the importance of creating
annotation for lower-resourced languages in a thoughtful and ethical way that
includes the languages' speakers as part of the development process. We
conclude with recommended guidelines for resource development.",https://github.com/masakhane-io/masakhane-ner,867
Learning to Fold Real Garments with One Arm: A Case Study in Cloud-Based Robotics Research,0.195379,"Autonomous fabric manipulation is a longstanding challenge in robotics, but
evaluating progress is difficult due to the cost and diversity of robot
hardware. Using Reach, a cloud robotics platform that enables low-latency
remote execution of control policies on physical robots, we present the first
systematic benchmarking of fabric manipulation algorithms on physical hardware.
We develop 4 novel learning-based algorithms that model expert actions,
keypoints, reward functions, and dynamic motions, and we compare these against
4 learning-free and inverse dynamics algorithms on the task of folding a
crumpled T-shirt with a single robot arm. The entire lifecycle of data
collection, model training, and policy evaluation is performed remotely without
physical access to the robot workcell. Results suggest a new algorithm
combining imitation learning with analytic methods achieves 84% of human-level
performance on the folding task. See
https://sites.google.com/berkeley.edu/cloudfolding for all data, code, models,
and supplemental material.",https://github.com/google-research/pyreach,167441
An Online Semantic Mapping System for Extending and Enhancing Visual SLAM,0.323573,"We present a real-time semantic mapping approach for mobile vision systems
with a 2D to 3D object detection pipeline and rapid data association for
generated landmarks. Besides the semantic map enrichment the associated
detections are further introduced as semantic constraints into a simultaneous
localization and mapping (SLAM) system for pose correction purposes. This way,
we are able generate additional meaningful information that allows to achieve
higher-level tasks, while simultaneously leveraging the view-invariance of
object detections to improve the accuracy and the robustness of the odometry
estimation. We propose tracklets of locally associated object observations to
handle ambiguous and false predictions and an uncertainty-based greedy
association scheme for an accelerated processing time. Our system reaches
real-time capabilities with an average iteration duration of 65~ms and is able
to improve the pose estimation of a state-of-the-art SLAM by up to 68% on a
public dataset. Additionally, we implemented our approach as a modular ROS
package that makes it straightforward for integration in arbitrary graph-based
SLAM methods.",None,5797
Learning Sketches for Decomposing Planning Problems into Subproblems of Bounded Width: Extended Version,0.0184864,"Recently, sketches have been introduced as a general language for
representing the subgoal structure of instances drawn from the same domain.
Sketches are collections of rules of the form C -> E over a given set of
features where C expresses Boolean conditions and E expresses qualitative
changes. Each sketch rule defines a subproblem: going from a state that
satisfies C to a state that achieves the change expressed by E or a goal state.
Sketches can encode simple goal serializations, general policies, or
decompositions of bounded width that can be solved greedily, in polynomial
time, by the SIW_R variant of the SIW algorithm. Previous work has shown the
computational value of sketches over benchmark domains that, while tractable,
are challenging for domain-independent planners. In this work, we address the
problem of learning sketches automatically given a planning domain, some
instances of the target class of problems, and the desired bound on the sketch
width. We present a logical formulation of the problem, an implementation using
the ASP solver Clingo, and experimental results. The sketch learner and the
SIW_R planner yield a domain-independent planner that learns and exploits
domain structure in a crisp and explicit form.",https://doi.org/10.5281/zenodo.6381592,12924
Conformance Checking with Uncertainty via SMT (Extended Version),0.569054,"Logs of real-life processes often feature uncertainty pertaining the recorded
timestamps, data values, and/or events. We consider the problem of checking
conformance of uncertain logs against data-aware reference processes.
Specifically, we show how to solve it via SMT encodings, lifting previous work
on data-aware SMT-based conformance checking to this more sophisticated
setting. Our approach is modular, in that it homogeneously accommodates for
different types of uncertainty. Moreover, using appropriate cost functions,
different conformance checking tasks can be addressed. We show the correctness
of our approach and witness feasibility through a proof-of-concept
implementation.",https://github.com/bytekid/cocomot,8569
Unsupervised Sentence Textual Similarity with Compositional Phrase Semantics,0.0263546,"Measuring Sentence Textual Similarity (STS) is a classic task that can be
applied to many downstream NLP applications such as text generation and
retrieval. In this paper, we focus on unsupervised STS that works on various
domains but only requires minimal data and computational resources.
Theoretically, we propose a light-weighted Expectation-Correction (EC)
formulation for STS computation. EC formulation unifies unsupervised STS
approaches including the cosine similarity of Additively Composed (AC) sentence
embeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose
the Recursive Optimal Transport Similarity (ROTS) algorithm to capture the
compositional phrase semantics by composing multiple recursive EC formulations.
ROTS finishes in linear time and is faster than its predecessors. ROTS is
empirically more effective and scalable than previous approaches. Extensive
experiments on 29 STS tasks under various settings show the clear advantage of
ROTS over existing approaches. Detailed ablation studies demonstrate the
effectiveness of our approaches.",https://github.com/zihao-wang/rots,-1
Enhanced Training of Query-Based Object Detection via Selective Query Recollection,0.20579,"This paper investigates a phenomenon where query-based object detectors
mispredict at the last decoding stage while predicting correctly at an
intermediate stage. We review the training process and attribute the overlooked
phenomenon to two limitations: lack of training emphasis and cascading errors
from decoding sequence. We design and present Selective Query Recollection
(SQR), a simple and effective training strategy for query-based object
detectors. It cumulatively collects intermediate queries as decoding stages go
deeper and selectively forwards the queries to the downstream stages aside from
the sequential structure. Such-wise, SQR places training emphasis on later
stages and allows later stages to work with intermediate queries from earlier
stages directly. SQR can be easily plugged into various query-based object
detectors and significantly enhances their performance while leaving the
inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR,
and Deformable-DETR across various settings (backbone, number of queries,
schedule) and consistently brings 1.4-2.8 AP improvement.",https://github.com/Fangyi-Chen/SQR,14250
Iterative Scene Graph Generation,0.299179,"The task of scene graph generation entails identifying object entities and
their corresponding interaction predicates in a given image (or video). Due to
the combinatorially large solution space, existing approaches to scene graph
generation assume certain factorization of the joint distribution to make the
estimation feasible (e.g., assuming that objects are conditionally independent
of predicate predictions). However, this fixed factorization is not ideal under
all scenarios (e.g., for images where an object entailed in interaction is
small and not discernible on its own). In this work, we propose a novel
framework for scene graph generation that addresses this limitation, as well as
introduces dynamic conditioning on the image, using message passing in a Markov
Random Field. This is implemented as an iterative refinement procedure wherein
each modification is conditioned on the graph generated in the previous
iteration. This conditioning across refinement steps allows joint reasoning
over entities and relations. This framework is realized via a novel and
end-to-end trainable transformer-based architecture. In addition, the proposed
framework can improve existing approach performance. Through extensive
experiments on Visual Genome and Action Genome benchmark datasets we show
improved performance on the scene graph generation.",https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch,14517
Region Embedding with Intra and Inter-View Contrastive Learning,0.111176,"Unsupervised region representation learning aims to extract dense and
effective features from unlabeled urban data. While some efforts have been made
for solving this problem based on multiple views, existing methods are still
insufficient in extracting representations in a view and/or incorporating
representations from different views. Motivated by the success of contrastive
learning for representation learning, we propose to leverage it for multi-view
region representation learning and design a model called ReMVC (Region
Embedding with Multi-View Contrastive Learning) by following two guidelines: i)
comparing a region with others within each view for effective representation
extraction and ii) comparing a region with itself across different views for
cross-view information sharing. We design the intra-view contrastive learning
module which helps to learn distinguished region embeddings and the inter-view
contrastive learning module which serves as a soft co-regularizer to constrain
the embedding parameters and transfer knowledge across multi-views. We exploit
the learned region embeddings in two downstream tasks named land usage
clustering and region popularity prediction. Extensive experiments demonstrate
that our model achieves impressive improvements compared with seven
state-of-the-art baseline methods, and the margins are over 30% in the land
usage clustering task.",https://github.com/Liang-NTU/ReMVC,2797
Pyramid Frequency Network with Spatial Attention Residual Refinement Module for Monocular Depth Estimation,0.114659,"Deep-learning-based approaches to depth estimation are rapidly advancing,
offering superior performance over existing methods. To estimate the depth in
real-world scenarios, depth estimation models require the robustness of various
noise environments. In this work, a Pyramid Frequency Network(PFN) with Spatial
Attention Residual Refinement Module(SARRM) is proposed to deal with the weak
robustness of existing deep-learning methods. To reconstruct depth maps with
accurate details, the SARRM constructs a residual fusion method with an
attention mechanism to refine the blur depth. The frequency division strategy
is designed, and the frequency pyramid network is developed to extract features
from multiple frequency bands. With the frequency strategy, PFN achieves better
visual accuracy than state-of-the-art methods in both indoor and outdoor scenes
on Make3D, KITTI depth, and NYUv2 datasets. Additional experiments on the noisy
NYUv2 dataset demonstrate that PFN is more reliable than existing deep-learning
methods in high-noise scenes.",None,-1
Saga: A Platform for Continuous Construction and Serving of Knowledge At Scale,0.0485768,"We introduce Saga, a next-generation knowledge construction and serving
platform for powering knowledge-based applications at industrial scale. Saga
follows a hybrid batch-incremental design to continuously integrate billions of
facts about real-world entities and construct a central knowledge graph that
supports multiple production use cases with diverse requirements around data
freshness, accuracy, and availability. In this paper, we discuss the unique
challenges associated with knowledge graph construction at industrial scale,
and review the main components of Saga and how they address these challenges.
Finally, we share lessons-learned from a wide array of production use cases
powered by Saga.",None,3350
Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics,0.0709868,"Few images on the Web receive alt-text descriptions that would make them
accessible to blind and low vision (BLV) users. Image-based NLG systems have
progressed to the point where they can begin to address this persistent
societal problem, but these systems will not be fully successful unless we
evaluate them on metrics that guide their development correctly. Here, we argue
against current referenceless metrics -- those that don't rely on
human-generated ground-truth descriptions -- on the grounds that they do not
align with the needs of BLV users. The fundamental shortcoming of these metrics
is that they do not take context into account, whereas contextual information
is highly valued by BLV users. To substantiate these claims, we present a study
with BLV participants who rated descriptions along a variety of dimensions. An
in-depth analysis reveals that the lack of context-awareness makes current
referenceless metrics inadequate for advancing image accessibility. As a
proof-of-concept, we provide a contextual version of the referenceless metric
CLIPScore which begins to address the disconnect to the BLV data. An accessible
HTML version of this paper is available at
https://elisakreiss.github.io/contextual-description-evaluation/paper/reflessmetrics.html",https://github.com/elisakreiss/contextual-description-evaluation,38178
Efficient CNN with uncorrelated Bag of Features pooling,0.00986672,"Despite the superior performance of CNN, deploying them on low computational
power devices is still limited as they are typically computationally expensive.
One key cause of the high complexity is the connection between the convolution
layers and the fully connected layers, which typically requires a high number
of parameters. To alleviate this issue, Bag of Features (BoF) pooling has been
recently proposed. BoF learns a dictionary, that is used to compile a histogram
representation of the input. In this paper, we propose an approach that builds
on top of BoF pooling to boost its efficiency by ensuring that the items of the
learned dictionary are non-redundant. We propose an additional loss term, based
on the pair-wise correlation of the items of the dictionary, which complements
the standard loss to explicitly regularize the model to learn a more diverse
and rich dictionary. The proposed strategy yields an efficient variant of BoF
and further boosts its performance, without any additional parameters.",None,31204
Event Collapse in Contrast Maximization Frameworks,0.314508,"Contrast maximization (CMax) is a framework that provides state-of-the-art
results on several event-based computer vision tasks, such as ego-motion or
optical flow estimation. However, it may suffer from a problem called event
collapse, which is an undesired solution where events are warped into too few
pixels. As prior works have largely ignored the issue or proposed workarounds,
it is imperative to analyze this phenomenon in detail. Our work demonstrates
event collapse in its simplest form and proposes collapse metrics by using
first principles of space-time deformation based on differential geometry and
physics. We experimentally show on publicly available datasets that the
proposed metrics mitigate event collapse and do not harm well-posed warps. To
the best of our knowledge, regularizers based on the proposed metrics are the
only effective solution against event collapse in the experimental settings
considered, compared with other methods. We hope that this work inspires
further research to tackle more complex warp models.",None,7090
"Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus",0.771684,"The Abstraction and Reasoning Corpus (ARC) aims at benchmarking the
performance of general artificial intelligence algorithms. The ARC's focus on
broad generalization and few-shot learning has made it difficult to solve using
pure machine learning. A more promising approach has been to perform program
synthesis within an appropriately designed Domain Specific Language (DSL).
However, these too have seen limited success. We propose Abstract Reasoning
with Graph Abstractions (ARGA), a new object-centric framework that first
represents images using graphs and then performs a search for a correct program
in a DSL that is based on the abstracted graph space. The complexity of this
combinatorial search is tamed through the use of constraint acquisition, state
hashing, and Tabu search. An extensive set of experiments demonstrates the
promise of ARGA in tackling some of the complicated object-centric tasks of the
ARC rather efficiently, producing programs that are correct and easy to
understand.",https://github.com/khalil-research/ARGA-AAAI23,8505
The SIGMORPHON 2022 Shared Task on Morpheme Segmentation,0.11548,"The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems
to decompose a word into a sequence of morphemes and covered most types of
morphology: compounds, derivations, and inflections. Subtask 1, word-level
morpheme segmentation, covered 5 million words in 9 languages (Czech, English,
Spanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13
system submissions from 7 teams and the best system averaged 97.29% F1 score
across all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2,
sentence-level morpheme segmentation, covered 18,735 sentences in 3 languages
(Czech, English, Mongolian), received 10 system submissions from 3 teams, and
the best systems outperformed all three state-of-the-art subword tokenization
methods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis
and support any type of future studies, we released all system predictions, the
evaluation script, and all gold standard datasets.",https://github.com/sigmorphon/2022SegmentationST,23512
GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation,0.0510388,"Birds-eye-view (BEV) semantic segmentation is critical for autonomous driving
for its powerful spatial representation ability. It is challenging to estimate
the BEV semantic maps from monocular images due to the spatial gap, since it is
implicitly required to realize both the perspective-to-BEV transformation and
segmentation. We present a novel two-stage Geometry Prior-based Transformation
framework named GitNet, consisting of (i) the geometry-guided pre-alignment and
(ii) ray-based transformer. In the first stage, we decouple the BEV
segmentation into the perspective image segmentation and geometric prior-based
mapping, with explicit supervision by projecting the BEV semantic labels onto
the image plane to learn visibility-aware features and learnable geometry to
translate into BEV space. Second, the pre-aligned coarse BEV features are
further deformed by ray-based transformers to take visibility knowledge into
account. GitNet achieves the leading performance on the challenging nuScenes
and Argoverse Datasets.",None,51801
Attention Option-Critic,0.0210231,"Temporal abstraction in reinforcement learning is the ability of an agent to
learn and use high-level behaviors, called options. The option-critic
architecture provides a gradient-based end-to-end learning method to construct
options. We propose an attention-based extension to this framework, which
enables the agent to learn to focus different options on different aspects of
the observation space. We show that this leads to behaviorally diverse options
which are also capable of state abstraction, and prevents the degeneracy
problems of option domination and frequent option switching that occur in
option-critic, while achieving a similar sample complexity. We also demonstrate
the more efficient, interpretable, and reusable nature of the learned options
in comparison with option-critic, through different transfer learning tasks.
Experimental results in a relatively simple four-rooms environment and the more
complex ALE (Arcade Learning Environment) showcase the efficacy of our
approach.",None,33465
Coarse-to-Fine Sparse Sequential Recommendation,0.648534,"Sequential recommendation aims to model dynamic user behavior from historical
interactions. Self-attentive methods have proven effective at capturing
short-term dynamics and long-term preferences. Despite their success, these
approaches still struggle to model sparse data, on which they struggle to learn
high-quality item representations. We propose to model user dynamics from
shopping intents and interacted items simultaneously. The learned intents are
coarse-grained and work as prior knowledge for item recommendation. To this
end, we present a coarse-to-fine self-attention framework, namely CaFe, which
explicitly learns coarse-grained and fine-grained sequential dynamics.
Specifically, CaFe first learns intents from coarse-grained sequences which are
dense and hence provide high-quality user intent representations. Then, CaFe
fuses intent representations into item encoder outputs to obtain improved item
representations. Finally, we infer recommended items based on representations
of items and corresponding intents. Experiments on sparse datasets show that
CaFe outperforms state-of-the-art self-attentive recommenders by 44.03% NDCG@5
on average.",None,104016
Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models,0.0856483,"Structural probing work has found evidence for latent syntactic information
in pre-trained language models. However, much of this analysis has focused on
monolingual models, and analyses of multilingual models have employed
correlational methods that are confounded by the choice of probing tasks. In
this study, we causally probe multilingual language models (XGLM and
multilingual BERT) as well as monolingual BERT-based models across various
languages; we do this by performing counterfactual perturbations on neuron
activations and observing the effect on models' subject-verb agreement
probabilities. We observe where in the model and to what extent syntactic
agreement is encoded in each language. We find significant neuron overlap
across languages in autoregressive multilingual language models, but not masked
language models. We also find two distinct layer-wise effect patterns and two
distinct sets of neurons used for syntactic agreement, depending on whether the
subject and verb are separated by other tokens. Finally, we find that
behavioral analyses of language models are likely underestimating how sensitive
masked language models are to syntactic information.",https://github.com/aaronmueller/multilingual-lm-intervention,8208
A Streamlit-based Artificial Intelligence Trust Platform for Next-Generation Wireless Networks,0.0288089,"With the rapid development and integration of artificial intelligence (AI)
methods in next-generation networks (NextG), AI algorithms have provided
significant advantages for NextG in terms of frequency spectrum usage,
bandwidth, latency, and security. A key feature of NextG is the integration of
AI, i.e., self-learning architecture based on self-supervised algorithms, to
improve the performance of the network. A secure AI-powered structure is also
expected to protect NextG networks against cyber-attacks. However, AI itself
may be attacked, i.e., model poisoning targeted by attackers, and it results in
cybersecurity violations. This paper proposes an AI trust platform using
Streamlit for NextG networks that allows researchers to evaluate, defend,
certify, and verify their AI models and applications against adversarial
threats of evasion, poisoning, extraction, and interference.",https://github.com/muratkuzlu/NextG,4955
Deep Virtual-to-Real Distillation for Pedestrian Crossing Prediction,0.165789,"Pedestrian crossing is one of the most typical behavior which conflicts with
natural driving behavior of vehicles. Consequently, pedestrian crossing
prediction is one of the primary task that influences the vehicle planning for
safe driving. However, current methods that rely on the practically collected
data in real driving scenes cannot depict and cover all kinds of scene
condition in real traffic world. To this end, we formulate a deep virtual to
real distillation framework by introducing the synthetic data that can be
generated conveniently, and borrow the abundant information of pedestrian
movement in synthetic videos for the pedestrian crossing prediction in real
data with a simple and lightweight implementation. In order to verify this
framework, we construct a benchmark with 4667 virtual videos owning about 745k
frames (called Virtual-PedCross-4667), and evaluate the proposed method on two
challenging datasets collected in real driving situations, i.e., JAAD and PIE
datasets. State-of-the-art performance of this framework is demonstrated by
exhaustive experiment analysis. The dataset and code can be downloaded from the
website \url{http://www.lotvs.net/code_data/}.",None,6089
Processing the structure of documents: Logical Layout Analysis of historical newspapers in French,0.125131,"Background. In recent years, libraries and archives led important
digitisation campaigns that opened the access to vast collections of historical
documents. While such documents are often available as XML ALTO documents, they
lack information about their logical structure. In this paper, we address the
problem of Logical Layout Analysis applied to historical documents in French.
We propose a rule-based method, that we evaluate and compare with two
Machine-Learning models, namely RIPPER and Gradient Boosting. Our data set
contains French newspapers, periodicals and magazines, published in the first
half of the twentieth century in the Franche-Comt\'e Region. Results. Our
rule-based system outperforms the two other models in nearly all evaluations.
It has especially better Recall results, indicating that our system covers more
types of every logical label than the other two models. When comparing RIPPER
with Gradient Boosting, we can observe that Gradient Boosting has better
Precision scores but RIPPER has better Recall scores. Conclusions. The
evaluation shows that our system outperforms the two Machine Learning models,
and provides significantly higher Recall. It also confirms that our system can
be used to produce annotated data sets that are large enough to envisage
Machine Learning or Deep Learning approaches for the task of Logical Layout
Analysis. Combining rules and Machine Learning models into hybrid systems could
potentially provide even better performances. Furthermore, as the layout in
historical documents evolves rapidly, one possible solution to overcome this
problem would be to apply Rule Learning algorithms to bootstrap rule sets
adapted to different publication periods.",None,703
Differentiable Agent-based Epidemiology,0.173609,"Mechanistic simulators are an indispensable tool for epidemiology to explore
the behavior of complex, dynamic infections under varying conditions and
navigate uncertain environments. Agent-based models (ABMs) are an increasingly
popular simulation paradigm that can represent the heterogeneity of contact
interactions with granular detail and agency of individual behavior. However,
conventional ABM frameworks are not differentiable and present challenges in
scalability; due to which it is non-trivial to connect them to auxiliary data
sources. In this paper, we introduce GradABM: a scalable, differentiable design
for agent-based modeling that is amenable to gradient-based learning with
automatic differentiation. GradABM can quickly simulate million-size
populations in few seconds on commodity hardware, integrate with deep neural
networks and ingest heterogeneous data sources. This provides an array of
practical benefits for calibration, forecasting, and evaluating policy
interventions. We demonstrate the efficacy of GradABM via extensive experiments
with real COVID-19 and influenza datasets.",None,50851
DialCrowd 2.0: A Quality-Focused Dialog System Crowdsourcing Toolkit,0.033941,"Dialog system developers need high-quality data to train, fine-tune and
assess their systems. They often use crowdsourcing for this since it provides
large quantities of data from many workers. However, the data may not be of
sufficiently good quality. This can be due to the way that the requester
presents a task and how they interact with the workers. This paper introduces
DialCrowd 2.0 to help requesters obtain higher quality data by, for example,
presenting tasks more clearly and facilitating effective communication with
workers. DialCrowd 2.0 guides developers in creating improved Human
Intelligence Tasks (HITs) and is directly applicable to the workflows used
currently by developers and researchers.",None,14484
Grounding Aleatoric Uncertainty for Unsupervised Environment Design,0.156145,"Adaptive curricula in reinforcement learning (RL) have proven effective for
producing policies robust to discrepancies between the train and test
environment. Recently, the Unsupervised Environment Design (UED) framework
generalized RL curricula to generating sequences of entire environments,
leading to new methods with robust minimax regret properties. Problematically,
in partially-observable or stochastic settings, optimal policies may depend on
the ground-truth distribution over aleatoric parameters of the environment in
the intended deployment setting, while curriculum learning necessarily shifts
the training distribution. We formalize this phenomenon as curriculum-induced
covariate shift (CICS), and describe how its occurrence in aleatoric parameters
can lead to suboptimal policies. Directly sampling these parameters from the
ground-truth distribution avoids the issue, but thwarts curriculum learning. We
propose SAMPLR, a minimax regret UED method that optimizes the ground-truth
utility function, even when the underlying training data is biased due to CICS.
We prove, and validate on challenging domains, that our approach preserves
optimality under the ground-truth distribution, while promoting robustness
across the full range of environment settings.",https://github.com/facebookresearch/nle,18829
D2A-BSP: Distilled Data Association Belief Space Planning with Performance Guarantees Under Budget Constraints,0.23873,"Unresolved data association in ambiguous and perceptually aliased
environments leads to multi-modal hypotheses on both the robot's and the
environment state. To avoid catastrophic results, when operating in such
ambiguous environments, it is crucial to reason about data association within
Belief Space Planning (BSP). However, explicitly considering all possible data
associations, the number of hypotheses grows exponentially with the planning
horizon and determining the optimal action sequence quickly becomes
intractable. Moreover, with hard budget constraints where some non-negligible
hypotheses must be pruned, achieving performance guarantees is crucial. In this
work we present a computationally efficient novel approach that utilizes only a
distilled subset of hypotheses to solve BSP problems while reasoning about data
association. Furthermore, to provide performance guarantees, we derive error
bounds with respect to the optimal solution. We then demonstrate our approach
in an extremely aliased environment, where we manage to significantly reduce
computation time without compromising on the quality of the solution.",None,2485
Towards Open Set Video Anomaly Detection,0.190223,"Open Set Video Anomaly Detection (OpenVAD) aims to identify abnormal events
from video data where both known anomalies and novel ones exist in testing.
Unsupervised models learned solely from normal videos are applicable to any
testing anomalies but suffer from a high false positive rate. In contrast,
weakly supervised methods are effective in detecting known anomalies but could
fail in an open world. We develop a novel weakly supervised method for the
OpenVAD problem by integrating evidential deep learning (EDL) and normalizing
flows (NFs) into a multiple instance learning (MIL) framework. Specifically, we
propose to use graph neural networks and triplet loss to learn discriminative
features for training the EDL classifier, where the EDL is capable of
identifying the unknown anomalies by quantifying the uncertainty. Moreover, we
develop an uncertainty-aware selection strategy to obtain clean anomaly
instances and a NFs module to generate the pseudo anomalies. Our method is
superior to existing approaches by inheriting the advantages of both the
unsupervised NFs and the weakly-supervised MIL framework. Experimental results
on multiple real-world video datasets show the effectiveness of our method.",None,5177
Investigating data partitioning strategies for crosslinguistic low-resource ASR evaluation,0.0102124,"Many automatic speech recognition (ASR) data sets include a single
pre-defined test set consisting of one or more speakers whose speech never
appears in the training set. This ""hold-speaker(s)-out"" data partitioning
strategy, however, may not be ideal for data sets in which the number of
speakers is very small. This study investigates ten different data split
methods for five languages with minimal ASR training resources. We find that
(1) model performance varies greatly depending on which speaker is selected for
testing; (2) the average word error rate (WER) across all held-out speakers is
comparable not only to the average WER over multiple random splits but also to
any given individual random split; (3) WER is also generally comparable when
the data is split heuristically or adversarially; (4) utterance duration and
intensity are comparatively more predictive factors of variability regardless
of the data split. These results suggest that the widely used hold-speakers-out
approach to ASR data partitioning can yield results that do not reflect model
performance on unseen data or speakers. Random splits can yield more reliable
and generalizable estimates when facing data sparsity.",None,1027
Egocentric Prediction of Action Target in 3D,0.188295,"We are interested in anticipating as early as possible the target location of
a person's object manipulation action in a 3D workspace from egocentric vision.
It is important in fields like human-robot collaboration, but has not yet
received enough attention from vision and learning communities. To stimulate
more research on this challenging egocentric vision task, we propose a large
multimodality dataset of more than 1 million frames of RGB-D and IMU streams,
and provide evaluation metrics based on our high-quality 2D and 3D labels from
semi-automatic annotation. Meanwhile, we design baseline methods using
recurrent neural networks and conduct various ablation studies to validate
their effectiveness. Our results demonstrate that this new task is worthy of
further study by researchers in robotics, vision, and learning communities.",None,30070
Improving Low-Resource Speech Recognition with Pretrained Speech Models: Continued Pretraining vs. Semi-Supervised Training,0.0240993,"Self-supervised Transformer based models, such as wav2vec 2.0 and HuBERT,
have produced significant improvements over existing approaches to automatic
speech recognition (ASR). This is evident in the performance of the wav2vec 2.0
based pretrained XLSR-53 model across many languages when fine-tuned with
available labeled data. However, the performance from finetuning these models
can be dependent on the amount of in-language or similar-to-in-language data
included in the pretraining dataset. In this paper we investigate continued
pretraining (CoPT) with unlabeled in-language audio data on the XLSR-53
pretrained model in several low-resource languages. CoPT is more
computationally efficient than semi-supervised training (SST), the standard
approach of utilizing unlabeled data in ASR, since it omits the need for
pseudo-labeling of the unlabeled data. We show CoPT results in word error rates
(WERs), equal to or slightly better than using SST. In addition, we show that
using the CoPT model for pseudo-labeling, and using these labels in SST,
results in further improvements in WER.",None,604
Differentiable Point-Based Radiance Fields for Efficient View Synthesis,0.485825,"We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers.",None,97412
CNSNet: A Cleanness-Navigated-Shadow Network for Shadow Removal,0.191235,"The key to shadow removal is recovering the contents of the shadow regions
with the guidance of the non-shadow regions. Due to the inadequate long-range
modeling, the CNN-based approaches cannot thoroughly investigate the
information from the non-shadow regions. To solve this problem, we propose a
novel cleanness-navigated-shadow network (CNSNet), with a shadow-oriented
adaptive normalization (SOAN) module and a shadow-aware aggregation with
transformer (SAAT) module based on the shadow mask. Under the guidance of the
shadow mask, the SOAN module formulates the statistics from the non-shadow
region and adaptively applies them to the shadow region for region-wise
restoration. The SAAT module utilizes the shadow mask to precisely guide the
restoration of each shadowed pixel by considering the highly relevant pixels
from the shadow-free regions for global pixel-wise restoration. Extensive
experiments on three benchmark datasets (ISTD, ISTD+, and SRD) show that our
method achieves superior de-shadowing performance.",None,6653
No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models,0.0772127,"Recent research has shown the existence of significant redundancy in large
Transformer models. One can prune the redundant parameters without
significantly sacrificing the generalization performance. However, we question
whether the redundant parameters could have contributed more if they were
properly trained. To answer this question, we propose a novel training strategy
that encourages all parameters to be trained sufficiently. Specifically, we
adaptively adjust the learning rate for each parameter according to its
sensitivity, a robust gradient-based measure reflecting this parameter's
contribution to the model performance. A parameter with low sensitivity is
redundant, and we improve its fitting by increasing its learning rate. In
contrast, a parameter with high sensitivity is well-trained, and we regularize
it by decreasing its learning rate to prevent further overfitting. We conduct
extensive experiments on natural language understanding, neural machine
translation, and image classification to demonstrate the effectiveness of the
proposed schedule. Analysis shows that the proposed schedule indeed reduces the
redundancy and improves generalization performance.",https://github.com/cliang1453/SAGE,82331
TODE-Trans: Transparent Object Depth Estimation with Transformer,0.0839233,"Transparent objects are widely used in industrial automation and daily life.
However, robust visual recognition and perception of transparent objects have
always been a major challenge. Currently, most commercial-grade depth cameras
are still not good at sensing the surfaces of transparent objects due to the
refraction and reflection of light. In this work, we present a
transformer-based transparent object depth estimation approach from a single
RGB-D input. We observe that the global characteristics of the transformer make
it easier to extract contextual information to perform depth estimation of
transparent areas. In addition, to better enhance the fine-grained features, a
feature fusion module (FFM) is designed to assist coherent prediction. Our
empirical evidence demonstrates that our model delivers significant
improvements in recent popular datasets, e.g., 25% gain on RMSE and 21% gain on
REL compared to previous state-of-the-art convolutional-based counterparts in
ClearGrasp dataset. Extensive results show that our transformer-based model
enables better aggregation of the object's RGB and inaccurate depth information
to obtain a better depth representation. Our code and the pre-trained model
will be available at https://github.com/yuchendoudou/TODE.",None,6668
FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment,0.242347,"We present Face Swapping GAN (FSGAN) for face swapping and reenactment.
Unlike previous work, we offer a subject agnostic swapping scheme that can be
applied to pairs of faces without requiring training on those faces. We derive
a novel iterative deep learning--based approach for face reenactment which
adjusts significant pose and expression variations that can be applied to a
single image or a video sequence. For video sequences, we introduce a
continuous interpolation of the face views based on reenactment, Delaunay
Triangulation, and barycentric coordinates. Occluded face regions are handled
by a face completion network. Finally, we use a face blending network for
seamless blending of the two faces while preserving the target skin color and
lighting conditions. This network uses a novel Poisson blending loss combining
Poisson optimization with a perceptual loss. We compare our approach to
existing state-of-the-art systems and show our results to be both qualitatively
and quantitatively superior. This work describes extensions of the FSGAN
method, proposed in an earlier conference version of our work, as well as
additional experiments and results.",None,4658
User-Centric Gender Rewriting,0.151459,"In this paper, we define the task of gender rewriting in contexts involving
two users (I and/or You) - first and second grammatical persons with
independent grammatical gender preferences. We focus on Arabic, a
gender-marking morphologically rich language. We develop a multi-step system
that combines the positive aspects of both rule-based and neural rewriting
models. Our results successfully demonstrate the viability of this approach on
a recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5
on a blind test set. Our proposed system improves over previous work on the
first-person-only version of this task, by 3.05 absolute increase in M2 F0.5.
We demonstrate a use case of our gender rewriting system by using it to
post-edit the output of a commercial MT system to provide personalized outputs
based on the users' grammatical gender preferences. We make our code, data, and
models publicly available.",https://github.com/CAMeL-Lab/gender-rewriting/,16583
An Accelerator for Rule Induction in Fuzzy Rough Theory,0.0421142,"Rule-based classifier, that extract a subset of induced rules to efficiently
learn/mine while preserving the discernibility information, plays a crucial
role in human-explainable artificial intelligence. However, in this era of big
data, rule induction on the whole datasets is computationally intensive. So
far, to the best of our knowledge, no known method focusing on accelerating
rule induction has been reported. This is first study to consider the
acceleration technique to reduce the scale of computation in rule induction. We
propose an accelerator for rule induction based on fuzzy rough theory; the
accelerator can avoid redundant computation and accelerate the building of a
rule classifier. First, a rule induction method based on consistence degree,
called Consistence-based Value Reduction (CVR), is proposed and used as basis
to accelerate. Second, we introduce a compacted search space termed Key Set,
which only contains the key instances required to update the induced rule, to
conduct value reduction. The monotonicity of Key Set ensures the feasibility of
our accelerator. Third, a rule-induction accelerator is designed based on Key
Set, and it is theoretically guaranteed to display the same results as the
unaccelerated version. Specifically, the rank preservation property of Key Set
ensures consistency between the rule induction achieved by the accelerator and
the unaccelerated method. Finally, extensive experiments demonstrate that the
proposed accelerator can perform remarkably faster than the unaccelerated
rule-based classifier methods, especially on datasets with numerous instances.",https://github.com/RUC-DWBI-ML/A-CVRC,15039
On the Transformation of Latent Space in Fine-Tuned NLP Models,0.140874,"We study the evolution of latent space in fine-tuned NLP models. Different
from the commonly used probing-framework, we opt for an unsupervised method to
analyze representations. More specifically, we discover latent concepts in the
representational space using hierarchical clustering. We then use an alignment
function to gauge the similarity between the latent space of a pre-trained
model and its fine-tuned version. We use traditional linguistic concepts to
facilitate our understanding and also study how the model space transforms
towards task-specific information. We perform a thorough analysis, comparing
pre-trained and fine-tuned models across three models and three downstream
tasks. The notable findings of our work are: i) the latent space of the higher
layers evolve towards task-specific concepts, ii) whereas the lower layers
retain generic concepts acquired in the pre-trained model, iii) we discovered
that some concepts in the higher layers acquire polarity towards the output
class, and iv) that these concepts can be used for generating adversarial
triggers.",https://github.com/nelson-liu/,4704
A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference,0.0236129,"We study the problem of combining neural networks with symbolic reasoning.
Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL),
such as DeepProbLog, perform exponential-time exact inference, limiting the
scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference
(A-NeSI): a new framework for PNL that uses neural networks for scalable
approximate inference. A-NeSI 1) performs approximate inference in polynomial
time without changing the semantics of probabilistic logics; 2) is trained
using data generated by the background knowledge; 3) can generate symbolic
explanations of predictions; and 4) can guarantee the satisfaction of logical
constraints at test time, which is vital in safety-critical applications. Our
experiments show that A-NeSI is the first end-to-end method to solve three
neurosymbolic tasks with exponential combinatorial scaling. Finally, our
experiments show that A-NeSI achieves explainability and safety without a
penalty in performance.",https://github.com/HEmile/a-nesi,40025
On the Utility of Prediction Sets in Human-AI Teams,0.0213838,"Research on human-AI teams usually provides experts with a single label,
which ignores the uncertainty in a model's recommendation. Conformal prediction
(CP) is a well established line of research that focuses on building a
theoretically grounded, calibrated prediction set, which may contain multiple
labels. We explore how such prediction sets impact expert decision-making in
human-AI teams. Our evaluation on human subjects finds that set valued
predictions positively impact experts. However, we notice that the predictive
sets provided by CP can be very large, which leads to unhelpful AI assistants.
To mitigate this, we introduce D-CP, a method to perform CP on some examples
and defer to experts. We prove that D-CP can reduce the prediction set size of
non-deferred examples. We show how D-CP performs in quantitative and in human
subject experiments ($n=120$). Our results suggest that CP prediction sets
improve human-AI team performance over showing the top-1 prediction alone, and
that experts find D-CP prediction sets are more useful than CP prediction sets.",https://github.com/cambridge-mlg/d-cp,9728
On Web-based Visual Corpus Construction for Visual Document Understanding,0.022225,"In recent years, research on visual document understanding (VDU) has grown
significantly, with a particular emphasis on the development of self-supervised
learning methods. However, one of the significant challenges faced in this
field is the limited availability of publicly accessible visual corpora or
extensive collections of images with detailed text annotations, particularly
for non-Latin or resource-scarce languages. To address this challenge, we
propose Web-based Visual Corpus Builder (Webvicob), a dataset generator engine
capable of constructing large-scale, multilingual visual corpora from raw
Wikipedia HTML dumps. Our experiments demonstrate that the data generated by
Webvicob can be used to train robust VDU models that perform well on various
downstream tasks, such as DocVQA and post-OCR parsing. Furthermore, when using
a dataset of 1 million images generated by Webvicob, we observed an improvement
of over 13% on the DocVQA Task 3 compared to a dataset of 11 million images
from the IIT-CDIP. The implementation of our engine is publicly available on
https://github.com/clovaai/webvicob",https://github.com/clovaai/webvicob,5053
Improving Simultaneous Machine Translation with Monolingual Data,0.101356,"Simultaneous machine translation (SiMT) is usually done via sequence-level
knowledge distillation (Seq-KD) from a full-sentence neural machine translation
(NMT) model. However, there is still a significant performance gap between NMT
and SiMT. In this work, we propose to leverage monolingual data to improve
SiMT, which trains a SiMT student on the combination of bilingual data and
external monolingual data distilled by Seq-KD. Preliminary experiments on En-Zh
and En-Ja news domain corpora demonstrate that monolingual data can
significantly improve translation quality (e.g., +3.15 BLEU on En-Zh). Inspired
by the behavior of human simultaneous interpreters, we propose a novel
monolingual sampling strategy for SiMT, considering both chunk length and
monotonicity. Experimental results show that our sampling strategy consistently
outperforms the random sampling strategy (and other conventional typical NMT
monolingual sampling strategies) by avoiding the key problem of SiMT --
hallucination, and has better scalability. We achieve +0.72 BLEU improvements
on average against random sampling on En-Zh and En-Ja. Data and codes can be
found at https://github.com/hexuandeng/Mono4SiMT.",https://github.com/hexuandeng/Mono4SiMT,123783
"Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",0.563233,"While the problem of hallucinations in neural machine translation has long
been recognized, so far the progress on its alleviation is very little. Indeed,
recently it turned out that without artificially encouraging models to
hallucinate, previously existing methods fall short and even the standard
sequence log-probability is more informative. It means that characteristics
internal to the model can give much more information than we expect, and before
using external models and measures, we first need to ask: how far can we go if
we use nothing but the translation model itself ? We propose to use a method
that evaluates the percentage of the source contribution to a generated
translation. Intuitively, hallucinations are translations ""detached"" from the
source, hence they can be identified by low source contribution. This method
improves detection accuracy for the most severe hallucinations by a factor of 2
and is able to alleviate hallucinations at test time on par with the previous
best approach that relies on external models. Next, if we move away from
internal model characteristics and allow external tools, we show that using
sentence similarity from cross-lingual embeddings further improves these
results.",https://github.com/facebookresearch/stopes,2821
FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery,0.0262085,"Understanding users' intentions in e-commerce platforms requires commonsense
knowledge. In this paper, we present FolkScope, an intention knowledge graph
construction framework to reveal the structure of humans' minds about
purchasing items. As commonsense knowledge is usually ineffable and not
expressed explicitly, it is challenging to perform information extraction.
Thus, we propose a new approach that leverages the generation power of large
language models~(LLMs) and human-in-the-loop annotation to semi-automatically
construct the knowledge graph. LLMs first generate intention assertions via
e-commerce-specific prompts to explain shopping behaviors, where the intention
can be an open reason or a predicate falling into one of 18 categories aligning
with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility
and typicality labels of sampled intentions as training data in order to
populate human judgments to all automatic generations. Last, to structurize the
assertions, we propose pattern mining and conceptualization to form more
condensed and abstract knowledge. Extensive evaluations and studies demonstrate
that our constructed knowledge graph can well model e-commerce knowledge and
have many potential applications.",https://github.com/HKUST-KnowComp/FolkScope,14329
Czech Dataset for Cross-lingual Subjectivity Classification,0.1393,"In this paper, we introduce a new Czech subjectivity dataset of 10k manually
annotated subjective and objective sentences from movie reviews and
descriptions. Our prime motivation is to provide a reliable dataset that can be
used with the existing English dataset as a benchmark to test the ability of
pre-trained multilingual models to transfer knowledge between Czech and English
and vice versa. Two annotators annotated the dataset reaching 0.83 of the
Cohen's \k{appa} inter-annotator agreement. To the best of our knowledge, this
is the first subjectivity dataset for the Czech language. We also created an
additional dataset that consists of 200k automatically labeled sentences. Both
datasets are freely available for research purposes. Furthermore, we fine-tune
five pre-trained BERT-like models to set a monolingual baseline for the new
dataset and we achieve 93.56% of accuracy. We fine-tune models on the existing
English dataset for which we obtained results that are on par with the current
state-of-the-art results. Finally, we perform zero-shot cross-lingual
subjectivity classification between Czech and English to verify the usability
of our dataset as the cross-lingual benchmark. We compare and discuss the
cross-lingual and monolingual results and the ability of multilingual models to
transfer knowledge between languages.",https://github.com/pauli31/czech-subjectivity-dataset,3078
Depth-aware Neural Style Transfer using Instance Normalization,0.103069,"Neural Style Transfer (NST) is concerned with the artistic stylization of
visual media. It can be described as the process of transferring the style of
an artistic image onto an ordinary photograph. Recently, a number of studies
have considered the enhancement of the depth-preserving capabilities of the NST
algorithms to address the undesired effects that occur when the input content
images include numerous objects at various depths. Our approach uses a deep
residual convolutional network with instance normalization layers that utilizes
an advanced depth prediction network to integrate depth preservation as an
additional loss function to content and style. We demonstrate results that are
effective in retaining the depth and global structure of content images. Three
different evaluation processes show that our system is capable of preserving
the structure of the stylized results while exhibiting style-capture
capabilities and aesthetic qualities comparable or superior to state-of-the-art
methods. Project page:
https://ioannoue.github.io/depth-aware-nst-using-in.html.",https://ioannoue.github.io/depth-aware-nst-using-in.html,97
Standby-Based Deadlock Avoidance Method for Multi-Agent Pickup and Delivery Tasks,0.300571,"The multi-agent pickup and delivery (MAPD) problem, in which multiple agents
iteratively carry materials without collisions, has received significant
attention. However, many conventional MAPD algorithms assume a specifically
designed grid-like environment, such as an automated warehouse. Therefore, they
have many pickup and delivery locations where agents can stay for a lengthy
period, as well as plentiful detours to avoid collisions owing to the freedom
of movement in a grid. By contrast, because a maze-like environment such as a
search-and-rescue or construction site has fewer pickup/delivery locations and
their numbers may be unbalanced, many agents concentrate on such locations
resulting in inefficient operations, often becoming stuck or deadlocked. Thus,
to improve the transportation efficiency even in a maze-like restricted
environment, we propose a deadlock avoidance method, called standby-based
deadlock avoidance (SBDA). SBDA uses standby nodes determined in real-time
using the articulation-point-finding algorithm, and the agent is guaranteed to
stay there for a finite amount of time. We demonstrated that our proposed
method outperforms a conventional approach. We also analyzed how the parameters
used for selecting standby nodes affect the performance.",None,-1
A Combined Approach of Process Mining and Rule-based AI for Study Planning and Monitoring in Higher Education,0.0263593,"This paper presents an approach of using methods of process mining and
rule-based artificial intelligence to analyze and understand study paths of
students based on campus management system data and study program models.
Process mining techniques are used to characterize successful study paths, as
well as to detect and visualize deviations from expected plans. These insights
are combined with recommendations and requirements of the corresponding study
programs extracted from examination regulations. Here, event calculus and
answer set programming are used to provide models of the study programs which
support planning and conformance checking while providing feedback on possible
study plan violations. In its combination, process mining and rule-based
artificial intelligence are used to support study planning and monitoring by
deriving rules and recommendations for guiding students to more suitable study
paths with higher success rates. Two applications will be implemented, one for
students and one for study program designers.",None,-1
Human-Centered Concept Explanations for Neural Networks,0.168384,"Understanding complex machine learning models such as deep neural networks
with explanations is crucial in various applications. Many explanations stem
from the model perspective, and may not necessarily effectively communicate why
the model is making its predictions at the right level of abstraction. For
example, providing importance weights to individual pixels in an image can only
express which parts of that particular image are important to the model, but
humans may prefer an explanation which explains the prediction by concept-based
thinking. In this work, we review the emerging area of concept based
explanations. We start by introducing concept explanations including the class
of Concept Activation Vectors (CAV) which characterize concepts using vectors
in appropriate spaces of neural activations, and discuss different properties
of useful concepts, and approaches to measure the usefulness of concept
vectors. We then discuss approaches to automatically extract concepts, and
approaches to address some of their caveats. Finally, we discuss some case
studies that showcase the utility of such concept-based explanations in
synthetic settings and real world applications.",None,-1
MentSum: A Resource for Exploring Summarization of Mental Health Online Posts,0.0,"Mental health remains a significant challenge of public health worldwide.
With increasing popularity of online platforms, many use the platforms to share
their mental health conditions, express their feelings, and seek help from the
community and counselors. Some of these platforms, such as Reachout, are
dedicated forums where the users register to seek help. Others such as Reddit
provide subreddits where the users publicly but anonymously post their mental
health distress. Although posts are of varying length, it is beneficial to
provide a short, but informative summary for fast processing by the counselors.
To facilitate research in summarization of mental health online posts, we
introduce Mental Health Summarization dataset, MentSum, containing over 24k
carefully selected user posts from Reddit, along with their short user-written
summary (called TLDR) in English from 43 mental health subreddits. This
domain-specific dataset could be of interest not only for generating short
summaries on Reddit, but also for generating summaries of posts on the
dedicated mental health forums such as Reachout. We further evaluate both
extractive and abstractive state-of-the-art summarization baselines in terms of
Rouge scores, and finally conduct an in-depth human evaluation study of both
user-written and system-generated summaries, highlighting challenges in this
research.",https://github.com/miso-belica/sumy,-1
Applying wav2vec2 for Speech Recognition on Bengali Common Voices Dataset,0.846832,"Speech is inherently continuous, where discrete words, phonemes and other
units are not clearly segmented, and so speech recognition has been an active
research problem for decades. In this work we have fine-tuned wav2vec 2.0 to
recognize and transcribe Bengali speech -- training it on the Bengali Common
Voice Speech Dataset. After training for 71 epochs, on a training set
consisting of 36919 mp3 files, we achieved a training loss of 0.3172 and WER of
0.2524 on a validation set of size 7,747. Using a 5-gram language model, the
Levenshtein Distance was 2.6446 on a test set of size 7,747. Then the training
set and validation set were combined, shuffled and split into 85-15 ratio.
Training for 7 more epochs on this combined dataset yielded an improved
Levenshtein Distance of 2.60753 on the test set. Our model was the best
performing one, achieving a Levenshtein Distance of 6.234 on a hidden dataset,
which was 1.1049 units lower than other competing submissions.",https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec,-1
User-Controllable Latent Transformer for StyleGAN Image Layout Editing,0.381482,"Latent space exploration is a technique that discovers interpretable latent
directions and manipulates latent codes to edit various attributes in images
generated by generative adversarial networks (GANs). However, in previous work,
spatial control is limited to simple transformations (e.g., translation and
rotation), and it is laborious to identify appropriate latent directions and
adjust their parameters. In this paper, we tackle the problem of editing the
StyleGAN image layout by annotating the image directly. To do so, we propose an
interactive framework for manipulating latent codes in accordance with the user
inputs. In our framework, the user annotates a StyleGAN image with locations
they want to move or not and specifies a movement direction by mouse dragging.
From these user inputs and initial latent codes, our latent transformer based
on a transformer encoder-decoder architecture estimates the output latent
codes, which are fed to the StyleGAN generator to obtain a result image. To
train our latent transformer, we utilize synthetic data and pseudo-user inputs
generated by off-the-shelf StyleGAN and optical flow models, without manual
supervision. Quantitative and qualitative evaluations demonstrate the
effectiveness of our method over existing methods.",https://github.com/justinpinkney/awesome-pretrained-stylegan2,-1
Measuring Geographic Performance Disparities of Offensive Language Classifiers,0.0313544,"Text classifiers are applied at scale in the form of one-size-fits-all
solutions. Nevertheless, many studies show that classifiers are biased
regarding different languages and dialects. When measuring and discovering
these biases, some gaps present themselves and should be addressed. First,
``Does language, dialect, and topical content vary across geographical
regions?'' and secondly ``If there are differences across the regions, do they
impact model performance?''. We introduce a novel dataset called GeoOLID with
more than 14 thousand examples across 15 geographically and demographically
diverse cities to address these questions. We perform a comprehensive analysis
of geographical-related content and their impact on performance disparities of
offensive language detection models. Overall, we find that current models do
not generalize across locations. Likewise, we show that while offensive
language models produce false positives on African American English, model
performance is not correlated with each city's minority population proportions.
Warning: This paper contains offensive language.",None,-1
Deep Learning based Automatic Detection of Dicentric Chromosome,0.0111227,"Automatic detection of dicentric chromosomes is an essential step to estimate
radiation exposure and development of end to end emergency bio dosimetry
systems. During accidents, a large amount of data is required to be processed
for extensive testing to formulate a medical treatment plan for the masses,
which requires this process to be automated. Current approaches require human
adjustments according to the data and therefore need a human expert to
calibrate the system. This paper proposes a completely data driven framework
which requires minimum intervention of field experts and can be deployed in
emergency cases with relative ease. Our approach involves YOLOv4 to detect the
chromosomes and remove the debris in each image, followed by a classifier that
differentiates between an analysable chromosome and a non-analysable one.
Images are extracted from YOLOv4 based on the protocols described by
WHO-BIODOSNET. The analysable chromosome is classified as Monocentric or
Dicentric and an image is accepted for consideration of dose estimation based
on the analysable chromosome count. We report an accuracy in dicentric
identification of 94.33% on a 1:1 split of Dicentric and Monocentric
Chromosomes.",None,-1
Overview of the WANLP 2022 Shared Task on Propaganda Detection in Arabic,0.991252,"Propaganda is the expression of an opinion or an action by an individual or a
group deliberately designed to influence the opinions or the actions of other
individuals or groups with reference to predetermined ends, which is achieved
by means of well-defined rhetorical and psychological devices. Propaganda
techniques are commonly used in social media to manipulate or to mislead users.
Thus, there has been a lot of recent research on automatic detection of
propaganda techniques in text as well as in memes. However, so far the focus
has been primarily on English. With the aim to bridge this language gap, we ran
a shared task on detecting propaganda techniques in Arabic tweets as part of
the WANLP 2022 workshop, which included two subtasks. Subtask~1 asks to
identify the set of propaganda techniques used in a tweet, which is a
multilabel classification problem, while Subtask~2 asks to detect the
propaganda techniques used in a tweet together with the exact span(s) of text
in which each propaganda technique appears. The task attracted 63 team
registrations, and eventually 14 and 3 teams made submissions for subtask 1 and
2, respectively. Finally, 11 teams submitted system description papers.",None,-1
Unsupervised Segmentation of Hyperspectral Remote Sensing Images with Superpixels,0.453867,"In this paper, we propose an unsupervised method for hyperspectral remote
sensing image segmentation. The method exploits the mean-shift clustering
algorithm that takes as input a preliminary hyperspectral superpixels
segmentation together with the spectral pixel information. The proposed method
does not require the number of segmentation classes as input parameter, and it
does not exploit any a-priori knowledge about the type of land-cover or
land-use to be segmented (e.g. water, vegetation, building etc.). Experiments
on Salinas, SalinasA, Pavia Center and Pavia University datasets are carried
out. Performance are measured in terms of normalized mutual information,
adjusted Rand index and F1-score. Results demonstrate the validity of the
proposed method in comparison with the state of the art.",https://github.com/mpBarbato/Unsupervised-Segmentation-of-Hyperspectral-Remote-Sensing-Images-with-Superpixels,-1
Backdoor Attack is a Devil in Federated GAN-based Medical Image Synthesis,0.135652,"Deep Learning-based image synthesis techniques have been applied in
healthcare research for generating medical images to support open research.
Training generative adversarial neural networks (GAN) usually requires large
amounts of training data. Federated learning (FL) provides a way of training a
central model using distributed data from different medical institutions while
keeping raw data locally. However, FL is vulnerable to backdoor attack, an
adversarial by poisoning training data, given the central server cannot access
the original data directly. Most backdoor attack strategies focus on
classification models and centralized domains. In this study, we propose a way
of attacking federated GAN (FedGAN) by treating the discriminator with a
commonly used data poisoning strategy in backdoor attack classification models.
We demonstrate that adding a small trigger with size less than 0.5 percent of
the original image size can corrupt the FL-GAN model. Based on the proposed
attack, we provide two effective defense strategies: global malicious detection
and local training regularization. We show that combining the two defense
strategies yields a robust medical image generation.",None,-1
Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction,0.20498,"Dimensionality reduction is crucial both for visualization and preprocessing
high dimensional data for machine learning. We introduce a novel method based
on a hierarchy built on 1-nearest neighbor graphs in the original space which
is used to preserve the grouping properties of the data distribution on
multiple levels. The core of the proposal is an optimization-free projection
that is competitive with the latest versions of t-SNE and UMAP in performance
and visualization quality while being an order of magnitude faster in run-time.
Furthermore, its interpretable mechanics, the ability to project new data, and
the natural separation of data clusters in visualizations make it a general
purpose unsupervised dimension reduction technique. In the paper, we argue
about the soundness of the proposed method and evaluate it on a diverse
collection of datasets with sizes varying from 1K to 11M samples and dimensions
from 28 to 16K. We perform comparisons with other state-of-the-art methods on
multiple metrics and target dimensions highlighting its efficiency and
performance. Code is available at https://github.com/koulakis/h-nne",https://github.com/koulakis/h-nne,-1
Facial Action Unit Recognition Based on Transfer Learning,0.0671004,"Facial action unit recognition is an important task for facial analysis.
Owing to the complex collection environment, facial action unit recognition in
the wild is still challenging. The 3rd competition on affective behavior
analysis in-the-wild (ABAW) has provided large amount of facial images with
facial action unit annotations. In this paper, we introduce a facial action
unit recognition method based on transfer learning. We first use available
facial images with expression labels to train the feature extraction network.
Then we fine-tune the network for facial action unit recognition.",None,-1
PLM-ICD: Automatic ICD Coding with Pretrained Language Models,0.718648,"Automatically classifying electronic health records (EHRs) into diagnostic
codes has been challenging to the NLP community. State-of-the-art methods
treated this problem as a multilabel classification problem and proposed
various architectures to model this problem. However, these systems did not
leverage the superb performance of pretrained language models, which achieved
superb performance on natural language understanding tasks. Prior work has
shown that pretrained language models underperformed on this task with the
regular finetuning scheme. Therefore, this paper aims at analyzing the causes
of the underperformance and developing a framework for automatic ICD coding
with pretrained language models. We spotted three main issues through the
experiments: 1) large label space, 2) long input sequences, and 3) domain
mismatch between pretraining and fine-tuning. We propose PLMICD, a framework
that tackles the challenges with various strategies. The experimental results
show that our proposed framework can overcome the challenges and achieves
state-of-the-art performance in terms of multiple metrics on the benchmark
MIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD",https://github.com/MiuLab/PLM-ICD,-1
Anomaly detection optimization using big data and deep learning to reduce false-positive,0.125993,"Anomaly-based Intrusion Detection System (IDS) has been a hot research topic
because of its ability to detect new threats rather than only memorized
signatures threats of signature-based IDS. Especially after the availability of
advanced technologies that increase the number of hacking tools and increase
the risk impact of an attack. The problem of any anomaly-based model is its
high false-positive rate. The high false-positive rate is the reason why
anomaly IDS is not commonly applied in practice. Because anomaly-based models
classify an unseen pattern as a threat where it may be normal but not included
in the training dataset. This type of problem is called overfitting where the
model is not able to generalize. Optimizing Anomaly-based models by having a
big training dataset that includes all possible normal cases may be an optimal
solution but could not be applied in practice. Although we can increase the
number of training samples to include much more normal cases, still we need a
model that has more ability to generalize. In this research paper, we propose
applying deep model instead of traditional models because it has more ability
to generalize. Thus, we will obtain less false-positive by using big data and
deep model. We made a comparison between machine learning and deep learning
algorithms in the optimization of anomaly-based IDS by decreasing the
false-positive rate. We did an experiment on the NSL-KDD benchmark and compared
our results with one of the best used classifiers in traditional learning in
IDS optimization. The experiment shows 10% lower false-positive by using deep
learning instead of traditional learning.",None,-1
Learning to Reuse Distractors to support Multiple Choice Question Generation in Education,0.10301,"Multiple choice questions (MCQs) are widely used in digital learning systems,
as they allow for automating the assessment process. However, due to the
increased digital literacy of students and the advent of social media
platforms, MCQ tests are widely shared online, and teachers are continuously
challenged to create new questions, which is an expensive and time-consuming
task. A particularly sensitive aspect of MCQ creation is to devise relevant
distractors, i.e., wrong answers that are not easily identifiable as being
wrong. This paper studies how a large existing set of manually created answers
and distractors for questions over a variety of domains, subjects, and
languages can be leveraged to help teachers in creating new MCQs, by the smart
reuse of existing distractors. We built several data-driven models based on
context-aware question and distractor representations, and compared them with
static feature-based models. The proposed models are evaluated with automated
metrics and in a realistic user test with teachers. Both automatic and human
evaluations indicate that context-aware models consistently outperform a static
feature-based approach. For our best-performing context-aware model, on average
3 distractors out of the 10 shown to teachers were rated as high-quality
distractors. We create a performance benchmark, and make it public, to enable
comparison between different approaches and to introduce a more standardized
evaluation of the task. The benchmark contains a test of 298 educational
questions covering multiple subjects & languages and a 77k multilingual pool of
distractor vocabulary for future research.",https://github.com/semerekiros/dist-retrieval,-1
A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning,0.0116928,"Large datasets as required for deep learning of lip reading do not exist in
many languages. In this paper we present the dataset GLips (German Lips)
consisting of 250,000 publicly available videos of the faces of speakers of the
Hessian Parliament, which was processed for word-level lip reading using an
automatic pipeline. The format is similar to that of the English language LRW
(Lip Reading in the Wild) dataset, with each video encoding one word of
interest in a context of 1.16 seconds duration, which yields compatibility for
studying transfer learning between both datasets. By training a deep neural
network, we investigate whether lip reading has language-independent features,
so that datasets of different languages can be used to improve lip reading
models. We demonstrate learning from scratch and show that transfer learning
from LRW to GLips and vice versa improves learning speed and performance, in
particular for the validation set.",None,-1
Aggregate effects of advertising decisions: a complex systems look at search engine advertising via an experimental study,0.365903,"Purpose: We model group advertising decisions, which are the collective
decisions of every single advertiser within the set of advertisers who are
competing in the same auction or vertical industry, and examine resulting
market outcomes, via a proposed simulation framework named EXP-SEA
(Experimental Platform for Search Engine Advertising) supporting experimental
studies of collective behaviors in the context of search engine advertising.
Design: We implement the EXP-SEA to validate the proposed simulation framework,
also conduct three experimental studies on the aggregate impact of electronic
word-of-mouth, the competition level, and strategic bidding behaviors. EXP-SEA
supports heterogeneous participants, various auction mechanisms, and also
ranking and pricing algorithms. Findings: Findings from our three experiments
show that (a) both the market profit and advertising indexes such as number of
impressions and number of clicks are larger when the eWOM effect presents,
meaning social media certainly has some effect on search engine advertising
outcomes, (b) the competition level has a monotonic increasing effect on the
market performance, thus search engines have an incentive to encourage both the
eWOM among search users and competition among advertisers, and (c) given the
market-level effect of the percentage of advertisers employing a dynamic greedy
bidding strategy, there is a cut-off point for strategic bidding behaviors.
Originality: This is one of the first research works to explore collective
group decisions and resulting phenomena in the complex context of search engine
advertising via developing and validating a simulation framework that supports
assessments of various advertising strategies and estimations of the impact of
mechanisms on the search market.",None,-1
An Empirical Study on Disentanglement of Negative-free Contrastive Learning,0.0483385,"Negative-free contrastive learning methods have attracted a lot of attention
with simplicity and impressive performances for large-scale pretraining.
However, its disentanglement property remains unexplored. In this paper, we
examine negative-free contrastive learning methods to study the disentanglement
property empirically. We find that existing disentanglement metrics fail to
make meaningful measurements for high-dimensional representation models, so we
propose a new disentanglement metric based on Mutual Information between latent
representations and data factors. With this proposed metric, we benchmark the
disentanglement property of negative-free contrastive learning on both popular
synthetic datasets and a real-world dataset CelebA. Our study shows that the
investigated methods can learn a well-disentangled subset of representation. As
far as we know, we are the first to extend the study of disentangled
representation learning to high-dimensional representation space and introduce
negative-free contrastive learning methods into this area. The source code of
this paper is available at
\url{https://github.com/noahcao/disentanglement_lib_med}.",https://github.com/noahcao/disentanglement_lib_med,-1
Using Deep Mixture-of-Experts to Detect Word Meaning Shift for TempoWiC,0.416355,"This paper mainly describes the dma submission to the TempoWiC task, which
achieves a macro-F1 score of 77.05% and attains the first place in this task.
We first explore the impact of different pre-trained language models. Then we
adopt data cleaning, data augmentation, and adversarial training strategies to
enhance the model generalization and robustness. For further improvement, we
integrate POS information and word semantic representation using a
Mixture-of-Experts (MoE) approach. The experimental results show that MoE can
overcome the feature overuse issue and combine the context, POS, and word
semantic features well. Additionally, we use a model ensemble method for the
final prediction, which has been proven effective by many research works.",None,-1
Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems,0.226315,"Users interacting with voice assistants today need to phrase their requests
in a very specific manner to elicit an appropriate response. This limits the
user experience, and is partly due to the lack of reasoning capabilities of
dialogue platforms and the hand-crafted rules that require extensive labor. One
possible way to improve user experience and relieve the manual efforts of
designers is to build an end-to-end dialogue system that can do reasoning
itself while perceiving user's utterances. In this work, we propose a novel
method to incorporate the knowledge reasoning capability into dialogue systems
in a more scalable and generalizable manner. Our proposed method allows a
single transformer model to directly walk on a large-scale knowledge graph to
generate responses. To the best of our knowledge, this is the first work to
have transformer models generate responses by reasoning over differentiable
knowledge graphs. We investigate the reasoning abilities of the proposed method
on both task-oriented and domain-specific chit-chat dialogues. Empirical
results show that this method can effectively and efficiently incorporate a
knowledge graph into a dialogue system with fully-interpretable reasoning
paths.",https://github.com/Pascalson/DiffKG-Dialog,-1
Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges,0.260691,"This is a report on the NSF Future Directions Workshop on Automatic
Evaluation of Dialog. The workshop explored the current state of the art along
with its limitations and suggested promising directions for future work in this
important and very rapidly changing area of research.",None,-1
Transcending Scaling Laws with 0.1% Extra Compute,0.187019,"Scaling language models improves performance but comes with significant
computational costs. This paper proposes UL2R, a method that substantially
improves existing language models and their scaling curves with a relatively
tiny amount of extra compute. The key idea is to continue training a
state-of-the-art large language model (e.g., PaLM) on a few more steps with
UL2's mixture-of-denoiser objective. We show that, with almost negligible extra
computational costs and no new sources of data, we are able to substantially
improve the scaling properties of large language models on downstream metrics.
In this paper, we continue training PaLM with UL2R, introducing a new set of
models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B
scale, we show an approximately 2x computational savings rate where U-PaLM
achieves the same performance as the final PaLM 540B model at around half its
computational budget (i.e., saving $\sim$4.4 million TPUv4 hours). We further
show that this improved scaling curve leads to 'emergent abilities' on
challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM
on some tasks or demonstrates better quality at much smaller scale (62B as
opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many
few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question
answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual
tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide
qualitative examples showing the new capabilities of U-PaLM for single and
multi-span infilling.",None,-1
Explaining Chest X-ray Pathologies in Natural Language,0.194063,"Most deep learning algorithms lack explanations for their predictions, which
limits their deployment in clinical practice. Approaches to improve
explainability, especially in medical imaging, have often been shown to convey
limited information, be overly reassuring, or lack robustness. In this work, we
introduce the task of generating natural language explanations (NLEs) to
justify predictions made on medical images. NLEs are human-friendly and
comprehensive, and enable the training of intrinsically explainable models. To
this goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging
dataset with NLEs. It contains over 38,000 NLEs, which explain the presence of
various thoracic pathologies and chest X-ray findings. We propose a general
approach to solve the task and evaluate several architectures on this dataset,
including via clinician assessment.",https://github.com/maximek3/MIMIC-NLE,-1
"Global Counterfactual Explanations: Investigations, Implementations and Improvements",0.15848,"Counterfactual explanations have been widely studied in explainability, with
a range of application dependent methods emerging in fairness, recourse and
model understanding. However, the major shortcoming associated with these
methods is their inability to provide explanations beyond the local or
instance-level. While some works touch upon the notion of a global explanation,
typically suggesting to aggregate masses of local explanations in the hope of
ascertaining global properties, few provide frameworks that are either reliable
or computationally tractable. Meanwhile, practitioners are requesting more
efficient and interactive explainability tools. We take this opportunity to
investigate existing global methods, with a focus on implementing and improving
Actionable Recourse Summaries (AReS), the only known global counterfactual
explanation framework for recourse.",None,-1
Task-Balanced Distillation for Object Detection,0.104887,"Mainstream object detectors are commonly constituted of two sub-tasks,
including classification and regression tasks, implemented by two parallel
heads. This classic design paradigm inevitably leads to inconsistent spatial
distributions between classification score and localization quality (IOU).
Therefore, this paper alleviates this misalignment in the view of knowledge
distillation. First, we observe that the massive teacher achieves a higher
proportion of harmonious predictions than the lightweight student. Based on
this intriguing observation, a novel Harmony Score (HS) is devised to estimate
the alignment of classification and regression qualities. HS models the
relationship between two sub-tasks and is seen as prior knowledge to promote
harmonious predictions for the student. Second, this spatial misalignment will
result in inharmonious region selection when distilling features. To alleviate
this problem, a novel Task-decoupled Feature Distillation (TFD) is proposed by
flexibly balancing the contributions of classification and regression tasks.
Eventually, HD and TFD constitute the proposed method, named Task-Balanced
Distillation (TBD). Extensive experiments demonstrate the considerable
potential and generalization of the proposed method. Specifically, when
equipped with TBD, RetinaNet with ResNet-50 achieves 41.0 mAP under the COCO
benchmark, outperforming the recent FGD and FRS.",None,-1
Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation,0.219117,"Generating adversarial examples for Neural Machine Translation (NMT) with
single Round-Trip Translation (RTT) has achieved promising results by releasing
the meaning-preserving restriction. However, a potential pitfall for this
approach is that we cannot decide whether the generated examples are
adversarial to the target NMT model or the auxiliary backward one, as the
reconstruction error through the RTT can be related to either. To remedy this
problem, we propose a new criterion for NMT adversarial examples based on the
Doubly Round-Trip Translation (DRTT). Specifically, apart from the
source-target-source RTT, we also consider the target-source-target one, which
is utilized to pick out the authentic adversarial examples for the target NMT
model. Additionally, to enhance the robustness of the NMT model, we introduce
the masked language models to construct bilingual adversarial pairs based on
DRTT, which are used to train the NMT model directly. Extensive experiments on
both the clean and noisy test sets (including the artificial and natural noise)
show that our approach substantially improves the robustness of NMT models.",https://github.com/lisasiyu/DRTT,-1
Image Segmentation-based Unsupervised Multiple Objects Discovery,0.027201,"Unsupervised object discovery aims to localize objects in images, while
removing the dependence on annotations required by most deep learning-based
methods. To address this problem, we propose a fully unsupervised, bottom-up
approach, for multiple objects discovery. The proposed approach is a two-stage
framework. First, instances of object parts are segmented by using the
intra-image similarity between self-supervised local features. The second step
merges and filters the object parts to form complete object instances. The
latter is performed by two CNN models that capture semantic information on
objects from the entire dataset. We demonstrate that the pseudo-labels
generated by our method provide a better precision-recall trade-off than
existing single and multiple objects discovery methods. In particular, we
provide state-of-the-art results for both unsupervised class-agnostic object
detection and unsupervised image segmentation.",None,-1
Positive Unlabeled Contrastive Learning,0.0900314,"Self-supervised pretraining on unlabeled data followed by supervised
fine-tuning on labeled data is a popular paradigm for learning from limited
labeled examples. We extend this paradigm to the classical positive unlabeled
(PU) setting, where the task is to learn a binary classifier given only a few
labeled positive samples, and (often) a large amount of unlabeled samples
(which could be positive or negative).
  We first propose a simple extension of standard infoNCE family of contrastive
losses, to the PU setting; and show that this learns superior representations,
as compared to existing unsupervised and supervised approaches. We then develop
a simple methodology to pseudo-label the unlabeled samples using a new
PU-specific clustering scheme; these pseudo-labels can then be used to train
the final (positive vs. negative) classifier. Our method handily outperforms
state-of-the-art PU methods over several standard PU benchmark datasets, while
not requiring a-priori knowledge of any class prior (which is a common
assumption in other PU methods). We also provide a simple theoretical analysis
that motivates our methods.",None,-1
A Web Application for Experimenting and Validating Remote Measurement of Vital Signs,0.0251424,"With a surge in online medical advising remote monitoring of patient vitals
is required. This can be facilitated with the Remote Photoplethysmography
(rPPG) techniques that compute vital signs from facial videos. It involves
processing video frames to obtain skin pixels, extracting the cardiac data from
it and applying signal processing filters to extract the Blood Volume Pulse
(BVP) signal. Different algorithms are applied to the BVP signal to estimate
the various vital signs. We implemented a web application framework to measure
a person's Heart Rate (HR), Heart Rate Variability (HRV), Oxygen Saturation
(SpO2), Respiration Rate (RR), Blood Pressure (BP), and stress from the face
video. The rPPG technique is highly sensitive to illumination and motion
variation. The web application guides the users to reduce the noise due to
these variations and thereby yield a cleaner BVP signal. The accuracy and
robustness of the framework was validated with the help of volunteers.",None,-1
Adaptive Discrete Communication Bottlenecks with Dynamic Vector Quantization,0.0985149,"Vector Quantization (VQ) is a method for discretizing latent representations
and has become a major part of the deep learning toolkit. It has been
theoretically and empirically shown that discretization of representations
leads to improved generalization, including in reinforcement learning where
discretization can be used to bottleneck multi-agent communication to promote
agent specialization and robustness. The discretization tightness of most
VQ-based methods is defined by the number of discrete codes in the
representation vector and the codebook size, which are fixed as
hyperparameters. In this work, we propose learning to dynamically select
discretization tightness conditioned on inputs, based on the hypothesis that
data naturally contains variations in complexity that call for different levels
of representational coarseness. We show that dynamically varying tightness in
communication bottlenecks can improve model performance on visual reasoning and
reinforcement learning tasks.",None,-1
Improving Cross-Modal Retrieval with Set of Diverse Embeddings,0.00939534,"Cross-modal retrieval across image and text modalities is a challenging task
due to its inherent ambiguity: An image often exhibits various situations, and
a caption can be coupled with diverse images. Set-based embedding has been
studied as a solution to this problem. It seeks to encode a sample into a set
of different embedding vectors that capture different semantics of the sample.
In this paper, we present a novel set-based embedding method, which is distinct
from previous work in two aspects. First, we present a new similarity function
called smooth-Chamfer similarity, which is designed to alleviate the side
effects of existing similarity functions for set-based embedding. Second, we
propose a novel set prediction module to produce a set of embedding vectors
that effectively captures diverse semantics of input by the slot attention
mechanism. Our method is evaluated on the COCO and Flickr30K datasets across
different visual backbones, where it outperforms existing methods including
ones that demand substantially larger computation at inference.",None,-1
Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology,0.109145,"Histopathology whole slide images (WSIs) can reveal significant
inter-hospital variability such as illumination, color or optical artifacts.
These variations, caused by the use of different scanning protocols across
medical centers (staining, scanner), can strongly harm algorithms
generalization on unseen protocols. This motivates development of new methods
to limit such drop of performances. In this paper, to enhance robustness on
unseen target protocols, we propose a new test-time data augmentation based on
multi domain image-to-image translation. It allows to project images from
unseen protocol into each source domain before classifying them and ensembling
the predictions. This test-time augmentation method results in a significant
boost of performances for domain generalization. To demonstrate its
effectiveness, our method has been evaluated on 2 different histopathology
tasks where it outperforms conventional domain generalization, standard H&E
specific color augmentation/normalization and standard test-time augmentation
techniques. Our code is publicly available at
https://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling.",https://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling,-1
An Intelligent Assistant for Converting City Requirements to Formal Specification,0.061598,"As more and more monitoring systems have been deployed to smart cities, there
comes a higher demand for converting new human-specified requirements to
machine-understandable formal specifications automatically. However, these
human-specific requirements are often written in English and bring missing,
inaccurate, or ambiguous information. In this paper, we present CitySpec, an
intelligent assistant system for requirement specification in smart cities.
CitySpec not only helps overcome the language differences brought by English
requirements and formal specifications, but also offers solutions to those
missing, inaccurate, or ambiguous information. The goal of this paper is to
demonstrate how CitySpec works. Specifically, we present three demos: (1)
interactive completion of requirements in CitySpec; (2) human-in-the-loop
correction while CitySepc encounters exceptions; (3) online learning in
CitySpec.",None,-1
CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation,0.864665,"Existing reference-free metrics have obvious limitations for evaluating
controlled text generation models. Unsupervised metrics can only provide a
task-agnostic evaluation result which correlates weakly with human judgments,
whereas supervised ones may overfit task-specific data with poor generalization
ability to other datasets. In this paper, we propose an unsupervised
reference-free metric called CTRLEval, which evaluates controlled text
generation from different aspects by formulating each aspect into multiple text
infilling tasks. On top of these tasks, the metric assembles the generation
probabilities from a pre-trained language model without any model training.
Experimental results show that our metric has higher correlations with human
judgments than other baselines, while obtaining better generalization of
evaluating generated texts from different models and with different qualities.",https://github.com/thu-coai/CTRLEval,-1
Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest,0.0660193,"Medical Relation Extraction (MRE) task aims to extract relations between
entities in medical texts. Traditional relation extraction methods achieve
impressive success by exploring the syntactic information, e.g., dependency
tree. However, the quality of the 1-best dependency tree for medical texts
produced by an out-of-domain parser is relatively limited so that the
performance of medical relation extraction method may degenerate. To this end,
we propose a method to jointly model semantic and syntactic information from
medical texts based on causal explanation theory. We generate dependency
forests consisting of the semantic-embedded 1-best dependency tree. Then, a
task-specific causal explainer is adopted to prune the dependency forests,
which are further fed into a designed graph convolutional network to learn the
corresponding representation for downstream task. Empirically, the various
comparisons on benchmark medical datasets demonstrate the effectiveness of our
model.",None,-1
Box2Seg: Learning Semantics of 3D Point Clouds with Box-Level Supervision,0.0,"Learning dense point-wise semantics from unstructured 3D point clouds with
fewer labels, although a realistic problem, has been under-explored in
literature. While existing weakly supervised methods can effectively learn
semantics with only a small fraction of point-level annotations, we find that
the vanilla bounding box-level annotation is also informative for semantic
segmentation of large-scale 3D point clouds. In this paper, we introduce a
neural architecture, termed Box2Seg, to learn point-level semantics of 3D point
clouds with bounding box-level supervision. The key to our approach is to
generate accurate pseudo labels by exploring the geometric and topological
structure inside and outside each bounding box. Specifically, an
attention-based self-training (AST) technique and Point Class Activation
Mapping (PCAM) are utilized to estimate pseudo-labels. The network is further
trained and refined with pseudo labels. Experiments on two large-scale
benchmarks including S3DIS and ScanNet demonstrate the competitive performance
of the proposed method. In particular, the proposed network can be trained with
cheap, or even off-the-shelf bounding box-level annotations and subcloud-level
tags.",None,-1
Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework,0.106779,"Simultaneous machine translation (SiMT) starts translating while receiving
the streaming source inputs, and hence the source sentence is always incomplete
during translating. Different from the full-sentence MT using the conventional
seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,
which forces each target word to only align with a partial source prefix to
adapt to the incomplete source in streaming inputs. However, the source words
in the front positions are always illusoryly considered more important since
they appear in more prefixes, resulting in position bias, which makes the model
pay more attention on the front source positions in testing. In this paper, we
first analyze the phenomenon of position bias in SiMT, and develop a
Length-Aware Framework to reduce the position bias by bridging the structural
gap between SiMT and full-sentence MT. Specifically, given the streaming
inputs, we first predict the full-sentence length and then fill the future
source position with positional encoding, thereby turning the streaming inputs
into a pseudo full-sentence. The proposed framework can be integrated into most
existing SiMT methods to further improve performance. Experiments on two
representative SiMT methods, including the state-of-the-art adaptive policy,
show that our method successfully reduces the position bias and thereby
achieves better SiMT performance.",https://github.com/pytorch/fairseq/tree/master/examples/simultaneous_translation,-1
SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners,0.165422,"Recently, self-supervised Masked Autoencoders (MAE) have attracted
unprecedented attention for their impressive representation learning ability.
However, the pretext task, Masked Image Modeling (MIM), reconstructs the
missing local patches, lacking the global understanding of the image. This
paper extends MAE to a fully supervised setting by adding a supervised
classification branch, thereby enabling MAE to learn global features from
golden labels effectively. The proposed Supervised MAE (SupMAE) only exploits a
visible subset of image patches for classification, unlike the standard
supervised pre-training where all image patches are used. Through experiments,
we demonstrate that SupMAE is not only more training efficient but it also
learns more robust and transferable features. Specifically, SupMAE achieves
comparable performance with MAE using only 30% of compute when evaluated on
ImageNet with the ViT-B/16 model. SupMAE's robustness on ImageNet variants and
transfer learning performance outperforms MAE and standard supervised
pre-training counterparts. Codes are available at
https://github.com/enyac-group/supmae.",https://github.com/enyac-group/supmae,-1
Explainability in reinforcement learning: perspective and position,0.0866628,"Artificial intelligence (AI) has been embedded into many aspects of people's
daily lives and it has become normal for people to have AI make decisions for
them. Reinforcement learning (RL) models increase the space of solvable
problems with respect to other machine learning paradigms. Some of the most
interesting applications are in situations with non-differentiable expected
reward function, operating in unknown or underdefined environment, as well as
for algorithmic discovery that surpasses performance of any teacher, whereby
agent learns from experimental experience through simple feedback. The range of
applications and their social impact is vast, just to name a few: genomics,
game-playing (chess, Go, etc.), general optimization, financial investment,
governmental policies, self-driving cars, recommendation systems, etc. It is
therefore essential to improve the trust and transparency of RL-based systems
through explanations. Most articles dealing with explainability in artificial
intelligence provide methods that concern supervised learning and there are
very few articles dealing with this in the area of RL. The reasons for this are
the credit assignment problem, delayed rewards, and the inability to assume
that data is independently and identically distributed (i.i.d.). This position
paper attempts to give a systematic overview of existing methods in the
explainable RL area and propose a novel unified taxonomy, building and
expanding on the existing ones. The position section describes pragmatic
aspects of how explainability can be observed. The gap between the parties
receiving and generating the explanation is especially emphasized. To reduce
the gap and achieve honesty and truthfulness of explanations, we set up three
pillars: proactivity, risk attitudes, and epistemological constraints. To this
end, we illustrate our proposal on simple variants of the shortest path
problem.",None,-1
Prediction-based One-shot Dynamic Parking Pricing,0.0223606,"Many U.S. metropolitan cities are notorious for their severe shortage of
parking spots. To this end, we present a proactive prediction-driven
optimization framework to dynamically adjust parking prices. We use
state-of-the-art deep learning technologies such as neural ordinary
differential equations (NODEs) to design our future parking occupancy rate
prediction model given historical occupancy rates and price information. Owing
to the continuous and bijective characteristics of NODEs, in addition, we
design a one-shot price optimization method given a pre-trained prediction
model, which requires only one iteration to find the optimal solution. In other
words, we optimize the price input to the pre-trained prediction model to
achieve targeted occupancy rates in the parking blocks. We conduct experiments
with the data collected in San Francisco and Seattle for years. Our prediction
model shows the best accuracy in comparison with various temporal or
spatio-temporal forecasting models. Our one-shot optimization method greatly
outperforms other black-box and white-box search methods in terms of the search
time and always returns the optimal price solution.",None,-1
Human-centric Image Cropping with Partition-aware and Content-preserving Features,0.0915717,"Image cropping aims to find visually appealing crops in an image, which is an
important yet challenging task. In this paper, we consider a specific and
practical application: human-centric image cropping, which focuses on the
depiction of a person. To this end, we propose a human-centric image cropping
method with two novel feature designs for the candidate crop: partition-aware
feature and content-preserving feature. For partition-aware feature, we divide
the whole image into nine partitions based on the human bounding box and treat
different partitions in a candidate crop differently conditioned on the human
information. For content-preserving feature, we predict a heatmap indicating
the important content to be included in a good crop, and extract the geometric
relation between the heatmap and a candidate crop. Extensive experiments
demonstrate that our method can perform favorably against state-of-the-art
image cropping methods on human-centric image cropping task. Code is available
at https://github.com/bcmi/Human-Centric-Image-Cropping.",https://github.com/bcmi/Human-Centric-Image-Cropping,-1
Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning,0.147612,"Automatically discovering failures in vision models under real-world settings
remains an open challenge. This work demonstrates how off-the-shelf,
large-scale, image-to-text and text-to-image models, trained on vast amounts of
data, can be leveraged to automatically find such failures. In essence, a
conditional text-to-image generative model is used to generate large amounts of
synthetic, yet realistic, inputs given a ground-truth label. Misclassified
inputs are clustered and a captioning model is used to describe each cluster.
Each cluster's description is used in turn to generate more inputs and assess
whether specific clusters induce more failures than expected. We use this
pipeline to demonstrate that we can effectively interrogate classifiers trained
on ImageNet to find specific failure cases and discover spurious correlations.
We also show that we can scale the approach to generate adversarial datasets
targeting specific classifier architectures. This work serves as a
proof-of-concept demonstrating the utility of large-scale generative models to
automatically discover bugs in vision models in an open-ended manner. We also
describe a number of limitations and pitfalls related to this approach.",None,-1
Evolved Open-Endedness in Cultural Evolution: A New Dimension in Open-Ended Evolution Research,0.0416543,"The goal of Artificial Life research, as articulated by Chris Langton, is ""to
contribute to theoretical biology by locating life-as-we-know-it within the
larger picture of life-as-it-could-be"" (1989, p.1). The study and pursuit of
open-ended evolution in artificial evolutionary systems exemplifies this goal.
However, open-ended evolution research is hampered by two fundamental issues;
the struggle to replicate open-endedness in an artificial evolutionary system,
and the fact that we only have one system (genetic evolution) from which to
draw inspiration. Here we argue that cultural evolution should be seen not only
as another real-world example of an open-ended evolutionary system, but that
the unique qualities seen in cultural evolution provide us with a new
perspective from which we can assess the fundamental properties of, and ask new
questions about, open-ended evolutionary systems, especially in regard to
evolved open-endedness and transitions from bounded to unbounded evolution.
Here we provide an overview of culture as an evolutionary system, highlight the
interesting case of human cultural evolution as an open-ended evolutionary
system, and contextualise cultural evolution under the framework of (evolved)
open-ended evolution. We go on to provide a set of new questions that can be
asked once we consider cultural evolution within the framework of open-ended
evolution, and introduce new insights that we may be able to gain about evolved
open-endedness as a result of asking these questions.",None,-1
Missingness Bias in Model Debugging,0.0450498,"Missingness, or the absence of features from an input, is a concept
fundamental to many model debugging tools. However, in computer vision, pixels
cannot simply be removed from an image. One thus tends to resort to heuristics
such as blacking out pixels, which may in turn introduce bias into the
debugging process. We study such biases and, in particular, show how
transformer-based architectures can enable a more natural implementation of
missingness, which side-steps these issues and improves the reliability of
model debugging in practice. Our code is available at
https://github.com/madrylab/missingness",https://github.com/madrylab/missingness,-1
First Competitive Ant Colony Scheme for the CARP,0.176177,"This paper addresses the Capacitated Arc Routing Problem (CARP) using an Ant
Colony Optimization scheme. Ant Colony schemes can compute solutions for medium
scale instances of VRP. The proposed Ant Colony is dedicated to large-scale
instances of CARP with more than 140 nodes and 190 arcs to service. The Ant
Colony scheme is coupled with a local search procedure and provides high
quality solutions. The benchmarks we carried out prove possible to obtain
solutions as profitable as CARPET ones can be obtained using such scheme when a
sufficient number of iterations is devoted to the ants. It competes with the
Genetic Algorithm of Lacomme et al. regarding solution quality but it is more
time consuming on large scale instances. The method has been intensively
benchmarked on the well-known instances of Eglese, DeArmon and the last ones of
Belenguer and Benavent. This research report is a step forward CARP resolution
by Ant Colony proving ant schemes can compete with Taboo search methods and
Genetic Algorithms",None,-1
Deep Convolutional Learning-Aided Detector for Generalized Frequency Division Multiplexing with Index Modulation,0.11584,"In this paper, a deep convolutional neural network-based symbol detection and
demodulation is proposed for generalized frequency division multiplexing with
index modulation (GFDM-IM) scheme in order to improve the error performance of
the system. The proposed method first pre-processes the received signal by
using a zero-forcing (ZF) detector and then uses a neural network consisting of
a convolutional neural network (CNN) followed by a fully-connected neural
network (FCNN). The FCNN part uses only two fully-connected layers, which can
be adapted to yield a trade-off between complexity and bit error rate (BER)
performance. This two-stage approach prevents the getting stuck of neural
network in a saddle point and enables IM blocks processing independently. It
has been demonstrated that the proposed deep convolutional neural network-based
detection and demodulation scheme provides better BER performance compared to
ZF detector with a reasonable complexity increase. We conclude that
non-orthogonal waveforms combined with IM schemes with the help of deep
learning is a promising physical layer (PHY) scheme for future wireless
networks",None,-1
HYPRO: A Hybridly Normalized Probabilistic Model for Long-Horizon Prediction of Event Sequences,0.589843,"In this paper, we tackle the important yet under-investigated problem of
making long-horizon prediction of event sequences. Existing state-of-the-art
models do not perform well at this task due to their autoregressive structure.
We propose HYPRO, a hybridly normalized probabilistic model that naturally fits
this task: its first part is an autoregressive base model that learns to
propose predictions; its second part is an energy function that learns to
reweight the proposals such that more realistic predictions end up with higher
probabilities. We also propose efficient training and inference algorithms for
this model. Experiments on multiple real-world datasets demonstrate that our
proposed HYPRO model can significantly outperform previous models at making
long-horizon predictions of future events. We also conduct a range of ablation
studies to investigate the effectiveness of each component of our proposed
methods.",https://github.com/iLampard/hypro_tpp,-1
Multimodal data matters: language model pre-training over structured and unstructured electronic health records,0.0537785,"As two important textual modalities in electronic health records (EHR), both
structured data (clinical codes) and unstructured data (clinical narratives)
have recently been increasingly applied to the healthcare domain. Most existing
EHR-oriented studies, however, either focus on a particular modality or
integrate data from different modalities in a straightforward manner, which
usually treats structured and unstructured data as two independent sources of
information about patient admission and ignore the intrinsic interactions
between them. In fact, the two modalities are documented during the same
encounter where structured data inform the documentation of unstructured data
and vice versa. In this paper, we proposed a Medical Multimodal Pre-trained
Language Model, named MedM-PLM, to learn enhanced EHR representations over
structured and unstructured data and explore the interaction of two modalities.
In MedM-PLM, two Transformer-based neural network components are firstly
adopted to learn representative characteristics from each modality. A
cross-modal module is then introduced to model their interactions. We
pre-trained MedM-PLM on the MIMIC-III dataset and verified the effectiveness of
the model on three downstream clinical tasks, i.e., medication recommendation,
30-day readmission prediction and ICD coding. Extensive experiments demonstrate
the power of MedM-PLM compared with state-of-the-art methods. Further analyses
and visualizations show the robustness of our model, which could potentially
provide more comprehensive interpretations for clinical decision-making.",https://git.openi.org.cn/liusc/3-6-li-usicen-multi-modal-pretrain,-1
Mathematical model of printing-imaging channel for blind detection of fake copy detection patterns,0.14588,"Nowadays, copy detection patterns (CDP) appear as a very promising
anti-counterfeiting technology for physical object protection. However, the
advent of deep learning as a powerful attacking tool has shown that the general
authentication schemes are unable to compete and fail against such attacks. In
this paper, we propose a new mathematical model of printing-imaging channel for
the authentication of CDP together with a new detection scheme based on it. The
results show that even deep learning created copy fakes unknown at the training
stage can be reliably authenticated based on the proposed approach and using
only digital references of CDP during authentication.",None,-1
"Env-Aware Anomaly Detection: Ignore Style Changes, Stay True to Content!",0.205522,"We introduce a formalization and benchmark for the unsupervised anomaly
detection task in the distribution-shift scenario. Our work builds upon the
iWildCam dataset, and, to the best of our knowledge, we are the first to
propose such an approach for visual data. We empirically validate that
environment-aware methods perform better in such cases when compared with the
basic Empirical Risk Minimization (ERM). We next propose an extension for
generating positive samples for contrastive methods that considers the
environment labels when training, improving the ERM baseline score by 8.7%.",None,-1
Learning Uncertainty with Artificial Neural Networks for Improved Predictive Process Monitoring,0.0989542,"The inability of artificial neural networks to assess the uncertainty of
their predictions is an impediment to their widespread use. We distinguish two
types of learnable uncertainty: model uncertainty due to a lack of training
data and noise-induced observational uncertainty. Bayesian neural networks use
solid mathematical foundations to learn the model uncertainties of their
predictions. The observational uncertainty can be calculated by adding one
layer to these networks and augmenting their loss functions. Our contribution
is to apply these uncertainty concepts to predictive process monitoring tasks
to train uncertainty-based models to predict the remaining time and outcomes.
Our experiments show that uncertainty estimates allow more and less accurate
predictions to be differentiated and confidence intervals to be constructed in
both regression and classification tasks. These conclusions remain true even in
early stages of running processes. Moreover, the deployed techniques are fast
and produce more accurate predictions. The learned uncertainty could increase
users' confidence in their process prediction systems, promote better
cooperation between humans and these systems, and enable earlier
implementations with smaller datasets.",None,-1
Generative Pretraining for Black-Box Optimization,0.073136,"Many problems in science and engineering involve optimizing an expensive
black-box function over a high-dimensional space. For such black-box
optimization (BBO) problems, we typically assume a small budget for online
function evaluations, but also often have access to a fixed, offline dataset
for pretraining. Prior approaches seek to utilize the offline data to
approximate the function or its inverse but are not sufficiently accurate far
from the data distribution. We propose BONET, a generative framework for
pretraining a novel black-box optimizer using offline datasets. In BONET, we
train an autoregressive model on fixed-length trajectories derived from an
offline dataset. We design a sampling strategy to synthesize trajectories from
offline data using a simple heuristic of rolling out monotonic transitions from
low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using
a causally masked Transformer and evaluate it on Design-Bench, where we rank
the best on average, outperforming state-of-the-art baselines.",https://github.com/siddarthk97/bonet,-1
Watching the News: Towards VideoQA Models that can Read,0.950213,"Video Question Answering methods focus on commonsense reasoning and visual
cognition of objects or persons and their interactions over time. Current
VideoQA approaches ignore the textual information present in the video.
Instead, we argue that textual information is complementary to the action and
provides essential contextualisation cues to the reasoning process. To this
end, we propose a novel VideoQA task that requires reading and understanding
the text in the video. To explore this direction, we focus on news videos and
require QA systems to comprehend and answer questions about the topics
presented by combining visual and textual cues in the video. We introduce the
``NewsVideoQA'' dataset that comprises more than $8,600$ QA pairs on $3,000+$
news videos obtained from diverse news channels from around the world. We
demonstrate the limitations of current Scene Text VQA and VideoQA methods and
propose ways to incorporate scene text information into VideoQA methods.",https://github.com/facebookresearch/mmf,-1
An Effective Iterated Two-stage Heuristic Algorithm for the Multiple Traveling Salesmen Problem,0.0268297,"The multiple Traveling Salesmen Problem (mTSP) is a general extension of the
famous NP-hard Traveling Salesmen Problem (TSP), that there are m (m > 1)
salesmen to visit the cities. In this paper, we address the mTSP with both the
minsum objective and minmax objective, which aims at minimizing the total
length of the $m$ tours and the length of the longest tour among all the m
tours, respectively. We propose an iterated two-stage heuristic algorithm
called ITSHA for the mTSP. Each iteration of ITSHA consists of an
initialization stage and an improvement stage. The initialization stage aims to
generate high-quality and diverse initial solutions. The improvement stage
mainly applies the variable neighborhood search (VNS) approach based on our
proposed effective local search neighborhoods to optimize the initial solution.
Moreover, some local optima escaping approaches are employed to enhance the
search ability of the algorithm. Extensive experimental results on a wide range
of public benchmark instances show that ITSHA significantly outperforms the
state-of-the-art heuristic algorithms in solving the mTSP on both the
objectives.",None,-1
Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning,0.0689657,"Federated Learning (FL) is a machine learning technique that addresses the
privacy challenges in terms of access rights of local datasets by enabling the
training of a model across nodes holding their data samples locally. To achieve
decentralized federated learning, blockchain-based FL was proposed as a
distributed FL architecture. In decentralized FL, the chief is eliminated from
the learning process as workers collaborate between each other to train the
global model. Decentralized FL applications need to account for the additional
delay incurred by blockchain-based FL deployments. Particularly in this
setting, to detect targeted/untargeted poisoning attacks, we investigate the
end-to-end learning completion latency of a realistic decentralized FL process
protected against poisoning attacks. We propose a technique which consists in
decoupling the monitoring phase from the detection phase in defenses against
poisoning attacks in a decentralized federated learning deployment that aim at
monitoring the behavior of the workers. We demonstrate that our proposed
blockchain-based monitoring improved network scalability, robustness and time
efficiency. The parallelization of operations results in minimized latency over
the end-to-end communication, computation, and consensus delays incurred during
the FL and blockchain operations.",https://github.com/LiTrans/BSMD/tree/master/,-1
Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL,0.124733,"Multi-agent Reinforcement Learning (MARL) is a powerful tool for training
autonomous agents acting independently in a common environment. However, it can
lead to sub-optimal behavior when individual incentives and group incentives
diverge. Humans are remarkably capable at solving these social dilemmas. It is
an open problem in MARL to replicate such cooperative behaviors in selfish
agents. In this work, we draw upon the idea of formal contracting from
economics to overcome diverging incentives between agents in MARL. We propose
an augmentation to a Markov game where agents voluntarily agree to binding
transfers of reward, under pre-specified conditions. Our contributions are
theoretical and empirical. First, we show that this augmentation makes all
subgame-perfect equilibria of all Fully Observable Markov Games exhibit
socially optimal behavior, given a sufficiently rich space of contracts. Next,
we show that for general contract spaces, and even under partial observability,
richer contract spaces lead to higher welfare. Hence, contract space design
solves an exploration-exploitation tradeoff, sidestepping incentive issues. We
complement our theoretical analysis with experiments. Issues of exploration in
the contracting augmentation are mitigated using a training methodology
inspired by multi-objective reinforcement learning: Multi-Objective Contract
Augmentation Learning (MOCA). We test our methodology in static, single-move
games, as well as dynamic domains that simulate traffic, pollution management
and common pool resource management.",https://github.com/Algorithmic-Alignment-Lab/contracts,-1
PEANUT: Predicting and Navigating to Unseen Targets,0.217298,"Efficient ObjectGoal navigation (ObjectNav) in novel environments requires an
understanding of the spatial and semantic regularities in environment layouts.
In this work, we present a straightforward method for learning these
regularities by predicting the locations of unobserved objects from incomplete
semantic maps. Our method differs from previous prediction-based navigation
methods, such as frontier potential prediction or egocentric map completion, by
directly predicting unseen targets while leveraging the global context from all
previously explored areas. Our prediction model is lightweight and can be
trained in a supervised manner using a relatively small amount of passively
collected data. Once trained, the model can be incorporated into a modular
pipeline for ObjectNav without the need for any reinforcement learning. We
validate the effectiveness of our method on the HM3D and MP3D ObjectNav
datasets. We find that it achieves the state-of-the-art on both datasets,
despite not using any additional data for training.",https://github.com/open-mmlab/mmsegmentation,-1
Tele-Knowledge Pre-training for Fault Analysis,0.100149,"In this work, we share our experience on tele-knowledge pre-training for
fault analysis, a crucial task in telecommunication applications that requires
a wide range of knowledge normally found in both machine log data and product
documents. To organize this knowledge from experts uniformly, we propose to
create a Tele-KG (tele-knowledge graph). Using this valuable data, we further
propose a tele-domain language pre-training model TeleBERT and its
knowledge-enhanced version, a tele-knowledge re-training model KTeleBERT. which
includes effective prompt hints, adaptive numerical data encoding, and two
knowledge injection paradigms. Concretely, our proposal includes two stages:
first, pre-training TeleBERT on 20 million tele-related corpora, and then
re-training it on 1 million causal and machine-related corpora to obtain
KTeleBERT. Our evaluation on multiple tasks related to fault analysis in
tele-applications, including root-cause analysis, event association prediction,
and fault chain tracing, shows that pre-training a language model with
tele-domain data is beneficial for downstream tasks. Moreover, the KTeleBERT
re-training further improves the performance of task models, highlighting the
effectiveness of incorporating diverse tele-knowledge into the model.",None,-1
Unobserved Local Structures Make Compositional Generalization Hard,0.304974,"While recent work has convincingly showed that sequence-to-sequence models
struggle to generalize to new compositions (termed compositional
generalization), little is known on what makes compositional generalization
hard on a particular test instance. In this work, we investigate what are the
factors that make generalization to certain test instances challenging. We
first substantiate that indeed some examples are more difficult than others by
showing that different models consistently fail or succeed on the same test
instances. Then, we propose a criterion for the difficulty of an example: a
test instance is hard if it contains a local structure that was not observed at
training time. We formulate a simple decision rule based on this criterion and
empirically show it predicts instance-level generalization well across 5
different semantic parsing datasets, substantially better than alternative
decision rules. Last, we show local structures can be leveraged for creating
difficult adversarial compositional splits and also to improve compositional
generalization under limited training budgets by strategically selecting
examples for the training set.",https://github.com/benbogin/unobserved-local-structures,-1
NeReF: Neural Refractive Field for Fluid Surface Reconstruction and Implicit Representation,0.0475887,"Existing neural reconstruction schemes such as Neural Radiance Field (NeRF)
are largely focused on modeling opaque objects. We present a novel neural
refractive field(NeReF) to recover wavefront of transparent fluids by
simultaneously estimating the surface position and normal of the fluid front.
Unlike prior arts that treat the reconstruction target as a single layer of the
surface, NeReF is specifically formulated to recover a volumetric normal field
with its corresponding density field. A query ray will be refracted by NeReF
according to its accumulated refractive point and normal, and we employ the
correspondences and uniqueness of refracted ray for NeReF optimization. We show
NeReF, as a global optimization scheme, can more robustly tackle refraction
distortions detrimental to traditional methods for correspondence matching.
Furthermore, the continuous NeReF representation of wavefront enables view
synthesis as well as normal integration. We validate our approach on both
synthetic and real data and show it is particularly suitable for sparse
multi-view acquisition. We hence build a small light field array and experiment
on various surface shapes to demonstrate high fidelity NeReF reconstruction.",None,-1
Audio-visual speech enhancement with a deep Kalman filter generative model,0.00852009,"Deep latent variable generative models based on variational autoencoder (VAE)
have shown promising performance for audiovisual speech enhancement (AVSE). The
underlying idea is to learn a VAEbased audiovisual prior distribution for clean
speech data, and then combine it with a statistical noise model to recover a
speech signal from a noisy audio recording and video (lip images) of the target
speaker. Existing generative models developed for AVSE do not take into account
the sequential nature of speech data, which prevents them from fully
incorporating the power of visual data. In this paper, we present an
audiovisual deep Kalman filter (AV-DKF) generative model which assumes a
first-order Markov chain model for the latent variables and effectively fuses
audiovisual data. Moreover, we develop an efficient inference methodology to
estimate speech signals at test time. We conduct a set of experiments to
compare different variants of generative models for speech enhancement. The
results demonstrate the superiority of the AV-DKF model compared with both its
audio-only version and the non-sequential audio-only and audiovisual VAE-based
models.",https://github.com/XiaoyuBIE1994/DVAE_SE,-1
SARAS-Net: Scale and Relation Aware Siamese Network for Change Detection,0.0279308,"Change detection (CD) aims to find the difference between two images at
different times and outputs a change map to represent whether the region has
changed or not. To achieve a better result in generating the change map, many
State-of-The-Art (SoTA) methods design a deep learning model that has a
powerful discriminative ability. However, these methods still get lower
performance because they ignore spatial information and scaling changes between
objects, giving rise to blurry or wrong boundaries. In addition to these, they
also neglect the interactive information of two different images. To alleviate
these problems, we propose our network, the Scale and Relation-Aware Siamese
Network (SARAS-Net) to deal with this issue. In this paper, three modules are
proposed that include relation-aware, scale-aware, and cross-transformer to
tackle the problem of scene change detection more effectively. To verify our
model, we tested three public datasets, including LEVIR-CD, WHU-CD, and DSFIN,
and obtained SoTA accuracy. Our code is available at
https://github.com/f64051041/SARAS-Net.",https://github.com/f64051041/SARAS-Net,-1
RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures,0.0901979,"We introduce RIM-Net, a neural network which learns recursive implicit fields
for unsupervised inference of hierarchical shape structures. Our network
recursively decomposes an input 3D shape into two parts, resulting in a binary
tree hierarchy. Each level of the tree corresponds to an assembly of shape
parts, represented as implicit functions, to reconstruct the input shape. At
each node of the tree, simultaneous feature decoding and shape decomposition
are carried out by their respective feature and part decoders, with weight
sharing across the same hierarchy level. As an implicit field decoder, the part
decoder is designed to decompose a sub-shape, via a two-way branched
reconstruction, where each branch predicts a set of parameters defining a
Gaussian to serve as a local point distribution for shape reconstruction. With
reconstruction losses accounted for at each hierarchy level and a decomposition
loss at each node, our network training does not require any ground-truth
segmentations, let alone hierarchies. Through extensive experiments and
comparisons to state-of-the-art alternatives, we demonstrate the quality,
consistency, and interpretability of hierarchical structural inference by
RIM-Net.",None,-1
Classification of Hyperspectral Images Using SVM with Shape-adaptive Reconstruction and Smoothed Total Variation,0.0523907,"In this work, a novel algorithm called SVM with Shape-adaptive Reconstruction
and Smoothed Total Variation (SaR-SVM-STV) is introduced to classify
hyperspectral images, which makes full use of spatial and spectral information.
The Shape-adaptive Reconstruction (SaR) is introduced to preprocess each pixel
based on the Pearson Correlation between pixels in its shape-adaptive (SA)
region. Support Vector Machines (SVMs) are trained to estimate the pixel-wise
probability maps of each class. Then the Smoothed Total Variation (STV) model
is applied to denoise and generate the final classification map. Experiments
show that SaR-SVM-STV outperforms the SVM-STV method with a few training
labels, demonstrating the significance of reconstructing hyperspectral images
before classification.",https://github.com/ckn3/SA-Recon,-1
On Calibrating Semantic Segmentation Models: Analyses and An Algorithm,0.0,"We study the problem of semantic segmentation calibration. Lots of solutions
have been proposed to approach model miscalibration of confidence in image
classification. However, to date, confidence calibration research on semantic
segmentation is still limited. We provide a systematic study on the calibration
of semantic segmentation models and propose a simple yet effective approach.
First, we find that model capacity, crop size, multi-scale testing, and
prediction correctness have impact on calibration. Among them, prediction
correctness, especially misprediction, is more important to miscalibration due
to over-confidence. Next, we propose a simple, unifying, and effective
approach, namely selective scaling, by separating correct/incorrect prediction
for scaling and more focusing on misprediction logit smoothing. Then, we study
popular existing calibration methods and compare them with selective scaling on
semantic segmentation calibration. We conduct extensive experiments with a
variety of benchmarks on both in-domain and domain-shift calibration and show
that selective scaling consistently outperforms other methods.",https://github.com/dwang181/selectivecal,-1
Discrete-Constrained Regression for Local Counting Models,0.0779338,"Local counts, or the number of objects in a local area, is a continuous value
by nature. Yet recent state-of-the-art methods show that formulating counting
as a classification task performs better than regression. Through a series of
experiments on carefully controlled synthetic data, we show that this
counter-intuitive result is caused by imprecise ground truth local counts.
Factors such as biased dot annotations and incorrectly matched Gaussian kernels
used to generate ground truth counts introduce deviations from the true local
counts. Standard continuous regression is highly sensitive to these errors,
explaining the performance gap between classification and regression. To
mitigate the sensitivity, we loosen the regression formulation from a
continuous scale to a discrete ordering and propose a novel
discrete-constrained (DC) regression. Applied to crowd counting, DC-regression
is more accurate than both classification and standard regression on three
public benchmarks. A similar advantage also holds for the age estimation task,
verifying the overall effectiveness of DC-regression.",None,-1
Impact of Tokenization on Language Models: An Analysis for Turkish,0.113866,"Tokenization is an important text preprocessing step to prepare input tokens
for deep language models. WordPiece and BPE are de facto methods employed by
important models, such as BERT and GPT. However, the impact of tokenization can
be different for morphologically rich languages, such as Turkic languages,
where many words can be generated by adding prefixes and suffixes. We compare
five tokenizers at different granularity levels, i.e. their outputs vary from
smallest pieces of characters to the surface form of words, including a
Morphological-level tokenizer. We train these tokenizers and pretrain
medium-sized language models using RoBERTa pretraining procedure on the Turkish
split of the OSCAR corpus. We then fine-tune our models on six downstream
tasks. Our experiments, supported by statistical tests, reveal that
Morphological-level tokenizer has challenging performance with de facto
tokenizers. Furthermore, we find that increasing the vocabulary size improves
the performance of Morphological and Word-level tokenizers more than that of de
facto tokenizers. The ratio of the number of vocabulary parameters to the total
number of model parameters can be empirically chosen as 20% for de facto
tokenizers and 40% for other tokenizers to obtain a reasonable trade-off
between model size and performance.",None,-1
Event-Based Dense Reconstruction Pipeline,0.0661083,"Event cameras are a new type of sensors that are different from traditional
cameras. Each pixel is triggered asynchronously by event. The trigger event is
the change of the brightness irradiated on the pixel. If the increment or
decrement of brightness is higher than a certain threshold, an event is output.
Compared with traditional cameras, event cameras have the advantages of high
dynamic range and no motion blur. Since events are caused by the apparent
motion of intensity edges, the majority of 3D reconstructed maps consist only
of scene edges, i.e., semi-dense maps, which is not enough for some
applications. In this paper, we propose a pipeline to realize event-based dense
reconstruction. First, deep learning is used to reconstruct intensity images
from events. And then, structure from motion (SfM) is used to estimate camera
intrinsic, extrinsic and sparse point cloud. Finally, multi-view stereo (MVS)
is used to complete dense reconstruction.",https://github.com/colmap/colmap,-1
Towards Equalised Odds as Fairness Metric in Academic Performance Prediction,0.02998,"The literature for fairness-aware machine learning knows a plethora of
different fairness notions. It is however wellknown, that it is impossible to
satisfy all of them, as certain notions contradict each other. In this paper,
we take a closer look at academic performance prediction (APP) systems and try
to distil which fairness notions suit this task most. For this, we scan recent
literature proposing guidelines as to which fairness notion to use and apply
these guidelines onto APP. Our findings suggest equalised odds as most suitable
notion for APP, based on APP's WYSIWYG worldview as well as potential long-term
improvements for the population.",None,-1
Balancing Stability and Plasticity through Advanced Null Space in Continual Learning,0.17419,"Continual learning is a learning paradigm that learns tasks sequentially with
resources constraints, in which the key challenge is stability-plasticity
dilemma, i.e., it is uneasy to simultaneously have the stability to prevent
catastrophic forgetting of old tasks and the plasticity to learn new tasks
well. In this paper, we propose a new continual learning approach, Advanced
Null Space (AdNS), to balance the stability and plasticity without storing any
old data of previous tasks. Specifically, to obtain better stability, AdNS
makes use of low-rank approximation to obtain a novel null space and projects
the gradient onto the null space to prevent the interference on the past tasks.
To control the generation of the null space, we introduce a non-uniform
constraint strength to further reduce forgetting. Furthermore, we present a
simple but effective method, intra-task distillation, to improve the
performance of the current task. Finally, we theoretically find that null space
plays a key role in plasticity and stability, respectively. Experimental
results show that the proposed method can achieve better performance compared
to state-of-the-art continual learning approaches.",None,-1
Self-consistent Reasoning For Solving Math Word Problems,0.038944,"Math word problems (MWPs) is a task that automatically derives solution
expression from a giving math problems in text. The previous studies suffer
from spurious correlations between input text and output expression. To
mitigate this issue, we propose a self-consistent reasoning framework called
SCR, which attempts to adopt a pruning strategy to correct the output
distribution shift so as to implicitly fix those spurious correlative samples.
Specifically, we firstly obtain a sub-network by pruning a roberta2tree model,
for the sake to use the gap on output distribution between the original
roberta2tree model and the pruned sub-network to expose spurious correlative
samples. Then, we calibrate the output distribution shift by applying symmetric
Kullback-Leibler divergence to alleviate spurious correlations. In addition,
SCR generates equivalent expressions, thereby, capturing the original text's
logic rather than relying on hints from original text. Extensive experiments on
two large-scale benchmarks demonstrate that our model substantially outperforms
the strong baseline methods.",None,-1
GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks,0.221326,"Time series synthesis is an important research topic in the field of deep
learning, which can be used for data augmentation. Time series data types can
be broadly classified into regular or irregular. However, there are no existing
generative models that show good performance for both types without any model
changes. Therefore, we present a general purpose model capable of synthesizing
regular and irregular time series data. To our knowledge, we are the first
designing a general purpose time series synthesis model, which is one of the
most challenging settings for time series synthesis. To this end, we design a
generative adversarial network-based method, where many related techniques are
carefully integrated into a single framework, ranging from neural
ordinary/controlled differential equations to continuous time-flow processes.
Our method outperforms all existing methods.",None,-1
CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models,0.190023,"We motivate and introduce CHARD: Clinical Health-Aware Reasoning across
Dimensions, to investigate the capability of text generation models to act as
implicit clinical knowledge bases and generate free-flow textual explanations
about various health-related conditions across several dimensions. We collect
and present an associated dataset, CHARDat, consisting of explanations about 52
health conditions across three clinical dimensions. We conduct extensive
experiments using BART and T5 along with data augmentation, and perform
automatic, human, and qualitative analyses. We show that while our models can
perform decently, CHARD is very challenging with strong potential for further
exploration.",https://github.com/styfeng/CHARD,-1
Domain Adaptation meets Individual Fairness. And they get along,0.0872508,"Many instances of algorithmic bias are caused by distributional shifts. For
example, machine learning (ML) models often perform worse on demographic groups
that are underrepresented in the training data. In this paper, we leverage this
connection between algorithmic fairness and distribution shifts to show that
algorithmic fairness interventions can help ML models overcome distribution
shifts, and that domain adaptation methods (for overcoming distribution shifts)
can mitigate algorithmic biases. In particular, we show that (i) enforcing
suitable notions of individual fairness (IF) can improve the
out-of-distribution accuracy of ML models under the covariate shift assumption
and that (ii) it is possible to adapt representation alignment methods for
domain adaptation to enforce individual fairness. The former is unexpected
because IF interventions were not developed with distribution shifts in mind.
The latter is also unexpected because representation alignment is not a common
approach in the individual fairness literature.",None,-1
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,0.0687773,"Sequence generation models are increasingly being used to translate natural
language into programs, i.e. to perform executable semantic parsing. The fact
that semantic parsing aims to predict programs that can lead to executed
actions in the real world motivates developing safe systems. This in turn makes
measuring calibration -- a central component to safety -- particularly
important. We investigate the calibration of popular generation models across
four popular semantic parsing datasets, finding that it varies across models
and datasets. We then analyze factors associated with calibration error and
release new confidence-based challenge splits of two parsing datasets. To
facilitate the inclusion of calibration in semantic parsing evaluations, we
release a library for computing calibration metrics.",https://github.com/esteng/calibration_metric,-1
Hyper-parameter tuning of physics-informed neural networks: Application to Helmholtz problems,0.0629597,"We consider physics-informed neural networks (PINNs) [Raissi et al.,
J.~Comput. Phys. 278 (2019) 686-707] for forward physical problems. In order to
find optimal PINNs configuration, we introduce a hyper-parameter optimization
(HPO) procedure via Gaussian processes-based Bayesian optimization. We apply
the HPO to Helmholtz equation for bounded domains and conduct a thorough study,
focusing on: (i) performance, (ii) the collocation points density $r$ and (iii)
the frequency $\kappa$, confirming the applicability and necessity of the
method. Numerical experiments are performed in two and three dimensions,
including comparison to finite element methods.",https://github.com/lululxvi/deepxde/,-1
KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,0.864665,"Relative positional embeddings (RPE) have received considerable attention
since RPEs effectively model the relative distance among tokens and enable
length extrapolation. We propose KERPLE, a framework that generalizes relative
position embedding for extrapolation by kernelizing positional differences. We
achieve this goal using conditionally positive definite (CPD) kernels, a class
of functions known for generalizing distance metrics. To maintain the inner
product interpretation of self-attention, we show that a CPD kernel can be
transformed into a PD kernel by adding a constant offset. This offset is
implicitly absorbed in the Softmax normalization during self-attention. The
diversity of CPD kernels allows us to derive various RPEs that enable length
extrapolation in a principled way. Experiments demonstrate that the logarithmic
variant achieves excellent extrapolation performance on three large language
modeling datasets. Our implementation and pretrained checkpoints are released
at https://github.com/chijames/KERPLE.git.",https://github.com/chijames/KERPLE.git,-1
Self-Distribution Distillation: Efficient Uncertainty Estimation,0.0244671,"Deep learning is increasingly being applied in safety-critical domains. For
these scenarios it is important to know the level of uncertainty in a model's
prediction to ensure appropriate decisions are made by the system. Deep
ensembles are the de-facto standard approach to obtaining various measures of
uncertainty. However, ensembles often significantly increase the resources
required in the training and/or deployment phases. Approaches have been
developed that typically address the costs in one of these phases. In this work
we propose a novel training approach, self-distribution distillation (S2D),
which is able to efficiently train a single model that can estimate
uncertainties. Furthermore it is possible to build ensembles of these models
and apply hierarchical ensemble distillation approaches. Experiments on
CIFAR-100 showed that S2D models outperformed standard models and Monte-Carlo
dropout. Additional out-of-distribution detection experiments on LSUN, Tiny
ImageNet, SVHN showed that even a standard deep ensemble can be outperformed
using S2D based ensembles and novel distilled models.",None,-1
Can Requirements Engineering Support Explainable Artificial Intelligence? Towards a User-Centric Approach for Explainability Requirements,0.0493507,"With the recent proliferation of artificial intelligence systems, there has
been a surge in the demand for explainability of these systems. Explanations
help to reduce system opacity, support transparency, and increase stakeholder
trust. In this position paper, we discuss synergies between requirements
engineering (RE) and Explainable AI (XAI). We highlight challenges in the field
of XAI, and propose a framework and research directions on how RE practices can
help to mitigate these challenges.",None,-1
Retrieval Augmentation for Commonsense Reasoning: A Unified Approach,0.349357,"A common thread of retrieval-augmented methods in the existing literature
focuses on retrieving encyclopedic knowledge, such as Wikipedia, which
facilitates well-defined entity and relation spaces that can be modeled.
However, applying such methods to commonsense reasoning tasks faces two unique
challenges, i.e., the lack of a general large-scale corpus for retrieval and a
corresponding effective commonsense retriever. In this paper, we systematically
investigate how to leverage commonsense knowledge retrieval to improve
commonsense reasoning tasks. We proposed a unified framework of
retrieval-augmented commonsense reasoning (called RACo), including a newly
constructed commonsense corpus with over 20 million documents and novel
strategies for training a commonsense retriever. We conducted experiments on
four different commonsense reasoning tasks. Extensive evaluation results showed
that our proposed RACo can significantly outperform other knowledge-enhanced
method counterparts, achieving new SoTA performance on the CommonGen and CREAK
leaderboards.",https://github.com/wyu97/RACo,-1
Mapless Navigation of a Hybrid Aerial Underwater Vehicle with Deep Reinforcement Learning Through Environmental Generalization,0.357525,"Previous works showed that Deep-RL can be applied to perform mapless
navigation, including the medium transition of Hybrid Unmanned Aerial
Underwater Vehicles (HUAUVs). This paper presents new approaches based on the
state-of-the-art actor-critic algorithms to address the navigation and medium
transition problems for a HUAUV. We show that a double critic Deep-RL with
Recurrent Neural Networks improves the navigation performance of HUAUVs using
solely range data and relative localization. Our Deep-RL approaches achieved
better navigation and transitioning capabilities with a solid generalization of
learning through distinct simulated scenarios, outperforming previous
approaches.",None,-1
MISeval: a Metric Library for Medical Image Segmentation Evaluation,0.099136,"Correct performance assessment is crucial for evaluating modern artificial
intelligence algorithms in medicine like deep-learning based medical image
segmentation models. However, there is no universal metric library in Python
for standardized and reproducible evaluation. Thus, we propose our open-source
publicly available Python package MISeval: a metric library for Medical Image
Segmentation Evaluation. The implemented metrics can be intuitively used and
easily integrated into any performance assessment pipeline. The package
utilizes modern CI/CD strategies to ensure functionality and stability. MISeval
is available from PyPI (miseval) and GitHub:
https://github.com/frankkramer-lab/miseval.",https://github.com/frankkramer-lab/miseval,-1
Attention-based Random Forest and Contamination Model,0.0253792,"A new approach called ABRF (the attention-based random forest) and its
modifications for applying the attention mechanism to the random forest (RF)
for regression and classification are proposed. The main idea behind the
proposed ABRF models is to assign attention weights with trainable parameters
to decision trees in a specific way. The weights depend on the distance between
an instance, which falls into a corresponding leaf of a tree, and instances,
which fall in the same leaf. This idea stems from representation of the
Nadaraya-Watson kernel regression in the form of a RF. Three modifications of
the general approach are proposed. The first one is based on applying the
Huber's contamination model and on computing the attention weights by solving
quadratic or linear optimization problems. The second and the third
modifications use the gradient-based algorithms for computing trainable
parameters. Numerical experiments with various regression and classification
datasets illustrate the proposed method.",None,-1
Rethinking the Role of Pre-Trained Networks in Source-Free Domain Adaptation,0.0491142,"Source-free domain adaptation (SFDA) aims to adapt a source model trained on
a fully-labeled source domain to an unlabeled target domain. Large-data
pre-trained networks are used to initialize source models during source
training, and subsequently discarded. However, source training can cause the
model to overfit to source data distribution and lose applicable target domain
knowledge. We propose to integrate the pre-trained network into the target
adaptation process as it has diversified features important for generalization
and provides an alternate view of features and classification decisions
different from the source model. We propose to distil useful target domain
information through a co-learning strategy to improve target pseudolabel
quality for finetuning the source model. Evaluation on 4 benchmark datasets
show that our proposed strategy improves adaptation performance and can be
successfully integrated with existing SFDA methods. Leveraging modern
pre-trained networks that have stronger representation learning ability in the
co-learning strategy further boosts performance.",None,-1
Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach,0.149337,"Large Language Models have demonstrated remarkable few-shot performance, but
the performance can be sensitive to the selection of few-shot instances. We
propose PATRON, a new method that uses prompt-based uncertainty estimation for
data selection for pre-trained language model fine-tuning under cold-start
scenarios, i.e., no initial labeled data are available. In PATRON, we design
(1) a prompt-based uncertainty propagation approach to estimate the importance
of data points and (2) a partition-then-rewrite (PTR) strategy to promote
sample diversity when querying for annotations. Experiments on six text
classification datasets show that PATRON outperforms the strongest cold-start
data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON
achieves 91.0% and 92.1% of the fully supervised performance based on vanilla
fine-tuning and prompt-based learning respectively. Our implementation of
PATRON is available at \url{https://github.com/yueyu1030/Patron}.",https://github.com/yueyu1030/Patron,-1
Interstellar Object Accessibility and Mission Design,0.0199953,"Interstellar objects (ISOs) are fascinating and under-explored celestial
objects, providing physical laboratories to understand the formation of our
solar system and probe the composition and properties of material formed in
exoplanetary systems. This paper will discuss the accessibility of and mission
design to ISOs with varying characteristics, including a discussion of state
covariance estimation over the course of a cruise, handoffs from traditional
navigation approaches to novel autonomous navigation for fast flyby regimes,
and overall recommendations about preparing for the future in situ exploration
of these targets. The lessons learned also apply to the fast flyby of other
small bodies including long-period comets and potentially hazardous asteroids,
which also require a tactical response with similar characteristics",None,-1
Error Compensation Framework for Flow-Guided Video Inpainting,0.742475,"The key to video inpainting is to use correlation information from as many
reference frames as possible. Existing flow-based propagation methods split the
video synthesis process into multiple steps: flow completion -> pixel
propagation -> synthesis. However, there is a significant drawback that the
errors in each step continue to accumulate and amplify in the next step. To
this end, we propose an Error Compensation Framework for Flow-guided Video
Inpainting (ECFVI), which takes advantage of the flow-based method and offsets
its weaknesses. We address the weakness with the newly designed flow completion
module and the error compensation network that exploits the error guidance map.
Our approach greatly improves the temporal consistency and the visual quality
of the completed videos. Experimental results show the superior performance of
our proposed method with the speed up of x6, compared to the state-of-the-art
methods. In addition, we present a new benchmark dataset for evaluation by
supplementing the weaknesses of existing test datasets.",None,-1
An Outlier Exposure Approach to Improve Visual Anomaly Detection Performance for Mobile Robots,0.0371808,"We consider the problem of building visual anomaly detection systems for
mobile robots. Standard anomaly detection models are trained using large
datasets composed only of non-anomalous data. However, in robotics
applications, it is often the case that (potentially very few) examples of
anomalies are available. We tackle the problem of exploiting these data to
improve the performance of a Real-NVP anomaly detection model, by minimizing,
jointly with the Real-NVP loss, an auxiliary outlier exposure margin loss. We
perform quantitative experiments on a novel dataset (which we publish as
supplementary material) designed for anomaly detection in an indoor patrolling
scenario. On a disjoint test set, our approach outperforms alternatives and
shows that exposing even a small number of anomalous frames yields significant
performance improvements.",https://github.com/idsia-robotics/hazard-detection,-1
Ray Priors through Reprojection: Improving Neural Radiance Fields for Novel View Extrapolation,0.163209,"Neural Radiance Fields (NeRF) have emerged as a potent paradigm for
representing scenes and synthesizing photo-realistic images. A main limitation
of conventional NeRFs is that they often fail to produce high-quality
renderings under novel viewpoints that are significantly different from the
training viewpoints. In this paper, instead of exploiting few-shot image
synthesis, we study the novel view extrapolation setting that (1) the training
images can well describe an object, and (2) there is a notable discrepancy
between the training and test viewpoints' distributions. We present RapNeRF
(RAy Priors) as a solution. Our insight is that the inherent appearances of a
3D surface's arbitrary visible projections should be consistent. We thus
propose a random ray casting policy that allows training unseen views using
seen views. Furthermore, we show that a ray atlas pre-computed from the
observed rays' viewing directions could further enhance the rendering quality
for extrapolated views. A main limitation is that RapNeRF would remove the
strong view-dependent effects because it leverages the multi-view consistency
property.",None,-1
Disentangled Federated Learning for Tackling Attributes Skew via Invariant Aggregation and Diversity Transferring,0.10243,"Attributes skew hinders the current federated learning (FL) frameworks from
consistent optimization directions among the clients, which inevitably leads to
performance reduction and unstable convergence. The core problems lie in that:
1) Domain-specific attributes, which are non-causal and only locally valid, are
indeliberately mixed into global aggregation. 2) The one-stage optimizations of
entangled attributes cannot simultaneously satisfy two conflicting objectives,
i.e., generalization and personalization. To cope with these, we proposed
disentangled federated learning (DFL) to disentangle the domain-specific and
cross-invariant attributes into two complementary branches, which are trained
by the proposed alternating local-global optimization independently.
Importantly, convergence analysis proves that the FL system can be stably
converged even if incomplete client models participate in the global
aggregation, which greatly expands the application scope of FL. Extensive
experiments verify that DFL facilitates FL with higher performance, better
interpretability, and faster convergence rate, compared with SOTA FL methods on
both manually synthesized and realistic attributes skew datasets.",None,-1
Enhancing Document-level Relation Extraction by Entity Knowledge Injection,0.131847,"Document-level relation extraction (RE) aims to identify the relations
between entities throughout an entire document. It needs complex reasoning
skills to synthesize various knowledge such as coreferences and commonsense.
Large-scale knowledge graphs (KGs) contain a wealth of real-world facts, and
can provide valuable knowledge to document-level RE. In this paper, we propose
an entity knowledge injection framework to enhance current document-level RE
models. Specifically, we introduce coreference distillation to inject
coreference knowledge, endowing an RE model with the more general capability of
coreference reasoning. We also employ representation reconciliation to inject
factual knowledge and aggregate KG representations and document representations
into a unified space. The experiments on two benchmark datasets validate the
generalization of our entity knowledge injection framework and the consistent
improvement to several document-level RE models.",https://github.com/nju-websoft/KIRE,-1
AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach,0.515258,"Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).
It is a well-studied topic in several resource-rich languages. However, the
development of computational linguistic resources is still in its infancy
despite the existence of numerous languages that are historically and literary
rich. Assamese, an Indian scheduled language, spoken by more than 25 million
people, falls under this category. In this paper, we present a Deep Learning
(DL)-based POS tagger for Assamese. The development process is divided into two
stages. In the first phase, several pre-trained word embeddings are employed to
train several tagging models. This allows us to evaluate the performance of the
word embeddings in the POS tagging task. The top-performing model from the
first phase is employed to annotate another set of new sentences. In the second
phase, the model is trained further using the fresh dataset. Finally, we attain
a tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for
further study on DL-based Assamese POS tagging.",None,-1
Neural Architecture Search for Dense Prediction Tasks in Computer Vision,0.0369878,"The success of deep learning in recent years has lead to a rising demand for
neural network architecture engineering. As a consequence, neural architecture
search (NAS), which aims at automatically designing neural network
architectures in a data-driven manner rather than manually, has evolved as a
popular field of research. With the advent of weight sharing strategies across
architectures, NAS has become applicable to a much wider range of problems. In
particular, there are now many publications for dense prediction tasks in
computer vision that require pixel-level predictions, such as semantic
segmentation or object detection. These tasks come with novel challenges, such
as higher memory footprints due to high-resolution data, learning multi-scale
representations, longer training times, and more complex and larger neural
architectures. In this manuscript, we provide an overview of NAS for dense
prediction tasks by elaborating on these novel challenges and surveying ways to
address them to ease future research and application of existing methods to
novel problems.",None,-1
Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,0.381437,"While large-scale neural language models, such as GPT2 and BART, have
achieved impressive results on various text generation tasks, they tend to get
stuck in undesirable sentence-level loops with maximization-based decoding
algorithms (\textit{e.g.}, greedy search). This phenomenon is counter-intuitive
since there are few consecutive sentence-level repetitions in human corpora
(e.g., 0.02\% in Wikitext-103). To investigate the underlying reasons for
generating consecutive sentence-level repetitions, we study the relationship
between the probabilities of the repetitive tokens and their previous
repetitions in the context. Through our quantitative experiments, we find that
1) Language models have a preference to repeat the previous sentence; 2) The
sentence-level repetitions have a \textit{self-reinforcement effect}: the more
times a sentence is repeated in the context, the higher the probability of
continuing to generate that sentence; 3) The sentences with higher initial
probabilities usually have a stronger self-reinforcement effect. Motivated by
our findings, we propose a simple and effective training method \textbf{DITTO}
(Pseu\underline{D}o-Repet\underline{IT}ion
Penaliza\underline{T}i\underline{O}n), where the model learns to penalize
probabilities of sentence-level repetitions from pseudo repetitive data.
Although our method is motivated by mitigating repetitions, experiments show
that DITTO not only mitigates the repetition issue without sacrificing
perplexity, but also achieves better generation quality. Extensive experiments
on open-ended text generation (Wikitext-103) and text summarization
(CNN/DailyMail) demonstrate the generality and effectiveness of our method.",https://github.com/Jxu-Thu/DITTO,-1
Lagrangian Manifold Monte Carlo on Monge Patches,0.349657,"The efficiency of Markov Chain Monte Carlo (MCMC) depends on how the
underlying geometry of the problem is taken into account. For distributions
with strongly varying curvature, Riemannian metrics help in efficient
exploration of the target distribution. Unfortunately, they have significant
computational overhead due to e.g. repeated inversion of the metric tensor, and
current geometric MCMC methods using the Fisher information matrix to induce
the manifold are in practice slow. We propose a new alternative Riemannian
metric for MCMC, by embedding the target distribution into a higher-dimensional
Euclidean space as a Monge patch and using the induced metric determined by
direct geometric reasoning. Our metric only requires first-order gradient
information and has fast inverse and determinants, and allows reducing the
computational complexity of individual iterations from cubic to quadratic in
the problem dimensionality. We demonstrate how Lagrangian Monte Carlo in this
metric efficiently explores the target distributions.",https://github.com/mahaa2/EmbeddedLMC,-1
Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language,0.165064,"To solve difficult tasks, humans ask questions to acquire knowledge from
external sources. In contrast, classical reinforcement learning agents lack
such an ability and often resort to exploratory behavior. This is exacerbated
as few present-day environments support querying for knowledge. In order to
study how agents can be taught to query external knowledge via language, we
first introduce two new environments: the grid-world-based Q-BabyAI and the
text-based Q-TextWorld. In addition to physical interactions, an agent can
query an external knowledge source specialized for these environments to gather
information. Second, we propose the ""Asking for Knowledge"" (AFK) agent, which
learns to generate language commands to query for meaningful knowledge that
helps solve the tasks. AFK leverages a non-parametric memory, a pointer
mechanism and an episodic exploration bonus to tackle (1) irrelevant
information, (2) a large query language space, (3) delayed reward for making
meaningful queries. Extensive experiments demonstrate that the AFK agent
outperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld
environments.",https://ioujenliu.github.io/AFK,-1
Evaluating Long-Term Memory in 3D Mazes,0.346024,"Intelligent agents need to remember salient information to reason in
partially-observed environments. For example, agents with a first-person view
should remember the positions of relevant objects even if they go out of view.
Similarly, to effectively navigate through rooms agents need to remember the
floor plan of how rooms are connected. However, most benchmark tasks in
reinforcement learning do not test long-term memory in agents, slowing down
progress in this important research direction. In this paper, we introduce the
Memory Maze, a 3D domain of randomized mazes specifically designed for
evaluating long-term memory in agents. Unlike existing benchmarks, Memory Maze
measures long-term memory separate from confounding agent abilities and
requires the agent to localize itself by integrating information over time.
With Memory Maze, we propose an online reinforcement learning benchmark, a
diverse offline dataset, and an offline probing evaluation. Recording a human
player establishes a strong baseline and verifies the need to build up and
retain memories, which is reflected in their gradually increasing rewards
within each episode. We find that current algorithms benefit from training with
truncated backpropagation through time and succeed on small mazes, but fall
short of human performance on the large mazes, leaving room for future
algorithmic designs to be evaluated on the Memory Maze.",https://github.com/jurgisp/memory-maze,-1
Zero-Shot Text Classification with Self-Training,0.0704144,"Recent advances in large pretrained language models have increased attention
to zero-shot text classification. In particular, models finetuned on natural
language inference datasets have been widely adopted as zero-shot classifiers
due to their promising results and off-the-shelf availability. However, the
fact that such models are unfamiliar with the target task can lead to
instability and performance issues. We propose a plug-and-play method to bridge
this gap using a simple self-training approach, requiring only the class names
along with an unlabeled dataset, and without the need for domain expertise or
trial and error. We show that fine-tuning the zero-shot classifier on its most
confident predictions leads to significant performance gains across a wide
range of text classification tasks, presumably since self-training adapts the
zero-shot model to the task at hand.",https://github.com/IBM/zero-shot-classification-boost-with-self-training,-1
SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control,0.439925,"Despite the growing success of diffusion models in continuous-valued domains
(e.g., images), similar efforts for discrete domains such as text have yet to
match the performance of autoregressive language models. In this work, we
present SSD-LM -- a diffusion-based language model with two key design choices.
First, SSD-LM is semi-autoregressive, iteratively generating blocks of text,
allowing for flexible output length at decoding time while enabling local
bidirectional context updates. Second, it is simplex-based, performing
diffusion on the natural vocabulary space rather than a learned latent space,
allowing us to incorporate classifier guidance and modular control using
off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on
unconstrained text generation benchmarks, and show that it matches or
outperforms strong autoregressive GPT-2 models across standard quality and
diversity metrics, while vastly outperforming diffusion-based baselines. On
controlled text generation, SSD-LM also outperforms competitive baselines, with
an extra advantage in modularity.",https://github.com/xhan77/ssd-lm,-1
Enhanced Physics-Informed Neural Networks with Augmented Lagrangian Relaxation Method (AL-PINNs),0.20289,"Physics-Informed Neural Networks (PINNs) have become a prominent application
of deep learning in scientific computation, as they are powerful approximators
of solutions to nonlinear partial differential equations (PDEs). There have
been numerous attempts to facilitate the training process of PINNs by adjusting
the weight of each component of the loss function, called adaptive
loss-balancing algorithms. In this paper, we propose an Augmented Lagrangian
relaxation method for PINNs (AL-PINNs). We treat the initial and boundary
conditions as constraints for the optimization problem of the PDE residual. By
employing Augmented Lagrangian relaxation, the constrained optimization problem
becomes a sequential max-min problem so that the learnable parameters $\lambda$
adaptively balance each loss component. Our theoretical analysis reveals that
the sequence of minimizers of the proposed loss functions converges to an
actual solution for the Helmholtz, viscous Burgers, and Klein--Gordon
equations. We demonstrate through various numerical experiments that AL-PINNs
yield a much smaller relative error compared with that of state-of-the-art
adaptive loss-balancing algorithms.",https://github.com/HwijaeSon/AL-PINNs,-1
Gradient-Based Constrained Sampling from Language Models,0.213554,"Large pretrained language models generate fluent text but are notoriously
hard to controllably sample from. In this work, we study constrained sampling
from such language models: generating text that satisfies user-defined
constraints, while maintaining fluency and the model's performance in a
downstream task. We propose MuCoLa -- a sampling procedure that combines the
log-likelihood of the language model with arbitrary (differentiable)
constraints in a single energy function, and then generates samples in a
non-autoregressive manner. Specifically, it initializes the entire output
sequence with noise and follows a Markov chain defined by Langevin Dynamics
using the gradients of the energy function. We evaluate MuCoLa on text
generation with soft and hard constraints as well as their combinations
obtaining significant improvements over competitive baselines for toxicity
avoidance, sentiment control, and keyword-guided generation.",https://github.com/Sachin19/mucoco/tree/sampling,-1
Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues,0.0330328,"Movies reflect society and also hold power to transform opinions. Social
biases and stereotypes present in movies can cause extensive damage due to
their reach. These biases are not always found to be the need of storyline but
can creep in as the author's bias. Movie production houses would prefer to
ascertain that the bias present in a script is the story's demand. Today, when
deep learning models can give human-level accuracy in multiple tasks, having an
AI solution to identify the biases present in the script at the writing stage
can help them avoid the inconvenience of stalled release, lawsuits, etc. Since
AI solutions are data intensive and there exists no domain specific data to
address the problem of biases in scripts, we introduce a new dataset of movie
scripts that are annotated for identity bias. The dataset contains dialogue
turns annotated for (i) bias labels for seven categories, viz., gender,
race/ethnicity, religion, age, occupation, LGBTQ, and other, which contains
biases like body shaming, personality bias, etc. (ii) labels for sensitivity,
stereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated
with context awareness, (iv) target groups and reason for bias labels and (v)
expert-driven group-validation process for high quality annotations. We also
report various baseline performances for bias identification and category
detection on our dataset.",https://github.com/sahoonihar/HIBD_LREC_2022,-1
Competency Assessment for Autonomous Agents using Deep Generative Models,0.033462,"For autonomous agents to act as trustworthy partners to human users, they
must be able to reliably communicate their competency for the tasks they are
asked to perform. Towards this objective, we develop probabilistic world models
based on deep generative modelling that allow for the simulation of agent
trajectories and accurate calculation of tasking outcome probabilities. By
combining the strengths of conditional variational autoencoders with recurrent
neural networks, the deep generative world model can probabilistically forecast
trajectories over long horizons to task completion. We show how these
forecasted trajectories can be used to calculate outcome probability
distributions, which enable the precise assessment of agent competency for
specific tasks and initial settings.",None,-1
Continual Spatio-Temporal Graph Convolutional Networks,0.070485,"Graph-based reasoning over skeleton data has emerged as a promising approach
for human action recognition. However, the application of prior graph-based
methods, which predominantly employ whole temporal sequences as their input, to
the setting of online inference entails considerable computational redundancy.
In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph
Convolutional Neural Network as a Continual Inference Network, which can
perform step-by-step predictions in time without repeat frame processing. To
evaluate our method, we create a continual version of ST-GCN, CoST-GCN,
alongside two derived methods with different self-attention mechanisms, CoAGCN
and CoS-TR. We investigate weight transfer strategies and architectural
modifications for inference acceleration, and perform experiments on the NTU
RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar
predictive accuracy, we observe up to 109x reduction in time complexity,
on-hardware accelerations of 26x, and reductions in maximum allocated memory of
52% during online inference.",https://github.com/lukashedegaard/continual-skeletons,-1
On the Role of Bidirectionality in Language Model Pre-Training,0.0363403,"Prior work on language model pre-training has explored different
architectures and learning objectives, but differences in data, hyperparameters
and evaluation make a principled comparison difficult. In this work, we focus
on bidirectionality as a key factor that differentiates existing approaches,
and present a comprehensive study of its role in next token prediction, text
infilling, zero-shot priming and fine-tuning. We propose a new framework that
generalizes prior approaches, including fully unidirectional models like GPT,
fully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.
Our framework distinguishes between two notions of bidirectionality
(bidirectional context and bidirectional attention) and allows us to control
each of them separately. We find that the optimal configuration is largely
application-dependent (e.g., bidirectional attention is beneficial for
fine-tuning and infilling, but harmful for next token prediction and zero-shot
priming). We train models with up to 6.7B parameters, and find differences to
remain consistent at scale. While prior work on scaling has focused on
left-to-right autoregressive models, our results suggest that this approach
comes with some trade-offs, and it might be worthwhile to develop very large
bidirectional models.",None,-1
Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks,0.232683,"In the pursuit of explaining implicit regularization in deep learning,
prominent focus was given to matrix and tensor factorizations, which correspond
to simplified neural networks. It was shown that these models exhibit an
implicit tendency towards low matrix and tensor ranks, respectively. Drawing
closer to practical deep learning, the current paper theoretically analyzes the
implicit regularization in hierarchical tensor factorization, a model
equivalent to certain deep convolutional neural networks. Through a dynamical
systems lens, we overcome challenges associated with hierarchy, and establish
implicit regularization towards low hierarchical tensor rank. This translates
to an implicit regularization towards locality for the associated convolutional
networks. Inspired by our theory, we design explicit regularization
discouraging locality, and demonstrate its ability to improve the performance
of modern convolutional networks on non-local tasks, in defiance of
conventional wisdom by which architectural changes are needed. Our work
highlights the potential of enhancing neural networks via theoretical analysis
of their implicit regularization.",https://github.com/asafmaman101/imp_reg_htf,-1
HyperShot: Few-Shot Learning by Kernel HyperNetworks,0.0336622,"Few-shot models aim at making predictions using a minimal number of labeled
examples from a given task. The main challenge in this area is the one-shot
setting where only one element represents each class. We propose HyperShot -
the fusion of kernels and hypernetwork paradigm. Compared to reference
approaches that apply a gradient-based adjustment of the parameters, our model
aims to switch the classification module parameters depending on the task's
embedding. In practice, we utilize a hypernetwork, which takes the aggregated
information from support data and returns the classifier's parameters
handcrafted for the considered problem. Moreover, we introduce the kernel-based
representation of the support examples delivered to hypernetwork to create the
parameters of the classification module. Consequently, we rely on relations
between embeddings of the support examples instead of direct feature values
provided by the backbone models. Thanks to this approach, our model can adapt
to highly different tasks.",None,-1
Word Order Matters when you Increase Masking,0.0363693,"Word order, an essential property of natural languages, is injected in
Transformer-based neural language models using position encoding. However,
recent experiments have shown that explicit position encoding is not always
useful, since some models without such feature managed to achieve state-of-the
art performance on some tasks. To understand better this phenomenon, we examine
the effect of removing position encodings on the pre-training objective itself
(i.e., masked language modelling), to test whether models can reconstruct
position information from co-occurrences alone. We do so by controlling the
amount of masked tokens in the input sentence, as a proxy to affect the
importance of position information for the task. We find that the necessity of
position information increases with the amount of masking, and that masked
language models without position encodings are not able to reconstruct this
information on the task. These findings point towards a direct relationship
between the amount of masking and the ability of Transformers to capture
order-sensitive aspects of language using position encoding.",https://github.com/rycolab/artificial-languages,-1
SLiDE: Self-supervised LiDAR De-snowing through Reconstruction Difficulty,0.295948,"LiDAR is widely used to capture accurate 3D outdoor scene structures.
However, LiDAR produces many undesirable noise points in snowy weather, which
hamper analyzing meaningful 3D scene structures. Semantic segmentation with
snow labels would be a straightforward solution for removing them, but it
requires laborious point-wise annotation. To address this problem, we propose a
novel self-supervised learning framework for snow points removal in LiDAR point
clouds. Our method exploits the structural characteristic of the noise points:
low spatial correlation with their neighbors. Our method consists of two deep
neural networks: Point Reconstruction Network (PR-Net) reconstructs each point
from its neighbors; Reconstruction Difficulty Network (RD-Net) predicts
point-wise difficulty of the reconstruction by PR-Net, which we call
reconstruction difficulty. With simple post-processing, our method effectively
detects snow points without any label. Our method achieves the state-of-the-art
performance among label-free approaches and is comparable to the
fully-supervised method. Moreover, we demonstrate that our method can be
exploited as a pretext task to improve label-efficiency of supervised training
of de-snowing.",None,-1
Segmentation Enhanced Lameness Detection in Dairy Cows from RGB and Depth Video,0.270865,"Cow lameness is a severe condition that affects the life cycle and life
quality of dairy cows and results in considerable economic losses. Early
lameness detection helps farmers address illnesses early and avoid negative
effects caused by the degeneration of cows' condition. We collected a dataset
of short clips of cows passing through a hallway exiting a milking station and
annotated the degree of lameness of the cows. This paper explores the resulting
dataset and provides a detailed description of the data collection process.
Additionally, we proposed a lameness detection method that leverages
pre-trained neural networks to extract discriminative features from videos and
assign a binary score to each cow indicating its condition: ""healthy"" or
""lame."" We improve this approach by forcing the model to focus on the structure
of the cow, which we achieve by substituting the RGB videos with binary
segmentation masks predicted with a trained segmentation model. This work aims
to encourage research and provide insights into the applicability of computer
vision models for cow lameness detection on farms.",None,-1
Volume Rendering Digest (for NeRF),0.333182,"Neural Radiance Fields employ simple volume rendering as a way to overcome
the challenges of differentiating through ray-triangle intersections by
leveraging a probabilistic notion of visibility. This is achieved by assuming
the scene is composed by a cloud of light-emitting particles whose density
changes in space. This technical report summarizes the derivations for
differentiable volume rendering. It is a condensed version of previous reports,
but rewritten in the context of NeRF, and adopting its commonly used notation.",None,-1
Depth Field Networks for Generalizable Multi-view Scene Representation,0.0471364,"Modern 3D computer vision leverages learning to boost geometric reasoning,
mapping image data to classical structures such as cost volumes or epipolar
constraints to improve matching. These architectures are specialized according
to the particular problem, and thus require significant task-specific tuning,
often leading to poor domain generalization performance. Recently, generalist
Transformer architectures have achieved impressive results in tasks such as
optical flow and depth estimation by encoding geometric priors as inputs rather
than as enforced constraints. In this paper, we extend this idea and propose to
learn an implicit, multi-view consistent scene representation, introducing a
series of 3D data augmentation techniques as a geometric inductive prior to
increase view diversity. We also show that introducing view synthesis as an
auxiliary task further improves depth estimation. Our Depth Field Networks
(DeFiNe) achieve state-of-the-art results in stereo and video depth estimation
without explicit geometric constraints, and improve on zero-shot domain
generalization by a wide margin.",None,-1
Frequency-Aware Self-Supervised Monocular Depth Estimation,0.04854,"We present two versatile methods to generally enhance self-supervised
monocular depth estimation (MDE) models. The high generalizability of our
methods is achieved by solving the fundamental and ubiquitous problems in
photometric loss function. In particular, from the perspective of spatial
frequency, we first propose Ambiguity-Masking to suppress the incorrect
supervision under photometric loss at specific object boundaries, the cause of
which could be traced to pixel-level ambiguity. Second, we present a novel
frequency-adaptive Gaussian low-pass filter, designed to robustify the
photometric loss in high-frequency regions. We are the first to propose
blurring images to improve depth estimators with an interpretable analysis.
Both modules are lightweight, adding no parameters and no need to manually
change the network structures. Experiments show that our methods provide
performance boosts to a large number of existing models, including those who
claimed state-of-the-art, while introducing no extra inference computation at
all.",https://github.com/xingyuuchen/freq-aware-depth,-1
MMGL: Multi-Scale Multi-View Global-Local Contrastive learning for Semi-supervised Cardiac Image Segmentation,0.153213,"With large-scale well-labeled datasets, deep learning has shown significant
success in medical image segmentation. However, it is challenging to acquire
abundant annotations in clinical practice due to extensive expertise
requirements and costly labeling efforts. Recently, contrastive learning has
shown a strong capacity for visual representation learning on unlabeled data,
achieving impressive performance rivaling supervised learning in many domains.
In this work, we propose a novel multi-scale multi-view global-local
contrastive learning (MMGL) framework to thoroughly explore global and local
features from different scales and views for robust contrastive learning
performance, thereby improving segmentation performance with limited
annotations. Extensive experiments on the MM-WHS dataset demonstrate the
effectiveness of MMGL framework on semi-supervised cardiac image segmentation,
outperforming the state-of-the-art contrastive learning methods by a large
margin.",None,-1
Adaptive Mean-Residue Loss for Robust Facial Age Estimation,0.158796,"Automated facial age estimation has diverse real-world applications in
multimedia analysis, e.g., video surveillance, and human-computer interaction.
However, due to the randomness and ambiguity of the aging process, age
assessment is challenging. Most research work over the topic regards the task
as one of age regression, classification, and ranking problems, and cannot well
leverage age distribution in representing labels with age ambiguity. In this
work, we propose a simple yet effective loss function for robust facial age
estimation via distribution learning, i.e., adaptive mean-residue loss, in
which, the mean loss penalizes the difference between the estimated age
distribution's mean and the ground-truth age, whereas the residue loss
penalizes the entropy of age probability out of dynamic top-K in the
distribution. Experimental results in the datasets FG-NET and CLAP2016 have
validated the effectiveness of the proposed loss. Our code is available at
https://github.com/jacobzhaoziyuan/AMR-Loss.",None,-1
arXivEdits: Understanding the Human Revision Process in Scientific Writing,0.0943967,"Scientific publications are the primary means to communicate research
discoveries, where the writing quality is of crucial importance. However, prior
work studying the human editing process in this domain mainly focused on the
abstract or introduction sections, resulting in an incomplete picture. In this
work, we provide a complete computational framework for studying text revision
in scientific writing. We first introduce arXivEdits, a new annotated corpus of
751 full papers from arXiv with gold sentence alignment across their multiple
versions of revision, as well as fine-grained span-level edits and their
underlying intentions for 1,000 sentence pairs. It supports our data-driven
analysis to unveil the common strategies practiced by researchers for revising
their papers. To scale up the analysis, we also develop automatic methods to
extract revision at document-, sentence-, and word-levels. A neural CRF
sentence alignment model trained on our corpus achieves 93.8 F1, enabling the
reliable matching of sentences between different versions. We formulate the
edit extraction task as a span alignment problem, and our proposed method
extracts more fine-grained and explainable edits, compared to the commonly used
diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1
on the fine-grained intent classification task. Our data and system are
released at tiny.one/arxivedits.",https://tiny.one/arxivedits,-1
Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes,0.265738,"We study offline reinforcement learning (RL) in partially observable Markov
decision processes. In particular, we aim to learn an optimal policy from a
dataset collected by a behavior policy which possibly depends on the latent
state. Such a dataset is confounded in the sense that the latent state
simultaneously affects the action and the observation, which is prohibitive for
existing offline RL algorithms. To this end, we propose the \underline{P}roxy
variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization
(\texttt{P3O}) algorithm, which addresses the confounding bias and the
distributional shift between the optimal and behavior policies in the context
of general function approximation. At the core of \texttt{P3O} is a coupled
sequence of pessimistic confidence regions constructed via proximal causal
inference, which is formulated as minimax estimation. Under a partial coverage
assumption on the confounded dataset, we prove that \texttt{P3O} achieves a
$n^{-1/2}$-suboptimality, where $n$ is the number of trajectories in the
dataset. To our best knowledge, \texttt{P3O} is the first provably efficient
offline RL algorithm for POMDPs with a confounded dataset.",None,-1
Graph-Based Multilingual Label Propagation for Low-Resource Part-of-Speech Tagging,0.0407773,"Part-of-Speech (POS) tagging is an important component of the NLP pipeline,
but many low-resource languages lack labeled data for training. An established
method for training a POS tagger in such a scenario is to create a labeled
training set by transferring from high-resource languages. In this paper, we
propose a novel method for transferring labels from multiple high-resource
source to low-resource target languages. We formalize POS tag projection as
graph-based label propagation. Given translations of a sentence in multiple
languages, we create a graph with words as nodes and alignment links as edges
by aligning words for all language pairs. We then propagate node labels from
source to target using a Graph Neural Network augmented with transformer
layers. We show that our propagation creates training sets that allow us to
train POS taggers for a diverse set of languages. When combined with enhanced
contextualized embeddings, our method achieves a new state-of-the-art for
unsupervised POS tagging of low-resource languages.",https://github.com/ayyoobimani/GLP-POS,-1
Using Multi-Encoder Fusion Strategies to Improve Personalized Response Selection,0.0521568,"Personalized response selection systems are generally grounded on persona.
However, there exists a co-relation between persona and empathy, which is not
explored well in these systems. Also, faithfulness to the conversation context
plunges when a contradictory or an off-topic response is selected. This paper
attempts to address these issues by proposing a suite of fusion strategies that
capture the interaction between persona, emotion, and entailment information of
the utterances. Ablation studies on the Persona-Chat dataset show that
incorporating emotion and entailment improves the accuracy of response
selection. We combine our fusion strategies and concept-flow encoding to train
a BERT-based model which outperforms the previous methods by margins larger
than 2.3 % on original personas and 1.9 % on revised personas in terms of
hits@1 (top-1 accuracy), achieving a new state-of-the-art performance on the
Persona-Chat dataset.",https://github.com/allenai/allennlp-models,-1
Attention-Aware Anime Line Drawing Colorization,0.0739663,"Automatic colorization of anime line drawing has attracted much attention in
recent years since it can substantially benefit the animation industry.
User-hint based methods are the mainstream approach for line drawing
colorization, while reference-based methods offer a more intuitive approach.
Nevertheless, although reference-based methods can improve feature aggregation
of the reference image and the line drawing, the colorization results are not
compelling in terms of color consistency or semantic correspondence. In this
paper, we introduce an attention-based model for anime line drawing
colorization, in which a channel-wise and spatial-wise Convolutional Attention
module is used to improve the ability of the encoder for feature extraction and
key area perception, and a Stop-Gradient Attention module with cross-attention
and self-attention is used to tackle the cross-domain long-range dependency
problem. Extensive experiments show that our method outperforms other SOTA
methods, with more accurate line structure and semantic color information.",None,-1
ET5: A Novel End-to-end Framework for Conversational Machine Reading Comprehension,0.0318155,"Conversational machine reading comprehension (CMRC) aims to assist computers
to understand an natural language text and thereafter engage in a multi-turn
conversation to answer questions related to the text. Existing methods
typically require three steps: (1) decision making based on entailment
reasoning; (2) span extraction if required by the above decision; (3) question
rephrasing based on the extracted span. However, for nearly all these methods,
the span extraction and question rephrasing steps cannot fully exploit the
fine-grained entailment reasoning information in decision making step because
of their relative independence, which will further enlarge the information gap
between decision making and question phrasing. Thus, to tackle this problem, we
propose a novel end-to-end framework for conversational machine reading
comprehension based on shared parameter mechanism, called entailment reasoning
T5 (ET5). Despite the lightweight of our proposed framework, experimental
results show that the proposed ET5 achieves new state-of-the-art results on the
ShARC leaderboard with the BLEU-4 score of 55.2. Our model and code are
publicly available at https://github.com/Yottaxx/ET5.",https://github.com/Yottaxx/ET5,-1
Zero-Shot Cost Models for Out-of-the-box Learned Cost Prediction,0.155043,"In this paper, we introduce zero-shot cost models which enable learned cost
estimation that generalizes to unseen databases. In contrast to
state-of-the-art workload-driven approaches which require to execute a large
set of training queries on every new database, zero-shot cost models thus allow
to instantiate a learned cost model out-of-the-box without expensive training
data collection. To enable such zero-shot cost models, we suggest a new
learning paradigm based on pre-trained cost models. As core contributions to
support the transfer of such a pre-trained cost model to unseen databases, we
introduce a new model architecture and representation technique for encoding
query workloads as input to those models. As we will show in our evaluation,
zero-shot cost estimation can provide more accurate cost estimates than
state-of-the-art models for a wide range of (real-world) databases without
requiring any query executions on unseen databases. Furthermore, we show that
zero-shot cost models can be used in a few-shot mode that further improves
their quality by retraining them just with a small number of additional
training queries on the unseen database.",None,-1
Semantic-aligned Fusion Transformer for One-shot Object Detection,0.692228,"One-shot object detection aims at detecting novel objects according to merely
one given instance. With extreme data scarcity, current approaches explore
various feature fusions to obtain directly transferable meta-knowledge. Yet,
their performances are often unsatisfactory. In this paper, we attribute this
to inappropriate correlation methods that misalign query-support semantics by
overlooking spatial structures and scale variances. Upon analysis, we leverage
the attention mechanism and propose a simple but effective architecture named
Semantic-aligned Fusion Transformer (SaFT) to resolve these issues.
Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale
semantic enhancement and a horizontal fusion module (HFM) for cross-sample
feature fusion. Together, they broaden the vision for each feature point from
the support to a whole augmented feature pyramid from the query, facilitating
semantic-aligned associations. Extensive experiments on multiple benchmarks
demonstrate the superiority of our framework. Without fine-tuning on novel
classes, it brings significant performance gains to one-stage baselines,
lifting state-of-the-art results to a higher level.",None,-1
ASR Error Correction with Constrained Decoding on Operation Prediction,0.0335647,"Error correction techniques remain effective to refine outputs from automatic
speech recognition (ASR) models. Existing end-to-end error correction methods
based on an encoder-decoder architecture process all tokens in the decoding
phase, creating undesirable latency. In this paper, we propose an ASR error
correction method utilizing the predictions of correction operations. More
specifically, we construct a predictor between the encoder and the decoder to
learn if a token should be kept (""K""), deleted (""D""), or changed (""C"") to
restrict decoding to only part of the input sequence embeddings (the ""C""
tokens) for fast inference. Experiments on three public datasets demonstrate
the effectiveness of the proposed approach in reducing the latency of the
decoding process in ASR correction. It enhances the inference speed by at least
three times (3.4 and 5.7 times) while maintaining the same level of accuracy
(with WER reductions of 0.53% and 1.69% respectively) for our two proposed
models compared to a solid encoder-decoder baseline. In the meantime, we
produce and release a benchmark dataset contributing to the ASR error
correction community to foster research along this line.",https://github.com/yangjingyuan/ConstDecoder,-1
"Streaming, fast and accurate on-device Inverse Text Normalization for Automatic Speech Recognition",0.0327378,"Automatic Speech Recognition (ASR) systems typically yield output in lexical
form. However, humans prefer a written form output. To bridge this gap, ASR
systems usually employ Inverse Text Normalization (ITN).
  In previous works, Weighted Finite State Transducers (WFST) have been
employed to do ITN. WFSTs are nicely suited to this task but their size and
run-time costs can make deployment on embedded applications challenging.
  In this paper, we describe the development of an on-device ITN system that is
streaming, lightweight & accurate. At the core of our system is a streaming
transformer tagger, that tags lexical tokens from ASR. The tag informs which
ITN category might be applied, if at all. Following that, we apply an
ITN-category-specific WFST, only on the tagged text, to reliably perform the
ITN conversion. We show that the proposed ITN solution performs equivalent to
strong baselines, while being significantly smaller in size and retaining
customization capabilities.",None,-1
Spiking Approximations of the MaxPooling Operation in Deep SNNs,0.0316409,"Spiking Neural Networks (SNNs) are an emerging domain of biologically
inspired neural networks that have shown promise for low-power AI. A number of
methods exist for building deep SNNs, with Artificial Neural Network
(ANN)-to-SNN conversion being highly successful. MaxPooling layers in
Convolutional Neural Networks (CNNs) are an integral component to downsample
the intermediate feature maps and introduce translational invariance, but the
absence of their hardware-friendly spiking equivalents limits such CNNs'
conversion to deep SNNs. In this paper, we present two hardware-friendly
methods to implement Max-Pooling in deep SNNs, thus facilitating easy
conversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute
SNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware
(with MNIST, FMNIST, & CIFAR10 dataset); thus, showing the feasibility of our
approach.",None,-1
Multi-label Transformer for Action Unit Detection,0.300782,"Action Unit (AU) Detection is the branch of affective computing that aims at
recognizing unitary facial muscular movements. It is key to unlock unbiased
computational face representations and has therefore aroused great interest in
the past few years. One of the main obstacles toward building efficient deep
learning based AU detection system is the lack of wide facial image databases
annotated by AU experts. In that extent the ABAW challenge paves the way toward
better AU detection as it involves a 2M frames AU annotated dataset. In this
paper, we present our submission to the ABAW3 challenge. In a nutshell, we
applied a multi-label detection transformer that leverage multi-head attention
to learn which part of the face image is the most relevant to predict each AU.",None,-1
Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer,0.750273,"Learning Bird's Eye View (BEV) representation from surrounding-view cameras
is of great importance for autonomous driving. In this work, we propose a
Geometry-guided Kernel Transformer (GKT), a novel 2D-to-BEV representation
learning mechanism. GKT leverages the geometric priors to guide the transformer
to focus on discriminative regions and unfolds kernel features to generate BEV
representation. For fast inference, we further introduce a look-up table (LUT)
indexing method to get rid of the camera's calibrated parameters at runtime.
GKT can run at $72.3$ FPS on 3090 GPU / $45.6$ FPS on 2080ti GPU and is robust
to the camera deviation and the predefined BEV height. And GKT achieves the
state-of-the-art real-time segmentation results, i.e., 38.0 mIoU
(100m$\times$100m perception range at a 0.5m resolution) on the nuScenes val
set. Given the efficiency, effectiveness, and robustness, GKT has great
practical values in autopilot scenarios, especially for real-time running
systems. Code and models will be available at
\url{https://github.com/hustvl/GKT}.",https://github.com/hustvl/GKT,-1
PanoDepth: A Two-Stage Approach for Monocular Omnidirectional Depth Estimation,0.0835791,"Omnidirectional 3D information is essential for a wide range of applications
such as Virtual Reality, Autonomous Driving, Robotics, etc. In this paper, we
propose a novel, model-agnostic, two-stage pipeline for omnidirectional
monocular depth estimation. Our proposed framework PanoDepth takes one 360
image as input, produces one or more synthesized views in the first stage, and
feeds the original image and the synthesized images into the subsequent stereo
matching stage. In the second stage, we propose a differentiable Spherical
Warping Layer to handle omnidirectional stereo geometry efficiently and
effectively. By utilizing the explicit stereo-based geometric constraints in
the stereo matching stage, PanoDepth can generate dense high-quality depth. We
conducted extensive experiments and ablation studies to evaluate PanoDepth with
both the full pipeline as well as the individual modules in each stage. Our
results show that PanoDepth outperforms the state-of-the-art approaches by a
large margin for 360 monocular depth estimation.",None,-1
Abstraction for Deep Reinforcement Learning,0.0117083,"We characterise the problem of abstraction in the context of deep
reinforcement learning. Various well established approaches to analogical
reasoning and associative memory might be brought to bear on this issue, but
they present difficulties because of the need for end-to-end differentiability.
We review developments in AI and machine learning that could facilitate their
adoption.",None,-1
GCDT: A Chinese RST Treebank for Multigenre and Multilingual Discourse Parsing,0.156064,"A lack of large-scale human-annotated data has hampered the hierarchical
discourse parsing of Chinese. In this paper, we present GCDT, the largest
hierarchical discourse treebank for Mandarin Chinese in the framework of
Rhetorical Structure Theory (RST). GCDT covers over 60K tokens across five
genres of freely available text, using the same relation inventory as
contemporary RST treebanks for English. We also report on this dataset's
parsing experiments, including state-of-the-art (SOTA) scores for Chinese RST
parsing and RST parsing on the English GUM dataset, using cross-lingual
training in Chinese and English with multilingual embeddings.",https://github.com/logan-siyao-peng/GCDT,-1
From Indoor To Outdoor: Unsupervised Domain Adaptive Gait Recognition,0.156882,"Gait recognition is an important AI task, which has been progressed rapidly
with the development of deep learning. However, existing learning based gait
recognition methods mainly focus on the single domain, especially the
constrained laboratory environment. In this paper, we study a new problem of
unsupervised domain adaptive gait recognition (UDA-GR), that learns a gait
identifier with supervised labels from the indoor scenes (source domain), and
is applied to the outdoor wild scenes (target domain). For this purpose, we
develop an uncertainty estimation and regularization based UDA-GR method.
Specifically, we investigate the characteristic of gaits in the indoor and
outdoor scenes, for estimating the gait sample uncertainty, which is used in
the unsupervised fine-tuning on the target domain to alleviate the noises of
the pseudo labels. We also establish a new benchmark for the proposed problem,
experimental results on which show the effectiveness of the proposed method. We
will release the benchmark and source code in this work to the public.",None,-1
Meta Policy Learning for Cold-Start Conversational Recommendation,0.117734,"Conversational recommender systems (CRS) explicitly solicit users'
preferences for improved recommendations on the fly. Most existing CRS
solutions count on a single policy trained by reinforcement learning for a
population of users. However, for users new to the system, such a global policy
becomes ineffective to satisfy them, i.e., the cold-start challenge. In this
paper, we study CRS policy learning for cold-start users via meta-reinforcement
learning. We propose to learn a meta policy and adapt it to new users with only
a few trials of conversational recommendations. To facilitate fast policy
adaptation, we design three synergetic components. Firstly, we design a
meta-exploration policy dedicated to identifying user preferences via a few
exploratory conversations, which accelerates personalized policy adaptation
from the meta policy. Secondly, we adapt the item recommendation module for
each user to maximize the recommendation quality based on the collected
conversation states during conversations. Thirdly, we propose a
Transformer-based state encoder as the backbone to connect the previous two
components. It provides comprehensive state representations by modeling
complicated relations between positive and negative feedback during the
conversation. Extensive experiments on three datasets demonstrate the advantage
of our solution in serving new users, compared with a rich set of
state-of-the-art CRS solutions.",https://github.com/zdchu/MetaCRS.git,-1
Weakly Supervised 3D Point Cloud Segmentation via Multi-Prototype Learning,0.365344,"Addressing the annotation challenge in 3D Point Cloud segmentation has
inspired research into weakly supervised learning. Existing approaches mainly
focus on exploiting manifold and pseudo-labeling to make use of large unlabeled
data points. A fundamental challenge here lies in the large intra-class
variations of local geometric structure, resulting in subclasses within a
semantic class. In this work, we leverage this intuition and opt for
maintaining an individual classifier for each subclass. Technically, we design
a multi-prototype classifier, each prototype serves as the classifier weights
for one subclass. To enable effective updating of multi-prototype classifier
weights, we propose two constraints respectively for updating the prototypes
w.r.t. all point features and for encouraging the learning of diverse
prototypes. Experiments on weakly supervised 3D point cloud segmentation tasks
validate the efficacy of proposed method in particular at low-label regime. Our
hypothesis is also verified given the consistent discovery of semantic
subclasses at no cost of additional annotations.",None,-1
Effective Image Tampering Localization with Multi-Scale ConvNeXt Feature Fusion,0.0985381,"With the widespread use of powerful image editing tools, image tampering
becomes easy and realistic. Existing image forensic methods still face
challenges of low generalization performance and robustness. In this letter, we
propose an effective image tampering localization scheme based on ConvNeXt
network and multi-scale feature fusion. Stacked ConvNeXt blocks are used as an
encoder to capture hierarchical multi-scale features, which are then fused in
decoder for locating tampered pixels accurately. Combined loss and effective
data augmentation are adopted to further improve the model performance.
Extensive experimental results show that localization performance of our
proposed scheme outperforms other state-of-the-art ones. The source code will
be available at https://github.com/ZhuHC98/ITL-SSN.",https://github.com/ZhuHC98/ITL-SSN,-1
Accurate Polygonal Mapping of Buildings in Satellite Imagery,0.212006,"This paper studies the problem of polygonal mapping of buildings by tackling
the issue of mask reversibility that leads to a notable performance gap between
the predicted masks and polygons from the learning-based methods. We addressed
such an issue by exploiting the hierarchical supervision (of bottom-level
vertices, mid-level line segments and the high-level regional masks) and
proposed a novel interaction mechanism of feature embedding sourced from
different levels of supervision signals to obtain reversible building masks for
polygonal mapping of buildings. As a result, we show that the learned
reversible building masks take all the merits of the advances of deep
convolutional neural networks for high-performing polygonal mapping of
buildings. In the experiments, we evaluated our method on the two public
benchmarks of AICrowd and Inria. On the AICrowd dataset, our proposed method
obtains unanimous improvements on the metrics of AP, APboundary and PoLiS. For
the Inria dataset, our proposed method also obtains very competitive results on
the metrics of IoU and Accuracy. The models and source code are available at
https://github.com/SarahwXU.",https://github.com/SarahwXU/HiSup,-1
Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs,0.41179,"Question answering over temporal knowledge graphs (KGs) efficiently uses
facts contained in a temporal KG, which records entity relations and when they
occur in time, to answer natural language questions (e.g., ""Who was the
president of the US before Obama?""). These questions often involve three
time-related challenges that previous work fail to adequately address: 1)
questions often do not specify exact timestamps of interest (e.g., ""Obama""
instead of 2000); 2) subtle lexical differences in time relations (e.g.,
""before"" vs ""after""); 3) off-the-shelf temporal KG embeddings that previous
work builds on ignore the temporal order of timestamps, which is crucial for
answering temporal-order related questions. In this paper, we propose a
time-sensitive question answering (TSQA) framework to tackle these problems.
TSQA features a timestamp estimation module to infer the unwritten timestamp
from the question. We also employ a time-sensitive KG encoder to inject
ordering information into the temporal KG embeddings that TSQA is based on.
With the help of techniques to reduce the search space for potential answers,
TSQA significantly outperforms the previous state of the art on a new benchmark
for question answering over temporal KGs, especially achieving a 32% (absolute)
error reduction on complex questions that require multiple steps of reasoning
over facts in the temporal KG.",https://github.com/apoorvumang/CronKGQA,-1
SCVRL: Shuffled Contrastive Video Representation Learning,0.0967325,"We propose SCVRL, a novel contrastive-based framework for self-supervised
learning for videos. Differently from previous contrast learning based methods
that mostly focus on learning visual semantics (e.g., CVRL), SCVRL is capable
of learning both semantic and motion patterns. For that, we reformulate the
popular shuffling pretext task within a modern contrastive learning paradigm.
We show that our transformer-based network has a natural capacity to learn
motion in self-supervised settings and achieves strong performance,
outperforming CVRL on four benchmarks.",None,-1
Cross-view and Cross-domain Underwater Localization based on Optical Aerial and Acoustic Underwater Images,0.197529,"Cross-view image matches have been widely explored on terrestrial image
localization using aerial images from drones or satellites. This study expands
the cross-view image match idea and proposes a cross-domain and cross-view
localization framework. The method identifies the correlation between color
aerial images and underwater acoustic images to improve the localization of
underwater vehicles that travel in partially structured environments such as
harbors and marinas. The approach is validated on a real dataset acquired by an
underwater vehicle in a marina. The results show an improvement in the
localization when compared to the dead reckoning of the vehicle.",https://github.com/matheusbg8/aracati2017,-1
FaceDancer: Pose- and Occlusion-Aware High Fidelity Face Swapping,0.148432,"In this work, we present a new single-stage method for subject agnostic face
swapping and identity transfer, named FaceDancer. We have two major
contributions: Adaptive Feature Fusion Attention (AFFA) and Interpreted Feature
Similarity Regularization (IFSR). The AFFA module is embedded in the decoder
and adaptively learns to fuse attribute features and features conditioned on
identity information without requiring any additional facial segmentation
process. In IFSR, we leverage the intermediate features in an identity encoder
to preserve important attributes such as head pose, facial expression,
lighting, and occlusion in the target face, while still transferring the
identity of the source face with high fidelity. We conduct extensive
quantitative and qualitative experiments on various datasets and show that the
proposed FaceDancer outperforms other state-of-the-art networks in terms of
identityn transfer, while having significantly better pose preservation than
most of the previous methods.",https://github.com/felixrosberg/FaceDancer,-1
OPD: Single-view 3D Openable Part Detection,0.386091,"We address the task of predicting what parts of an object can open and how
they move when they do so. The input is a single image of an object, and as
output we detect what parts of the object can open, and the motion parameters
describing the articulation of each openable part. To tackle this task, we
create two datasets of 3D objects: OPDSynth based on existing synthetic
objects, and OPDReal based on RGBD reconstructions of real objects. We then
design OPDRCNN, a neural architecture that detects openable parts and predicts
their motion parameters. Our experiments show that this is a challenging task
especially when considering generalization across object categories, and the
limited amount of information in a single image. Our architecture outperforms
baselines and prior work especially for RGB image inputs. Short video summary
at https://www.youtube.com/watch?v=P85iCaD0rfc",https://github.com/facebookresearch/detectron2,-1
Rediscovering Argumentation Principles Utilizing Collective Attacks,0.0722339,"Argumentation Frameworks (AFs) are a key formalism in AI research. Their
semantics have been investigated in terms of principles, which define
characteristic properties in order to deliver guidance for analysing
established and developing new semantics. Because of the simple structure of
AFs, many desired properties hold almost trivially, at the same time hiding
interesting concepts behind syntactic notions. We extend the principle-based
approach to Argumentation Frameworks with Collective Attacks (SETAFs) and
provide a comprehensive overview of common principles for their semantics. Our
analysis shows that investigating principles based on decomposing the given
SETAF (e.g. directionality or SCC-recursiveness) poses additional challenges in
comparison to usual AFs. We introduce the notion of the reduct as well as the
modularization principle for SETAFs which will prove beneficial for this kind
of investigation. We then demonstrate how our findings can be utilized for
incremental computation of extensions and give a novel parameterized
tractability result for verifying preferred extensions.",None,-1
Who's the Expert? On Multi-source Belief Change,0.0432163,"Consider the following belief change/merging scenario. A group of information
sources gives a sequence of reports about the state of the world at various
instances (e.g. different points in time). The true states at these instances
are unknown to us. The sources have varying levels of expertise, also unknown
to us, and may be knowledgeable on some topics but not others. This may cause
sources to report false statements in areas they lack expertise. What should we
believe on the basis of these reports? We provide a framework in which to
explore this problem, based on an extension of propositional logic with
expertise formulas. This extended language allows us to express beliefs about
the state of the world at each instance, as well as beliefs about the expertise
of each source. We propose several postulates, provide a couple of families of
concrete operators, and analyse these operators with respect to the postulates.",None,-1
Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer,0.208626,"We exploit the pre-trained seq2seq model mBART for multilingual text style
transfer. Using machine translated data as well as gold aligned English
sentences yields state-of-the-art results in the three target languages we
consider. Besides, in view of the general scarcity of parallel data, we propose
a modular approach for multilingual formality transfer, which consists of two
training strategies that target adaptation to both language and task. Our
approach achieves competitive performance without monolingual task-specific
parallel data and can be applied to other style transfer tasks as well as to
other languages.",https://github.com/laihuiyuan/multilingual-tst,-1
Unified Fourier-based Kernel and Nonlinearity Design for Equivariant Networks on Homogeneous Spaces,0.124379,"We introduce a unified framework for group equivariant networks on
homogeneous spaces derived from a Fourier perspective. We consider
tensor-valued feature fields, before and after a convolutional layer. We
present a unified derivation of kernels via the Fourier domain by leveraging
the sparsity of Fourier coefficients of the lifted feature fields. The sparsity
emerges when the stabilizer subgroup of the homogeneous space is a compact Lie
group. We further introduce a nonlinear activation, via an elementwise
nonlinearity on the regular representation after lifting and projecting back to
the field through an equivariant convolution. We show that other methods
treating features as the Fourier coefficients in the stabilizer subgroup are
special cases of our activation. Experiments on $SO(3)$ and $SE(3)$ show
state-of-the-art performance in spherical vector field regression, point cloud
classification, and molecular completion.",None,-1
Frame-wise Action Representations for Long Videos via Sequence Contrastive Learning,0.477596,"Prior works on action representation learning mainly focus on designing
various architectures to extract the global representations for short video
clips. In contrast, many practical applications such as video alignment have
strong demand for learning dense representations for long videos. In this
paper, we introduce a novel contrastive action representation learning (CARL)
framework to learn frame-wise action representations, especially for long
videos, in a self-supervised manner. Concretely, we introduce a simple yet
efficient video encoder that considers spatio-temporal context to extract
frame-wise representations. Inspired by the recent progress of self-supervised
learning, we present a novel sequence contrastive loss (SCL) applied on two
correlated views obtained through a series of spatio-temporal data
augmentations. SCL optimizes the embedding space by minimizing the
KL-divergence between the sequence similarity of two augmented views and a
prior Gaussian distribution of timestamp distance. Experiments on FineGym,
PennAction and Pouring datasets show that our method outperforms previous
state-of-the-art by a large margin for downstream fine-grained action
classification. Surprisingly, although without training on paired videos, our
approach also shows outstanding performance on video alignment and fine-grained
frame retrieval tasks. Code and models are available at
https://github.com/minghchen/CARL_code.",https://github.com/minghchen/CARL_code,-1
Efficient Training of Neural Transducer for Speech Recognition,0.0331364,"As one of the most popular sequence-to-sequence modeling approaches for
speech recognition, the RNN-Transducer has achieved evolving performance with
more and more sophisticated neural network models of growing size and
increasing training epochs. While strong computation resources seem to be the
prerequisite of training superior models, we try to overcome it by carefully
designing a more efficient training pipeline. In this work, we propose an
efficient 3-stage progressive training pipeline to build highly-performing
neural transducer models from scratch with very limited computation resources
in a reasonable short time period. The effectiveness of each stage is
experimentally verified on both Librispeech and Switchboard corpora. The
proposed pipeline is able to train transducer models approaching
state-of-the-art performance with a single GPU in just 2-3 weeks. Our best
conformer transducer achieves 4.1% WER on Librispeech test-other with only 35
epochs of training.",None,-1
DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing,0.138272,"The meaningful use of electronic health records (EHR) continues to progress
in the digital era with clinical decision support systems augmented by
artificial intelligence. A priority in improving provider experience is to
overcome information overload and reduce the cognitive burden so fewer medical
errors and cognitive biases are introduced during patient care. One major type
of medical error is diagnostic error due to systematic or predictable errors in
judgment that rely on heuristics. The potential for clinical natural language
processing (cNLP) to model diagnostic reasoning in humans with forward
reasoning from data to diagnosis and potentially reduce the cognitive burden
and medical error has not been investigated. Existing tasks to advance the
science in cNLP have largely focused on information extraction and named entity
recognition through classification tasks. We introduce a novel suite of tasks
coined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for
developing and evaluating cNLP models with clinical diagnostic reasoning
ability. The suite includes six tasks from ten publicly available datasets
addressing clinical text understanding, medical knowledge reasoning, and
diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to
be a natural language generation framework to evaluate pre-trained language
models. Experiments with state-of-the-art pre-trained generative language
models using large general domain models and models that were continually
trained on a medical corpus demonstrate opportunities for improvement when
evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab
repository with a systematic approach to load and evaluate models for the cNLP
community.",https://git.doit.wisc.edu/smph-public/dom/uw-icu-data-scienc/drbench,-1
Detecting Arbitrary Keypoints on Limbs and Skis with Sparse Partly Correct Segmentation Masks,0.323584,"Analyses based on the body posture are crucial for top-class athletes in many
sports disciplines. If at all, coaches label only the most important keypoints,
since manual annotations are very costly. This paper proposes a method to
detect arbitrary keypoints on the limbs and skis of professional ski jumpers
that requires a few, only partly correct segmentation masks during training.
Our model is based on the Vision Transformer architecture with a special design
for the input tokens to query for the desired keypoints. Since we use
segmentation masks only to generate ground truth labels for the freely
selectable keypoints, partly correct segmentation masks are sufficient for our
training procedure. Hence, there is no need for costly hand-annotated
segmentation masks. We analyze different training techniques for freely
selected and standard keypoints, including pseudo labels, and show in our
experiments that only a few partly correct segmentation masks are sufficient
for learning to detect arbitrary keypoints on limbs and skis.",https://github.com/kaulquappe23/arbitrary-keypoints-skijump,-1
Transformer-Based Named Entity Recognition for French Using Adversarial Adaptation to Similar Domain Corpora,0.0420105,"Named Entity Recognition (NER) involves the identification and classification
of named entities in unstructured text into predefined classes. NER in
languages with limited resources, like French, is still an open problem due to
the lack of large, robust, labelled datasets. In this paper, we propose a
transformer-based NER approach for French using adversarial adaptation to
similar domain or general corpora for improved feature extraction and better
generalization. We evaluate our approach on three labelled datasets and show
that our adaptation framework outperforms the corresponding non-adaptive models
for various combinations of transformer models, source datasets and target
corpora.",None,-1
"Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings",0.313047,"In order to equip NLP systems with selective prediction capability, several
task-specific approaches have been proposed. However, which approaches work
best across tasks or even if they consistently outperform the simplest baseline
'MaxProb' remains to be explored. To this end, we systematically study
'selective prediction' in a large-scale setup of 17 datasets across several NLP
tasks. Through comprehensive experiments under in-domain (IID), out-of-domain
(OOD), and adversarial (ADV) settings, we show that despite leveraging
additional resources (held-out data/computation), none of the existing
approaches consistently and considerably outperforms MaxProb in all three
settings. Furthermore, their performance does not translate well across tasks.
For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate
Detection datasets but does not fare well on NLI datasets, especially in the
OOD setting. Thus, we recommend that future selective prediction approaches
should be evaluated across tasks and settings for reliable estimation of their
capabilities.",None,-1
Sample Constrained Treatment Effect Estimation,0.113525,"Treatment effect estimation is a fundamental problem in causal inference. We
focus on designing efficient randomized controlled trials, to accurately
estimate the effect of some treatment on a population of $n$ individuals. In
particular, we study sample-constrained treatment effect estimation, where we
must select a subset of $s \ll n$ individuals from the population to experiment
on. This subset must be further partitioned into treatment and control groups.
Algorithms for partitioning the entire population into treatment and control
groups, or for choosing a single representative subset, have been well-studied.
The key challenge in our setting is jointly choosing a representative subset
and a partition for that set.
  We focus on both individual and average treatment effect estimation, under a
linear effects model. We give provably efficient experimental designs and
corresponding estimators, by identifying connections to discrepancy
minimization and leverage-score-based sampling used in randomized numerical
linear algebra. Our theoretical results obtain a smooth transition to known
guarantees when $s$ equals the population size. We also empirically demonstrate
the performance of our algorithms.",None,-1
MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages,0.0744702,"While there has been a recent burgeoning of applications at the intersection
of natural and programming languages, such as code generation and code
summarization, these applications are usually English-centric. This creates a
barrier for program developers who are not proficient in English. To mitigate
this gap in technology development across languages, we propose a multilingual
dataset, MCoNaLa, to benchmark code generation from natural language commands
extending beyond English. Modeled off of the methodology from the English
Code/Natural Language Challenge (CoNaLa) dataset, we annotated a total of 896
NL-code pairs in three languages: Spanish, Japanese, and Russian. We present a
quantitative evaluation of performance on the MCoNaLa dataset by testing with
state-of-the-art code generation systems. While the difficulties vary across
these three languages, all systems lag significantly behind their English
counterparts, revealing the challenges in adapting code generation to new
languages.",https://github.com/zorazrw/multilingual-conala,-1
Robust Contrastive Learning against Noisy Views,0.313825,"Contrastive learning relies on an assumption that positive pairs contain
related views, e.g., patches of an image or co-occurring multimodal signals of
a video, that share certain underlying information about an instance. But what
if this assumption is violated? The literature suggests that contrastive
learning produces suboptimal representations in the presence of noisy views,
e.g., false positive pairs with no apparent shared information. In this work,
we propose a new contrastive loss function that is robust against noisy views.
We provide rigorous theoretical justifications by showing connections to robust
symmetric losses for noisy binary classification and by establishing a new
contrastive bound for mutual information maximization based on the Wasserstein
distance measure. The proposed loss is completely modality-agnostic and a
simple drop-in replacement for the InfoNCE loss, which makes it easy to apply
to existing contrastive frameworks. We show that our approach provides
consistent improvements over the state-of-the-art on image, video, and graph
contrastive learning benchmarks that exhibit a variety of real-world noise
patterns.",https://github.com/chingyaoc/RINCE,-1
SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation,0.122721,"Feature regression is a simple way to distill large neural network models to
smaller ones. We show that with simple changes to the network architecture,
regression can outperform more complex state-of-the-art approaches for
knowledge distillation from self-supervised models. Surprisingly, the addition
of a multi-layer perceptron head to the CNN backbone is beneficial even if used
only during distillation and discarded in the downstream task. Deeper
non-linear projections can thus be used to accurately mimic the teacher without
changing inference architecture and time. Moreover, we utilize independent
projection heads to simultaneously distill multiple teacher networks. We also
find that using the same weakly augmented image as input for both teacher and
student networks aids distillation. Experiments on ImageNet dataset demonstrate
the efficacy of the proposed changes in various self-supervised distillation
settings.",https://github.com/UCDvision/simreg,-1
Speciesist bias in AI -- How AI applications perpetuate discrimination and unfair outcomes against animals,0.0240382,"Massive efforts are made to reduce biases in both data and algorithms in
order to render AI applications fair. These efforts are propelled by various
high-profile cases where biased algorithmic decision-making caused harm to
women, people of color, minorities, etc. However, the AI fairness field still
succumbs to a blind spot, namely its insensitivity to discrimination against
animals. This paper is the first to describe the 'speciesist bias' and
investigate it in several different AI systems. Speciesist biases are learned
and solidified by AI applications when they are trained on datasets in which
speciesist patterns prevail. These patterns can be found in image recognition
systems, large language models, and recommender systems. Therefore, AI
technologies currently play a significant role in perpetuating and normalizing
violence against animals. This can only be changed when AI fairness frameworks
widen their scope and include mitigation measures for speciesist biases. This
paper addresses the AI community in this regard and stresses the influence AI
systems can have on either increasing or reducing the violence that is
inflicted on animals, and especially on farmed animals.",None,-1
Sparse2Dense: Learning to Densify 3D Features for 3D Object Detection,0.555502,"LiDAR-produced point clouds are the major source for most state-of-the-art 3D
object detectors. Yet, small, distant, and incomplete objects with sparse or
few points are often hard to detect. We present Sparse2Dense, a new framework
to efficiently boost 3D detection performance by learning to densify point
clouds in latent space. Specifically, we first train a dense point 3D detector
(DDet) with a dense point cloud as input and design a sparse point 3D detector
(SDet) with a regular point cloud as input. Importantly, we formulate the
lightweight plug-in S2D module and the point cloud reconstruction module in
SDet to densify 3D features and train SDet to produce 3D features, following
the dense 3D features in DDet. So, in inference, SDet can simulate dense 3D
features from regular (sparse) point cloud inputs without requiring dense
inputs. We evaluate our method on the large-scale Waymo Open Dataset and the
Waymo Domain Adaptation Dataset, showing its high performance and efficiency
over the state of the arts.",None,-1
Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition,0.0350972,"Self-supervised learning (SSL) based models have been shown to generate
powerful representations that can be used to improve the performance of
downstream speech tasks. Several state-of-the-art SSL models are available, and
each of these models optimizes a different loss which gives rise to the
possibility of their features being complementary. This paper proposes using an
ensemble of such SSL representations and models, which exploits the
complementary nature of the features extracted by the various pretrained
models. We hypothesize that this results in a richer feature representation and
shows results for the ASR downstream task. To this end, we use three SSL models
that have shown excellent results on ASR tasks, namely HuBERT, Wav2vec2.0, and
WaveLM. We explore the ensemble of models fine-tuned for the ASR task and the
ensemble of features using the embeddings obtained from the pre-trained models
for a downstream ASR task. We get improved performance over individual models
and pre-trained features using Librispeech(100h) and WSJ dataset for the
downstream tasks.",None,-1
Semantic-Oriented Unlabeled Priming for Large-Scale Language Models,0.495175,"Due to the high costs associated with finetuning large language models,
various recent works propose to adapt them to specific tasks without any
parameter updates through in-context learning. Unfortunately, for in-context
learning there is currently no way to leverage unlabeled data, which is often
much easier to obtain in large quantities than labeled examples. In this work,
we therefore investigate ways to make use of unlabeled examples to improve the
zero-shot performance of pretrained language models without any finetuning: We
introduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies
examples by retrieving semantically similar unlabeled examples, assigning
labels to them in a zero-shot fashion, and then using them for in-context
learning. We also propose bag-of-contexts priming, a new priming strategy that
is more suitable for our setting and enables the usage of more examples than
fit into the context window.",None,-1
Layer or Representation Space: What makes BERT-based Evaluation Metrics Robust?,0.356865,"The evaluation of recent embedding-based evaluation metrics for text
generation is primarily based on measuring their correlation with human
evaluations on standard benchmarks. However, these benchmarks are mostly from
similar domains to those used for pretraining word embeddings. This raises
concerns about the (lack of) generalization of embedding-based metrics to new
and noisy domains that contain a different vocabulary than the pretraining
data. In this paper, we examine the robustness of BERTScore, one of the most
popular embedding-based metrics for text generation. We show that (a) an
embedding-based metric that has the highest correlation with human evaluations
on a standard benchmark can have the lowest correlation if the amount of input
noise or unknown tokens increases, (b) taking embeddings from the first layer
of pretrained models improves the robustness of all metrics, and (c) the
highest robustness is achieved when using character-level embeddings, instead
of token-based embeddings, from the first layer of the pretrained model.",https://github.com/long21wt/,-1
"Two ways to make your robot proactive: reasoning about human intentions, or reasoning about possible futures",0.029901,"Robots sharing their space with humans need to be proactive in order to be
helpful. Proactive robots are able to act on their own initiative in an
anticipatory way to benefit humans. In this work, we investigate two ways to
make robots proactive. One way is to recognize humans' intentions and to act to
fulfill them, like opening the door that you are about to cross. The other way
is to reason about possible future threats or opportunities and to act to
prevent or to foster them, like recommending you to take an umbrella since rain
has been forecasted. In this paper, we present approaches to realize these two
types of proactive behavior. We then present an integrated system that can
generate proactive robot behavior by reasoning on both factors: intentions and
predictions. We illustrate our system on a sample use case including a domestic
robot and a human. We first run this use case with the two separate proactive
systems, intention-based and prediction-based, and then run it with our
integrated system. The results show that the integrated system is able to take
into account a broader variety of aspects that are needed for proactivity.",None,-1
Coordinating CAV Swarms at Intersections with a Deep Learning Model,0.256302,"Connected and automated vehicles (CAVs) are viewed as a special kind of
robots that have the potential to significantly improve the safety and
efficiency of traffic. In contrast to many swarm robotics studies that are
demonstrated in labs by employing a small number of robots, CAV studies aims to
achieve cooperative driving of unceasing robot swarm flows. However, how to get
the optimal passing order of such robot swarm flows even for a signal-free
intersection is an NP-hard problem (specifically, enumerating based algorithm
takes days to find the optimal solution to a 20-CAV scenario). Here, we
introduce a novel cooperative driving algorithm (AlphaOrder) that combines
offline deep learning and online tree searching to find a near-optimal passing
order in real-time. AlphaOrder builds a pointer network model from solved
scenarios and generates near-optimal passing orders instantaneously for new
scenarios. Furthermore, our approach provides a general approach to managing
preemptive resource sharing between swarm robotics (e.g., scheduling multiple
automated guided vehicles (AGVs) and unmanned aerial vehicles (UAVs) at
conflicting areas",None,-1
Model-based Multi-agent Reinforcement Learning: Recent Progress and Prospects,0.209551,"Significant advances have recently been achieved in Multi-Agent Reinforcement
Learning (MARL) which tackles sequential decision-making problems involving
multiple participants. However, MARL requires a tremendous number of samples
for effective training. On the other hand, model-based methods have been shown
to achieve provable advantages of sample efficiency. However, the attempts of
model-based methods to MARL have just started very recently. This paper
presents a review of the existing research on model-based MARL, including
theoretical analyses, algorithms, and applications, and analyzes the advantages
and potential of model-based MARL. Specifically, we provide a detailed taxonomy
of the algorithms and point out the pros and cons for each algorithm according
to the challenges inherent to multi-agent scenarios. We also outline promising
directions for future development of this field.",None,-1
Large Language Models with Controllable Working Memory,0.274222,"Large language models (LLMs) have led to a series of breakthroughs in natural
language processing (NLP), owing to their excellent understanding and
generation abilities. Remarkably, what further sets these models apart is the
massive amounts of world knowledge they internalize during pretraining. While
many downstream applications provide the model with an informational context to
aid its performance on the underlying task, how the model's world knowledge
interacts with the factual information presented in the context remains under
explored. As a desirable behavior, an LLM should give precedence to the context
whenever it contains task-relevant information that conflicts with the model's
memorized knowledge. This enables model predictions to be grounded in the
context, which can then be used to update or correct specific model predictions
without frequent retraining. By contrast, when the context is irrelevant to the
task, the model should ignore it and fall back on its internal knowledge. In
this paper, we undertake a first joint study of the aforementioned two
properties, namely controllability and robustness, in the context of LLMs. We
demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned)
could exhibit poor controllability and robustness, which do not scale with
increasing model size. As a solution, we propose a novel method - Knowledge
Aware FineTuning (KAFT) - to strengthen both controllability and robustness by
incorporating counterfactual and irrelevant contexts to standard supervised
datasets. Our comprehensive evaluation showcases the utility of KAFT across
model architectures and sizes.",None,-1
NewsStories: Illustrating articles with visual summaries,0.0723251,"Recent self-supervised approaches have used large-scale image-text datasets
to learn powerful representations that transfer to many tasks without
finetuning. These methods often assume that there is one-to-one correspondence
between its images and their (short) captions. However, many tasks require
reasoning about multiple images and long text narratives, such as describing
news articles with visual summaries. Thus, we explore a novel setting where the
goal is to learn a self-supervised visual-language representation that is
robust to varying text length and the number of images. In addition, unlike
prior work which assumed captions have a literal relation to the image, we
assume images only contain loose illustrative correspondence with the text. To
explore this problem, we introduce a large-scale multimodal dataset containing
over 31M articles, 22M images and 1M videos. We show that state-of-the-art
image-text alignment methods are not robust to longer narratives with multiple
images. Finally, we introduce an intuitive baseline that outperforms these
methods on zero-shot image-set retrieval by 10% on the GoodNews dataset.",None,-1
Multi-View Object Pose Refinement With Differentiable Renderer,0.309528,"This paper introduces a novel multi-view 6 DoF object pose refinement
approach focusing on improving methods trained on synthetic data. It is based
on the DPOD detector, which produces dense 2D-3D correspondences between the
model vertices and the image pixels in each frame. We have opted for the use of
multiple frames with known relative camera transformations, as it allows
introduction of geometrical constraints via an interpretable ICP-like loss
function. The loss function is implemented with a differentiable renderer and
is optimized iteratively. We also demonstrate that a full detection and
refinement pipeline, which is trained solely on synthetic data, can be used for
auto-labeling real data. We perform quantitative evaluation on LineMOD,
Occlusion, Homebrewed and YCB-V datasets and report excellent performance in
comparison to the state-of-the-art methods trained on the synthetic and real
data. We demonstrate empirically that our approach requires only a few frames
and is robust to close camera locations and noise in extrinsic camera
calibration, making its practical usage easier and more ubiquitous.",None,-1
HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition,0.409595,"Pre-training with self-supervised models, such as Hidden-unit BERT (HuBERT)
and wav2vec 2.0, has brought significant improvements in automatic speech
recognition (ASR). However, these models usually require an expensive
computational cost to achieve outstanding performance, slowing down the
inference speed. To improve the model efficiency, we propose an early exit
scheme for ASR, namely HuBERT-EE, that allows the model to stop the inference
dynamically. In HuBERT-EE, multiple early exit branches are added at the
intermediate layers, and each branch is used to decide whether a prediction can
be exited early. Experimental results on the LibriSpeech dataset show that
HuBERT-EE can accelerate the inference of a large-scale HuBERT model while
simultaneously balancing the trade-off between the word error rate (WER)
performance and the latency.",None,-1
Scalable Joint Learning of Wireless Multiple-Access Policies and their Signaling,0.0841056,"In this paper, we apply an multi-agent reinforcement learning (MARL)
framework allowing the base station (BS) and the user equipments (UEs) to
jointly learn a channel access policy and its signaling in a wireless multiple
access scenario. In this framework, the BS and UEs are reinforcement learning
(RL) agents that need to cooperate in order to deliver data. The comparison
with a contention-free and a contention-based baselines shows that our
framework achieves a superior performance in terms of goodput even in high
traffic situations while maintaining a low collision rate. The scalability of
the proposed method is studied, since it is a major problem in MARL and this
paper provides the first results in order to address it.",None,-1
Towards Privacy-Preserving Person Re-identification via Person Identify Shift,0.0236535,"Recently privacy concerns of person re-identification (ReID) raise more and
more attention and preserving the privacy of the pedestrian images used by ReID
methods become essential. De-identification (DeID) methods alleviate privacy
issues by removing the identity-related of the ReID data. However, most of the
existing DeID methods tend to remove all personal identity-related information
and compromise the usability of de-identified data on the ReID task. In this
paper, we aim to develop a technique that can achieve a good trade-off between
privacy protection and data usability for person ReID. To achieve this, we
propose a novel de-identification method designed explicitly for person ReID,
named Person Identify Shift (PIS). PIS removes the absolute identity in a
pedestrian image while preserving the identity relationship between image
pairs. By exploiting the interpolation property of variational auto-encoder,
PIS shifts each pedestrian image from the current identity to another with a
new identity, resulting in images still preserving the relative identities.
Experimental results show that our method has a better trade-off between
privacy-preserving and model performance than existing de-identification
methods and can defend against human and model attacks for data privacy.",None,-1
Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation,0.104874,"Neural machine translation (NMT) has become the de-facto standard in
real-world machine translation applications. However, NMT models can
unpredictably produce severely pathological translations, known as
hallucinations, that seriously undermine user trust. It becomes thus crucial to
implement effective preventive strategies to guarantee their proper
functioning. In this paper, we address the problem of hallucination detection
in NMT by following a simple intuition: as hallucinations are detached from the
source content, they exhibit encoder-decoder attention patterns that are
statistically different from those of good quality translations. We frame this
problem with an optimal transport formulation and propose a fully unsupervised,
plug-in detector that can be used with any attention-based NMT model.
Experimental results show that our detector not only outperforms all previous
model-based detectors, but is also competitive with detectors that employ large
models trained on millions of samples.",https://github.com/deep-spin/,-1
Neural Neighbor Style Transfer,0.844796,"We propose Neural Neighbor Style Transfer (NNST), a pipeline that offers
state-of-the-art quality, generalization, and competitive efficiency for
artistic style transfer. Our approach is based on explicitly replacing neural
features extracted from the content input (to be stylized) with those from a
style exemplar, then synthesizing the final output based on these rearranged
features. While the spirit of our approach is similar to prior work, we show
that our design decisions dramatically improve the final visual quality.",None,-1
Distance Matters in Human-Object Interaction Detection,0.108058,"Human-Object Interaction (HOI) detection has received considerable attention
in the context of scene understanding. Despite the growing progress on
benchmarks, we realize that existing methods often perform unsatisfactorily on
distant interactions, where the leading causes are two-fold: 1) Distant
interactions are by nature more difficult to recognize than close ones. A
natural scene often involves multiple humans and objects with intricate spatial
relations, making the interaction recognition for distant human-object largely
affected by complex visual context. 2) Insufficient number of distant
interactions in benchmark datasets results in under-fitting on these instances.
To address these problems, in this paper, we propose a novel two-stage method
for better handling distant interactions in HOI detection. One essential
component in our method is a novel Far Near Distance Attention module. It
enables information propagation between humans and objects, whereby the spatial
distance is skillfully taken into consideration. Besides, we devise a novel
Distance-Aware loss function which leads the model to focus more on distant yet
rare interactions. We conduct extensive experiments on two challenging datasets
- HICO-DET and V-COCO. The results demonstrate that the proposed method can
surpass existing approaches by a large margin, resulting in new
state-of-the-art performance.",None,-1
Iterative Next Boundary Detection for Instance Segmentation of Tree Rings in Microscopy Images of Shrub Cross Sections,0.0261334,"We address the problem of detecting tree rings in microscopy images of shrub
cross sections. This can be regarded as a special case of the instance
segmentation task with several unique challenges such as the concentric
circular ring shape of the objects and high precision requirements that result
in inadequate performance of existing methods. We propose a new iterative
method which we term Iterative Next Boundary Detection (INBD). It intuitively
models the natural growth direction, starting from the center of the shrub
cross section and detecting the next ring boundary in each iteration step. In
our experiments, INBD shows superior performance to generic instance
segmentation methods and is the only one with a built-in notion of
chronological order. Our dataset and source code are available at
http://github.com/alexander-g/INBD.",http://github.com/alexander-g/INBD,-1
An Ultra-low Power TinyML System for Real-time Visual Processing at Edge,0.609592,"Tiny machine learning (TinyML), executing AI workloads on resource and power
strictly restricted systems, is an important and challenging topic. This brief
firstly presents an extremely tiny backbone to construct high efficiency CNN
models for various visual tasks. Then, a specially designed neural co-processor
(NCP) is interconnected with MCU to build an ultra-low power TinyML system,
which stores all features and weights on chip and completely removes both of
latency and power consumption in off-chip memory access. Furthermore, an
application specific instruction-set is further presented for realizing agile
development and rapid deployment. Extensive experiments demonstrate that the
proposed TinyML system based on our model, NCP and instruction set yields
considerable accuracy and achieves a record ultra-low power of 160mW while
implementing object detection and recognition at 30FPS. The demo video is
available on \url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}.",None,-1
Fusing Local Similarities for Retrieval-based 3D Orientation Estimation of Unseen Objects,0.195651,"In this paper, we tackle the task of estimating the 3D orientation of
previously-unseen objects from monocular images. This task contrasts with the
one considered by most existing deep learning methods which typically assume
that the testing objects have been observed during training. To handle the
unseen objects, we follow a retrieval-based strategy and prevent the network
from learning object-specific features by computing multi-scale local
similarities between the query image and synthetically-generated reference
images. We then introduce an adaptive fusion module that robustly aggregates
the local similarities into a global similarity score of pairwise images.
Furthermore, we speed up the retrieval process by developing a fast retrieval
strategy. Our experiments on the LineMOD, LineMOD-Occluded, and T-LESS datasets
show that our method yields a significantly better generalization to unseen
objects than previous works. Our code and pre-trained models are available at
https://sailor-z.github.io/projects/Unseen_Object_Pose.html.",https://sailor-z.github.io/projects/Unseen_Object_Pose.html,-1
Instance-Specific Feature Propagation for Referring Segmentation,0.389129,"Referring segmentation aims to generate a segmentation mask for the target
instance indicated by a natural language expression. There are typically two
kinds of existing methods: one-stage methods that directly perform segmentation
on the fused vision and language features; and two-stage methods that first
utilize an instance segmentation model for instance proposal and then select
one of these instances via matching them with language features. In this work,
we propose a novel framework that simultaneously detects the target-of-interest
via feature propagation and generates a fine-grained segmentation mask. In our
framework, each instance is represented by an Instance-Specific Feature (ISF),
and the target-of-referring is identified by exchanging information among all
ISFs using our proposed Feature Propagation Module (FPM). Our instance-aware
approach learns the relationship among all objects, which helps to better
locate the target-of-interest than one-stage methods. Comparing to two-stage
methods, our approach collaboratively and interactively utilizes both vision
and language information for synchronous identification and segmentation. In
the experimental tests, our method outperforms previous state-of-the-art
methods on all three RefCOCO series datasets.",None,-1
Full-Text Argumentation Mining on Scientific Publications,0.0224286,"Scholarly Argumentation Mining (SAM) has recently gained attention due to its
potential to help scholars with the rapid growth of published scientific
literature. It comprises two subtasks: argumentative discourse unit recognition
(ADUR) and argumentative relation extraction (ARE), both of which are
challenging since they require e.g. the integration of domain knowledge, the
detection of implicit statements, and the disambiguation of argument structure.
While previous work focused on dataset construction and baseline methods for
specific document sections, such as abstract or results, full-text scholarly
argumentation mining has seen little progress. In this work, we introduce a
sequential pipeline model combining ADUR and ARE for full-text SAM, and provide
a first analysis of the performance of pretrained language models (PLMs) on
both subtasks. We establish a new SotA for ADUR on the Sci-Arg corpus,
outperforming the previous best reported result by a large margin (+7% F1). We
also present the first results for ARE, and thus for the full AM pipeline, on
this benchmark dataset. Our detailed error analysis reveals that non-contiguous
ADUs as well as the interpretation of discourse connectors pose major
challenges and that data annotation needs to be more consistent.",https://github.com/DFKI-NLP/sam,-1
Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation,0.556403,"The performance of multilingual pretrained models is highly dependent on the
availability of monolingual or parallel text present in a target language.
Thus, the majority of the world's languages cannot benefit from recent progress
in NLP as they have no or limited textual data. To expand possibilities of
using NLP technology in these under-represented languages, we systematically
study strategies that relax the reliance on conventional language resources
through the use of bilingual lexicons, an alternative resource with much better
language coverage. We analyze different strategies to synthesize textual or
labeled data using lexicons, and how this data can be combined with monolingual
or parallel text when available. For 19 under-represented languages across 3
tasks, our methods lead to consistent improvements of up to 5 and 15 points
with and without extra monolingual text respectively. Overall, our study
highlights how NLP methods can be adapted to thousands more languages that are
under-served by current technology",https://github.com/cindyxinyiwang/expand-via-lexicon-based-adaptation,-1
ExAIS: Executable AI Semantics,0.18404,"Neural networks can be regarded as a new programming paradigm, i.e., instead
of building ever-more complex programs through (often informal) logical
reasoning in the programmers' mind, complex 'AI' systems are built by
optimising generic neural network models with big data. In this new paradigm,
AI frameworks such as TensorFlow and PyTorch play a key role, which is as
essential as the compiler for traditional programs. It is known that the lack
of a proper semantics for programming languages (such as C), i.e., a
correctness specification for compilers, has contributed to many problematic
program behaviours and security issues. While it is in general hard to have a
correctness specification for compilers due to the high complexity of
programming languages and their rapid evolution, we have a unique opportunity
to do it right this time for neural networks (which have a limited set of
functions, and most of them have stable semantics). In this work, we report our
effort on providing a correctness specification of neural network frameworks
such as TensorFlow. We specify the semantics of almost all TensorFlow layers in
the logical programming language Prolog. We demonstrate the usefulness of the
semantics through two applications. One is a fuzzing engine for TensorFlow,
which features a strong oracle and a systematic way of generating valid neural
networks. The other is a model validation approach which enables consistent bug
reporting for TensorFlow models.",None,-1
Benchmarking Long-tail Generalization with Likelihood Splits,0.0099038,"In order to reliably process natural language, NLP systems must generalize to
the long tail of rare utterances. We propose a method to create challenging
benchmarks that require generalizing to the tail of the distribution by
re-splitting existing datasets. We create 'Likelihood Splits' where examples
that are assigned lower likelihood by a pre-trained language model (LM) are
placed in the test set, and more likely examples are in the training set. This
simple approach can be customized to construct meaningful train-test splits for
a wide range of tasks. Likelihood Splits surface more challenges than random
splits: relative error rates of state-of-the-art models increase by 59% for
semantic parsing on Spider, 93% for natural language inference on SNLI, and 33%
for yes/no question answering on BoolQ, on our splits compared with the
corresponding random splits. Moreover, Likelihood Splits create fairer
benchmarks than adversarial filtering; when the LM used to create the splits is
also employed as the task model, our splits do not unfairly penalize the LM.",https://github.com/ameyagodbole/long-tail-likelihood-splits,-1
Cross-Domain Aspect Extraction using Transformers Augmented with Knowledge Graphs,0.0197215,"The extraction of aspect terms is a critical step in fine-grained sentiment
analysis of text. Existing approaches for this task have yielded impressive
results when the training and testing data are from the same domain. However,
these methods show a drastic decrease in performance when applied to
cross-domain settings where the domain of the testing data differs from that of
the training data. To address this lack of extensibility and robustness, we
propose a novel approach for automatically constructing domain-specific
knowledge graphs that contain information relevant to the identification of
aspect terms. We introduce a methodology for injecting information from these
knowledge graphs into Transformer models, including two alternative mechanisms
for knowledge insertion: via query enrichment and via manipulation of attention
patterns. We demonstrate state-of-the-art performance on benchmark datasets for
cross-domain aspect term extraction using our approach and investigate how the
amount of external knowledge available to the Transformer impacts model
performance.",None,-1
Covariance Matrix Adaptation MAP-Annealing,0.353541,"Single-objective optimization algorithms search for the single
highest-quality solution with respect to an objective. Quality diversity (QD)
optimization algorithms, such as Covariance Matrix Adaptation MAP-Elites
(CMA-ME), search for a collection of solutions that are both high-quality with
respect to an objective and diverse with respect to specified measure
functions. However, CMA-ME suffers from three major limitations highlighted by
the QD community: prematurely abandoning the objective in favor of exploration,
struggling to explore flat objectives, and having poor performance for
low-resolution archives. We propose a new quality diversity algorithm,
Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), that addresses all three
limitations. We provide theoretical justifications for the new algorithm with
respect to each limitation. Our theory informs our experiments, which support
the theory and show that CMA-MAE achieves state-of-the-art performance and
robustness.",https://github.com/icaros-usc/pyribs,-1
GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost,0.208634,"Large pre-trained models have revolutionized natural language processing
(NLP) research and applications, but high training costs and limited data
resources have prevented their benefits from being shared equally amongst
speakers of all the world's languages. To address issues of cross-linguistic
access to such models and reduce energy consumption for sustainability during
large-scale model training, this study proposes an effective and
energy-efficient framework called GreenPLM that uses bilingual lexicons to
directly ""translate"" pre-trained language models of one language into another
at almost no additional cost. We validate this approach in 18 languages' BERT
models and show that this framework is comparable to, if not better than, other
heuristics with high training costs. In addition, given lightweight continued
pre-training on limited data where available, this framework outperforms the
original monolingual language models in six out of seven tested languages with
up to 200x less pre-training efforts. Aiming at the Leave No One Behind
Principle (LNOB), our approach manages to reduce inequalities between languages
and energy consumption greatly. We make our codes and models publicly available
here: \url{https://github.com/qcznlp/GreenPLMs}",https://github.com/qcznlp/GreenPLMs,-1
MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing,0.728664,"Snow removal causes challenges due to its characteristic of complex
degradations. To this end, targeted treatment of multi-scale snow degradations
is critical for the network to learn effective snow removal. In order to handle
the diverse scenes, we propose a multi-scale projection transformer
(MSP-Former), which understands and covers a variety of snow degradation
features in a multi-path manner, and integrates comprehensive scene context
information for clean reconstruction via self-attention operation. For the
local details of various snow degradations, the local capture module is
introduced in parallel to assist in the rebuilding of a clean image. Such
design achieves the SOTA performance on three desnowing benchmark datasets
while costing the low parameters and computational complexity, providing a
guarantee of practicality.",None,-1
Heatmap Distribution Matching for Human Pose Estimation,0.0382242,"For tackling the task of 2D human pose estimation, the great majority of the
recent methods regard this task as a heatmap estimation problem, and optimize
the heatmap prediction using the Gaussian-smoothed heatmap as the optimization
objective and using the pixel-wise loss (e.g. MSE) as the loss function. In
this paper, we show that optimizing the heatmap prediction in such a way, the
model performance of body joint localization, which is the intrinsic objective
of this task, may not be consistently improved during the optimization process
of the heatmap prediction. To address this problem, from a novel perspective,
we propose to formulate the optimization of the heatmap prediction as a
distribution matching problem between the predicted heatmap and the dot
annotation of the body joint directly. By doing so, our proposed method does
not need to construct the Gaussian-smoothed heatmap and can achieve a more
consistent model performance improvement during the optimization of the heatmap
prediction. We show the effectiveness of our proposed method through extensive
experiments on the COCO dataset and the MPII dataset.",None,-1
Neutral Utterances are Also Causes: Enhancing Conversational Causal Emotion Entailment with Social Commonsense Knowledge,0.605682,"Conversational Causal Emotion Entailment aims to detect causal utterances for
a non-neutral targeted utterance from a conversation. In this work, we build
conversations as graphs to overcome implicit contextual modelling of the
original entailment style. Following the previous work, we further introduce
the emotion information into graphs. Emotion information can markedly promote
the detection of causal utterances whose emotion is the same as the targeted
utterance. However, it is still hard to detect causal utterances with different
emotions, especially neutral ones. The reason is that models are limited in
reasoning causal clues and passing them between utterances. To alleviate this
problem, we introduce social commonsense knowledge (CSK) and propose a
Knowledge Enhanced Conversation graph (KEC). KEC propagates the CSK between two
utterances. As not all CSK is emotionally suitable for utterances, we therefore
propose a sentiment-realized knowledge selecting strategy to filter CSK. To
process KEC, we further construct the Knowledge Enhanced Directed Acyclic Graph
networks. Experimental results show that our method outperforms baselines and
infers more causes with different emotions from the targeted utterance.",https://github.com/LeqsNaN/KEC,-1
WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection,0.148902,"Monocular 3D object detection is one of the most challenging tasks in 3D
scene understanding. Due to the ill-posed nature of monocular imagery, existing
monocular 3D detection methods highly rely on training with the manually
annotated 3D box labels on the LiDAR point clouds. This annotation process is
very laborious and expensive. To dispense with the reliance on 3D box labels,
in this paper we explore the weakly supervised monocular 3D detection.
Specifically, we first detect 2D boxes on the image. Then, we adopt the
generated 2D boxes to select corresponding RoI LiDAR points as the weak
supervision. Eventually, we adopt a network to predict 3D boxes which can
tightly align with associated RoI LiDAR points. This network is learned by
minimizing our newly-proposed 3D alignment loss between the 3D box estimates
and the corresponding RoI LiDAR points. We will illustrate the potential
challenges of the above learning problem and resolve these challenges by
introducing several effective designs into our method. Codes will be available
at https://github.com/SPengLiang/WeakM3D.",https://github.com/SPengLiang/WeakM3D,-1
CRUSH: Contextually Regularized and User anchored Self-supervised Hate speech Detection,0.147823,"The last decade has witnessed a surge in the interaction of people through
social networking platforms. While there are several positive aspects of these
social platforms, the proliferation has led them to become the breeding ground
for cyber-bullying and hate speech. Recent advances in NLP have often been used
to mitigate the spread of such hateful content. Since the task of hate speech
detection is usually applicable in the context of social networks, we introduce
CRUSH, a framework for hate speech detection using user-anchored
self-supervision and contextual regularization. Our proposed approach secures ~
1-12% improvement in test set metrics over best performing previous approaches
on two types of tasks and multiple popular english social media datasets.",https://github.com/parag1604/CRUSH,-1
PHEMEPlus: Enriching Social Media Rumour Verification with External Evidence,0.0808957,"Work on social media rumour verification utilises signals from posts, their
propagation and users involved. Other lines of work target identifying and
fact-checking claims based on information from Wikipedia, or trustworthy news
articles without considering social media context. However works combining the
information from social media with external evidence from the wider web are
lacking. To facilitate research in this direction, we release a novel dataset,
PHEMEPlus, an extension of the PHEME benchmark, which contains social media
conversations as well as relevant external evidence for each rumour. We
demonstrate the effectiveness of incorporating such evidence in improving
rumour verification models. Additionally, as part of the evidence collection,
we evaluate various ways of query formulation to identify the most effective
method.",https://github.com/JohnNLP/PhemePlus,-1
Building Open Knowledge Graph for Metal-Organic Frameworks (MOF-KG): Challenges and Case Studies,0.0131912,"Metal-Organic Frameworks (MOFs) are a class of modular, porous crystalline
materials that have great potential to revolutionize applications such as gas
storage, molecular separations, chemical sensing, catalysis, and drug delivery.
The Cambridge Structural Database (CSD) reports 10,636 synthesized MOF crystals
which in addition contains ca. 114,373 MOF-like structures. The sheer number of
synthesized (plus potentially synthesizable) MOF structures requires
researchers pursue computational techniques to screen and isolate MOF
candidates. In this demo paper, we describe our effort on leveraging knowledge
graph methods to facilitate MOF prediction, discovery, and synthesis. We
present challenges and case studies about (1) construction of a MOF knowledge
graph (MOF-KG) from structured and unstructured sources and (2) leveraging the
MOF-KG for discovery of new or missing knowledge.",None,-1
A unified 3D framework for Organs at Risk Localization and Segmentation for Radiation Therapy Planning,0.164469,"Automatic localization and segmentation of organs-at-risk (OAR) in CT are
essential pre-processing steps in medical image analysis tasks, such as
radiation therapy planning. For instance, the segmentation of OAR surrounding
tumors enables the maximization of radiation to the tumor area without
compromising the healthy tissues. However, the current medical workflow
requires manual delineation of OAR, which is prone to errors and is
annotator-dependent. In this work, we aim to introduce a unified 3D pipeline
for OAR localization-segmentation rather than novel localization or
segmentation architectures. To the best of our knowledge, our proposed
framework fully enables the exploitation of 3D context information inherent in
medical imaging. In the first step, a 3D multi-variate regression network
predicts organs' centroids and bounding boxes. Secondly, 3D organ-specific
segmentation networks are leveraged to generate a multi-organ segmentation map.
Our method achieved an overall Dice score of $0.9260\pm 0.18 \%$ on the
VISCERAL dataset containing CT scans with varying fields of view and multiple
organs.",None,-1
Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference,0.0217908,"It has been shown that NLI models are usually biased with respect to the
word-overlap between premise and hypothesis; they take this feature as a
primary cue for predicting the entailment label. In this paper, we focus on an
overlooked aspect of the overlap bias in NLI models: the reverse word-overlap
bias. Our experimental results demonstrate that current NLI models are highly
biased towards the non-entailment label on instances with low overlap, and the
existing debiasing methods, which are reportedly successful on existing
challenge datasets, are generally ineffective in addressing this category of
bias. We investigate the reasons for the emergence of the overlap bias and the
role of minority examples in its mitigation. For the former, we find that the
word-overlap bias does not stem from pre-training, and for the latter, we
observe that in contrast to the accepted assumption, eliminating minority
examples does not affect the generalizability of debiasing methods with respect
to the overlap bias.",https://github.com/sara-rajaee/reverse_bias,-1
Jitter Does Matter: Adapting Gaze Estimation to New Domains,0.0261829,"Deep neural networks have demonstrated superior performance on
appearance-based gaze estimation tasks. However, due to variations in person,
illuminations, and background, performance degrades dramatically when applying
the model to a new domain. In this paper, we discover an interesting gaze
jitter phenomenon in cross-domain gaze estimation, i.e., the gaze predictions
of two similar images can be severely deviated in target domain. This is
closely related to cross-domain gaze estimation tasks, but surprisingly, it has
not been noticed yet previously. Therefore, we innovatively propose to utilize
the gaze jitter to analyze and optimize the gaze domain adaptation task. We
find that the high-frequency component (HFC) is an important factor that leads
to jitter. Based on this discovery, we add high-frequency components to input
images using the adversarial attack and employ contrastive learning to
encourage the model to obtain similar representations between original and
perturbed data, which reduces the impacts of HFC. We evaluate the proposed
method on four cross-domain gaze estimation tasks, and experimental results
demonstrate that it significantly reduces the gaze jitter and improves the gaze
estimation performance in target domains.",None,-1
CL3D: Unsupervised Domain Adaptation for Cross-LiDAR 3D Detection,0.0675324,"Domain adaptation for Cross-LiDAR 3D detection is challenging due to the
large gap on the raw data representation with disparate point densities and
point arrangements. By exploring domain-invariant 3D geometric characteristics
and motion patterns, we present an unsupervised domain adaptation method that
overcomes above difficulties. First, we propose the Spatial Geometry Alignment
module to extract similar 3D shape geometric features of the same object class
to align two domains, while eliminating the effect of distinct point
distributions. Second, we present Temporal Motion Alignment module to utilize
motion features in sequential frames of data to match two domains. Prototypes
generated from two modules are incorporated into the pseudo-label reweighting
procedure and contribute to our effective self-training framework for the
target domain. Extensive experiments show that our method achieves
state-of-the-art performance on cross-device datasets, especially for the
datasets with large gaps captured by mechanical scanning LiDARs and solid-state
LiDARs in various scenes. Project homepage is at
https://github.com/4DVLab/CL3D.git",https://github.com/4DVLab/CL3D.git,-1
"A Distant Supervision Corpus for Extracting Biomedical Relationships Between Chemicals, Diseases and Genes",0.130882,"We introduce ChemDisGene, a new dataset for training and evaluating
multi-class multi-label document-level biomedical relation extraction models.
Our dataset contains 80k biomedical research abstracts labeled with mentions of
chemicals, diseases, and genes, portions of which human experts labeled with 18
types of biomedical relationships between these entities (intended for
evaluation), and the remainder of which (intended for training) has been
distantly labeled via the CTD database with approximately 78\% accuracy. In
comparison to similar preexisting datasets, ours is both substantially larger
and cleaner; it also includes annotations linking mentions to their entities.
We also provide three baseline deep neural network relation extraction models
trained and evaluated on our new dataset.",https://github.com/chanzuckerberg/ChemDisGene,-1
Unbiased Knowledge Distillation for Recommendation,0.0803389,"As a promising solution for model compression, knowledge distillation (KD)
has been applied in recommender systems (RS) to reduce inference latency.
Traditional solutions first train a full teacher model from the training data,
and then transfer its knowledge (\ie \textit{soft labels}) to supervise the
learning of a compact student model. However, we find such a standard
distillation paradigm would incur serious bias issue -- popular items are more
heavily recommended after the distillation. This effect prevents the student
model from making accurate and fair recommendations, decreasing the
effectiveness of RS.
  In this work, we identify the origin of the bias in KD -- it roots in the
biased soft labels from the teacher, and is further propagated and intensified
during the distillation. To rectify this, we propose a new KD method with a
stratified distillation strategy. It first partitions items into multiple
groups according to their popularity, and then extracts the ranking knowledge
within each group to supervise the learning of the student. Our method is
simple and teacher-agnostic -- it works on distillation stage without affecting
the training of the teacher model. We conduct extensive theoretical and
empirical studies to validate the effectiveness of our proposal. We release our
code at: https://github.com/chengang95/UnKD.",None,-1
Receding Horizon Inverse Reinforcement Learning,0.375268,"Inverse reinforcement learning (IRL) seeks to infer a cost function that
explains the underlying goals and preferences of expert demonstrations. This
paper presents receding horizon inverse reinforcement learning (RHIRL), a new
IRL algorithm for high-dimensional, noisy, continuous systems with black-box
dynamic models. RHIRL addresses two key challenges of IRL: scalability and
robustness. To handle high-dimensional continuous systems, RHIRL matches the
induced optimal trajectories with expert demonstrations locally in a receding
horizon manner and 'stitches' together the local solutions to learn the cost;
it thereby avoids the 'curse of dimensionality'. This contrasts sharply with
earlier algorithms that match with expert demonstrations globally over the
entire high-dimensional state space. To be robust against imperfect expert
demonstrations and control noise, RHIRL learns a state-dependent cost function
'disentangled' from system dynamics under mild conditions. Experiments on
benchmark tasks show that RHIRL outperforms several leading IRL algorithms in
most instances. We also prove that the cumulative error of RHIRL grows linearly
with the task duration.",None,-1
Interpretable Multimodal Emotion Recognition using Hybrid Fusion of Speech and Image Data,0.0,"This paper proposes a multimodal emotion recognition system based on hybrid
fusion that classifies the emotions depicted by speech utterances and
corresponding images into discrete classes. A new interpretability technique
has been developed to identify the important speech & image features leading to
the prediction of particular emotion classes. The proposed system's
architecture has been determined through intensive ablation studies. It fuses
the speech & image features and then combines speech, image, and intermediate
fusion outputs. The proposed interpretability technique incorporates the divide
& conquer approach to compute shapely values denoting each speech & image
feature's importance. We have also constructed a large-scale dataset (IIT-R
SIER dataset), consisting of speech utterances, corresponding images, and class
labels, i.e., 'anger,' 'happy,' 'hate,' and 'sad.' The proposed system has
achieved 83.29% accuracy for emotion recognition. The enhanced performance of
the proposed system advocates the importance of utilizing complementary
information from multiple modalities for emotion recognition.",https://github.com/MIntelligence-Group/SpeechImg EmoRec,-1
One Agent To Rule Them All: Towards Multi-agent Conversational AI,0.183114,"The increasing volume of commercially available conversational agents (CAs)
on the market has resulted in users being burdened with learning and adopting
multiple agents to accomplish their tasks. Though prior work has explored
supporting a multitude of domains within the design of a single agent, the
interaction experience suffers due to the large action space of desired
capabilities. To address these problems, we introduce a new task BBAI:
Black-Box Agent Integration, focusing on combining the capabilities of multiple
black-box CAs at scale. We explore two techniques: question agent pairing and
question response pairing aimed at resolving this task. Leveraging these
techniques, we design One For All (OFA), a scalable system that provides a
unified interface to interact with multiple CAs. Additionally, we introduce
MARS: Multi-Agent Response Selection, a new encoder model for question response
pairing that jointly encodes user question and agent response pairs. We
demonstrate that OFA is able to automatically and accurately integrate an
ensemble of commercially available CAs spanning disparate domains.
Specifically, using the MARS encoder we achieve the highest accuracy on our
BBAI task, outperforming strong baselines.",https://github.com/ChrisIsKing/black-box-multi-agent-integation,-1
FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,0.277523,"Transformers have been shown to be able to perform deductive reasoning on a
logical rulebase containing rules and statements written in natural language.
Recent works show that such models can also produce the reasoning steps (i.e.,
the proof graph) that emulate the model's logical reasoning process. Currently,
these black-box models generate both the proof graph and intermediate
inferences within the same model and thus may be unfaithful. In this work, we
frame the deductive logical reasoning task by defining three modular
components: rule selection, fact selection, and knowledge composition. The rule
and fact selection steps select the candidate rule and facts to be used and
then the knowledge composition combines them to generate new inferences. This
ensures model faithfulness by assured causal relation from the proof step to
the inference reasoning. To test our framework, we propose FaiRR (Faithful and
Robust Reasoner) where the above three components are independently modeled by
transformers. We observe that FaiRR is robust to novel language perturbations,
and is faster at inference than previous works on existing reasoning datasets.
Additionally, in contrast to black-box generative models, the errors made by
FaiRR are more interpretable due to the modular approach.",https://github.com/INK-USC/FaiRR,-1
Watermarking Pre-trained Language Models with Backdooring,0.127135,"Large pre-trained language models (PLMs) have proven to be a crucial
component of modern natural language processing systems. PLMs typically need to
be fine-tuned on task-specific downstream datasets, which makes it hard to
claim the ownership of PLMs and protect the developer's intellectual property
due to the catastrophic forgetting phenomenon. We show that PLMs can be
watermarked with a multi-task learning framework by embedding backdoors
triggered by specific inputs defined by the owners, and those watermarks are
hard to remove even though the watermarked PLMs are fine-tuned on multiple
downstream tasks. In addition to using some rare words as triggers, we also
show that the combination of common words can be used as backdoor triggers to
avoid them being easily detected. Extensive experiments on multiple datasets
demonstrate that the embedded watermarks can be robustly extracted with a high
success rate and less influenced by the follow-up fine-tuning.",None,-1
Detecting Deepfake by Creating Spatio-Temporal Regularity Disruption,0.324802,"Despite encouraging progress in deepfake detection, generalization to unseen
forgery types remains a significant challenge due to the limited forgery clues
explored during training. In contrast, we notice a common phenomenon in
deepfake: fake video creation inevitably disrupts the statistical regularity in
original videos. Inspired by this observation, we propose to boost the
generalization of deepfake detection by distinguishing the ""regularity
disruption"" that does not appear in real videos. Specifically, by carefully
examining the spatial and temporal properties, we propose to disrupt a real
video through a Pseudo-fake Generator and create a wide range of pseudo-fake
videos for training. Such practice allows us to achieve deepfake detection
without using fake videos and improves the generalization ability in a simple
and efficient manner. To jointly capture the spatial and temporal disruptions,
we propose a Spatio-Temporal Enhancement block to learn the regularity
disruption across space and time on our self-created videos. Through
comprehensive experiments, our method exhibits excellent performance on several
datasets.",https://github.com/MarekKowalski/FaceSwap,-1
Neural Contourlet Network for Monocular 360 Depth Estimation,0.0071068,"For a monocular 360 image, depth estimation is a challenging because the
distortion increases along the latitude. To perceive the distortion, existing
methods devote to designing a deep and complex network architecture. In this
paper, we provide a new perspective that constructs an interpretable and sparse
representation for a 360 image. Considering the importance of the geometric
structure in depth estimation, we utilize the contourlet transform to capture
an explicit geometric cue in the spectral domain and integrate it with an
implicit cue in the spatial domain. Specifically, we propose a neural
contourlet network consisting of a convolutional neural network and a
contourlet transform branch. In the encoder stage, we design a spatial-spectral
fusion module to effectively fuse two types of cues. Contrary to the encoder,
we employ the inverse contourlet transform with learned low-pass subbands and
band-pass directional subbands to compose the depth in the decoder. Experiments
on the three popular panoramic image datasets demonstrate that the proposed
approach outperforms the state-of-the-art schemes with faster convergence. Code
is available at
https://github.com/zhijieshen-bjtu/Neural-Contourlet-Network-for-MODE.",None,-1
MMKGR: Multi-hop Multi-modal Knowledge Graph Reasoning,0.33275,"Multi-modal knowledge graphs (MKGs) include not only the relation triplets,
but also related multi-modal auxiliary data (i.e., texts and images), which
enhance the diversity of knowledge. However, the natural incompleteness has
significantly hindered the applications of MKGs. To tackle the problem,
existing studies employ the embedding-based reasoning models to infer the
missing knowledge after fusing the multi-modal features. However, the reasoning
performance of these methods is limited due to the following problems: (1)
ineffective fusion of multi-modal auxiliary features; (2) lack of complex
reasoning ability as well as inability to conduct the multi-hop reasoning which
is able to infer more missing knowledge. To overcome these problems, we propose
a novel model entitled MMKGR (Multi-hop Multi-modal Knowledge Graph Reasoning).
Specifically, the model contains the following two components: (1) a unified
gate-attention network which is designed to generate effective multi-modal
complementary features through sufficient attention interaction and noise
reduction; (2) a complementary feature-aware reinforcement learning method
which is proposed to predict missing elements by performing the multi-hop
reasoning process, based on the features obtained in component (1). The
experimental results demonstrate that MMKGR outperforms the state-of-the-art
approaches in the MKG reasoning task.",None,-1
Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation,0.352096,"We present Referee, a novel framework for sentence summarization that can be
trained reference-free (i.e., requiring no gold summaries for supervision),
while allowing direct control for compression ratio. Our work is the first to
demonstrate that reference-free, controlled sentence summarization is feasible
via the conceptual framework of Symbolic Knowledge Distillation (West et al.,
2022), where latent knowledge in pre-trained language models is distilled via
explicit examples sampled from the teacher models, further purified with three
types of filters: length, fidelity, and Information Bottleneck. Moreover, we
uniquely propose iterative distillation of knowledge, where student models from
the previous iteration of distillation serve as teacher models in the next
iteration. Starting off from a relatively modest set of GPT3-generated
summaries, we demonstrate how iterative knowledge distillation can lead to
considerably smaller, but better summarizers with sharper controllability. A
useful by-product of this iterative distillation process is a high-quality
dataset of sentence-summary pairs with varying degrees of compression ratios.
Empirical results demonstrate that the final student models vastly outperform
the much larger GPT3-Instruct model in terms of the controllability of
compression ratios, without compromising the quality of resulting
summarization.",https://github.com/msclar/referee,-1
Online Learning of Reusable Abstract Models for Object Goal Navigation,0.0201566,"In this paper, we present a novel approach to incrementally learn an Abstract
Model of an unknown environment, and show how an agent can reuse the learned
model for tackling the Object Goal Navigation task. The Abstract Model is a
finite state machine in which each state is an abstraction of a state of the
environment, as perceived by the agent in a certain position and orientation.
The perceptions are high-dimensional sensory data (e.g., RGB-D images), and the
abstraction is reached by exploiting image segmentation and the Taskonomy model
bank. The learning of the Abstract Model is accomplished by executing actions,
observing the reached state, and updating the Abstract Model with the acquired
information. The learned models are memorized by the agent, and they are reused
whenever it recognizes to be in an environment that corresponds to the stored
model. We investigate the effectiveness of the proposed approach for the Object
Goal Navigation task, relying on public benchmarks. Our results show that the
reuse of learned Abstract Models can boost performance on Object Goal
Navigation.",None,-1
Understanding and Improving Knowledge Distillation for Quantization-Aware Training of Large Transformer Encoders,0.0759059,"Knowledge distillation (KD) has been a ubiquitous method for model
compression to strengthen the capability of a lightweight model with the
transferred knowledge from the teacher. In particular, KD has been employed in
quantization-aware training (QAT) of Transformer encoders like BERT to improve
the accuracy of the student model with the reduced-precision weight parameters.
However, little is understood about which of the various KD approaches best
fits the QAT of Transformers. In this work, we provide an in-depth analysis of
the mechanism of KD on attention recovery of quantized large Transformers. In
particular, we reveal that the previously adopted MSE loss on the attention
score is insufficient for recovering the self-attention information. Therefore,
we propose two KD methods; attention-map and attention-output losses.
Furthermore, we explore the unification of both losses to address
task-dependent preference between attention-map and output losses. The
experimental results on various Transformer encoder models demonstrate that the
proposed KD methods achieve state-of-the-art accuracy for QAT with sub-2-bit
weight quantization.",https://github.com/MarsJacobs/kd-qat-large-enc,-1
A2: Efficient Automated Attacker for Boosting Adversarial Training,0.0870884,"Based on the significant improvement of model robustness by AT (Adversarial
Training), various variants have been proposed to further boost the
performance. Well-recognized methods have focused on different components of AT
(e.g., designing loss functions and leveraging additional unlabeled data). It
is generally accepted that stronger perturbations yield more robust models.
However, how to generate stronger perturbations efficiently is still missed. In
this paper, we propose an efficient automated attacker called A2 to boost AT by
generating the optimal perturbations on-the-fly during training. A2 is a
parameterized automated attacker to search in the attacker space for the best
attacker against the defense model and examples. Extensive experiments across
different datasets demonstrate that A2 generates stronger perturbations with
low extra cost and reliably improves the robustness of various AT methods
against different attacks.",https://github.com/huyvnphan/PyTorch_CIFAR10,-1
Proportional Fairness in Federated Learning,0.139622,"With the increasingly broad deployment of federated learning (FL) systems in
the real world, it is critical but challenging to ensure fairness in FL, i.e.
reasonably satisfactory performances for each of the numerous diverse clients.
In this work, we introduce and study a new fairness notion in FL, called
proportional fairness (PF), which is based on the relative change of each
client's performance. From its connection with the bargaining games, we propose
PropFair, a novel and easy-to-implement algorithm for finding proportionally
fair solutions in FL and study its convergence properties. Through extensive
experiments on vision and language datasets, we demonstrate that PropFair can
approximately find PF solutions, and it achieves a good balance between the
average performances of all clients and of the worst 10% clients. Our code is
available at
\url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL}.",https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL,-1
HSGNet: Object Re-identification with Hierarchical Similarity Graph Network,0.0603983,"Object re-identification method is made up of backbone network, feature
aggregation, and loss function. However, most backbone networks lack a special
mechanism to handle rich scale variations and mine discriminative feature
representations. In this paper, we firstly design a hierarchical similarity
graph module (HSGM) to reduce the conflict of backbone and re-identification
networks. The designed HSGM builds a rich hierarchical graph to mine the
mapping relationships between global-local and local-local. Secondly, we divide
the feature map along with the spatial and channel directions in each
hierarchical graph. The HSGM applies the spatial features and channel features
extracted from different locations as nodes, respectively, and utilizes the
similarity scores between nodes to construct spatial and channel similarity
graphs. During the learning process of HSGM, we utilize a learnable parameter
to re-optimize the importance of each position, as well as evaluate the
correlation between different nodes. Thirdly, we develop a novel hierarchical
similarity graph network (HSGNet) by embedding the HSGM in the backbone
network. Furthermore, HSGM can be easily embedded into backbone networks of any
depth to improve object re-identification ability. Finally, extensive
experiments on three large-scale object datasets demonstrate that the proposed
HSGNet is superior to state-of-the-art object re-identification approaches.",None,-1
Evidential Temporal-aware Graph-based Social Event Detection via Dempster-Shafer Theory,0.312603,"The rising popularity of online social network services has attracted lots of
research on mining social media data, especially on mining social events.
Social event detection, due to its wide applications, has now become a trivial
task. State-of-the-art approaches exploiting Graph Neural Networks (GNNs)
usually follow a two-step strategy: 1) constructing text graphs based on
various views (\textit{co-user}, \textit{co-entities} and
\textit{co-hashtags}); and 2) learning a unified text representation by a
specific GNN model. Generally, the results heavily rely on the quality of the
constructed graphs and the specific message passing scheme. However, existing
methods have deficiencies in both aspects: 1) They fail to recognize the noisy
information induced by unreliable views. 2) Temporal information which works as
a vital indicator of events is neglected in most works. To this end, we propose
ETGNN, a novel Evidential Temporal-aware Graph Neural Network. Specifically, we
construct view-specific graphs whose nodes are the texts and edges are
determined by several types of shared elements respectively. To incorporate
temporal information into the message passing scheme, we introduce a novel
temporal-aware aggregator which assigns weights to neighbours according to an
adaptive time exponential decay formula. Considering the view-specific
uncertainty, the representations of all views are converted into mass functions
through evidential deep learning (EDL) neural networks, and further combined
via Dempster-Shafer theory (DST) to make the final detection. Experimental
results on three real-world datasets demonstrate the effectiveness of ETGNN in
accuracy, reliability and robustness in social event detection.",None,-1
Detecting Emerging Technologies and their Evolution using Deep Learning and Weak Signal Analysis,0.05385,"Emerging technologies can have major economic impacts and affect strategic
stability. Yet, early identification of emerging technologies remains
challenging. In order to identify emerging technologies in a timely and
reliable manner, a comprehensive examination of relevant scientific and
technological (S&T) trends and their related references is required. This
examination is generally done by domain experts and requires significant
amounts of time and effort to gain insights. The use of domain experts to
identify emerging technologies from S&T trends may limit the capacity to
analyse large volumes of information and introduce subjectivity in the
assessments. Decision support systems are required to provide accurate and
reliable evidence-based indicators through constant and continuous monitoring
of the environment and help identify signals of emerging technologies that
could alter security and economic prosperity. For example, the research field
of hypersonics has recently witnessed several advancements having profound
technological, commercial, and national security implications. In this work, we
present a multi-layer quantitative approach able to identify future signs from
scientific publications on hypersonics by leveraging deep learning and weak
signal analysis. The proposed framework can help strategic planners and domain
experts better identify and monitor emerging technology trends.",None,-1
Mitigating Artifacts in Real-World Video Super-Resolution Models,0.436746,"The recurrent structure is a prevalent framework for the task of video
super-resolution, which models the temporal dependency between frames via
hidden states. When applied to real-world scenarios with unknown and complex
degradations, hidden states tend to contain unpleasant artifacts and propagate
them to restored frames. In this circumstance, our analyses show that such
artifacts can be largely alleviated when the hidden state is replaced with a
cleaner counterpart. Based on the observations, we propose a Hidden State
Attention (HSA) module to mitigate artifacts in real-world video
super-resolution. Specifically, we first adopt various cheap filters to produce
a hidden state pool. For example, Gaussian blur filters are for smoothing
artifacts while sharpening filters are for enhancing details. To aggregate a
new hidden state that contains fewer artifacts from the hidden state pool, we
devise a Selective Cross Attention (SCA) module, in which the attention between
input features and each hidden state is calculated. Equipped with HSA, our
proposed method, namely FastRealVSR, is able to achieve 2x speedup while
obtaining better performance than Real-BasicVSR. Codes will be available at
https://github.com/TencentARC/FastRealVSR",https://github.com/TencentARC/FastRealVSR,-1
"Runtime Analysis for the NSGA-II: Proving, Quantifying, and Explaining the Inefficiency For Many Objectives",0.0512094,"The NSGA-II is one of the most prominent algorithms to solve multi-objective
optimization problems. Despite numerous successful applications, several
studies have shown that the NSGA-II is less effective for larger numbers of
objectives. In this work, we use mathematical runtime analyses to rigorously
demonstrate and quantify this phenomenon. We show that even on the simple
$m$-objective generalization of the discrete OneMinMax benchmark, where every
solution is Pareto optimal, the NSGA-II also with large population sizes cannot
compute the full Pareto front (objective vectors of all Pareto optima) in
sub-exponential time when the number of objectives is at least three. The
reason for this unexpected behavior lies in the fact that in the computation of
the crowding distance, the different objectives are regarded independently.
This is not a problem for two objectives, where any sorting of a pair-wise
incomparable set of solutions according to one objective is also such a sorting
according to the other objective (in the inverse order).",None,-1
Machine Learning-Powered Course Allocation,0.250844,"We study the course allocation problem, where universities assign course
schedules to students. The current state-of-the-art mechanism, Course Match,
has one major shortcoming: students make significant mistakes when reporting
their preferences, which negatively affects welfare and fairness. To address
this issue, we introduce a new mechanism, Machine Learning-powered Course Match
(MLCM). At the core of MLCM is a machine learning-powered preference
elicitation module that iteratively asks personalized pairwise comparison
queries to alleviate students' reporting mistakes. Extensive computational
experiments, grounded in real-world data, demonstrate that MLCM, with only ten
comparison queries, significantly increases both average and minimum student
utility by 7%-11% and 17%-29%, respectively. Finally, we highlight MLCM's
robustness to changes in the environment and show how our design minimizes the
risk of upgrading to MLCM while making the upgrade process simple for
universities and seamless for their students.",None,-1
Will It Blend? Mixing Training Paradigms & Prompting for Argument Quality Prediction,0.112255,"This paper describes our contributions to the Shared Task of the 9th Workshop
on Argument Mining (2022). Our approach uses Large Language Models for the task
of Argument Quality Prediction. We perform prompt engineering using GPT-3, and
also investigate the training paradigms multi-task learning, contrastive
learning, and intermediate-task training. We find that a mixed prediction setup
outperforms single models. Prompting GPT-3 works best for predicting argument
validity, and argument novelty is best estimated by a model trained using all
three training paradigms.",https://github.com/m0re4u/argmining2022,-1
Towards Real-time High-Definition Image Snow Removal: Efficient Pyramid Network with Asymmetrical Encoder-decoder Architecture,0.105823,"In winter scenes, the degradation of images taken under snow can be pretty
complex, where the spatial distribution of snowy degradation is varied from
image to image. Recent methods adopt deep neural networks to directly recover
clean scenes from snowy images. However, due to the paradox caused by the
variation of complex snowy degradation, achieving reliable High-Definition
image desnowing performance in real time is a considerable challenge. We
develop a novel Efficient Pyramid Network with asymmetrical encoder-decoder
architecture for real-time HD image desnowing. The general idea of our proposed
network is to utilize the multi-scale feature flow fully and implicitly mine
clean cues from features. Compared with previous state-of-the-art desnowing
methods, our approach achieves a better complexity-performance trade-off and
effectively handles the processing difficulties of HD and Ultra-HD images.
  The extensive experiments on three large-scale image desnowing datasets
demonstrate that our method surpasses all state-of-the-art approaches by a
large margin both quantitatively and qualitatively, boosting the PSNR metric
from 31.76 dB to 34.10 dB on the CSD test dataset and from 28.29 dB to 30.87 dB
on the SRRS test dataset.",None,-1
A Transparency Index Framework for AI in Education,0.055811,"Numerous AI ethics checklists and frameworks have been proposed focusing on
different dimensions of ethical AI such as fairness, explainability, and
safety. Yet, no such work has been done on developing transparent AI systems
for real-world educational scenarios. This paper presents a Transparency Index
framework that has been iteratively co-designed with different stakeholders of
AI in education, including educators, ed-tech experts, and AI practitioners. We
map the requirements of transparency for different categories of stakeholders
of AI in education and demonstrate that transparency considerations are
embedded in the entire AI development process from the data collection stage
until the AI system is deployed in the real world and iteratively improved. We
also demonstrate how transparency enables the implementation of other ethical
AI dimensions in Education like interpretability, accountability, and safety.
In conclusion, we discuss the directions for future research in this newly
emerging field. The main contribution of this study is that it highlights the
importance of transparency in developing AI-powered educational technologies
and proposes an index framework for its conceptualization for AI in education.",None,-1
Making Your First Choice: To Address Cold Start Problem in Vision Active Learning,0.224089,"Active learning promises to improve annotation efficiency by iteratively
selecting the most important data to be annotated first. However, we uncover a
striking contradiction to this promise: active learning fails to select data as
efficiently as random selection at the first few choices. We identify this as
the cold start problem in vision active learning, caused by a biased and
outlier initial query. This paper seeks to address the cold start problem by
exploiting the three advantages of contrastive learning: (1) no annotation is
required; (2) label diversity is ensured by pseudo-labels to mitigate bias; (3)
typical data is determined by contrastive features to reduce outliers.
Experiments are conducted on CIFAR-10-LT and three medical imaging datasets
(i.e. Colon Pathology, Abdominal CT, and Blood Cell Microscope). Our initial
query not only significantly outperforms existing active querying strategies
but also surpasses random selection by a large margin. We foresee our solution
to the cold start problem as a simple yet strong baseline to choose the initial
query for vision active learning. Code is available:
https://github.com/c-liangyu/CSVAL",https://github.com/c-liangyu/CSVAL,-1
Consistent Dropout for Policy Gradient Reinforcement Learning,0.0129276,"Dropout has long been a staple of supervised learning, but is rarely used in
reinforcement learning. We analyze why naive application of dropout is
problematic for policy-gradient learning algorithms and introduce consistent
dropout, a simple technique to address this instability. We demonstrate
consistent dropout enables stable training with A2C and PPO in both continuous
and discrete action environments across a wide range of dropout probabilities.
Finally, we show that consistent dropout enables the online training of complex
architectures such as GPT without needing to disable the model's native
dropout.",https://github.com/DLR-RM/stable-baselines3,-1
Frustratingly Easy Label Projection for Cross-lingual Transfer,0.263285,"Translating training data into many languages has emerged as a practical
solution for improving cross-lingual transfer. For tasks that involve
span-level annotations, such as information extraction or question answering,
an additional label projection step is required to map annotated spans onto the
translated texts. Recently, a few efforts have utilized a simple
mark-then-translate method to jointly perform translation and projection by
inserting special markers around the labeled spans in the original sentence.
However, as far as we are aware, no empirical analysis has been conducted on
how this approach compares to traditional annotation projection based on word
alignment. In this paper, we present an extensive empirical study across 57
languages and three tasks (QA, NER, and Event Extraction) to evaluate the
effectiveness and limitations of both methods, filling an important gap in the
literature. Experimental results show that our optimized version of
mark-then-translate, which we call EasyProject, is easily applied to many
languages and works surprisingly well, outperforming the more complex word
alignment-based methods. We analyze several key factors that affect the
end-task performance, and show EasyProject works well because it can accurately
preserve label span boundaries after translation. We will publicly release all
our code and data.",https://github.com/edchengg/easyproject,-1
Talking About Large Language Models,0.0744917,"Thanks to rapid progress in artificial intelligence, we have entered an era
when technology and philosophy intersect in interesting ways. Sitting squarely
at the centre of this intersection are large language models (LLMs). The more
adept LLMs become at mimicking human language, the more vulnerable we become to
anthropomorphism, to seeing the systems in which they are embedded as more
human-like than they really are. This trend is amplified by the natural
tendency to use philosophically loaded terms, such as ""knows"", ""believes"", and
""thinks"", when describing these systems. To mitigate this trend, this paper
advocates the practice of repeatedly stepping back to remind ourselves of how
LLMs, and the systems of which they form a part, actually work. The hope is
that increased scientific precision will encourage more philosophical nuance in
the discourse around artificial intelligence, both within the field and in the
public sphere.",None,-1
Brain tumor detection using artificial convolutional neural networks,0.194736,"In this paper, a convolutional neural network (CNN) was used to classify NMR
images of human brains with 4 different types of tumors: meningioma, glioma and
pituitary gland tumors. During the training phase of this project, an accuracy
of 100% was obtained, meanwhile, in the evaluation phase the precision was 96%.",None,-1
Dynamic Proposals for Efficient Object Detection,0.105131,"Object detection is a basic computer vision task to loccalize and categorize
objects in a given image. Most state-of-the-art detection methods utilize a
fixed number of proposals as an intermediate representation of object
candidates, which is unable to adapt to different computational constraints
during inference. In this paper, we propose a simple yet effective method which
is adaptive to different computational resources by generating dynamic
proposals for object detection. We first design a module to make a single
query-based model to be able to inference with different numbers of proposals.
Further, we extend it to a dynamic model to choose the number of proposals
according to the input image, greatly reducing computational costs. Our method
achieves significant speed-up across a wide range of detection models including
two-stage and query-based models while obtaining similar or even better
accuracy.",None,-1
Model Agnostic Local Explanations of Reject,0.0,"The application of machine learning based decision making systems in safety
critical areas requires reliable high certainty predictions. Reject options are
a common way of ensuring a sufficiently high certainty of predictions made by
the system. While being able to reject uncertain samples is important, it is
also of importance to be able to explain why a particular sample was rejected.
However, explaining general reject options is still an open problem. We propose
a model agnostic method for locally explaining arbitrary reject options by
means of interpretable models and counterfactual explanations.",https://github.com/andreArtelt/LocalModelAgnosticExplanationReject,-1
Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe,0.179457,"Privacy concerns have attracted increasing attention in data-driven products
due to the tendency of machine learning models to memorize sensitive training
data. Generating synthetic versions of such data with a formal privacy
guarantee, such as differential privacy (DP), provides a promising path to
mitigating these privacy concerns, but previous approaches in this direction
have typically failed to produce synthetic data of high quality. In this work,
we show that a simple and practical recipe in the text domain is effective:
simply fine-tuning a pretrained generative language model with DP enables the
model to generate useful synthetic text with strong privacy protection. Through
extensive empirical analyses on both benchmark and private customer data, we
demonstrate that our method produces synthetic text that is competitive in
terms of utility with its non-private counterpart, meanwhile providing strong
protection against potential privacy leakages.",https://github.com/microsoft/dp-transformers,-1
CoNSoLe: Convex Neural Symbolic Learning,0.0643839,"Learning the underlying equation from data is a fundamental problem in many
disciplines. Recent advances rely on Neural Networks (NNs) but do not provide
theoretical guarantees in obtaining the exact equations owing to the
non-convexity of NNs. In this paper, we propose Convex Neural Symbolic Learning
(CoNSoLe) to seek convexity under mild conditions. The main idea is to
decompose the recovering process into two steps and convexify each step. In the
first step of searching for right symbols, we convexify the deep Q-learning.
The key is to maintain double convexity for both the negative Q-function and
the negative reward function in each iteration, leading to provable convexity
of the negative optimal Q function to learn the true symbol connections.
Conditioned on the exact searching result, we construct a Locally Convex
equation Learner (LoCaL) neural network to convexify the estimation of symbol
coefficients. With such a design, we quantify a large region with strict
convexity in the loss surface of LoCaL for commonly used physical functions.
Finally, we demonstrate the superior performance of the CoNSoLe framework over
the state-of-the-art on a diverse set of datasets.",None,-1
Lightweight Monocular Depth Estimation through Guided Decoding,0.154182,"We present a lightweight encoder-decoder architecture for monocular depth
estimation, specifically designed for embedded platforms. Our main contribution
is the Guided Upsampling Block (GUB) for building the decoder of our model.
Motivated by the concept of guided image filtering, GUB relies on the image to
guide the decoder on upsampling the feature representation and the depth map
reconstruction, achieving high resolution results with fine-grained details.
Based on multiple GUBs, our model outperforms the related methods on the NYU
Depth V2 dataset in terms of accuracy while delivering up to 35.1 fps on the
NVIDIA Jetson Nano and up to 144.5 fps on the NVIDIA Xavier NX. Similarly, on
the KITTI dataset, inference is possible with up to 23.7 fps on the Jetson Nano
and 102.9 fps on the Xavier NX. Our code and models are made publicly
available.",https://github.com/mic-rud/GuidedDecoding,-1
ASSIST: Towards Label Noise-Robust Dialogue State Tracking,0.305378,"The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state
tracking (DST). However, substantial noise has been discovered in its state
annotations. Such noise brings about huge challenges for training DST models
robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have
been published recently, there are still lots of noisy labels, especially in
the training set. Besides, it is costly to rectify all the problematic
annotations. In this paper, instead of improving the annotation quality
further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt
dIalogue State Tracking), to train DST models robustly from noisy labels.
ASSIST first generates pseudo labels for each sample in the training set by
using an auxiliary model trained on a small clean dataset, then puts the
generated pseudo labels and vanilla noisy labels together to train the primary
model. We show the validity of ASSIST theoretically. Experimental results also
demonstrate that ASSIST improves the joint goal accuracy of DST by up to
$28.16\%$ on MultiWOZ 2.0 and $8.41\%$ on MultiWOZ 2.4, compared to using only
the vanilla noisy labels.",https://github.com/smartyfh/DST-ASSIST,-1
EC-KitY: Evolutionary Computation Tool Kit in Python with Seamless Machine Learning Integration,0.0144059,"EC-KitY is a comprehensive Python library for doing evolutionary computation
(EC), licensed under the BSD 3-Clause License, and compatible with
scikit-learn. Designed with modern software engineering and machine learning
integration in mind, EC-KitY can support all popular EC paradigms, including
genetic algorithms, genetic programming, coevolution, evolutionary
multi-objective optimization, and more. This paper provides an overview of the
package, including the ease of setting up an EC experiment, the architecture,
the main features, and a comparison with other libraries.",https://github.com/EC-KitY/EC-KitY,-1
Synthesis of Stabilizing Recurrent Equilibrium Network Controllers,0.219489,"We propose a parameterization of a nonlinear dynamic controller based on the
recurrent equilibrium network, a generalization of the recurrent neural
network. We derive constraints on the parameterization under which the
controller guarantees exponential stability of a partially observed dynamical
system with sector bounded nonlinearities. Finally, we present a method to
synthesize this controller using projected policy gradient methods to maximize
a reward function with arbitrary structure. The projection step involves the
solution of convex optimization problems. We demonstrate the proposed method
with simulated examples of controlling nonlinear plants, including plants
modeled with neural networks.",https://github.com/neelayjunnarkar/stabilizing-ren,-1
On the Role of Parallel Data in Cross-lingual Transfer Learning,0.0127168,"While prior work has established that the use of parallel data is conducive
for cross-lingual learning, it is unclear if the improvements come from the
data itself, or if it is the modeling of parallel interactions that matters.
Exploring this, we examine the usage of unsupervised machine translation to
generate synthetic parallel data, and compare it to supervised machine
translation and gold parallel data. We find that even model generated parallel
data can be useful for downstream tasks, in both a general setting (continued
pretraining) as well as the task-specific setting (translate-train), although
our best results are still obtained using real parallel data. Our findings
suggest that existing multilingual models do not exploit the full potential of
monolingual data, and prompt the community to reconsider the traditional
categorization of cross-lingual learning approaches.",https://github.com/facebookresearch/XLM,-1
ELLE: Efficient Lifelong Pre-training for Emerging Data,0.0850234,"Current pre-trained language models (PLM) are typically trained with static
data, ignoring that in real-world scenarios, streaming data of various sources
may continuously grow. This requires PLMs to integrate the information from all
the sources in a lifelong manner. Although this goal could be achieved by
exhaustive pre-training on all the existing data, such a process is known to be
computationally expensive. To this end, we propose ELLE, aiming at efficient
lifelong pre-training for emerging data. Specifically, ELLE consists of (1)
function preserved model expansion, which flexibly expands an existing PLM's
width and depth to improve the efficiency of knowledge acquisition; and (2)
pre-trained domain prompts, which disentangle the versatile knowledge learned
during pre-training and stimulate the proper knowledge for downstream tasks. We
experiment ELLE with streaming data from 5 domains on BERT and GPT. The results
show the superiority of ELLE over various lifelong learning baselines in both
pre-training efficiency and downstream performances. The codes are publicly
available at https://github.com/thunlp/ELLE.",https://github.com/thunlp/ELLE,-1
Global Contentious Politics Database (GLOCON) Annotation Manuals,0.0134448,"The database creation utilized automated text processing tools that detect if
a news article contains a protest event, locate protest information within the
article, and extract pieces of information regarding the detected protest
events. The basis of training and testing the automated tools is the GLOCON
Gold Standard Corpus (GSC), which contains news articles from multiple sources
from each focus country. The articles in the GSC were manually coded by skilled
annotators in both classification and extraction tasks with the utmost accuracy
and consistency that automated tool development demands. In order to assure
these, the annotation manuals in this document lay out the rules according to
which annotators code the news articles. Annotators refer to the manuals at all
times for all annotation tasks and apply the rules that they contain. The
content of the annotation manual is built on the general principles and
standards of linguistic annotation laid out in other prominent annotation
manuals such as ACE, CAMEO, and TimeML. These principles, however, have been
adapted or rather modified heavily to accommodate the social scientific
concepts and variables employed in the EMW project. The manual has been molded
throughout a long trial and error process that accompanied the annotation of
the GSC. It owes much of its current shape to the meticulous work and
invaluable feedback provided by highly specialized teams of annotators, whose
diligence and expertise greatly increased the quality of the corpus.",None,-1
ORCA: A Challenging Benchmark for Arabic Language Understanding,0.0517807,"Due to their crucial role in all NLP, several benchmarks have been proposed
to evaluate pretrained language models. In spite of these efforts, no public
benchmark of diverse nature currently exists for evaluation of Arabic. This
makes it challenging to measure progress for both Arabic and multilingual
language models. This challenge is compounded by the fact that any benchmark
targeting Arabic needs to take into account the fact that Arabic is not a
single language but rather a collection of languages and varieties. In this
work, we introduce ORCA, a publicly available benchmark for Arabic language
understanding evaluation. ORCA is carefully constructed to cover diverse Arabic
varieties and a wide range of challenging Arabic understanding tasks exploiting
60 different datasets across seven NLU task clusters. To measure current
progress in Arabic NLU, we use ORCA to offer a comprehensive comparison between
18 multilingual and Arabic language models. We also provide a public
leaderboard with a unified single-number evaluation metric (ORCA score) to
facilitate future research.",None,-1
Game-theoretic Objective Space Planning,0.13983,"Generating competitive strategies and performing continuous motion planning
simultaneously in an adversarial setting is a challenging problem. In addition,
understanding the intent of other agents is crucial to deploying autonomous
systems in adversarial multi-agent environments. Existing approaches either
discretize agent action by grouping similar control inputs, sacrificing
performance in motion planning, or plan in uninterpretable latent spaces,
producing hard-to-understand agent behaviors. Furthermore, the most popular
policy optimization frameworks do not recognize the long-term effect of actions
and become myopic. This paper proposes an agent action discretization method
via abstraction that provides clear intentions of agent actions, an efficient
offline pipeline of agent population synthesis, and a planning strategy using
counterfactual regret minimization with function approximation. Finally, we
experimentally validate our findings on scaled autonomous vehicles in a
head-to-head racing setting. We demonstrate that using the proposed framework
significantly improves learning, improves the win rate against different
opponents, and the improvements can be transferred to unseen opponents in an
unseen environment.",None,-1
Are AlphaZero-like Agents Robust to Adversarial Perturbations?,0.0287271,"The success of AlphaZero (AZ) has demonstrated that neural-network-based Go
AIs can surpass human performance by a large margin. Given that the state space
of Go is extremely large and a human player can play the game from any legal
state, we ask whether adversarial states exist for Go AIs that may lead them to
play surprisingly wrong actions. In this paper, we first extend the concept of
adversarial examples to the game of Go: we generate perturbed states that are
``semantically'' equivalent to the original state by adding meaningless moves
to the game, and an adversarial state is a perturbed state leading to an
undoubtedly inferior action that is obvious even for Go beginners. However,
searching the adversarial state is challenging due to the large, discrete, and
non-differentiable search space. To tackle this challenge, we develop the first
adversarial attack on Go AIs that can efficiently search for adversarial states
by strategically reducing the search space. This method can also be extended to
other board games such as NoGo. Experimentally, we show that the actions taken
by both Policy-Value neural network (PV-NN) and Monte Carlo tree search (MCTS)
can be misled by adding one or two meaningless stones; for example, on 58\% of
the AlphaGo Zero self-play games, our method can make the widely used KataGo
agent with 50 simulations of MCTS plays a losing action by adding two
meaningless stones. We additionally evaluated the adversarial examples found by
our algorithm with amateur human Go players and 90\% of examples indeed lead
the Go agent to play an obviously inferior action. Our code is available at
\url{https://PaperCode.cc/GoAttack}.",https://PaperCode.cc/GoAttack,-1
LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging,0.234114,"We present LINGUIST, a method for generating annotated data for Intent
Classification and Slot Tagging (IC+ST), via fine-tuning AlexaTM 5B, a
5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a
flexible instruction prompt. In a 10-shot novel intent setting for the SNIPS
dataset, LINGUIST surpasses state-of-the-art approaches (Back-Translation and
Example Extrapolation) by a wide margin, showing absolute improvement for the
target intents of +1.9 points on IC Recall and +2.5 points on ST F1 Score. In
the zero-shot cross-lingual setting of the mATIS++ dataset, LINGUIST
out-performs a strong baseline of Machine Translation with Slot Alignment by
+4.14 points absolute on ST F1 Score across 6 languages, while matching
performance on IC. Finally, we verify our results on an internal large-scale
multilingual dataset for conversational agent IC+ST and show significant
improvements over a baseline which uses Back-Translation, Paraphrasing and Slot
Catalog Resampling. To our knowledge, we are the first to demonstrate
instruction fine-tuning of a large-scale seq2seq model to control the outputs
of multilingual intent- and slot-labeled data generation.",None,-1
ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation,0.134241,"We study the text generation task under the approach of pre-trained language
models (PLMs). Typically, an auto-regressive (AR) method is adopted for
generating texts in a token-by-token manner. Despite many advantages of AR
generation, it usually suffers from inefficient inference. Therefore,
non-autoregressive (NAR) models are proposed to generate all target tokens
simultaneously. However, NAR models usually generate texts of lower quality due
to the absence of token dependency in the output text. In this paper, we
propose ELMER: an efficient and effective PLM for NAR text generation to
explicitly model the token dependency during NAR generation. By leveraging the
early exit technique, ELMER enables the token generations at different layers,
according to their prediction confidence (a more confident token will exit at a
lower layer). Besides, we propose a novel pre-training objective, Layer
Permutation Language Modeling, to pre-train ELMER by permuting the exit layer
for each token in sequences. Experiments on three text generation tasks show
that ELMER significantly outperforms NAR models and further narrows the
performance gap with AR PLMs (\eg ELMER (29.92) vs BART (30.61) ROUGE-L in
XSUM) while achieving over 10 times inference speedup.",https://github.com/RUCAIBox/ELMER,-1
Learning Quantum Entanglement Distillation with Noisy Classical Communications,0.0705383,"Quantum networking relies on the management and exploitation of entanglement.
Practical sources of entangled qubits are imperfect, producing mixed quantum
state with reduced fidelity with respect to ideal Bell pairs. Therefore, an
important primitive for quantum networking is entanglement distillation, whose
goal is to enhance the fidelity of entangled qubits through local operations
and classical communication (LOCC). Existing distillation protocols assume the
availability of ideal, noiseless, communication channels. In this paper, we
study the case in which communication takes place over noisy binary symmetric
channels. We propose to implement local processing through parameterized
quantum circuits (PQCs) that are optimized to maximize the average fidelity,
while accounting for communication errors. The introduced approach, Noise
Aware-LOCCNet (NA-LOCCNet), is shown to have significant advantages over
existing protocols designed for noiseless communications.",https://github.com/kclip/Noise-Aware-LOCCNet,-1
Monotonic segmental attention for automatic speech recognition,0.050855,"We introduce a novel segmental-attention model for automatic speech
recognition. We restrict the decoder attention to segments to avoid quadratic
runtime of global attention, better generalize to long sequences, and
eventually enable streaming. We directly compare global-attention and different
segmental-attention modeling variants. We develop and compare two separate
time-synchronous decoders, one specifically taking the segmental nature into
account, yielding further improvements. Using time-synchronous decoding for
segmental models is novel and a step towards streaming applications. Our
experiments show the importance of a length model to predict the segment
boundaries. The final best segmental-attention model using segmental decoding
performs better than global-attention, in contrast to other monotonic attention
approaches in the literature. Further, we observe that the segmental model
generalizes much better to long sequences of up to several minutes.",https://github.com/rwth-i6/returnn-experiments/tree/master/,-1
Selecting and combining complementary feature representations and classifiers for hate speech detection,0.192072,"Hate speech is a major issue in social networks due to the high volume of
data generated daily. Recent works demonstrate the usefulness of machine
learning (ML) in dealing with the nuances required to distinguish between
hateful posts from just sarcasm or offensive language. Many ML solutions for
hate speech detection have been proposed by either changing how features are
extracted from the text or the classification algorithm employed. However, most
works consider only one type of feature extraction and classification
algorithm. This work argues that a combination of multiple feature extraction
techniques and different classification models is needed. We propose a
framework to analyze the relationship between multiple feature extraction and
classification techniques to understand how they complement each other. The
framework is used to select a subset of complementary techniques to compose a
robust multiple classifiers system (MCS) for hate speech detection. The
experimental study considering four hate speech classification datasets
demonstrates that the proposed framework is a promising methodology for
analyzing and designing high-performing MCS for this task. MCS system obtained
using the proposed framework significantly outperforms the combination of all
models and the homogeneous and heterogeneous selection heuristics,
demonstrating the importance of having a proper selection scheme. Source code,
figures, and dataset splits can be found in the GitHub repository:
https://github.com/Menelau/Hate-Speech-MCS.",https://github.com/Menelau/Hate-Speech-MCS,-1
META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI,0.396694,"Task-oriented dialogue (TOD) systems have been widely used by mobile phone
intelligent assistants to accomplish tasks such as calendar scheduling or hotel
reservation. Current TOD systems usually focus on multi-turn text/speech
interaction, then they would call back-end APIs designed for TODs to perform
the task. However, this API-based architecture greatly limits the
information-searching capability of intelligent assistants and may even lead to
task failure if TOD-specific APIs are not available or the task is too
complicated to be executed by the provided APIs. In this paper, we propose a
new TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A
GUI-TOD system can directly perform GUI operations on real APPs and execute
tasks without invoking TOD-specific backend APIs. Furthermore, we release
META-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile
GUI. We also propose a multi-model action prediction and response model, which
show promising results on META-GUI. The dataset, codes and leaderboard are
publicly available.",https://x-lance.github.io/,-1
Are High-Resolution Event Cameras Really Needed?,0.154045,"Due to their outstanding properties in challenging conditions, event cameras
have become indispensable in a wide range of applications, ranging from
automotive, computational photography, and SLAM. However, as further
improvements are made to the sensor design, modern event cameras are trending
toward higher and higher sensor resolutions, which result in higher bandwidth
and computational requirements on downstream tasks. Despite this trend, the
benefits of using high-resolution event cameras to solve standard computer
vision tasks are still not clear. In this work, we report the surprising
discovery that, in low-illumination conditions and at high speeds,
low-resolution cameras can outperform high-resolution ones, while requiring a
significantly lower bandwidth. We provide both empirical and theoretical
evidence for this claim, which indicates that high-resolution event cameras
exhibit higher per-pixel event rates, leading to higher temporal noise in
low-illumination conditions and at high speeds. As a result, in most cases,
high-resolution event cameras show a lower task performance, compared to lower
resolution sensors in these conditions. We empirically validate our findings
across several tasks, namely image reconstruction, optical flow estimation, and
camera pose tracking, both on synthetic and real data. We believe that these
findings will provide important guidelines for future trends in event camera
development.",None,-1
Implementing Deep Learning-Based Approaches for Article Summarization in Indian Languages,0.168081,"The research on text summarization for low-resource Indian languages has been
limited due to the availability of relevant datasets. This paper presents a
summary of various deep-learning approaches used for the ILSUM 2022 Indic
language summarization datasets. The ISUM 2022 dataset consists of news
articles written in Indian English, Hindi, and Gujarati respectively, and their
ground-truth summarizations. In our work, we explore different pre-trained
seq2seq models and fine-tune those with the ILSUM 2022 datasets. In our case,
the fine-tuned SoTA PEGASUS model worked the best for English, the fine-tuned
IndicBART model with augmented data for Hindi, and again fine-tuned PEGASUS
model along with a translation mapping-based approach for Gujarati. Our scores
on the obtained inferences were evaluated using ROUGE-1, ROUGE-2, and ROUGE-4
as the evaluation metrics.",None,-1
Weight Predictor Network with Feature Selection for Small Sample Tabular Biomedical Data,0.0,"Tabular biomedical data is often high-dimensional but with a very small
number of samples. Although recent work showed that well-regularised simple
neural networks could outperform more sophisticated architectures on tabular
data, they are still prone to overfitting on tiny datasets with many
potentially irrelevant features. To combat these issues, we propose Weight
Predictor Network with Feature Selection (WPFS) for learning neural networks
from high-dimensional and small sample data by reducing the number of learnable
parameters and simultaneously performing feature selection. In addition to the
classification network, WPFS uses two small auxiliary networks that together
output the weights of the first layer of the classification model. We evaluate
on nine real-world biomedical datasets and demonstrate that WPFS outperforms
other standard as well as more recent methods typically applied to tabular
data. Furthermore, we investigate the proposed feature selection mechanism and
show that it improves performance while providing useful insights into the
learning task.",https://github.com/andreimargeloiu/WPFS,-1
V-Doc : Visual questions answers with Documents,0.0485617,"We propose V-Doc, a question-answering tool using document images and PDF,
mainly for researchers and general non-deep learning experts looking to
generate, process, and understand the document visual question answering tasks.
The V-Doc supports generating and using both extractive and abstractive
question-answer pairs using documents images. The extractive QA selects a
subset of tokens or phrases from the document contents to predict the answers,
while the abstractive QA recognises the language in the content and generates
the answer based on the trained model. Both aspects are crucial to
understanding the documents, especially in an image format. We include a
detailed scenario of question generation for the abstractive QA task. V-Doc
supports a wide range of datasets and models, and is highly extensible through
a declarative, framework-agnostic platform.",https://github.com/usydnlp/vdoc,-1
Visual-tactile Fusion for Transparent Object Grasping in Complex Backgrounds,0.043459,"The accurate detection and grasping of transparent objects are challenging
but of significance to robots. Here, a visual-tactile fusion framework for
transparent object grasping under complex backgrounds and variant light
conditions is proposed, including the grasping position detection, tactile
calibration, and visual-tactile fusion based classification. First, a
multi-scene synthetic grasping dataset generation method with a Gaussian
distribution based data annotation is proposed. Besides, a novel grasping
network named TGCNN is proposed for grasping position detection, showing good
results in both synthetic and real scenes. In tactile calibration, inspired by
human grasping, a fully convolutional network based tactile feature extraction
method and a central location based adaptive grasping strategy are designed,
improving the success rate by 36.7% compared to direct grasping. Furthermore, a
visual-tactile fusion method is proposed for transparent objects
classification, which improves the classification accuracy by 34%. The proposed
framework synergizes the advantages of vision and touch, and greatly improves
the grasping efficiency of transparent objects.",None,-1
TransLog: A Unified Transformer-based Framework for Log Anomaly Detection,0.444545,"Log anomaly detection is a key component in the field of artificial
intelligence for IT operations (AIOps). Considering log data of variant
domains, retraining the whole network for unknown domains is inefficient in
real industrial scenarios especially for low-resource domains. However,
previous deep models merely focused on extracting the semantics of log sequence
in the same domain, leading to poor generalization on multi-domain logs.
Therefore, we propose a unified Transformer-based framework for log anomaly
detection (\ourmethod{}), which is comprised of the pretraining and
adapter-based tuning stage. Our model is first pretrained on the source domain
to obtain shared semantic knowledge of log data. Then, we transfer the
pretrained model to the target domain via the adapter-based tuning. The
proposed method is evaluated on three public datasets including one source
domain and two target domains. The experimental results demonstrate that our
simple yet efficient approach, with fewer trainable parameters and lower
training costs in the target domain, achieves state-of-the-art performance on
three benchmarks.",None,-1
Pronunciation Modeling of Foreign Words for Mandarin ASR by Considering the Effect of Language Transfer,0.00974654,"One of the challenges in automatic speech recognition is foreign words
recognition. It is observed that a speaker's pronunciation of a foreign word is
influenced by his native language knowledge, and such phenomenon is known as
the effect of language transfer. This paper focuses on examining the phonetic
effect of language transfer in automatic speech recognition. A set of lexical
rules is proposed to convert an English word into Mandarin phonetic
representation. In this way, a Mandarin lexicon can be augmented by including
English words. Hence, the Mandarin ASR system becomes capable to recognize
English words without retraining or re-estimation of the acoustic model
parameters. Using the lexicon that derived from the proposed rules, the ASR
performance of Mandarin English mixed speech is improved without harming the
accuracy of Mandarin only speech. The proposed lexical rules are generalized
and they can be directly applied to unseen English words.",None,-1
Simplifying Multilingual News Clustering Through Projection From a Shared Space,0.0540551,"The task of organizing and clustering multilingual news articles for media
monitoring is essential to follow news stories in real time. Most approaches to
this task focus on high-resource languages (mostly English), with low-resource
languages being disregarded. With that in mind, we present a much simpler
online system that is able to cluster an incoming stream of documents without
depending on language-specific features. We empirically demonstrate that the
use of multilingual contextual embeddings as the document representation
significantly improves clustering quality. We challenge previous crosslingual
approaches by removing the precondition of building monolingual clusters. We
model the clustering process as a set of linear classifiers to aggregate
similar documents, and correct closely-related multilingual clusters through
merging in an online fashion. Our system achieves state-of-the-art results on a
multilingual news stream clustering dataset, and we introduce a new evaluation
for zero-shot news clustering in multiple languages. We make our code available
as open-source.",https://github.com/Priberam/projected-news-clustering,-1
Predicting Vegetation Stratum Occupancy from Airborne LiDAR Data with Deep Learning,0.0898018,"We propose a new deep learning-based method for estimating the occupancy of
vegetation strata from airborne 3D LiDAR point clouds. Our model predicts
rasterized occupancy maps for three vegetation strata corresponding to lower,
medium, and higher cover. Our weakly-supervised training scheme allows our
network to only be supervised with vegetation occupancy values aggregated over
cylindrical plots containing thousands of points. Such ground truth is easier
to produce than pixel-wise or point-wise annotations. Our method outperforms
handcrafted and deep learning baselines in terms of precision by up to 30%,
while simultaneously providing visual and interpretable predictions. We provide
an open-source implementation along with a dataset of 199 agricultural plots to
train and evaluate weakly supervised occupancy regression algorithms.",https://github.com/ekalinicheva/plot_vegetation_coverage,-1
DePlot: One-shot visual language reasoning by plot-to-table translation,0.365707,"Visual language such as charts and plots is ubiquitous in the human world.
Comprehending plots and charts requires strong reasoning skills. Prior
state-of-the-art (SOTA) models require at least tens of thousands of training
examples and their reasoning capabilities are still much limited, especially on
complex human-written queries. This paper presents the first one-shot solution
to visual language reasoning. We decompose the challenge of visual language
reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over
the translated text. The key in this method is a modality conversion module,
named as DePlot, which translates the image of a plot or chart to a linearized
table. The output of DePlot can then be directly used to prompt a pretrained
large language model (LLM), exploiting the few-shot reasoning capabilities of
LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing
unified task formats and metrics, and train DePlot end-to-end on this task.
DePlot can then be used off-the-shelf together with LLMs in a plug-and-play
fashion. Compared with a SOTA model finetuned on more than >28k data points,
DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over
finetuned SOTA on human-written queries from the task of chart QA.",github.com/google-research/google-research/tree/master/deplot,-1
"Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots",0.0332911,"Few-shot table-to-text generation is a task of composing fluent and faithful
sentences to convey table content using limited data. Despite many efforts
having been made towards generating impressive fluent sentences by fine-tuning
powerful pre-trained language models, the faithfulness of generated content
still needs to be improved. To this end, this paper proposes a novel approach
Attend, Memorize and Generate (called AMG), inspired by the text generation
process of humans. In particular, AMG (1) attends over the multi-granularity of
context using a novel strategy based on table slot level and traditional
token-by-token level attention to exploit both the table structure and natural
linguistic information; (2) dynamically memorizes the table slot allocation
states; and (3) generates faithful sentences according to both the context and
memory allocation states. Comprehensive experiments with human evaluation on
three domains (i.e., humans, songs, and books) of the Wiki dataset show that
our model can generate higher qualified texts when compared with several
state-of-the-art baselines, in both fluency and faithfulness.",https://github.com/wentinghome/AMG,-1
Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild,0.420928,"Talking face generation with great practical significance has attracted more
attention in recent audio-visual studies. How to achieve accurate lip
synchronization is a long-standing challenge to be further investigated.
Motivated by xxx, in this paper, an AttnWav2Lip model is proposed by
incorporating spatial attention module and channel attention module into
lip-syncing strategy. Rather than focusing on the unimportant regions of the
face image, the proposed AttnWav2Lip model is able to pay more attention on the
lip region reconstruction. To our limited knowledge, this is the first attempt
to introduce attention mechanism to the scheme of talking face generation. An
extensive experiments have been conducted to evaluate the effectiveness of the
proposed model. Compared to the baseline measured by LSE-D and LSE-C metrics, a
superior performance has been demonstrated on the benchmark lip synthesis
datasets, including LRW, LRS2 and LRS3.",None,-1
A Deep Neural Network for Multiclass Bridge Element Parsing in Inspection Image Analysis,0.0609363,"Aerial robots such as drones have been leveraged to perform bridge
inspections. Inspection images with both recognizable structural elements and
apparent surface defects can be collected by onboard cameras to provide
valuable information for the condition assessment. This article aims to
determine a suitable deep neural network (DNN) for parsing multiclass bridge
elements in inspection images. An extensive set of quantitative evaluations
along with qualitative examples show that High-Resolution Net (HRNet) possesses
the desired ability. With data augmentation and a training sample of 130
images, a pre-trained HRNet is efficiently transferred to the task of
structural element parsing and has achieved a 92.67% mean F1-score and 86.33%
mean IoU.",None,-1
Generalizing Math Word Problem Solvers via Solution Diversification,0.0654744,"Current math word problem (MWP) solvers are usually Seq2Seq models trained by
the (one-problem; one-solution) pairs, each of which is made of a problem
description and a solution showing reasoning flow to get the correct answer.
However, one MWP problem naturally has multiple solution equations. The
training of an MWP solver with (one-problem; one-solution) pairs excludes other
correct solutions, and thus limits the generalizability of the MWP solver. One
feasible solution to this limitation is to augment multiple solutions to a
given problem. However, it is difficult to collect diverse and accurate augment
solutions through human efforts. In this paper, we design a new training
framework for an MWP solver by introducing a solution buffer and a solution
discriminator. The buffer includes solutions generated by an MWP solver to
encourage the training data diversity. The discriminator controls the quality
of buffered solutions to participate in training. Our framework is flexibly
applicable to a wide setting of fully, semi-weakly and weakly supervised
training for all Seq2Seq MWP solvers. We conduct extensive experiments on a
benchmark dataset Math23k and a new dataset named Weak12k, and show that our
framework improves the performance of various MWP solvers under different
settings by generating correct and diverse solutions.",https://github.com/LZhenwen/Solution Diversity,-1
AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies,0.0540617,"Existing correspondence datasets for two-dimensional (2D) cartoon suffer from
simple frame composition and monotonic movements, making them insufficient to
simulate real animations. In this work, we present a new 2D animation visual
correspondence dataset, AnimeRun, by converting open source three-dimensional
(3D) movies to full scenes in 2D style, including simultaneous moving
background and interactions of multiple subjects. Our analyses show that the
proposed dataset not only resembles real anime more in image composition, but
also possesses richer and more complex motion patterns compared to existing
datasets. With this dataset, we establish a comprehensive benchmark by
evaluating several existing optical flow and segment matching methods, and
analyze shortcomings of these methods on animation data. Data, code and other
supplementary materials are available at
https://lisiyao21.github.io/projects/AnimeRun.",https://github.com/lisiyao21/AnimeRun,-1
Towards Accurate Open-Set Recognition via Background-Class Regularization,0.109949,"In open-set recognition (OSR), classifiers should be able to reject
unknown-class samples while maintaining high closed-set classification
accuracy. To effectively solve the OSR problem, previous studies attempted to
limit latent feature space and reject data located outside the limited space
via offline analyses, e.g., distance-based feature analyses, or complicated
network architectures. To conduct OSR via a simple inference process (without
offline analyses) in standard classifier architectures, we use distance-based
classifiers instead of conventional Softmax classifiers. Afterwards, we design
a background-class regularization strategy, which uses background-class data as
surrogates of unknown-class ones during training phase. Specifically, we
formulate a novel regularization loss suitable for distance-based classifiers,
which reserves sufficiently large class-wise latent feature spaces for known
classes and forces background-class samples to be located far away from the
limited spaces. Through our extensive experiments, we show that the proposed
method provides robust OSR results, while maintaining high closed-set
classification accuracy.",https://github.com/Anjin-Liu/Openset_Learning_AOSR,-1
Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions,0.0810836,"Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural
Networks (NNs) for tasks requiring perception and reasoning. Most NeSy systems
rely on continuous relaxation of logical knowledge, and no discrete decisions
are made within the model pipeline. Furthermore, these methods assume that the
symbolic rules are given. In this paper, we propose Deep Symbolic Learning
(DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a
(set of) perception functions which map continuous data to discrete symbols,
and a symbolic function over the set of symbols. DSL learns simultaneously the
perception and symbolic functions while being trained only on their composition
(NeSy-function). The key novelty of DSL is that it can create internal
(interpretable) symbolic representations and map them to perception inputs
within a differentiable NN learning pipeline. The created symbols are
automatically selected to generate symbolic functions that best explain the
data. We provide experimental analysis to substantiate the efficacy of DSL in
simultaneously learning perception and symbolic functions.",None,-1
CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction,0.0,"Knowledge graph (KG) link prediction is a fundamental task in artificial
intelligence, with applications in natural language processing, information
retrieval, and biomedicine. Recently, promising results have been achieved by
leveraging cross-modal information in KGs, using ensembles that combine
knowledge graph embeddings (KGEs) and contextual language models (LMs).
However, existing ensembles are either (1) not consistently effective in terms
of ranking accuracy gains or (2) impractically inefficient on larger datasets
due to the combinatorial explosion problem of pairwise ranking with deep
language models. In this paper, we propose a novel tiered ranking architecture
CascadER to maintain the ranking accuracy of full ensembling while improving
efficiency considerably. CascadER uses LMs to rerank the outputs of more
efficient base KGEs, relying on an adaptive subset selection scheme aimed at
invoking the LMs minimally while maximizing accuracy gain over the KGE.
Extensive experiments demonstrate that CascadER improves MRR by up to 9 points
over KGE baselines, setting new state-of-the-art performance on four benchmarks
while improving efficiency by one or more orders of magnitude over competitive
cross-modal baselines. Our empirical analyses reveal that diversity of models
across modalities and preservation of individual models' confidence signals
help explain the effectiveness of CascadER, and suggest promising directions
for cross-modal cascaded architectures. Code and pretrained models are
available at https://github.com/tsafavi/cascader.",None,-1
A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank,0.279343,"We show that the choice of pretraining languages affects downstream
cross-lingual transfer for BERT-based models. We inspect zero-shot performance
in balanced data conditions to mitigate data size confounds, classifying
pretraining languages that improve downstream performance as donors, and
languages that are improved in zero-shot performance as recipients. We develop
a method of quadratic time complexity in the number of languages to estimate
these relations, instead of an exponential exhaustive computation of all
possible combinations. We find that our method is effective on a diverse set of
languages spanning different linguistic features and two downstream tasks. Our
findings can inform developers of large-scale multilingual language models in
choosing better pretraining configurations.",https://github.com/SLAB-NLP/linguistic-blood-bank,-1
How to Understand Masked Autoencoders,0.175142,"""Masked Autoencoders (MAE) Are Scalable Vision Learners"" revolutionizes the
self-supervised learning method in that it not only achieves the
state-of-the-art for image pre-training, but is also a milestone that bridges
the gap between visual and linguistic masked autoencoding (BERT-style)
pre-trainings. However, to our knowledge, to date there are no theoretical
perspectives to explain the powerful expressivity of MAE. In this paper, we,
for the first time, propose a unified theoretical framework that provides a
mathematical understanding for MAE. Specifically, we explain the patch-based
attention approaches of MAE using an integral kernel under a non-overlapping
domain decomposition setting. To help the research community to further
comprehend the main reasons of the great success of MAE, based on our
framework, we pose five questions and answer them with mathematical rigor using
insights from operator theory.",https://github.com/facebookresearch/mae,-1
Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation,0.205302,"In reinforcement learning for visual navigation, it is common to develop a
model for each new task, and train that model from scratch with task-specific
interactions in 3D environments. However, this process is expensive; massive
amounts of interactions are needed for the model to generalize well. Moreover,
this process is repeated whenever there is a change in the task type or the
goal modality. We present a unified approach to visual navigation using a novel
modular transfer learning model. Our model can effectively leverage its
experience from one source task and apply it to multiple target tasks (e.g.,
ObjectNav, RoomNav, ViewNav) with various goal modalities (e.g., image, sketch,
audio, label). Furthermore, our model enables zero-shot experience learning,
whereby it can solve the target tasks without receiving any task-specific
interactive training. Our experiments on multiple photorealistic datasets and
challenging tasks show that our approach learns faster, generalizes better, and
outperforms SoTA models by a significant margin.",https://vision.cs.utexas.edu/projects/zsel/,-1
Accelerating Code Search with Deep Hashing and Code Classification,0.49958,"Code search is to search reusable code snippets from source code corpus based
on natural languages queries. Deep learning-based methods of code search have
shown promising results. However, previous methods focus on retrieval accuracy
but lacked attention to the efficiency of the retrieval process. We propose a
novel method CoSHC to accelerate code search with deep hashing and code
classification, aiming to perform an efficient code search without sacrificing
too much accuracy. To evaluate the effectiveness of CoSHC, we apply our method
to five code search models. Extensive experimental results indicate that
compared with previous code search baselines, CoSHC can save more than 90% of
retrieval time meanwhile preserving at least 99% of retrieval accuracy.",None,-1
Accelerating Machine Learning via the Weber-Fechner Law,0.00324147,"The Weber-Fechner Law observes that human perception scales as the logarithm
of the stimulus. We argue that learning algorithms for human concepts could
benefit from the Weber-Fechner Law. Specifically, we impose Weber-Fechner on
simple neural networks, with or without convolution, via the logarithmic power
series of their sorted output. Our experiments show surprising performance and
accuracy on the MNIST data set within a few training iterations and limited
computational resources, suggesting that Weber-Fechner can accelerate machine
learning of human concepts.",None,-1
Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment,0.18398,"Despite recent progress towards scaling up multimodal vision-language models,
these models are still known to struggle on compositional generalization
benchmarks such as Winoground. We find that a critical component lacking from
current vision-language models is relation-level alignment: the ability to
match directional semantic relations in text (e.g., ""mug in grass"") with
spatial relationships in the image (e.g., the position of the mug relative to
the grass). To tackle this problem, we show that relation alignment can be
enforced by encouraging the directed language attention from 'mug' to 'grass'
(capturing the semantic relation 'in') to match the directed visual attention
from the mug to the grass. Tokens and their corresponding objects are softly
identified using the cross-modal attention. We prove that this notion of soft
relation alignment is equivalent to enforcing congruence between vision and
language attention matrices under a 'change of basis' provided by the
cross-modal attention matrix. Intuitively, our approach projects visual
attention into the language attention space to calculate its divergence from
the actual language attention, and vice versa. We apply our Cross-modal
Attention Congruence Regularization (CACR) loss to UNITER and improve on the
state-of-the-art approach to Winoground.",None,-1
Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets,0.595504,"Natural language processing models often exploit spurious correlations
between task-independent features and labels in datasets to perform well only
within the distributions they are trained on, while not generalising to
different task distributions. We propose to tackle this problem by generating a
debiased version of a dataset, which can then be used to train a debiased,
off-the-shelf model, by simply replacing its training data. Our approach
consists of 1) a method for training data generators to generate high-quality,
label-consistent data samples; and 2) a filtering mechanism for removing data
points that contribute to spurious correlations, measured in terms of
z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and
we evaluate on a large suite of debiased, out-of-distribution, and adversarial
test sets. Results show that models trained on our debiased datasets generalise
better than those trained on the original datasets in all settings. On the
majority of the datasets, our method outperforms or performs comparably to
previous state-of-the-art debiasing strategies, and when combined with an
orthogonal technique, product-of-experts, it improves further and outperforms
previous best results of SNLI-hard and MNLI-hard.",https://github.com/jimmycode/,-1
Automatic Fine-grained Glomerular Lesion Recognition in Kidney Pathology,0.366728,"Recognition of glomeruli lesions is the key for diagnosis and treatment
planning in kidney pathology; however, the coexisting glomerular structures
such as mesangial regions exacerbate the difficulties of this task. In this
paper, we introduce a scheme to recognize fine-grained glomeruli lesions from
whole slide images. First, a focal instance structural similarity loss is
proposed to drive the model to locate all types of glomeruli precisely. Then an
Uncertainty Aided Apportionment Network is designed to carry out the
fine-grained visual classification without bounding-box annotations. This
double branch-shaped structure extracts common features of the child class from
the parent class and produces the uncertainty factor for reconstituting the
training dataset. Results of slide-wise evaluation illustrate the effectiveness
of the entire scheme, with an 8-22% improvement of the mean Average Precision
compared with remarkable detection methods. The comprehensive results clearly
demonstrate the effectiveness of the proposed method.",None,-1
Concentration Network for Reinforcement Learning of Large-Scale Multi-Agent Systems,0.0841201,"When dealing with a series of imminent issues, humans can naturally
concentrate on a subset of these concerning issues by prioritizing them
according to their contributions to motivational indices, e.g., the probability
of winning a game. This idea of concentration offers insights into
reinforcement learning of sophisticated Large-scale Multi-Agent Systems (LMAS)
participated by hundreds of agents. In such an LMAS, each agent receives a long
series of entity observations at each step, which can overwhelm existing
aggregation networks such as graph attention networks and cause inefficiency.
In this paper, we propose a concentration network called ConcNet. First,
ConcNet scores the observed entities considering several motivational indices,
e.g., expected survival time and state value of the agents, and then ranks,
prunes, and aggregates the encodings of observed entities to extract features.
Second, distinct from the well-known attention mechanism, ConcNet has a unique
motivational subnetwork to explicitly consider the motivational indices when
scoring the observed entities. Furthermore, we present a concentration policy
gradient architecture that can learn effective policies in LMAS from scratch.
Extensive experiments demonstrate that the presented architecture has excellent
scalability and flexibility, and significantly outperforms existing methods on
LMAS benchmarks.",https://github.com/binary-husky/hmp2g/tree/aaai-conc,-1
Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging,0.0409213,"In recent years, large-scale pre-trained language models (PLMs) have made
extraordinary progress in most NLP tasks. But, in the unsupervised POS tagging
task, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA)
performance. The recent SOTA performance is yielded by a Guassian HMM variant
proposed by He et al. (2018). However, as a generative model, HMM makes very
strong independence assumptions, making it very challenging to incorporate
contexualized word representations from PLMs. In this work, we for the first
time propose a neural conditional random field autoencoder (CRF-AE) model for
unsupervised POS tagging. The discriminative encoder of CRF-AE can
straightforwardly incorporate ELMo word representations. Moreover, inspired by
feature-rich HMM, we reintroduce hand-crafted features into the decoder of
CRF-AE. Finally, experiments clearly show that our model outperforms previous
state-of-the-art models by a large margin on Penn Treebank and multilingual
Universal Dependencies treebank v2.0.",https://github.com/Jacob-Zhou/FeatureCRFAE,-1
Constants of motion network,0.0313939,"The beauty of physics is that there is usually a conserved quantity in an
always-changing system, known as the constant of motion. Finding the constant
of motion is important in understanding the dynamics of the system, but
typically requires mathematical proficiency and manual analytical work. In this
paper, we present a neural network that can simultaneously learn the dynamics
of the system and the constants of motion from data. By exploiting the
discovered constants of motion, it can produce better predictions on dynamics
and can work on a wider range of systems than Hamiltonian-based neural
networks. In addition, the training progresses of our method can be used as an
indication of the number of constants of motion in a system which could be
useful in studying a novel physical system.",https://github.com/machine-discovery/comet/,-1
Stain Isolation-based Guidance for Improved Stain Translation,0.17324,"Unsupervised and unpaired domain translation using generative adversarial
neural networks, and more precisely CycleGAN, is state of the art for the stain
translation of histopathology images. It often, however, suffers from the
presence of cycle-consistent but non structure-preserving errors. We propose an
alternative approach to the set of methods which, relying on segmentation
consistency, enable the preservation of pathology structures. Focusing on
immunohistochemistry (IHC) and multiplexed immunofluorescence (mIF), we
introduce a simple yet effective guidance scheme as a loss function that
leverages the consistency of stain translation with stain isolation.
Qualitative and quantitative experiments show the ability of the proposed
approach to improve translation between the two domains.",None,-1
Sockeye 3: Fast Neural Machine Translation with PyTorch,0.0147661,"Sockeye 3 is the latest version of the Sockeye toolkit for Neural Machine
Translation (NMT). Now based on PyTorch, Sockeye 3 provides faster model
implementations and more advanced features with a further streamlined codebase.
This enables broader experimentation with faster iteration, efficient training
of stronger and faster models, and the flexibility to move new ideas quickly
from research to production. When running comparable models, Sockeye 3 is up to
126% faster than other PyTorch implementations on GPUs and up to 292% faster on
CPUs. Sockeye 3 is open source software released under the Apache 2.0 license.",https://github.com/awslabs/sockeye,-1
Controllable User Dialogue Act Augmentation for Dialogue State Tracking,0.0925982,"Prior work has demonstrated that data augmentation is useful for improving
dialogue state tracking. However, there are many types of user utterances,
while the prior method only considered the simplest one for augmentation,
raising the concern about poor generalization capability. In order to better
cover diverse dialogue acts and control the generation quality, this paper
proposes controllable user dialogue act augmentation (CUDA-DST) to augment user
utterances with diverse behaviors. With the augmented data, different state
trackers gain improvement and show better robustness, achieving the
state-of-the-art performance on MultiWOZ 2.1",https://github.com/MiuLab/CUDA-DST,-1
RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering,0.264317,"Finding accurate correspondences among different views is the Achilles' heel
of unsupervised Multi-View Stereo (MVS). Existing methods are built upon the
assumption that corresponding pixels share similar photometric features.
However, multi-view images in real scenarios observe non-Lambertian surfaces
and experience occlusions. In this work, we propose a novel approach with
neural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences
among views. Specifically, we impose a depth rendering consistency loss to
constrain the geometry features close to the object surface to alleviate
occlusions. Concurrently, we introduce a reference view synthesis loss to
generate consistent supervision, even for non-Lambertian surfaces. Extensive
experiments on DTU and Tanks\&Temples benchmarks demonstrate that our RC-MVSNet
approach achieves state-of-the-art performance over unsupervised MVS frameworks
and competitive performance to many supervised methods.The code is released at
https://github.com/Boese0601/RC-MVSNet",https://github.com/Boese0601/RC-MVSNet,-1
Dispersed Pixel Perturbation-based Imperceptible Backdoor Trigger for Image Classifier Models,0.229456,"Typical deep neural network (DNN) backdoor attacks are based on triggers
embedded in inputs. Existing imperceptible triggers are computationally
expensive or low in attack success. In this paper, we propose a new backdoor
trigger, which is easy to generate, imperceptible, and highly effective. The
new trigger is a uniformly randomly generated three-dimensional (3D) binary
pattern that can be horizontally and/or vertically repeated and mirrored and
superposed onto three-channel images for training a backdoored DNN model.
Dispersed throughout an image, the new trigger produces weak perturbation to
individual pixels, but collectively holds a strong recognizable pattern to
train and activate the backdoor of the DNN. We also analytically reveal that
the trigger is increasingly effective with the improving resolution of the
images. Experiments are conducted using the ResNet-18 and MLP models on the
MNIST, CIFAR-10, and BTSR datasets. In terms of imperceptibility, the new
trigger outperforms existing triggers, such as BadNets, Trojaned NN, and Hidden
Backdoor, by over an order of magnitude. The new trigger achieves an almost
100% attack success rate, only reduces the classification accuracy by less than
0.7%-2.4%, and invalidates the state-of-the-art defense techniques.",None,-1
Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone Disambiguation,0.156703,"Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in
Mandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest
challenges is the task of polyphone disambiguation. Most of the previous
polyphone disambiguation models are trained on manually annotated datasets, and
publicly available datasets for polyphone disambiguation are scarce. In this
paper we propose a simple back-translation-style data augmentation method for
mandarin Chinese polyphone disambiguation, utilizing a large amount of
unlabeled text data. Inspired by the back-translation technique proposed in the
field of machine translation, we build a Grapheme-to-Phoneme (G2P) model to
predict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme
(P2G) model to predict pronunciation into text. Meanwhile, a window-based
matching strategy and a multi-model scoring strategy are proposed to judge the
correctness of the pseudo-label. We design a data balance strategy to improve
the accuracy of some typical polyphonic characters in the training set with
imbalanced distribution or data scarcity. The experimental result shows the
effectiveness of the proposed back-translation-style data augmentation method.",None,-1
Neuro-Symbolic Verification of Deep Neural Networks,0.143792,"Formal verification has emerged as a powerful approach to ensure the safety
and reliability of deep neural networks. However, current verification tools
are limited to only a handful of properties that can be expressed as
first-order constraints over the inputs and output of a network. While
adversarial robustness and fairness fall under this category, many real-world
properties (e.g., ""an autonomous vehicle has to stop in front of a stop sign"")
remain outside the scope of existing verification technology. To mitigate this
severe practical restriction, we introduce a novel framework for verifying
neural networks, named neuro-symbolic verification. The key idea is to use
neural networks as part of the otherwise logical specification, enabling the
verification of a wide variety of complex, real-world properties, including the
one above. Moreover, we demonstrate how neuro-symbolic verification can be
implemented on top of existing verification infrastructure for neural networks,
making our framework easily accessible to researchers and practitioners alike.",https://github.com/LebronX/Neuro-Symbolic-Verification,-1
Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification,0.0576919,"Semi-supervised learning (SSL) is a common approach to learning predictive
models using not only labeled examples, but also unlabeled examples. While SSL
for the simple tasks of classification and regression has received a lot of
attention from the research community, this is not properly investigated for
complex prediction tasks with structurally dependent variables. This is the
case of multi-label classification and hierarchical multi-label classification
tasks, which may require additional information, possibly coming from the
underlying distribution in the descriptive space provided by unlabeled
examples, to better face the challenging task of predicting simultaneously
multiple class labels.
  In this paper, we investigate this aspect and propose a (hierarchical)
multi-label classification method based on semi-supervised learning of
predictive clustering trees. We also extend the method towards ensemble
learning and propose a method based on the random forest approach. Extensive
experimental evaluation conducted on 23 datasets shows significant advantages
of the proposed method and its extension with respect to their supervised
counterparts. Moreover, the method preserves interpretability and reduces the
time complexity of classical tree-based models.",https://github.com/knowledge-technologies/clus,-1
"Continual Learning, Fast and Slow",0.90107,"According to the Complementary Learning Systems (CLS)
theory~\cite{mcclelland1995there} in neuroscience, humans do effective
\emph{continual learning} through two complementary systems: a fast learning
system centered on the hippocampus for rapid learning of the specifics,
individual experiences; and a slow learning system located in the neocortex for
the gradual acquisition of structured knowledge about the environment.
Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a
general continual learning framework comprising a fast learning system for
supervised learning of pattern-separated representation from specific tasks and
a slow learning system for representation learning of task-agnostic general
representation via Self-Supervised Learning (SSL). DualNets can seamlessly
incorporate both representation types into a holistic framework to facilitate
better continual learning in deep neural networks. Via extensive experiments,
we demonstrate the promising results of DualNets on a wide range of continual
learning protocols, ranging from the standard offline, task-aware setting to
the challenging online, task-free scenario. Notably, on the
CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly
different visual images, DualNets can achieve competitive performance with
existing state-of-the-art dynamic architecture
strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive
ablation studies to validate DualNets efficacy, robustness, and scalability.
Code will be made available at \url{https://github.com/phquang/DualNet}.",https://github.com/phquang/DualNet,-1
Relighting4D: Neural Relightable Human from Videos,0.0817647,"Human relighting is a highly desirable yet challenging task. Existing works
either require expensive one-light-at-a-time (OLAT) captured data using light
stage or cannot freely change the viewpoints of the rendered body. In this
work, we propose a principled framework, Relighting4D, that enables
free-viewpoints relighting from only human videos under unknown illuminations.
Our key insight is that the space-time varying geometry and reflectance of the
human body can be decomposed as a set of neural fields of normal, occlusion,
diffuse, and specular maps. These neural fields are further integrated into
reflectance-aware physically based rendering, where each vertex in the neural
field absorbs and reflects the light from the environment. The whole framework
can be learned from videos in a self-supervised manner, with physically
informed priors designed for regularization. Extensive experiments on both real
and synthetic datasets demonstrate that our framework is capable of relighting
dynamic human actors with free-viewpoints.",None,-1
Tabula: Efficiently Computing Nonlinear Activation Functions for Secure Neural Network Inference,0.0308655,"Multiparty computation approaches to secure neural network inference
traditionally rely on garbled circuits for securely executing nonlinear
activation functions. However, garbled circuits require excessive communication
between server and client, impose significant storage overheads, and incur
large runtime penalties. To eliminate these costs, we propose an alternative to
garbled circuits: Tabula, an algorithm based on secure lookup tables. Tabula
leverages neural networks' ability to be quantized and employs a secure lookup
table approach to efficiently, securely, and accurately compute neural network
nonlinear activation functions. Compared to garbled circuits with quantized
inputs, when computing individual nonlinear functions, our experiments show
Tabula uses between $35 \times$-$70 \times$ less communication, is over
$100\times$ faster, and uses a comparable amount of storage. This leads to
significant performance gains over garbled circuits with quantized inputs
during secure inference on neural networks: Tabula reduces overall
communication by up to $9 \times$ and achieves a speedup of up to $50 \times$,
while imposing comparable storage costs.",https://github.com/tabulainference/tabula,-1
Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search,0.0270512,"Complex reasoning problems contain states that vary in the computational cost
required to determine a good action plan. Taking advantage of this property, we
propose Adaptive Subgoal Search (AdaSubS), a search method that adaptively
adjusts the planning horizon. To this end, AdaSubS generates diverse sets of
subgoals at different distances. A verification mechanism is employed to filter
out unreachable subgoals swiftly, allowing to focus on feasible further
subgoals. In this way, AdaSubS benefits from the efficiency of planning with
longer subgoals and the fine control with the shorter ones, and thus scales
well to difficult planning problems. We show that AdaSubS significantly
surpasses hierarchical planning algorithms on three complex reasoning tasks:
Sokoban, the Rubik's Cube, and inequality proving benchmark INT.",https://github.com/AdaptiveSubgoalSearch/adaptive_subs,-1
Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database,0.0577684,"Parsing natural language questions into executable logical forms is a useful
and interpretable way to perform question answering on structured data such as
knowledge bases (KB) or databases (DB). However, existing approaches on
semantic parsing cannot adapt to both modalities, as they suffer from the
exponential growth of the logical form candidates and can hardly generalize to
unseen data. In this work, we propose Uni-Parser, a unified semantic parser for
question answering (QA) on both KB and DB. We introduce the primitive (relation
and entity in KB, and table name, column name and cell value in DB) as an
essential element in our framework. The number of primitives grows linearly
with the number of retrieved relations in KB and DB, preventing us from dealing
with exponential logic form candidates. We leverage the generator to predict
final logical forms by altering and composing topranked primitives with
different operations (e.g. select, where, count). With sufficiently pruned
search space by a contrastive primitive ranker, the generator is empowered to
capture the composition of primitives enhancing its generalization ability. We
achieve competitive results on multiple KB and DB QA benchmarks more
efficiently, especially in the compositional and zero-shot settings.",https://github.com/ServiceNow/picard/,-1
GreenKGC: A Lightweight Knowledge Graph Completion Method,0.17274,"Knowledge graph completion (KGC) aims to discover missing relationships
between entities in knowledge graphs (KGs). Most prior KGC work focuses on
learning embeddings for entities and relations through a simple scoring
function. Yet, a higher-dimensional embedding space is usually required for a
better reasoning capability, which leads to a larger model size and hinders
applicability to real-world problems (e.g., large-scale KGs or mobile/edge
computing). A lightweight modularized KGC solution, called GreenKGC, is
proposed in this work to address this issue. GreenKGC consists of three
modules: representation learning, feature pruning, and decision learning, to
extract discriminant KG features and make accurate predictions on missing
relationships using classifiers and negative sampling. Experimental results
demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in
most datasets. In addition, low-dimensional GreenKGC can achieve competitive or
even better performance against high-dimensional models with a much smaller
model size.",https://github.com/yunchengwang/GreenKGC,-1
Converge to the Truth: Factual Error Correction via Iterative Constrained Editing,0.028256,"Given a possibly false claim sentence, how can we automatically correct it
with minimal editing? Existing methods either require a large number of pairs
of false and corrected claims for supervised training or do not handle well
errors spanning over multiple tokens within an utterance. In this paper, we
propose VENCE, a novel method for factual error correction (FEC) with minimal
edits. VENCE formulates the FEC problem as iterative sampling editing actions
with respect to a target density function. We carefully design the target
function with predicted truthfulness scores from an offline trained fact
verification model. VENCE samples the most probable editing positions based on
back-calculated gradients of the truthfulness score concerning input tokens and
the editing actions using a distantly-supervised language model (T5).
Experiments on a public dataset show that VENCE improves the well-adopted SARI
metric by 5.3 (or a relative improvement of 11.8%) over the previous best
distantly-supervised methods.",https://github.com/jiangjiechen/VENCE,-1
IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation,0.24642,"The T5 model and its unified text-to-text paradigm contributed in advancing
the state-of-the-art for many natural language processing tasks. While some
multilingual variants of the T5 model have recently been introduced, their
performances were found to provide suboptimal performances for languages other
than English if compared to monolingual variants. We are motivated by these
findings to introduce IT5, the first family of encoder-decoder transformer
models pretrained specifically on Italian. We perform a thorough cleaning of a
web-crawled Italian corpus including more than 40 billion words and use it to
pretrain three IT5 models of different sizes. The performance of IT5 models and
their multilingual counterparts is then evaluated on a broad range of natural
language understanding and generation benchmarks for Italian. We find the
monolingual IT5 models to provide the best scale-to-performance ratio across
tested models, consistently outperforming their multilingual counterparts and
setting a new state-of-the-art for most Italian conditional language generation
tasks.",https://github.com/gsarti/it5,-1
NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night,0.0569667,"The semantic segmentation of nighttime scenes is a challenging problem that
is key to impactful applications like self-driving cars. Yet, it has received
little attention compared to its daytime counterpart. In this paper, we propose
NightLab, a novel nighttime segmentation framework that leverages multiple deep
learning models imbued with night-aware features to yield State-of-The-Art
(SoTA) performance on multiple night segmentation benchmarks. Notably, NightLab
contains models at two levels of granularity, i.e. image and regional, and each
level is composed of light adaptation and segmentation modules. Given a
nighttime image, the image level model provides an initial segmentation
estimate while, in parallel, a hardness detection module identifies regions and
their surrounding context that need further analysis. A regional level model
focuses on these difficult regions to provide a significantly improved
segmentation. All the models in NightLab are trained end-to-end using a set of
proposed night-aware losses without handcrafted heuristics. Extensive
experiments on the NightCity and BDD100K datasets show NightLab achieves SoTA
performance compared to concurrent methods.",https://github.com/xdeng7/NightLab,-1
"BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking",0.875486,"Estimating human motion from video is an active research area due to its many
potential applications. Most state-of-the-art methods predict human shape and
posture estimates for individual images and do not leverage the temporal
information available in video. Many ""in the wild"" sequences of human motion
are captured by a moving camera, which adds the complication of conflated
camera and human motion to the estimation. We therefore present BodySLAM, a
monocular SLAM system that jointly estimates the position, shape, and posture
of human bodies, as well as the camera trajectory. We also introduce a novel
human motion model to constrain sequential body postures and observe the scale
of the scene. Through a series of experiments on video sequences of human
motion captured by a moving monocular camera, we demonstrate that BodySLAM
improves estimates of all human body parameters and camera poses when compared
to estimating these separately.",https://github.com/dorianhenning/hpe3d,-1
C-SENN: Contrastive Self-Explaining Neural Network,0.0524778,"In this study, we use a self-explaining neural network (SENN), which learns
unsupervised concepts, to acquire concepts that are easy for people to
understand automatically. In concept learning, the hidden layer retains
verbalizable features relevant to the output, which is crucial when adapting to
real-world environments where explanations are required. However, it is known
that the interpretability of concepts output by SENN is reduced in general
settings, such as autonomous driving scenarios. Thus, this study combines
contrastive learning with concept learning to improve the readability of
concepts and the accuracy of tasks. We call this model Contrastive
Self-Explaining Neural Network (C-SENN).",None,-1
EBOCA: Evidences for BiOmedical Concepts Association Ontology,0.0375241,"There is a large number of online documents data sources available nowadays.
The lack of structure and the differences between formats are the main
difficulties to automatically extract information from them, which also has a
negative impact on its use and reuse. In the biomedical domain, the DISNET
platform emerged to provide researchers with a resource to obtain information
in the scope of human disease networks by means of large-scale heterogeneous
sources. Specifically in this domain, it is critical to offer not only the
information extracted from different sources, but also the evidence that
supports it. This paper proposes EBOCA, an ontology that describes (i)
biomedical domain concepts and associations between them, and (ii) evidences
supporting these associations; with the objective of providing an schema to
improve the publication and description of evidences and biomedical
associations in this domain. The ontology has been successfully evaluated to
ensure there are no errors, modelling pitfalls and that it meets the previously
defined functional requirements. Test data coming from a subset of DISNET and
automatic association extractions from texts has been transformed according to
the proposed ontology to create a Knowledge Graph that can be used in real
scenarios, and which has also been used for the evaluation of the presented
ontology.",None,-1
Filler Word Detection and Classification: A Dataset and Benchmark,0.0556088,"Filler words such as `uh' or `um' are sounds or words people use to signal
they are pausing to think. Finding and removing filler words from recordings is
a common and tedious task in media editing. Automatically detecting and
classifying filler words could greatly aid in this task, but few studies have
been published on this problem to date. A key reason is the absence of a
dataset with annotated filler words for model training and evaluation. In this
work, we present a novel speech dataset, PodcastFillers, with 35K annotated
filler words and 50K annotations of other sounds that commonly occur in
podcasts such as breaths, laughter, and word repetitions. We propose a pipeline
that leverages VAD and ASR to detect filler candidates and a classifier to
distinguish between filler word types. We evaluate our proposed pipeline on
PodcastFillers, compare to several baselines, and present a detailed ablation
study. In particular, we evaluate the importance of using ASR and how it
compares to a transcription-free approach resembling keyword spotting. We show
that our pipeline obtains state-of-the-art results, and that leveraging ASR
strongly outperforms a keyword spotting approach. We make PodcastFillers
publicly available, in the hope that our work serves as a benchmark for future
research.",https://github.com/podcastfillers,-1
R2-Trans:Fine-Grained Visual Categorization with Redundancy Reduction,0.083468,"Fine-grained visual categorization (FGVC) aims to discriminate similar
subcategories, whose main challenge is the large intraclass diversities and
subtle inter-class differences. Existing FGVC methods usually select
discriminant regions found by a trained model, which is prone to neglect other
potential discriminant information. On the other hand, the massive interactions
between the sequence of image patches in ViT make the resulting class-token
contain lots of redundant information, which may also impacts FGVC performance.
In this paper, we present a novel approach for FGVC, which can simultaneously
make use of partial yet sufficient discriminative information in environmental
cues and also compress the redundant information in class-token with respect to
the target. Specifically, our model calculates the ratio of high-weight regions
in a batch, adaptively adjusts the masking threshold and achieves moderate
extraction of background information in the input space. Moreover, we also use
the Information Bottleneck~(IB) approach to guide our network to learn a
minimum sufficient representations in the feature space. Experimental results
on three widely-used benchmark datasets verify that our approach can achieve
outperforming performance than other state-of-the-art approaches and baseline
models.",None,-1
DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization,0.146663,"LiDAR mapping is important yet challenging in self-driving and mobile
robotics. To tackle such a global point cloud registration problem, DeepMapping
converts the complex map estimation into a self-supervised training of simple
deep networks. Despite its broad convergence range on small datasets,
DeepMapping still cannot produce satisfactory results on large-scale datasets
with thousands of frames. This is due to the lack of loop closures and exact
cross-frame point correspondences, and the slow convergence of its global
localization network. We propose DeepMapping2 by adding two novel techniques to
address these issues: (1) organization of training batch based on map topology
from loop closing, and (2) self-supervised local-to-global point consistency
loss leveraging pairwise registration. Our experiments and ablation studies on
public datasets (KITTI, NCLT, and Nebula) demonstrate the effectiveness of our
method.",None,-1
DetIE: Multilingual Open Information Extraction Inspired by Object Detection,0.0343405,"State of the art neural methods for open information extraction (OpenIE)
usually extract triplets (or tuples) iteratively in an autoregressive or
predicate-based manner in order not to produce duplicates. In this work, we
propose a different approach to the problem that can be equally or more
successful. Namely, we present a novel single-pass method for OpenIE inspired
by object detection algorithms from computer vision. We use an order-agnostic
loss based on bipartite matching that forces unique predictions and a
Transformer-based encoder-only architecture for sequence labeling. The proposed
approach is faster and shows superior or similar performance in comparison with
state of the art models on standard benchmarks in terms of both quality metrics
and inference time. Our model sets the new state of the art performance of
67.7% F1 on CaRB evaluated as OIE2016 while being 3.35x faster at inference
than previous state of the art. We also evaluate the multilingual version of
our model in the zero-shot setting for two languages and introduce a strategy
for generating synthetic multilingual data to fine-tune the model for each
specific language. In this setting, we show performance improvement 15% on
multilingual Re-OIE2016, reaching 75% F1 for both Portuguese and Spanish
languages. Code and models are available at
https://github.com/sberbank-ai/DetIE.",https://github.com/sberbank-ai/DetIE,-1
Cross-identity Video Motion Retargeting with Joint Transformation and Synthesis,0.0755,"In this paper, we propose a novel dual-branch Transformation-Synthesis
network (TS-Net), for video motion retargeting. Given one subject video and one
driving video, TS-Net can produce a new plausible video with the subject
appearance of the subject video and motion pattern of the driving video. TS-Net
consists of a warp-based transformation branch and a warp-free synthesis
branch. The novel design of dual branches combines the strengths of
deformation-grid-based transformation and warp-free generation for better
identity preservation and robustness to occlusion in the synthesized videos. A
mask-aware similarity module is further introduced to the transformation branch
to reduce computational overhead. Experimental results on face and dance
datasets show that TS-Net achieves better performance in video motion
retargeting than several state-of-the-art models as well as its single-branch
variants. Our code is available at https://github.com/nihaomiao/WACV23_TSNet.",None,-1
An Anomaly Detection Method for Satellites Using Monte Carlo Dropout,0.0738144,"Recently, there has been a significant amount of interest in satellite
telemetry anomaly detection (AD) using neural networks (NN). For AD purposes,
the current approaches focus on either forecasting or reconstruction of the
time series, and they cannot measure the level of reliability or the
probability of correct detection. Although the Bayesian neural network
(BNN)-based approaches are well known for time series uncertainty estimation,
they are computationally intractable. In this paper, we present a tractable
approximation for BNN based on the Monte Carlo (MC) dropout method for
capturing the uncertainty in the satellite telemetry time series, without
sacrificing accuracy. For time series forecasting, we employ an NN, which
consists of several Long Short-Term Memory (LSTM) layers followed by various
dense layers. We employ the MC dropout inside each LSTM layer and before the
dense layers for uncertainty estimation. With the proposed uncertainty region
and by utilizing a post-processing filter, we can effectively capture the
anomaly points. Numerical results show that our proposed time series AD
approach outperforms the existing methods from both prediction accuracy and AD
perspectives.",None,-1
Learning Semantics-Aware Locomotion Skills from Human Demonstration,0.133459,"The semantics of the environment, such as the terrain type and property,
reveals important information for legged robots to adjust their behaviors. In
this work, we present a framework that learns semantics-aware locomotion skills
from perception for quadrupedal robots, such that the robot can traverse
through complex offroad terrains with appropriate speeds and gaits using
perception information. Due to the lack of high-fidelity outdoor simulation,
our framework needs to be trained directly in the real world, which brings
unique challenges in data efficiency and safety. To ensure sample efficiency,
we pre-train the perception model with an off-road driving dataset. To avoid
the risks of real-world policy exploration, we leverage human demonstration to
train a speed policy that selects a desired forward speed from camera image.
For maximum traversability, we pair the speed policy with a gait selector,
which selects a robust locomotion gait for each forward speed. Using only 40
minutes of human demonstration data, our framework learns to adjust the speed
and gait of the robot based on perceived terrain semantics, and enables the
robot to walk over 6km without failure at close-to-optimal speed.",None,-1
Optimizing LLVM Pass Sequences with Shackleton: A Linear Genetic Programming Framework,0.0389369,"In this paper we introduce Shackleton as a generalized framework enabling the
application of linear genetic programming -- a technique under the umbrella of
evolutionary algorithms -- to a variety of use cases. We also explore here a
novel application for this class of methods: optimizing sequences of LLVM
optimization passes. The algorithm underpinning Shackleton is discussed, with
an emphasis on the effects of different features unique to the framework when
applied to LLVM pass sequences. Combined with analysis of different
hyperparameter settings, we report the results on automatically optimizing pass
sequences using Shackleton for two software applications at differing
complexity levels. Finally, we reflect on the advantages and limitations of our
current implementation and lay out a path for further improvements. These
improvements aim to surpass hand-crafted solutions with an automatic discovery
method for an optimal pass sequence.",None,-1
Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training,0.273958,"Large-scale vision-language pre-trained (VLP) models are prone to hallucinate
non-existent visual objects when generating text based on visual information.
In this paper, we systematically study the object hallucination problem from
three aspects. First, we examine recent state-of-the-art VLP models, showing
that they still hallucinate frequently, and models achieving better scores on
standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate
how different types of image encoding in VLP influence hallucination, including
region-based, grid-based, and patch-based. Surprisingly, we find that
patch-based features perform the best and smaller patch resolution yields a
non-trivial reduction in object hallucination. Third, we decouple various VLP
objectives and demonstrate that token-level image-text alignment and controlled
generation are crucial to reducing hallucination. Based on that, we propose a
simple yet effective VLP loss named ObjMLM to further mitigate object
hallucination. Results show that it reduces object hallucination by up to 17.4%
when tested on two benchmarks (COCO Caption for in-domain and NoCaps for
out-of-domain evaluation).",https://github.com/wenliangdai/VLP-Object-Hallucination,-1
Evaluating a Synthetic Image Dataset Generated with Stable Diffusion,0.120976,"We generate synthetic images with the ""Stable Diffusion"" image generation
model using the Wordnet taxonomy and the definitions of concepts it contains.
This synthetic image database can be used as training data for data
augmentation in machine learning applications, and it is used to investigate
the capabilities of the Stable Diffusion model.
  Analyses show that Stable Diffusion can produce correct images for a large
number of concepts, but also a large variety of different representations. The
results show differences depending on the test concepts considered and problems
with very specific concepts. These evaluations were performed using a vision
transformer model for image classification.",https://github.com/borisdayma/dalle-mini,-1
Error Correction Code Transformer,0.162487,"Error correction code is a major part of the communication physical layer,
ensuring the reliable transfer of data over noisy channels. Recently, neural
decoders were shown to outperform classical decoding techniques. However, the
existing neural approaches present strong overfitting due to the exponential
training complexity, or a restrictive inductive bias due to reliance on Belief
Propagation. Recently, Transformers have become methods of choice in many
applications thanks to their ability to represent complex interactions between
elements. In this work, we propose to extend for the first time the Transformer
architecture to the soft decoding of linear codes at arbitrary block lengths.
We encode each channel's output dimension to high dimension for better
representation of the bits information to be processed separately. The
element-wise processing allows the analysis of the channel output reliability,
while the algebraic code and the interaction between the bits are inserted into
the model via an adapted masked self-attention module. The proposed approach
demonstrates the extreme power and flexibility of Transformers and outperforms
existing state-of-the-art neural decoders by large margins at a fraction of
their time complexity.",https://github.com/yoniLc/ECCT,-1
Disentangling visual and written concepts in CLIP,0.100827,"The CLIP network measures the similarity between natural text and images; in
this work, we investigate the entanglement of the representation of word images
and natural images in its image encoder. First, we find that the image encoder
has an ability to match word images with natural images of scenes described by
those words. This is consistent with previous research that suggests that the
meaning and the spelling of a word might be entangled deep within the network.
On the other hand, we also find that CLIP has a strong ability to match
nonsense words, suggesting that processing of letters is separated from
processing of their meaning. To explicitly determine whether the spelling
capability of CLIP is separable, we devise a procedure for identifying
representation subspaces that selectively isolate or eliminate spelling
capabilities. We benchmark our methods against a range of retrieval tasks, and
we also test them by measuring the appearance of text in CLIP-guided generated
images. We find that our methods are able to cleanly separate spelling
capabilities of CLIP from the visual processing of natural images.",None,-1
Yet Another Format of Universal Dependencies for Korean,0.0140402,"In this study, we propose a morpheme-based scheme for Korean dependency
parsing and adopt the proposed scheme to Universal Dependencies. We present the
linguistic rationale that illustrates the motivation and the necessity of
adopting the morpheme-based format, and develop scripts that convert between
the original format used by Universal Dependencies and the proposed
morpheme-based format automatically. The effectiveness of the proposed format
for Korean dependency parsing is then testified by both statistical and neural
models, including UDPipe and Stanza, with our carefully constructed
morpheme-based word embedding for Korean. morphUD outperforms parsing results
for all Korean UD treebanks, and we also present detailed error analyses.",https://github.com/jungyeul/morphUD-korean,-1
Point3D: tracking actions as moving points with 3D CNNs,0.0,"Spatio-temporal action recognition has been a challenging task that involves
detecting where and when actions occur. Current state-of-the-art action
detectors are mostly anchor-based, requiring sensitive anchor designs and huge
computations due to calculating large numbers of anchor boxes. Motivated by
nascent anchor-free approaches, we propose Point3D, a flexible and
computationally efficient network with high precision for spatio-temporal
action recognition. Our Point3D consists of a Point Head for action
localization and a 3D Head for action classification. Firstly, Point Head is
used to track center points and knot key points of humans to localize the
bounding box of an action. These location features are then piped into a
time-wise attention to learn long-range dependencies across frames. The 3D Head
is later deployed for the final action classification. Our Point3D achieves
state-of-the-art performance on the JHMDB, UCF101-24, and AVA benchmarks in
terms of frame-mAP and video-mAP. Comprehensive ablation studies also
demonstrate the effectiveness of each module proposed in our Point3D.",None,-1
Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing N-gram Language Models,0.0378315,"Although over 300M around the world speak Bangla, scant work has been done in
improving Bangla voice-to-text transcription due to Bangla being a low-resource
language. However, with the introduction of the Bengali Common Voice 9.0 speech
dataset, Automatic Speech Recognition (ASR) models can now be significantly
improved. With 399hrs of speech recordings, Bengali Common Voice is the largest
and most diversified open-source Bengali speech corpus in the world. In this
paper, we outperform the SOTA pretrained Bengali ASR models by finetuning a
pretrained wav2vec2 model on the common voice dataset. We also demonstrate how
to significantly improve the performance of an ASR model by adding an n-gram
language model as a post-processor. Finally, we do some experiments and
hyperparameter tuning to generate a robust Bangla ASR model that is better than
the existing ASR models.",None,-1
Disentangled Latent Transformer for Interpretable Monocular Height Estimation,0.140766,"Monocular height estimation (MHE) from remote sensing imagery has high
potential in generating 3D city models efficiently for a quick response to
natural disasters. Most existing works pursue higher performance. However,
there is little research exploring the interpretability of MHE networks. In
this paper, we target at exploring how deep neural networks predict height from
a single monocular image. Towards a comprehensive understanding of MHE
networks, we propose to interpret them from multiple levels: 1) Neurons:
unit-level dissection. Exploring the semantic and height selectivity of the
learned internal deep representations; 2) Instances: object-level
interpretation. Studying the effects of different semantic classes, scales, and
spatial contexts on height estimation; 3) Attribution: pixel-level analysis.
Understanding which input pixels are important for the height estimation. Based
on the multi-level interpretation, a disentangled latent Transformer network is
proposed towards a more compact, reliable, and explainable deep model for
monocular height estimation. Furthermore, a novel unsupervised semantic
segmentation task based on height estimation is first introduced in this work.
Additionally, we also construct a new dataset for joint semantic segmentation
and height estimation. Our work provides novel insights for both understanding
and designing MHE models.",https://github.com/ShadowXZT/DLT-Height-Estimation.pytorch,-1
Statistical guarantees for sparse deep learning,0.0236104,"Neural networks are becoming increasingly popular in applications, but our
mathematical understanding of their potential and limitations is still limited.
In this paper, we further this understanding by developing statistical
guarantees for sparse deep learning. In contrast to previous work, we consider
different types of sparsity, such as few active connections, few active nodes,
and other norm-based types of sparsity. Moreover, our theories cover important
aspects that previous theories have neglected, such as multiple outputs,
regularization, and l2-loss. The guarantees have a mild dependence on network
widths and depths, which means that they support the application of sparse but
wide and deep networks from a statistical perspective. Some of the concepts and
tools that we use in our derivations are uncommon in deep learning and, hence,
might be of additional interest.",None,-1
PARAGEN : A Parallel Generation Toolkit,0.19709,"PARAGEN is a PyTorch-based NLP toolkit for further development on parallel
generation. PARAGEN provides thirteen types of customizable plugins, helping
users to experiment quickly with novel ideas across model architectures,
optimization, and learning strategies. We implement various features, such as
unlimited data loading and automatic model selection, to enhance its industrial
usage. ParaGen is now deployed to support various research and industry
applications at ByteDance. PARAGEN is available at
https://github.com/bytedance/ParaGen.",https://github.com/bytedance/ParaGen,-1
Towards Practical Differential Privacy in Data Analysis: Understanding the Effect of Epsilon on Utility in Private ERM,0.0375582,"In this paper, we focus our attention on private Empirical Risk Minimization
(ERM), which is one of the most commonly used data analysis method. We take the
first step towards solving the above problem by theoretically exploring the
effect of epsilon (the parameter of differential privacy that determines the
strength of privacy guarantee) on utility of the learning model. We trace the
change of utility with modification of epsilon and reveal an established
relationship between epsilon and utility. We then formalize this relationship
and propose a practical approach for estimating the utility under an arbitrary
value of epsilon. Both theoretical analysis and experimental results
demonstrate high estimation accuracy and broad applicability of our approach in
practical applications. As providing algorithms with strong utility guarantees
that also give privacy when possible becomes more and more accepted, our
approach would have high practical value and may be likely to be adopted by
companies and organizations that would like to preserve privacy but are
unwilling to compromise on utility.",None,-1
"Fast, Accurate and Memory-Efficient Partial Permutation Synchronization",0.058781,"Previous partial permutation synchronization (PPS) algorithms, which are
commonly used for multi-object matching, often involve computation-intensive
and memory-demanding matrix operations. These operations become intractable for
large scale structure-from-motion datasets. For pure permutation
synchronization, the recent Cycle-Edge Message Passing (CEMP) framework
suggests a memory-efficient and fast solution. Here we overcome the restriction
of CEMP to compact groups and propose an improved algorithm, CEMP-Partial, for
estimating the corruption levels of the observed partial permutations. It
allows us to subsequently implement a nonconvex weighted projected power method
without the need of spectral initialization. The resulting new PPS algorithm,
MatchFAME (Fast, Accurate and Memory-Efficient Matching), only involves sparse
matrix operations, and thus enjoys lower time and space complexities in
comparison to previous PPS algorithms. We prove that under adversarial
corruption, though without additive noise and with certain assumptions,
CEMP-Partial is able to exactly classify corrupted and clean partial
permutations. We demonstrate the state-of-the-art accuracy, speed and memory
efficiency of our method on both synthetic and real datasets.",None,-1
Momentum Calibration for Text Generation,0.0269251,"The input and output of most text generation tasks can be transformed to two
sequences of tokens and they can be modeled using sequence-to-sequence learning
modeling tools such as Transformers. These models are usually trained by
maximizing the likelihood the output text sequence and assumes the input
sequence and all gold preceding tokens are given during training, while during
inference the model suffers from the exposure bias problem (i.e., it only has
access to its previously predicted tokens rather gold tokens during beam
search). In this paper, we propose MoCa ({\bf Mo}mentum {\bf Ca}libration) for
text generation. MoCa is an online method that dynamically generates slowly
evolving (but consistent) samples using a momentum moving average generator
with beam search and MoCa learns to align its model scores of these samples
with their actual qualities. Experiments on four text generation datasets
(i.e., CNN/DailyMail, XSum, SAMSum and Gigaword) show MoCa consistently
improves strong pre-trained transformers using vanilla fine-tuning and we
achieve the state-of-the-art results on CNN/DailyMail and SAMSum datasets.",None,-1
Graph-in-Graph Network for Automatic Gene Ontology Description Generation,0.028141,"Gene Ontology (GO) is the primary gene function knowledge base that enables
computational tasks in biomedicine. The basic element of GO is a term, which
includes a set of genes with the same function. Existing research efforts of GO
mainly focus on predicting gene term associations. Other tasks, such as
generating descriptions of new terms, are rarely pursued. In this paper, we
propose a novel task: GO term description generation. This task aims to
automatically generate a sentence that describes the function of a GO term
belonging to one of the three categories, i.e., molecular function, biological
process, and cellular component. To address this task, we propose a
Graph-in-Graph network that can efficiently leverage the structural information
of GO. The proposed network introduces a two-layer graph: the first layer is a
graph of GO terms where each node is also a graph (gene graph). Such a
Graph-in-Graph network can derive the biological functions of GO terms and
generate proper descriptions. To validate the effectiveness of the proposed
network, we build three large-scale benchmark datasets. By incorporating the
proposed Graph-in-Graph network, the performances of seven different
sequence-to-sequence models can be substantially boosted across all evaluation
metrics, with up to 34.7%, 14.5%, and 39.1% relative improvements in BLEU,
ROUGE-L, and METEOR, respectively.",None,-1
Unsupervised Homography Estimation with Coplanarity-Aware GAN,0.287097,"Estimating homography from an image pair is a fundamental problem in image
alignment. Unsupervised learning methods have received increasing attention in
this field due to their promising performance and label-free training. However,
existing methods do not explicitly consider the problem of plane-induced
parallax, which will make the predicted homography compromised on multiple
planes. In this work, we propose a novel method HomoGAN to guide unsupervised
homography estimation to focus on the dominant plane. First, a multi-scale
transformer network is designed to predict homography from the feature pyramids
of input images in a coarse-to-fine fashion. Moreover, we propose an
unsupervised GAN to impose coplanarity constraint on the predicted homography,
which is realized by using a generator to predict a mask of aligned regions,
and then a discriminator to check if two masked feature maps are induced by a
single homography. To validate the effectiveness of HomoGAN and its components,
we conduct extensive experiments on a large-scale dataset, and the results show
that our matching error is 22% lower than the previous SOTA method. Code is
available at https://github.com/megvii-research/HomoGAN.",https://github.com/megvii-research/HomoGAN,-1
Keyword localisation in untranscribed speech using visually grounded speech models,0.0272242,"Keyword localisation is the task of finding where in a speech utterance a
given query keyword occurs. We investigate to what extent keyword localisation
is possible using a visually grounded speech (VGS) model. VGS models are
trained on unlabelled images paired with spoken captions. These models are
therefore self-supervised -- trained without any explicit textual label or
location information. To obtain training targets, we first tag training images
with soft text labels using a pretrained visual classifier with a fixed
vocabulary. This enables a VGS model to predict the presence of a written
keyword in an utterance, but not its location. We consider four ways to equip
VGS models with localisations capabilities. Two of these -- a saliency approach
and input masking -- can be applied to an arbitrary prediction model after
training, while the other two -- attention and a score aggregation approach --
are incorporated directly into the structure of the model. Masked-based
localisation gives some of the best reported localisation scores from a VGS
model, with an accuracy of 57% when the system knows that a keyword occurs in
an utterance and need to predict its location. In a setting where localisation
is performed after detection, an $F_1$ of 25% is achieved, and in a setting
where a keyword spotting ranking pass is first performed, we get a localisation
P@10 of 32%. While these scores are modest compared to the idealised setting
with unordered bag-of-word-supervision (from transcriptions), these models do
not receive any textual or location supervision. Further analyses show that
these models are limited by the first detection or ranking pass. Moreover,
individual keyword localisation performance is correlated with the tagging
performance from the visual classifier. We also show qualitatively how and
where semantic mistakes occur, e.g. that the model locates surfer when queried
with ocean.",https://github.com/kayodeolaleye/,-1
Dynamic Collaborative Multi-Agent Reinforcement Learning Communication for Autonomous Drone Reforestation,0.25135,"We approach autonomous drone-based reforestation with a collaborative
multi-agent reinforcement learning (MARL) setup. Agents can communicate as part
of a dynamically changing network. We explore collaboration and communication
on the back of a high-impact problem. Forests are the main resource to control
rising CO2 conditions. Unfortunately, the global forest volume is decreasing at
an unprecedented rate. Many areas are too large and hard to traverse to plant
new trees. To efficiently cover as much area as possible, here we propose a
Graph Neural Network (GNN) based communication mechanism that enables
collaboration. Agents can share location information on areas needing
reforestation, which increases viewed area and planted tree count. We compare
our proposed communication mechanism with a multi-agent baseline without the
ability to communicate. Results show how communication enables collaboration
and increases collective performance, planting precision and the risk-taking
propensity of individual agents.",None,-1
Online Segmentation of LiDAR Sequences: Dataset and Algorithm,0.0778331,"Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles.
However, most semantic datasets and algorithms used for LiDAR sequence
segmentation operate on $360^\circ$ frames, causing an acquisition latency
incompatible with real-time applications. To address this issue, we first
introduce HelixNet, a $10$ billion point dataset with fine-grained labels,
timestamps, and sensor rotation information necessary to accurately assess the
real-time readiness of segmentation algorithms. Second, we propose Helix4D, a
compact and efficient spatio-temporal transformer architecture specifically
designed for rotating LiDAR sequences. Helix4D operates on acquisition slices
corresponding to a fraction of a full sensor rotation, significantly reducing
the total latency. Helix4D reaches accuracy on par with the best segmentation
algorithms on HelixNet and SemanticKITTI with a reduction of over $5\times$ in
terms of latency and $50\times$ in model size. The code and data are available
at: https://romainloiseau.fr/helixnet",https://romainloiseau.fr/helixnet,-1
AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages,0.318618,"In recent years, multilingual pre-trained language models have gained
prominence due to their remarkable performance on numerous downstream Natural
Language Processing tasks (NLP). However, pre-training these large multilingual
language models requires a lot of training data, which is not available for
African Languages. Active learning is a semi-supervised learning algorithm, in
which a model consistently and dynamically learns to identify the most
beneficial samples to train itself on, in order to achieve better optimization
and performance on downstream tasks. Furthermore, active learning effectively
and practically addresses real-world data scarcity. Despite all its benefits,
active learning, in the context of NLP and especially multilingual language
models pretraining, has received little consideration. In this paper, we
present AfroLM, a multilingual language model pretrained from scratch on 23
African languages (the largest effort to date) using our novel self-active
learning framework. Pretrained on a dataset significantly (14x) smaller than
existing baselines, AfroLM outperforms many multilingual pretrained language
models (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text
classification, and sentiment analysis). Additional out-of-domain sentiment
analysis experiments show that \textbf{AfroLM} is able to generalize well
across various domains. We release the code source, and our datasets used in
our framework at https://github.com/bonaventuredossou/MLM_AL.",https://github.com/bonaventuredossou/MLM_AL,-1
MorDeephy: Face Morphing Detection Via Fused Classification,0.0596139,"Face morphing attack detection (MAD) is one of the most challenging tasks in
the field of face recognition nowadays. In this work, we introduce a novel deep
learning strategy for a single image face morphing detection, which implies the
discrimination of morphed face images along with a sophisticated face
recognition task in a complex classification scheme. It is directed onto
learning the deep facial features, which carry information about the
authenticity of these features. Our work also introduces several additional
contributions: the public and easy-to-use face morphing detection benchmark and
the results of our wild datasets filtering strategy. Our method, which we call
MorDeephy, achieved the state of the art performance and demonstrated a
prominent ability for generalising the task of morphing detection to unseen
scenarios.",None,-1
Controllable and Guided Face Synthesis for Unconstrained Face Recognition,0.200178,"Although significant advances have been made in face recognition (FR), FR in
unconstrained environments remains challenging due to the domain gap between
the semi-constrained training datasets and unconstrained testing scenarios. To
address this problem, we propose a controllable face synthesis model (CFSM)
that can mimic the distribution of target datasets in a style latent space.
CFSM learns a linear subspace with orthogonal bases in the style latent space
with precise control over the diversity and degree of synthesis. Furthermore,
the pre-trained synthesis model can be guided by the FR model, making the
resulting images more beneficial for FR model training. Besides, target dataset
distributions are characterized by the learned orthogonal bases, which can be
utilized to measure the distributional similarity among face datasets. Our
approach yields significant performance gains on unconstrained benchmarks, such
as IJB-B, IJB-C, TinyFace and IJB-S (+5.76% Rank1).",None,-1
Overlap-guided Gaussian Mixture Models for Point Cloud Registration,0.325637,"Probabilistic 3D point cloud registration methods have shown competitive
performance in overcoming noise, outliers, and density variations. However,
registering point cloud pairs in the case of partial overlap is still a
challenge. This paper proposes a novel overlap-guided probabilistic
registration approach that computes the optimal transformation from matched
Gaussian Mixture Model (GMM) parameters. We reformulate the registration
problem as the problem of aligning two Gaussian mixtures such that a
statistical discrepancy measure between the two corresponding mixtures is
minimized. We introduce a Transformer-based detection module to detect
overlapping regions, and represent the input point clouds using GMMs by guiding
their alignment through overlap scores computed by this detection module.
Experiments show that our method achieves superior registration accuracy and
efficiency than state-of-the-art methods when handling point clouds with
partial overlap and different densities on synthetic and real-world datasets.
https://github.com/gfmei/ogmm",https://github.com/gfmei/ogmm,-1
DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models,0.295265,"We introduce DoWhy-GCM, an extension of the DoWhy Python library, that
leverages graphical causal models. Unlike existing causality libraries, which
mainly focus on effect estimation questions, with DoWhy-GCM, users can ask a
wide range of additional causal questions, such as identifying the root causes
of outliers and distributional changes, causal structure learning, attributing
causal influences, and diagnosis of causal structures. To this end, DoWhy-GCM
users first model cause-effect relations between variables in a system under
study through a graphical causal model, fit the causal mechanisms of variables
next, and then ask the causal question. All these steps take only a few lines
of code in DoWhy-GCM.
  The library is available at https://github.com/py-why/dowhy.",https://github.com/py-why/dowhy,-1
Object-Guided Day-Night Visual Localization in Urban Scenes,0.0865136,"We introduce Object-Guided Localization (OGuL) based on a novel method of
local-feature matching. Direct matching of local features is sensitive to
significant changes in illumination. In contrast, object detection often
survives severe changes in lighting conditions. The proposed method first
detects semantic objects and establishes correspondences of those objects
between images. Object correspondences provide local coarse alignment of the
images in the form of a planar homography. These homographies are consequently
used to guide the matching of local features. Experiments on standard urban
localization datasets (Aachen, Extended-CMU-Season, RobotCar-Season) show that
OGuL significantly improves localization results with as simple local features
as SIFT, and its performance competes with the state-of-the-art CNN-based
methods trained for day-to-night localization.",None,-1
"The $(1+(λ,λ))$ Global SEMO Algorithm",0.0841984,"The $(1+(\lambda,\lambda))$ genetic algorithm is a recently proposed
single-objective evolutionary algorithm with several interesting properties. We
show that its main working principle, mutation with a high rate and crossover
as repair mechanism, can be transported also to multi-objective evolutionary
computation. We define the $(1+(\lambda,\lambda))$ global SEMO algorithm, a
variant of the classic global SEMO algorithm, and prove that it optimizes the
OneMinMax benchmark asymptotically faster than the global SEMO. Following the
single-objective example, we design a one-fifth rule inspired dynamic parameter
setting (to the best of our knowledge for the first time in discrete
multi-objective optimization) and prove that it further improves the runtime to
$O(n^2)$, whereas the best runtime guarantee for the global SEMO is only $O(n^2
\log n)$.",None,-1
Analysis of Randomization Effects on Sim2Real Transfer in Reinforcement Learning for Robotic Manipulation Tasks,0.0787736,"Randomization is currently a widely used approach in Sim2Real transfer for
data-driven learning algorithms in robotics. Still, most Sim2Real studies
report results for a specific randomization technique and often on a highly
customized robotic system, making it difficult to evaluate different
randomization approaches systematically. To address this problem, we define an
easy-to-reproduce experimental setup for a robotic reach-and-balance
manipulator task, which can serve as a benchmark for comparison. We compare
four randomization strategies with three randomized parameters both in
simulation and on a real robot. Our results show that more randomization helps
in Sim2Real transfer, yet it can also harm the ability of the algorithm to find
a good policy in simulation. Fully randomized simulations and fine-tuning show
differentiated results and translate better to the real robot than the other
approaches tested.",https://josifovski.github.io/sim2real-randomization-effects/,-1
Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition,0.034546,"The practice of fine-tuning Pre-trained Language Models (PLMs) from general
or domain-specific data to a specific task with limited resources, has gained
popularity within the field of natural language processing (NLP). In this work,
we re-visit this assumption and carry out an investigation in clinical NLP,
specifically Named Entity Recognition on drugs and their related attributes. We
compare Transformer models that are trained from scratch to fine-tuned
BERT-based LLMs namely BERT, BioBERT, and ClinicalBERT. Furthermore, we examine
the impact of an additional CRF layer on such models to encourage contextual
learning. We use n2c2-2018 shared task data for model development and
evaluations. The experimental outcomes show that 1) CRF layers improved all
language models; 2) referring to BIO-strict span level evaluation using
macro-average F1 score, although the fine-tuned LLMs achieved 0.83+ scores, the
TransformerCRF model trained from scratch achieved 0.78+, demonstrating
comparable performances with much lower cost - e.g. with 39.80\% less training
parameters; 3) referring to BIO-strict span-level evaluation using
weighted-average F1 score, ClinicalBERT-CRF, BERT-CRF, and TransformerCRF
exhibited lower score differences, with 97.59\%/97.44\%/96.84\% respectively.
4) applying efficient training by down-sampling for better data distribution
further reduced the training cost and need for data, while maintaining similar
scores - i.e. around 0.02 points lower compared to using the full dataset. Our
models will be hosted at \url{https://github.com/HECTA-UoM/TransformerCRF}",https://github.com/HECTA-UoM/TransformerCRF,-1
"Data Smells: Categories, Causes and Consequences, and Detection of Suspicious Data in AI-based Systems",0.0444522,"High data quality is fundamental for today's AI-based systems. However,
although data quality has been an object of research for decades, there is a
clear lack of research on potential data quality issues (e.g., ambiguous,
extraneous values). These kinds of issues are latent in nature and thus often
not obvious. Nevertheless, they can be associated with an increased risk of
future problems in AI-based systems (e.g., technical debt, data-induced
faults). As a counterpart to code smells in software engineering, we refer to
such issues as Data Smells. This article conceptualizes data smells and
elaborates on their causes, consequences, detection, and use in the context of
AI-based systems. In addition, a catalogue of 36 data smells divided into three
categories (i.e., Believability Smells, Understandability Smells, Consistency
Smells) is presented. Moreover, the article outlines tool support for detecting
data smells and presents the result of an initial smell detection on more than
240 real-world datasets.",https://github.com/mkerschbaumer/rb-data-smell-detection,-1
Lyapunov function approach for approximation algorithm design and analysis: with applications in submodular maximization,0.0570575,"We propose a two-phase systematical framework for approximation algorithm
design and analysis via Lyapunov function. The first phase consists of using
Lyapunov function as an input and outputs a continuous-time approximation
algorithm with a provable approximation ratio. The second phase then converts
this continuous-time algorithm to a discrete-time algorithm with almost the
same approximation ratio along with provable time complexity. One distinctive
feature of our framework is that we only need to know the parametric form of
the Lyapunov function whose complete specification will not be decided until
the end of the first phase by maximizing the approximation ratio of the
continuous-time algorithm. Some immediate benefits of the Lyapunov function
approach include: (i) unifying many existing algorithms; (ii) providing a
guideline to design and analyze new algorithms; and (iii) offering new
perspectives to potentially improve existing algorithms. We use various
submodular maximization problems as running examples to illustrate our
framework.",None,-1
Watermarking Pre-trained Encoders in Contrastive Learning,0.0,"Contrastive learning has become a popular technique to pre-train image
encoders, which could be used to build various downstream classification models
in an efficient way. This process requires a large amount of data and
computation resources. Hence, the pre-trained encoders are an important
intellectual property that needs to be carefully protected. It is challenging
to migrate existing watermarking techniques from the classification tasks to
the contrastive learning scenario, as the owner of the encoder lacks the
knowledge of the downstream tasks which will be developed from the encoder in
the future. We propose the \textit{first} watermarking methodology for the
pre-trained encoders. We introduce a task-agnostic loss function to effectively
embed into the encoder a backdoor as the watermark. This backdoor can still
exist in any downstream models transferred from the encoder. Extensive
evaluations over different contrastive learning algorithms, datasets, and
downstream tasks indicate our watermarks exhibit high effectiveness and
robustness against different adversarial operations.",https://github.com/facebookresearch/moco,-1
Holistic Transformer: A Joint Neural Network for Trajectory Prediction and Decision-Making of Autonomous Vehicles,0.0226889,"Trajectory prediction and behavioral decision-making are two important tasks
for autonomous vehicles that require good understanding of the environmental
context; behavioral decisions are better made by referring to the outputs of
trajectory predictions. However, most current solutions perform these two tasks
separately. Therefore, a joint neural network that combines multiple cues is
proposed and named as the holistic transformer to predict trajectories and make
behavioral decisions simultaneously. To better explore the intrinsic
relationships between cues, the network uses existing knowledge and adopts
three kinds of attention mechanisms: the sparse multi-head type for reducing
noise impact, feature selection sparse type for optimally using partial prior
knowledge, and multi-head with sigmoid activation type for optimally using
posteriori knowledge. Compared with other trajectory prediction models, the
proposed model has better comprehensive performance and good interpretability.
Perceptual noise robustness experiments demonstrate that the proposed model has
good noise robustness. Thus, simultaneous trajectory prediction and behavioral
decision-making combining multiple cues can reduce computational costs and
enhance semantic relationships between scenes and agents.",None,-1
Privacy-Preserving Model Upgrades with Bidirectional Compatible Training in Image Retrieval,0.038713,"The task of privacy-preserving model upgrades in image retrieval desires to
reap the benefits of rapidly evolving new models without accessing the raw
gallery images. A pioneering work introduced backward-compatible training,
where the new model can be directly deployed in a backfill-free manner, i.e.,
the new query can be directly compared to the old gallery features. Despite a
possible solution, its improvement in sequential model upgrades is gradually
limited by the fixed and under-quality old gallery embeddings. To this end, we
propose a new model upgrade paradigm, termed Bidirectional Compatible Training
(BiCT), which will upgrade the old gallery embeddings by forward-compatible
training towards the embedding space of the backward-compatible new model. We
conduct comprehensive experiments to verify the prominent improvement by BiCT
and interestingly observe that the inconspicuous loss weight of backward
compatibility actually plays an essential role for both backward and forward
retrieval performance. To summarize, we introduce a new and valuable problem
named privacy-preserving model upgrades, with a proper solution BiCT. Several
intriguing insights are further proposed to get the most out of our method.",None,-1
Parametrically Retargetable Decision-Makers Tend To Seek Power,0.122917,"If capable AI agents are generally incentivized to seek power in service of
the objectives we specify for them, then these systems will pose enormous
risks, in addition to enormous benefits. In fully observable environments, most
reward functions have an optimal policy which seeks power by keeping options
open and staying alive. However, the real world is neither fully observable,
nor must trained agents be even approximately reward-optimal. We consider a
range of models of AI decision-making, from optimal, to random, to choices
informed by learning and interacting with an environment. We discover that many
decision-making functions are retargetable, and that retargetability is
sufficient to cause power-seeking tendencies. Our functional criterion is
simple and broad. We show that a range of qualitatively dissimilar
decision-making procedures incentivize agents to seek power. We demonstrate the
flexibility of our results by reasoning about learned policy incentives in
Montezuma's Revenge. These results suggest a safety risk: Eventually,
retargetable training procedures may train real-world agents which seek power
over humans.",None,-1
On the Effect of Information Asymmetry in Human-AI Teams,0.164404,"Over the last years, the rising capabilities of artificial intelligence (AI)
have improved human decision-making in many application areas. Teaming between
AI and humans may even lead to complementary team performance (CTP), i.e., a
level of performance beyond the ones that can be reached by AI or humans
individually. Many researchers have proposed using explainable AI (XAI) to
enable humans to rely on AI advice appropriately and thereby reach CTP.
However, CTP is rarely demonstrated in previous work as often the focus is on
the design of explainability, while a fundamental prerequisite -- the presence
of complementarity potential between humans and AI -- is often neglected.
Therefore, we focus on the existence of this potential for effective human-AI
decision-making. Specifically, we identify information asymmetry as an
essential source of complementarity potential, as in many real-world
situations, humans have access to different contextual information. By
conducting an online experiment, we demonstrate that humans can use such
contextual information to adjust the AI's decision, finally resulting in CTP.",None,-1
EpiGNN: Exploring Spatial Transmission with Graph Neural Network for Regional Epidemic Forecasting,0.324043,"Epidemic forecasting is the key to effective control of epidemic transmission
and helps the world mitigate the crisis that threatens public health. To better
understand the transmission and evolution of epidemics, we propose EpiGNN, a
graph neural network-based model for epidemic forecasting. Specifically, we
design a transmission risk encoding module to characterize local and global
spatial effects of regions in epidemic processes and incorporate them into the
model. Meanwhile, we develop a Region-Aware Graph Learner (RAGL) that takes
transmission risk, geographical dependencies, and temporal information into
account to better explore spatial-temporal dependencies and makes regions aware
of related regions' epidemic situations. The RAGL can also combine with
external resources, such as human mobility, to further improve prediction
performance. Comprehensive experiments on five real-world epidemic-related
datasets (including influenza and COVID-19) demonstrate the effectiveness of
our proposed method and show that EpiGNN outperforms state-of-the-art baselines
by 9.48% in RMSE.",https://github.com/Xiefeng69/EpiGNN,-1
SRL-SOA: Self-Representation Learning with Sparse 1D-Operational Autoencoder for Hyperspectral Image Band Selection,0.215909,"The band selection in the hyperspectral image (HSI) data processing is an
important task considering its effect on the computational complexity and
accuracy. In this work, we propose a novel framework for the band selection
problem: Self-Representation Learning (SRL) with Sparse 1D-Operational
Autoencoder (SOA). The proposed SLR-SOA approach introduces a novel autoencoder
model, SOA, that is designed to learn a representation domain where the data
are sparsely represented. Moreover, the network composes of 1D-operational
layers with the non-linear neuron model. Hence, the learning capability of
neurons (filters) is greatly improved with shallow architectures. Using compact
architectures is especially crucial in autoencoders as they tend to overfit
easily because of their identity mapping objective. Overall, we show that the
proposed SRL-SOA band selection approach outperforms the competing methods over
two HSI data including Indian Pines and Salinas-A considering the achieved land
cover classification accuracies. The software implementation of the SRL-SOA
approach is shared publicly at https://github.com/meteahishali/SRL-SOA.",https://github.com/meteahishali/SRL-SOA,-1
Bi-level Doubly Variational Learning for Energy-based Latent Variable Models,0.0635628,"Energy-based latent variable models (EBLVMs) are more expressive than
conventional energy-based models. However, its potential on visual tasks are
limited by its training process based on maximum likelihood estimate that
requires sampling from two intractable distributions. In this paper, we propose
Bi-level doubly variational learning (BiDVL), which is based on a new bi-level
optimization framework and two tractable variational distributions to
facilitate learning EBLVMs. Particularly, we lead a decoupled EBLVM consisting
of a marginal energy-based distribution and a structural posterior to handle
the difficulties when learning deep EBLVMs on images. By choosing a symmetric
KL divergence in the lower level of our framework, a compact BiDVL for visual
tasks can be obtained. Our model achieves impressive image generation
performance over related works. It also demonstrates the significant capacity
of testing image reconstruction and out-of-distribution detection.",None,-1
Motif-topology and Reward-learning improved Spiking Neural Network for Efficient Multi-sensory Integration,0.380091,"Network architectures and learning principles are key in forming complex
functions in artificial neural networks (ANNs) and spiking neural networks
(SNNs). SNNs are considered the new-generation artificial networks by
incorporating more biological features than ANNs, including dynamic spiking
neurons, functionally specified architectures, and efficient learning
paradigms. In this paper, we propose a Motif-topology and Reward-learning
improved SNN (MR-SNN) for efficient multi-sensory integration. MR-SNN contains
13 types of 3-node Motif topologies which are first extracted from independent
single-sensory learning paradigms and then integrated for multi-sensory
classification. The experimental results showed higher accuracy and stronger
robustness of the proposed MR-SNN than other conventional SNNs without using
Motifs. Furthermore, the proposed reward learning paradigm was biologically
plausible and can better explain the cognitive McGurk effect caused by
incongruent visual and auditory sensory signals.",https://github.com/thomasaimondy/Motif-SNN,-1
Adversarial Robustness through the Lens of Convolutional Filters,0.0498931,"Deep learning models are intrinsically sensitive to distribution shifts in
the input data. In particular, small, barely perceivable perturbations to the
input data can force models to make wrong predictions with high confidence. An
common defense mechanism is regularization through adversarial training which
injects worst-case perturbations back into training to strengthen the decision
boundaries, and to reduce overfitting. In this context, we perform an
investigation of 3x3 convolution filters that form in adversarially-trained
models. Filters are extracted from 71 public models of the linf-RobustBench
CIFAR-10/100 and ImageNet1k leaderboard and compared to filters extracted from
models built on the same architectures but trained without robust
regularization. We observe that adversarially-robust models appear to form more
diverse, less sparse, and more orthogonal convolution filters than their normal
counterparts. The largest differences between robust and normal models are
found in the deepest layers, and the very first convolution layer, which
consistently and predominantly forms filters that can partially eliminate
perturbations, irrespective of the architecture. Data & Project website:
https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens",https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens,-1
AeDet: Azimuth-invariant Multi-view 3D Object Detection,0.280531,"Recent LSS-based multi-view 3D object detection has made tremendous progress,
by processing the features in Brid-Eye-View (BEV) via the convolutional
detector. However, the typical convolution ignores the radial symmetry of the
BEV features and increases the difficulty of the detector optimization. To
preserve the inherent property of the BEV features and ease the optimization,
we propose an azimuth-equivariant convolution (AeConv) and an
azimuth-equivariant anchor. The sampling grid of AeConv is always in the radial
direction, thus it can learn azimuth-invariant BEV features. The proposed
anchor enables the detection head to learn predicting azimuth-irrelevant
targets. In addition, we introduce a camera-decoupled virtual depth to unify
the depth prediction for the images with different camera intrinsic parameters.
The resultant detector is dubbed Azimuth-equivariant Detector (AeDet).
Extensive experiments are conducted on nuScenes, and AeDet achieves a 62.0%
NDS, surpassing the recent multi-view 3D object detectors such as PETRv2 and
BEVDepth by a large margin. Project page: https://fcjian.github.io/aedet.",https://fcjian.github.io/aedet/,-1
EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL,0.370522,"Reinforcement learning (RL) in long horizon and sparse reward tasks is
notoriously difficult and requires a lot of training steps. A standard solution
to speed up the process is to leverage additional reward signals, shaping it to
better guide the learning process. In the context of language-conditioned RL,
the abstraction and generalisation properties of the language input provide
opportunities for more efficient ways of shaping the reward. In this paper, we
leverage this idea and propose an automated reward shaping method where the
agent extracts auxiliary objectives from the general language goal. These
auxiliary objectives use a question generation (QG) and question answering (QA)
system: they consist of questions leading the agent to try to reconstruct
partial information about the global goal using its own trajectory. When it
succeeds, it receives an intrinsic reward proportional to its confidence in its
answer. This incentivizes the agent to generate trajectories which
unambiguously explain various aspects of the general language goal. Our
experimental study shows that this approach, which does not require engineer
intervention to design the auxiliary objectives, improves sample efficiency by
effectively directing exploration.",https://github.com/flowersteam/EAGER,-1
RerrFact: Reduced Evidence Retrieval Representations for Scientific Claim Verification,0.0518103,"Exponential growth in digital information outlets and the race to publish has
made scientific misinformation more prevalent than ever. However, the task to
fact-verify a given scientific claim is not straightforward even for
researchers. Scientific claim verification requires in-depth knowledge and
great labor from domain experts to substantiate supporting and refuting
evidence from credible scientific sources. The SciFact dataset and
corresponding task provide a benchmarking leaderboard to the community to
develop automatic scientific claim verification systems via extracting and
assimilating relevant evidence rationales from source abstracts. In this work,
we propose a modular approach that sequentially carries out binary
classification for every prediction subtask as in the SciFact leaderboard. Our
simple classifier-based approach uses reduced abstract representations to
retrieve relevant abstracts. These are further used to train the relevant
rationale-selection model. Finally, we carry out two-step stance predictions
that first differentiate non-relevant rationales and then identify supporting
or refuting rationales for a given claim. Experimentally, our system RerrFact
with no fine-tuning, simple design, and a fraction of model parameters fairs
competitively on the leaderboard against large-scale, modular, and joint
modeling approaches. We make our codebase available at
https://github.com/ashishrana160796/RerrFact.",https://github.com/ashishrana160796/RerrFact,-1
HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing,0.179063,"Deep learning algorithms are dependent on the availability of large-scale
annotated clinical text datasets. The lack of such publicly available datasets
is the biggest bottleneck for the development of clinical Natural Language
Processing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deep
learning models to classify instances from new classes of which no training
data have been seen before. Prompt-based learning is an emerging ZSL technique
where we define task-based templates for NLP tasks. We developed a novel
prompt-based clinical NLP framework called HealthPrompt and applied the
paradigm of prompt-based learning on clinical texts. In this technique, rather
than fine-tuning a Pre-trained Language Model(PLM), the task definitions are
tuned by defining a prompt template. We performed an in-depth analysis of
HealthPrompt on six different PLMs in a no-data setting. Our experiments prove
that prompts effectively capture the context of clinical texts and perform
remarkably well without any training data.",None,-1
E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning,0.418858,"The ability to recognize analogies is fundamental to human cognition.
Existing benchmarks to test word analogy do not reveal the underneath process
of analogical reasoning of neural models. Holding the belief that models
capable of reasoning should be right for the right reasons, we propose a
first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning
benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in
English) problems sourced from the Civil Service Exams, which require intensive
background knowledge to solve. More importantly, we design a free-text
explanation scheme to explain whether an analogy should be drawn, and manually
annotate them for each and every question and candidate answer. Empirical
results suggest that this benchmark is very challenging for some
state-of-the-art models for both explanation generation and analogical question
answering tasks, which invites further research in this area.",https://github.com/Tiiiger/bert_score,-1
Multiple Object Tracking from appearance by hierarchically clustering tracklets,0.0170233,"Current approaches in Multiple Object Tracking (MOT) rely on the
spatio-temporal coherence between detections combined with object appearance to
match objects from consecutive frames. In this work, we explore MOT using
object appearances as the main source of association between objects in a
video, using spatial and temporal priors as weighting factors. We form initial
tracklets by leveraging on the idea that instances of an object that are close
in time should be similar in appearance, and build the final object tracks by
fusing the tracklets in a hierarchical fashion. We conduct extensive
experiments that show the effectiveness of our method over three different MOT
benchmarks, MOT17, MOT20, and DanceTrack, being competitive in MOT17 and MOT20
and establishing state-of-the-art results in DanceTrack.",https://github.com/NII-Satoh-Lab/MOT_FCG,-1
Normalization Perturbation: A Simple Domain Generalization Method for Real-World Domain Shifts,0.030888,"Improving model's generalizability against domain shifts is crucial,
especially for safety-critical applications such as autonomous driving.
Real-world domain styles can vary substantially due to environment changes and
sensor noises, but deep models only know the training domain style. Such domain
style gap impedes model generalization on diverse real-world domains. Our
proposed Normalization Perturbation (NP) can effectively overcome this domain
style overfitting problem. We observe that this problem is mainly caused by the
biased distribution of low-level features learned in shallow CNN layers. Thus,
we propose to perturb the channel statistics of source domain features to
synthesize various latent styles, so that the trained deep model can perceive
diverse potential domains and generalizes well even without observations of
target domain data in training. We further explore the style-sensitive channels
for effective style synthesis. Normalization Perturbation only relies on a
single source domain and is surprisingly effective and extremely easy to
implement. Extensive experiments verify the effectiveness of our method for
generalizing models under real-world domain shifts.",None,-1
SAFER: Safe Collision Avoidance using Focused and Efficient Trajectory Search with Reinforcement Learning,0.00442258,"Collision avoidance is key for mobile robots and agents to operate safely in
the real world. In this work we present SAFER, an efficient and effective
collision avoidance system that is able to improve safety by correcting the
control commands sent by an operator. It combines real-world reinforcement
learning (RL), search-based online trajectory planning, and automatic emergency
intervention, e.g. automatic emergency braking (AEB). The goal of the RL is to
learn an effective corrective control action that is used in a focused search
for collision-free trajectories, and to reduce the frequency of triggering
automatic emergency braking. This novel setup enables the RL policy to learn
safely and directly on mobile robots in a real-world indoor environment,
minimizing actual crashes even during training. Our real-world experiments show
that, when compared with several baselines, our approach enjoys a higher
average speed, lower crash rate, less emergency intervention, smaller
computation overhead, and smoother overall control.",None,-1
Incorporating Explicit Knowledge in Pre-trained Language Models for Passage Re-ranking,0.0517301,"Passage re-ranking is to obtain a permutation over the candidate passage set
from retrieval stage. Re-rankers have been boomed by Pre-trained Language
Models (PLMs) due to their overwhelming advantages in natural language
understanding. However, existing PLM based re-rankers may easily suffer from
vocabulary mismatch and lack of domain specific knowledge. To alleviate these
problems, explicit knowledge contained in knowledge graph is carefully
introduced in our work. Specifically, we employ the existing knowledge graph
which is incomplete and noisy, and first apply it in passage re-ranking task.
To leverage a reliable knowledge, we propose a novel knowledge graph
distillation method and obtain a knowledge meta graph as the bridge between
query and passage. To align both kinds of embedding in the latent space, we
employ PLM as text encoder and graph neural network over knowledge meta graph
as knowledge encoder. Besides, a novel knowledge injector is designed for the
dynamic interaction between text and knowledge encoder. Experimental results
demonstrate the effectiveness of our method especially in queries requiring
in-depth domain knowledge.",https://github.com/DQ0408/KERM,-1
Feature-Style Encoder for Style-Based GAN Inversion,0.0709391,"We propose a novel architecture for GAN inversion, which we call
Feature-Style encoder. The style encoder is key for the manipulation of the
obtained latent codes, while the feature encoder is crucial for optimal image
reconstruction. Our model achieves accurate inversion of real images from the
latent space of a pre-trained style-based GAN model, obtaining better
perceptual quality and lower reconstruction error than existing methods. Thanks
to its encoder structure, the model allows fast and accurate image editing.
Additionally, we demonstrate that the proposed encoder is especially
well-suited for inversion and editing on videos. We conduct extensive
experiments for several style-based generators pre-trained on different data
domains. Our proposed method yields state-of-the-art results for style-based
GAN inversion, significantly outperforming competing approaches. Source codes
are available at https://github.com/InterDigitalInc/FeatureStyleEncoder .",None,-1
CodeS: Towards Code Model Generalization Under Distribution Shift,0.0169153,"Distribution shift has been a longstanding challenge for the reliable
deployment of deep learning (DL) models due to unexpected accuracy degradation.
Although DL has been becoming a driving force for large-scale source code
analysis in the big code era, limited progress has been made on distribution
shift analysis and benchmarking for source code tasks. To fill this gap, this
paper initiates to propose CodeS, a distribution shift benchmark dataset, for
source code learning. Specifically, CodeS supports two programming languages
(Java and Python) and five shift types (task, programmer, time-stamp, token,
and concrete syntax tree). Extensive experiments based on CodeS reveal that 1)
out-of-distribution detectors from other domains (e.g., computer vision) do not
generalize to source code, 2) all code classification models suffer from
distribution shifts, 3) representation-based shifts have a higher impact on the
model than others, and 4) pre-trained bimodal models are relatively more
resistant to distribution shifts.",None,-1
Challenges and Opportunities of Edge AI for Next-Generation Implantable BMIs,0.0802557,"Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.",None,-1
Promises and Pitfalls of Threshold-based Auto-labeling,0.0518983,"Creating large-scale high-quality labeled datasets is a major bottleneck in
supervised machine learning workflows. Threshold-based auto-labeling (TBAL),
where validation data obtained from humans is used to find a confidence
threshold above which the data is machine-labeled, reduces reliance on manual
annotation. TBAL is emerging as a widely-used solution in practice. Given the
long shelf-life and diverse usage of the resulting datasets, understanding when
the data obtained by such auto-labeling systems can be relied on is crucial.
This is the first work to analyze TBAL systems and derive sample complexity
bounds on the amount of human-labeled validation data required for guaranteeing
the quality of machine-labeled data. Our results provide two crucial insights.
First, reasonable chunks of unlabeled data can be automatically and accurately
labeled by seemingly bad models. Second, a hidden downside of TBAL systems is
potentially prohibitive validation data usage. Together, these insights
describe the promise and pitfalls of using such systems. We validate our
theoretical guarantees with extensive experiments on synthetic and real
datasets.",https://github.com/harit7/TBAL,-1
Controllable Dynamic Multi-Task Architectures,0.366017,"Multi-task learning commonly encounters competition for resources among
tasks, specifically when model capacity is limited. This challenge motivates
models which allow control over the relative importance of tasks and total
compute cost during inference time. In this work, we propose such a
controllable multi-task network that dynamically adjusts its architecture and
weights to match the desired task preference as well as the resource
constraints. In contrast to the existing dynamic multi-task approaches that
adjust only the weights within a fixed architecture, our approach affords the
flexibility to dynamically control the total computational cost and match the
user-preferred task importance better. We propose a disentangled training of
two hypernetworks, by exploiting task affinity and a novel branching
regularized loss, to take input preferences and accordingly predict
tree-structured models with adapted weights. Experiments on three multi-task
benchmarks, namely PASCAL-Context, NYU-v2, and CIFAR-100, show the efficacy of
our approach. Project page is available at https://www.nec-labs.com/~mas/DYMU.",https://www.nec-labs.com/˜mas/DYMU,-1
Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue,0.0768896,"Complex conversation settings such as persuasion involve communicating
changes in attitude or behavior, so users' perspectives need to be addressed,
even when not directly related to the topic. In this work, we contribute a
novel modular dialogue system framework that seamlessly integrates factual
information and social content into persuasive dialogue. Our framework is
generalizable to any dialogue tasks that have mixed social and task contents.
We conducted a study that compared user evaluations of our framework versus a
baseline end-to-end generation model. We found our framework was evaluated more
favorably in all dimensions including competence and friendliness, compared to
the end-to-end model which does not explicitly handle social content or factual
questions.",None,-1
Signature Entrenchment and Conceptual Changes in Automated Theory Repair,0.0441077,"Human beliefs change, but so do the concepts that underpin them. The recent
Abduction, Belief Revision and Conceptual Change (ABC) repair system combines
several methods from automated theory repair to expand, contract, or reform
logical structures representing conceptual knowledge in artificial agents. In
this paper we focus on conceptual change: repair not only of the membership of
logical concepts, such as what animals can fly, but also concepts themselves,
such that birds may be divided into flightless and flying birds, by changing
the signature of the logical theory used to represent them. We offer a method
for automatically evaluating entrenchment in the signature of a Datalog theory,
in order to constrain automated theory repair to succinct and intuitive
outcomes. Formally, signature entrenchment measures the inferential
contributions of every logical language element used to express conceptual
knowledge, i.e., predicates and the arguments, ranking possible repairs to
retain valuable logical concepts and reject redundant or implausible
alternatives. This quantitative measurement of signature entrenchment offers a
guide to the plausibility of conceptual changes, which we aim to contrast with
human judgements of concept entrenchment in future work.",https://github.com/XuerLi/Publications/tree/main/ACS2021,-1
Chart Question Answering: State of the Art and Future Directions,0.159901,"Information visualizations such as bar charts and line charts are very common
for analyzing data and discovering critical insights. Often people analyze
charts to answer questions that they have in mind. Answering such questions can
be challenging as they often require a significant amount of perceptual and
cognitive effort. Chart Question Answering (CQA) systems typically take a chart
and a natural language question as input and automatically generate the answer
to facilitate visual data analysis. Over the last few years, there has been a
growing body of literature on the task of CQA. In this survey, we
systematically review the current state-of-the-art research focusing on the
problem of chart question answering. We provide a taxonomy by identifying
several important dimensions of the problem domain including possible inputs
and outputs of the task and discuss the advantages and limitations of proposed
solutions. We then summarize various evaluation techniques used in the surveyed
papers. Finally, we outline the open challenges and future research
opportunities related to chart question answering.",None,-1
The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error,0.052317,"Discourse analysis allows us to attain inferences of a text document that
extend beyond the sentence-level. The current performance of discourse models
is very low on texts outside of the training distribution's coverage,
diminishing the practical utility of existing models. There is need for a
measure that can inform us to what extent our model generalizes from the
training to the test sample when these samples may be drawn from distinct
distributions. While this can be estimated via distribution shift, we argue
that this does not directly correlate with change in the observed error of a
classifier (i.e. error-gap). Thus, we propose to use a statistic from the
theoretical domain adaptation literature which can be directly tied to
error-gap. We study the bias of this statistic as an estimator of error-gap
both theoretically and through a large-scale empirical study of over 2400
experiments on 6 discourse datasets from domains including, but not limited to:
news, biomedical texts, TED talks, Reddit posts, and fiction. Our results not
only motivate our proposal and help us to understand its limitations, but also
provide insight on the properties of discourse models and datasets which
improve performance in domain adaptation. For instance, we find that non-news
datasets are slightly easier to transfer to than news datasets when the
training and test sets are very different. Our code and an associated Python
package are available to allow practitioners to make more informed model and
dataset choices.",https://github.com/anthonysicilia/change-that-matters-,-1
The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs,0.0306281,"Despite widespread use of LLMs as conversational agents, evaluations of
performance fail to capture a crucial aspect of communication: interpreting
language in context -- incorporating its pragmatics. Humans interpret language
using beliefs and prior knowledge about the world. For example, we intuitively
understand the response ""I wore gloves"" to the question ""Did you leave
fingerprints?"" as meaning ""No"". To investigate whether LLMs have the ability to
make this type of inference, known as an implicature, we design a simple task
and evaluate four categories of widely used state-of-the-art models. We find
that, despite only evaluating on utterances that require a binary inference
(yes or no), models in three of these categories perform close to random.
However, LLMs instruction-tuned at the example-level perform significantly
better. These results suggest that certain fine-tuning strategies are far
better at inducing pragmatic understanding in models. We present our findings
as the starting point for further research into evaluating how LLMs interpret
language in context and to drive the development of more pragmatic and useful
models of human discourse.",https://github.com/LauraRuis/do-pigs-fly,-1
AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech Translation,0.967567,"To alleviate the data scarcity problem in End-to-end speech translation (ST),
pre-training on data for speech recognition and machine translation is
considered as an important technique. However, the modality gap between speech
and text prevents the ST model from efficiently inheriting knowledge from the
pre-trained models. In this work, we propose AdaTranS for end-to-end ST. It
adapts the speech features with a new shrinking mechanism to mitigate the
length mismatch between speech and text features by predicting word boundaries.
Experiments on the MUST-C dataset demonstrate that AdaTranS achieves better
performance than the other shrinking-based methods, with higher inference speed
and lower memory usage. Further experiments also show that AdaTranS can be
equipped with additional alignment losses to further improve performance.",https://github.com/google/sentencepiece,-1
Parallel Instance Query Network for Named Entity Recognition,0.194345,"Named entity recognition (NER) is a fundamental task in natural language
processing. Recent works treat named entity recognition as a reading
comprehension task, constructing type-specific queries manually to extract
entities. This paradigm suffers from three issues. First, type-specific queries
can only extract one type of entities per inference, which is inefficient.
Second, the extraction for different types of entities is isolated, ignoring
the dependencies between them. Third, query construction relies on external
knowledge and is difficult to apply to realistic scenarios with hundreds of
entity types. To deal with them, we propose Parallel Instance Query Network
(PIQN), which sets up global and learnable instance queries to extract entities
from a sentence in a parallel manner. Each instance query predicts one entity,
and by feeding all instance queries simultaneously, we can query all entities
in parallel. Instead of being constructed from external knowledge, instance
queries can learn their different query semantics during training. For training
the model, we treat label assignment as a one-to-many Linear Assignment Problem
(LAP) and dynamically assign gold entities to instance queries with minimal
assignment cost. Experiments on both nested and flat NER datasets demonstrate
that our proposed method outperforms previous state-of-the-art models.",https://github.com/tricktreat/piqn,-1
Rethinking Knowledge Graph Evaluation Under the Open-World Assumption,0.0480693,"Most knowledge graphs (KGs) are incomplete, which motivates one important
research topic on automatically complementing knowledge graphs. However,
evaluation of knowledge graph completion (KGC) models often ignores the
incompleteness -- facts in the test set are ranked against all unknown triplets
which may contain a large number of missing facts not included in the KG yet.
Treating all unknown triplets as false is called the closed-world assumption.
This closed-world assumption might negatively affect the fairness and
consistency of the evaluation metrics. In this paper, we study KGC evaluation
under a more realistic setting, namely the open-world assumption, where unknown
triplets are considered to include many missing facts not included in the
training or test sets. For the currently most used metrics such as mean
reciprocal rank (MRR) and Hits@K, we point out that their behavior may be
unexpected under the open-world assumption. Specifically, with not many missing
facts, their numbers show a logarithmic trend with respect to the true strength
of the model, and thus, the metric increase could be insignificant in terms of
reflecting the true model improvement. Further, considering the variance, we
show that the degradation in the reported numbers may result in incorrect
comparisons between different models, where stronger models may have lower
metric numbers. We validate the phenomenon both theoretically and
experimentally. Finally, we suggest possible causes and solutions for this
problem. Our code and data are available at
https://github.com/GraphPKU/Open-World-KG .",https://github.com/GraphPKU/Open-World-KG,-1
Scalable Multi-view Clustering with Graph Filtering,0.186256,"With the explosive growth of multi-source data, multi-view clustering has
attracted great attention in recent years. Most existing multi-view methods
operate in raw feature space and heavily depend on the quality of original
feature representation. Moreover, they are often designed for feature data and
ignore the rich topology structure information. Accordingly, in this paper, we
propose a generic framework to cluster both attribute and graph data with
heterogeneous features. It is capable of exploring the interplay between
feature and structure. Specifically, we first adopt graph filtering technique
to eliminate high-frequency noise to achieve a clustering-friendly smooth
representation. To handle the scalability challenge, we develop a novel
sampling strategy to improve the quality of anchors. Extensive experiments on
attribute and graph benchmarks demonstrate the superiority of our approach with
respect to state-of-the-art approaches.",https://github.com/EricliuLiang/SMC,-1
Conflict-Based Search for Explainable Multi-Agent Path Finding,0.0844598,"In the Multi-Agent Path Finding (MAPF) problem, the goal is to find
non-colliding paths for agents in an environment, such that each agent reaches
its goal from its initial location. In safety-critical applications, a human
supervisor may want to verify that the plan is indeed collision-free. To this
end, a recent work introduces a notion of explainability for MAPF based on a
visualization of the plan as a short sequence of images representing time
segments, where in each time segment the trajectories of the agents are
disjoint. Then, the explainable MAPF problem asks for a set of non-colliding
paths that admits a short-enough explanation. Explainable MAPF adds a new
difficulty to MAPF, in that it is NP-hard with respect to the size of the
environment, and not just the number of agents. Thus, traditional MAPF
algorithms are not equipped to directly handle explainable-MAPF. In this work,
we adapt Conflict Based Search (CBS), a well-studied algorithm for MAPF, to
handle explainable MAPF. We show how to add explainability constraints on top
of the standard CBS tree and its underlying A* search. We examine the
usefulness of this approach and, in particular, the tradeoff between planning
time and explainability.",https://github.com/atb033/multi-agent-path-planning,-1
Encoding Concepts in Graph Neural Networks,0.100233,"The opaque reasoning of Graph Neural Networks induces a lack of human trust.
Existing graph network explainers attempt to address this issue by providing
post-hoc explanations, however, they fail to make the model itself more
interpretable. To fill this gap, we introduce the Concept Encoder Module, the
first differentiable concept-discovery approach for graph networks. The
proposed approach makes graph networks explainable by design by first
discovering graph concepts and then using these to solve the task. Our results
demonstrate that this approach allows graph networks to: (i) attain model
accuracy comparable with their equivalent vanilla versions, (ii) discover
meaningful concepts that achieve high concept completeness and purity scores,
(iii) provide high-quality concept-based logic explanations for their
prediction, and (iv) support effective interventions at test time: these can
increase human trust as well as significantly improve model performance.",None,-1
A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching,0.109283,"We present a scalable combinatorial algorithm for globally optimizing over
the space of geometrically consistent mappings between 3D shapes. We use the
mathematically elegant formalism proposed by Windheuser et al. (ICCV 2011)
where 3D shape matching was formulated as an integer linear program over the
space of orientation-preserving diffeomorphisms. Until now, the resulting
formulation had limited practical applicability due to its complicated
constraint structure and its large size. We propose a novel primal heuristic
coupled with a Lagrange dual problem that is several orders of magnitudes
faster compared to previous solvers. This allows us to handle shapes with
substantially more triangles than previously solvable. We demonstrate
compelling results on diverse datasets, and, even showcase that we can address
the challenging setting of matching two partial shapes without availability of
complete shapes. Our code is publicly available at
http://github.com/paul0noah/sm-comb .",http://github.com/paul0noah/sm-comb,-1
Few-Shot Diffusion Models,0.964063,"Denoising diffusion probabilistic models (DDPM) are powerful hierarchical
latent variable models with remarkable sample generation quality and training
stability. These properties can be attributed to parameter sharing in the
generative hierarchy, as well as a parameter-free diffusion-based inference
procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a
framework for few-shot generation leveraging conditional DDPMs. FSDMs are
trained to adapt the generative process conditioned on a small set of images
from a given class by aggregating image patch information using a set-based
Vision Transformer (ViT). At test time, the model is able to generate samples
from previously unseen classes conditioned on as few as 5 samples from that
class. We empirically show that FSDM can perform few-shot generation and
transfer to new datasets. We benchmark variants of our method on complex vision
datasets for few-shot learning and compare to unconditional and conditional
DDPM baselines. Additionally, we show how conditioning the model on patch-based
input set information improves training convergence.",None,-1
Semi-Supervised Learning of Optical Flow by Flow Supervisor,0.11754,"A training pipeline for optical flow CNNs consists of a pretraining stage on
a synthetic dataset followed by a fine tuning stage on a target dataset.
However, obtaining ground truth flows from a target video requires a tremendous
effort. This paper proposes a practical fine tuning method to adapt a
pretrained model to a target dataset without ground truth flows, which has not
been explored extensively. Specifically, we propose a flow supervisor for
self-supervision, which consists of parameter separation and a student output
connection. This design is aimed at stable convergence and better accuracy over
conventional self-supervision methods which are unstable on the fine tuning
task. Experimental results show the effectiveness of our method compared to
different self-supervision methods for semi-supervised learning. In addition,
we achieve meaningful improvements over state-of-the-art optical flow models on
Sintel and KITTI benchmarks by exploiting additional unlabeled datasets. Code
is available at https://github.com/iwbn/flow-supervisor.",https://github.com/iwbn/flow-supervisor,-1
Flow-Adapter Architecture for Unsupervised Machine Translation,0.0229765,"In this work, we propose a flow-adapter architecture for unsupervised NMT. It
leverages normalizing flows to explicitly model the distributions of
sentence-level latent representations, which are subsequently used in
conjunction with the attention mechanism for the translation task. The primary
novelties of our model are: (a) capturing language-specific sentence
representations separately for each language using normalizing flows and (b)
using a simple transformation of these latent representations for translating
from one language to another. This architecture allows for unsupervised
training of each language independently. While there is prior work on latent
variables for supervised MT, to the best of our knowledge, this is the first
work that uses latent variables and normalizing flows for unsupervised MT. We
obtain competitive results on several unsupervised MT benchmarks.",https://github.com/facebookresearch/XLM/blob/main/get-data-nmt.sh,-1
Towards Self-Supervised Category-Level Object Pose and Size Estimation,0.0627353,"In this work, we tackle the challenging problem of category-level object pose
and size estimation from a single depth image. Although previous
fully-supervised works have demonstrated promising performance, collecting
ground-truth pose labels is generally time-consuming and labor-intensive.
Instead, we propose a label-free method that learns to enforce the geometric
consistency between category template mesh and observed object point cloud
under a self-supervision manner. Specifically, our method consists of three key
components: differentiable shape deformation, registration, and rendering. In
particular, shape deformation and registration are applied to the template mesh
to eliminate the differences in shape, pose and scale. A differentiable
renderer is then deployed to enforce geometric consistency between point clouds
lifted from the rendered depth and the observed scene for self-supervision. We
evaluate our approach on real-world datasets and find that our approach
outperforms the simple traditional baseline by large margins while being
competitive with some fully-supervised approaches.",https://github.com/mentian/object-deformnet,-1
A knowledge graph representation learning approach to predict novel kinase-substrate interactions,0.0304687,"The human proteome contains a vast network of interacting kinases and
substrates. Even though some kinases have proven to be immensely useful as
therapeutic targets, a majority are still understudied. In this work, we
present a novel knowledge graph representation learning approach to predict
novel interaction partners for understudied kinases. Our approach uses a
phosphoproteomic knowledge graph constructed by integrating data from iPTMnet,
Protein Ontology, Gene Ontology and BioKG. The representation of kinases and
substrates in this knowledge graph are learned by performing directed random
walks on triples coupled with a modified SkipGram or CBOW model. These
representations are then used as an input to a supervised classification model
to predict novel interactions for understudied kinases. We also present a
post-predictive analysis of the predicted interactions and an ablation study of
the phosphoproteomic knowledge graph to gain an insight into the biology of the
understudied kinases.",https://github.com/udel-cbcb/ikg_v2_public.git,-1
Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography,0.152704,"Displacement estimation is a critical step of virtually all Ultrasound
Elastography (USE) techniques. Two main features make this task unique compared
to the general optical flow problem: the high-frequency nature of ultrasound
radio-frequency (RF) data and the governing laws of physics on the displacement
field. Recently, the architecture of the optical flow networks has been
modified to be able to use RF data. Also, semi-supervised and unsupervised
techniques have been employed for USE by considering prior knowledge of
displacement continuity in the form of the first- and second-derivative
regularizers. Despite these attempts, no work has considered the tissue
compression pattern, and displacements in axial and lateral directions have
been assumed to be independent. However, tissue motion pattern is governed by
laws of physics in USE, rendering the axial and the lateral displacements
highly correlated. In this paper, we propose Physically Inspired ConsTraint for
Unsupervised Regularized Elastography (PICTURE), where we impose constraints on
the Poisson's ratio to improve lateral displacement estimates. Experiments on
phantom and in vivo data show that PICTURE substantially improves the quality
of the lateral displacement estimation.",None,-1
Grounding Answers for Visual Questions Asked by Visually Impaired People,0.245315,"Visual question answering is the task of answering questions about images. We
introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually
grounds answers to visual questions asked by people with visual impairments. We
analyze our dataset and compare it with five VQA-Grounding datasets to
demonstrate what makes it similar and different. We then evaluate the SOTA VQA
and VQA-Grounding models and demonstrate that current SOTA algorithms often
fail to identify the correct visual evidence where the answer is located. These
models regularly struggle when the visual evidence occupies a small fraction of
the image, for images that are higher quality, as well as for visual questions
that require skills in text recognition. The dataset, evaluation server, and
leaderboard all can be found at the following link:
https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.",https://github.com/CCYChongyanChen/VizWizVQAGroundingCrowdSourcing,-1
A Dataset for Medical Instructional Video Classification and Question Answering,0.353692,"This paper introduces a new challenge and datasets to foster research toward
designing systems that can understand medical videos and provide visual answers
to natural language questions. We believe medical videos may provide the best
possible answers to many first aids, medical emergency, and medical education
questions. Toward this, we created the MedVidCL and MedVidQA datasets and
introduce the tasks of Medical Video Classification (MVC) and Medical Visual
Answer Localization (MVAL), two tasks that focus on cross-modal (medical
language and medical video) understanding. The proposed tasks and datasets have
the potential to support the development of sophisticated downstream
applications that can benefit the public and medical practitioners. Our
datasets consist of 6,117 annotated videos for the MVC task and 3,010 annotated
questions and answers timestamps from 899 videos for the MVAL task. These
datasets have been verified and corrected by medical informatics experts. We
have also benchmarked each task with the created MedVidCL and MedVidQA datasets
and proposed the multimodal learning methods that set competitive baselines for
future research.",https://github.com/deepaknlp/MedVidQACL,-1
Multi-Class Segmentation from Aerial Views using Recursive Noise Diffusion,0.0664204,"Semantic segmentation from aerial views is a crucial task for autonomous
drones, as they rely on precise and accurate segmentation to navigate safely
and efficiently. However, aerial images present unique challenges such as
diverse viewpoints, extreme scale variations, and high scene complexity. In
this paper, we propose an end-to-end multi-class semantic segmentation
diffusion model that addresses these challenges. We introduce recursive
denoising to allow information to propagate through the denoising process, as
well as a hierarchical multi-scale approach that complements the diffusion
process. Our method achieves competitive results on the UAVid dataset and
state-of-the-art performance on the Vaihingen Building segmentation benchmark.
Being the first iteration of this method, it shows great promise for future
improvements.",https://github.com/benediktkol/recursive-noise-diffusion,-1
Generalized Inter-class Loss for Gait Recognition,0.131801,"Gait recognition is a unique biometric technique that can be performed at a
long distance non-cooperatively and has broad applications in public safety and
intelligent traffic systems. Previous gait works focus more on minimizing the
intra-class variance while ignoring the significance in constraining
inter-class variance. To this end, we propose a generalized inter-class loss
which resolves the inter-class variance from both sample-level feature
distribution and class-level feature distribution. Instead of equal penalty
strength on pair scores, the proposed loss optimizes sample-level inter-class
feature distribution by dynamically adjusting the pairwise weight. Further, in
class-level distribution, generalized inter-class loss adds a constraint on the
uniformity of inter-class feature distribution, which forces the feature
representations to approximate a hypersphere and keep maximal inter-class
variance. In addition, the proposed method automatically adjusts the margin
between classes which enables the inter-class feature distribution to be more
flexible. The proposed method can be generalized to different gait recognition
networks and achieves significant improvements. We conduct a series of
experiments on CASIA-B and OUMVLP, and the experimental results show that the
proposed loss can significantly improve the performance and achieves the
state-of-the-art performances.",https://github.com/ShiqiYu/OpenGait.git,-1
Putting AI Ethics into Practice: The Hourglass Model of Organizational AI Governance,0.303449,"The organizational use of artificial intelligence (AI) has rapidly spread
across various sectors. Alongside the awareness of the benefits brought by AI,
there is a growing consensus on the necessity of tackling the risks and
potential harms, such as bias and discrimination, brought about by advanced AI
technologies. A multitude of AI ethics principles have been proposed to tackle
these risks, but the outlines of organizational processes and practices for
ensuring socially responsible AI development are in a nascent state. To address
the paucity of comprehensive governance models, we present an AI governance
framework, the hourglass model of organizational AI governance, which targets
organizations that develop and use AI systems. The framework is designed to
help organizations deploying AI systems translate ethical AI principles into
practice and align their AI systems and processes with the forthcoming European
AI Act. The hourglass framework includes governance requirements at the
environmental, organizational, and AI system levels. At the AI system level, we
connect governance requirements to AI system life cycles to ensure governance
throughout the system's life span. The governance model highlights the systemic
nature of AI governance and opens new research avenues into its practical
implementation, the mechanisms that connect different AI governance layers, and
the dynamics between the AI governance actors. The model also offers a starting
point for organizational decision-makers to consider the governance components
needed to ensure social acceptability, mitigate risks, and realize the
potential of AI.",None,-1
RetroGraph: Retrosynthetic Planning with Graph Search,0.299694,"Retrosynthetic planning, which aims to find a reaction pathway to synthesize
a target molecule, plays an important role in chemistry and drug discovery.
This task is usually modeled as a search problem. Recently, data-driven methods
have attracted many research interests and shown promising results for
retrosynthetic planning. We observe that the same intermediate molecules are
visited many times in the searching process, and they are usually independently
treated in previous tree-based methods (e.g., AND-OR tree search, Monte Carlo
tree search). Such redundancies make the search process inefficient. We propose
a graph-based search policy that eliminates the redundant explorations of any
intermediate molecules. As searching over a graph is more complicated than over
a tree, we further adopt a graph neural network to guide the search over
graphs. Meanwhile, our method can search a batch of targets together in the
graph and remove the inter-target duplication in the tree-based search methods.
Experimental results on two datasets demonstrate the effectiveness of our
method. Especially on the widely used USPTO benchmark, we improve the search
success rate to 99.47%, advancing previous state-of-the-art performance for 2.6
points.",https://github.com/binghong-ml/retro_star,-1
Motion-inductive Self-supervised Object Discovery in Videos,0.262059,"In this paper, we consider the task of unsupervised object discovery in
videos. Previous works have shown promising results via processing optical
flows to segment objects. However, taking flow as input brings about two
drawbacks. First, flow cannot capture sufficient cues when objects remain
static or partially occluded. Second, it is challenging to establish temporal
coherency from flow-only input, due to the missing texture information. To
tackle these limitations, we propose a model for directly processing
consecutive RGB frames, and infer the optical flow between any pair of frames
using a layered representation, with the opacity channels being treated as the
segmentation. Additionally, to enforce object permanence, we apply temporal
consistency loss on the inferred masks from randomly-paired frames, which refer
to the motions at different paces, and encourage the model to segment the
objects even if they may not move at the current time point. Experimentally, we
demonstrate superior performance over previous state-of-the-art methods on
three public video segmentation datasets (DAVIS2016, SegTrackv2, and FBMS-59),
while being computationally efficient by avoiding the overhead of computing
optical flow as input.",None,-1
Algorithmic decision making methods for fair credit scoring,0.0304288,"The effectiveness of machine learning in evaluating the creditworthiness of
loan applicants has been demonstrated for a long time. However, there is
concern that the use of automated decision-making processes may result in
unequal treatment of groups or individuals, potentially leading to
discriminatory outcomes. This paper seeks to address this issue by evaluating
the effectiveness of 12 leading bias mitigation methods across 5 different
fairness metrics, as well as assessing their accuracy and potential
profitability for financial institutions. Through our analysis, we have
identified the challenges associated with achieving fairness while maintaining
accuracy and profitabiliy, and have highlighted both the most successful and
least successful mitigation methods. Ultimately, our research serves to bridge
the gap between experimental machine learning and its practical applications in
the finance industry.",None,-1
Residual Swin Transformer Channel Attention Network for Image Demosaicing,0.509102,"Image demosaicing is problem of interpolating full- resolution color images
from raw sensor (color filter array) data. During last decade, deep neural
networks have been widely used in image restoration, and in particular, in
demosaicing, attaining significant performance improvement. In recent years,
vision transformers have been designed and successfully used in various
computer vision applications. One of the recent methods of image restoration
based on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art
performance with a smaller number of parameters than neural network-based
methods. Inspired by the success of SwinIR, we propose in this paper a novel
Swin Transformer-based network for image demosaicing, called RSTCANet. To
extract image features, RSTCANet stacks several residual Swin Transformer
Channel Attention blocks (RSTCAB), introducing the channel attention for each
two successive ST blocks. Extensive experiments demonstrate that RSTCANet out-
performs state-of-the-art image demosaicing methods, and has a smaller number
of parameters.",None,-1
A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking,0.353987,"Large-scale graph training is a notoriously challenging problem for graph
neural networks (GNNs). Due to the nature of evolving graph structures into the
training process, vanilla GNNs usually fail to scale up, limited by the GPU
memory space. Up to now, though numerous scalable GNN architectures have been
proposed, we still lack a comprehensive survey and fair benchmark of this
reservoir to find the rationale for designing scalable GNNs. To this end, we
first systematically formulate the representative methods of large-scale graph
training into several branches and further establish a fair and consistent
benchmark for them by a greedy hyperparameter searching. In addition, regarding
efficiency, we theoretically evaluate the time and space complexity of various
branches and empirically compare them w.r.t GPU memory usage, throughput, and
convergence. Furthermore, We analyze the pros and cons for various branches of
scalable GNNs and then present a new ensembling training manner, named EnGCN,
to address the existing issues. Our code is available at
https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking.",https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking,-1
The DLCC Node Classification Benchmark for Analyzing Knowledge Graph Embeddings,0.058313,"Knowledge graph embedding is a representation learning technique that
projects entities and relations in a knowledge graph to continuous vector
spaces. Embeddings have gained a lot of uptake and have been heavily used in
link prediction and other downstream prediction tasks. Most approaches are
evaluated on a single task or a single group of tasks to determine their
overall performance. The evaluation is then assessed in terms of how well the
embedding approach performs on the task at hand. Still, it is hardly evaluated
(and often not even deeply understood) what information the embedding
approaches are actually learning to represent.
  To fill this gap, we present the DLCC (Description Logic Class Constructors)
benchmark, a resource to analyze embedding approaches in terms of which kinds
of classes they can represent. Two gold standards are presented, one based on
the real-world knowledge graph DBpedia and one synthetic gold standard. In
addition, an evaluation framework is provided that implements an experiment
protocol so that researchers can directly use the gold standard. To demonstrate
the use of DLCC, we compare multiple embedding approaches using the gold
standards. We find that many DL constructors on DBpedia are actually learned by
recognizing different correlated patterns than those defined in the gold
standard and that specific DL constructors, such as cardinality constraints,
are particularly hard to be learned for most embedding approaches.",None,-1
Predicting Customer Lifetime Value in Free-to-Play Games,0.186403,"As game companies increasingly embrace a service-oriented business model, the
need for predictive models of players becomes more pressing. Multiple
activities, such as user acquisition, live game operations or game design need
to be supported with information about the choices made by the players and the
choices they could make in the future. This is especially true in the context
of free-to-play games, where the absence of a pay wall and the erratic nature
of the players' playing and spending behavior make predictions about the
revenue and allocation of budget and resources extremely challenging. In this
chapter we will present an overview of customer lifetime value modeling across
different fields, we will introduce the challenges specific to free-to-play
games across different platforms and genres and we will discuss the
state-of-the-art solutions with practical examples and references to existing
implementations.",https://github.com/CamDavidsonPilon/lifetimes,-1
Patents Phrase to Phrase Semantic Matching Dataset,0.00606206,"There are many general purpose benchmark datasets for Semantic Textual
Similarity but none of them are focused on technical concepts found in patents
and scientific publications. This work aims to fill this gap by presenting a
new human rated contextual phrase to phrase matching dataset. The entire
dataset contains close to $50,000$ rated phrase pairs, each with a CPC
(Cooperative Patent Classification) class as a context. This paper describes
the dataset and some baseline models.",None,-1
Not Just Plain Text! Fuel Document-Level Relation Extraction with Explicit Syntax Refinement and Subsentence Modeling,0.0531447,"Document-level relation extraction (DocRE) aims to identify semantic labels
among entities within a single document. One major challenge of DocRE is to dig
decisive details regarding a specific entity pair from long text. However, in
many cases, only a fraction of text carries required information, even in the
manually labeled supporting evidence. To better capture and exploit instructive
information, we propose a novel expLicit syntAx Refinement and Subsentence
mOdeliNg based framework (LARSON). By introducing extra syntactic information,
LARSON can model subsentences of arbitrary granularity and efficiently screen
instructive ones. Moreover, we incorporate refined syntax into text
representations which further improves the performance of LARSON. Experimental
results on three benchmark datasets (DocRED, CDR, and GDA) demonstrate that
LARSON significantly outperforms existing methods.",None,-1
Exploration of the possibility of infusing Social Media Trends into generating NFT Recommendations,0.0115862,"Recommendations Systems have been identified to be one of the integral
elements of driving sales in e-commerce sites. The utilization of opinion
mining data extracted from trends has been attempted to improve the
recommendations that can be provided by baseline methods in this research when
user-click data is lacking or is difficult to be collected due to privacy
concerns.
  Utilizing social trends to influence the recommendations generated for a set
of unique items has been explored with the use of a suggested scoring
mechanism. Embracing concepts from decentralized networks that are expected to
change how users interact via the internet over the next couple of decades, the
suggested Recommendations System attempts to make use of multiple sources of
information, applying coherent information retrieval techniques to extract
probable trending items.
  The proposed Recommendations Architecture in the research presents a method
to integrate social trends with recommendations to produce promising outputs.",None,-1
Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation,0.375568,"Federated learning (FL) can be essential in knowledge representation,
reasoning, and data mining applications over multi-source knowledge graphs
(KGs). A recent study FedE first proposes an FL framework that shares entity
embeddings of KGs across all clients. However, entity embedding sharing from
FedE would incur a severe privacy leakage. Specifically, the known entity
embedding can be used to infer whether a specific relation between two entities
exists in a private client. In this paper, we introduce a novel attack method
that aims to recover the original data based on the embedding information,
which is further used to evaluate the vulnerabilities of FedE. Furthermore, we
propose a Federated learning paradigm with privacy-preserving Relation
embedding aggregation (FedR) to tackle the privacy issue in FedE. Besides,
relation embedding sharing can significantly reduce the communication cost due
to its smaller size of queries. We conduct extensive experiments to evaluate
FedR with five different KG embedding models and three datasets. Compared to
FedE, FedR achieves similar utility and significant improvements regarding
privacy-preserving effect and communication efficiency on the link prediction
task.",None,23079
Practical Exposure Correction: Great Truths Are Always Simple,0.415105,"Improving the visual quality of the given degraded observation by correcting
exposure level is a fundamental task in the computer vision community. Existing
works commonly lack adaptability towards unknown scenes because of the
data-driven patterns (deep networks) and limited regularization (traditional
optimization), and they usually need time-consuming inference. These two points
heavily limit their practicability. In this paper, we establish a Practical
Exposure Corrector (PEC) that assembles the characteristics of efficiency and
performance. To be concrete, we rethink the exposure correction to provide a
linear solution with exposure-sensitive compensation. Around generating the
compensation, we introduce an exposure adversarial function as the key engine
to fully extract valuable information from the observation. By applying the
defined function, we construct a segmented shrinkage iterative scheme to
generate the desired compensation. Its shrinkage nature supplies powerful
support for algorithmic stability and robustness. Extensive experimental
evaluations fully reveal the superiority of our proposed PEC. The code is
available at https://rsliu.tech/PEC.",https://rsliu.tech/PEC,11667
Data-driven prediction of Air Traffic Controllers reactions to resolving conflicts,0.100555,"With the aim to enhance automation in conflict detection and resolution
(CD&R) tasks in the Air Traffic Management domain, in this paper we propose
deep learning techniques (DL) that can learn models of Air Traffic Controllers'
(ATCO) reactions in resolving conflicts that can violate separation minimum
constraints among aircraft trajectories: This implies learning when the ATCO
will react towards resolving a conflict, and how he/she will react. Timely
reactions, to which this paper aims, focus on when do reactions happen, aiming
to predict the trajectory points, as the trajectory evolves, that the ATCO
issues a conflict resolution action, while also predicting the type of
resolution action (if any). Towards this goal, the paper formulates the ATCO
reactions prediction problem for CD&R, and presents DL methods that can model
ATCO timely reactions and evaluates these methods in real-world data sets,
showing their efficacy in prediction with very high accuracy.",None,4539
BlazePose GHUM Holistic: Real-time 3D Human Landmarks and Pose Estimation,0.930517,"We present BlazePose GHUM Holistic, a lightweight neural network pipeline for
3D human body landmarks and pose estimation, specifically tailored to real-time
on-device inference. BlazePose GHUM Holistic enables motion capture from a
single RGB image including avatar control, fitness tracking and AR/VR effects.
Our main contributions include i) a novel method for 3D ground truth data
acquisition, ii) updated 3D body tracking with additional hand landmarks and
iii) full body pose estimation from a monocular image.",https://mediapipe.dev,19004
Towards Robust k-Nearest-Neighbor Machine Translation,0.11967,"k-Nearest-Neighbor Machine Translation (kNN-MT) becomes an important research
direction of NMT in recent years. Its main idea is to retrieve useful key-value
pairs from an additional datastore to modify translations without updating the
NMT model. However, the underlying retrieved noisy pairs will dramatically
deteriorate the model performance. In this paper, we conduct a preliminary
study and find that this problem results from not fully exploiting the
prediction of the NMT model. To alleviate the impact of noise, we propose a
confidence-enhanced kNN-MT model with robust training. Concretely, we introduce
the NMT confidence to refine the modeling of two important components of
kNN-MT: kNN distribution and the interpolation weight. Meanwhile we inject two
types of perturbations into the retrieved pairs for robust training.
Experimental results on four benchmark datasets demonstrate that our model not
only achieves significant improvements over current kNN-MT models, but also
exhibits better robustness. Our code is available at
https://github.com/DeepLearnXMU/Robust-knn-mt.",https://github.com/DeepLearnXMU/Robust-knn-mt,4230
3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization,0.156893,"Although deep-learning based methods for monocular pedestrian detection have
made great progress, they are still vulnerable to heavy occlusions. Using
multi-view information fusion is a potential solution but has limited
applications, due to the lack of annotated training samples in existing
multi-view datasets, which increases the risk of overfitting. To address this
problem, a data augmentation method is proposed to randomly generate 3D
cylinder occlusions, on the ground plane, which are of the average size of
pedestrians and projected to multiple views, to relieve the impact of
overfitting in the training. Moreover, the feature map of each view is
projected to multiple parallel planes at different heights, by using
homographies, which allows the CNNs to fully utilize the features across the
height of each pedestrian to infer the locations of pedestrians on the ground
plane. The proposed 3DROM method has a greatly improved performance in
comparison with the state-of-the-art deep-learning based methods for multi-view
pedestrian detection.",https://github.com/xjtlu-cvlab/3DROM,5075
Synthesizing Personalized Non-speech Vocalization from Discrete Speech Representations,0.132106,"We formulated non-speech vocalization (NSV) modeling as a text-to-speech task
and verified its viability. Specifically, we evaluated the phonetic
expressivity of HUBERT speech units on NSVs and verified our model's ability to
control over speaker timbre even though the training data is speaker few-shot.
In addition, we substantiated that the heterogeneity in recording conditions is
the major obstacle for NSV modeling. Finally, we discussed five improvements
over our method for future research. Audio samples of synthesized NSVs are
available on our demo page: https://resemble-ai.github.io/reLaugh.",None,4373
Domain-General Crowd Counting in Unseen Scenarios,0.048108,"Domain shift across crowd data severely hinders crowd counting models to
generalize to unseen scenarios. Although domain adaptive crowd counting
approaches close this gap to a certain extent, they are still dependent on the
target domain data to adapt (e.g. finetune) their models to the specific
domain. In this paper, we aim to train a model based on a single source domain
which can generalize well on any unseen domain. This falls into the realm of
domain generalization that remains unexplored in crowd counting. We first
introduce a dynamic sub-domain division scheme which divides the source domain
into multiple sub-domains such that we can initiate a meta-learning framework
for domain generalization. The sub-domain division is dynamically refined
during the meta-learning. Next, in order to disentangle domain-invariant
information from domain-specific information in image features, we design the
domain-invariant and -specific crowd memory modules to re-encode image
features. Two types of losses, i.e. feature reconstruction and orthogonal
losses, are devised to enable this disentanglement. Extensive experiments on
several standard crowd counting benchmarks i.e. SHA, SHB, QNRF, and NWPU, show
the strong generalizability of our method.",https://github.com/ZPDu/Domain-general-Crowd-Counting-in-Unseen-Scenarios,2009
Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities,0.161478,"As for other forms of AI, speech recognition has recently been examined with
respect to performance disparities across different user cohorts. One approach
to achieve fairness in speech recognition is to (1) identify speaker cohorts
that suffer from subpar performance and (2) apply fairness mitigation measures
targeting the cohorts discovered. In this paper, we report on initial findings
with both discovery and mitigation of performance disparities using data from a
product-scale AI assistant speech recognition system. We compare cohort
discovery based on geographic and demographic information to a more scalable
method that groups speakers without human labels, using speaker embedding
technology. For fairness mitigation, we find that oversampling of
underrepresented cohorts, as well as modeling speaker cohort membership by
additional input variables, reduces the gap between top- and bottom-performing
cohorts, without deteriorating overall recognition accuracy.",None,31243
nerf2nerf: Pairwise Registration of Neural Radiance Fields,0.132844,"We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io",None,9133
I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning,0.0633187,"Knowledge graph (KG) embedding seeks to learn vector representations for
entities and relations. Conventional models reason over graph structures, but
they suffer from the issues of graph incompleteness and long-tail entities.
Recent studies have used pre-trained language models to learn embeddings based
on the textual information of entities and relations, but they cannot take
advantage of graph structures. In the paper, we show empirically that these two
kinds of features are complementary for KG embedding. To this end, we propose
CoLE, a Co-distillation Learning method for KG Embedding that exploits the
complementarity of graph structures and text information. Its graph embedding
model employs Transformer to reconstruct the representation of an entity from
its neighborhood subgraph. Its text embedding model uses a pre-trained language
model to generate entity representations from the soft prompts of their names,
descriptions, and relational neighbors. To let the two model promote each
other, we propose co-distillation learning that allows them to distill
selective knowledge from each other's prediction logits. In our co-distillation
learning, each model serves as both a teacher and a student. Experiments on
benchmark datasets demonstrate that the two models outperform their related
baselines, and the ensemble method CoLE with co-distillation learning advances
the state-of-the-art of KG embedding.",https://github.com/nju-websoft/CoLE,11617
Unpaired Image Translation via Vector Symbolic Architectures,0.0839373,"Image-to-image translation has played an important role in enabling synthetic
data for computer vision. However, if the source and target domains have a
large semantic mismatch, existing techniques often suffer from source content
corruption aka semantic flipping. To address this problem, we propose a new
paradigm for image-to-image translation using Vector Symbolic Architectures
(VSA), a theoretical framework which defines algebraic operations in a
high-dimensional vector (hypervector) space. We introduce VSA-based constraints
on adversarial learning for source-to-target translations by learning a
hypervector mapping that inverts the translation to ensure consistency with
source content. We show both qualitatively and quantitatively that our method
improves over other state-of-the-art techniques.",https://github.com/facebookresearch/vsait,146
Exploration of Machine Learning Classification Models Used for Behavioral Biometrics Authentication,0.0927617,"Mobile devices have been manufactured and enhanced at growing rates in the
past decades. While this growth has significantly evolved the capability of
these devices, their security has been falling behind. This contrast in
development between capability and security of mobile devices is a significant
problem with the sensitive information of the public at risk. Continuing the
previous work in this field, this study identifies key Machine Learning
algorithms currently being used for behavioral biometric mobile authentication
schemes and aims to provide a comprehensive review of these algorithms when
used with touch dynamics and phone movement. Throughout this paper the
benefits, limitations, and recommendations for future work will be discussed.",None,664
ALLSH: Active Learning Guided by Local Sensitivity and Hardness,0.0537378,"Active learning, which effectively collects informative unlabeled data for
annotation, reduces the demand for labeled data. In this work, we propose to
retrieve unlabeled samples with a local sensitivity and hardness-aware
acquisition function. The proposed method generates data copies through local
perturbations and selects data points whose predictive likelihoods diverge the
most from their copies. We further empower our acquisition function by
injecting the select-worst case perturbation. Our method achieves consistent
gains over the commonly used active learning strategies in various
classification tasks. Furthermore, we observe consistent improvements over the
baselines on the study of prompt selection in prompt-based few-shot learning.
These experiments demonstrate that our acquisition guided by local sensitivity
and hardness can be effective and beneficial for many NLP tasks.",https://github.com/szhang42/allsh,19975
Hierarchical Phrase-based Sequence-to-Sequence Learning,0.0478948,"We describe a neural transducer that maintains the flexibility of standard
sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases
as a source of inductive bias during training and as explicit constraints
during inference. Our approach trains two models: a discriminative parser based
on a bracketing transduction grammar whose derivation tree hierarchically
aligns source and target phrases, and a neural seq2seq model that learns to
translate the aligned phrases one-by-one. We use the same seq2seq model to
translate at all phrase scales, which results in two inference modes: one mode
in which the parser is discarded and only the seq2seq component is used at the
sequence-level, and another in which the parser is combined with the seq2seq
model. Decoding in the latter mode is done with the cube-pruned CKY algorithm,
which is more involved but can make use of new translation rules during
inference. We formalize our model as a source-conditioned synchronous grammar
and develop an efficient variational inference algorithm for training. When
applied on top of both randomly initialized and pretrained seq2seq models, we
find that both inference modes performs well compared to baselines on small
scale machine translation benchmarks.",https://github.com/berlino/btg-seq2seq,29122
IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering in Indoor Scenes,0.446895,"Indoor scenes exhibit significant appearance variations due to myriad
interactions between arbitrarily diverse object shapes, spatially-changing
materials, and complex lighting. Shadows, highlights, and inter-reflections
caused by visible and invisible light sources require reasoning about
long-range interactions for inverse rendering, which seeks to recover the
components of image formation, namely, shape, material, and lighting. In this
work, our intuition is that the long-range attention learned by transformer
architectures is ideally suited to solve longstanding challenges in
single-image inverse rendering. We demonstrate with a specific instantiation of
a dense vision transformer, IRISformer, that excels at both single-task and
multi-task reasoning required for inverse rendering. Specifically, we propose a
transformer architecture to simultaneously estimate depths, normals,
spatially-varying albedo, roughness and lighting from a single image of an
indoor scene. Our extensive evaluations on benchmark datasets demonstrate
state-of-the-art results on each of the above tasks, enabling applications like
object insertion and material editing in a single unconstrained real image,
with greater photorealism than prior works. Code and data are publicly released
at https://github.com/ViLab-UCSD/IRISformer.",https://github.com/ViLab-UCSD/IRISformer,38190
PaCo: Parameter-Compositional Multi-Task Reinforcement Learning,0.127767,"The purpose of multi-task reinforcement learning (MTRL) is to train a single
policy that can be applied to a set of different tasks. Sharing parameters
allows us to take advantage of the similarities among tasks. However, the gaps
between contents and difficulties of different tasks bring us challenges on
both which tasks should share the parameters and what parameters should be
shared, as well as the optimization challenges due to parameter sharing. In
this work, we introduce a parameter-compositional approach (PaCo) as an attempt
to address these challenges. In this framework, a policy subspace represented
by a set of parameters is learned. Policies for all the single tasks lie in
this subspace and can be composed by interpolating with the learned set. It
allows not only flexible parameter sharing but also a natural way to improve
training. We demonstrate the state-of-the-art performance on Meta-World
benchmarks, verifying the effectiveness of the proposed approach.",https://github.com/facebookresearch/mtrl,30737
Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models,0.864665,"This paper presents exploratory work on whether and to what extent biases
against queer and trans people are encoded in large language models (LLMs) such
as BERT. We also propose a method for reducing these biases in downstream
tasks: finetuning the models on data written by and/or about queer people. To
measure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,
modeled after other bias-detection benchmarks but addressing homophobic and
transphobic biases. We found that BERT shows significant homophobic bias, but
this bias can be mostly mitigated by finetuning BERT on a natural language
corpus written by members of the LGBTQ+ community.",https://github.com/katyfelkner/,1371
A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation,0.629889,"Early exiting allows instances to exit at different layers according to the
estimation of difficulty. Previous works usually adopt heuristic metrics such
as the entropy of internal outputs to measure instance difficulty, which
suffers from generalization and threshold-tuning. In contrast, learning to
exit, or learning to predict instance difficulty is a more appealing way.
Though some effort has been devoted to employing such ""learn-to-exit"" modules,
it is still unknown whether and how well the instance difficulty can be
learned. As a response, we first conduct experiments on the learnability of
instance difficulty, which demonstrates that modern neural models perform
poorly on predicting instance difficulty. Based on this observation, we propose
a simple-yet-effective Hash-based Early Exiting approach (HashEE) that replaces
the learn-to-exit modules with hash functions to assign each token to a fixed
exiting layer. Different from previous methods, HashEE requires no internal
classifiers nor extra parameters, and therefore is more efficient. Experimental
results on classification, regression, and generation tasks demonstrate that
HashEE can achieve higher performance with fewer FLOPs and inference time
compared with previous state-of-the-art early exiting methods.",https://github.com/txsun1997/HashEE,21857
Interacting Hand-Object Pose Estimation via Dense Mutual Attention,0.0578264,"3D hand-object pose estimation is the key to the success of many computer
vision applications. The main focus of this task is to effectively model the
interaction between the hand and an object. To this end, existing works either
rely on interaction constraints in a computationally-expensive iterative
optimization, or consider only a sparse correlation between sampled hand and
object keypoints. In contrast, we propose a novel dense mutual attention
mechanism that is able to model fine-grained dependencies between the hand and
the object. Specifically, we first construct the hand and object graphs
according to their mesh structures. For each hand node, we aggregate features
from every object node by the learned attention and vice versa for each object
node. Thanks to such dense mutual attention, our method is able to produce
physically plausible poses with high quality and real-time inference speed.
Extensive quantitative and qualitative experiments on large benchmark datasets
show that our method outperforms state-of-the-art methods. The code is
available at https://github.com/rongakowang/DenseMutualAttention.git.",https://github.com/rongakowang/DenseMutualAttention.git,24
Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,0.412992,"Despite their strong performance on many tasks, pre-trained language models
have been shown to struggle on out-of-distribution compositional
generalization. Meanwhile, recent work has shown considerable improvements on
many NLP tasks from model scaling. Can scaling up model size also improve
compositional generalization in semantic parsing? We evaluate encoder-decoder
models up to 11B parameters and decoder-only models up to 540B parameters, and
compare model scaling curves for three different methods for applying a
pre-trained language model to a new task: fine-tuning all parameters, prompt
tuning, and in-context learning. We observe that fine-tuning generally has flat
or negative scaling curves on out-of-distribution compositional generalization
in semantic parsing evaluations. In-context learning has positive scaling
curves, but is generally outperformed by much smaller fine-tuned models.
Prompt-tuning can outperform fine-tuning, suggesting further potential
improvements from scaling as it exhibits a more positive scaling curve.
Additionally, we identify several error trends that vary with model scale. For
example, larger models are generally better at modeling the syntax of the
output space, but are also more prone to certain types of overfitting. Overall,
our study highlights limitations of current techniques for effectively
leveraging model scale for compositional generalization, while our analysis
also suggests promising directions for future work.",https://github.com/microsoft/compositional-generalization-span-level-attention,20836
Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud Analysis,0.115337,"Point clouds are characterized by irregularity and unstructuredness, which
pose challenges in efficient data exploitation and discriminative feature
extraction. In this paper, we present an unsupervised deep neural architecture
called Flattening-Net to represent irregular 3D point clouds of arbitrary
geometry and topology as a completely regular 2D point geometry image (PGI)
structure, in which coordinates of spatial points are captured in colors of
image pixels. \mr{Intuitively, Flattening-Net implicitly approximates a locally
smooth 3D-to-2D surface flattening process while effectively preserving
neighborhood consistency.} \mr{As a generic representation modality, PGI
inherently encodes the intrinsic property of the underlying manifold structure
and facilitates surface-style point feature aggregation.} To demonstrate its
potential, we construct a unified learning framework directly operating on PGIs
to achieve \mr{diverse types of high-level and low-level} downstream
applications driven by specific task networks, including classification,
segmentation, reconstruction, and upsampling. Extensive experiments demonstrate
that our methods perform favorably against the current state-of-the-art
competitors. We will make the code and data publicly available at
https://github.com/keeganhk/Flattening-Net.",https://github.com/keeganhk/Flattening-Net,9714
A Computational Inflection for Scientific Discovery,0.0419618,"We stand at the foot of a significant inflection in the trajectory of
scientific discovery. As society continues on its fast-paced digital
transformation, so does humankind's collective scientific knowledge and
discourse. We now read and write papers in digitized form, and a great deal of
the formal and informal processes of science are captured digitally --
including papers, preprints and books, code and datasets, conference
presentations, and interactions in social networks and collaboration and
communication platforms. The transition has led to the creation and growth of a
tremendous amount of information -- much of which is available for public
access -- opening exciting opportunities for computational models and systems
that analyze and harness it. In parallel, exponential growth in data processing
power has fueled remarkable advances in artificial intelligence, including
large neural language models capable of learning powerful representations from
unstructured text. Dramatic changes in scientific communication -- such as the
advent of the first scientific journal in the 17th century -- have historically
catalyzed revolutions in scientific thought. The confluence of societal and
computational trends suggests that computer science is poised to ignite a
revolution in the scientific process itself.",None,101093
AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,0.204335,"This paper proposes AdaTest, a novel adaptive test pattern generation
framework for efficient and reliable Hardware Trojan (HT) detection. HT is a
backdoor attack that tampers with the design of victim integrated circuits
(ICs). AdaTest improves the existing HT detection techniques in terms of
scalability and accuracy of detecting smaller Trojans in the presence of noise
and variations. To achieve high trigger coverage, AdaTest leverages
Reinforcement Learning (RL) to produce a diverse set of test inputs.
Particularly, we progressively generate test vectors with high reward values in
an iterative manner. In each iteration, the test set is evaluated and
adaptively expanded as needed. Furthermore, AdaTest integrates adaptive
sampling to prioritize test samples that provide more information for HT
detection, thus reducing the number of samples while improving the sample
quality for faster exploration. We develop AdaTest with a Software/Hardware
co-design principle and provide an optimized on-chip architecture solution.
AdaTest's architecture minimizes the hardware overhead in two ways:(i)
Deploying circuit emulation on programmable hardware to accelerate reward
evaluation of the test input; (ii) Pipelining each computation stage in AdaTest
by automatically constructing auxiliary circuit for test input generation,
reward evaluation, and adaptive sampling. We evaluate AdaTest's performance on
various HT benchmarks and compare it with two prior works that use logic
testing for HT detection. Experimental results show that AdaTest engenders up
to two orders of test generation speedup and two orders of test set size
reduction compared to the prior works while achieving the same level or higher
Trojan detection rate.",None,31252
Hierarchical Decision Transformer,0.0362703,"Sequence models in reinforcement learning require task knowledge to estimate
the task policy. This paper presents a hierarchical algorithm for learning a
sequence model from demonstrations. The high-level mechanism guides the
low-level controller through the task by selecting sub-goals for the latter to
reach. This sequence replaces the returns-to-go of previous methods, improving
its performance overall, especially in tasks with longer episodes and scarcer
rewards. We validate our method in multiple tasks of OpenAIGym, D4RL and
RoboMimic benchmarks. Our method outperforms the baselines in eight out of ten
tasks of varied horizons and reward frequencies without prior task knowledge,
showing the advantages of the hierarchical model approach for learning from
demonstrations using a sequence model.",None,3
Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval,0.110502,"Multi-document summarization (MDS) assumes a set of topic-related documents
are provided as input. In practice, this document set is not always available;
it would need to be retrieved given an information need, i.e. a question or
topic statement, a setting we dub ""open-domain"" MDS. We study this more
challenging setting by formalizing the task and bootstrapping it using existing
datasets, retrievers and summarizers. Via extensive automatic and human
evaluation, we determine: (1) state-of-the-art summarizers suffer large
reductions in performance when applied to open-domain MDS, (2) additional
training in the open-domain setting can reduce this sensitivity to imperfect
retrieval, and (3) summarizers are insensitive to the retrieval of duplicate
documents and the order of retrieved documents, but highly sensitive to other
errors, like the retrieval of irrelevant documents. Based on our results, we
provide practical guidelines to enable future work on open-domain MDS, e.g. how
to choose the number of retrieved documents to summarize. Our results suggest
that new retrieval and summarization methods and annotated resources for
training and evaluation are necessary for further progress in the open-domain
setting.",https://github.com/allenai/open-mds,13874
Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings,0.390131,"Automatic depression detection on Twitter can help individuals privately and
conveniently understand their mental health status in the early stages before
seeing mental health professionals. Most existing black-box-like deep learning
methods for depression detection largely focused on improving classification
performance. However, explaining model decisions is imperative in health
research because decision-making can often be high-stakes and life-and-death.
Reliable automatic diagnosis of mental health problems including depression
should be supported by credible explanations justifying models' predictions. In
this work, we propose a novel explainable model for depression detection on
Twitter. It comprises a novel encoder combining hierarchical attention
mechanisms and feed-forward neural networks. To support psycholinguistic
studies, our model leverages metaphorical concept mappings as input. Thus, it
not only detects depressed individuals, but also identifies features of such
users' tweets and associated metaphor concept mappings.",None,55352
Defending Black-box Skeleton-based Human Activity Classifiers,0.113883,"Skeletal motions have been heavily replied upon for human activity
recognition (HAR). Recently, a universal vulnerability of skeleton-based HAR
has been identified across a variety of classifiers and data, calling for
mitigation. To this end, we propose the first black-box defense method for
skeleton-based HAR to our best knowledge. Our method is featured by full
Bayesian treatments of the clean data, the adversaries and the classifier,
leading to (1) a new Bayesian Energy-based formulation of robust discriminative
classifiers, (2) a new adversary sampling scheme based on natural motion
manifolds, and (3) a new post-train Bayesian strategy for black-box defense. We
name our framework Bayesian Energy-based Adversarial Training or BEAT. BEAT is
straightforward but elegant, which turns vulnerable black-box classifiers into
robust ones without sacrificing accuracy. It demonstrates surprising and
universal effectiveness across a wide range of skeletal HAR classifiers and
datasets, under various attacks. Code is available at
https://github.com/realcrane/RobustActionRecogniser.",https://github.com/realcrane/Defending-Black-box-Skeleton-based-Human-Activity-Classiﬁers,1271
A Transfer Learning Based Model for Text Readability Assessment in German,0.0348457,"Text readability assessment has a wide range of applications for different
target people, from language learners to people with disabilities. The fast
pace of textual content production on the web makes it impossible to measure
text complexity without the benefit of machine learning and natural language
processing techniques. Although various research addressed the readability
assessment of English text in recent years, there is still room for improvement
of the models for other languages. In this paper, we proposed a new model for
text complexity assessment for German text based on transfer learning. Our
results show that the model outperforms more classical solutions based on
linguistic features extraction from input text. The best model is based on the
BERT pre-trained language model achieved the Root Mean Square Error (RMSE) of
0.483.",None,12355
Contrastive Learning of Coarse-Grained Force Fields,0.0324126,"Coarse-grained models have proven helpful for simulating complex systems over
long timescales to provide molecular insights into various processes.
Methodologies for systematic parameterization of the underlying energy
function, or force field that describes the interactions among different
components of the system are of great interest for ensuring simulation
accuracy. We present a new method, potential contrasting, to enable efficient
learning of force fields that can accurately reproduce the conformational
distribution produced with all-atom simulations. Potential contrasting
generalizes the noise contrastive estimation method with umbrella sampling to
better learn the complex energy landscape of molecular systems. When applied to
the Trp-cage protein, we found that the technique produces force fields that
thoroughly capture the thermodynamics of the folding process despite the use of
only $\alpha$-Carbons in the coarse-grained model. We further showed that
potential contrasting could be applied over large datasets that combine the
conformational ensembles of many proteins to ensure the transferability of
coarse-grained force fields. We anticipate potential contrasting to be a
powerful tool for building general-purpose coarse-grained force fields.",None,-1
Tools and Practices for Responsible AI Engineering,0.32918,"Responsible Artificial Intelligence (AI) - the practice of developing,
evaluating, and maintaining accurate AI systems that also exhibit essential
properties such as robustness and explainability - represents a multifaceted
challenge that often stretches standard machine learning tooling, frameworks,
and testing methods beyond their limits. In this paper, we present two new
software libraries - hydra-zen and the rAI-toolbox - that address critical
needs for responsible AI engineering. hydra-zen dramatically simplifies the
process of making complex AI applications configurable, and their behaviors
reproducible. The rAI-toolbox is designed to enable methods for evaluating and
enhancing the robustness of AI-models in a way that is scalable and that
composes naturally with other popular ML frameworks. We describe the design
principles and methodologies that make these tools effective, including the use
of property-based testing to bolster the reliability of the tools themselves.
Finally, we demonstrate the composability and flexibility of the tools by
showing how various use cases from adversarial robustness and explainable AI
can be concisely implemented with familiar APIs.",https://github.com/mit-ll-responsible-ai/,4156
Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks,0.0308773,"Early diagnosis and intervention are clinically considered the paramount part
of treating cerebral palsy (CP), so it is essential to design an efficient and
interpretable automatic prediction system for CP. We highlight a significant
difference between CP infants' frequency of human movement and that of the
healthy group, which improves prediction performance. However, the existing
deep learning-based methods did not use the frequency information of infants'
movement for CP prediction. This paper proposes a frequency attention informed
graph convolutional network and validates it on two consumer-grade RGB video
datasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency
attention module aids in improving both classification performance and system
interpretability. In addition, we design a frequency-binning method that
retains the critical frequency of the human joint position data while filtering
the noise. Our prediction performance achieves state-of-the-art research on
both datasets. Our work demonstrates the effectiveness of frequency information
in supporting the prediction of CP non-intrusively and provides a way for
supporting the early diagnosis of CP in the resource-limited regions where the
clinical resources are not abundant.",https://github.com/zhz95/FAIGCN,3498
Diverse Imagenet Models Transfer Better,0.00360202,"A commonly accepted hypothesis is that models with higher accuracy on
Imagenet perform better on other downstream tasks, leading to much research
dedicated to optimizing Imagenet accuracy. Recently this hypothesis has been
challenged by evidence showing that self-supervised models transfer better than
their supervised counterparts, despite their inferior Imagenet accuracy. This
calls for identifying the additional factors, on top of Imagenet accuracy, that
make models transferable. In this work we show that high diversity of the
features learnt by the model promotes transferability jointly with Imagenet
accuracy. Encouraged by the recent transferability results of self-supervised
models, we propose a method that combines self-supervised and supervised
pretraining to generate models with both high diversity and high accuracy, and
as a result high transferability. We demonstrate our results on several
architectures and multiple downstream tasks, including both single-label and
multi-label classification.",None,14108
Generative Category-Level Shape and Pose Estimation with Semantic Primitives,0.13557,"Empowering autonomous agents with 3D understanding for daily objects is a
grand challenge in robotics applications. When exploring in an unknown
environment, existing methods for object pose estimation are still not
satisfactory due to the diversity of object shapes. In this paper, we propose a
novel framework for category-level object shape and pose estimation from a
single RGB-D image. To handle the intra-category variation, we adopt a semantic
primitive representation that encodes diverse shapes into a unified latent
space, which is the key to establish reliable correspondences between observed
point clouds and estimated shapes. Then, by using a SIM(3)-invariant shape
descriptor, we gracefully decouple the shape and pose of an object, thus
supporting latent shape optimization of target objects in arbitrary poses.
Extensive experiments show that the proposed method achieves SOTA pose
estimation performance and better generalization in the real-world dataset.
Code and video are available at https://zju3dv.github.io/gCasp.",https://zju3dv.github.io/gCasp,4589
Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation,0.0229858,"After the great success of Vision Transformer variants (ViTs) in computer
vision, it has also demonstrated great potential in domain adaptive semantic
segmentation. Unfortunately, straightforwardly applying local ViTs in domain
adaptive semantic segmentation does not bring in expected improvement. We find
that the pitfall of local ViTs is due to the severe high-frequency components
generated during both the pseudo-label construction and features alignment for
target domains. These high-frequency components make the training of local ViTs
very unsmooth and hurt their transferability. In this paper, we introduce a
low-pass filtering mechanism, momentum network, to smooth the learning dynamics
of target domain features and pseudo labels. Furthermore, we propose a dynamic
of discrepancy measurement to align the distributions in the source and target
domains via dynamic weights to evaluate the importance of the samples. After
tackling the above issues, extensive experiments on sim2real benchmarks show
that the proposed method outperforms the state-of-the-art methods. Our codes
are available at https://github.com/alpc91/TransDA",https://github.com/alpc91/TransDA,24953
Extreme Compression for Pre-trained Transformers Made Simple and Efficient,0.455049,"Extreme compression, particularly ultra-low bit precision (binary/ternary)
quantization, has been proposed to fit large NLP models on resource-constraint
devices. However, to preserve the accuracy for such aggressive compression
schemes, cutting-edge methods usually introduce complicated compression
pipelines, e.g., multi-stage expensive knowledge distillation with extensive
hyperparameter tuning. Also, they oftentimes focus less on smaller transformer
models that have already been heavily compressed via knowledge distillation and
lack a systematic study to show the effectiveness of their methods. In this
paper, we perform a very comprehensive systematic study to measure the impact
of many key hyperparameters and training strategies from previous works. As a
result, we find out that previous baselines for ultra-low bit precision
quantization are significantly under-trained. Based on our study, we propose a
simple yet effective compression pipeline for extreme compression, named XTC.
XTC demonstrates that (1) we can skip the pre-training knowledge distillation
to obtain a 5-layer BERT while achieving better performance than previous
state-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization
plus layer reduction is able to reduce the model size by 50x, resulting in new
state-of-the-art results on GLUE tasks.",https://github.com/microsoft/DeepSpeed,7580
C-Pack of IPAs: A C90 Program Benchmark of Introductory Programming Assignments,0.13291,"Due to the vast number of students enrolled in Massive Open Online Courses
(MOOCs), there has been an increasing number of automated program repair
techniques focused on introductory programming assignments (IPAs). Such
techniques take advantage of previous correct student implementations in order
to provide automated, comprehensive, and personalized feedback to students.
  This paper presents C-Pack-IPAs, a publicly available benchmark of students'
programs submitted for 25 different IPAs. C-Pack-IPAs contains semantically
correct, semantically incorrect, and syntactically incorrect programs plus a
test suite for each IPA. Hence, C-Pack-IPAs can be used to help evaluate the
development of novel semantic, as well as syntactic, automated program repair
frameworks, focused on providing feedback to novice programmers.",https://github.com/pmorvalho/C-Pack-IPAs,2685
A Hierarchical Bayesian Approach to Inverse Reinforcement Learning with Symbolic Reward Machines,0.0569389,"A misspecified reward can degrade sample efficiency and induce undesired
behaviors in reinforcement learning (RL) problems. We propose symbolic reward
machines for incorporating high-level task knowledge when specifying the reward
signals. Symbolic reward machines augment existing reward machine formalism by
allowing transitions to carry predicates and symbolic reward outputs. This
formalism lends itself well to inverse reinforcement learning, whereby the key
challenge is determining appropriate assignments to the symbolic values from a
few expert demonstrations. We propose a hierarchical Bayesian approach for
inferring the most likely assignments such that the concretized reward machine
can discriminate expert demonstrated trajectories from other trajectories with
high accuracy. Experimental results show that learned reward machines can
significantly improve training efficiency for complex RL tasks and generalize
well across different task environment configurations.",None,2911
ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations,0.864665,"Precise representations of 3D faces are beneficial to various computer vision
and graphics applications. Due to the data discretization and model linearity,
however, it remains challenging to capture accurate identity and expression
clues in current studies. This paper presents a novel 3D morphable face model,
namely ImFace, to learn a nonlinear and continuous space with implicit neural
representations. It builds two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
and designs an improved learning strategy to extend embeddings of expressions
to allow more diverse changes. We further introduce a Neural Blend-Field to
learn sophisticated details by adaptively blending a series of local fields. In
addition to ImFace, an effective preprocessing pipeline is proposed to address
the issue of watertight input requirement in implicit representations, enabling
them to work with common facial surfaces for the first time. Extensive
experiments are performed to demonstrate the superiority of ImFace.",https://github.com/MingwuZheng/ImFace,11135
Zero-Shot Retrieval with Search Agents and Hybrid Environments,0.0405104,"Learning to search is the task of building artificial agents that learn to
autonomously use a search box to find information. So far, it has been shown
that current language models can learn symbolic query reformulation policies,
in combination with traditional term-based retrieval, but fall short of
outperforming neural retrievers. We extend the previous learning to search
setup to a hybrid environment, which accepts discrete query refinement
operations, after a first-pass retrieval step via a dual encoder. Experiments
on the BEIR task show that search agents, trained via behavioral cloning,
outperform the underlying search system based on a combined dual encoder
retriever and cross encoder reranker. Furthermore, we find that simple
heuristic Hybrid Retrieval Environments (HRE) can improve baseline performance
by several nDCG points. The search agent based on HRE (HARE) matches
state-of-the-art performance, balanced in both zero-shot and in-domain
evaluations, via interpretable actions, and at twice the speed.",None,2397
Cycle-Consistent Counterfactuals by Latent Transformations,0.248931,"CounterFactual (CF) visual explanations try to find images similar to the
query image that change the decision of a vision system to a specified outcome.
Existing methods either require inference-time optimization or joint training
with a generative adversarial model which makes them time-consuming and
difficult to use in practice. We propose a novel approach, Cycle-Consistent
Counterfactuals by Latent Transformations (C3LT), which learns a latent
transformation that automatically generates visual CFs by steering in the
latent space of generative models. Our method uses cycle consistency between
the query and CF latent representations which helps our training to find better
solutions. C3LT can be easily plugged into any state-of-the-art pretrained
generative network. This enables our method to generate high-quality and
interpretable CF images at high resolution such as those in ImageNet. In
addition to several established metrics for evaluating CF explanations, we
introduce a novel metric tailored to assess the quality of the generated CF
examples and validate the effectiveness of our method on an extensive set of
experiments.",https://github.com/IBM/Contrastive-Explanation-Method,8748
Solutions for Fine-grained and Long-tailed Snake Species Recognition in SnakeCLEF 2022,0.067733,"Automatic snake species recognition is important because it has vast
potential to help lower deaths and disabilities caused by snakebites. We
introduce our solution in SnakeCLEF 2022 for fine-grained snake species
recognition on a heavy long-tailed class distribution. First, a network
architecture is designed to extract and fuse features from multiple modalities,
i.e. photograph from visual modality and geographic locality information from
language modality. Then, logit adjustment based methods are studied to relieve
the impact caused by the severe class imbalance. Next, a combination of
supervised and self-supervised learning method is proposed to make full use of
the dataset, including both labeled training data and unlabeled testing data.
Finally, post processing strategies, such as multi-scale and multi-crop
test-time-augmentation, location filtering and model ensemble, are employed for
better performance. With an ensemble of several different models, a private
score 82.65%, ranking the 3rd, is achieved on the final leaderboard.",None,13984
Transfer Learning based Search Space Design for Hyperparameter Tuning,0.0676497,"The tuning of hyperparameters becomes increasingly important as machine
learning (ML) models have been extensively applied in data mining applications.
Among various approaches, Bayesian optimization (BO) is a successful
methodology to tune hyper-parameters automatically. While traditional methods
optimize each tuning task in isolation, there has been recent interest in
speeding up BO by transferring knowledge across previous tasks. In this work,
we introduce an automatic method to design the BO search space with the aid of
tuning history from past tasks. This simple yet effective approach can be used
to endow many existing BO methods with transfer learning capabilities. In
addition, it enjoys the three advantages: universality, generality, and
safeness. The extensive experiments show that our approach considerably boosts
BO by designing a promising and compact search space instead of using the
entire space, and outperforms the state-of-the-arts on a wide range of
benchmarks, including machine learning and deep learning tuning tasks, and
neural architecture search.",None,10542
Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions,0.203454,"Pruning is one of the predominant approaches for compressing deep neural
networks (DNNs). Lately, coresets (provable data summarizations) were leveraged
for pruning DNNs, adding the advantage of theoretical guarantees on the
trade-off between the compression rate and the approximation error. However,
coresets in this domain were either data-dependent or generated under
restrictive assumptions on both the model's weights and inputs. In real-world
scenarios, such assumptions are rarely satisfied, limiting the applicability of
coresets. To this end, we suggest a novel and robust framework for computing
such coresets under mild assumptions on the model's weights and without any
assumption on the training data. The idea is to compute the importance of each
neuron in each layer with respect to the output of the following layer. This is
achieved by a combination of L\""{o}wner ellipsoid and Caratheodory theorem. Our
method is simultaneously data-independent, applicable to various networks and
datasets (due to the simplified assumptions), and theoretically supported.
Experimental results show that our method outperforms existing coreset based
neural pruning approaches across a wide range of networks and datasets. For
example, our method achieved a $62\%$ compression rate on ResNet50 on ImageNet
with $1.09\%$ drop in accuracy.",None,506
Summarizing Patients Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models,0.343444,"Automatically summarizing patients' main problems from daily progress notes
using natural language processing methods helps to battle against information
and cognitive overload in hospital settings and potentially assists providers
with computerized diagnostic decision support. Problem list summarization
requires a model to understand, abstract, and generate clinical documentation.
In this work, we propose a new NLP task that aims to generate a list of
problems in a patient's daily care plan using input from the provider's
progress notes during hospitalization. We investigate the performance of T5 and
BART, two state-of-the-art seq2seq transformer architectures, in solving this
problem. We provide a corpus built on top of progress notes from publicly
available electronic health record progress notes in the Medical Information
Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain
text, and we experiment with a data augmentation method and a domain adaptation
pre-training method to increase exposure to medical vocabulary and knowledge.
Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence
embedding, and F-score on medical concepts. Results show that T5 with domain
adaptive pre-training achieves significant performance gains compared to a
rule-based system and general domain pre-trained language models, indicating a
promising direction for tackling the problem summarization task.",//git.doit.wisc.edu/smph/dom/UW-ICU-Data-Science-Lab/drbench,9607
Uncertainty-aware Panoptic Segmentation,0.551462,"Reliable scene understanding is indispensable for modern autonomous systems.
Current learning-based methods typically try to maximize their performance
based on segmentation metrics that only consider the quality of the
segmentation. However, for the safe operation of a system in the real world it
is crucial to consider the uncertainty in the prediction as well. In this work,
we introduce the novel task of uncertainty-aware panoptic segmentation, which
aims to predict per-pixel semantic and instance segmentations, together with
per-pixel uncertainty estimates. We define two novel metrics to facilitate its
quantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the
panoptic Expected Calibration Error (pECE). We further propose the novel
top-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task.
Our architecture employs a simple yet effective panoptic fusion module that
leverages the predicted uncertainties. Furthermore, we provide several strong
baselines combining state-of-the-art panoptic segmentation networks with
sampling-free uncertainty estimation techniques. Extensive evaluations show
that our EvPSNet achieves the new state-of-the-art for the standard Panoptic
Quality (PQ), as well as for our uncertainty-aware panoptic metrics. We make
the code available at: \url{https://github.com/kshitij3112/EvPSNet}",https://github.com/kshitij3112/EvPSNet,115008
Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task,0.094437,"Although transformer-based Neural Language Models demonstrate impressive
performance on a variety of tasks, their generalization abilities are not well
understood. They have been shown to perform strongly on subject-verb number
agreement in a wide array of settings, suggesting that they learned to track
syntactic dependencies during their training even without explicit supervision.
In this paper, we examine the extent to which BERT is able to perform
lexically-independent subject-verb number agreement (NA) on targeted syntactic
templates. To do so, we disrupt the lexical patterns found in naturally
occurring stimuli for each targeted structure in a novel fine-grained analysis
of BERT's behavior. Our results on nonce sentences suggest that the model
generalizes well for simple templates, but fails to perform
lexically-independent syntactic generalization when as little as one attractor
is present.",https://github.com/karimlasri/does-bert-really-agree,8215
Mutation Models: Learning to Generate Levels by Imitating Evolution,0.0343829,"Search-based procedural content generation (PCG) is a well-known method for
level generation in games. Its key advantage is that it is generic and able to
satisfy functional constraints. However, due to the heavy computational costs
to run these algorithms online, search-based PCG is rarely utilized for
real-time generation. In this paper, we introduce mutation models, a new type
of iterative level generator based on machine learning. We train a model to
imitate the evolutionary process and use the trained model to generate levels.
This trained model is able to modify noisy levels sequentially to create better
levels without the need for a fitness function during inference. We evaluate
our trained models on a 2D maze generation task. We compare several different
versions of the method: training the models either at the end of evolution
(normal evolution) or every 100 generations (assisted evolution) and using the
model as a mutation function during evolution. Using the assisted evolution
process, the final trained models are able to generate mazes with a success
rate of 99% and high diversity of 86%. The trained model is many times faster
than the evolutionary process it was trained on. This work opens the door to a
new way of learning level generators guided by an evolutionary process, meaning
automatic creation of generators with specifiable constraints and objectives
that are fast enough for runtime deployment in games.",None,22404
EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points,0.287269,"Neural radiance fields (NeRF) achieve highly photo-realistic novel-view
synthesis, but it's a challenging problem to edit the scenes modeled by
NeRF-based methods, especially for dynamic scenes. We propose editable neural
radiance fields that enable end-users to easily edit dynamic scenes and even
support topological changes. Input with an image sequence from a single camera,
our network is trained fully automatically and models topologically varying
dynamics using our picked-out surface key points. Then end-users can edit the
scene by easily dragging the key points to desired new positions. To achieve
this, we propose a scene analysis method to detect and initialize key points by
considering the dynamics in the scene, and a weighted key points strategy to
model topologically varying dynamics by joint key points and weights
optimization. Our method supports intuitive multi-dimensional (up to 3D)
editing and can generate novel scenes that are unseen in the input sequence.
Experiments demonstrate that our method achieves high-quality editing on
various dynamic scenes and outperforms the state-of-the-art. Our code and
captured data are available at https://chengwei-zheng.github.io/EditableNeRF/.",None,3333
Panoramic Human Activity Recognition,0.111816,"To obtain a more comprehensive activity understanding for a crowded scene, in
this paper, we propose a new problem of panoramic human activity recognition
(PAR), which aims to simultaneous achieve the individual action, social group
activity, and global activity recognition. This is a challenging yet practical
problem in real-world applications. For this problem, we develop a novel
hierarchical graph neural network to progressively represent and model the
multi-granularity human activities and mutual social relations for a crowd of
people. We further build a benchmark to evaluate the proposed method and other
existing related methods. Experimental results verify the rationality of the
proposed PAR problem, the effectiveness of our method and the usefulness of the
benchmark. We will release the source code and benchmark to the public for
promoting the study on this problem.",https://github.com/RuizeHan/PAR,10875
Learning by Distilling Context,0.318681,"Language models significantly benefit from context tokens, such as prompts or
scratchpads. They perform better when prompted with informative instructions,
and they acquire new reasoning capabilities by generating a scratch-pad before
predicting the final answers. However, they do not \textit{internalize} these
performance gains, which disappear when the context tokens are gone. Our work
proposes to apply context distillation so that a language model can improve
itself by internalizing these gains. Concretely, given a synthetic unlabeled
input for the target task, we condition the model on ``[instructions] +
[task-input]'' to predict ``[scratch-pad] + [final answer]''; then we fine-tune
the same model to predict its own ``[final answer]'' conditioned on the
``[task-input]'', without seeing the ``[instructions]'' or using the
``[scratch-pad]''.
  We show that context distillation is a general method to train language
models, and it can effectively internalize 3 types of training signals. First,
it can internalize abstract task instructions and explanations, so we can
iteratively update the model parameters with new instructions and overwrite old
ones. Second, it can internalize step-by-step reasoning for complex tasks
(e.g., 8-digit addition), and such a newly acquired capability proves to be
useful for other downstream tasks. Finally, it can internalize concrete
training examples, and it outperforms directly learning with gradient descent
by 9\% on the SPIDER Text-to-SQL dataset; furthermore, combining context
distillation operations can internalize more training examples than the context
window size allows.",None,38020
Point-Level Region Contrast for Object Detection Pre-Training,0.264829,"In this work we present point-level region contrast, a self-supervised
pre-training approach for the task of object detection. This approach is
motivated by the two key factors in detection: localization and recognition.
While accurate localization favors models that operate at the pixel- or
point-level, correct recognition typically relies on a more holistic,
region-level view of objects. Incorporating this perspective in pre-training,
our approach performs contrastive learning by directly sampling individual
point pairs from different regions. Compared to an aggregated representation
per region, our approach is more robust to the change in input region quality,
and further enables us to implicitly improve initial region assignments via
online knowledge distillation during training. Both advantages are important
when dealing with imperfect regions encountered in the unsupervised setting.
Experiments show point-level region contrast improves on state-of-the-art
pre-training methods for object detection and segmentation across multiple
tasks and datasets, and we provide extensive ablation studies and
visualizations to aid understanding. Code will be made available.",None,115578
Improving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms,0.101086,"Attention has become one of the most commonly used mechanisms in deep
learning approaches. The attention mechanism can help the system focus more on
the feature space's critical regions. For example, high amplitude regions can
play an important role for Speech Emotion Recognition (SER). In this paper, we
identify misalignments between the attention and the signal amplitude in the
existing multi-head self-attention. To improve the attention area, we propose
to use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA)
mechanism in combination with the multi-head self-attention. Through the FA
mechanism, the network can detect the largest amplitude part in the segment. By
employing the CA mechanism, the network can modulate the information flow by
assigning different weights to each attention head and improve the utilization
of surrounding contexts. To evaluate the proposed method, experiments are
performed with the IEMOCAP and RAVDESS datasets. Experimental results show that
the proposed framework significantly outperforms the state-of-the-art
approaches on both datasets.",None,5129
MPANet: Multi-Patch Attention For Infrared Small Target object Detection,0.140356,"Infrared small target detection (ISTD) has attracted widespread attention and
been applied in various fields. Due to the small size of infrared targets and
the noise interference from complex backgrounds, the performance of ISTD using
convolutional neural networks (CNNs) is restricted. Moreover, the constriant
that long-distance dependent features can not be encoded by the vanilla CNNs
also impairs the robustness of capturing targets' shapes and locations in
complex scenarios. To this end, a multi-patch attention network (MPANet) based
on the axial-attention encoder and the multi-scale patch branch (MSPB)
structure is proposed. Specially, an axial-attention-improved encoder
architecture is designed to highlight the effective features of small targets
and suppress background noises. Furthermore, the developed MSPB structure fuses
the coarse-grained and fine-grained features from different semantic scales.
Extensive experiments on the SIRST dataset show the superiority performance and
effectiveness of the proposed MPANet compared to the state-of-the-art methods.",None,15602
Editable Indoor Lighting Estimation,0.0564228,"We present a method for estimating lighting from a single perspective image
of an indoor scene. Previous methods for predicting indoor illumination usually
focus on either simple, parametric lighting that lack realism, or on richer
representations that are difficult or even impossible to understand or modify
after prediction. We propose a pipeline that estimates a parametric light that
is easy to edit and allows renderings with strong shadows, alongside with a
non-parametric texture with high-frequency information necessary for realistic
rendering of specular objects. Once estimated, the predictions obtained with
our model are interpretable and can easily be modified by an artist/user with a
few mouse clicks. Quantitative and qualitative results show that our approach
makes indoor lighting estimation easier to handle by a casual user, while still
producing competitive results.",None,6586
HLT-MT: High-resource Language-specific Training for Multilingual Neural Machine Translation,0.103051,"Multilingual neural machine translation (MNMT) trained in multiple language
pairs has attracted considerable attention due to fewer model parameters and
lower training costs by sharing knowledge among multiple languages.
Nonetheless, multilingual training is plagued by language interference
degeneration in shared parameters because of the negative interference among
different translation directions, especially on high-resource languages. In
this paper, we propose the multilingual translation model with the
high-resource language-specific training (HLT-MT) to alleviate the negative
interference, which adopts the two-stage training with the language-specific
selection mechanism. Specifically, we first train the multilingual model only
with the high-resource pairs and select the language-specific modules at the
top of the decoder to enhance the translation quality of high-resource
directions. Next, the model is further trained on all available corpora to
transfer knowledge from high-resource languages (HRLs) to low-resource
languages (LRLs). Experimental results show that HLT-MT outperforms various
strong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic
experiments validate the effectiveness of our method in mitigating the negative
interference in multilingual training.",None,97925
Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video,0.864665,"We present HandAvatar, a novel representation for hand animation and
rendering, which can generate smoothly compositional geometry and
self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as
a high-resolution mesh topology to fit personalized hand shapes. Sequentially,
we decompose hand geometry into per-bone rigid parts, and then re-compose
paired geometry encodings to derive an across-part consistent occupancy field.
As for texture modeling, we propose a self-occlusion-aware shading field
(SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record
albedo information under a wide variety of hand poses. Moreover, directed soft
occupancy is designed to describe the ray-to-surface relation, which is
leveraged to generate an illumination field for the disentanglement of
pose-independent albedo and pose-dependent illumination. Trained from monocular
video data, our HandAvatar can perform free-pose hand animation and rendering
while at the same time achieving superior appearance fidelity. We also
demonstrate that HandAvatar provides a route for hand appearance editing.
Project website: https://seanchenxy.github.io/HandAvatarWeb.",None,2845
Keep Me Updated! Memory Management in Long-term Conversations,0.0809541,"Remembering important information from the past and continuing to talk about
it in the present are crucial in long-term conversations. However, previous
literature does not deal with cases where the memorized information is
outdated, which may cause confusion in later conversations. To address this
issue, we present a novel task and a corresponding dataset of memory management
in long-term conversations, in which bots keep track of and bring up the latest
information about users while conversing through multiple sessions. In order to
support more precise and interpretable memory, we represent memory as
unstructured text descriptions of key information and propose a new mechanism
of memory management that selectively eliminates invalidated or redundant
information. Experimental results show that our approach outperforms the
baselines that leave the stored memory unchanged in terms of engagingness and
humanness, with larger performance gap especially in the later sessions.",https://github.com/naver-ai/carecall-memory,6394
Learning Deformable Object Manipulation from Expert Demonstrations,0.300517,"We present a novel Learning from Demonstration (LfD) method, Deformable
Manipulation from Demonstrations (DMfD), to solve deformable manipulation tasks
using states or images as inputs, given expert demonstrations. Our method uses
demonstrations in three different ways, and balances the trade-off between
exploring the environment online and using guidance from experts to explore
high dimensional spaces effectively. We test DMfD on a set of representative
manipulation tasks for a 1-dimensional rope and a 2-dimensional cloth from the
SoftGym suite of tasks, each with state and image observations. Our method
exceeds baseline performance by up to 12.9% for state-based tasks and up to
33.44% on image-based tasks, with comparable or better robustness to
randomness. Additionally, we create two challenging environments for folding a
2D cloth using image-based observations, and set a performance benchmark for
them. We deploy DMfD on a real robot with a minimal loss in normalized
performance during real-world execution compared to simulation (~6%). Source
code is on github.com/uscresl/dmfd",https://github.com/uscresl/dmfd,445
Multi-level Consistency Learning for Semi-supervised Domain Adaptation,0.17978,"Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from
a fully labeled source domain to a scarcely labeled target domain. In this
paper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA.
Specifically, our MCL regularizes the consistency of different views of target
domain samples at three levels: (i) at inter-domain level, we robustly and
accurately align the source and target domains using a prototype-based optimal
transport method that utilizes the pros and cons of different views of target
samples; (ii) at intra-domain level, we facilitate the learning of both
discriminative and compact target feature representations by proposing a novel
class-wise contrastive clustering loss; (iii) at sample level, we follow
standard practice and improve the prediction accuracy by conducting a
consistency-based self-training. Empirically, we verified the effectiveness of
our MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet,
and Office-Home datasets, and the experimental results demonstrate that our MCL
framework achieves the state-of-the-art performance.",https://github.com/chester256/MCL,30637
NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries,0.0512775,"We develop NL2INTERFACE to explore the potential of generating usable
interactive multi-visualization interfaces from natural language queries. With
NL2INTERFACE, users can directly write natural language queries to
automatically generate a fully interactive multi-visualization interface
without any extra effort of learning a tool or programming language. Further,
users can interact with the interfaces to easily transform the data and quickly
see the results in the visualizations.",None,11020
DKM: Dense Kernelized Feature Matching for Geometry Estimation,0.440571,"Feature matching is a challenging computer vision task that involves finding
correspondences between two images of a 3D scene. In this paper we consider the
dense approach instead of the more common sparse paradigm, thus striving to
find all correspondences. Perhaps counter-intuitively, dense methods have
previously shown inferior performance to their sparse and semi-sparse
counterparts for estimation of two-view geometry. This changes with our novel
dense method, which outperforms both dense and sparse methods on geometry
estimation. The novelty is threefold: First, we propose a kernel regression
global matcher. Secondly, we propose warp refinement through stacked feature
maps and depthwise convolution kernels. Thirdly, we propose learning dense
confidence through consistent depth and a balanced sampling approach for dense
confidence maps. Through extensive experiments we confirm that our proposed
dense method, \textbf{D}ense \textbf{K}ernelized Feature \textbf{M}atching,
sets a new state-of-the-art on multiple geometry estimation benchmarks. In
particular, we achieve an improvement on MegaDepth-1500 of +4.9 and +8.9
AUC$@5^{\circ}$ compared to the best previous sparse method and dense method
respectively. Our code is provided at https://github.com/Parskatt/dkm",https://github.com/Parskatt/dkm,29085
PSDoodle: Searching for App Screens via Interactive Sketching,0.0891442,"Keyword-based mobile screen search does not account for screen content and
fails to operate as a universal tool for all levels of users. Visual searching
(e.g., image, sketch) is structured and easy to adopt. Current visual search
approaches count on a complete screen and are therefore slow and tedious.
PSDoodle employs a deep neural network to recognize partial screen element
drawings instantly on a digital drawing interface and shows results in
real-time. PSDoodle is the first tool that utilizes partial sketches and
searches for screens in an interactive iterative way. PSDoodle supports
different drawing styles and retrieves search results that are relevant to the
user's sketch query. A short video demonstration is available online at:
https://youtu.be/3cVLHFm5pY4",https://github.com/soumikmohianuta/PSDoodle,2830
Learning to Revise References for Faithful Summarization,0.220904,"In real-world scenarios with naturally occurring datasets, reference
summaries are noisy and may contain information that cannot be inferred from
the source text. On large news corpora, removing low quality samples has been
shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora,
filtering is detrimental to performance. To improve reference quality while
retaining all data, we propose a new approach: to selectively re-write
unsupported reference sentences to better reflect source data. We automatically
generate a synthetic dataset of positive and negative revisions by corrupting
supported sentences and learn to revise reference sentences with contrastive
learning. The intensity of revisions is treated as a controllable attribute so
that, at inference, diverse candidates can be over-generated-then-rescored to
balance faithfulness and abstraction. To test our methods, we extract noisy
references from publicly available MIMIC-III discharge summaries for the task
of hospital-course summarization, and vary the data on which models are
trained. According to metrics and human evaluation, models trained on revised
clinical references are much more faithful, informative, and fluent than models
trained on original or filtered data.",https://github.com/amazon-research/summary-reference-revision,30354
Search to Pass Messages for Temporal Knowledge Graph Completion,0.0207208,"Completing missing facts is a fundamental task for temporal knowledge graphs
(TKGs). Recently, graph neural network (GNN) based methods, which can
simultaneously explore topological and temporal information, have become the
state-of-the-art (SOTA) to complete TKGs. However, these studies are based on
hand-designed architectures and fail to explore the diverse topological and
temporal properties of TKG. To address this issue, we propose to use neural
architecture search (NAS) to design data-specific message passing architecture
for TKG completion. In particular, we develop a generalized framework to
explore topological and temporal information in TKGs. Based on this framework,
we design an expressive search space to fully capture various properties of
different TKGs. Meanwhile, we adopt a search algorithm, which trains a supernet
structure by sampling single path for efficient search with less cost. We
further conduct extensive experiments on three benchmark datasets. The results
show that the searched architectures by our method achieve the SOTA
performances. Besides, the searched models can also implicitly reveal diverse
properties in different TKGs. Our code is released in
https://github.com/striderdu/SPA.",https://github.com/striderdu/SPA,9577
A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots,0.452555,"A slot value might be provided segment by segment over multiple-turn
interactions in a dialog, especially for some important information such as
phone numbers and names. It is a common phenomenon in daily life, but little
attention has been paid to it in previous work. To fill the gap, this paper
defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds
a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset
includes a total of 40K dialogs and 500K utterances from four different
domains: Chinese names, phone numbers, ID numbers and license plate numbers.
The data is well annotated with sub-slot values, slot values, dialog states and
actions. We find some new linguistic phenomena and interactive manners in SSTOD
which raise critical challenges of building dialog agents for the task. We test
three state-of-the-art dialog models on SSTOD and find they cannot handle the
task well on any of the four domains. We also investigate an improved model by
involving slot knowledge in a plug-in manner. More work should be done to meet
the new challenges raised from SSTOD which widely exists in real-life
applications. The dataset and code are publicly available via
https://github.com/shunjiu/SSTOD.",https://github.com/shunjiu/SSTOD,7165
CarFi: Rider Localization Using Wi-Fi CSI,0.0640184,"With the rise of hailing services, people are increasingly relying on shared
mobility (e.g., Uber, Lyft) drivers to pick up for transportation. However,
such drivers and riders have difficulties finding each other in urban areas as
GPS signals get blocked by skyscrapers, in crowded environments (e.g., in
stadiums, airports, and bars), at night, and in bad weather. It wastes their
time, creates a bad user experience, and causes more CO2 emissions due to idle
driving. In this work, we explore the potential of Wi-Fi to help drivers to
determine the street side of the riders. Our proposed system is called CarFi
that uses Wi-Fi CSI from two antennas placed inside a moving vehicle and a
data-driven technique to determine the street side of the rider. By collecting
real-world data in realistic and challenging settings by blocking the signal
with other people and other parked cars, we see that CarFi is 95.44% accurate
in rider-side determination in both line of sight (LoS) and non-line of sight
(nLoS) conditions, and can be run on an embedded GPU in real-time.",None,2064
A Knowledge Graph Embeddings based Approach for Author Name Disambiguation using Literals,0.0952867,"Scholarly data is growing continuously containing information about the
articles from a plethora of venues including conferences, journals, etc. Many
initiatives have been taken to make scholarly data available as Knowledge
Graphs (KGs). These efforts to standardize these data and make them accessible
have also led to many challenges such as exploration of scholarly articles,
ambiguous authors, etc. This study more specifically targets the problem of
Author Name Disambiguation (AND) on Scholarly KGs and presents a novel
framework, Literally Author Name Disambiguation (LAND), which utilizes
Knowledge Graph Embeddings (KGEs) using multimodal literal information
generated from these KGs. This framework is based on three components: 1)
Multimodal KGEs, 2) A blocking procedure, and finally, 3) Hierarchical
Agglomerative Clustering. Extensive experiments have been conducted on two
newly created KGs: (i) KG containing information from Scientometrics Journal
from 1978 onwards (OC-782K), and (ii) a KG extracted from a well-known
benchmark for AND provided by AMiner (AMiner-534K). The results show that our
proposed architecture outperforms our baselines of 8-14% in terms of the F1
score and shows competitive performances on a challenging benchmark such as
AMiner. The code and the datasets are publicly available through Github:
https://github.com/sntcristian/and-kge and
Zenodo:https://doi.org/10.5281/zenodo.6309855 respectively.",None,19721
Cross-Spectral Neural Radiance Fields,0.628712,"We propose X-NeRF, a novel method to learn a Cross-Spectral scene
representation given images captured from cameras with different light spectrum
sensitivity, based on the Neural Radiance Fields formulation. X-NeRF optimizes
camera poses across spectra during training and exploits Normalized
Cross-Device Coordinates (NXDC) to render images of different modalities from
arbitrary viewpoints, which are aligned and at the same resolution. Experiments
on 16 forward-facing scenes, featuring color, multi-spectral and infrared
images, confirm the effectiveness of X-NeRF at modeling Cross-Spectral scene
representations.",None,12441
FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference,0.842614,"Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that
sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the
architecture used for FiD was chosen by making minimal modifications to a
standard T5 model, which our analysis shows to be highly suboptimal for a
retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to
the encoder, while the majority of inference time results from memory bandwidth
constraints in the decoder. We propose two simple changes to the FiD
architecture to alleviate memory bandwidth constraints, and speed up inference
by 7x. This allows us to use a much larger decoder at modest cost. We denote
FiD with the above modifications as FiDO, and show that it strongly improves
performance over existing FiD models for a wide range of inference budgets. For
example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves
better performance than FiD-Large.",None,20836
Consent as a Foundation for Responsible Autonomy,0.390681,"This paper focuses on a dynamic aspect of responsible autonomy, namely, to
make intelligent agents be responsible at run time. That is, it considers
settings where decision making by agents impinges upon the outcomes perceived
by other agents. For an agent to act responsibly, it must accommodate the
desires and other attitudes of its users and, through other agents, of their
users.
  The contribution of this paper is twofold. First, it provides a conceptual
analysis of consent, its benefits and misuses, and how understanding consent
can help achieve responsible autonomy. Second, it outlines challenges for AI
(in particular, for agents and multiagent systems) that merit investigation to
form as a basis for modeling consent in multiagent systems and applying consent
to achieve responsible autonomy.",None,31895
3D Dual-Fusion: Dual-Domain Dual-Query Camera-LiDAR Fusion for 3D Object Detection,0.426232,"Fusing data from cameras and LiDAR sensors is an essential technique to
achieve robust 3D object detection. One key challenge in camera-LiDAR fusion
involves mitigating the large domain gap between the two sensors in terms of
coordinates and data distribution when fusing their features. In this paper, we
propose a novel camera-LiDAR fusion architecture called, 3D Dual-Fusion, which
is designed to mitigate the gap between the feature representations of camera
and LiDAR data. The proposed method fuses the features of the camera-view and
3D voxel-view domain and models their interactions through deformable
attention. We redesign the transformer fusion encoder to aggregate the
information from the two domains. Two major changes include 1) dual query-based
deformable attention to fuse the dual-domain features interactively and 2) 3D
local self-attention to encode the voxel-domain queries prior to dual-query
decoding. The results of an experimental evaluation show that the proposed
camera-LiDAR fusion architecture achieved competitive performance on the KITTI
and nuScenes datasets, with state-of-the-art performances in some 3D object
detection benchmarks categories.",None,3955
Differentiable Inference of Temporal Logic Formulas,0.0345488,"We demonstrate the first Recurrent Neural Network architecture for learning
Signal Temporal Logic formulas, and present the first systematic comparison of
formula inference methods. Legacy systems embed much expert knowledge which is
not explicitly formalized. There is great interest in learning formal
specifications that characterize the ideal behavior of such systems -- that is,
formulas in temporal logic that are satisfied by the system's output signals.
Such specifications can be used to better understand the system's behavior and
improve design of its next iteration. Previous inference methods either assumed
certain formula templates, or did a heuristic enumeration of all possible
templates. This work proposes a neural network architecture that infers the
formula structure via gradient descent, eliminating the need for imposing any
specific templates. It combines learning of formula structure and parameters in
one optimization. Through systematic comparison, we demonstrate that this
method achieves similar or better mis-classification rates (MCR) than
enumerative and lattice methods. We also observe that different formulas can
achieve similar MCR, empirically demonstrating the under-determinism of the
problem of temporal logic inference.",https://github.com/nicaless/fernn,51
Don't Say What You Don't Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search,0.141384,"Abstractive summarization systems today produce fluent and relevant output,
but often ""hallucinate"" statements not supported by the source text. We analyze
the connection between hallucinations and training data, and find evidence that
models hallucinate because they train on target summaries that are unsupported
by the source. Based on our findings, we present PINOCCHIO, a new decoding
method that improves the consistency of a transformer-based abstractive
summarizer by constraining beam search to avoid hallucinations. Given the model
states and outputs at a given step, PINOCCHIO detects likely model
hallucinations based on various measures of attribution to the source text.
PINOCCHIO backtracks to find more consistent output, and can opt to produce no
summary at all when no consistent generation can be found. In experiments, we
find that PINOCCHIO improves the consistency of generation (in terms of F1) by
an average of~67% on two abstractive summarization datasets.",https://github.com/allenai/pinocchio,14304
NAN: Noise-Aware NeRFs for Burst-Denoising,0.106838,"Burst denoising is now more relevant than ever, as computational photography
helps overcome sensitivity issues inherent in mobile phones and small cameras.
A major challenge in burst-denoising is in coping with pixel misalignment,
which was so far handled with rather simplistic assumptions of simple motion,
or the ability to align in pre-processing. Such assumptions are not realistic
in the presence of large motion and high levels of noise. We show that Neural
Radiance Fields (NeRFs), originally suggested for physics-based novel-view
rendering, can serve as a powerful framework for burst denoising. NeRFs have an
inherent capability of handling noise as they integrate information from
multiple images, but they are limited in doing so, mainly since they build on
pixel-wise operations which are suitable to ideal imaging conditions. Our
approach, termed NAN, leverages inter-view and spatial information in NeRFs to
better deal with noise. It achieves state-of-the-art results in burst denoising
and is especially successful in coping with large movement and occlusions,
under very high levels of noise. With the rapid advances in accelerating NeRFs,
it could provide a powerful platform for denoising in challenging environments.",None,6308
Visually-Augmented Language Modeling,0.209329,"Human language is grounded on multimodal knowledge including visual knowledge
like colors, sizes, and shapes. However, current large-scale pre-trained
language models rely on text-only self-supervised training with massive text
data, which precludes them from utilizing relevant visual information when
necessary. To address this, we propose a novel pre-training framework, named
VaLM, to Visually-augment text tokens with retrieved relevant images for
Language Modeling. Specifically, VaLM builds on a novel latent text-image
alignment method via an image retrieval module to fetch corresponding images
given a textual context. With the visually-augmented context, VaLM uses a
visual knowledge fusion layer to enable multimodal grounded language modeling
by attending to both text context and visual knowledge in images. We evaluate
VaLM on various visual knowledge-intensive commonsense reasoning tasks, which
require visual information to excel. The experimental results illustrate that
VaLM outperforms all strong language-only and vision-language baselines with
substantial gains in reasoning object commonsense including color, size, and
shape. Our code is available at https://github.com/Victorwz/VaLM.",https://github.com/Victorwz/VaLM,-1
Thermodynamics-informed neural networks for physically realistic mixed reality,0.0159536,"The imminent impact of immersive technologies in society urges for active
research in real-time and interactive physics simulation for virtual worlds to
be realistic. In this context, realistic means to be compliant to the laws of
physics. In this paper we present a method for computing the dynamic response
of (possibly non-linear and dissipative) deformable objects induced by
real-time user interactions in mixed reality using deep learning. The
graph-based architecture of the method ensures the thermodynamic consistency of
the predictions, whereas the visualization pipeline allows a natural and
realistic user experience. Two examples of virtual solids interacting with
virtual or physical solids in mixed reality scenarios are provided to prove the
performance of the method.",https://github.com/quercushernandez,-1
Better plain ViT baselines for ImageNet-1k,0.0544374,"It is commonly accepted that the Vision Transformer model requires
sophisticated regularization techniques to excel at ImageNet-1k scale data.
Surprisingly, we find this is not the case and standard data augmentation is
sufficient. This note presents a few minor modifications to the original Vision
Transformer (ViT) vanilla training setting that dramatically improve the
performance of plain ViT models. Notably, 90 epochs of training surpass 76%
top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic
ResNet50 baseline, and 300 epochs of training reach 80% in less than one day.",https://github.com/google-research/big_vision,-1
Improving Monocular Visual Odometry Using Learned Depth,0.0599123,"Monocular visual odometry (VO) is an important task in robotics and computer
vision. Thus far, how to build accurate and robust monocular VO systems that
can work well in diverse scenarios remains largely unsolved. In this paper, we
propose a framework to exploit monocular depth estimation for improving VO. The
core of our framework is a monocular depth estimation module with a strong
generalization capability for diverse scenes. It consists of two separate
working modes to assist the localization and mapping. With a single monocular
image input, the depth estimation module predicts a relative depth to help the
localization module on improving the accuracy. With a sparse depth map and an
RGB image input, the depth estimation module can generate accurate
scale-consistent depth for dense mapping. Compared with current learning-based
VO methods, our method demonstrates a stronger generalization ability to
diverse scenes. More significantly, our framework is able to boost the
performances of existing geometry-based VO methods by a large margin.",None,-1
AdaPrompt: Adaptive Model Training for Prompt-based NLP,0.758025,"Prompt-based learning, with its capability to tackle zero-shot and few-shot
NLP tasks, has gained much attention in community. The main idea is to bridge
the gap between NLP downstream tasks and language modeling (LM), by mapping
these tasks into natural language prompts, which are then filled by pre-trained
language models (PLMs). However, for prompt learning, there are still two
salient gaps between NLP tasks and pretraining. First, prompt information is
not necessarily sufficiently present during LM pretraining. Second,
task-specific data are not necessarily well represented during pretraining. We
address these two issues by proposing AdaPrompt, adaptively retrieving external
data for continual pretraining of PLMs by making use of both task and prompt
characteristics. In addition, we make use of knowledge in Natural Language
Inference models for deriving adaptive verbalizers. Experimental results on
five NLP benchmarks show that AdaPrompt can improve over standard PLMs in
few-shot settings. In addition, in zero-shot settings, our method outperforms
standard prompt-based methods by up to 26.35\% relative error reduction.",https://github.com/cylnlp/AdaPrompt,-1
Quantum policy gradient algorithms,0.0545127,"Understanding the power and limitations of quantum access to data in machine
learning tasks is primordial to assess the potential of quantum computing in
artificial intelligence. Previous works have already shown that speed-ups in
learning are possible when given quantum access to reinforcement learning
environments. Yet, the applicability of quantum algorithms in this setting
remains very limited, notably in environments with large state and action
spaces. In this work, we design quantum algorithms to train state-of-the-art
reinforcement learning policies by exploiting quantum interactions with an
environment. However, these algorithms only offer full quadratic speed-ups in
sample complexity over their classical analogs when the trained policies
satisfy some regularity conditions. Interestingly, we find that reinforcement
learning policies derived from parametrized quantum circuits are well-behaved
with respect to these conditions, which showcases the benefit of a
fully-quantum reinforcement learning framework.",None,-1
Optimizing Relevance Maps of Vision Transformers Improves Robustness,0.304706,"It has been observed that visual classification models often rely mostly on
the image background, neglecting the foreground, which hurts their robustness
to distribution changes. To alleviate this shortcoming, we propose to monitor
the model's relevancy signal and manipulate it such that the model is focused
on the foreground object. This is done as a finetuning step, involving
relatively few samples consisting of pairs of images and their associated
foreground masks. Specifically, we encourage the model's relevancy map (i) to
assign lower relevance to background regions, (ii) to consider as much
information as possible from the foreground, and (iii) we encourage the
decisions to have high confidence. When applied to Vision Transformer (ViT)
models, a marked improvement in robustness to domain shifts is observed.
Moreover, the foreground masks can be obtained automatically, from a
self-supervised variant of the ViT model itself; therefore no additional
supervision is required.",https://github.com/hila-chefer/RobustViT,-1
Federated Learning with Position-Aware Neurons,0.153098,"Federated Learning (FL) fuses collaborative models from local nodes without
centralizing users' data. The permutation invariance property of neural
networks and the non-i.i.d. data across clients make the locally updated
parameters imprecisely aligned, disabling the coordinate-based parameter
averaging. Traditional neurons do not explicitly consider position information.
Hence, we propose Position-Aware Neurons (PANs) as an alternative, fusing
position-related values (i.e., position encodings) into neuron outputs. PANs
couple themselves to their positions and minimize the possibility of
dislocation, even updating on heterogeneous data. We turn on/off PANs to
disable/enable the permutation invariance property of neural networks. PANs are
tightly coupled with positions when applied to FL, making parameters across
clients pre-aligned and facilitating coordinate-based parameter averaging. PANs
are algorithm-agnostic and could universally improve existing FL algorithms.
Furthermore, ""FL with PANs"" is simple to implement and computationally
friendly.",https://github.com/IBM/FedMA,-1
Transformer Embeddings of Irregularly Spaced Events and Their Participants,0.134577,"The neural Hawkes process (Mei & Eisner, 2017) is a generative model of
irregularly spaced sequences of discrete events. To handle complex domains with
many event types, Mei et al. (2020a) further consider a setting in which each
event in the sequence updates a deductive database of facts (via
domain-specific pattern-matching rules); future events are then conditioned on
the database contents. They show how to convert such a symbolic system into a
neuro-symbolic continuous-time generative model, in which each database fact
and the possible event has a time-varying embedding that is derived from its
symbolic provenance.
  In this paper, we modify both models, replacing their recurrent LSTM-based
architectures with flatter attention-based architectures (Vaswani et al.,
2017), which are simpler and more parallelizable. This does not appear to hurt
our accuracy, which is comparable to or better than that of the original models
as well as (where applicable) previous attention-based methods (Zuo et al.,
2020; Zhang et al., 2020a).",https://github.com/yangalan123/anhp-andtt,-1
Robust Local Preserving and Global Aligning Network for Adversarial Domain Adaptation,0.192469,"Unsupervised domain adaptation (UDA) requires source domain samples with
clean ground truth labels during training. Accurately labeling a large number
of source domain samples is time-consuming and laborious. An alternative is to
utilize samples with noisy labels for training. However, training with noisy
labels can greatly reduce the performance of UDA. In this paper, we address the
problem that learning UDA models only with access to noisy labels and propose a
novel method called robust local preserving and global aligning network
(RLPGA). RLPGA improves the robustness of the label noise from two aspects. One
is learning a classifier by a robust informative-theoretic-based loss function.
The other is constructing two adjacency weight matrices and two negative weight
matrices by the proposed local preserving module to preserve the local topology
structures of input data. We conduct theoretical analysis on the robustness of
the proposed RLPGA and prove that the robust informative-theoretic-based loss
and the local preserving module are beneficial to reduce the empirical risk of
the target domain. A series of empirical studies show the effectiveness of our
proposed RLPGA.",None,-1
Feedback Gradient Descent: Efficient and Stable Optimization with Orthogonality for DNNs,0.0341998,"The optimization with orthogonality has been shown useful in training deep
neural networks (DNNs). To impose orthogonality on DNNs, both computational
efficiency and stability are important. However, existing methods utilizing
Riemannian optimization or hard constraints can only ensure stability while
those using soft constraints can only improve efficiency. In this paper, we
propose a novel method, named Feedback Gradient Descent (FGD), to our
knowledge, the first work showing high efficiency and stability simultaneously.
FGD induces orthogonality based on the simple yet indispensable Euler
discretization of a continuous-time dynamical system on the tangent bundle of
the Stiefel manifold. In particular, inspired by a numerical integration method
on manifolds called Feedback Integrators, we propose to instantiate it on the
tangent bundle of the Stiefel manifold for the first time. In the extensive
image classification experiments, FGD comprehensively outperforms the existing
state-of-the-art methods in terms of accuracy, efficiency, and stability.",None,-1
Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering,0.140885,"Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a
two-stage framework that first retrieves external knowledge given the visual
question and then predicts the answer based on the retrieved content. However,
the retrieved knowledge is often inadequate. Retrievals are frequently too
general and fail to cover specific knowledge needed to answer the question.
Also, the naturally available supervision (whether the passage contains the
correct answer) is weak and does not guarantee question relevancy. To address
these issues, we propose an Entity-Focused Retrieval (EnFoRe) model that
provides stronger supervision during training and recognizes question-relevant
entities to help retrieve more specific knowledge. Experiments show that our
EnFoRe model achieves superior retrieval performance on OK-VQA, the currently
largest outside-knowledge VQA dataset. We also combine the retrieved knowledge
with state-of-the-art VQA models, and achieve a new state-of-the-art
performance on OK-VQA.",https://github.com/jialinwu17/EnFoRe.git,-1
MGTR: End-to-End Mutual Gaze Detection with Transformer,0.0616853,"People's looking at each other or mutual gaze is ubiquitous in our daily
interactions, and detecting mutual gaze is of great significance for
understanding human social scenes. Current mutual gaze detection methods focus
on two-stage methods, whose inference speed is limited by the two-stage
pipeline and the performance in the second stage is affected by the first one.
In this paper, we propose a novel one-stage mutual gaze detection framework
called Mutual Gaze TRansformer or MGTR to perform mutual gaze detection in an
end-to-end manner. By designing mutual gaze instance triples, MGTR can detect
each human head bounding box and simultaneously infer mutual gaze relationship
based on global image information, which streamlines the whole process with
simplicity. Experimental results on two mutual gaze datasets show that our
method is able to accelerate mutual gaze detection process without losing
performance. Ablation study shows that different components of MGTR can capture
different levels of semantic information in images. Code is available at
https://github.com/Gmbition/MGTR",https://github.com/Gmbition/MGTR,-1
ComMU: Dataset for Combinatorial Music Generation,0.0820048,"Commercial adoption of automatic music composition requires the capability of
generating diverse and high-quality music suitable for the desired context
(e.g., music for romantic movies, action games, restaurants, etc.). In this
paper, we introduce combinatorial music generation, a new task to create
varying background music based on given conditions. Combinatorial music
generation creates short samples of music with rich musical metadata, and
combines them to produce a complete music. In addition, we introduce ComMU, the
first symbolic music dataset consisting of short music samples and their
corresponding 12 musical metadata for combinatorial music generation. Notable
properties of ComMU are that (1) dataset is manually constructed by
professional composers with an objective guideline that induces regularity, and
(2) it has 12 musical metadata that embraces composers' intentions. Our results
show that we can generate diverse high-quality music only with metadata, and
that our unique metadata such as track-role and extended chord quality improves
the capacity of the automatic composition. We highly recommend watching our
video before reading the paper (https://pozalabs.github.io/ComMU).",None,-1
Revealing interactions between HVDC cross-area flows and frequency stability with explainable AI,0.0838132,"The energy transition introduces more volatile energy sources into the power
grids. In this context, power transfer between different synchronous areas
through High Voltage Direct Current (HVDC) links becomes increasingly
important. Such links can balance volatile generation by enabling long-distance
transport or by leveraging their fast control behavior. Here, we investigate
the interaction of power imbalances - represented through the power grid
frequency - and power flows on HVDC links between synchronous areas in Europe.
We use explainable machine learning to identify key dependencies and
disentangle the interaction of critical features. Our results show that
market-based HVDC flows introduce deterministic frequency deviations, which
however can be mitigated through strict ramping limits. Moreover, varying HVDC
operation modes strongly affect the interaction with the grid. In particular,
we show that load-frequency control via HVDC links can both have control-like
or disturbance-like impacts on frequency stability.",None,-1
CCPT: Automatic Gameplay Testing and Validation with Curiosity-Conditioned Proximal Trajectories,0.117466,"This paper proposes a novel deep reinforcement learning algorithm to perform
automatic analysis and detection of gameplay issues in complex 3D navigation
environments. The Curiosity-Conditioned Proximal Trajectories (CCPT) method
combines curiosity and imitation learning to train agents to methodically
explore in the proximity of known trajectories derived from expert
demonstrations. We show how CCPT can explore complex environments, discover
gameplay issues and design oversights in the process, and recognize and
highlight them directly to game designers. We further demonstrate the
effectiveness of the algorithm in a novel 3D navigation environment which
reflects the complexity of modern AAA video games. Our results show a higher
level of coverage and bug discovery than baselines methods, and it hence can
provide a valuable tool for game designers to identify issues in game design
automatically.",None,-1
Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification,0.0524352,"To learn camera-view invariant features for person Re-IDentification (Re-ID),
the cross-camera image pairs of each person play an important role. However,
such cross-view training samples could be unavailable under the ISolated Camera
Supervised (ISCS) setting, e.g., a surveillance system deployed across distant
scenes. To handle this challenging problem, a new pipeline is introduced by
synthesizing the cross-camera samples in the feature space for model training.
Specifically, the feature encoder and generator are end-to-end optimized under
a novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint
learning procedure raises concern on the stability of generative model
training. Therefore, a new feature generator, $\sigma$-Regularized Conditional
Variational Autoencoder ($\sigma$-Reg.~CVAE), is proposed with theoretical and
experimental analysis on its robustness. Extensive experiments on two ISCS
person Re-ID datasets demonstrate the superiority of our CCSFG to the
competitors.",https://github.com/ftd-Wuchao/CCSFG,-1
Help Me Explore: Minimal Social Interventions for Graph-Based Autotelic Agents,0.076653,"In the quest for autonomous agents learning open-ended repertoires of skills,
most works take a Piagetian perspective: learning trajectories are the results
of interactions between developmental agents and their physical environment.
The Vygotskian perspective, on the other hand, emphasizes the centrality of the
socio-cultural environment: higher cognitive functions emerge from
transmissions of socio-cultural processes internalized by the agent. This paper
argues that both perspectives could be coupled within the learning of autotelic
agents to foster their skill acquisition. To this end, we make two
contributions: 1) a novel social interaction protocol called Help Me Explore
(HME), where autotelic agents can benefit from both individual and socially
guided exploration. In social episodes, a social partner suggests goals at the
frontier of the learning agent knowledge. In autotelic episodes, agents can
either learn to master their own discovered goals or autonomously rehearse
failed social goals; 2) GANGSTR, a graph-based autotelic agent for manipulation
domains capable of decomposing goals into sequences of intermediate sub-goals.
We show that when learning within HME, GANGSTR overcomes its individual
learning limits by mastering the most complex configurations (e.g. stacks of 5
blocks) with only few social interventions.",https://github.com/akakzia/gangstr,-1
End-to-End Image-Based Fashion Recommendation,0.259308,"In fashion-based recommendation settings, incorporating the item image
features is considered a crucial factor, and it has shown significant
improvements to many traditional models, including but not limited to matrix
factorization, auto-encoders, and nearest neighbor models. While there are
numerous image-based recommender approaches that utilize dedicated deep neural
networks, comparisons to attribute-aware models are often disregarded despite
their ability to be easily extended to leverage items' image features. In this
paper, we propose a simple yet effective attribute-aware model that
incorporates image features for better item representation learning in item
recommendation tasks. The proposed model utilizes items' image features
extracted by a calibrated ResNet50 component. We present an ablation study to
compare incorporating the image features using three different techniques into
the recommender system component that can seamlessly leverage any available
items' attributes. Experiments on two image-based real-world recommender
systems datasets show that the proposed model significantly outperforms all
state-of-the-art image-based models.",https://github.com/Shereen-Elsayed/ImgRec,-1
Mediators: Conversational Agents Explaining NLP Model Behavior,0.0212622,"The human-centric explainable artificial intelligence (HCXAI) community has
raised the need for framing the explanation process as a conversation between
human and machine. In this position paper, we establish desiderata for
Mediators, text-based conversational agents which are capable of explaining the
behavior of neural models interactively using natural language. From the
perspective of natural language processing (NLP) research, we engineer a
blueprint of such a Mediator for the task of sentiment analysis and assess how
far along current research is on the path towards dialogue-based explanations.",None,-1
Controllable Radiance Fields for Dynamic Face Synthesis,0.093731,"Recent work on 3D-aware image synthesis has achieved compelling results using
advances in neural rendering. However, 3D-aware synthesis of face dynamics
hasn't received much attention. Here, we study how to explicitly control
generative model synthesis of face dynamics exhibiting non-rigid motion (e.g.,
facial expression change), while simultaneously ensuring 3D-awareness. For this
we propose a Controllable Radiance Field (CoRF): 1) Motion control is achieved
by embedding motion features within the layered latent motion space of a
style-based generator; 2) To ensure consistency of background, motion features
and subject-specific attributes such as lighting, texture, shapes, albedo, and
identity, a face parsing net, a head regressor and an identity encoder are
incorporated. On head image/video data we show that CoRFs are 3D-aware while
enabling editing of identity, viewing directions, and motion.",https://github.com/YadiraF/DECA,-1
Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks,0.154868,"This paper considers the problem of helping humans exercise scalable
oversight over deep neural networks (DNNs). Adversarial examples can be useful
by helping to reveal weaknesses in DNNs, but they can be difficult to interpret
or draw actionable conclusions from. Some previous works have proposed using
human-interpretable adversarial attacks including copy/paste attacks in which
one natural image pasted into another causes an unexpected misclassification.
We build on these with two contributions. First, we introduce Search for
Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully
automated method for finding copy/paste attacks. Second, we use SNAFUE to red
team an ImageNet classifier. We reproduce copy/paste attacks from previous
works and find hundreds of other easily-describable vulnerabilities, all
without a human in the loop. Code is available at
https://github.com/thestephencasper/snafue",https://github.com/thestephencasper/snafue,-1
RAILD: Towards Leveraging Relation Features for Inductive Link Prediction In Knowledge Graphs,0.0520463,"Due to the open world assumption, Knowledge Graphs (KGs) are never complete.
In order to address this issue, various Link Prediction (LP) methods are
proposed so far. Some of these methods are inductive LP models which are
capable of learning representations for entities not seen during training.
However, to the best of our knowledge, none of the existing inductive LP models
focus on learning representations for unseen relations. In this work, a novel
Relation Aware Inductive Link preDiction (RAILD) is proposed for KG completion
which learns representations for both unseen entities and unseen relations. In
addition to leveraging textual literals associated with both entities and
relations by employing language models, RAILD also introduces a novel
graph-based approach to generate features for relations. Experiments are
conducted with different existing and newly created challenging benchmark
datasets and the results indicate that RAILD leads to performance improvement
over the state-of-the-art models. Moreover, since there are no existing
inductive LP models which learn representations for unseen relations, we have
created our own baselines and the results obtained with RAILD also outperform
these baselines.",https://github.com/GenetAsefa/RAILD,-1
Robust Region Feature Synthesizer for Zero-Shot Object Detection,0.167227,"Zero-shot object detection aims at incorporating class semantic vectors to
realize the detection of (both seen and) unseen classes given an unconstrained
test image. In this study, we reveal the core challenges in this research area:
how to synthesize robust region features (for unseen objects) that are as
intra-class diverse and inter-class separable as the real samples, so that
strong unseen object detectors can be trained upon them. To address these
challenges, we build a novel zero-shot object detection framework that contains
an Intra-class Semantic Diverging component and an Inter-class Structure
Preserving component. The former is used to realize the one-to-more mapping to
obtain diverse visual features from each class semantic vector, preventing
miss-classifying the real unseen objects as image backgrounds. While the latter
is used to avoid the synthesized features too scattered to mix up the
inter-class and foreground-background relationship. To demonstrate the
effectiveness of the proposed approach, comprehensive experiments on PASCAL
VOC, COCO, and DIOR datasets are conducted. Notably, our approach achieves the
new state-of-the-art performance on PASCAL VOC and COCO and it is the first
study to carry out zero-shot object detection in remote sensing imagery.",None,-1
Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping,0.0,"In machine learning, an agent needs to estimate uncertainty to efficiently
explore and adapt and to make effective decisions. A common approach to
uncertainty estimation maintains an ensemble of models. In recent years,
several approaches have been proposed for training ensembles, and conflicting
views prevail with regards to the importance of various ingredients of these
approaches. In this paper, we aim to address the benefits of two ingredients --
prior functions and bootstrapping -- which have come into question. We show
that prior functions can significantly improve an ensemble agent's joint
predictions across inputs and that bootstrapping affords additional benefits if
the signal-to-noise ratio varies across inputs. Our claims are justified by
both theoretical and experimental results.",https://github.com/deepmind/enn,-1
UniCLIP: Unified Framework for Contrastive Language-Image Pre-training,0.976482,"Pre-training vision-language models with contrastive objectives has shown
promising results that are both scalable to large uncurated datasets and
transferable to many downstream applications. Some following works have
targeted to improve data efficiency by adding self-supervision terms, but
inter-domain (image-text) contrastive loss and intra-domain (image-image)
contrastive loss are defined on individual spaces in those works, so many
feasible combinations of supervision are overlooked. To overcome this issue, we
propose UniCLIP, a Unified framework for Contrastive Language-Image
Pre-training. UniCLIP integrates the contrastive loss of both inter-domain
pairs and intra-domain pairs into a single universal space. The discrepancies
that occur when integrating contrastive loss between different domains are
resolved by the three key components of UniCLIP: (1) augmentation-aware feature
embedding, (2) MP-NCE loss, and (3) domain dependent similarity measure.
UniCLIP outperforms previous vision-language pre-training methods on various
single- and multi-modality downstream tasks. In our experiments, we show that
each component that comprises UniCLIP contributes well to the final
performance.",https://github.com/Sense-GVT/DeCLIP,-1
LCP-dropout: Compression-based Multiple Subword Segmentation for Neural Machine Translation,0.0145887,"In this study, we propose a simple and effective preprocessing method for
subword segmentation based on a data compression algorithm. Compression-based
subword segmentation has recently attracted significant attention as a
preprocessing method for training data in Neural Machine Translation. Among
them, BPE/BPE-dropout is one of the fastest and most effective method compared
to conventional approaches. However, compression-based approach has a drawback
in that generating multiple segmentations is difficult due to the determinism.
To overcome this difficulty, we focus on a probabilistic string algorithm,
called locally-consistent parsing (LCP), that has been applied to achieve
optimum compression. Employing the probabilistic mechanism of LCP, we propose
LCP-dropout for multiple subword segmentation that improves BPE/BPE-dropout,
and show that it outperforms various baselines in learning from especially
small training data.",https://github.com/moses-smt/mosesdecoder,-1
ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select,0.56112,"We study the problem of extracting N-ary relation tuples from scientific
articles. This task is challenging because the target knowledge tuples can
reside in multiple parts and modalities of the document. Our proposed method
ReSel decomposes this task into a two-stage procedure that first retrieves the
most relevant paragraph/table and then selects the target entity from the
retrieved component. For the high-level retrieval stage, ReSel designs a simple
and effective feature set, which captures multi-level lexical and semantic
similarities between the query and components. For the low-level selection
stage, ReSel designs a cross-modal entity correlation graph along with a
multi-view architecture, which models both semantic and document-structural
relations between entities. Our experiments on three scientific information
extraction datasets show that ReSel outperforms state-of-the-art baselines
significantly.",https://github.com/night-chen/ReSel,-1
Learning Personalized Human-Aware Robot Navigation Using Virtual Reality Demonstrations from a User Study,0.0570904,"For the most comfortable, human-aware robot navigation, subjective user
preferences need to be taken into account. This paper presents a novel
reinforcement learning framework to train a personalized navigation controller
along with an intuitive virtual reality demonstration interface. The conducted
user study provides evidence that our personalized approach significantly
outperforms classical approaches with more comfortable human-robot experiences.
We achieve these results using only a few demonstration trajectories from
non-expert users, who predominantly appreciate the intuitive demonstration
setup. As we show in the experiments, the learned controller generalizes well
to states not covered in the demonstration data, while still reflecting user
preferences during navigation. Finally, we transfer the navigation controller
without loss in performance to a real robot.",None,-1
Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning,0.499211,"Non-exemplar class-incremental learning is to recognize both the old and new
classes when old class samples cannot be saved. It is a challenging task since
representation optimization and feature retention can only be achieved under
supervision from new classes. To address this problem, we propose a novel
self-sustaining representation expansion scheme. Our scheme consists of a
structure reorganization strategy that fuses main-branch expansion and
side-branch updating to maintain the old features, and a main-branch
distillation scheme to transfer the invariant knowledge. Furthermore, a
prototype selection mechanism is proposed to enhance the discrimination between
the old and new classes by selectively incorporating new samples into the
distillation process. Extensive experiments on three benchmarks demonstrate
significant incremental performance, outperforming the state-of-the-art methods
by a margin of 3%, 3% and 6%, respectively.",None,-1
Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images,0.13716,"Developing an AI-assisted gland segmentation method from histology images is
critical for automatic cancer diagnosis and prognosis; however, the high cost
of pixel-level annotations hinders its applications to broader diseases.
Existing weakly-supervised semantic segmentation methods in computer vision
achieve degenerative results for gland segmentation, since the characteristics
and problems of glandular datasets are different from general object datasets.
We observe that, unlike natural images, the key problem with histology images
is the confusion of classes owning to morphological homogeneity and low color
contrast among different tissues. To this end, we propose a novel method Online
Easy Example Mining (OEEM) that encourages the network to focus on credible
supervision signals rather than noisy signals, therefore mitigating the
influence of inevitable false predictions in pseudo-masks. According to the
characteristics of glandular datasets, we design a strong framework for gland
segmentation. Our results exceed many fully-supervised methods and
weakly-supervised methods for gland segmentation over 4.4% and 6.04% at mIoU,
respectively. Code is available at https://github.com/xmed-lab/OEEM.",https://github.com/xmed-lab/OEEM,-1
Efficient Remote Photoplethysmography with Temporal Derivative Modules and Time-Shift Invariant Loss,0.336553,"We present a lightweight neural model for remote heart rate estimation
focused on the efficient spatio-temporal learning of facial
photoplethysmography (PPG) based on i) modelling of PPG dynamics by
combinations of multiple convolutional derivatives, and ii) increased
flexibility of the model to learn possible offsets between the facial video PPG
and the ground truth. PPG dynamics are modelled by a Temporal Derivative Module
(TDM) constructed by the incremental aggregation of multiple convolutional
derivatives, emulating a Taylor series expansion up to the desired order.
Robustness to ground truth offsets is handled by the introduction of TALOS
(Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based
models. We verify the effectiveness of our model by reporting accuracy and
efficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to
existing models, our approach shows competitive heart rate estimation accuracy
with a much lower number of parameters and lower computational cost.",None,-1
Scribble-Supervised LiDAR Semantic Segmentation,0.166655,"Densely annotating LiDAR point clouds remains too expensive and
time-consuming to keep up with the ever growing volume of data. While current
literature focuses on fully-supervised performance, developing efficient
methods that take advantage of realistic weak supervision have yet to be
explored. In this paper, we propose using scribbles to annotate LiDAR point
clouds and release ScribbleKITTI, the first scribble-annotated dataset for
LiDAR semantic segmentation. Furthermore, we present a pipeline to reduce the
performance gap that arises when using such weak annotations. Our pipeline
comprises of three stand-alone contributions that can be combined with any
LiDAR semantic segmentation model to achieve up to 95.7% of the
fully-supervised performance while using only 8% labeled points. Our scribble
annotations and code are available at github.com/ouenal/scribblekitti.",https://github.com/ouenal/scribblekitti,-1
Depth Estimation with Simplified Transformer,0.0695427,"Transformer and its variants have shown state-of-the-art results in many
vision tasks recently, ranging from image classification to dense prediction.
Despite of their success, limited work has been reported on improving the model
efficiency for deployment in latency-critical applications, such as autonomous
driving and robotic navigation. In this paper, we aim at improving upon the
existing transformers in vision, and propose a method for self-supervised
monocular Depth Estimation with Simplified Transformer (DEST), which is
efficient and particularly suitable for deployment on GPU-based platforms.
Through strategic design choices, our model leads to significant reduction in
model size, complexity, as well as inference latency, while achieving superior
accuracy as compared to state-of-the-art. We also show that our design
generalize well to other dense prediction task without bells and whistles.",None,-1
Towards Climate Awareness in NLP Research,0.270496,"The climate impact of AI, and NLP research in particular, has become a
serious issue given the enormous amount of energy that is increasingly being
used for training and running computational models. Consequently, increasing
focus is placed on efficient NLP. However, this important initiative lacks
simple guidelines that would allow for systematic climate reporting of NLP
research. We argue that this deficiency is one of the reasons why very few
publications in NLP report key figures that would allow a more thorough
examination of environmental impact. As a remedy, we propose a climate
performance model card with the primary purpose of being practically usable
with only limited information about experiments and the underlying computer
hardware. We describe why this step is essential to increase awareness about
the environmental impact of NLP research and, thereby, paving the way for more
thorough discussions.",https://github.com/danielhers/climate-awareness-nlp,-1
Real Time Egocentric Segmentation for Video-self Avatar in Mixed Reality,0.0571824,"In this work we present our real-time egocentric body segmentation algorithm.
Our algorithm achieves a frame rate of 66 fps for an input resolution of
640x480, thanks to our shallow network inspired in Thundernet's architecture.
Besides, we put a strong emphasis on the variability of the training data. More
concretely, we describe the creation process of our Egocentric Bodies
(EgoBodies) dataset, composed of almost 10,000 images from three datasets,
created both from synthetic methods and real capturing. We conduct experiments
to understand the contribution of the individual datasets; compare Thundernet
model trained with EgoBodies with simpler and more complex previous approaches
and discuss their corresponding performance in a real-life setup in terms of
segmentation quality and inference times. The described trained semantic
segmentation algorithm is already integrated in an end-to-end system for Mixed
Reality (MR), making it possible for users to see his/her own body while being
immersed in a MR scene.",None,-1
"Sampling-based inference for large linear models, with application to linearised Laplace",0.0546013,"Large-scale linear models are ubiquitous throughout machine learning, with
contemporary application as surrogate models for neural network uncertainty
quantification; that is, the linearised Laplace method. Alas, the computational
cost associated with Bayesian linear models constrains this method's
application to small networks, small output spaces and small datasets. We
address this limitation by introducing a scalable sample-based Bayesian
inference method for conjugate Gaussian multi-output linear models, together
with a matching method for hyperparameter (regularisation) selection.
Furthermore, we use a classic feature normalisation method (the g-prior) to
resolve a previously highlighted pathology of the linearised Laplace method.
Together, these contributions allow us to perform linearised neural network
inference with ResNet-18 on CIFAR100 (11M parameters, 100 outputs x 50k
datapoints), with ResNet-50 on Imagenet (50M parameters, 1000 outputs x 1.2M
datapoints) and with a U-Net on a high-resolution tomographic reconstruction
task (2M parameters, 251k output~dimensions).",None,-1
AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation,0.105288,"Dense retrievers have made significant strides in text retrieval and
open-domain question answering, even though most achievements were made
possible only with large amounts of human supervision. In this work, we aim to
develop unsupervised methods by proposing two methods that create pseudo
query-document pairs and train dense retrieval models in an annotation-free and
scalable manner: query extraction and transferred query generation. The former
method produces pseudo queries by selecting salient spans from the original
document. The latter utilizes generation models trained for other NLP tasks
(e.g., summarization) to produce pseudo queries. Extensive experiments show
that models trained with the proposed augmentation methods can perform
comparably well (or better) to multiple strong baselines. Combining those
strategies leads to further improvements, achieving the state-of-the-art
performance of unsupervised dense retrieval on both BEIR and ODQA datasets.",None,-1
GCS-Q: Quantum Graph Coalition Structure Generation,0.0752265,"The problem of generating an optimal coalition structure for a given
coalition game of rational agents is to find a partition that maximizes their
social welfare and is known to be NP-hard. This paper proposes GCS-Q, a novel
quantum-supported solution for Induced Subgraph Games (ISGs) in coalition
structure generation. GCS-Q starts by considering the grand coalition as
initial coalition structure and proceeds by iteratively splitting the
coalitions into two nonempty subsets to obtain a coalition structure with a
higher coalition value. In particular, given an $n$-agent ISG, the GCS-Q solves
the optimal split problem $\mathcal{O} (n)$ times using a quantum annealing
device, exploring $\mathcal{O}(2^n)$ partitions at each step. We show that
GCS-Q outperforms the currently best classical solvers with its runtime in the
order of $n^2$ and an expected worst-case approximation ratio of $93\%$ on
standard benchmark datasets.",None,-1
Audio-Adaptive Activity Recognition Across Video Domains,0.385063,"This paper strives for activity recognition under domain shift, for example
caused by change of scenery or camera viewpoint. The leading approaches reduce
the shift in activity appearance by adversarial training and self-supervised
learning. Different from these vision-focused works we leverage activity sounds
for domain adaptation as they have less variance across domains and can
reliably indicate which activities are not happening. We propose an
audio-adaptive encoder and associated learning methods that discriminatively
adjust the visual feature representation as well as addressing shifts in the
semantic distribution. To further eliminate domain-specific features and
include domain-invariant activity sounds for recognition, an audio-infused
recognizer is proposed, which effectively models the cross-modal interaction
across domains. We also introduce the new task of actor shift, with a
corresponding audio-visual dataset, to challenge our method with situations
where the activity appearance changes dramatically. Experiments on this
dataset, EPIC-Kitchens and CharadesEgo show the effectiveness of our approach.",https://github.com/open-mmlab/mmaction2,-1
Unified Vision and Language Prompt Learning,0.515446,"Prompt tuning, a parameter- and data-efficient transfer learning paradigm
that tunes only a small number of parameters in a model's input space, has
become a trend in the vision community since the emergence of large
vision-language models like CLIP. We present a systematic study on two
representative prompt tuning methods, namely text prompt tuning and visual
prompt tuning. A major finding is that none of the unimodal prompt tuning
methods performs consistently well: text prompt tuning fails on data with high
intra-class visual variances while visual prompt tuning cannot handle low
inter-class variances. To combine the best from both worlds, we propose a
simple approach called Unified Prompt Tuning (UPT), which essentially learns a
tiny neural network to jointly optimize prompts across different modalities.
Extensive experiments on over 11 vision datasets show that UPT achieves a
better trade-off than the unimodal counterparts on few-shot learning
benchmarks, as well as on domain generalization benchmarks. Code and models
will be released to facilitate future research.",https://github.com/yuhangzang/UPT,-1
Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction,0.0834575,"We present Claim-Dissector: a novel latent variable model for fact-checking
and analysis, which given a claim and a set of retrieved evidences jointly
learns to identify: (i) the relevant evidences to the given claim, (ii) the
veracity of the claim. We propose to disentangle the per-evidence relevance
probability and its contribution to the final veracity probability in an
interpretable way -- the final veracity probability is proportional to a linear
ensemble of per-evidence relevance probabilities. In this way, the individual
contributions of evidences towards the final predicted probability can be
identified. In per-evidence relevance probability, our model can further
distinguish whether each relevant evidence is supporting (S) or refuting (R)
the claim. This allows to quantify how much the S/R probability contributes to
the final verdict or to detect disagreeing evidence.
  Despite its interpretable nature, our system achieves results competitive
with state-of-the-art on the FEVER dataset, as compared to typical two-stage
system pipelines, while using significantly fewer parameters. It also sets new
state-of-the-art on FAVIQ and RealFC datasets. Furthermore, our analysis shows
that our model can learn fine-grained relevance cues while using coarse-grained
supervision, and we demonstrate it in 2 ways. (i) We show that our model can
achieve competitive sentence recall while using only paragraph-level relevance
supervision. (ii) Traversing towards the finest granularity of relevance, we
show that our model is capable of identifying relevance at the token level. To
do this, we present a new benchmark TLR-FEVER focusing on token-level
interpretability -- humans annotate tokens in relevant evidences they
considered essential when making their judgment. Then we measure how similar
are these annotations to the tokens our model is focusing on.",https://github.com/KNOT-FIT-BUT/ClaimDissector,-1
ImplantFormer: Vision Transformer based Implant Position Regression Using Dental CBCT Data,0.140551,"Implant prosthesis is the most appropriate treatment for dentition defect or
dentition loss, which usually involves a surgical guide design process to
decide the implant position. However, such design heavily relies on the
subjective experiences of dentists. In this paper, a transformer-based Implant
Position Regression Network, ImplantFormer, is proposed to automatically
predict the implant position based on the oral CBCT data. We creatively propose
to predict the implant position using the 2D axial view of the tooth crown area
and fit a centerline of the implant to obtain the actual implant position at
the tooth root. Convolutional stem and decoder are designed to coarsely extract
image features before the operation of patch embedding and integrate
multi-level feature maps for robust prediction, respectively. As both
long-range relationship and local features are involved, our approach can
better represent global information and achieves better location performance.
Extensive experiments on a dental implant dataset through five-fold
cross-validation demonstrated that the proposed ImplantFormer achieves superior
performance than existing methods.",None,-1
Retrieval of surgical phase transitions using reinforcement learning,0.0471553,"In minimally invasive surgery, surgical workflow segmentation from video
analysis is a well studied topic. The conventional approach defines it as a
multi-class classification problem, where individual video frames are
attributed a surgical phase label.
  We introduce a novel reinforcement learning formulation for offline phase
transition retrieval. Instead of attempting to classify every video frame, we
identify the timestamp of each phase transition. By construction, our model
does not produce spurious and noisy phase transitions, but contiguous phase
blocks. We investigate two different configurations of this model. The first
does not require processing all frames in a video (only <60% and <20% of frames
in 2 different applications), while producing results slightly under the
state-of-the-art accuracy. The second configuration processes all video frames,
and outperforms the state-of-the art at a comparable computational cost.
  We compare our method against the recent top-performing frame-based
approaches TeCNO and Trans-SVNet on the public dataset Cholec80 and also on an
in-house dataset of laparoscopic sacrocolpopexy. We perform both a frame-based
(accuracy, precision, recall and F1-score) and an event-based (event ratio)
evaluation of our algorithms.",None,-1
Admissible Policy Teaching through Reward Design,0.151813,"We study reward design strategies for incentivizing a reinforcement learning
agent to adopt a policy from a set of admissible policies. The goal of the
reward designer is to modify the underlying reward function cost-efficiently
while ensuring that any approximately optimal deterministic policy under the
new reward function is admissible and performs well under the original reward
function. This problem can be viewed as a dual to the problem of optimal reward
poisoning attacks: instead of forcing an agent to adopt a specific policy, the
reward designer incentivizes an agent to avoid taking actions that are
inadmissible in certain states. Perhaps surprisingly, and in contrast to the
problem of optimal reward poisoning attacks, we first show that the reward
design problem for admissible policy teaching is computationally challenging,
and it is NP-hard to find an approximately optimal reward modification. We then
proceed by formulating a surrogate problem whose optimal solution approximates
the optimal solution to the reward design problem in our setting, but is more
amenable to optimization techniques and analysis. For this surrogate problem,
we present characterization results that provide bounds on the value of the
optimal solution. Finally, we design a local search algorithm to solve the
surrogate problem and showcase its utility using simulation-based experiments.",None,-1
Rank-N-Contrast: Learning Continuous Representations for Regression,0.108341,"Deep regression models typically learn in an end-to-end fashion without
explicitly emphasizing a regression-aware representation. Consequently, the
learned representations exhibit fragmentation and fail to capture the
continuous nature of sample orders, inducing suboptimal results across a wide
range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a
framework that learns continuous representations for regression by contrasting
samples against each other based on their rankings in the target space. We
demonstrate, theoretically and empirically, that RNC guarantees the desired
order of learned representations in accordance with the target orders, enjoying
not only better performance but also significantly improved robustness,
efficiency, and generalization. Extensive experiments using five real-world
regression datasets that span computer vision, human-computer interaction, and
healthcare verify that RNC achieves state-of-the-art performance, highlighting
its intriguing properties including better data efficiency, robustness to
spurious targets and data corruptions, and generalization to distribution
shifts. Code is available at: https://github.com/kaiwenzha/Rank-N-Contrast.",https://github.com/kaiwenzha/Rank-N-Contrast,-1
SALTED: A Framework for SAlient Long-Tail Translation Error Detection,0.568988,"Traditional machine translation (MT) metrics provide an average measure of
translation quality that is insensitive to the long tail of behavioral problems
in MT. Examples include translation of numbers, physical units, dropped content
and hallucinations. These errors, which occur rarely and unpredictably in
Neural Machine Translation (NMT), greatly undermine the reliability of
state-of-the-art MT systems. Consequently, it is important to have visibility
into these problems during model development. Towards this direction, we
introduce SALTED, a specifications-based framework for behavioral testing of MT
models that provides fine-grained views of salient long-tail errors, permitting
trustworthy visibility into previously invisible problems. At the core of our
approach is the development of high-precision detectors that flag errors (or
alternatively, verify output correctness) between a source sentence and a
system output. We demonstrate that such detectors could be used not just to
identify salient long-tail errors in MT systems, but also for higher-recall
filtering of the training data, fixing targeted errors with model fine-tuning
in NMT and generating novel data for metamorphic testing to elicit further bugs
in models.",https://github.com/pytorch/fairseq/tree/main/examples/wmt21,-1
Breaking Bad News in the Era of Artificial Intelligence and Algorithmic Medicine: An Exploration of Disclosure and its Ethical Justification using the Hedonic Calculus,0.0,"An appropriate ethical framework around the use of Artificial Intelligence
(AI) in healthcare has become a key desirable with the increasingly widespread
deployment of this technology. Advances in AI hold the promise of improving the
precision of outcome prediction at the level of the individual. However, the
addition of these technologies to patient-clinician interactions, as with any
complex human interaction, has potential pitfalls. While physicians have always
had to carefully consider the ethical background and implications of their
actions, detailed deliberations around fast-moving technological progress may
not have kept up. We use a common but key challenge in healthcare interactions,
the disclosure of bad news (likely imminent death), to illustrate how the
philosophical framework of the 'Felicific Calculus' developed in the 18th
century by Jeremy Bentham, may have a timely quasi-quantitative application in
the age of AI. We show how this ethical algorithm can be used to assess, across
seven mutually exclusive and exhaustive domains, whether an AI-supported action
can be morally justified.",None,-1
Meta-Learning Sparse Compression Networks,0.0702609,"Recent work in Deep Learning has re-imagined the representation of data as
functions mapping from a coordinate space to an underlying continuous signal.
When such functions are approximated by neural networks this introduces a
compelling alternative to the more common multi-dimensional array
representation. Recent work on such Implicit Neural Representations (INRs) has
shown that - following careful architecture search - INRs can outperform
established compression methods such as JPEG (e.g. Dupont et al., 2021). In
this paper, we propose crucial steps towards making such ideas scalable:
Firstly, we employ state-of-the-art network sparsification techniques to
drastically improve compression. Secondly, introduce the first method allowing
for sparsification to be employed in the inner-loop of commonly used
Meta-Learning algorithms, drastically improving both compression and the
computational cost of learning INRs. The generality of this formalism allows us
to present results on diverse data modalities such as images, manifolds, signed
distance functions, 3D shapes and scenes, several of which establish new
state-of-the-art results.",None,-1
StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,0.547094,"Inferring spatial relations in natural language is a crucial ability an
intelligent system should possess. The bAbI dataset tries to capture tasks
relevant to this domain (task 17 and 19). However, these tasks have several
limitations. Most importantly, they are limited to fixed expressions, they are
limited in the number of reasoning steps required to solve them, and they fail
to test the robustness of models to input that contains irrelevant or redundant
information. In this paper, we present a new Question-Answering dataset called
StepGame for robust multi-hop spatial reasoning in texts. Our experiments
demonstrate that state-of-the-art models on the bAbI dataset struggle on the
StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented
Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental
results on both datasets show that our model outperforms all the baselines with
superior generalization and robustness performance.",None,-1
Gaussian Multi-head Attention for Simultaneous Machine Translation,0.299599,"Simultaneous machine translation (SiMT) outputs translation while receiving
the streaming source inputs, and hence needs a policy to determine where to
start translating. The alignment between target and source words often implies
the most informative source word for each target word, and hence provides the
unified control over translation quality and latency, but unfortunately the
existing SiMT methods do not explicitly model the alignment to perform the
control. In this paper, we propose Gaussian Multi-head Attention (GMA) to
develop a new SiMT policy by modeling alignment and translation in a unified
manner. For SiMT policy, GMA models the aligned source position of each target
word, and accordingly waits until its aligned position to start translating. To
integrate the learning of alignment into the translation model, a Gaussian
distribution centered on predicted aligned position is introduced as an
alignment-related prior, which cooperates with translation-related soft
attention to determine the final attention. Experiments on En-Vi and De-En
tasks show that our method outperforms strong baselines on the trade-off
between translation and latency.",https://github.com/ictnlp/GMA,-1
Eliciting Best Practices for Collaboration with Computational Notebooks,0.0658785,"Despite the widespread adoption of computational notebooks, little is known
about best practices for their usage in collaborative contexts. In this paper,
we fill this gap by eliciting a catalog of best practices for collaborative
data science with computational notebooks. With this aim, we first look for
best practices through a multivocal literature review. Then, we conduct
interviews with professional data scientists to assess their awareness of these
best practices. Finally, we assess the adoption of best practices through the
analysis of 1,380 Jupyter notebooks retrieved from the Kaggle platform.
Findings reveal that experts are mostly aware of the best practices and tend to
adopt them in their daily work. Nonetheless, they do not consistently follow
all the recommendations as, depending on specific contexts, some are deemed
unfeasible or counterproductive due to the lack of proper tool support. As
such, we envision the design of notebook solutions that allow data scientists
not to have to prioritize exploration and rapid prototyping over writing code
of quality.",None,-1
RangeUDF: Semantic Surface Reconstruction from 3D Point Clouds,0.0692941,"We present RangeUDF, a new implicit representation based framework to recover
the geometry and semantics of continuous 3D scene surfaces from point clouds.
Unlike occupancy fields or signed distance fields which can only model closed
3D surfaces, our approach is not restricted to any type of topology. Being
different from the existing unsigned distance fields, our framework does not
suffer from any surface ambiguity. In addition, our RangeUDF can jointly
estimate precise semantics for continuous surfaces. The key to our approach is
a range-aware unsigned distance function together with a surface-oriented
semantic segmentation module. Extensive experiments show that RangeUDF clearly
surpasses state-of-the-art approaches for surface reconstruction on four point
cloud datasets. Moreover, RangeUDF demonstrates superior generalization
capability across multiple unseen datasets, which is nearly impossible for all
existing approaches.",https://github.com/vLAR-group/RangeUDF,-1
CORL: Research-oriented Deep Offline Reinforcement Learning Library,0.482613,"CORL is an open-source library that provides thoroughly benchmarked
single-file implementations of both deep offline and offline-to-online
reinforcement learning algorithms. It emphasizes a simple developing experience
with a straightforward codebase and a modern analysis tracking tool. In CORL,
we isolate methods implementation into separate single files, making
performance-relevant details easier to recognize. Additionally, an experiment
tracking feature is available to help log metrics, hyperparameters,
dependencies, and more to the cloud. Finally, we have ensured the reliability
of the implementations by benchmarking commonly employed D4RL datasets
providing a transparent source of results that can be reused for robust
evaluation tools such as performance profiles, probability of improvement, or
expected online performance.",https://github.com/corl-team/CORL,-1
S$^2$SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers,0.656669,"The task of converting a natural language question into an executable SQL
query, known as text-to-SQL, is an important branch of semantic parsing. The
state-of-the-art graph-based encoder has been successfully used in this task
but does not model the question syntax well. In this paper, we propose
S$^2$SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL
parsers, which effectively leverages the syntactic dependency information of
questions in text-to-SQL to improve the performance. We also employ the
decoupling constraint to induce diverse relational edge embedding, which
further improves the network's performance. Experiments on the Spider and
robustness setting Spider-Syn demonstrate that the proposed approach
outperforms all existing methods when pre-training models are used, resulting
in a performance ranks first on the Spider leaderboard.",None,-1
Private Quantiles Estimation in the Presence of Atoms,0.0431577,"We consider the differentially private estimation of multiple quantiles (MQ)
of a distribution from a dataset, a key building block in modern data analysis.
We apply the recent non-smoothed Inverse Sensitivity (IS) mechanism to this
specific problem. We establish that the resulting method is closely related to
the recently published ad hoc algorithm JointExp. In particular, they share the
same computational complexity and a similar efficiency. We prove the
statistical consistency of these two algorithms for continuous distributions.
Furthermore, we demonstrate both theoretically and empirically that this method
suffers from an important lack of performance in the case of peaked
distributions, which can degrade up to a potentially catastrophic impact in the
presence of atoms. Its smoothed version (i.e. by applying a max kernel to its
output density) would solve this problem, but remains an open challenge to
implement. As a proxy, we propose a simple and numerically efficient method
called Heuristically Smoothed JointExp (HSJointExp), which is endowed with
performance guarantees for a broad class of distributions and achieves results
that are orders of magnitude better on problematic datasets.",https://github.com/opendp/smartnoise-core,-1
MoEC: Mixture of Expert Clusters,0.202376,"Sparsely Mixture of Experts (MoE) has received great interest due to its
promising scaling capability with affordable computational overhead. MoE
converts dense layers into sparse experts, and utilizes a gated routing network
to make experts conditionally activated. However, as the number of experts
grows, MoE with outrageous parameters suffers from overfitting and sparse data
allocation. Such problems are especially severe on tasks with limited data,
thus hindering the progress for MoE models to improve performance by scaling
up. In this work, we propose Mixture of Expert Clusters - a general approach to
enable expert layers to learn more diverse and appropriate knowledge by
imposing variance-based constraints on the routing stage. We further propose a
cluster-level expert dropout strategy specifically designed for the expert
cluster structure. Our experiments reveal that MoEC could improve performance
on machine translation and natural language understanding tasks, and raise the
performance upper bound for scaling up experts under limited data. We also
verify that MoEC plays a positive role in mitigating overfitting and sparse
data allocation.",None,-1
SVG Vector Font Generation for Chinese Characters with Transformer,0.269779,"Designing fonts for Chinese characters is highly labor-intensive and
time-consuming. While the latest methods successfully generate the English
alphabet vector font, despite the high demand for automatic font generation,
Chinese vector font generation has been an unsolved problem owing to its
complex shape and numerous characters. This study addressed the problem of
automatically generating Chinese vector fonts from only a single style and
content reference. We proposed a novel network architecture with Transformer
and loss functions to capture structural features without differentiable
rendering. Although the dataset range was still limited to the sans-serif
family, we successfully generated the Chinese vector font for the first time
using the proposed method.",None,-1
Normalizing Flows for Human Pose Anomaly Detection,0.19621,"Video anomaly detection is an ill-posed problem because it relies on many
parameters such as appearance, pose, camera angle, background, and more. We
distill the problem to anomaly detection of human pose, thus decreasing the
risk of nuisance parameters such as appearance affecting the result. Focusing
on pose alone also has the side benefit of reducing bias against distinct
minority groups. Our model works directly on human pose graph sequences and is
exceptionally lightweight (~1K parameters), capable of running on any machine
able to run the pose estimation with negligible additional resources. We
leverage the highly compact pose representation in a normalizing flows
framework, which we extend to tackle the unique characteristics of
spatio-temporal pose data and show its advantages in this use case. The
algorithm is quite general and can handle training data of only normal examples
as well as a supervised setting that consists of labeled normal and abnormal
examples. We report state-of-the-art results on two anomaly detection
benchmarks - the unsupervised ShanghaiTech dataset and the recent supervised
UBnormal dataset.",https://github.com/orhir/STG-NF,-1
YOLO and Mask R-CNN for Vehicle Number Plate Identification,0.209749,"License plate scanners have grown in popularity in parking lots during the
past few years. In order to quickly identify license plates, traditional plate
recognition devices used in parking lots employ a fixed source of light and
shooting angles. For skewed angles, such as license plate images taken with
ultra-wide angle or fisheye lenses, deformation of the license plate
recognition plate can also be quite severe, impairing the ability of standard
license plate recognition systems to identify the plate. Mask RCNN gadget that
may be utilised for oblique pictures and various shooting angles. The results
of the experiments show that the suggested design will be capable of
classifying license plates with bevel angles larger than 0/60. Character
recognition using the suggested Mask R-CNN approach has advanced significantly
as well. The proposed Mask R-CNN method has also achieved significant progress
in character recognition, which is tilted more than 45 degrees as compared to
the strategy of employing the YOLOv2 model. Experiment results also suggest
that the methodology presented in the open data plate collecting is better than
other techniques (known as the AOLP dataset).",None,-1
Generating Multiple-Length Summaries via Reinforcement Learning for Unsupervised Sentence Summarization,0.0286216,"Sentence summarization shortens given texts while maintaining core contents
of the texts. Unsupervised approaches have been studied to summarize texts
without human-written summaries. However, recent unsupervised models are
extractive, which remove words from texts and thus they are less flexible than
abstractive summarization. In this work, we devise an abstractive model based
on reinforcement learning without ground-truth summaries. We formulate the
unsupervised summarization based on the Markov decision process with rewards
representing the summary quality. To further enhance the summary quality, we
develop a multi-summary learning mechanism that generates multiple summaries
with varying lengths for a given text, while making the summaries mutually
enhance each other. Experimental results show that the proposed model
substantially outperforms both abstractive and extractive models, yet
frequently generating new words not contained in input texts.",https://github.com/dmhyun/MSRP,-1
Proximal PanNet: A Model-Based Deep Network for Pansharpening,0.0667985,"Recently, deep learning techniques have been extensively studied for
pansharpening, which aims to generate a high resolution multispectral (HRMS)
image by fusing a low resolution multispectral (LRMS) image with a high
resolution panchromatic (PAN) image. However, existing deep learning-based
pansharpening methods directly learn the mapping from LRMS and PAN to HRMS.
These network architectures always lack sufficient interpretability, which
limits further performance improvements. To alleviate this issue, we propose a
novel deep network for pansharpening by combining the model-based methodology
with the deep learning method. Firstly, we build an observation model for
pansharpening using the convolutional sparse coding (CSC) technique and design
a proximal gradient algorithm to solve this model. Secondly, we unfold the
iterative algorithm into a deep network, dubbed as Proximal PanNet, by learning
the proximal operators using convolutional neural networks. Finally, all the
learnable modules can be automatically learned in an end-to-end manner.
Experimental results on some benchmark datasets show that our network performs
better than other advanced methods both quantitatively and qualitatively.",None,-1
Sparse Conditional Hidden Markov Model for Weakly Supervised Named Entity Recognition,0.113824,"Weakly supervised named entity recognition methods train label models to
aggregate the token annotations of multiple noisy labeling functions (LFs)
without seeing any manually annotated labels. To work well, the label model
needs to contextually identify and emphasize well-performed LFs while
down-weighting the under-performers. However, evaluating the LFs is challenging
due to the lack of ground truths. To address this issue, we propose the sparse
conditional hidden Markov model (Sparse-CHMM). Instead of predicting the entire
emission matrix as other HMM-based methods, Sparse-CHMM focuses on estimating
its diagonal elements, which are considered as the reliability scores of the
LFs. The sparse scores are then expanded to the full-fledged emission matrix
with pre-defined expansion functions. We also augment the emission with
weighted XOR scores, which track the probabilities of an LF observing incorrect
entities. Sparse-CHMM is optimized through unsupervised learning with a
three-stage training pipeline that reduces the training difficulty and prevents
the model from falling into local optima. Compared with the baselines in the
Wrench benchmark, Sparse-CHMM achieves a 3.01 average F1 score improvement on
five comprehensive datasets. Experiments show that each component of
Sparse-CHMM is effective, and the estimated LF reliabilities strongly correlate
with true LF F1 scores.",https://github.com/Yinghao-Li/Sparse-CHMM,-1
A Graph-Based Method for Soccer Action Spotting Using Unsupervised Player Classification,0.160044,"Action spotting in soccer videos is the task of identifying the specific time
when a certain key action of the game occurs. Lately, it has received a large
amount of attention and powerful methods have been introduced. Action spotting
involves understanding the dynamics of the game, the complexity of events, and
the variation of video sequences. Most approaches have focused on the latter,
given that their models exploit the global visual features of the sequences. In
this work, we focus on the former by (a) identifying and representing the
players, referees, and goalkeepers as nodes in a graph, and by (b) modeling
their temporal interactions as sequences of graphs. For the player
identification, or player classification task, we obtain an accuracy of 97.72%
in our annotated benchmark. For the action spotting task, our method obtains an
overall performance of 57.83% average-mAP by combining it with other
audiovisual modalities. This performance surpasses similar graph-based methods
and has competitive results with heavy computing methods. Code and data are
available at https://github.com/IPCV/soccer_action_spotting.",https://github.com/IPCV/soccer_action_spotting,-1
MIRROR: Differentiable Deep Social Projection for Assistive Human-Robot Communication,0.231766,"Communication is a hallmark of intelligence. In this work, we present MIRROR,
an approach to (i) quickly learn human models from human demonstrations, and
(ii) use the models for subsequent communication planning in assistive
shared-control settings. MIRROR is inspired by social projection theory, which
hypothesizes that humans use self-models to understand others. Likewise, MIRROR
leverages self-models learned using reinforcement learning to bootstrap human
modeling. Experiments with simulated humans show that this approach leads to
rapid learning and more robust models compared to existing behavioral cloning
and state-of-the-art imitation learning methods. We also present a
human-subject study using the CARLA simulator which shows that (i) MIRROR is
able to scale to complex domains with high-dimensional observations and
complicated world physics and (ii) provides effective assistive communication
that enabled participants to drive more safely in adverse weather conditions.",https://github.com/clear-nus/mirror,-1
DialogUSR: Complex Dialogue Utterance Splitting and Reformulation for Multiple Intent Detection,0.0696536,"While interacting with chatbots, users may elicit multiple intents in a
single dialogue utterance. Instead of training a dedicated multi-intent
detection model, we propose DialogUSR, a dialogue utterance splitting and
reformulation task that first splits multi-intent user query into several
single-intent sub-queries and then recovers all the coreferred and omitted
information in the sub-queries. DialogUSR can serve as a plug-in and
domain-agnostic module that empowers the multi-intent detection for the
deployed chatbots with minimal efforts. We collect a high-quality naturally
occurring dataset that covers 23 domains with a multi-step crowd-souring
procedure. To benchmark the proposed dataset, we propose multiple action-based
generative models that involve end-to-end and two-stage training, and conduct
in-depth analyses on the pros and cons of the proposed baselines.",https://github.com/MrZhengXin/multi_intent_2022,-1
Transformers are Adaptable Task Planners,0.0334752,"Every home is different, and every person likes things done in their
particular way. Therefore, home robots of the future need to both reason about
the sequential nature of day-to-day tasks and generalize to user's preferences.
To this end, we propose a Transformer Task Planner(TTP) that learns high-level
actions from demonstrations by leveraging object attribute-based
representations. TTP can be pre-trained on multiple preferences and shows
generalization to unseen preferences using a single demonstration as a prompt
in a simulated dishwasher loading task. Further, we demonstrate real-world dish
rearrangement using TTP with a Franka Panda robotic arm, prompted using a
single human demonstration.",None,-1
Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors,0.230276,"Robustness of machine learning models on ever-changing real-world data is
critical, especially for applications affecting human well-being such as
content moderation. New kinds of abusive language continually emerge in online
discussions in response to current events (e.g., COVID-19), and the deployed
abuse detection systems should be updated regularly to remain accurate. In this
paper, we show that general abusive language classifiers tend to be fairly
reliable in detecting out-of-domain explicitly abusive utterances but fail to
detect new types of more subtle, implicit abuse. Next, we propose an
interpretability technique, based on the Testing Concept Activation Vector
(TCAV) method from computer vision, to quantify the sensitivity of a trained
model to the human-defined concepts of explicit and implicit abusive language,
and use that to explain the generalizability of the model on new data, in this
case, COVID-related anti-Asian hate speech. Extending this technique, we
introduce a novel metric, Degree of Explicitness, for a single instance and
show that the new metric is beneficial in suggesting out-of-domain unlabeled
examples to effectively enrich the training data with informative, implicitly
abusive texts.",https://github.com/IsarNejad/TCAV-for-Text-Classifiers,-1
Face Relighting with Geometrically Consistent Shadows,0.260233,"Most face relighting methods are able to handle diffuse shadows, but struggle
to handle hard shadows, such as those cast by the nose. Methods that propose
techniques for handling hard shadows often do not produce geometrically
consistent shadows since they do not directly leverage the estimated face
geometry while synthesizing them. We propose a novel differentiable algorithm
for synthesizing hard shadows based on ray tracing, which we incorporate into
training our face relighting model. Our proposed algorithm directly utilizes
the estimated face geometry to synthesize geometrically consistent hard
shadows. We demonstrate through quantitative and qualitative experiments on
Multi-PIE and FFHQ that our method produces more geometrically consistent
shadows than previous face relighting methods while also achieving
state-of-the-art face relighting performance under directional lighting. In
addition, we demonstrate that our differentiable hard shadow modeling improves
the quality of the estimated face geometry over diffuse shading models.",https://github.com/andrewhou1/GeomConsistentFR,-1
Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss,0.734401,"In this paper, we study color image inpainting as a pure quaternion matrix
completion problem. In the literature, the theoretical guarantee for quaternion
matrix completion is not well-established. Our main aim is to propose a new
minimization problem with an objective combining nuclear norm and a quadratic
loss weighted among three channels. To fill the theoretical vacancy, we obtain
the error bound in both clean and corrupted regimes, which relies on some new
results of quaternion matrices. A general Gaussian noise is considered in
robust completion where all observations are corrupted. Motivated by the error
bound, we propose to handle unbalanced or correlated noise via a cross-channel
weight in the quadratic loss, with the main purpose of rebalancing noise level,
or removing noise correlation. Extensive experimental results on synthetic and
color image data are presented to confirm and demonstrate our theoretical
findings.",None,-1
Hero-Gang Neural Model For Named Entity Recognition,0.0435158,"Named entity recognition (NER) is a fundamental and important task in NLP,
aiming at identifying named entities (NEs) from free text. Recently, since the
multi-head attention mechanism applied in the Transformer model can effectively
capture longer contextual information, Transformer-based models have become the
mainstream methods and have achieved significant performance in this task.
Unfortunately, although these models can capture effective global context
information, they are still limited in the local feature and position
information extraction, which is critical in NER. In this paper, to address
this limitation, we propose a novel Hero-Gang Neural structure (HGN), including
the Hero and Gang module, to leverage both global and local information to
promote NER. Specifically, the Hero module is composed of a Transformer-based
encoder to maintain the advantage of the self-attention mechanism, and the Gang
module utilizes a multi-window recurrent module to extract local features and
position information under the guidance of the Hero module. Afterward, the
proposed multi-window attention effectively combines global information and
multiple local features for predicting entity labels. Experimental results on
several benchmark datasets demonstrate the effectiveness of our proposed model.",https://github.com/jinpeng01/HGN,-1
The Quest for a Common Model of the Intelligent Decision Maker,0.373907,"The premise of the Multi-disciplinary Conference on Reinforcement Learning
and Decision Making is that multiple disciplines share an interest in
goal-directed decision making over time. The idea of this paper is to sharpen
and deepen this premise by proposing a perspective on the decision maker that
is substantive and widely held across psychology, artificial intelligence,
economics, control theory, and neuroscience, which I call the ""common model of
the intelligent agent"". The common model does not include anything specific to
any organism, world, or application domain. The common model does include
aspects of the decision maker's interaction with its world (there must be input
and output, and a goal) and internal components of the decision maker (for
perception, decision-making, internal evaluation, and a world model). I
identify these aspects and components, note that they are given different names
in different disciplines but refer essentially to the same ideas, and discuss
the challenges and benefits of devising a neutral terminology that can be used
across disciplines. It is time to recognize and build on the convergence of
multiple diverse disciplines on a substantive common model of the intelligent
agent.",None,-1
MABEL: Attenuating Gender Bias using Textual Entailment Data,0.183385,"Pre-trained language models encode undesirable social biases, which are
further exacerbated in downstream use. To this end, we propose MABEL (a Method
for Attenuating Gender Bias using Entailment Labels), an intermediate
pre-training approach for mitigating gender bias in contextualized
representations. Key to our approach is the use of a contrastive learning
objective on counterfactually augmented, gender-balanced entailment pairs from
natural language inference (NLI) datasets. We also introduce an alignment
regularizer that pulls identical entailment pairs along opposite gender
directions closer. We extensively evaluate our approach on intrinsic and
extrinsic metrics, and show that MABEL outperforms previous task-agnostic
debiasing approaches in terms of fairness. It also preserves task performance
after fine-tuning on downstream tasks. Together, these findings demonstrate the
suitability of NLI data as an effective means of bias mitigation, as opposed to
only using unlabeled sentences in the literature. Finally, we identify that
existing approaches often use evaluation settings that are insufficient or
inconsistent. We make an effort to reproduce and compare previous methods, and
call for unifying the evaluation settings across gender debiasing methods for
better future comparison.",https://github.com/princeton-nlp/MABEL,-1
Federated Learning with Heterogeneous Architectures using Graph HyperNetworks,0.192381,"Standard Federated Learning (FL) techniques are limited to clients with
identical network architectures. This restricts potential use-cases like
cross-platform training or inter-organizational collaboration when both data
privacy and architectural proprietary are required. We propose a new FL
framework that accommodates heterogeneous client architecture by adopting a
graph hypernetwork for parameter sharing. A property of the graph hyper network
is that it can adapt to various computational graphs, thereby allowing
meaningful parameter sharing across models. Unlike existing solutions, our
framework does not limit the clients to share the same architecture type, makes
no use of external data and does not require clients to disclose their model
architecture. Compared with distillation-based and non-graph hypernetwork
baselines, our method performs notably better on standard benchmarks. We
additionally show encouraging generalization performance to unseen
architectures.",None,-1
Predicting Future Occupancy Grids in Dynamic Environment with Spatio-Temporal Learning,0.0917861,"Reliably predicting future occupancy of highly dynamic urban environments is
an important precursor for safe autonomous navigation. Common challenges in the
prediction include forecasting the relative position of other vehicles,
modelling the dynamics of vehicles subjected to different traffic conditions,
and vanishing surrounding objects. To tackle these challenges, we propose a
spatio-temporal prediction network pipeline that takes the past information
from the environment and semantic labels separately for generating future
occupancy predictions. Compared to the current SOTA, our approach predicts
occupancy for a longer horizon of 3 seconds and in a relatively complex
environment from the nuScenes dataset. Our experimental results demonstrate the
ability of spatio-temporal networks to understand scene dynamics without the
need for HD-Maps and explicit modeling dynamic objects. We publicly release our
occupancy grid dataset based on nuScenes to support further research.",https://github.com/ksm26/SpatioTemporal-Predictions,-1
MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning,0.0541301,"Transformer language models (TLMs) are critical for most NLP tasks, but they
are difficult to create for low-resource languages because of how much
pretraining data they require. In this work, we investigate two techniques for
training monolingual TLMs in a low-resource setting: greatly reducing TLM size,
and complementing the masked language modeling objective with two
linguistically rich supervised tasks (part-of-speech tagging and dependency
parsing). Results from 7 diverse languages indicate that our model, MicroBERT,
is able to produce marked improvements in downstream task evaluations relative
to a typical monolingual TLM pretraining approach. Specifically, we find that
monolingual MicroBERT models achieve gains of up to 18% for parser LAS and 11%
for NER F1 compared to a multilingual baseline, mBERT, while having less than
1% of its parameter count. We conclude reducing TLM parameter count and using
labeled data for pretraining low-resource TLMs can yield large quality benefits
and in some cases produce models that outperform multilingual approaches.",https://github.com/lgessler/microbert,-1
Masked Event Modeling: Self-Supervised Pretraining for Event Cameras,0.140199,"Event cameras asynchronously capture brightness changes with low latency,
high temporal resolution, and high dynamic range. However, annotation of event
data is a costly and laborious process, which limits the use of deep learning
methods for classification and other semantic tasks with the event modality. To
reduce the dependency on labeled event data, we introduce Masked Event Modeling
(MEM), a self-supervised framework for events. Our method pretrains a neural
network on unlabeled events, which can originate from any event camera
recording. Subsequently, the pretrained model is finetuned on a downstream
task, leading to a consistent improvement of the task accuracy. For example,
our method reaches state-of-the-art classification accuracy across three
datasets, N-ImageNet, N-Cars, and N-Caltech101, increasing the top-1 accuracy
of previous work by significant margins. When tested on real-world event data,
MEM is even superior to supervised RGB-based pretraining. The models pretrained
with MEM are also label-efficient and generalize well to the dense task of
semantic image segmentation.",https://github.com/tum-vision/mem,-1
Unsupervised Face Morphing Attack Detection via Self-paced Anomaly Detection,0.366079,"The supervised-learning-based morphing attack detection (MAD) solutions
achieve outstanding success in dealing with attacks from known morphing
techniques and known data sources. However, given variations in the morphing
attacks, the performance of supervised MAD solutions drops significantly due to
the insufficient diversity and quantity of the existing MAD datasets. To
address this concern, we propose a completely unsupervised MAD solution via
self-paced anomaly detection (SPL-MAD) by leveraging the existing large-scale
face recognition (FR) datasets and the unsupervised nature of convolutional
autoencoders. Using general FR datasets that might contain unintentionally and
unlabeled manipulated samples to train an autoencoder can lead to a diverse
reconstruction behavior of attack and bona fide samples. We analyze this
behavior empirically to provide a solid theoretical ground for designing our
unsupervised MAD solution. This also results in proposing to integrate our
adapted modified self-paced learning paradigm to enhance the reconstruction
error separability between the bona fide and attack samples in a completely
unsupervised manner. Our experimental results on a diverse set of MAD
evaluation datasets show that the proposed unsupervised SPL-MAD solution
outperforms the overall performance of a wide range of supervised MAD solutions
and provides higher generalizability on unknown attacks.",https://github.com/meilfang/SPL-MAD,-1
"3MASSIV: Multilingual, Multimodal and Multi-Aspect dataset of Social Media Short Videos",0.0594032,"We present 3MASSIV, a multilingual, multimodal and multi-aspect,
expertly-annotated dataset of diverse short videos extracted from short-video
social media platform - Moj. 3MASSIV comprises of 50k short videos (20 seconds
average duration) and 100K unlabeled videos in 11 different languages and
captures popular short video trends like pranks, fails, romance, comedy
expressed via unique audio-visual formats like self-shot videos, reaction
videos, lip-synching, self-sung songs, etc. 3MASSIV presents an opportunity for
multimodal and multilingual semantic understanding on these unique videos by
annotating them for concepts, affective states, media types, and audio
language. We present a thorough analysis of 3MASSIV and highlight the variety
and unique aspects of our dataset compared to other contemporary popular
datasets with strong baselines. We also show how the social media content in
3MASSIV is dynamic and temporal in nature, which can be used for semantic
understanding tasks and cross-lingual analysis.",None,-1
Type-aware Embeddings for Multi-Hop Reasoning over Knowledge Graphs,0.659135,"Multi-hop reasoning over real-life knowledge graphs (KGs) is a highly
challenging problem as traditional subgraph matching methods are not capable to
deal with noise and missing information. To address this problem, it has been
recently introduced a promising approach based on jointly embedding logical
queries and KGs into a low-dimensional space to identify answer entities.
However, existing proposals ignore critical semantic knowledge inherently
available in KGs, such as type information. To leverage type information, we
propose a novel TypE-aware Message Passing (TEMP) model, which enhances the
entity and relation representations in queries, and simultaneously improves
generalization, deductive and inductive reasoning. Remarkably, TEMP is a
plug-and-play model that can be easily incorporated into existing
embedding-based models to improve their performance. Extensive experiments on
three real-world datasets demonstrate TEMP's effectiveness.",https://github.com/zhiweihu1103/QE-TEMP,-1
A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,0.173265,"We have recently witnessed a number of impressive results on hard
mathematical reasoning problems with language models. At the same time, the
robustness of these models has also been called into question; recent works
have shown that models can rely on shallow patterns in the problem description
when generating a solution. Building on the idea of behavioral testing, we
propose a novel framework, which pins down the causal effect of various factors
in the input, e.g., the surface form of the problem text, the operands, and
math operators on the output solution. By grounding the behavioral analysis in
a causal graph describing an intuitive reasoning process, we study the behavior
of language models in terms of robustness and sensitivity to direct
interventions in the input space. We apply our framework on a test bed of math
word problems. Our analysis shows that robustness does not appear to
continuously improve as a function of size, but the GPT-3 Davinci models (175B)
achieve a dramatic improvement in both robustness and sensitivity compared to
all other GPT variants.",https://github.com/alestolfo/causal-math,-1
Local Sliced-Wasserstein Feature Sets for Illumination-invariant Face Recognition,0.120969,"We present a new method for face recognition from digital images acquired
under varying illumination conditions. The method is based on mathematical
modeling of local gradient distributions using the Radon Cumulative
Distribution Transform (R-CDT). We demonstrate that lighting variations cause
certain types of deformations of local image gradient distributions which, when
expressed in R-CDT domain, can be modeled as a subspace. Face recognition is
then performed using a nearest subspace in R-CDT domain of local gradient
distributions. Experiment results demonstrate the proposed method outperforms
other alternatives in several face recognition tasks with challenging
illumination conditions. Python code implementing the proposed method is
available, which is integrated as a part of the software package PyTransKit.",https://github.com/rohdelab/drcdt face,-1
Dynamic Dialogue Policy for Continual Reinforcement Learning,0.0255129,"Continual learning is one of the key components of human learning and a
necessary requirement of artificial intelligence. As dialogue can potentially
span infinitely many topics and tasks, a task-oriented dialogue system must
have the capability to continually learn, dynamically adapting to new
challenges while preserving the knowledge it already acquired. Despite the
importance, continual reinforcement learning of the dialogue policy has
remained largely unaddressed. The lack of a framework with training protocols,
baseline models and suitable metrics, has so far hindered research in this
direction. In this work we fill precisely this gap, enabling research in
dialogue policy optimisation to go from static to dynamic learning. We provide
a continual learning algorithm, baseline architectures and metrics for
assessing continual learning models. Moreover, we propose the dynamic dialogue
policy transformer (DDPT), a novel dynamic architecture that can integrate new
knowledge seamlessly, is capable of handling large state spaces and obtains
significant zero-shot performance when being exposed to unseen domains, without
any growth in network parameter size.",None,-1
Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia,0.0272929,"While neural networks demonstrate a remarkable ability to model linguistic
content, capturing contextual information related to a speaker's conversational
role is an open area of research. In this work, we analyze the effect of
speaker role on language use through the game of Mafia, in which participants
are assigned either an honest or a deceptive role. In addition to building a
framework to collect a dataset of Mafia game records, we demonstrate that there
are differences in the language produced by players with different roles. We
confirm that classification models are able to rank deceptive players as more
suspicious than honest ones based only on their use of language. Furthermore,
we show that training models on two auxiliary tasks outperforms a standard
BERT-based text classification approach. We also present methods for using our
trained models to identify features that distinguish between player roles,
which could be used to assist players during the Mafia game.",http://github.com/dallinger/Dallinger,-1
Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages,0.208133,"Multilingual Pretrained Language Models (MPLMs) have shown their strong
multilinguality in recent empirical cross-lingual transfer studies. In this
paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)
pipeline to improve the zero-shot performance on low-resource languages (LRLs)
by augmenting the context with semantically similar sentences retrieved from a
high-resource language (HRL) as prompts. PARC improves the zero-shot
performance on three downstream tasks (binary sentiment classification, topic
categorization and natural language inference) with multilingual parallel test
sets across 10 LRLs covering 6 language families in both unlabeled settings
(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the
finetuning baseline by 3.7%. We find a significant positive correlation between
cross-lingual transfer performance on one side, and the similarity between the
high- and low-resource languages as well as the amount of low-resource
pretraining data on the other side. A robustness analysis suggests that PARC
has the potential to achieve even stronger performance with more powerful
MPLMs.",https://github.com/ercong21/parc,-1
UIT-ViCoV19QA: A Dataset for COVID-19 Community-based Question Answering on Vietnamese Language,0.0,"For the last two years, from 2020 to 2021, COVID-19 has broken disease
prevention measures in many countries, including Vietnam, and negatively
impacted various aspects of human life and the social community. Besides, the
misleading information in the community and fake news about the pandemic are
also serious situations. Therefore, we present the first Vietnamese
community-based question answering dataset for developing question answering
systems for COVID-19 called UIT-ViCoV19QA. The dataset comprises 4,500
question-answer pairs collected from trusted medical sources, with at least one
answer and at most four unique paraphrased answers per question. Along with the
dataset, we set up various deep learning models as baseline to assess the
quality of our dataset and initiate the benchmark results for further research
through commonly used metrics such as BLEU, METEOR, and ROUGE-L. We also
illustrate the positive effects of having multiple paraphrased answers
experimented on these models, especially on Transformer - a dominant
architecture in the field of study.",None,-1
Heterogeneous-Agent Mirror Learning: A Continuum of Solutions to Cooperative MARL,0.314076,"The necessity for cooperation among intelligent machines has popularised
cooperative multi-agent reinforcement learning (MARL) in the artificial
intelligence (AI) research community. However, many research endeavors have
been focused on developing practical MARL algorithms whose effectiveness has
been studied only empirically, thereby lacking theoretical guarantees. As
recent studies have revealed, MARL methods often achieve performance that is
unstable in terms of reward monotonicity or suboptimal at convergence. To
resolve these issues, in this paper, we introduce a novel framework named
Heterogeneous-Agent Mirror Learning (HAML) that provides a general template for
MARL algorithmic designs. We prove that algorithms derived from the HAML
template satisfy the desired properties of the monotonic improvement of the
joint reward and the convergence to Nash equilibrium. We verify the
practicality of HAML by proving that the current state-of-the-art cooperative
MARL algorithms, HATRPO and HAPPO, are in fact HAML instances. Next, as a
natural outcome of our theory, we propose HAML extensions of two well-known RL
algorithms, HAA2C (for A2C) and HADDPG (for DDPG), and demonstrate their
effectiveness against strong baselines on StarCraftII and Multi-Agent MuJoCo
tasks.",https://github.com/anonymouswater/HAML,-1
Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production,0.0823805,"Mixture of Experts (MoE) models with conditional execution of sparsely
activated layers have enabled training models with a much larger number of
parameters. As a result, these models have achieved significantly better
quality on various natural language processing tasks including machine
translation. However, it remains challenging to deploy such models in real-life
scenarios due to the large memory requirements and inefficient inference. In
this work, we introduce a highly efficient inference framework with several
optimization approaches to accelerate the computation of sparse models and cut
down the memory consumption significantly. While we achieve up to 26x speed-up
in terms of throughput, we also reduce the model size almost to one eighth of
the original 32-bit float model by quantizing expert weights into 4-bit
integers. As a result, we are able to deploy 136x larger models with 27% less
cost and significantly better quality compared to the existing solutions. This
enables a paradigm shift in deploying large scale multilingual MoE transformers
models replacing the traditional practice of distilling teacher models into
dozens of smaller models per language or task.",https://github.com/NVIDIA/FasterTransformer,-1
Hidden Agenda: a Social Deduction Game with Diverse Learned Equilibria,0.133866,"A key challenge in the study of multiagent cooperation is the need for
individual agents not only to cooperate effectively, but to decide with whom to
cooperate. This is particularly critical in situations when other agents have
hidden, possibly misaligned motivations and goals. Social deduction games offer
an avenue to study how individuals might learn to synthesize potentially
unreliable information about others, and elucidate their true motivations. In
this work, we present Hidden Agenda, a two-team social deduction game that
provides a 2D environment for studying learning agents in scenarios of unknown
team alignment. The environment admits a rich set of strategies for both teams.
Reinforcement learning agents trained in Hidden Agenda show that agents can
learn a variety of behaviors, including partnering and voting without need for
communication in natural language.",None,-1
ContraReg: Contrastive Learning of Multi-modality Unsupervised Deformable Image Registration,0.137185,"Establishing voxelwise semantic correspondence across distinct imaging
modalities is a foundational yet formidable computer vision task. Current
multi-modality registration techniques maximize hand-crafted inter-domain
similarity functions, are limited in modeling nonlinear intensity-relationships
and deformations, and may require significant re-engineering or underperform on
new tasks, datasets, and domain pairs. This work presents ContraReg, an
unsupervised contrastive representation learning approach to multi-modality
deformable registration. By projecting learned multi-scale local patch features
onto a jointly learned inter-domain embedding space, ContraReg obtains
representations useful for non-rigid multi-modality alignment. Experimentally,
ContraReg achieves accurate and robust results with smooth and invertible
deformations across a series of baselines and ablations on a neonatal T1-T2
brain MRI registration task with all methods validated over a wide range of
deformation regularization strengths.",None,-1
Motron: Multimodal Probabilistic Human Motion Forecasting,0.625971,"Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic
maximum-likelihood motions and corresponding confidence values for each mode.
Our model aims to be tightly integrated with the robotic
planning-control-interaction loop; outputting physically feasible human motions
and being computationally efficient. We demonstrate the performance of our
model on several challenging real-world motion forecasting datasets,
outperforming a wide array of generative/variational methods while providing
state-of-the-art single-output motions if required. Both using significantly
less computational power than state-of-the art algorithms.",https://github.com/TUM-AAS/motron-cvpr22,-1
Distillation-Resistant Watermarking for Model Protection in NLP,0.422039,"How can we protect the intellectual property of trained NLP models? Modern
NLP models are prone to stealing by querying and distilling from their publicly
exposed APIs. However, existing protection methods such as watermarking only
work for images but are not applicable to text. We propose
Distillation-Resistant Watermarking (DRW), a novel technique to protect NLP
models from being stolen via distillation. DRW protects a model by injecting
watermarks into the victim's prediction probability corresponding to a secret
key and is able to detect such a key by probing a suspect model. We prove that
a protected model still retains the original accuracy within a certain bound.
We evaluate DRW on a diverse set of NLP tasks including text classification,
part-of-speech tagging, and named entity recognition. Experiments show that DRW
protects the original model and detects stealing suspects at 100% mean average
precision for all four tasks while the prior method fails on two.",https://github.com/XuandongZhao/DRW,-1
Attribution-aware Weight Transfer: A Warm-Start Initialization for Class-Incremental Semantic Segmentation,0.375914,"In class-incremental semantic segmentation (CISS), deep learning
architectures suffer from the critical problems of catastrophic forgetting and
semantic background shift. Although recent works focused on these issues,
existing classifier initialization methods do not address the background shift
problem and assign the same initialization weights to both background and new
foreground class classifiers. We propose to address the background shift with a
novel classifier initialization method which employs gradient-based attribution
to identify the most relevant weights for new classes from the classifier's
weights for the previous background and transfers these weights to the new
classifier. This warm-start weight initialization provides a general solution
applicable to several CISS methods. Furthermore, it accelerates learning of new
classes while mitigating forgetting. Our experiments demonstrate significant
improvement in mIoU compared to the state-of-the-art CISS methods on the
Pascal-VOC 2012, ADE20K and Cityscapes datasets.",https://github.com/dfki-av/AWT-for-CISS,-1
Intelligent analysis of EEG signals to assess consumer decisions: A Study on Neuromarketing,0.30263,"Neuromarketing is an emerging field that combines neuroscience and marketing
to understand the factors that influence consumer decisions better. The study
proposes a method to understand consumers' positive and negative reactions to
advertisements (ads) and products by analysing electroencephalogram (EEG)
signals. These signals are recorded using a low-cost single electrode headset
from volunteers belonging to the ages 18-22. A detailed subject dependent (SD)
and subject independent (SI) analysis was performed employing machine learning
methods like Naive Bayes (NB), Support Vector Machine (SVM), k-nearest
neighbour and Decision Tree and the proposed deep learning (DL) model. SVM and
NB yielded an accuracy (Acc.) of 0.63 for the SD analysis. In SI analysis, SVM
performed better for the advertisement, product and gender-based analysis.
Furthermore, the performance of the DL model was on par with that of SVM,
especially, in product and ads-based analysis.",None,-1
Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning,0.133165,"In this paper, we elaborate upon recipes for building multilingual
representation models that are not only competitive with existing
state-of-the-art models but are also more parameter efficient, thereby
promoting better adoption in resource-constrained scenarios and practical
applications. We show that going beyond English-centric bitexts, coupled with a
novel sampling strategy aimed at reducing under-utilization of training data,
substantially boosts performance across model sizes for both Electra and MLM
pre-training objectives. We introduce XY-LENT: X-Y bitext enhanced Language
ENcodings using Transformers which not only achieves state-of-the-art
performance over 5 cross-lingual tasks within all model size bands, is also
competitive across bands. Our XY-LENT XL variant outperforms XLM-RXXL and
exhibits competitive performance with mT5 XXL while being 5x and 6x smaller
respectively. We then show that our proposed method helps ameliorate the curse
of multilinguality, with the XY-LENT XL achieving 99.3% GLUE performance and
98.5% SQuAD 2.0 performance compared to a SoTA English only model in the same
size band. We then analyze our models performance on extremely low resource
languages and posit that scaling alone may not be sufficient for improving the
performance in this scenario",None,-1
FFC-SE: Fast Fourier Convolution for Speech Enhancement,0.18841,"Fast Fourier convolution (FFC) is the recently proposed neural operator
showing promising performance in several computer vision problems. The FFC
operator allows employing large receptive field operations within early layers
of the neural network. It was shown to be especially helpful for inpainting of
periodic structures which are common in audio processing. In this work, we
design neural network architectures which adapt FFC for speech enhancement. We
hypothesize that a large receptive field allows these networks to produce more
coherent phases than vanilla convolutional models, and validate this hypothesis
experimentally. We found that neural networks based on Fast Fourier convolution
outperform analogous convolutional models and show better or comparable results
with other speech enhancement baselines.",None,-1
YOLO-FaceV2: A Scale and Occlusion Aware Face Detector,0.153585,"In recent years, face detection algorithms based on deep learning have made
great progress. These algorithms can be generally divided into two categories,
i.e. two-stage detector like Faster R-CNN and one-stage detector like YOLO.
Because of the better balance between accuracy and speed, one-stage detectors
have been widely used in many applications. In this paper, we propose a
real-time face detector based on the one-stage detector YOLOv5, named
YOLO-FaceV2. We design a Receptive Field Enhancement module called RFE to
enhance receptive field of small face, and use NWD Loss to make up for the
sensitivity of IoU to the location deviation of tiny objects. For face
occlusion, we present an attention module named SEAM and introduce Repulsion
Loss to solve it. Moreover, we use a weight function Slide to solve the
imbalance between easy and hard samples and use the information of the
effective receptive field to design the anchor. The experimental results on
WiderFace dataset show that our face detector outperforms YOLO and its variants
can be find in all easy, medium and hard subsets. Source code in
https://github.com/Krasjet-Yu/YOLO-FaceV2",https://github.com/Krasjet-Yu/YOLO-FaceV2,-1
Planning Assembly Sequence with Graph Transformer,0.233167,"Assembly sequence planning (ASP) is the essential process for modern
manufacturing, proven to be NP-complete thus its effective and efficient
solution has been a challenge for researchers in the field. In this paper, we
present a graph-transformer based framework for the ASP problem which is
trained and demonstrated on a self-collected ASP database. The ASP database
contains a self-collected set of LEGO models. The LEGO model is abstracted to a
heterogeneous graph structure after a thorough analysis of the original
structure and feature extraction. The ground truth assembly sequence is first
generated by brute-force search and then adjusted manually to in line with
human rational habits. Based on this self-collected ASP dataset, we propose a
heterogeneous graph-transformer framework to learn the latent rules for
assembly planning. We evaluated the proposed framework in a series of
experiment. The results show that the similarity of the predicted and ground
truth sequences can reach 0.44, a medium correlation measured by Kendall's
$\tau$. Meanwhile, we compared the different effects of node features and edge
features and generated a feasible and reasonable assembly sequence as a
benchmark for further research. Our data set and code is available on
https://github.com/AIR-DISCOVER/ICRA\_ASP.",https://github.com/AIR-DISCOVER/ICRA,-1
Keys to Better Image Inpainting: Structure and Texture Go Hand in Hand,0.131838,"Deep image inpainting has made impressive progress with recent advances in
image generation and processing algorithms. We claim that the performance of
inpainting algorithms can be better judged by the generated structures and
textures. Structures refer to the generated object boundary or novel geometric
structures within the hole, while texture refers to high-frequency details,
especially man-made repeating patterns filled inside the structural regions. We
believe that better structures are usually obtained from a coarse-to-fine
GAN-based generator network while repeating patterns nowadays can be better
modeled using state-of-the-art high-frequency fast fourier convolutional
layers. In this paper, we propose a novel inpainting network combining the
advantages of the two designs. Therefore, our model achieves a remarkable
visual quality to match state-of-the-art performance in both structure
generation and repeating texture synthesis using a single network. Extensive
experiments demonstrate the effectiveness of the method, and our conclusions
further highlight the two critical factors of image inpainting quality,
structures, and textures, as the future design directions of inpainting
networks.",https://github.com/SHI-Labs/FcF-Inpainting/,-1
Disentangling Visual Embeddings for Attributes and Objects,0.232578,"We study the problem of compositional zero-shot learning for object-attribute
recognition. Prior works use visual features extracted with a backbone network,
pre-trained for object classification and thus do not capture the subtly
distinct features associated with attributes. To overcome this challenge, these
studies employ supervision from the linguistic space, and use pre-trained word
embeddings to better separate and compose attribute-object pairs for
recognition. Analogous to linguistic embedding space, which already has unique
and agnostic embeddings for object and attribute, we shift the focus back to
the visual space and propose a novel architecture that can disentangle
attribute and object features in the visual space. We use visual decomposed
features to hallucinate embeddings that are representative for the seen and
novel compositions to better regularize the learning of our model. Extensive
experiments show that our method outperforms existing work with significant
margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created
based on VAW. The code, models, and dataset splits are publicly available at
https://github.com/nirat1606/OADis.",https://github.com/nirat1606/OADis,-1
Investigation of Data Augmentation Techniques for Disordered Speech Recognition,0.121116,"Disordered speech recognition is a highly challenging task. The underlying
neuro-motor conditions of people with speech disorders, often compounded with
co-occurring physical disabilities, lead to the difficulty in collecting large
quantities of speech required for system development. This paper investigates a
set of data augmentation techniques for disordered speech recognition,
including vocal tract length perturbation (VTLP), tempo perturbation and speed
perturbation. Both normal and disordered speech were exploited in the
augmentation process. Variability among impaired speakers in both the original
and augmented data was modeled using learning hidden unit contributions (LHUC)
based speaker adaptive training. The final speaker adapted system constructed
using the UASpeech corpus and the best augmentation approach based on speed
perturbation produced up to 2.92% absolute (9.3% relative) word error rate
(WER) reduction over the baseline system without data augmentation, and gave an
overall WER of 26.37% on the test set containing 16 dysarthric speakers.",None,-1
Effective and Efficient Training for Sequential Recommendation using Recency Sampling,0.0318045,"Many modern sequential recommender systems use deep neural networks, which
can effectively estimate the relevance of items but require a lot of time to
train. Slow training increases expenses, hinders product development timescales
and prevents the model from being regularly updated to adapt to changing user
preferences. Training such sequential models involves appropriately sampling
past user interactions to create a realistic training objective. The existing
training objectives have limitations. For instance, next item prediction never
uses the beginning of the sequence as a learning target, thereby potentially
discarding valuable data. On the other hand, the item masking used by BERT4Rec
is only weakly related to the goal of the sequential recommendation; therefore,
it requires much more time to obtain an effective model. Hence, we propose a
novel Recency-based Sampling of Sequences training objective that addresses
both limitations. We apply our method to various recent and state-of-the-art
model architectures - such as GRU4Rec, Caser, and SASRec. We show that the
models enhanced with our method can achieve performances exceeding or very
close to stateof-the-art BERT4Rec, but with much less training time.",None,-1
Changing the Representation: Examining Language Representation for Neural Sign Language Production,0.0225934,"Neural Sign Language Production (SLP) aims to automatically translate from
spoken language sentences to sign language videos. Historically the SLP task
has been broken into two steps; Firstly, translating from a spoken language
sentence to a gloss sequence and secondly, producing a sign language video
given a sequence of glosses. In this paper we apply Natural Language Processing
techniques to the first step of the SLP pipeline. We use language models such
as BERT and Word2Vec to create better sentence level embeddings, and apply
several tokenization techniques, demonstrating how these improve performance on
the low resource translation task of Text to Gloss. We introduce Text to
HamNoSys (T2H) translation, and show the advantages of using a phonetic
representation for sign language translation rather than a sign level gloss
representation. Furthermore, we use HamNoSys to extract the hand shape of a
sign and use this as additional supervision during training, further increasing
the performance on T2H. Assembling best practise, we achieve a BLEU-4 score of
26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art
baselines.",None,-1
Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information,0.158954,"We describe a simple and effective method (Spectral Attribute removaL; SAL)
to remove private or guarded information from neural representations. Our
method uses matrix decomposition to project the input representations into
directions with reduced covariance with the guarded information rather than
maximal covariance as factorization methods normally use. We begin with linear
information removal and proceed to generalize our algorithm to the case of
nonlinear information removal using kernels. Our experiments demonstrate that
our algorithm retains better main task performance after removing the guarded
information compared to previous work. In addition, our experiments demonstrate
that we need a relatively small amount of guarded attribute data to remove
information about these attributes, which lowers the exposure to sensitive data
and is more suitable for low-resource scenarios. Code is available at
https://github.com/jasonshaoshun/SAL.",https://github.com/jasonshaoshun/SAL,-1
Continuous diffusion for categorical data,0.987363,"Diffusion models have quickly become the go-to paradigm for generative
modelling of perceptual signals (such as images and sound) through iterative
refinement. Their success hinges on the fact that the underlying physical
phenomena are continuous. For inherently discrete and categorical data such as
language, various diffusion-inspired alternatives have been proposed. However,
the continuous nature of diffusion models conveys many benefits, and in this
work we endeavour to preserve it. We propose CDCD, a framework for modelling
categorical data with diffusion models that are continuous both in time and
input space. We demonstrate its efficacy on several language modelling tasks.",None,-1
Perceptual Quality Assessment for Digital Human Heads,0.0507855,"Digital humans are attracting more and more research interest during the last
decade, the generation, representation, rendering, and animation of which have
been put into large amounts of effort. However, the quality assessment of
digital humans has fallen behind. Therefore, to tackle the challenge of digital
human quality assessment issues, we propose the first large-scale quality
assessment database for three-dimensional (3D) scanned digital human heads
(DHHs). The constructed database consists of 55 reference DHHs and 1,540
distorted DHHs along with the subjective perceptual ratings. Then, a simple yet
effective full-reference (FR) projection-based method is proposed to evaluate
the visual quality of DHHs. The pretrained Swin Transformer tiny is employed
for hierarchical feature extraction and the multi-head attention module is
utilized for feature fusion. The experimental results reveal that the proposed
method exhibits state-of-the-art performance among the mainstream FR metrics.
The database is released at https://github.com/zzc-1998/DHHQA.",https://github.com/zzc-1998/DHHQA,-1
The Theory of Artificial Immutability: Protecting Algorithmic Groups Under Anti-Discrimination Law,0.0274627,"Artificial Intelligence (AI) is increasingly used to make important decisions
about people. While issues of AI bias and proxy discrimination are well
explored, less focus has been paid to the harms created by profiling based on
groups that do not map to or correlate with legally protected groups such as
sex or ethnicity. This raises a question: are existing equality laws able to
protect against emergent AI-driven inequality? This article examines the legal
status of algorithmic groups in North American and European non-discrimination
doctrine, law, and jurisprudence and will show that algorithmic groups are not
comparable to traditional protected groups. Nonetheless, these new groups are
worthy of protection. I propose a new theory of harm - ""the theory of
artificial immutability"" - that aims to bring AI groups within the scope of the
law. My theory describes how algorithmic groups act as de facto immutable
characteristics in practice that limit people's autonomy and prevent them from
achieving important goals.",None,-1
Identifying Ethical Issues in AI Partners in Human-AI Co-Creation,0.0452746,"Human-AI co-creativity involves humans and AI collaborating on a shared
creative product as partners. In many existing co-creative systems, users
communicate with the AI using buttons or sliders. However, typically, the AI in
co-creative systems cannot communicate back to humans, limiting their potential
to be perceived as partners. This paper starts with an overview of a
comparative study with 38 participants to explore the impact of AI-to-human
communication on user perception and engagement in co-creative systems and the
results show improved collaborative experience and user engagement with the
system incorporating AI-to-human communication. The results also demonstrate
that users perceive co-creative AI as more reliable, personal and intelligent
when it can communicate with the users. The results indicate a need to identify
potential ethical issues from an engaging communicating co-creative AI. Later
in the paper, we present some potential ethical issues in human-AI co-creation
and propose to use participatory design fiction as the research methodology to
investigate the ethical issues associated with a co-creative AI that
communicates with users.",None,-1
Leveraging Language for Accelerated Learning of Tool Manipulation,0.121862,"Robust and generalized tool manipulation requires an understanding of the
properties and affordances of different tools. We investigate whether
linguistic information about a tool (e.g., its geometry, common uses) can help
control policies adapt faster to new tools for a given task. We obtain diverse
descriptions of various tools in natural language and use pre-trained language
models to generate their feature representations. We then perform
language-conditioned meta-learning to learn policies that can efficiently adapt
to new tools given their corresponding text descriptions. Our results
demonstrate that combining linguistic information and meta-learning
significantly accelerates tool learning in several manipulation tasks including
pushing, lifting, sweeping, and hammering.",None,-1
Semantic optical fiber communication system,0.0844534,"The current optical communication systems minimize bit or symbol errors
without considering the semantic meaning behind digital bits, thus transmitting
a lot of unnecessary information. We propose and experimentally demonstrate a
semantic optical fiber communication (SOFC) system. Instead of encoding
information into bits for transmission, semantic information is extracted from
the source using deep learning. The generated semantic symbols are then
directly transmitted through an optical fiber. Compared with the bit-based
structure, the SOFC system achieved higher information compression and a more
stable performance, especially in the low received optical power regime, and
enhanced the robustness against optical link impairments. This work introduces
an intelligent optical communication system at the human analytical thinking
level, which is a significant step toward a breakthrough in the current optical
communication architecture.",None,-1
Semi-supervised New Event Type Induction and Description via Contrastive Loss-Enforced Batch Attention,0.0496441,"Most event extraction methods have traditionally relied on an annotated set
of event types. However, creating event ontologies and annotating supervised
training data are expensive and time-consuming. Previous work has proposed
semi-supervised approaches which leverage seen (annotated) types to learn how
to automatically discover new event types. State-of-the-art methods, both
semi-supervised or fully unsupervised, use a form of reconstruction loss on
specific tokens in a context. In contrast, we present a novel approach to
semi-supervised new event type induction using a masked contrastive loss, which
learns similarities between event mentions by enforcing an attention mechanism
over the data minibatch. We further disentangle the discovered clusters by
approximating the underlying manifolds in the data, which allows us to increase
normalized mutual information and Fowlkes-Mallows scores by over 20% absolute.
Building on these clustering results, we extend our approach to two new tasks:
predicting the type name of the discovered clusters and linking them to
FrameNet frames.",None,-1
Robust Double-Encoder Network for RGB-D Panoptic Segmentation,0.0172717,"Perception is crucial for robots that act in real-world environments, as
autonomous systems need to see and understand the world around them to act
properly. Panoptic segmentation provides an interpretation of the scene by
computing a pixelwise semantic label together with instance IDs. In this paper,
we address panoptic segmentation using RGB-D data of indoor scenes. We propose
a novel encoder-decoder neural network that processes RGB and depth separately
through two encoders. The features of the individual encoders are progressively
merged at different resolutions, such that the RGB features are enhanced using
complementary depth information. We propose a novel merging approach called
ResidualExcite, which reweighs each entry of the feature map according to its
importance. With our double-encoder architecture, we are robust to missing
cues. In particular, the same model can train and infer on RGB-D, RGB-only, and
depth-only input data, without the need to train specialized models. We
evaluate our method on publicly available datasets and show that our approach
achieves superior results compared to other common approaches for panoptic
segmentation.",https://github.com/PRBonn/PS-res-excite,-1
Learning to Walk by Steering: Perceptive Quadrupedal Locomotion in Dynamic Environments,0.142304,"We tackle the problem of perceptive locomotion in dynamic environments. In
this problem, a quadrupedal robot must exhibit robust and agile walking
behaviors in response to environmental clutter and moving obstacles. We present
a hierarchical learning framework, named PRELUDE, which decomposes the problem
of perceptive locomotion into high-level decision-making to predict navigation
commands and low-level gait generation to realize the target commands. In this
framework, we train the high-level navigation controller with imitation
learning on human demonstrations collected on a steerable cart and the
low-level gait controller with reinforcement learning (RL). Therefore, our
method can acquire complex navigation behaviors from human supervision and
discover versatile gaits from trial and error. We demonstrate the effectiveness
of our approach in simulation and with hardware experiments. Videos and code
can be found at the project page: https://ut-austin-rpl.github.io/PRELUDE.",https://ut-austin-rpl.github.io/PRELUDE,-1
AI-based Malware and Ransomware Detection Models,0.183894,"Cybercrime is one of the major digital threats of this century. In
particular, ransomware attacks have significantly increased, resulting in
global damage costs of tens of billion dollars. In this paper, we train and
test different Machine Learning and Deep Learning models for malware detection,
malware classification and ransomware detection. We introduce a novel and
flexible solution that combines two optimized models for malware and ransomware
detection. Our results demonstrate some improvements both in terms of detection
performances and flexibility. In particular, our combined models pave the way
for easier future enhancements using specialized and thus interchangeable
detection modules.",None,-1
Neural Enhanced Belief Propagation for Data Association in Multiobject Tracking,0.120501,"Situation-aware technologies enabled by multiobject tracking (MOT) methods
will create new services and applications in fields such as autonomous
navigation and applied ocean sciences. Belief propagation (BP) is a
state-of-the-art method for Bayesian MOT but fully relies on a statistical
model and preprocessed sensor measurements. In this paper, we establish a
hybrid method for model-based and data-driven MOT. The proposed neural enhanced
belief propagation (NEBP) approach complements BP by information learned from
raw sensor data with the goal to improve data association and to reject false
alarm measurements. We evaluate the performance of our NEBP approach for MOT on
the nuScenes autonomous driving dataset and demonstrate that it can outperform
state-of-the-art reference methods.",None,-1
Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models,0.0705356,"End-to-end spoken language understanding (SLU) systems are gaining popularity
over cascaded approaches due to their simplicity and ability to avoid error
propagation. However, these systems model sequence labeling as a sequence
prediction task causing a divergence from its well-established token-level
tagging formulation. We build compositional end-to-end SLU systems that
explicitly separate the added complexity of recognizing spoken mentions in SLU
from the NLU task of sequence labeling. By relying on intermediate decoders
trained for ASR, our end-to-end systems transform the input modality from
speech to token-level representations that can be used in the traditional
sequence labeling framework. This composition of ASR and NLU formulations in
our end-to-end SLU system offers direct compatibility with pre-trained ASR and
NLU systems, allows performance monitoring of individual components and enables
the use of globally normalized losses like CRF, making them attractive in
practical scenarios. Our models outperform both cascaded and direct end-to-end
models on a labeling task of named entity recognition across SLU benchmarks.",https://github.com/espnet/espnet,-1
Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations,0.18496,"Recently, weakly-supervised image segmentation using weak annotations like
scribbles has gained great attention, since such annotations are much easier to
obtain compared to time-consuming and label-intensive labeling at the
pixel/voxel level. However, because scribbles lack structure information of
region of interest (ROI), existing scribble-based methods suffer from poor
boundary localization. Furthermore, most current methods are designed for 2D
image segmentation, which do not fully leverage the volumetric information if
directly applied to image slices. In this paper, we propose a scribble-based
volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image
segmentation and improves boundary prediction. To achieve this, we augment a
2.5D attention UNet with a proposed label propagation module to extend semantic
information from scribbles and a combination of static and active boundary
prediction to learn ROI's boundary and regularize its shape. Extensive
experiments on three public datasets demonstrate Scribble2D5 significantly
outperforms current scribble-based methods and approaches the performance of
fully-supervised ones. Our code is available online.",https://github.com/Qybc/Scribble2D5,-1
Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation,0.0604272,"Most dominant neural machine translation (NMT) models are restricted to make
predictions only according to the local context of preceding words in a
left-to-right manner. Although many previous studies try to incorporate global
information into NMT models, there still exist limitations on how to
effectively exploit bidirectional global context. In this paper, we propose a
Confidence Based Bidirectional Global Context Aware (CBBGCA) training framework
for NMT, where the NMT model is jointly trained with an auxiliary conditional
masked language model (CMLM). The training consists of two stages: (1)
multi-task joint training; (2) confidence based knowledge distillation. At the
first stage, by sharing encoder parameters, the NMT model is additionally
supervised by the signal from the CMLM decoder that contains bidirectional
global contexts. Moreover, at the second stage, using the CMLM as teacher, we
further pertinently incorporate bidirectional global context to the NMT model
on its unconfidently-predicted target words via knowledge distillation.
Experimental results show that our proposed CBBGCA training framework
significantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on
three large-scale translation datasets, namely WMT'14 English-to-German, WMT'19
Chinese-to-English and WMT'14 English-to-French, respectively.",None,-1
Bounding Counterfactuals under Selection Bias,0.0304364,"Causal analysis may be affected by selection bias, which is defined as the
systematic exclusion of data from a certain subpopulation. Previous work in
this area focused on the derivation of identifiability conditions. We propose
instead a first algorithm to address both identifiable and unidentifiable
queries. We prove that, in spite of the missingness induced by the selection
bias, the likelihood of the available data is unimodal. This enables us to use
the causal expectation-maximisation scheme to obtain the values of causal
queries in the identifiable case, and to compute bounds otherwise. Experiments
demonstrate the approach to be practically viable. Theoretical convergence
characterisations are provided.",https://github.com/IDSIA-papers/2022-PGM-selection,-1
Biological Robots: Perspectives on an Emerging Interdisciplinary Field,0.0708454,"Advances in science and engineering often reveal the limitations of classical
approaches initially used to understand, predict, and control phenomena. With
progress, conceptual categories must often be re-evaluated to better track
recently discovered invariants across disciplines. It is essential to refine
frameworks and resolve conflicting boundaries between disciplines such that
they better facilitate, not restrict, experimental approaches and capabilities.
In this essay, we discuss issues at the intersection of developmental biology,
computer science, and robotics. In the context of biological robots, we explore
changes across concepts and previously distinct fields that are driven by
recent advances in materials, information, and life sciences. Herein, each
author provides their own perspective on the subject, framed by their own
disciplinary training. We argue that as with computation, certain aspects of
developmental biology and robotics are not tied to specific materials; rather,
the consilience of these fields can help to shed light on issues of multi-scale
control, self-assembly, and relationships between form and function. We hope
new fields can emerge as boundaries arising from technological limitations are
overcome, furthering practical applications from regenerative medicine to
useful synthetic living machines.",None,-1
DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata,0.111865,"Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic
videos. However, they require a slow iterative optimization process to
synthesize a single fixed-size short video, and they do not offer any
post-training control over the synthesis process. We propose Dynamic Neural
Cellular Automata (DyNCA), a framework for real-time and controllable dynamic
texture synthesis. Our method is built upon the recently introduced NCA models
and can synthesize infinitely long and arbitrary-sized realistic video textures
in real time. We quantitatively and qualitatively evaluate our model and show
that our synthesized videos appear more realistic than the existing results. We
improve the SOTA DyTS performance by $2\sim 4$ orders of magnitude. Moreover,
our model offers several real-time video controls including motion speed,
motion direction, and an editing brush tool. We exhibit our trained models in
an online interactive demo that runs on local hardware and is accessible on
personal computers and smartphones.",None,-1
Reconstructed Student-Teacher and Discriminative Networks for Anomaly Detection,0.0633162,"Anomaly detection is an important problem in computer vision; however, the
scarcity of anomalous samples makes this task difficult. Thus, recent anomaly
detection methods have used only normal images with no abnormal areas for
training. In this work, a powerful anomaly detection method is proposed based
on student-teacher feature pyramid matching (STPM), which consists of a student
and teacher network. Generative models are another approach to anomaly
detection. They reconstruct normal images from an input and compute the
difference between the predicted normal and the input. Unfortunately, STPM does
not have the ability to generate normal images. To improve the accuracy of
STPM, this work uses a student network, as in generative models, to reconstruct
normal features. This improves the accuracy; however, the anomaly maps for
normal images are not clean because STPM does not use anomaly images for
training, which decreases the accuracy of the image-level anomaly detection. To
further improve accuracy, a discriminative network trained with
pseudo-anomalies from anomaly maps is used in our method, which consists of two
pairs of student-teacher networks and a discriminative network. The method
displayed high accuracy on the MVTec anomaly detection dataset.",None,-1
Image Classification with Small Datasets: Overview and Benchmark,0.01992,"Image classification with small datasets has been an active research area in
the recent past. However, as research in this scope is still in its infancy,
two key ingredients are missing for ensuring reliable and truthful progress: a
systematic and extensive overview of the state of the art, and a common
benchmark to allow for objective comparisons between published methods. This
article addresses both issues. First, we systematically organize and connect
past studies to consolidate a community that is currently fragmented and
scattered. Second, we propose a common benchmark that allows for an objective
comparison of approaches. It consists of five datasets spanning various domains
(e.g., natural images, medical imagery, satellite data) and data types (RGB,
grayscale, multispectral). We use this benchmark to re-evaluate the standard
cross-entropy baseline and ten existing methods published between 2017 and 2021
at renowned venues. Surprisingly, we find that thorough hyper-parameter tuning
on held-out validation data results in a highly competitive baseline and
highlights a stunted growth of performance over the years. Indeed, only a
single specialized method dating back to 2019 clearly wins our benchmark and
outperforms the baseline classifier.",https://github.com/lorenzobrigato/gem,-1
Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors,0.0491637,"Multi-scale features have been proven highly effective for object detection
but often come with huge and even prohibitive extra computation costs,
especially for the recent Transformer-based detectors. In this paper, we
propose Iterative Multi-scale Feature Aggregation (IMFA) -- a generic paradigm
that enables efficient use of multi-scale features in Transformer-based object
detectors. The core idea is to exploit sparse multi-scale features from just a
few crucial locations, and it is achieved with two novel designs. First, IMFA
rearranges the Transformer encoder-decoder pipeline so that the encoded
features can be iteratively updated based on the detection predictions. Second,
IMFA sparsely samples scale-adaptive features for refined detection from just a
few keypoint locations under the guidance of prior detection predictions. As a
result, the sampled multi-scale features are sparse yet still highly beneficial
for object detection. Extensive experiments show that the proposed IMFA boosts
the performance of multiple Transformer-based object detectors significantly
yet with only slight computational overhead.",https://github.com/ZhangGongjie/IMFA,-1
Localized Vision-Language Matching for Open-vocabulary Object Detection,0.139618,"In this work, we propose an open-vocabulary object detection method that,
based on image-caption pairs, learns to detect novel object classes along with
a given set of known classes. It is a two-stage training approach that first
uses a location-guided image-caption matching technique to learn class labels
for both novel and known classes in a weakly-supervised manner and second
specializes the model for the object detection task using known class
annotations. We show that a simple language model fits better than a large
contextualized language model for detecting novel objects. Moreover, we
introduce a consistency-regularization technique to better exploit
image-caption pair information. Our method compares favorably to existing
open-vocabulary detection approaches while being data-efficient. Source code is
available at https://github.com/lmb-freiburg/locov .",https://github.com/lmb-freiburg/locov,-1
Robust Anytime Learning of Markov Decision Processes,0.0255625,"Markov decision processes (MDPs) are formal models commonly used in
sequential decision-making. MDPs capture the stochasticity that may arise, for
instance, from imprecise actuators via probabilities in the transition
function. However, in data-driven applications, deriving precise probabilities
from (limited) data introduces statistical errors that may lead to unexpected
or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise
probabilities but instead use so-called uncertainty sets in the transitions,
accounting for such limited data. Tools from the formal verification community
efficiently compute robust policies that provably adhere to formal
specifications, like safety constraints, under the worst-case instance in the
uncertainty set. We continuously learn the transition probabilities of an MDP
in a robust anytime-learning approach that combines a dedicated Bayesian
inference scheme with the computation of robust policies. In particular, our
method (1) approximates probabilities as intervals, (2) adapts to new data that
may be inconsistent with an intermediate model, and (3) may be stopped at any
time to compute a robust policy on the uMDP that faithfully captures the data
so far. Furthermore, our method is capable of adapting to changes in the
environment. We show the effectiveness of our approach and compare it to robust
policies computed on uMDPs learned by the UCRL2 reinforcement learning
algorithm in an experimental evaluation on several benchmarks.",https://github.com/LAVA-LAB/luiaard,-1
Formal Algorithms for Transformers,0.28929,"This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs.",None,-1
Concept Graph Neural Networks for Surgical Video Understanding,0.0,"We constantly integrate our knowledge and understanding of the world to
enhance our interpretation of what we see.
  This ability is crucial in application domains which entail reasoning about
multiple entities and concepts, such as AI-augmented surgery. In this paper, we
propose a novel way of integrating conceptual knowledge into temporal analysis
tasks via temporal concept graph networks. In the proposed networks, a global
knowledge graph is incorporated into the temporal analysis of surgical
instances, learning the meaning of concepts and relations as they apply to the
data. We demonstrate our results in surgical video data for tasks such as
verification of critical view of safety, as well as estimation of Parkland
grading scale. The results show that our method improves the recognition and
detection of complex benchmarks as well as enables other analytic applications
of interest.",https://github.com/CAMMA-public/ivtmetrics,-1
Understanding Interpersonal Conflict Types and their Impact on Perception Classification,0.034471,"Studies on interpersonal conflict have a long history and contain many
suggestions for conflict typology. We use this as the basis of a novel
annotation scheme and release a new dataset of situations and conflict aspect
annotations. We then build a classifier to predict whether someone will
perceive the actions of one individual as right or wrong in a given situation.
Our analyses include conflict aspects, but also generated clusters, which are
human validated, and show differences in conflict content based on the
relationship of participants to the author. Our findings have important
implications for understanding conflict and social norms.",https://github.com/caisa-lab/interpersonal-conflict-types,-1
Super forecasting the technological singularity risks from artificial intelligence,0.474144,"The article forecasts emerging cyber-risks from the integration of AI in
cybersecurity.",None,-1
Learning-based Motion Planning in Dynamic Environments Using GNNs and Temporal Encoding,0.0364763,"Learning-based methods have shown promising performance for accelerating
motion planning, but mostly in the setting of static environments. For the more
challenging problem of planning in dynamic environments, such as multi-arm
assembly tasks and human-robot interaction, motion planners need to consider
the trajectories of the dynamic obstacles and reason about temporal-spatial
interactions in very large state spaces. We propose a GNN-based approach that
uses temporal encoding and imitation learning with data aggregation for
learning both the embeddings and the edge prioritization policies. Experiments
show that the proposed methods can significantly accelerate online planning
over state-of-the-art complete dynamic planning algorithms. The learned models
can often reduce costly collision checking operations by more than 1000x, and
thus accelerating planning by up to 95%, while achieving high success rates on
hard instances as well.",None,-1
Introducing BEREL: BERT Embeddings for Rabbinic-Encoded Language,0.0913355,"We present a new pre-trained language model (PLM) for Rabbinic Hebrew, termed
Berel (BERT Embeddings for Rabbinic-Encoded Language). Whilst other PLMs exist
for processing Hebrew texts (e.g., HeBERT, AlephBert), they are all trained on
modern Hebrew texts, which diverges substantially from Rabbinic Hebrew in terms
of its lexicographical, morphological, syntactic and orthographic norms. We
demonstrate the superiority of Berel on Rabbinic texts via a challenge set of
Hebrew homographs. We release the new model and homograph challenge set for
unrestricted use.",None,-1
Enhancing Task Bot Engagement with Synthesized Open-Domain Dialog,0.0420489,"Many efforts have been made to construct dialog systems for different types
of conversations, such as task-oriented dialog (TOD) and open-domain dialog
(ODD). To better mimic human-level conversations that usually fuse various
dialog modes, it is essential to build a system that can effectively handle
both TOD and ODD and access different knowledge sources. To address the lack of
available data for the fused task, we propose a framework for automatically
generating dialogues that combine knowledge-grounded ODDs and TODs in various
settings. Additionally, we introduce a unified model PivotBot that is capable
of appropriately adopting TOD and ODD modes and accessing different knowledge
sources in order to effectively tackle the fused task. Evaluation results
demonstrate the superior ability of the proposed model to switch seamlessly
between TOD and ODD tasks.",None,-1
KnowGL: Knowledge Generation and Linking from Text,0.0349317,"We propose KnowGL, a tool that allows converting text into structured
relational data represented as a set of ABox assertions compliant with the TBox
of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a
sequence generation task by leveraging pre-trained sequence-to-sequence
language models, e.g. BART. Given a sentence, we fine-tune such models to
detect pairs of entity mentions and jointly generate a set of facts consisting
of the full set of semantic annotations for a KG, such as entity labels, entity
types, and their relationships. To showcase the capabilities of our tool, we
build a web application consisting of a set of UI widgets that help users to
navigate through the semantic data extracted from a given input text. We make
the KnowGL model available at https://huggingface.co/ibm/knowgl-large.",None,-1
"A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes",0.244512,"While datasets with single-label supervision have propelled rapid advances in
image classification, additional annotations are necessary in order to
quantitatively assess how models make predictions. To this end, for a subset of
ImageNet samples, we collect segmentation masks for the entire object and $18$
informative attributes. We call this dataset RIVAL10 (RIch Visual Attributes
with Localization), consisting of roughly $26k$ instances over $10$ classes.
Using RIVAL10, we evaluate the sensitivity of a broad set of models to noise
corruptions in foregrounds, backgrounds and attributes. In our analysis, we
consider diverse state-of-the-art architectures (ResNets, Transformers) and
training procedures (CLIP, SimCLR, DeiT, Adversarial Training). We find that,
somewhat surprisingly, in ResNets, adversarial training makes models more
sensitive to the background compared to foreground than standard training.
Similarly, contrastively-trained models also have lower relative foreground
sensitivity in both transformers and ResNets. Lastly, we observe intriguing
adaptive abilities of transformers to increase relative foreground sensitivity
as corruption level increases. Using saliency methods, we automatically
discover spurious features that drive the background sensitivity of models and
assess alignment of saliency maps with foregrounds. Finally, we quantitatively
study the attribution problem for neural features by comparing feature saliency
with ground-truth localization of semantic attributes.",None,-1
TENET: Transformer Encoding Network for Effective Temporal Flow on Motion Prediction,0.0,"This technical report presents an effective method for motion prediction in
autonomous driving. We develop a Transformer-based method for input encoding
and trajectory prediction. Besides, we propose the Temporal Flow Header to
enhance the trajectory encoding. In the end, an efficient K-means ensemble
method is used. Using our Transformer network and ensemble method, we win the
first place of Argoverse 2 Motion Forecasting Challenge with the
state-of-the-art brier-minFDE score of 1.90.",None,-1
A Robust Document Image Watermarking Scheme using Deep Neural Network,0.0,"Watermarking is an important copyright protection technology which generally
embeds the identity information into the carrier imperceptibly. Then the
identity can be extracted to prove the copyright from the watermarked carrier
even after suffering various attacks. Most of the existing watermarking
technologies take the nature images as carriers. Different from the natural
images, document images are not so rich in color and texture, and thus have
less redundant information to carry watermarks. This paper proposes an
end-to-end document image watermarking scheme using the deep neural network.
Specifically, an encoder and a decoder are designed to embed and extract the
watermark. A noise layer is added to simulate the various attacks that could be
encountered in reality, such as the Cropout, Dropout, Gaussian blur, Gaussian
noise, Resize, and JPEG Compression. A text-sensitive loss function is designed
to limit the embedding modification on characters. An embedding strength
adjustment strategy is proposed to improve the quality of watermarked image
with little loss of extraction accuracy. Experimental results show that the
proposed document image watermarking technology outperforms three
state-of-the-arts in terms of the robustness and image quality.",https://github.com/gslxr/Document-image-watermarking,-1
VPAIR -- Aerial Visual Place Recognition and Localization in Large-scale Outdoor Environments,0.900509,"Visual Place Recognition and Visual Localization are essential components in
navigation and mapping for autonomous vehicles especially in GNSS-denied
navigation scenarios. Recent work has focused on ground or close to ground
applications such as self-driving cars or indoor-scenarios and low-altitude
drone flights. However, applications such as Urban Air Mobility require
operations in large-scale outdoor environments at medium to high altitudes. We
present a new dataset named VPAIR. The dataset was recorded on board a light
aircraft flying at an altitude of more than 300 meters above ground capturing
images with a downwardfacing camera. Each image is paired with a high
resolution reference render including dense depth information and 6-DoF
reference poses. The dataset covers a more than one hundred kilometers long
trajectory over various types of challenging landscapes, e.g. urban, farmland
and forests. Experiments on this dataset illustrate the challenges introduced
by the change in perspective to a bird's eye view such as in-plane rotations.",https://github.com/AerVisLoc/vpair,-1
Densely Constrained Depth Estimator for Monocular 3D Object Detection,0.0885086,"Estimating accurate 3D locations of objects from monocular images is a
challenging problem because of lacking depth. Previous work shows that
utilizing the object's keypoint projection constraints to estimate multiple
depth candidates boosts the detection performance. However, the existing
methods can only utilize vertical edges as projection constraints for depth
estimation. So these methods only use a small number of projection constraints
and produce insufficient depth candidates, leading to inaccurate depth
estimation. In this paper, we propose a method that utilizes dense projection
constraints from edges of any direction. In this way, we employ much more
projection constraints and produce considerable depth candidates. Besides, we
present a graph matching weighting module to merge the depth candidates. The
proposed method DCD (Densely Constrained Detector) achieves state-of-the-art
performance on the KITTI and WOD benchmarks. Code is released at
https://github.com/BraveGroup/DCD.",https://github.com/BraveGroup/DCD,-1
Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences,0.0417663,"Generating complex behaviors that satisfy the preferences of non-expert users
is a crucial requirement for AI agents. Interactive reward learning from
trajectory comparisons (a.k.a. RLHF) is one way to allow non-expert users to
convey complex objectives by expressing preferences over short clips of agent
behaviors. Even though this parametric method can encode complex tacit
knowledge present in the underlying tasks, it implicitly assumes that the human
is unable to provide richer feedback than binary preference labels, leading to
intolerably high feedback complexity and poor user experience. While providing
a detailed symbolic closed-form specification of the objectives might be
tempting, it is not always feasible even for an expert user. However, in most
cases, humans are aware of how the agent should change its behavior along
meaningful axes to fulfill their underlying purpose, even if they are not able
to fully specify task objectives symbolically. Using this as motivation, we
introduce the notion of Relative Behavioral Attributes, which allows the users
to tweak the agent behavior through symbolic concepts (e.g., increasing the
softness or speed of agents' movement). We propose two practical methods that
can learn to model any kind of behavioral attributes from ordered behavior
clips. We demonstrate the effectiveness of our methods on four tasks with nine
different behavioral attributes, showing that once the attributes are learned,
end users can produce desirable agent behaviors relatively effortlessly, by
providing feedback just around ten times. This is over an order of magnitude
less than that required by the popular learning-from-human-preferences
baselines. The supplementary video and source code are available at:
https://guansuns.github.io/pages/rba.",None,-1
Code-DKT: A Code-based Knowledge Tracing Model for Programming Tasks,0.0170901,"Knowledge tracing (KT) models are a popular approach for predicting students'
future performance at practice problems using their prior attempts. Though many
innovations have been made in KT, most models including the state-of-the-art
Deep KT (DKT) mainly leverage each student's response either as correct or
incorrect, ignoring its content. In this work, we propose Code-based Deep
Knowledge Tracing (Code-DKT), a model that uses an attention mechanism to
automatically extract and select domain-specific code features to extend DKT.
We compared the effectiveness of Code-DKT against Bayesian and Deep Knowledge
Tracing (BKT and DKT) on a dataset from a class of 50 students attempting to
solve 5 introductory programming assignments. Our results show that Code-DKT
consistently outperforms DKT by 3.07-4.00% AUC across the 5 assignments, a
comparable improvement to other state-of-the-art domain-general KT models over
DKT. Finally, we analyze problem-specific performance through a set of case
studies for one assignment to demonstrate when and how code features improve
Code-DKT's predictions.",https://github.com/YangAzure/Code-DKT,-1
Boosting 3D Adversarial Attacks with Attacking On Frequency,0.00391449,"Deep neural networks (DNNs) have been shown to be vulnerable to adversarial
attacks. Recently, 3D adversarial attacks, especially adversarial attacks on
point clouds, have elicited mounting interest. However, adversarial point
clouds obtained by previous methods show weak transferability and are easy to
defend. To address these problems, in this paper we propose a novel point cloud
attack (dubbed AOF) that pays more attention on the low-frequency component of
point clouds. We combine the losses from point cloud and its low-frequency
component to craft adversarial samples. Extensive experiments validate that AOF
can improve the transferability significantly compared to state-of-the-art
(SOTA) attacks, and is more robust to SOTA 3D defense methods. Otherwise,
compared to clean point clouds, adversarial point clouds obtained by AOF
contain more deformation than outlier.",None,-1
Multi-Agent Chance-Constrained Stochastic Shortest Path with Application to Risk-Aware Intelligent Intersection,0.205748,"In transportation networks, where traffic lights have traditionally been used
for vehicle coordination, intersections act as natural bottlenecks. A
formidable challenge for existing automated intersections lies in detecting and
reasoning about uncertainty from the operating environment and human-driven
vehicles. In this paper, we propose a risk-aware intelligent intersection
system for autonomous vehicles (AVs) as well as human-driven vehicles (HVs). We
cast the problem as a novel class of Multi-agent Chance-Constrained Stochastic
Shortest Path (MCC-SSP) problems and devise an exact Integer Linear Programming
(ILP) formulation that is scalable in the number of agents' interaction points
(e.g., potential collision points at the intersection). In particular, when the
number of agents within an interaction point is small, which is often the case
in intersections, the ILP has a polynomial number of variables and constraints.
To further improve the running time performance, we show that the collision
risk computation can be performed offline. Additionally, a trajectory
optimization workflow is provided to generate risk-aware trajectories for any
given intersection. The proposed framework is implemented in CARLA simulator
and evaluated under a fully autonomous intersection with AVs only as well as in
a hybrid setup with a signalized intersection for HVs and an intelligent scheme
for AVs. As verified via simulations, the featured approach improves
intersection's efficiency by up to $200\%$ while also conforming to the
specified tunable risk threshold.",None,-1
Items from Psychometric Tests as Training Data for Personality Profiling Models of Twitter Users,0.111883,"Machine-learned models for author profiling in social media often rely on
data acquired via self-reporting-based psychometric tests (questionnaires)
filled out by social media users. This is an expensive but accurate data
collection strategy. Another, less costly alternative, which leads to
potentially more noisy and biased data, is to rely on labels inferred from
publicly available information in the profiles of the users, for instance
self-reported diagnoses or test results. In this paper, we explore a third
strategy, namely to directly use a corpus of items from validated psychometric
tests as training data. Items from psychometric tests often consist of
sentences from an I-perspective (e.g., ""I make friends easily.""). Such corpora
of test items constitute 'small data', but their availability for many concepts
is a rich resource. We investigate this approach for personality profiling, and
evaluate BERT classifiers fine-tuned on such psychometric test items for the
big five personality traits (openness, conscientiousness, extraversion,
agreeableness, neuroticism) and analyze various augmentation strategies
regarding their potential to address the challenges coming with such a small
corpus. Our evaluation on a publicly available Twitter corpus shows a
comparable performance to in-domain training for 4/5 personality traits with
T5-based data augmentation.",None,-1
An Intelligent Self-driving Truck System For Highway Transportation,0.0822899,"Recently, there have been many advances in autonomous driving society,
attracting a lot of attention from academia and industry. However, existing
works mainly focus on cars, extra development is still required for
self-driving truck algorithms and models. In this paper, we introduce an
intelligent self-driving truck system. Our presented system consists of three
main components, 1) a realistic traffic simulation module for generating
realistic traffic flow in testing scenarios, 2) a high-fidelity truck model
which is designed and evaluated for mimicking real truck response in real-world
deployment, 3) an intelligent planning module with learning-based decision
making algorithm and multi-mode trajectory planner, taking into account the
truck's constraints, road slope changes, and the surrounding traffic flow. We
provide quantitative evaluations for each component individually to demonstrate
the fidelity and performance of each part. We also deploy our proposed system
on a real truck and conduct real world experiments which shows our system's
capacity of mitigating sim-to-real gap. Our code is available at
https://github.com/InceptioResearch/IITS",https://github.com/InceptioResearch/IITS,-1
On the Effectiveness of Compact Biomedical Transformers,0.368471,"Language models pre-trained on biomedical corpora, such as BioBERT, have
recently shown promising results on downstream biomedical tasks. Many existing
pre-trained models, on the other hand, are resource-intensive and
computationally heavy owing to factors such as embedding size, hidden
dimension, and number of layers. The natural language processing (NLP)
community has developed numerous strategies to compress these models utilising
techniques such as pruning, quantisation, and knowledge distillation, resulting
in models that are considerably faster, smaller, and subsequently easier to use
in practice. By the same token, in this paper we introduce six lightweight
models, namely, BioDistilBERT, BioTinyBERT, BioMobileBERT, DistilBioBERT,
TinyBioBERT, and CompactBioBERT which are obtained either by knowledge
distillation from a biomedical teacher or continual learning on the Pubmed
dataset via the Masked Language Modelling (MLM) objective. We evaluate all of
our models on three biomedical tasks and compare them with BioBERT-v1.1 to
create efficient lightweight models that perform on par with their larger
counterparts. All the models will be publicly available on our Huggingface
profile at https://huggingface.co/nlpie and the codes used to run the
experiments will be available at
https://github.com/nlpie-research/Compact-Biomedical-Transformers.",https://huggingface.co/nlpie,-1
Flow-Guided Transformer for Video Inpainting,0.332367,"We propose a flow-guided transformer, which innovatively leverage the motion
discrepancy exposed by optical flows to instruct the attention retrieval in
transformer for high fidelity video inpainting. More specially, we design a
novel flow completion network to complete the corrupted flows by exploiting the
relevant flow features in a local temporal window. With the completed flows, we
propagate the content across video frames, and adopt the flow-guided
transformer to synthesize the rest corrupted regions. We decouple transformers
along temporal and spatial dimension, so that we can easily integrate the
locally relevant completed flows to instruct spatial attention only.
Furthermore, we design a flow-reweight module to precisely control the impact
of completed flows on each spatial transformer. For the sake of efficiency, we
introduce window partition strategy to both spatial and temporal transformers.
Especially in spatial transformer, we design a dual perspective spatial MHSA,
which integrates the global tokens to the window-based attention. Extensive
experiments demonstrate the effectiveness of the proposed method qualitatively
and quantitatively. Codes are available at https://github.com/hitachinsk/FGT.",https://github.com/hitachinsk/FGT,-1
Transformer Based Multi-Grained Features for Unsupervised Person Re-Identification,0.0369992,"Multi-grained features extracted from convolutional neural networks (CNNs)
have demonstrated their strong discrimination ability in supervised person
re-identification (Re-ID) tasks. Inspired by them, this work investigates the
way of extracting multi-grained features from a pure transformer network to
address the unsupervised Re-ID problem that is label-free but much more
challenging. To this end, we build a dual-branch network architecture based
upon a modified Vision Transformer (ViT). The local tokens output in each
branch are reshaped and then uniformly partitioned into multiple stripes to
generate part-level features, while the global tokens of two branches are
averaged to produce a global feature. Further, based upon offline-online
associated camera-aware proxies (O2CAP) that is a top-performing unsupervised
Re-ID method, we define offline and online contrastive learning losses with
respect to both global and part-level features to conduct unsupervised
learning. Extensive experiments on three person Re-ID datasets show that the
proposed method outperforms state-of-the-art unsupervised methods by a
considerable margin, greatly mitigating the gap to supervised counterparts.
Code will be available soon at https://github.com/RikoLi/WACV23-workshop-TMGF.",https://github.com/RikoLi/WACV23-workshop-TMGF,-1
Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology,0.591406,"Tissue phenotyping is a fundamental task in learning objective
characterizations of histopathologic biomarkers within the tumor-immune
microenvironment in cancer pathology. However, whole-slide imaging (WSI) is a
complex computer vision in which: 1) WSIs have enormous image resolutions with
precludes large-scale pixel-level efforts in data curation, and 2) diversity of
morphological phenotypes results in inter- and intra-observer variability in
tissue labeling. To address these limitations, current efforts have proposed
using pretrained image encoders (transfer learning from ImageNet,
self-supervised pretraining) in extracting morphological features from
pathology, but have not been extensively validated. In this work, we conduct a
search for good representations in pathology by training a variety of
self-supervised models with validation on a variety of weakly-supervised and
patch-level tasks. Our key finding is in discovering that Vision Transformers
using DINO-based knowledge distillation are able to learn data-efficient and
interpretable features in histology images wherein the different attention
heads learn distinct morphological phenotypes. We make evaluation code and
pretrained weights publicly-available at:
https://github.com/Richarizardd/Self-Supervised-ViT-Path.",https://github.com/Richarizardd/Self-Supervised-ViT-Path,-1
Bootstrapped Transformer for Offline Reinforcement Learning,0.0526886,"Offline reinforcement learning (RL) aims at learning policies from previously
collected static trajectory data without interacting with the real environment.
Recent works provide a novel perspective by viewing offline RL as a generic
sequence generation problem, adopting sequence models such as Transformer
architecture to model distributions over trajectories, and repurposing beam
search as a planning algorithm. However, the training datasets utilized in
general offline RL tasks are quite limited and often suffer from insufficient
distribution coverage, which could be harmful to training sequence generation
models yet has not drawn enough attention in the previous works. In this paper,
we propose a novel algorithm named Bootstrapped Transformer, which incorporates
the idea of bootstrapping and leverages the learned model to self-generate more
offline data to further boost the sequence model training. We conduct extensive
experiments on two offline RL benchmarks and demonstrate that our model can
largely remedy the existing offline RL training limitations and beat other
strong baseline methods. We also analyze the generated pseudo data and the
revealed characteristics may shed some light on offline RL training. The codes
are available at https://seqml.github.io/bootorl.",https://github.com/rail-berkeley/d4rl/blob/master/README.md,-1
Reconstruction of observed mechanical motions with Artificial Intelligence tools,0.0433225,"The goal of this paper is to determine the laws of observed trajectories
assuming that there is a mechanical system in the background and using these
laws to continue the observed motion in a plausible way. The laws are
represented by neural networks with a limited number of parameters. The
training of the networks follows the Extreme Learning Machine idea. We
determine laws for different levels of embedding, thus we can represent not
only the equation of motion but also the symmetries of different kinds. In the
recursive numerical evolution of the system, we require the fulfillment of all
the observed laws, within the determined numerical precision. In this way, we
can successfully reconstruct both integrable and chaotic motions, as we
demonstrate in the example of the gravity pendulum and the double pendulum.",None,-1
Guess What Moves: Unsupervised Video and Image Segmentation by Anticipating Motion,0.210135,"Motion, measured via optical flow, provides a powerful cue to discover and
learn objects in images and videos. However, compared to using appearance, it
has some blind spots, such as the fact that objects become invisible if they do
not move. In this work, we propose an approach that combines the strengths of
motion-based and appearance-based segmentation. We propose to supervise an
image segmentation network with the pretext task of predicting regions that are
likely to contain simple motion patterns, and thus likely to correspond to
objects. As the model only uses a single image as input, we can apply it in two
settings: unsupervised video segmentation, and unsupervised image segmentation.
We achieve state-of-the-art results for videos, and demonstrate the viability
of our approach on still images containing novel objects. Additionally we
experiment with different motion models and optical flow backbones and find the
method to be robust to these change. Project page and code available at
https://www.robots.ox.ac.uk/~vgg/research/gwm.",https://www.robots.ox.ac.uk/~vgg/research/gwm,-1
More Interpretable Graph Similarity Computation via Maximum Common Subgraph Inference,0.134577,"Graph similarity measurement, which computes the distance/similarity between
two graphs, arises in various graph-related tasks. Recent learning-based
methods lack interpretability, as they directly transform interaction
information between two graphs into one hidden vector and then map it to
similarity. To cope with this problem, this study proposes a more interpretable
end-to-end paradigm for graph similarity learning, named Similarity Computation
via Maximum Common Subgraph Inference (INFMCS). Our critical insight into
INFMCS is the strong correlation between similarity score and Maximum Common
Subgraph (MCS). We implicitly infer MCS to obtain the normalized MCS size, with
the supervision information being only the similarity score during training. To
capture more global information, we also stack some vanilla transformer encoder
layers with graph convolution layers and propose a novel permutation-invariant
node Positional Encoding. The entire model is quite simple yet effective.
Comprehensive experiments demonstrate that INFMCS consistently outperforms
state-of-the-art baselines for graph-graph classification and regression tasks.
Ablation experiments verify the effectiveness of the proposed computation
paradigm and other components. Also, visualization and statistics of results
reveal the interpretability of INFMCS.",https://github.com/cszhangzhen/H2MN,-1
MonoGround: Detecting Monocular 3D Objects from the Ground,0.149293,"Monocular 3D object detection has attracted great attention for its
advantages in simplicity and cost. Due to the ill-posed 2D to 3D mapping
essence from the monocular imaging process, monocular 3D object detection
suffers from inaccurate depth estimation and thus has poor 3D detection
results. To alleviate this problem, we propose to introduce the ground plane as
a prior in the monocular 3d object detection. The ground plane prior serves as
an additional geometric condition to the ill-posed mapping and an extra source
in depth estimation. In this way, we can get a more accurate depth estimation
from the ground. Meanwhile, to take full advantage of the ground plane prior,
we propose a depth-align training strategy and a precise two-stage depth
inference method tailored for the ground plane prior. It is worth noting that
the introduced ground plane prior requires no extra data sources like LiDAR,
stereo images, and depth information. Extensive experiments on the KITTI
benchmark show that our method could achieve state-of-the-art results compared
with other methods while maintaining a very fast speed. Our code and models are
available at https://github.com/cfzd/MonoGround.",https://github.com/cfzd/MonoGround,-1
Consistency Learning via Decoding Path Augmentation for Transformers in Human Object Interaction Detection,0.0588264,"Human-Object Interaction detection is a holistic visual recognition task that
entails object detection as well as interaction classification. Previous works
of HOI detection has been addressed by the various compositions of subset
predictions, e.g., Image -> HO -> I, Image -> HI -> O. Recently, transformer
based architecture for HOI has emerged, which directly predicts the HOI
triplets in an end-to-end fashion (Image -> HOI). Motivated by various
inference paths for HOI detection, we propose cross-path consistency learning
(CPC), which is a novel end-to-end learning strategy to improve HOI detection
for transformers by leveraging augmented decoding paths. CPC learning enforces
all the possible predictions from permuted inference sequences to be
consistent. This simple scheme makes the model learn consistent
representations, thereby improving generalization without increasing model
capacity. Our experiments demonstrate the effectiveness of our method, and we
achieved significant improvement on V-COCO and HICO-DET compared to the
baseline models. Our code is available at https://github.com/mlvlab/CPChoi.",https://github.com/mlvlab/CPChoi,-1
Are disentangled representations all you need to build speaker anonymization systems?,0.100921,"Speech signals contain a lot of sensitive information, such as the speaker's
identity, which raises privacy concerns when speech data get collected. Speaker
anonymization aims to transform a speech signal to remove the source speaker's
identity while leaving the spoken content unchanged. Current methods perform
the transformation by relying on content/speaker disentanglement and voice
conversion. Usually, an acoustic model from an automatic speech recognition
system extracts the content representation while an x-vector system extracts
the speaker representation. Prior work has shown that the extracted features
are not perfectly disentangled. This paper tackles how to improve features
disentanglement, and thus the converted anonymized speech. We propose enhancing
the disentanglement by removing speaker information from the acoustic model
using vector quantization. Evaluation done using the VoicePrivacy 2022 toolkit
showed that vector quantization helps conceal the original speaker identity
while maintaining utility for speech recognition.",https://colab.research.google.com/github/deep-privacy/SA-toolkit/blob/master/SA-colab.ipynb,-1
Domain Adaptation for Time-Series Classification to Mitigate Covariate Shift,0.0541704,"The performance of a machine learning model degrades when it is applied to
data from a similar but different domain than the data it has initially been
trained on. To mitigate this domain shift problem, domain adaptation (DA)
techniques search for an optimal transformation that converts the (current)
input data from a source domain to a target domain to learn a domain-invariant
representation that reduces domain discrepancy. This paper proposes a novel
supervised DA based on two steps. First, we search for an optimal
class-dependent transformation from the source to the target domain from a few
samples. We consider optimal transport methods such as the earth mover's
distance, Sinkhorn transport and correlation alignment. Second, we use
embedding similarity techniques to select the corresponding transformation at
inference. We use correlation metrics and higher-order moment matching
techniques. We conduct an extensive evaluation on time-series datasets with
domain shift including simulated and various online handwriting datasets to
demonstrate the performance.",None,-1
Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal,0.103173,"Transformer-based large language models are trained to make predictions about
the next word by aggregating representations of previous tokens through their
self-attention mechanism. In the field of cognitive modeling, such attention
patterns have recently been interpreted as embodying the process of cue-based
retrieval, in which attention over multiple targets is taken to generate
interference and latency during retrieval. Under this framework, this work
first defines an entropy-based predictor that quantifies the diffuseness of
self-attention, as well as distance-based predictors that capture the
incremental change in attention patterns across timesteps. Moreover, following
recent studies that question the informativeness of attention weights, we also
experiment with alternative methods for incorporating vector norms into
attention weights. Regression experiments using predictors calculated from the
GPT-2 language model show that these predictors deliver a substantially better
fit to held-out self-paced reading and eye-tracking data over a rigorous
baseline including GPT-2 surprisal. Additionally, the distance-based predictors
generally demonstrated higher predictive power, with effect sizes of up to 6.59
ms per standard deviation on self-paced reading times (compared to 2.82 ms for
surprisal) and 1.05 ms per standard deviation on eye-gaze durations (compared
to 3.81 ms for surprisal).",https://github.com/byungdoh/attn_dist,-1
Walk this Way! Entity Walks and Property Walks for RDF2vec,0.349561,"RDF2vec is a knowledge graph embedding mechanism which first extracts
sequences from knowledge graphs by performing random walks, then feeds those
into the word embedding algorithm word2vec for computing vector representations
for entities. In this poster, we introduce two new flavors of walk extraction
coined e-walks and p-walks, which put an emphasis on the structure or the
neighborhood of an entity respectively, and thereby allow for creating
embeddings which focus on similarity or relatedness. By combining the walk
strategies with order-aware and classic RDF2vec, as well as CBOW and skip-gram
word2vec embeddings, we conduct a preliminary evaluation with a total of 12
RDF2vec variants.",None,-1
Hybrid-Regressive Neural Machine Translation,0.0985043,"In this work, we empirically confirm that non-autoregressive translation with
an iterative refinement mechanism (IR-NAT) suffers from poor acceleration
robustness because it is more sensitive to decoding batch size and computing
device setting than autoregressive translation (AT). Inspired by it, we attempt
to investigate how to combine the strengths of autoregressive and
non-autoregressive translation paradigms better. To this end, we demonstrate
through synthetic experiments that prompting a small number of AT's predictions
can promote one-shot non-autoregressive translation to achieve the equivalent
performance of IR-NAT. Following this line, we propose a new two-stage
translation prototype called hybrid-regressive translation (HRT). Specifically,
HRT first generates discontinuous sequences via autoregression (e.g., make a
prediction every k tokens, k>1) and then fills in all previously skipped tokens
at once in a non-autoregressive manner. We also propose a bag of techniques to
effectively and efficiently train HRT without adding any model parameters. HRT
achieves the state-of-the-art BLEU score of 28.49 on the WMT En-De task and is
at least 1.5x faster than AT, regardless of batch size and device. In addition,
another bonus of HRT is that it successfully inherits the good characteristics
of AT in the deep-encoder-shallow-decoder architecture. Concretely, compared to
the vanilla HRT with a 6-layer encoder and 6-layer decoder, the inference speed
of HRT with a 12-layer encoder and 1-layer decoder is further doubled on both
GPU and CPU without BLEU loss.",None,-1
Data Feedback Loops: Model-driven Amplification of Dataset Biases,0.0981219,"Datasets scraped from the internet have been critical to the successes of
large-scale machine learning. Yet, this very success puts the utility of future
internet-derived datasets at potential risk, as model outputs begin to replace
human annotations as a source of supervision.
  In this work, we first formalize a system where interactions with one model
are recorded as history and scraped as training data in the future. We then
analyze its stability over time by tracking changes to a test-time bias
statistic (e.g. gender bias of model predictions). We find that the degree of
bias amplification is closely linked to whether the model's outputs behave like
samples from the training distribution, a behavior which we characterize and
define as consistent calibration. Experiments in three conditional prediction
scenarios - image classification, visual role-labeling, and language generation
- demonstrate that models that exhibit a sampling-like behavior are more
calibrated and thus more stable. Based on this insight, we propose an
intervention to help calibrate and stabilize unstable feedback systems.
  Code is available at https://github.com/rtaori/data_feedback.",https://github.com/rtaori/data_feedback,-1
Swin MAE: Masked Autoencoders for Small Datasets,0.12798,"The development of deep learning models in medical image analysis is majorly
limited by the lack of large-sized and well-annotated datasets. Unsupervised
learning does not require labels and is more suitable for solving medical image
analysis problems. However, most of the current unsupervised learning methods
need to be applied to large datasets. To make unsupervised learning applicable
to small datasets, we proposed Swin MAE, which is a masked autoencoder with
Swin Transformer as its backbone. Even on a dataset of only a few thousand
medical images and without using any pre-trained models, Swin MAE is still able
to learn useful semantic features purely from images. It can equal or even
slightly outperform the supervised model obtained by Swin Transformer trained
on ImageNet in terms of the transfer learning results of downstream tasks. The
code is publicly available at https://github.com/Zian-Xu/Swin-MAE.",https://github.com/Zian-Xu/Swin-MAE,-1
DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation,0.347625,"Task-oriented dialogue generation is challenging since the underlying
knowledge is often dynamic and effectively incorporating knowledge into the
learning process is hard. It is particularly challenging to generate both
human-like and informative responses in this setting. Recent research primarily
focused on various knowledge distillation methods where the underlying
relationship between the facts in a knowledge base is not effectively captured.
In this paper, we go one step further and demonstrate how the structural
information of a knowledge graph can improve the system's inference
capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue
system that effectively incorporates knowledge into a language model. Our
proposed system views relational knowledge as a knowledge graph and introduces
(1) a structure-aware knowledge embedding technique, and (2) a knowledge
graph-weighted attention masking strategy to facilitate the system selecting
relevant information during the dialogue generation. An empirical evaluation
demonstrates the effectiveness of DialoKG over state-of-the-art methods on
several standard benchmark datasets.",https://github.com/rashad101/DialoKG,-1
Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions,0.284149,"Current QA systems can generate reasonable-sounding yet false answers without
explanation or evidence for the generated answer, which is especially
problematic when humans cannot readily check the model's answers. This presents
a challenge for building trust in machine learning systems. We take inspiration
from real-world situations where difficult questions are answered by
considering opposing sides (see Irving et al., 2018). For multiple-choice QA
examples, we build a dataset of single arguments for both a correct and
incorrect answer option in a debate-style set-up as an initial step in training
models to produce explanations for two candidate answers. We use long contexts
-- humans familiar with the context write convincing explanations for
pre-selected correct and incorrect answers, and we test if those explanations
allow humans who have not read the full context to more accurately determine
the correct answer. We do not find that explanations in our set-up improve
human accuracy, but a baseline condition shows that providing human-selected
text snippets does improve accuracy. We use these findings to suggest ways of
improving the debate set up for future data collection efforts.",https://github.com/nyu-mll/single_turn_debate,-1
FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales,0.0682532,"Following how humans communicate, free-text rationales aim to use natural
language to explain neural language model (LM) behavior. However, free-text
rationales' unconstrained nature makes them prone to hallucination, so it is
important to have metrics for free-text rationale quality. Existing free-text
rationale metrics measure how consistent the rationale is with the LM's
predicted label, but there is no protocol for assessing such metrics'
reliability. Thus, we propose FRAME, a framework for evaluating rationale-label
consistency (RLC) metrics for free-text rationales. FRAME is based on three
axioms: (1) good metrics should yield highest scores for reference rationales,
which maximize RLC by construction; (2) good metrics should be appropriately
sensitive to semantic perturbation of rationales; and (3) good metrics should
be robust to variation in the LM's task performance. Across three text
classification datasets, we show that existing RLC metrics cannot satisfy all
three FRAME axioms, since they are implemented via model pretraining which
muddles the metric's signal. Then, we introduce a non-pretraining RLC metric
that greatly outperforms baselines on (1) and (3), while performing
competitively on (2). Finally, we discuss the limitations of using RLC to
evaluate free-text rationales.",None,-1
BOAT: Bilateral Local Attention Vision Transformer,0.111653,"Vision Transformers achieved outstanding performance in many computer vision
tasks. Early Vision Transformers such as ViT and DeiT adopt global
self-attention, which is computationally expensive when the number of patches
is large. To improve efficiency, recent Vision Transformers adopt local
self-attention mechanisms, where self-attention is computed within local
windows. Despite the fact that window-based local self-attention significantly
boosts efficiency, it fails to capture the relationships between distant but
similar patches in the image plane. To overcome this limitation of image-space
local attention, in this paper, we further exploit the locality of patches in
the feature space. We group the patches into multiple clusters using their
features, and self-attention is computed within every cluster. Such
feature-space local attention effectively captures the connections between
patches across different local windows but still relevant. We propose a
Bilateral lOcal Attention vision Transformer (BOAT), which integrates
feature-space local attention with image-space local attention. We further
integrate BOAT with both Swin and CSWin models, and extensive experiments on
several benchmark datasets demonstrate that our BOAT-CSWin model clearly and
consistently outperforms existing state-of-the-art CNN models and vision
Transformers.",None,-1
Concept Bottleneck Model with Additional Unsupervised Concepts,0.901364,"With the increasing demands for accountability, interpretability is becoming
an essential capability for real-world AI applications. However, most methods
utilize post-hoc approaches rather than training the interpretable model. In
this article, we propose a novel interpretable model based on the concept
bottleneck model (CBM). CBM uses concept labels to train an intermediate layer
as the additional visible layer. However, because the number of concept labels
restricts the dimension of this layer, it is difficult to obtain high accuracy
with a small number of labels. To address this issue, we integrate supervised
concepts with unsupervised ones trained with self-explaining neural networks
(SENNs). By seamlessly training these two types of concepts while reducing the
amount of computation, we can obtain both supervised and unsupervised concepts
simultaneously, even for large-sized images. We refer to the proposed model as
the concept bottleneck model with additional unsupervised concepts (CBM-AUC).
We experimentally confirmed that the proposed model outperformed CBM and SENN.
We also visualized the saliency map of each concept and confirmed that it was
consistent with the semantic meanings.",https://github.com/yewsiang/ConceptBottleneck,-1
Transforming Gait: Video-Based Spatiotemporal Gait Analysis,0.111685,"Human pose estimation from monocular video is a rapidly advancing field that
offers great promise to human movement science and rehabilitation. This
potential is tempered by the smaller body of work ensuring the outputs are
clinically meaningful and properly calibrated. Gait analysis, typically
performed in a dedicated lab, produces precise measurements including
kinematics and step timing. Using over 7000 monocular video from an
instrumented gait analysis lab, we trained a neural network to map 3D joint
trajectories and the height of individuals onto interpretable biomechanical
outputs including gait cycle timing and sagittal plane joint kinematics and
spatiotemporal trajectories. This task specific layer produces accurate
estimates of the timing of foot contact and foot off events. After parsing the
kinematic outputs into individual gait cycles, it also enables accurate
cycle-by-cycle estimates of cadence, step time, double and single support time,
walking speed and step length.",https://github.com/open-mmlab/mmpose,-1
Multi-Granularity Prediction for Scene Text Recognition,0.341716,"Scene text recognition (STR) has been an active research topic in computer
vision for years. To tackle this challenging problem, numerous innovative
methods have been successively proposed and incorporating linguistic knowledge
into STR models has recently become a prominent trend. In this work, we first
draw inspiration from the recent progress in Vision Transformer (ViT) to
construct a conceptually simple yet powerful vision STR model, which is built
upon ViT and outperforms previous state-of-the-art models for scene text
recognition, including both pure vision models and language-augmented methods.
To integrate linguistic knowledge, we further propose a Multi-Granularity
Prediction strategy to inject information from the language modality into the
model in an implicit way, i.e. , subword representations (BPE and WordPiece)
widely-used in NLP are introduced into the output space, in addition to the
conventional character level representation, while no independent language
model (LM) is adopted. The resultant algorithm (termed MGP-STR) is able to push
the performance envelop of STR to an even higher level. Specifically, it
achieves an average recognition accuracy of 93.35% on standard benchmarks. Code
is available at
https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR.",None,-1
Fine-tuned Sentiment Analysis of COVID-19 Vaccine-Related Social Media Data: Comparative Study,0.0457077,"This study investigated and compared public sentiment related to COVID-19
vaccines expressed on two popular social media platforms, Reddit and Twitter,
harvested from January 1, 2020, to March 1, 2022. To accomplish this task, we
created a fine-tuned DistilRoBERTa model to predict sentiments of approximately
9.5 million Tweets and 70 thousand Reddit comments. To fine-tune our model, our
team manually labeled the sentiment of 3600 Tweets and then augmented our
dataset by the method of back-translation. Text sentiment for each social media
platform was then classified with our fine-tuned model using Python and the
Huggingface sentiment analysis pipeline. Our results determined that the
average sentiment expressed on Twitter was more negative (52% positive) than
positive and the sentiment expressed on Reddit was more positive than negative
(53% positive). Though average sentiment was found to vary between these social
media platforms, both displayed similar behavior related to sentiment shared at
key vaccine-related developments during the pandemic. Considering this similar
trend in shared sentiment demonstrated across social media platforms, Twitter
and Reddit continue to be valuable data sources that public health officials
can utilize to strengthen vaccine confidence and combat misinformation. As the
spread of misinformation poses a range of psychological and psychosocial risks
(anxiety, fear, etc.), there is an urgency in understanding the public
perspective and attitude toward shared falsities. Comprehensive educational
delivery systems tailored to the population's expressed sentiments that
facilitate digital literacy, health information-seeking behavior, and precision
health promotion could aid in clarifying such misinformation.",None,-1
Learned k-NN Distance Estimation,0.0755769,"Big data mining is well known to be an important task for data science,
because it can provide useful observations and new knowledge hidden in given
large datasets. Proximity-based data analysis is particularly utilized in many
real-life applications. In such analysis, the distances to k nearest neighbors
are usually employed, thus its main bottleneck is derived from data retrieval.
Much efforts have been made to improve the efficiency of these analyses.
However, they still incur large costs, because they essentially need many data
accesses. To avoid this issue, we propose a machine-learning technique that
quickly and accurately estimates the k-NN distances (i.e., distances to the k
nearest neighbors) of a given query. We train a fully connected neural network
model and utilize pivots to achieve accurate estimation. Our model is designed
to have useful advantages: it infers distances to the k-NNs at a time, its
inference time is O(1) (no data accesses are incurred), but it keeps high
accuracy. Our experimental results and case studies on real datasets
demonstrate the efficiency and effectiveness of our solution.",https://github.com/arailly/pivnet,-1
Splatting-based Synthesis for Video Frame Interpolation,0.269029,"Frame interpolation is an essential video processing technique that adjusts
the temporal resolution of an image sequence. While deep learning has brought
great improvements to the area of video frame interpolation, techniques that
make use of neural networks can typically not easily be deployed in practical
applications like a video editor since they are either computationally too
demanding or fail at high resolutions. In contrast, we propose a deep learning
approach that solely relies on splatting to synthesize interpolated frames.
This splatting-based synthesis for video frame interpolation is not only much
faster than similar approaches, especially for multi-frame interpolation, but
can also yield new state-of-the-art results at high resolutions.",None,-1
Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations,0.394693,"Neural networks have achieved tremendous success in a large variety of
applications. However, their memory footprint and computational demand can
render them impractical in application settings with limited hardware or energy
resources. In this work, we propose a novel algorithm to find efficient
low-rank subnetworks. Remarkably, these subnetworks are determined and adapted
already during the training phase and the overall time and memory resources
required by both training and evaluating them are significantly reduced. The
main idea is to restrict the weight matrices to a low-rank manifold and to
update the low-rank factors rather than the full matrix during training. To
derive training updates that are restricted to the prescribed manifold, we
employ techniques from dynamic model order reduction for matrix differential
equations. This allows us to provide approximation, stability, and descent
guarantees. Moreover, our method automatically and dynamically adapts the ranks
during training to achieve the desired approximation accuracy. The efficiency
of the proposed method is demonstrated through a variety of numerical
experiments on fully-connected and convolutional networks.",https://github.com/COMPiLELab/DLRT,-1
Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances,0.0477389,"With the advent of Deep Learning (DL), Super-Resolution (SR) has also become
a thriving research area. However, despite promising results, the field still
faces challenges that require further research e.g., allowing flexible
upsampling, more effective loss functions, and better evaluation metrics. We
review the domain of SR in light of recent advances, and examine
state-of-the-art models such as diffusion (DDPM) and transformer-based SR
models. We present a critical discussion on contemporary strategies used in SR,
and identify promising yet unexplored research directions. We complement
previous surveys by incorporating the latest developments in the field such as
uncertainty-driven losses, wavelet networks, neural architecture search, novel
normalization methods, and the latests evaluation techniques. We also include
several visualizations for the models and methods throughout each chapter in
order to facilitate a global understanding of the trends in the field. This
review is ultimately aimed at helping researchers to push the boundaries of DL
applied to SR.",None,-1
Iterative Patch Selection for High-Resolution Image Recognition,0.0275361,"High-resolution images are prevalent in various applications, such as
autonomous driving and computer-aided diagnosis. However, training neural
networks on such images is computationally challenging and easily leads to
out-of-memory errors even on modern GPUs. We propose a simple method, Iterative
Patch Selection (IPS), which decouples the memory usage from the input size and
thus enables the processing of arbitrarily large images under tight hardware
constraints. IPS achieves this by selecting only the most salient patches,
which are then aggregated into a global representation for image recognition.
For both patch selection and aggregation, a cross-attention based transformer
is introduced, which exhibits a close connection to Multiple Instance Learning.
Our method demonstrates strong performance and has wide applicability across
different domains, training regimes and image sizes while using minimal
accelerator memory. For example, we are able to finetune our model on
whole-slide images consisting of up to 250k patches (>16 gigapixels) with only
5 GB of GPU VRAM at a batch size of 16.",https://github.com/benbergner/ips,-1
Unsupervised 4D LiDAR Moving Object Segmentation in Stationary Settings with Multivariate Occupancy Time Series,0.0848638,"In this work, we address the problem of unsupervised moving object
segmentation (MOS) in 4D LiDAR data recorded from a stationary sensor, where no
ground truth annotations are involved. Deep learning-based state-of-the-art
methods for LiDAR MOS strongly depend on annotated ground truth data, which is
expensive to obtain and scarce in existence. To close this gap in the
stationary setting, we propose a novel 4D LiDAR representation based on
multivariate time series that relaxes the problem of unsupervised MOS to a time
series clustering problem. More specifically, we propose modeling the change in
occupancy of a voxel by a multivariate occupancy time series (MOTS), which
captures spatio-temporal occupancy changes on the voxel level and its
surrounding neighborhood. To perform unsupervised MOS, we train a neural
network in a self-supervised manner to encode MOTS into voxel-level feature
representations, which can be partitioned by a clustering algorithm into moving
or stationary. Experiments on stationary scenes from the Raw KITTI dataset show
that our fully unsupervised approach achieves performance that is comparable to
that of supervised state-of-the-art approaches.",https://github.com/thkreutz/umosmots,-1
Contrastive Language-Image Pre-Training with Knowledge Graphs,0.061424,"Recent years have witnessed the fast development of large-scale pre-training
frameworks that can extract multi-modal representations in a unified form and
achieve promising performances when transferred to downstream tasks.
Nevertheless, existing approaches mainly focus on pre-training with simple
image-text pairs, while neglecting the semantic connections between concepts
from different modalities. In this paper, we propose a knowledge-based
pre-training framework, dubbed Knowledge-CLIP, which injects semantic
information into the widely used CLIP model. Through introducing
knowledge-based objectives in the pre-training process and utilizing different
types of knowledge graphs as training data, our model can semantically align
the representations in vision and language with higher quality, and enhance the
reasoning ability across scenarios and modalities. Extensive experiments on
various vision-language downstream tasks demonstrate the effectiveness of
Knowledge-CLIP compared with the original CLIP and competitive baselines.",None,-1
SoK: Differential Privacy on Graph-Structured Data,0.06181,"In this work, we study the applications of differential privacy (DP) in the
context of graph-structured data. We discuss the formulations of DP applicable
to the publication of graphs and their associated statistics as well as machine
learning on graph-based data, including graph neural networks (GNNs). The
formulation of DP in the context of graph-structured data is difficult, as
individual data points are interconnected (often non-linearly or sparsely).
This connectivity complicates the computation of individual privacy loss in
differentially private learning. The problem is exacerbated by an absence of a
single, well-established formulation of DP in graph settings. This issue
extends to the domain of GNNs, rendering private machine learning on
graph-structured data a challenging task. A lack of prior systematisation work
motivated us to study graph-based learning from a privacy perspective. In this
work, we systematise different formulations of DP on graphs, discuss challenges
and promising applications, including the GNN domain. We compare and separate
works into graph analysis tasks and graph learning tasks with GNNs. Finally, we
conclude our work with a discussion of open questions and potential directions
for further research in this area.",None,-1
PointInst3D: Segmenting 3D Instances by Points,0.123626,"The current state-of-the-art methods in 3D instance segmentation typically
involve a clustering step, despite the tendency towards heuristics, greedy
algorithms, and a lack of robustness to the changes in data statistics. In
contrast, we propose a fully-convolutional 3D point cloud instance segmentation
method that works in a per-point prediction fashion. In doing so it avoids the
challenges that clustering-based methods face: introducing dependencies among
different tasks of the model. We find the key to its success is assigning a
suitable target to each sampled point. Instead of the commonly used static or
distance-based assignment strategies, we propose to use an Optimal Transport
approach to optimally assign target masks to the sampled points according to
the dynamic matching costs. Our approach achieves promising results on both
ScanNet and S3DIS benchmarks. The proposed approach removes intertask
dependencies and thus represents a simpler and more flexible 3D instance
segmentation framework than other competing methods, while achieving improved
segmentation accuracy.",None,-1
UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes,0.930517,"We introduce UViM, a unified approach capable of modeling a wide range of
computer vision tasks. In contrast to previous models, UViM has the same
functional form for all tasks; it requires no task-specific modifications which
require extensive human expertise. The approach involves two components: (I) a
base model (feed-forward) which is trained to directly predict raw vision
outputs, guided by a learned discrete code and (II) a language model
(autoregressive) that is trained to generate the guiding code. These components
complement each other: the language model is well-suited to modeling structured
interdependent data, while the base model is efficient at dealing with
high-dimensional outputs. We demonstrate the effectiveness of UViM on three
diverse and challenging vision tasks: panoptic segmentation, depth prediction
and image colorization, where we achieve competitive and near state-of-the-art
results. Our experimental results suggest that UViM is a promising candidate
for a unified modeling approach in computer vision.",https://github.com/google-research/big_vision,-1
Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions,0.0710647,"Generalization across different environments with the same tasks is critical
for successful applications of visual reinforcement learning (RL) in real
scenarios. However, visual distractions -- which are common in real scenes --
from high-dimensional observations can be hurtful to the learned
representations in visual RL, thus degrading the performance of generalization.
To tackle this problem, we propose a novel approach, namely Characteristic
Reward Sequence Prediction (CRESP), to extract the task-relevant information by
learning reward sequence distributions (RSDs), as the reward signals are
task-relevant in RL and invariant to visual distractions. Specifically, to
effectively capture the task-relevant information via RSDs, CRESP introduces an
auxiliary task -- that is, predicting the characteristic functions of RSDs --
to learn task-relevant representations, because we can well approximate the
high-dimensional distributions by leveraging the corresponding characteristic
functions. Experiments demonstrate that CRESP significantly improves the
performance of generalization on unseen environments, outperforming several
state-of-the-arts on DeepMind Control tasks with different visual distractions.",None,-1
The State of Sparse Training in Deep Reinforcement Learning,0.0412124,"The use of sparse neural networks has seen rapid growth in recent years,
particularly in computer vision. Their appeal stems largely from the reduced
number of parameters required to train and store, as well as in an increase in
learning efficiency. Somewhat surprisingly, there have been very few efforts
exploring their use in Deep Reinforcement Learning (DRL). In this work we
perform a systematic investigation into applying a number of existing sparse
training techniques on a variety of DRL agents and environments. Our results
corroborate the findings from sparse training in the computer vision domain -
sparse networks perform better than dense networks for the same parameter count
- in the DRL domain. We provide detailed analyses on how the various components
in DRL are affected by the use of sparse networks and conclude by suggesting
promising avenues for improving the effectiveness of sparse training methods,
as well as for advancing their use in DRL.",None,-1
"""Think Before You Speak"": Improving Multi-Action Dialog Policy by Planning Single-Action Dialogs",0.0985866,"Multi-action dialog policy (MADP), which generates multiple atomic dialog
actions per turn, has been widely applied in task-oriented dialog systems to
provide expressive and efficient system responses. Existing MADP models usually
imitate action combinations from the labeled multi-action dialog samples. Due
to data limitations, they generalize poorly toward unseen dialog flows. While
interactive learning and reinforcement learning algorithms can be applied to
incorporate external data sources of real users and user simulators, they take
significant manual effort to build and suffer from instability. To address
these issues, we propose Planning Enhanced Dialog Policy (PEDP), a novel
multi-task learning framework that learns single-action dialog dynamics to
enhance multi-action prediction. Our PEDP method employs model-based planning
for conceiving what to express before deciding the current response through
simulating single-action dialogs. Experimental results on the MultiWOZ dataset
demonstrate that our fully supervised learning-based method achieves a solid
task success rate of 90.6%, improving 3% compared to the state-of-the-art
methods.",https://github.com/ShuoZhangXJTU/PEDP,-1
Adversarially-Aware Robust Object Detector,0.122694,"Object detection, as a fundamental computer vision task, has achieved a
remarkable progress with the emergence of deep neural networks. Nevertheless,
few works explore the adversarial robustness of object detectors to resist
adversarial attacks for practical applications in various real-world scenarios.
Detectors have been greatly challenged by unnoticeable perturbation, with sharp
performance drop on clean images and extremely poor performance on adversarial
images. In this work, we empirically explore the model training for adversarial
robustness in object detection, which greatly attributes to the conflict
between learning clean images and adversarial images. To mitigate this issue,
we propose a Robust Detector (RobustDet) based on adversarially-aware
convolution to disentangle gradients for model learning on clean and
adversarial images. RobustDet also employs the Adversarial Image Discriminator
(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable
robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that
our model effectively disentangles gradients and significantly enhances the
detection robustness with maintaining the detection ability on clean images.",https://github.com/7eu7d7/RobustDet,-1
Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization,0.552302,"The most advanced abstractive dialogue summarizers lack generalization
ability on new domains and the existing researches for domain adaptation in
summarization generally rely on large-scale pre-trainings. To explore the
lightweight fine-tuning methods for domain adaptation of dialogue
summarization, in this paper, we propose an efficient and generalizable
Domain-Oriented Prefix-tuning model, which utilizes a domain word initialized
prefix module to alleviate domain entanglement and adopts discrete prompts to
guide the model to focus on key contents of dialogues and enhance model
generalization. We conduct zero-shot experiments and build domain adaptation
benchmarks on two multi-domain dialogue summarization datasets, TODSum and
QMSum. Adequate experiments and qualitative analysis prove the effectiveness of
our methods.",https://github.com/Zeng-WH/DOP-Tuning,-1
An LSTM model for Twitter Sentiment Analysis,0.0364171,"Sentiment analysis on social media such as Twitter provides organizations and
individuals an effective way to monitor public emotions towards them and their
competitors. As a result, sentiment analysis has become an important and
challenging task. In this work, we have collected seven publicly available and
manually annotated twitter sentiment datasets. We create a new training and
testing dataset from the collected datasets. We develop an LSTM model to
classify sentiment of a tweet and evaluate the model with the new dataset.",None,-1
Learning to See Through with Events,0.211256,"Although synthetic aperture imaging (SAI) can achieve the seeing-through
effect by blurring out off-focus foreground occlusions while recovering
in-focus occluded scenes from multi-view images, its performance is often
deteriorated by dense occlusions and extreme lighting conditions. To address
the problem, this paper presents an Event-based SAI (E-SAI) method by relying
on the asynchronous events with extremely low latency and high dynamic range
acquired by an event camera. Specifically, the collected events are first
refocused by a Refocus-Net module to align in-focus events while scattering out
off-focus ones. Following that, a hybrid network composed of spiking neural
networks (SNNs) and convolutional neural networks (CNNs) is proposed to encode
the spatio-temporal information from the refocused events and reconstruct a
visual image of the occluded targets. Extensive experiments demonstrate that
our proposed E-SAI method can achieve remarkable performance in dealing with
very dense occlusions and extreme lighting conditions and produce high-quality
images from pure events. Codes and datasets are available at
https://dvs-whu.cn/projects/esai/.",https://github.com/YingqianWang/DeOccNet,-1
MMFN: Multi-Modal-Fusion-Net for End-to-End Driving,0.226933,"Inspired by the fact that humans use diverse sensory organs to perceive the
world, sensors with different modalities are deployed in end-to-end driving to
obtain the global context of the 3D scene. In previous works, camera and LiDAR
inputs are fused through transformers for better driving performance. These
inputs are normally further interpreted as high-level map information to assist
navigation tasks. Nevertheless, extracting useful information from the complex
map input is challenging, for redundant information may mislead the agent and
negatively affect driving performance. We propose a novel approach to
efficiently extract features from vectorized High-Definition (HD) maps and
utilize them in the end-to-end driving tasks. In addition, we design a new
expert to further enhance the model performance by considering multi-road
rules. Experimental results prove that both of the proposed improvements enable
our agent to achieve superior performance compared with other methods.",https://github.com/Kin-Zhang/mmfn,-1
DSI++: Updating Transformer Memory with New Documents,0.638969,"Differentiable Search Indices (DSIs) encode a corpus of documents in model
parameters and use the same model to answer user queries directly. Despite the
strong performance of DSI models, deploying them in situations where the corpus
changes over time is computationally expensive because reindexing the corpus
requires re-training the model. In this work, we introduce DSI++, a continual
learning challenge for DSI to incrementally index new documents while being
able to answer queries related to both previously and newly indexed documents.
Across different model scales and document identifier representations, we show
that continual indexing of new documents leads to considerable forgetting of
previously indexed documents. We also hypothesize and verify that the model
experiences forgetting events during training, leading to unstable learning. To
mitigate these issues, we investigate two approaches. The first focuses on
modifying the training dynamics. Flatter minima implicitly alleviate
forgetting, so we optimize for flatter loss basins and show that the model
stably memorizes more documents ($+12\%$). Next, we introduce a generative
memory to sample pseudo-queries for documents and supplement them during
continual indexing to prevent forgetting for the retrieval task. Extensive
experiments on novel continual indexing benchmarks based on Natural Questions
(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting
significantly. Concretely, it improves the average Hits@10 by $+21.1\%$ over
competitive baselines for NQ and requires $6$ times fewer model updates
compared to re-training the DSI model for incrementally indexing five corpora
in a sequence.",None,-1
Apport des ontologies pour le calcul de la similarité sémantique au sein d'un système de recommandation,0.136331,"Measurement of the semantic relatedness or likeness between terms, words, or
text data plays an important role in different applications dealing with
textual data such as knowledge acquisition, recommender system, and natural
language processing. Over the past few years, many ontologies have been
developed and used as a form of structured representation of knowledge bases
for information systems. The calculation of semantic similarity from ontology
has developed and depending on the context is complemented by other similarity
calculation methods. In this paper, we propose and carry on an approach for the
calculation of ontology-based semantic similarity using in the context of a
recommender system.",None,-1
ADAS: A Direct Adaptation Strategy for Multi-Target Domain Adaptive Semantic Segmentation,0.184127,"In this paper, we present a direct adaptation strategy (ADAS), which aims to
directly adapt a single model to multiple target domains in a semantic
segmentation task without pretrained domain-specific models. To do so, we
design a multi-target domain transfer network (MTDT-Net) that aligns visual
attributes across domains by transferring the domain distinctive features
through a new target adaptive denormalization (TAD) module. Moreover, we
propose a bi-directional adaptive region selection (BARS) that reduces the
attribute ambiguity among the class labels by adaptively selecting the regions
with consistent feature statistics. We show that our single MTDT-Net can
synthesize visually pleasing domain transferred images with complex driving
datasets, and BARS effectively filters out the unnecessary region of training
images for each target domain. With the collaboration of MTDT-Net and BARS, our
ADAS achieves state-of-the-art performance for multi-target domain adaptation
(MTDA). To the best of our knowledge, our method is the first MTDA method that
directly adapts to multiple domains in semantic segmentation.",None,-1
Using Forwards-Backwards Models to Approximate MDP Homomorphisms,0.0,"Reinforcement learning agents must painstakingly learn through trial and
error what sets of state-action pairs are value equivalent -- requiring an
often prohibitively large amount of environment experience. MDP homomorphisms
have been proposed that reduce the MDP of an environment to an abstract MDP,
enabling better sample efficiency. Consequently, impressive improvements have
been achieved when a suitable homomorphism can be constructed a priori --
usually by exploiting a practitioner's knowledge of environment symmetries. We
propose a novel approach to constructing homomorphisms in discrete action
spaces, which uses a learnt model of environment dynamics to infer which
state-action pairs lead to the same state -- which can reduce the size of the
state-action space by a factor as large as the cardinality of the original
action space. In MinAtar, we report an almost 4x improvement over a value-based
off-policy baseline in the low sample limit, when averaging over all games and
optimizers.",None,-1
Bayesian Optimization under Stochastic Delayed Feedback,0.270697,"Bayesian optimization (BO) is a widely-used sequential method for
zeroth-order optimization of complex and expensive-to-compute black-box
functions. The existing BO methods assume that the function evaluation
(feedback) is available to the learner immediately or after a fixed delay. Such
assumptions may not be practical in many real-life problems like online
recommendations, clinical trials, and hyperparameter tuning where feedback is
available after a random delay. To benefit from the experimental
parallelization in these problems, the learner needs to start new function
evaluations without waiting for delayed feedback. In this paper, we consider
the BO under stochastic delayed feedback problem. We propose algorithms with
sub-linear regret guarantees that efficiently address the dilemma of selecting
new function queries while waiting for randomly delayed feedback. Building on
our results, we also make novel contributions to batch BO and contextual
Gaussian process bandits. Experiments on synthetic and real-life datasets
verify the performance of our algorithms.",https://github.com/daizhongxiang/BO-SDF,-1
Re-Examining Calibration: The Case of Question Answering,0.0333021,"For users to trust model predictions, they need to understand model outputs,
particularly their confidence - calibration aims to adjust (calibrate) models'
confidence to match expected accuracy. We argue that the traditional
calibration evaluation does not promote effective calibrations: for example, it
can encourage always assigning a mediocre confidence score to all predictions,
which does not help users distinguish correct predictions from wrong ones.
Building on those observations, we propose a new calibration metric, MacroCE,
that better captures whether the model assigns low confidence to wrong
predictions and high confidence to correct predictions. Focusing on the
practical application of open-domain question answering, we examine
conventional calibration methods applied on the widely-used retriever-reader
pipeline, all of which do not bring significant gains under our new MacroCE
metric. Toward better calibration, we propose a new calibration method
(ConsCal) that uses not just final model predictions but whether multiple model
checkpoints make consistent predictions. Altogether, we provide an alternative
view of calibration along with a new metric, re-evaluation of existing
calibration methods on our metric, and proposal of a more effective calibration
method.",https://github.com/NoviScl/calibrateQA,-1
MHMS: Multimodal Hierarchical Multimedia Summarization,0.531967,"Multimedia summarization with multimodal output can play an essential role in
real-world applications, i.e., automatically generating cover images and titles
for news articles or providing introductions to online videos. In this work, we
propose a multimodal hierarchical multimedia summarization (MHMS) framework by
interacting visual and language domains to generate both video and textual
summaries. Our MHMS method contains video and textual segmentation and
summarization module, respectively. It formulates a cross-domain alignment
objective with optimal transport distance which leverages cross-domain
interaction to generate the representative keyframe and textual summary. We
evaluated MHMS on three recent multimodal datasets and demonstrated the
effectiveness of our method in producing high-quality multimodal summaries.",None,-1
Modeling Dual Read/Write Paths for Simultaneous Machine Translation,0.210929,"Simultaneous machine translation (SiMT) outputs translation while reading
source sentence and hence requires a policy to decide whether to wait for the
next source word (READ) or generate a target word (WRITE), the actions of which
form a read/write path. Although the read/write path is essential to SiMT
performance, no direct supervision is given to the path in the existing
methods. In this paper, we propose a method of dual-path SiMT which introduces
duality constraints to direct the read/write path. According to duality
constraints, the read/write path in source-to-target and target-to-source SiMT
models can be mapped to each other. As a result, the two SiMT models can be
optimized jointly by forcing their read/write paths to satisfy the mapping.
Experiments on En-Vi and De-En tasks show that our method can outperform strong
baselines under all latency.",https://github.com/ictnlp/,-1
Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines,0.169591,"Strong inductive biases give humans the ability to quickly learn to perform a
variety of tasks. Although meta-learning is a method to endow neural networks
with useful inductive biases, agents trained by meta-learning may sometimes
acquire very different strategies from humans. We show that co-training these
agents on predicting representations from natural language task descriptions
and programs induced to generate such tasks guides them toward more human-like
inductive biases. Human-generated language descriptions and program induction
models that add new learned primitives both contain abstract concepts that can
compress description length. Co-training on these representations result in
more human-like behavior in downstream meta-reinforcement learning agents than
less abstract controls (synthetic language descriptions, program induction
without learned primitives), suggesting that the abstraction supported by these
representations is key.",None,-1
CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation,0.394304,"3D human pose estimation can be handled by encoding the geometric
dependencies between the body parts and enforcing the kinematic constraints.
Recently, Transformer has been adopted to encode the long-range dependencies
between the joints in the spatial and temporal domains. While they had shown
excellence in long-range dependencies, studies have noted the need for
improving the locality of vision Transformers. In this direction, we propose a
novel pose estimation Transformer featuring rich representations of body joints
critical for capturing subtle changes across frames (i.e., inter-feature
representation). Specifically, through two novel interaction modules;
Cross-Joint Interaction and Cross-Frame Interaction, the model explicitly
encodes the local and global dependencies between the body joints. The proposed
architecture achieved state-of-the-art performance on two popular 3D human pose
estimation datasets, Human3.6 and MPI-INF-3DHP. In particular, our proposed
CrossFormer method boosts performance by 0.9% and 0.3%, compared to the closest
counterpart, PoseFormer, using the detected 2D poses and ground-truth settings
respectively.",https://github.com/xxx,-1
Uncertainty-aware Personal Assistant for Making Personalized Privacy Decisions,0.430717,"Many software systems, such as online social networks enable users to share
information about themselves. While the action of sharing is simple, it
requires an elaborate thought process on privacy: what to share, with whom to
share, and for what purposes. Thinking about these for each piece of content to
be shared is tedious. Recent approaches to tackle this problem build personal
assistants that can help users by learning what is private over time and
recommending privacy labels such as private or public to individual content
that a user considers sharing. However, privacy is inherently ambiguous and
highly personal. Existing approaches to recommend privacy decisions do not
address these aspects of privacy sufficiently. Ideally, a personal assistant
should be able to adjust its recommendation based on a given user, considering
that user's privacy understanding. Moreover, the personal assistant should be
able to assess when its recommendation would be uncertain and let the user make
the decision on her own. Accordingly, this paper proposes a personal assistant
that uses evidential deep learning to classify content based on its privacy
label. An important characteristic of the personal assistant is that it can
model its uncertainty in its decisions explicitly, determine that it does not
know the answer, and delegate from making a recommendation when its uncertainty
is high. By factoring in the user's own understanding of privacy, such as risk
factors or own labels, the personal assistant can personalize its
recommendations per user. We evaluate our proposed personal assistant using a
well-known data set. Our results show that our personal assistant can
accurately identify uncertain cases, personalize them to its user's needs, and
thus helps users preserve their privacy well.",None,-1
Pre-training to Match for Unified Low-shot Relation Extraction,0.536421,"Low-shot relation extraction~(RE) aims to recognize novel relations with very
few or even no samples, which is critical in real scenario application.
Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem
to be with similar target but require totally different underlying abilities.
In this paper, we propose Multi-Choice Matching Networks to unify low-shot
relation extraction. To fill in the gap between zero-shot and few-shot RE, we
propose the triplet-paraphrase meta-training, which leverages triplet
paraphrase to pre-train zero-shot label matching ability and uses meta-learning
paradigm to learn few-shot instance summarizing ability. Experimental results
on three different low-shot RE tasks show that the proposed method outperforms
strong baselines by a large margin, and achieve the best performance on
few-shot RE leaderboard.",https://github.com/fc-liu/MCMN,-1
"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",0.231077,"Understanding longer narratives or participating in conversations requires
tracking of discourse entities that have been mentioned. Indefinite noun
phrases (NPs), such as 'a dog', frequently introduce discourse entities but
this behavior is modulated by sentential operators such as negation. For
example, 'a dog' in 'Arthur doesn't own a dog' does not introduce a discourse
entity due to the presence of negation. In this work, we adapt the
psycholinguistic assessment of language models paradigm to higher-level
linguistic phenomena and introduce an English evaluation suite that targets the
knowledge of the interactions between sentential operators and indefinite NPs.
We use this evaluation suite for a fine-grained investigation of the entity
tracking abilities of the Transformer-based models GPT-2 and GPT-3. We find
that while the models are to a certain extent sensitive to the interactions we
investigate, they are all challenged by the presence of multiple NPs and their
behavior is not systematic, which suggests that even models at the scale of
GPT-3 do not fully acquire basic entity tracking abilities.",https://github.com/sebschu/discourse-entity-lm,-1
Code-Switching without Switching: Language Agnostic End-to-End Speech Translation,0.0542562,"We propose a) a Language Agnostic end-to-end Speech Translation model (LAST),
and b) a data augmentation strategy to increase code-switching (CS)
performance. With increasing globalization, multiple languages are increasingly
used interchangeably during fluent speech. Such CS complicates traditional
speech recognition and translation, as we must recognize which language was
spoken first and then apply a language-dependent recognizer and subsequent
translation component to generate the desired target language output. Such a
pipeline introduces latency and errors. In this paper, we eliminate the need
for that, by treating speech recognition and translation as one unified
end-to-end speech translation problem. By training LAST with both input
languages, we decode speech into one target language, regardless of the input
language. LAST delivers comparable recognition and speech translation accuracy
in monolingual usage, while reducing latency and error rate considerably when
CS is observed.",None,-1
Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in Dialog Systems,0.0285097,"Dialog systems are often designed or trained to output human-like responses.
However, some responses may be impossible for a machine to truthfully say (e.g.
""that movie made me cry""). Highly anthropomorphic responses might make users
uncomfortable or implicitly deceive them into thinking they are interacting
with a human. We collect human ratings on the feasibility of approximately 900
two-turn dialogs sampled from 9 diverse data sources. Ratings are for two
hypothetical machine embodiments: a futuristic humanoid robot and a digital
assistant. We find that for some data-sources commonly used to train dialog
systems, 20-30% of utterances are not viewed as possible for a machine. Rating
is marginally affected by machine embodiment. We explore qualitative and
quantitative reasons for these ratings. Finally, we build classifiers and
explore how modeling configuration might affect output permissibly, and discuss
implications for building less falsely anthropomorphic dialog systems.",https://github.com/DNGros/Robots-Dont-Cry,-1
Hybrid Multimodal Fusion for Humor Detection,0.0982724,"In this paper, we present our solution to the MuSe-Humor sub-challenge of the
Multimodal Emotional Challenge (MuSe) 2022. The goal of the MuSe-Humor
sub-challenge is to detect humor and calculate AUC from audiovisual recordings
of German football Bundesliga press conferences. It is annotated for humor
displayed by the coaches. For this sub-challenge, we first build a discriminant
model using the transformer module and BiLSTM module, and then propose a hybrid
fusion strategy to use the prediction results of each modality to improve the
performance of the model. Our experiments demonstrate the effectiveness of our
proposed model and hybrid fusion strategy on multimodal fusion, and the AUC of
our proposed model on the test set is 0.8972.",None,-1
Instance-specific and Model-adaptive Supervision for Semi-supervised Semantic Segmentation,0.150523,"Recently, semi-supervised semantic segmentation has achieved promising
performance with a small fraction of labeled data. However, most existing
studies treat all unlabeled data equally and barely consider the differences
and training difficulties among unlabeled instances. Differentiating unlabeled
instances can promote instance-specific supervision to adapt to the model's
evolution dynamically. In this paper, we emphasize the cruciality of instance
differences and propose an instance-specific and model-adaptive supervision for
semi-supervised semantic segmentation, named iMAS. Relying on the model's
performance, iMAS employs a class-weighted symmetric intersection-over-union to
evaluate quantitative hardness of each unlabeled instance and supervises the
training on unlabeled data in a model-adaptive manner. Specifically, iMAS
learns from unlabeled instances progressively by weighing their corresponding
consistency losses based on the evaluated hardness. Besides, iMAS dynamically
adjusts the augmentation for each instance such that the distortion degree of
augmented instances is adapted to the model's generalization capability across
the training course. Not integrating additional losses and training procedures,
iMAS can obtain remarkable performance gains against current state-of-the-art
approaches on segmentation benchmarks under different semi-supervised partition
protocols.",https://github.com/zhenzhao/iMAS,-1
Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling,0.0281999,"Recent studies in using deep reinforcement learning (DRL) to solve Job-shop
scheduling problems (JSSP) focus on construction heuristics. However, their
performance is still far from optimality, mainly because the underlying graph
representation scheme is unsuitable for modelling partial solutions at each
construction step. This paper proposes a novel DRL-guided improvement heuristic
for solving JSSP, where graph representation is employed to encode complete
solutions. We design a Graph Neural-Network-based representation scheme,
consisting of two modules to effectively capture the information of dynamic
topology and different types of nodes in graphs encountered during the
improvement process. To speed up solution evaluation during improvement, we
present a novel message-passing mechanism that can evaluate multiple solutions
simultaneously. We prove that the computational complexity of our method scales
linearly with problem size. Experiments on classic benchmarks show that the
improvement policy learned by our method outperforms state-of-the-art DRL-based
methods by a large margin.",https://github.com/zcaicaros/L2S,-1
Temporal Attention for Language Models,0.148747,"Pretrained language models based on the transformer architecture have shown
great success in NLP. Textual training data often comes from the web and is
thus tagged with time-specific information, but most language models ignore
this information. They are trained on the textual data alone, limiting their
ability to generalize temporally. In this work, we extend the key component of
the transformer architecture, i.e., the self-attention mechanism, and propose
temporal attention - a time-aware self-attention mechanism. Temporal attention
can be applied to any transformer model and requires the input texts to be
accompanied with their relevant time points. It allows the transformer to
capture this temporal information and create time-specific contextualized word
representations. We leverage these representations for the task of semantic
change detection; we apply our proposed mechanism to BERT and experiment on
three datasets in different languages (English, German, and Latin) that also
vary in time, size, and genre. Our proposed model achieves state-of-the-art
results on all the datasets.",https://github.com/guyrosin/temporal_attention,-1
Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation and Instance Generation,0.127394,"We study the problem of few-shot Fine-grained Entity Typing (FET), where only
a few annotated entity mentions with contexts are given for each entity type.
Recently, prompt-based tuning has demonstrated superior performance to standard
fine-tuning in few-shot scenarios by formulating the entity type classification
task as a ''fill-in-the-blank'' problem. This allows effective utilization of
the strong language modeling capability of Pre-trained Language Models (PLMs).
Despite the success of current prompt-based tuning approaches, two major
challenges remain: (1) the verbalizer in prompts is either manually designed or
constructed from external knowledge bases, without considering the target
corpus and label hierarchy information, and (2) current approaches mainly
utilize the representation power of PLMs, but have not explored their
generation power acquired through extensive general-domain pre-training. In
this work, we propose a novel framework for few-shot FET consisting of two
modules: (1) an entity type label interpretation module automatically learns to
relate type labels to the vocabulary by jointly leveraging few-shot instances
and the label hierarchy, and (2) a type-based contextualized instance generator
produces new instances based on given instances to enlarge the training set for
better generalization. On three benchmark datasets, our model outperforms
existing methods by significant margins. Code can be found at
https://github.com/teapot123/Fine-Grained-Entity-Typing.",https://github.com/teapot123/Fine-Grained-Entity-Typing,-1
Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP,0.057532,"Textual adversarial samples play important roles in multiple subfields of NLP
research, including security, evaluation, explainability, and data
augmentation. However, most work mixes all these roles, obscuring the problem
definitions and research goals of the security role that aims to reveal the
practical concerns of NLP models. In this paper, we rethink the research
paradigm of textual adversarial samples in security scenarios. We discuss the
deficiencies in previous work and propose our suggestions that the research on
the Security-oriented adversarial NLP (SoadNLP) should: (1) evaluate their
methods on security tasks to demonstrate the real-world concerns; (2) consider
real-world attackers' goals, instead of developing impractical methods. To this
end, we first collect, process, and release a security datasets collection
Advbench. Then, we reformalize the task and adjust the emphasis on different
goals in SoadNLP. Next, we propose a simple method based on heuristic rules
that can easily fulfill the actual adversarial goals to simulate real-world
attack methods. We conduct experiments on both the attack and the defense sides
on Advbench. Experimental results show that our method has higher practical
value, indicating that the research paradigm in SoadNLP may start from our new
benchmark. All the code and data of Advbench can be obtained at
\url{https://github.com/thunlp/Advbench}.",https://github.com/thunlp/Advbench,-1
Tractable Boolean and Arithmetic Circuits,0.0596396,"Tractable Boolean and arithmetic circuits have been studied extensively in AI
for over two decades now. These circuits were initially proposed as ""compiled
objects,"" meant to facilitate logical and probabilistic reasoning, as they
permit various types of inference to be performed in linear-time and a
feed-forward fashion like neural networks. In more recent years, the role of
tractable circuits has significantly expanded as they became a computational
and semantical backbone for some approaches that aim to integrate knowledge,
reasoning and learning. In this article, we review the foundations of tractable
circuits and some associated milestones, while focusing on their core
properties and techniques that make them particularly useful for the broad aims
of neuro-symbolic AI.",None,-1
PalGAN: Image Colorization with Palette Generative Adversarial Networks,0.0383625,"Multimodal ambiguity and color bleeding remain challenging in colorization.
To tackle these problems, we propose a new GAN-based colorization approach
PalGAN, integrated with palette estimation and chromatic attention. To
circumvent the multimodality issue, we present a new colorization formulation
that estimates a probabilistic palette from the input gray image first, then
conducts color assignment conditioned on the palette through a generative
model. Further, we handle color bleeding with chromatic attention. It studies
color affinities by considering both semantic and intensity correlation. In
extensive experiments, PalGAN outperforms state-of-the-arts in quantitative
evaluation and visual comparison, delivering notable diverse, contrastive, and
edge-preserving appearances. With the palette design, our method enables color
transfer between images even with irrelevant contexts.",https://github.com/shepnerd/PalGAN,-1
Multi-Attribute Open Set Recognition,0.134951,"Open Set Recognition (OSR) extends image classification to an open-world
setting, by simultaneously classifying known classes and identifying unknown
ones. While conventional OSR approaches can detect Out-of-Distribution (OOD)
samples, they cannot provide explanations indicating which underlying visual
attribute(s) (e.g., shape, color or background) cause a specific sample to be
unknown. In this work, we introduce a novel problem setup that generalizes
conventional OSR to a multi-attribute setting, where multiple visual attributes
are simultaneously recognized. Here, OOD samples can be not only identified but
also categorized by their unknown attribute(s). We propose simple extensions of
common OSR baselines to handle this novel scenario. We show that these
baselines are vulnerable to shortcuts when spurious correlations exist in the
training dataset. This leads to poor OOD performance which, according to our
experiments, is mainly due to unintended cross-attribute correlations of the
predicted confidence scores. We provide an empirical evidence showing that this
behavior is consistent across different baselines on both synthetic and real
world datasets.",None,-1
Explainability of Predictive Process Monitoring Results: Can You See My Data Issues?,0.0679808,"Predictive business process monitoring (PPM) has been around for several
years as a use case of process mining. PPM enables foreseeing the future of a
business process through predicting relevant information about how a running
process instance might end, related performance indicators, and other
predictable aspects. A big share of PPM approaches adopts a Machine Learning
(ML) technique to address a prediction task, especially non-process-aware PPM
approaches. Consequently, PPM inherits the challenges faced by ML approaches.
One of these challenges concerns the need to gain user trust in the predictions
generated. The field of explainable artificial intelligence (XAI) addresses
this issue. However, the choices made, and the techniques employed in a PPM
task, in addition to ML model characteristics, influence resulting
explanations. A comparison of the influence of different settings on the
generated explanations is missing. To address this gap, we investigate the
effect of different PPM settings on resulting data fed into an ML model and
consequently to a XAI method. We study how differences in resulting
explanations may indicate several issues in underlying data. We construct a
framework for our experiments including different settings at each stage of PPM
with XAI integrated as a fundamental part. Our experiments reveal several
inconsistencies, as well as agreements, between data characteristics (and hence
expectations about these data), important data used by the ML model as a result
of querying it, and explanations of predictions of the investigated ML model.",https://github.com/GhadaElkhawaga/PPM_XAI_Comparison.git,-1
Dual-Geometric Space Embedding Model for Two-View Knowledge Graphs,0.21976,"Two-view knowledge graphs (KGs) jointly represent two components: an ontology
view for abstract and commonsense concepts, and an instance view for specific
entities that are instantiated from ontological concepts. As such, these KGs
contain heterogeneous structures that are hierarchical, from the ontology-view,
and cyclical, from the instance-view. Despite these various structures in KGs,
most recent works on embedding KGs assume that the entire KG belongs to only
one of the two views but not both simultaneously. For works that seek to put
both views of the KG together, the instance and ontology views are assumed to
belong to the same geometric space, such as all nodes embedded in the same
Euclidean space or non-Euclidean product space, an assumption no longer
reasonable for two-view KGs where different portions of the graph exhibit
different structures. To address this issue, we define and construct a
dual-geometric space embedding model (DGS) that models two-view KGs using a
complex non-Euclidean geometric space, by embedding different portions of the
KG in different geometric spaces. DGS utilizes the spherical space, hyperbolic
space, and their intersecting space in a unified framework for learning
embeddings. Furthermore, for the spherical space, we propose novel closed
spherical space operators that directly operate in the spherical space without
the need for mapping to an approximate tangent space. Experiments on public
datasets show that DGS significantly outperforms previous state-of-the-art
baseline models on KG completion tasks, demonstrating its ability to better
model heterogeneous structures in KGs.",https://github.com/roshnigiyer/dgs,-1
Polysemanticity and Capacity in Neural Networks,0.933014,"Individual neurons in neural networks often represent a mixture of unrelated
features. This phenomenon, called polysemanticity, can make interpreting neural
networks more difficult and so we aim to understand its causes. We propose
doing so through the lens of feature \emph{capacity}, which is the fractional
dimension each feature consumes in the embedding space. We show that in a toy
model the optimal capacity allocation tends to monosemantically represent the
most important features, polysemantically represent less important features (in
proportion to their impact on the loss), and entirely ignore the least
important features. Polysemanticity is more prevalent when the inputs have
higher kurtosis or sparsity and more prevalent in some architectures than
others. Given an optimal allocation of capacity, we go on to study the geometry
of the embedding space. We find a block-semi-orthogonal structure, with
differing block sizes in different models, highlighting the impact of model
architecture on the interpretability of its neurons.",None,-1
Dynamic Local Aggregation Network with Adaptive Clusterer for Anomaly Detection,0.0599841,"Existing methods for anomaly detection based on memory-augmented autoencoder
(AE) have the following drawbacks: (1) Establishing a memory bank requires
additional memory space. (2) The fixed number of prototypes from subjective
assumptions ignores the data feature differences and diversity. To overcome
these drawbacks, we introduce DLAN-AC, a Dynamic Local Aggregation Network with
Adaptive Clusterer, for anomaly detection. First, The proposed DLAN can
automatically learn and aggregate high-level features from the AE to obtain
more representative prototypes, while freeing up extra memory space. Second,
The proposed AC can adaptively cluster video data to derive initial prototypes
with prior information. In addition, we also propose a dynamic redundant
clustering strategy (DRCS) to enable DLAN for automatically eliminating feature
clusters that do not contribute to the construction of prototypes. Extensive
experiments on benchmarks demonstrate that DLAN-AC outperforms most existing
methods, validating the effectiveness of our method. Our code is publicly
available at https://github.com/Beyond-Zw/DLAN-AC.",https://github.com/Beyond-Zw/DLAN-AC,-1
Check and Link: Pairwise Lesion Correspondence Guides Mammogram Mass Detection,0.118631,"Detecting mass in mammogram is significant due to the high occurrence and
mortality of breast cancer. In mammogram mass detection, modeling pairwise
lesion correspondence explicitly is particularly important. However, most of
the existing methods build relatively coarse correspondence and have not
utilized correspondence supervision. In this paper, we propose a new
transformer-based framework CL-Net to learn lesion detection and pairwise
correspondence in an end-to-end manner. In CL-Net, View-Interactive Lesion
Detector is proposed to achieve dynamic interaction across candidates of cross
views, while Lesion Linker employs the correspondence supervision to guide the
interaction process more accurately. The combination of these two designs
accomplishes precise understanding of pairwise lesion correspondence for
mammograms. Experiments show that CL-Net yields state-of-the-art performance on
the public DDSM dataset and our in-house dataset. Moreover, it outperforms
previous methods by a large margin in low FPI regime.",None,-1
DEER: Descriptive Knowledge Graph for Explaining Entity Relationships,0.115866,"We propose DEER (Descriptive Knowledge Graph for Explaining Entity
Relationships) - an open and informative form of modeling entity relationships.
In DEER, relationships between entities are represented by free-text relation
descriptions. For instance, the relationship between entities of machine
learning and algorithm can be represented as ``Machine learning explores the
study and construction of algorithms that can learn from and make predictions
on data.'' To construct DEER, we propose a self-supervised learning method to
extract relation descriptions with the analysis of dependency patterns and
generate relation descriptions with a transformer-based relation description
synthesizing model, where no human labeling is required. Experiments
demonstrate that our system can extract and generate high-quality relation
descriptions for explaining entity relationships. The results suggest that we
can build an open and informative knowledge graph without human annotation.",https://github.com/jeffhj/DEER,-1
Testing predictive automated driving systems: lessons learned and future recommendations,0.01095,"Conventional vehicles are certified through classical approaches, where
different physical certification tests are set up on test tracks to assess
required safety levels. These approaches are well suited for vehicles with
limited complexity and limited interactions with other entities as last-second
resources. However, these approaches do not allow to evaluate safety with real
behaviors for critical and edge cases, nor to evaluate the ability to
anticipate them in the mid or long term. This is particularly relevant for
automated and autonomous driving functions that make use of advanced predictive
systems to anticipate future actions and motions to be considered in the path
planning layer. In this paper, we present and analyze the results of physical
tests on proving grounds of several predictive systems in automated driving
functions developed within the framework of the BRAVE project. Based on our
experience in testing predictive automated driving functions, we identify the
main limitations of current physical testing approaches when dealing with
predictive systems, analyze the main challenges ahead, and provide a set of
practical actions and recommendations to consider in future physical testing
procedures for automated and autonomous driving functions.",None,-1
A Fast Post-Training Pruning Framework for Transformers,0.579286,"Pruning is an effective way to reduce the huge inference cost of Transformer
models. However, prior work on pruning Transformers requires retraining the
models. This can add high training cost and high complexity to model
deployment, making it difficult to use in many practical situations. To address
this, we propose a fast post-training pruning framework for Transformers that
does not require any retraining. Given a resource constraint and a sample
dataset, our framework automatically prunes the Transformer model using
structured sparsity methods. To retain high accuracy without retraining, we
introduce three novel techniques: (i) a lightweight mask search algorithm that
finds which heads and filters to prune based on the Fisher information; (ii)
mask rearrangement that complements the search algorithm; and (iii) mask tuning
that reconstructs the output activations for each layer. We apply our method to
BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD
benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x
speedup in inference latency, while maintaining < 1% loss in accuracy.
Importantly, our framework prunes Transformers in less than 3 minutes on a
single GPU, which is over two orders of magnitude faster than existing pruning
approaches that retrain the models.",https://github.com/WoosukKwon/retraining-free-pruning,-1
Read before Generate! Faithful Long Form Question Answering with Machine Reading,0.175469,"Long-form question answering (LFQA) aims to generate a paragraph-length
answer for a given question. While current work on LFQA using large pre-trained
model for generation are effective at producing fluent and somewhat relevant
content, one primary challenge lies in how to generate a faithful answer that
has less hallucinated content. We propose a new end-to-end framework that
jointly models answer generation and machine reading. The key idea is to
augment the generation model with fine-grained, answer-related salient
information which can be viewed as an emphasis on faithful facts.
State-of-the-art results on two LFQA datasets, ELI5 and MS MARCO, demonstrate
the effectiveness of our method, in comparison with strong baselines on
automatic and human evaluation metrics. A detailed analysis further proves the
competency of our methods in generating fluent, relevant, and more faithful
answers.",https://github.com/facebookresearch/faiss,-1
g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin,0.0553972,"Polyphone disambiguation is the most crucial task in Mandarin
grapheme-to-phoneme (g2p) conversion. Previous studies have approached this
problem using pre-trained language models, restricted output, and extra
information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we
propose a novel approach, called g2pW, which adapts learnable softmax-weights
to condition the outputs of BERT with the polyphonic character of interest and
its POS tagging. Rather than using the hard mask as in previous works, our
experiments show that learning a soft-weighting function for the candidate
phonemes benefits performance. In addition, our proposed g2pW does not require
extra pre-trained POS tagging models while using POS tags as auxiliary features
since we train the POS tagging model simultaneously with the unified encoder.
Experimental results show that our g2pW outperforms existing methods on the
public CPP dataset. All codes, model weights, and a user-friendly package are
publicly available.",https://github.com/GitYCC/g2pW,-1
CD Tools -- Condensed Detachment and Structure Generating Theorem Proving (System Description),0.0299554,"CD Tools is a Prolog library for experimenting with condensed detachment in
first-order ATP, which puts a recent formal view centered around proof
structures into practice. From the viewpoint of first-order ATP, condensed
detachment offers a setting that is relatively simple but with essential
features and serious applications, making it attractive as a basis for
developing and evaluating novel techniques. CD Tools includes specialized
provers based on the enumeration of proof structures. We focus here on one of
these, SGCD, which permits to blend goal- and axiom-driven proof search in
particularly flexible ways. In purely goal-driven configurations it acts
similarly to a prover of the clausal tableaux or connection method family. In
blended configurations its performance is much stronger, close to
state-of-the-art provers, while emitting relatively short proofs. Experiments
show characteristics and application possibilities of the structure generating
approach realized by that prover. For a historic problem often studied in ATP
it produced a new proof that is much shorter than any known one.",None,-1
BLIND: Bias Removal With No Demographics,0.0,"Models trained on real-world data tend to imitate and amplify social biases.
Common methods to mitigate biases require prior information on the types of
biases that should be mitigated (e.g., gender or racial bias) and the social
groups associated with each data sample. In this work, we introduce BLIND, a
method for bias removal with no prior knowledge of the demographics in the
dataset. While training a model on a downstream task, BLIND detects biased
samples using an auxiliary model that predicts the main model's success, and
down-weights those samples during the training process. Experiments with racial
and gender biases in sentiment classification and occupation classification
tasks demonstrate that BLIND mitigates social biases without relying on a
costly demographic annotation process. Our method is competitive with other
methods that require demographic information and sometimes even surpasses them.",https://github.com/technion-cs-nlp/BLIND,-1
Hierarchical Multi-Label Classification of Scientific Documents,0.208434,"Automatic topic classification has been studied extensively to assist
managing and indexing scientific documents in a digital collection. With the
large number of topics being available in recent years, it has become necessary
to arrange them in a hierarchy. Therefore, the automatic classification systems
need to be able to classify the documents hierarchically. In addition, each
paper is often assigned to more than one relevant topic. For example, a paper
can be assigned to several topics in a hierarchy tree. In this paper, we
introduce a new dataset for hierarchical multi-label text classification
(HMLTC) of scientific papers called SciHTC, which contains 186,160 papers and
1,233 categories from the ACM CCS tree. We establish strong baselines for HMLTC
and propose a multi-task learning approach for topic classification with
keyword labeling as an auxiliary task. Our best model achieves a Macro-F1 score
of 34.57% which shows that this dataset provides significant research
opportunities on hierarchical scientific topic classification. We make our
dataset and code available on Github.",https://github.com/msadat3/SciHTC,-1
Accelerating Shapley Explanation via Contributive Cooperator Selection,0.323573,"Even though Shapley value provides an effective explanation for a DNN model
prediction, the computation relies on the enumeration of all possible input
feature coalitions, which leads to the exponentially growing complexity. To
address this problem, we propose a novel method SHEAR to significantly
accelerate the Shapley explanation for DNN models, where only a few coalitions
of input features are involved in the computation. The selection of the feature
coalitions follows our proposed Shapley chain rule to minimize the absolute
error from the ground-truth Shapley values, such that the computation can be
both efficient and accurate. To demonstrate the effectiveness, we
comprehensively evaluate SHEAR across multiple metrics including the absolute
error from the ground-truth Shapley value, the faithfulness of the
explanations, and running speed. The experimental results indicate SHEAR
consistently outperforms state-of-the-art baseline methods across different
evaluation metrics, which demonstrates its potentials in real-world
applications where the computational resource is limited.",https://github.com/guanchuwang/SHEAR,-1
Composing RNNs and FSTs for Small Data: Recovering Missing Characters in Old Hawaiian Text,0.144488,"In contrast to the older writing system of the 19th century, modern Hawaiian
orthography employs characters for long vowels and glottal stops. These extra
characters account for about one-third of the phonemes in Hawaiian, so
including them makes a big difference to reading comprehension and
pronunciation. However, transliterating between older and newer texts is a
laborious task when performed manually. We introduce two related methods to
help solve this transliteration problem automatically, given that there were
not enough data to train an end-to-end deep learning model. One method is
implemented, end-to-end, using finite state transducers (FSTs). The other is a
hybrid deep learning approach which approximately composes an FST with a
recurrent neural network (RNN). We find that the hybrid approach outperforms
the end-to-end FST by partitioning the original problem into one part that can
be modelled by hand, using an FST, and into another part, which is easily
solved by an RNN trained on the available data.",None,-1
Self-conditioned Embedding Diffusion for Text Generation,0.643317,"Can continuous diffusion models bring the same performance breakthrough on
natural language they did for image generation? To circumvent the discrete
nature of text data, we can simply project tokens in a continuous space of
embeddings, as is standard in language modeling. We propose Self-conditioned
Embedding Diffusion, a continuous diffusion mechanism that operates on token
embeddings and allows to learn flexible and scalable diffusion models for both
conditional and unconditional text generation. Through qualitative and
quantitative evaluation, we show that our text diffusion models generate
samples comparable with those produced by standard autoregressive language
models - while being in theory more efficient on accelerator hardware at
inference time. Our work paves the way for scaling up diffusion models for
text, similarly to autoregressive models, and for improving performance with
recent refinements to continuous diffusion.",None,-1
Amodal Panoptic Segmentation,0.544673,"Humans have the remarkable ability to perceive objects as a whole, even when
parts of them are occluded. This ability of amodal perception forms the basis
of our perceptual and cognitive understanding of our world. To enable robots to
reason with this capability, we formulate and propose a novel task that we name
amodal panoptic segmentation. The goal of this task is to simultaneously
predict the pixel-wise semantic segmentation labels of the visible regions of
stuff classes and the instance segmentation labels of both the visible and
occluded regions of thing classes. To facilitate research on this new task, we
extend two established benchmark datasets with pixel-level amodal panoptic
segmentation labels that we make publicly available as KITTI-360-APS and
BDD100K-APS. We present several strong baselines, along with the amodal
panoptic quality (APQ) and amodal parsing coverage (APC) metrics to quantify
the performance in an interpretable manner. Furthermore, we propose the novel
amodal panoptic segmentation network (APSNet), as a first step towards
addressing this task by explicitly modeling the complex relationships between
the occluders and occludes. Extensive experimental evaluations demonstrate that
APSNet achieves state-of-the-art performance on both benchmarks and more
importantly exemplifies the utility of amodal recognition. The benchmarks are
available at http://amodal-panoptic.cs.uni-freiburg.de.",http://amodal-panoptic.cs.uni-freiburg.de,-1
Domain Generalization Strategy to Train Classifiers Robust to Spatial-Temporal Shift,0.0221615,"Deep learning-based weather prediction models have advanced significantly in
recent years. However, data-driven models based on deep learning are difficult
to apply to real-world applications because they are vulnerable to
spatial-temporal shifts. A weather prediction task is especially susceptible to
spatial-temporal shifts when the model is overfitted to locality and
seasonality. In this paper, we propose a training strategy to make the weather
prediction model robust to spatial-temporal shifts. We first analyze the effect
of hyperparameters and augmentations of the existing training strategy on the
spatial-temporal shift robustness of the model. Next, we propose an optimal
combination of hyperparameters and augmentation based on the analysis results
and a test-time augmentation. We performed all experiments on the W4C22
Transfer dataset and achieved the 1st performance.",https://github.com/seominseok0429/W4C22-Simple-Baseline-for-Weather-Forecasting-Using-Spatiotemporal-Context-Aggregation-Network,-1
Learning to Estimate Shapley Values with Vision Transformers,0.247802,"Transformers have become a default architecture in computer vision, but
understanding what drives their predictions remains a challenging problem.
Current explanation approaches rely on attention values or input gradients, but
these provide a limited view of a model's dependencies. Shapley values offer a
theoretically sound alternative, but their computational cost makes them
impractical for large, high-dimensional models. In this work, we aim to make
Shapley values practical for vision transformers (ViTs). To do so, we first
leverage an attention masking approach to evaluate ViTs with partial
information, and we then develop a procedure to generate Shapley value
explanations via a separate, learned explainer model. Our experiments compare
Shapley values to many baseline methods (e.g., attention rollout, GradCAM,
LRP), and we find that our approach provides more accurate explanations than
existing methods for ViTs.",https://github.com/suinleelab/vit-shapley,-1
Training Debiased Subnetworks with Contrastive Weight Pruning,0.0511866,"Neural networks are often biased to spuriously correlated features that
provide misleading statistical evidence that does not generalize. This raises
an interesting question: ``Does an optimal unbiased functional subnetwork exist
in a severely biased network? If so, how to extract such subnetwork?"" While
empirical evidence has been accumulated about the existence of such unbiased
subnetworks, these observations are mainly based on the guidance of
ground-truth unbiased samples. Thus, it is unexplored how to discover the
optimal subnetworks with biased training datasets in practice. To address this,
here we first present our theoretical insight that alerts potential limitations
of existing algorithms in exploring unbiased subnetworks in the presence of
strong spurious correlations. We then further elucidate the importance of
bias-conflicting samples on structure learning. Motivated by these
observations, we propose a Debiased Contrastive Weight Pruning (DCWP)
algorithm, which probes unbiased subnetworks without expensive group
annotations. Experimental results demonstrate that our approach significantly
outperforms state-of-the-art debiasing methods despite its considerable
reduction in the number of parameters.",None,-1
Uncertainty estimation for Cross-dataset performance in Trajectory prediction,0.040445,"While a lot of work has been carried on developing trajectory prediction
methods, and various datasets have been proposed for benchmarking this task,
little study has been done so far on the generalizability and the
transferability of these methods across dataset. In this paper, we observe the
performance of two of the latest state-of-the-art trajectory prediction methods
across four different datasets (Argoverse, NuScenes, Interaction, Shifts). This
analysis allows to gain some insights on the generalizability proprieties of
most recent trajectory prediction models and to analyze which dataset is more
representative of real driving scenes and therefore enables better
transferability. Furthermore we present a novel method to estimate prediction
uncertainty and show how it could be used to achieve better performance across
datasets.",None,-1
Structured Local Radiance Fields for Human Avatar Modeling,0.38099,"It is extremely challenging to create an animatable clothed human avatar from
RGB videos, especially for loose clothes due to the difficulties in motion
modeling. To address this problem, we introduce a novel representation on the
basis of recent neural scene rendering techniques. The core of our
representation is a set of structured local radiance fields, which are anchored
to the pre-defined nodes sampled on a statistical human body template. These
local radiance fields not only leverage the flexibility of implicit
representation in shape and appearance modeling, but also factorize cloth
deformations into skeleton motions, node residual translations and the dynamic
detail variations inside each individual radiance field. To learn our
representation from RGB data and facilitate pose generalization, we propose to
learn the node translations and the detail variations in a conditional
generative latent space. Overall, our method enables automatic construction of
animatable human avatars for various types of clothes without the need for
scanning subject-specific templates, and can generate realistic images with
dynamic details for novel poses. Experiment show that our method outperforms
state-of-the-art methods both qualitatively and quantitatively.",https://github.com/zju3dv/EasyMocap,-1
AAM-Gym: Artificial Intelligence Testbed for Advanced Air Mobility,0.0752258,"We introduce AAM-Gym, a research and development testbed for Advanced Air
Mobility (AAM). AAM has the potential to revolutionize travel by reducing
ground traffic and emissions by leveraging new types of aircraft such as
electric vertical take-off and landing (eVTOL) aircraft and new advanced
artificial intelligence (AI) algorithms. Validation of AI algorithms require
representative AAM scenarios, as well as a fast time simulation testbed to
evaluate their performance. Until now, there has been no such testbed available
for AAM to enable a common research platform for individuals in government,
industry, or academia. MIT Lincoln Laboratory has developed AAM-Gym to address
this gap by providing an ecosystem to develop, train, and validate new and
established AI algorithms across a wide variety of AAM use-cases. In this
paper, we use AAM-Gym to study the performance of two reinforcement learning
algorithms on an AAM use-case, separation assurance in AAM corridors. The
performance of the two algorithms is demonstrated based on a series of metrics
provided by AAM-Gym, showing the testbed's utility to AAM research.",None,-1
Text-Only Training for Image Captioning using Noise-Injected CLIP,0.587654,"We consider the task of image-captioning using only the CLIP model and
additional text data at training time, and no additional captioned images. Our
approach relies on the fact that CLIP is trained to make visual and textual
embeddings similar. Therefore, we only need to learn how to translate CLIP
textual embeddings back into text, and we can learn how to do this by learning
a decoder for the frozen CLIP text encoder using only text. We argue that this
intuition is ""almost correct"" because of a gap between the embedding spaces,
and propose to rectify this via noise injection during training. We demonstrate
the effectiveness of our approach by showing SOTA zero-shot image captioning
across four benchmarks, including style transfer. Code, data, and models are
available on GitHub.",https://github.com/DavidHuji/CapDec,-1
LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension,0.864665,"The application of Natural Language Processing (NLP) to specialized domains,
such as the law, has recently received a surge of interest. As many legal
services rely on processing and analyzing large collections of documents,
automating such tasks with NLP tools emerges as a key challenge. Many popular
language models, such as BERT or RoBERTa, are general-purpose models, which
have limitations on processing specialized legal terminology and syntax. In
addition, legal documents may contain specialized vocabulary from other
domains, such as medical terminology in personal injury text. Here, we propose
LegalRelectra, a legal-domain language model that is trained on mixed-domain
legal and medical corpora. We show that our model improves over general-domain
and single-domain medical and legal language models when processing
mixed-domain (personal injury) text. Our training architecture implements the
Electra framework, but utilizes Reformer instead of BERT for its generator and
discriminator. We show that this improves the model's performance on processing
long passages and results in better long-range text comprehension.",None,-1
"Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",0.0759378,"As natural language processing systems become more widespread, it is
necessary to address fairness issues in their implementation and deployment to
ensure that their negative impacts on society are understood and minimized.
However, there is limited work that studies fairness using a multilingual and
intersectional framework or on downstream tasks. In this paper, we introduce
four multilingual Equity Evaluation Corpora, supplementary test sets designed
to measure social biases, and a novel statistical framework for studying
unisectional and intersectional social biases in natural language processing.
We use these tools to measure gender, racial, ethnic, and intersectional social
biases across five models trained on emotion regression tasks in English,
Spanish, and Arabic. We find that many systems demonstrate statistically
significant unisectional and intersectional social biases.",https://github.com/ascamara/,-1
Learning Disentangled Textual Representations via Statistical Measures of Similarity,0.136846,"When working with textual data, a natural application of disentangled
representations is fair classification where the goal is to make predictions
without being biased (or influenced) by sensitive attributes that may be
present in the data (e.g., age, gender or race). Dominant approaches to
disentangle a sensitive attribute from textual representations rely on learning
simultaneously a penalization term that involves either an adversarial loss
(e.g., a discriminator) or an information measure (e.g., mutual information).
However, these methods require the training of a deep neural network with
several parameter updates for each update of the representation model. As a
matter of fact, the resulting nested optimization loop is both time consuming,
adding complexity to the optimization dynamic, and requires a fine
hyperparameter selection (e.g., learning rates, architecture). In this work, we
introduce a family of regularizers for learning disentangled representations
that do not require training. These regularizers are based on statistical
measures of similarity between the conditional probability distributions with
respect to the sensitive attributes. Our novel regularizers do not require
additional training, are faster and do not involve additional tuning while
achieving better results both when combined with pretrained and randomly
initialized text encoders.",https://github.com/PierreColombo/TORNADO,-1
BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning,0.0283117,"Current pre-trained language models rely on large datasets for achieving
state-of-the-art performance. However, past research has shown that not all
examples in a dataset are equally important during training. In fact, it is
sometimes possible to prune a considerable fraction of the training set while
maintaining the test performance. Established on standard vision benchmarks,
two gradient-based scoring metrics for finding important examples are GraNd and
its estimated version, EL2N. In this work, we employ these two metrics for the
first time in NLP. We demonstrate that these metrics need to be computed after
at least one epoch of fine-tuning and they are not reliable in early steps.
Furthermore, we show that by pruning a small portion of the examples with the
highest GraNd/EL2N scores, we can not only preserve the test accuracy, but also
surpass it. This paper details adjustments and implementation choices which
enable GraNd and EL2N to be applied to NLP.",None,-1
SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval,0.440908,"Sampling proper negatives from a large document pool is vital to effectively
train a dense retrieval model. However, existing negative sampling strategies
suffer from the uninformative or false negative problem. In this work, we
empirically show that according to the measured relevance scores, the negatives
ranked around the positives are generally more informative and less likely to
be false negatives. Intuitively, these negatives are not too hard (\emph{may be
false negatives}) or too easy (\emph{uninformative}). They are the ambiguous
negatives and need more attention during training. Thus, we propose a simple
ambiguous negatives sampling method, SimANS, which incorporates a new sampling
probability distribution to sample more ambiguous negatives. Extensive
experiments on four public and one industry datasets show the effectiveness of
our approach. We made the code and models publicly available in
\url{https://github.com/microsoft/SimXNS}.",https://github.com/microsoft/SimXNS,-1
"When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues",0.875936,"Indirect speech such as sarcasm achieves a constellation of discourse goals
in human communication. While the indirectness of figurative language warrants
speakers to achieve certain pragmatic goals, it is challenging for AI agents to
comprehend such idiosyncrasies of human communication. Though sarcasm
identification has been a well-explored topic in dialogue analysis, for
conversational systems to truly grasp a conversation's innate meaning and
generate appropriate responses, simply detecting sarcasm is not enough; it is
vital to explain its underlying sarcastic connotation to capture its true
essence. In this work, we study the discourse structure of sarcastic
conversations and propose a novel task - Sarcasm Explanation in Dialogue (SED).
Set in a multimodal and code-mixed setting, the task aims to generate natural
language explanations of satirical conversations. To this end, we curate WITS,
a new dataset to support our task. We propose MAF (Modality Aware Fusion), a
multimodal context-aware attention and global information fusion module to
capture multimodality and use it to benchmark WITS. The proposed attention
module surpasses the traditional multimodal fusion baselines and reports the
best performance on almost all metrics. Lastly, we carry out detailed analyses
both quantitatively and qualitatively.",https://github.com/LCS2-IIITD/MAF.git,-1
Social Choice Around the Block: On the Computational Social Choice of Blockchain,0.0,"One of the most innovative aspects of blockchain technology consists in the
introduction of an incentive layer to regulate the behavior of distributed
protocols. The designer of a blockchain system faces therefore issues that are
akin to those relevant for the design of economic mechanisms, and faces them in
a computational setting. From this perspective the present paper argues for the
importance of computational social choice in blockchain research. It identifies
a few challenges at the interface of the two fields that illustrate the strong
potential for cross-fertilization between them.",None,-1
LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection,0.436745,"Visual question answering (VQA) often requires an understanding of visual
concepts and language semantics, which relies on external knowledge. Most
existing methods exploit pre-trained language models or/and unstructured text,
but the knowledge in these resources are often incomplete and noisy. Some other
methods prefer to use knowledge graphs (KGs) which often have intensive
structured knowledge, but the research is still quite preliminary. In this
paper, we propose LaKo, a knowledge-driven VQA method via Late
Knowledge-to-text Injection. To effectively incorporate an external KG, we
transfer triples into textual format and propose a late injection mechanism for
knowledge fusion. Finally we address VQA as a text generation task with an
effective encoder-decoder paradigm, which achieves state-of-the-art results on
OKVQA dataset.",https://github.com/hackerchenzhuo/LaKo,-1
Radial Basis Function Networks for Convolutional Neural Networks to Learn Similarity Distance Metric and Improve Interpretability,0.0764379,"Radial basis function neural networks (RBFs) are prime candidates for pattern
classification and regression and have been used extensively in classical
machine learning applications. However, RBFs have not been integrated into
contemporary deep learning research and computer vision using conventional
convolutional neural networks (CNNs) due to their lack of adaptability with
modern architectures. In this paper, we adapt RBF networks as a classifier on
top of CNNs by modifying the training process and introducing a new activation
function to train modern vision architectures end-to-end for image
classification. The specific architecture of RBFs enables the learning of a
similarity distance metric to compare and find similar and dissimilar images.
Furthermore, we demonstrate that using an RBF classifier on top of any CNN
architecture provides new human-interpretable insights about the
decision-making process of the models. Finally, we successfully apply RBFs to a
range of CNN architectures and evaluate the results on benchmark computer
vision datasets.",None,-1
ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs,0.076371,"As Abstract Meaning Representation (AMR) implicitly involves compound
semantic annotations, we hypothesize auxiliary tasks which are semantically or
formally related can better enhance AMR parsing. We find that 1) Semantic role
labeling (SRL) and dependency parsing (DP), would bring more performance gain
than other tasks e.g. MT and summarization in the text-to-AMR transition even
with much less data. 2) To make a better fit for AMR, data from auxiliary tasks
should be properly ""AMRized"" to PseudoAMR before training. Knowledge from
shallow level parsing tasks can be better transferred to AMR Parsing with
structure transform. 3) Intermediate-task learning is a better paradigm to
introduce auxiliary tasks to AMR parsing, compared to multitask learning. From
an empirical perspective, we propose a principled method to involve auxiliary
tasks to boost AMR parsing. Extensive experiments show that our method achieves
new state-of-the-art performance on different benchmarks especially in
topology-related scores.",https://github.com/PKUnlp-icler/ATP,-1
Interpretable Hidden Markov Model-Based Deep Reinforcement Learning Hierarchical Framework for Predictive Maintenance of Turbofan Engines,0.0707783,"An open research question in deep reinforcement learning is how to focus the
policy learning of key decisions within a sparse domain. This paper emphasizes
combining the advantages of inputoutput hidden Markov models and reinforcement
learning towards interpretable maintenance decisions. We propose a novel
hierarchical-modeling methodology that, at a high level, detects and interprets
the root cause of a failure as well as the health degradation of the turbofan
engine, while, at a low level, it provides the optimal replacement policy. It
outperforms the baseline performance of deep reinforcement learning methods
applied directly to the raw data or when using a hidden Markov model without
such a specialized hierarchy. It also provides comparable performance to prior
work, however, with the additional benefit of interpretability.",None,-1
py-irt: A Scalable Item Response Theory Library for Python,0.0713391,"py-irt is a Python library for fitting Bayesian Item Response Theory (IRT)
models. py-irt estimates latent traits of subjects and items, making it
appropriate for use in IRT tasks as well as ideal-point models. py-irt is built
on top of the Pyro and PyTorch frameworks and uses GPU-accelerated training to
scale to large data sets. Code, documentation, and examples can be found at
https://github.com/nd-ball/py-irt. py-irt can be installed from the GitHub page
or the Python Package Index (PyPI).",https://github.com/nd-ball/py-irt,-1
When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment,0.709601,"AI systems are becoming increasingly intertwined with human life. In order to
effectively collaborate with humans and ensure safety, AI systems need to be
able to understand, interpret and predict human moral judgments and decisions.
Human moral judgments are often guided by rules, but not always. A central
challenge for AI safety is capturing the flexibility of the human moral mind --
the ability to determine when a rule should be broken, especially in novel or
unusual situations. In this paper, we present a novel challenge set consisting
of rule-breaking question answering (RBQA) of cases that involve potentially
permissible rule-breaking -- inspired by recent moral psychology studies. Using
a state-of-the-art large language model (LLM) as a basis, we propose a novel
moral chain of thought (MORALCOT) prompting strategy that combines the
strengths of LLMs with theories of moral reasoning developed in cognitive
science to predict human moral judgments. MORALCOT outperforms seven existing
LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to
capture the flexibility of the human moral mind. We also conduct a detailed
error analysis to suggest directions for future work to improve AI safety using
RBQA. Our data is open-sourced at
https://huggingface.co/datasets/feradauto/MoralExceptQA and code at
https://github.com/feradauto/MoralCoT",https://github.com/feradauto/MoralCoT,-1
Layout Aware Inpainting for Automated Furniture Removal in Indoor Scenes,0.0418575,"We address the problem of detecting and erasing furniture from a wide angle
photograph of a room. Inpainting large regions of an indoor scene often results
in geometric inconsistencies of background elements within the inpaint mask. To
address this problem, we utilize perceptual information (e.g. instance
segmentation, and room layout) to produce a geometrically consistent empty
version of a room. We share important details to make this system viable, such
as per-plane inpainting, automatic rectification, and texture refinement. We
provide detailed ablation along with qualitative examples, justifying our
design choices. We show an application of our system by removing real furniture
from a room and redecorating it with virtual furniture.",None,-1
Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification,0.0770757,"A key component of fact verification is thevevidence retrieval, often from
multiple documents. Recent approaches use dense representations and condition
the retrieval of each document on the previously retrieved ones. The latter
step is performed over all the documents in the collection, requiring storing
their dense representations in an index, thus incurring a high memory
footprint. An alternative paradigm is retrieve-and-rerank, where documents are
retrieved using methods such as BM25, their sentences are reranked, and further
documents are retrieved conditioned on these sentences, reducing the memory
requirements. However, such approaches can be brittle as they rely on
heuristics and assume hyperlinks between documents. We propose a novel
retrieve-and-rerank method for multi-hop retrieval, that consists of a
retriever that jointly scores documents in the knowledge source and sentences
from previously retrieved documents using an autoregressive formulation and is
guided by a proof system based on natural logic that dynamically terminates the
retrieval process if the evidence is deemed sufficient. This method is
competitive with current state-of-the-art methods on FEVER, HoVer and
FEVEROUS-S, while using $5$ to $10$ times less memory than competing systems.
Evaluation on an adversarial dataset indicates improved stability of our
approach compared to commonly deployed threshold-based methods. Finally, the
proof system helps humans predict model decisions correctly more often than
using the evidence alone.",https://github.com/Raldir/AdMIRaL,-1
Learning to Imitate Object Interactions from Internet Videos,0.0413538,"We study the problem of imitating object interactions from Internet videos.
This requires understanding the hand-object interactions in 4D, spatially in 3D
and over time, which is challenging due to mutual hand-object occlusions. In
this paper we make two main contributions: (1) a novel reconstruction technique
RHOV (Reconstructing Hands and Objects from Videos), which reconstructs 4D
trajectories of both the hand and the object using 2D image cues and temporal
smoothness constraints; (2) a system for imitating object interactions in a
physics simulator with reinforcement learning. We apply our reconstruction
technique to 100 challenging Internet videos. We further show that we can
successfully imitate a range of different object interactions in a physics
simulator. Our object-centric approach is not limited to human-like
end-effectors and can learn to imitate object interactions using different
embodiments, like a robotic arm with a parallel jaw gripper.",None,-1
CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command Recognition,0.379992,"With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
https://github.com/HLTCHKUST/CI-AVSR.",https://github.com/HLTCHKUST/CI-AVSR,-1
Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning,0.640243,"The spread of rumors along with breaking events seriously hinders the truth
in the era of social media. Previous studies reveal that due to the lack of
annotated resources, rumors presented in minority languages are hard to be
detected. Furthermore, the unforeseen breaking events not involved in
yesterday's news exacerbate the scarcity of data resources. In this work, we
propose a novel zero-shot framework based on prompt learning to detect rumors
falling in different domains or presented in different languages. More
specifically, we firstly represent rumor circulated on social media as diverse
propagation threads, then design a hierarchical prompt encoding mechanism to
learn language-agnostic contextual representations for both prompts and rumor
data. To further enhance domain adaptation, we model the domain-invariant
structural features from the propagation threads, to incorporate structural
position representations of influential community response. In addition, a new
virtual response augmentation method is used to improve model training.
Extensive experiments conducted on three real-world datasets demonstrate that
our proposed model achieves much better performance than state-of-the-art
methods and exhibits a superior capacity for detecting rumors at early stages.",https://github.com/PengyaoYi/zeroRumor,-1
"Entities, Dates, and Languages: Zero-Shot on Historical Texts with T0",0.317804,"In this work, we explore whether the recently demonstrated zero-shot
abilities of the T0 model extend to Named Entity Recognition for
out-of-distribution languages and time periods. Using a historical newspaper
corpus in 3 languages as test-bed, we use prompts to extract possible named
entities. Our results show that a naive approach for prompt-based zero-shot
multilingual Named Entity Recognition is error-prone, but highlights the
potential of such an approach for historical languages lacking labeled
datasets. Moreover, we also find that T0-like models can be probed to predict
the publication date and language of a document, which could be very relevant
for the study of historical texts.",https://github.com/bigscience-workshop/historical_texts,-1
Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model,0.249269,"Augmenting pretrained language models with retrievers has shown promise in
effectively solving common NLP problems, such as language modeling and question
answering. In this paper, we evaluate the strengths and weaknesses of popular
retriever-augmented language models, namely kNN-LM, REALM, DPR + FiD,
Contriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved
statements across different tasks. Our findings indicate that the simple
similarity metric employed by retrievers is insufficient for retrieving all the
necessary statements for reasoning. Additionally, the language models do not
exhibit strong reasoning even when provided with only the required statements.
Furthermore, when combined with imperfect retrievers, the performance of the
language models becomes even worse, e.g., Flan-T5's performance drops by 28.6%
when retrieving 5 statements using Contriever. While larger language models
improve performance, there is still a substantial room for enhancement. Our
further analysis indicates that multihop retrieve-and-read is promising for
large language models like GPT-3.5, but does not generalize to other language
models like Flan-T5-xxl.",https://github.com/McGill-NLP/retriever-lm-reasoning,-1
Less is More: Task-aware Layer-wise Distillation for Language Model Compression,0.132669,"Layer-wise distillation is a powerful tool to compress large models (i.e.
teacher models) into small ones (i.e., student models). The student distills
knowledge from the teacher by mimicking the hidden representations of the
teacher at every intermediate layer. However, layer-wise distillation is
difficult. Since the student has a smaller model capacity than the teacher, it
is often under-fitted. Furthermore, the hidden representations of the teacher
contain redundant information that the student does not necessarily need for
the target task's learning. To address these challenges, we propose a novel
Task-aware layEr-wise Distillation (TED). TED designs task-aware filters to
align the hidden representations of the student and the teacher at each layer.
The filters select the knowledge that is useful for the target task from the
hidden representations. As such, TED reduces the knowledge gap between the two
models and helps the student to fit better on the target task. We evaluate TED
in two scenarios: continual pre-training and fine-tuning. TED demonstrates
significant and consistent improvements over existing distillation methods in
both scenarios. Code is available at
https://github.com/cliang1453/task-aware-distillation.",https://github.com/cliang1453/task-aware-distillation,-1
Long-Tail Prediction Uncertainty Aware Trajectory Planning for Self-driving Vehicles,0.23465,"A typical trajectory planner of autonomous driving commonly relies on
predicting the future behavior of surrounding obstacles. Recently, deep
learning technology has been widely adopted to design prediction models due to
their impressive performance. However, such models may fail in the ""long-tail""
driving cases where the training data is sparse or unavailable, leading to
planner failures. To this end, this work proposes a trajectory planner to
consider the prediction model uncertainty arising from insufficient data for
safer performance. Firstly, an ensemble network structure estimates the
prediction model's uncertainty due to insufficient training data. Then a
trajectory planner is designed to consider the worst-case arising from
prediction uncertainty. The results show that the proposed method can improve
the safety of trajectory planning under the prediction uncertainty caused by
insufficient data. At the same time, with sufficient data, the framework will
not lead to overly conservative results. This technology helps to improve the
safety and reliability of autonomous vehicles under the long-tail data
distribution of the real world.",None,-1
Globally Optimal Event-Based Divergence Estimation for Ventral Landing,0.0443758,"Event sensing is a major component in bio-inspired flight guidance and
control systems. We explore the usage of event cameras for predicting
time-to-contact (TTC) with the surface during ventral landing. This is achieved
by estimating divergence (inverse TTC), which is the rate of radial optic flow,
from the event stream generated during landing. Our core contributions are a
novel contrast maximisation formulation for event-based divergence estimation,
and a branch-and-bound algorithm to exactly maximise contrast and find the
optimal divergence value. GPU acceleration is conducted to speed up the global
algorithm. Another contribution is a new dataset containing real event streams
from ventral landing that was employed to test and benchmark our method. Owing
to global optimisation, our algorithm is much more capable at recovering the
true divergence, compared to other heuristic divergence estimators or
event-based optic flow methods. With GPU acceleration, our method also achieves
competitive runtimes.",https://github.com/s-mcleod/ventral-landing-event-dataset,-1
Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions,0.0786805,"We describe an augmented intelligence system for simplifying and enhancing
the modeling experience for operations research. Using this system, the user
receives a suggested formulation of an optimization problem based on its
description. To facilitate this process, we build an intuitive user interface
system that enables the users to validate and edit the suggestions. We
investigate controlled generation techniques to obtain an automatic suggestion
of formulation. Then, we evaluate their effectiveness with a newly created
dataset of linear programming problems drawn from various application domains.",None,-1
An Attention-based Method for Action Unit Detection at the 3rd ABAW Competition,0.136719,"Facial Action Coding System is an approach for modeling the complexity of
human emotional expression. Automatic action unit (AU) detection is a crucial
research area in human-computer interaction. This paper describes our
submission to the third Affective Behavior Analysis in-the-wild (ABAW)
competition 2022. We proposed a method for detecting facial action units in the
video. At the first stage, a lightweight CNN-based feature extractor is
employed to extract the feature map from each video frame. Then, an attention
module is applied to refine the attention map. The attention encoded vector is
derived using a weighted sum of the feature map and the attention scores later.
Finally, the sigmoid function is used at the output layer to make the
prediction suitable for multi-label AUs detection. We achieved a macro F1 score
of 0.48 on the ABAW challenge validation set compared to 0.39 from the baseline
model.",None,-1
PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales,0.117174,"Neural language models (LMs) have achieved impressive results on various
language-based reasoning tasks by utilizing latent knowledge encoded in their
own pretrained parameters. To make this reasoning process more explicit, recent
works retrieve a rationalizing LM's internal knowledge by training or prompting
it to generate free-text rationales, which can be used to guide task
predictions made by either the same LM or a separate reasoning LM. However,
rationalizing LMs require expensive rationale annotation and/or computation,
without any assurance that their generated rationales improve LM task
performance or faithfully reflect LM decision-making. In this paper, we propose
PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns
to faithfully reason over rationales via counterfactual regularization. First,
PINTO maps out a suitable reasoning process for the task input by prompting a
frozen rationalizing LM to generate a free-text rationale. Second, PINTO's
reasoning LM is fine-tuned to solve the task using the generated rationale as
context, while regularized to output less confident predictions when the
rationale is perturbed. Across four datasets, we show that PINTO significantly
improves the generalization ability of the reasoning LM, yielding higher
performance on both in-distribution and out-of-distribution test sets. Also, we
find that PINTO's rationales are more faithful to its task predictions than
those generated by competitive baselines.",https://github.com/wangpf3/pinto-faithful-language-reasoning,-1
Qualia as physical measurements: a mathematical model of qualia and pure concepts,0.114126,"A space of qualia is defined to be a sober topological space whose points are
the qualia and whose open sets are the pure concepts in the sense of Lewis,
carrying additional algebraic structure that conveys the conscious experience
of subjective time and logical abstraction. This structure is analogous to that
of a space of physical measurements. It is conjectured that qualia and
measurements have the same nature, corresponding to fundamental processes via
which classical information is produced and physically stored, and that
therefore the hard problem of consciousness and the measurement problem are two
facets of the same problem. The space of qualia is independent from any
preexisting notions of spacetime and conscious agent, but its structure caters
for a derived geometric model of observer. Intersubjectivity is based on
relating different observers in a way that leads to a logical version of
quantum superposition.",None,-1
Motion Guided Deep Dynamic 3D Garments,0.151369,"Realistic dynamic garments on animated characters have many AR/VR
applications. While authoring such dynamic garment geometry is still a
challenging task, data-driven simulation provides an attractive alternative,
especially if it can be controlled simply using the motion of the underlying
character. In this work, we focus on motion guided dynamic 3D garments,
especially for loose garments. In a data-driven setup, we first learn a
generative space of plausible garment geometries. Then, we learn a mapping to
this space to capture the motion dependent dynamic deformations, conditioned on
the previous state of the garment as well as its relative position with respect
to the underlying body. Technically, we model garment dynamics, driven using
the input character motion, by predicting per-frame local displacements in a
canonical state of the garment that is enriched with frame-dependent skinning
weights to bring the garment to the global space. We resolve any remaining
per-frame collisions by predicting residual local displacements. The resultant
garment geometry is used as history to enable iterative rollout prediction. We
demonstrate plausible generalization to unseen body shapes and motion inputs,
and show improvements over multiple state-of-the-art alternatives.",https://geometry.cs.ucl.ac.uk/projects/2022/MotionDeepGarment/,-1
SRFeat: Learning Locally Accurate and Globally Consistent Non-Rigid Shape Correspondence,0.0844284,"In this work, we present a novel learning-based framework that combines the
local accuracy of contrastive learning with the global consistency of geometric
approaches, for robust non-rigid matching. We first observe that while
contrastive learning can lead to powerful point-wise features, the learned
correspondences commonly lack smoothness and consistency, owing to the purely
combinatorial nature of the standard contrastive losses. To overcome this
limitation we propose to boost contrastive feature learning with two types of
smoothness regularization that inject geometric information into correspondence
learning. With this novel combination in hand, the resulting features are both
highly discriminative across individual points, and, at the same time, lead to
robust and consistent correspondences, through simple proximity queries. Our
framework is general and is applicable to local feature learning in both the 3D
and 2D domains. We demonstrate the superiority of our approach through
extensive experiments on a wide range of challenging matching benchmarks,
including 3D non-rigid shape correspondence and 2D image keypoint matching.",https://github.com/craigleili/SRFeat,-1
Fruit Quality Assessment with Densely Connected Convolutional Neural Network,0.0352499,"Accurate recognition of food items along with quality assessment is of
paramount importance in the agricultural industry. Such automated systems can
speed up the wheel of the food processing sector and save tons of manual labor.
In this connection, the recent advancement of Deep learning-based architectures
has introduced a wide variety of solutions offering remarkable performance in
several classification tasks. In this work, we have exploited the concept of
Densely Connected Convolutional Neural Networks (DenseNets) for fruit quality
assessment. The feature propagation towards the deeper layers has enabled the
network to tackle the vanishing gradient problems and ensured the reuse of
features to learn meaningful insights. Evaluating on a dataset of 19,526 images
containing six fruits having three quality grades for each, the proposed
pipeline achieved a remarkable accuracy of 99.67%. The robustness of the model
was further tested for fruit classification and quality assessment tasks where
the model produced a similar performance, which makes it suitable for real-life
applications.",None,-1
SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation,0.133868,"Recent advances in image captioning have focused on scaling the data and
model size, substantially increasing the cost of pre-training and finetuning.
As an alternative to large models, we present SmallCap, which generates a
caption conditioned on an input image and related captions retrieved from a
datastore. Our model is lightweight and fast to train, as the only learned
parameters are in newly introduced cross-attention layers between a pre-trained
CLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without
additional finetuning and can exploit large-scale data in a training-free
fashion since the contents of the datastore can be readily replaced. Our
experiments show that SmallCap, trained only on COCO, has competitive
performance on this benchmark, and also transfers to other domains without
retraining, solely through retrieval from target-domain data. Further
improvement is achieved through the training-free exploitation of diverse
human-labeled and web data, which proves to be effective for a range of
domains, including the nocaps benchmark, designed to test generalization to
unseen visual concepts.",https://github.com/RitaRamo/smallcap,-1
Phantom Sponges: Exploiting Non-Maximum Suppression to Attack Deep Object Detectors,0.242467,"Adversarial attacks against deep learning-based object detectors have been
studied extensively in the past few years. Most of the attacks proposed have
targeted the model's integrity (i.e., caused the model to make incorrect
predictions), while adversarial attacks targeting the model's availability, a
critical aspect in safety-critical domains such as autonomous driving, have not
yet been explored by the machine learning research community. In this paper, we
propose a novel attack that negatively affects the decision latency of an
end-to-end object detection pipeline. We craft a universal adversarial
perturbation (UAP) that targets a widely used technique integrated in many
object detector pipelines -- non-maximum suppression (NMS). Our experiments
demonstrate the proposed UAP's ability to increase the processing time of
individual frames by adding ""phantom"" objects that overload the NMS algorithm
while preserving the detection of the original objects which allows the attack
to go undetected for a longer period of time.",https://github.com/AvishagS422/PhantomSponges,-1
Counterfactual harm,0.13071,"To act safely and ethically in the real world, agents must be able to reason
about harm and avoid harmful actions. However, to date there is no statistical
method for measuring harm and factoring it into algorithmic decisions. In this
paper we propose the first formal definition of harm and benefit using causal
models. We show that any factual definition of harm must violate basic
intuitions in certain scenarios, and show that standard machine learning
algorithms that cannot perform counterfactual reasoning are guaranteed to
pursue harmful policies following distributional shifts. We use our definition
of harm to devise a framework for harm-averse decision making using
counterfactual objective functions. We demonstrate this framework on the
problem of identifying optimal drug doses using a dose-response model learned
from randomized control trial data. We find that the standard method of
selecting doses using treatment effects results in unnecessarily harmful doses,
while our counterfactual approach allows us to identify doses that are
significantly less harmful without sacrificing efficacy.",None,-1
Methods of Informational Trends Analytics and Fake News Detection on Twitter,0.155608,"In the paper, different approaches for the analysis of news trends on Twitter
has been considered. For the analysis and case study, informational trends on
Twitter caused by Russian invasion of Ukraine in 2022 year have been studied. A
deep learning approach for fake news detection has been analyzed. The use of
the theory of frequent itemsets and association rules, graph theory for news
trends analytics have been considered.",None,-1
ProQA: Structural Prompt-based Pre-training for Unified Question Answering,0.734463,"Question Answering (QA) is a longstanding challenge in natural language
processing. Existing QA works mostly focus on specific question types,
knowledge domains, or reasoning skills. The specialty in QA research hinders
systems from modeling commonalities between tasks and generalization for wider
applications. To address this issue, we present ProQA, a unified QA paradigm
that solves various tasks through a single model. ProQA takes a unified
structural prompt as the bridge and improves the QA-centric ability by
structural prompt-based pre-training. Through a structurally designed
prompt-based input schema, ProQA concurrently models the knowledge
generalization for all QA tasks while keeping the knowledge customization for
every specific QA task. Furthermore, ProQA is pre-trained with structural
prompt-formatted large-scale synthesized corpus, which empowers the model with
the commonly-required QA ability. Experimental results on 11 QA benchmarks
demonstrate that ProQA consistently boosts performance on both full data
fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore,
ProQA exhibits strong ability in both continual learning and transfer learning
by taking the advantages of the structural prompt.",https://github.com/zhongwanjun/ProQA,-1
Actual Causality and Responsibility Attribution in Decentralized Partially Observable Markov Decision Processes,0.0707391,"Actual causality and a closely related concept of responsibility attribution
are central to accountable decision making. Actual causality focuses on
specific outcomes and aims to identify decisions (actions) that were critical
in realizing an outcome of interest. Responsibility attribution is
complementary and aims to identify the extent to which decision makers (agents)
are responsible for this outcome. In this paper, we study these concepts under
a widely used framework for multi-agent sequential decision making under
uncertainty: decentralized partially observable Markov decision processes
(Dec-POMDPs). Following recent works in RL that show correspondence between
POMDPs and Structural Causal Models (SCMs), we first establish a connection
between Dec-POMDPs and SCMs. This connection enables us to utilize a language
for describing actual causality from prior work and study existing definitions
of actual causality in Dec-POMDPs. Given that some of the well-known
definitions may lead to counter-intuitive actual causes, we introduce a novel
definition that more explicitly accounts for causal dependencies between
agents' actions. We then turn to responsibility attribution based on actual
causality, where we argue that in ascribing responsibility to an agent it is
important to consider both the number of actual causes in which the agent
participates, as well as its ability to manipulate its own degree of
responsibility. Motivated by these arguments we introduce a family of
responsibility attribution methods that extends prior work, while accounting
for the aforementioned considerations. Finally, through a simulation-based
experiment, we compare different definitions of actual causality and
responsibility attribution methods. The empirical results demonstrate the
qualitative difference between the considered definitions of actual causality
and their impact on attributed responsibility.",None,-1
CADOps-Net: Jointly Learning CAD Operation Types and Steps from Boundary-Representations,0.101813,"3D reverse engineering is a long sought-after, yet not completely achieved
goal in the Computer-Aided Design (CAD) industry. The objective is to recover
the construction history of a CAD model. Starting from a Boundary
Representation (B-Rep) of a CAD model, this paper proposes a new deep neural
network, CADOps-Net, that jointly learns the CAD operation types and the
decomposition into different CAD operation steps. This joint learning allows to
divide a B-Rep into parts that were created by various types of CAD operations
at the same construction step; therefore providing relevant information for
further recovery of the design history. Furthermore, we propose the novel
CC3D-Ops dataset that includes over $37k$ CAD models annotated with CAD
operation type labels and step labels. Compared to existing datasets, the
complexity and variety of CC3D-Ops models are closer to those used for
industrial purposes. Our experiments, conducted on the proposed CC3D-Ops and
the publicly available Fusion360 datasets, demonstrate the competitive
performance of CADOps-Net with respect to state-of-the-art, and confirm the
importance of the joint learning of CAD operation types and steps.",None,-1
Towards Improving Faithfulness in Abstractive Summarization,0.0640594,"Despite the success achieved in neural abstractive summarization based on
pre-trained language models, one unresolved issue is that the generated
summaries are not always faithful to the input document. There are two possible
causes of the unfaithfulness problem: (1) the summarization model fails to
understand or capture the gist of the input text, and (2) the model over-relies
on the language model to generate fluent but inadequate words. In this work, we
propose a Faithfulness Enhanced Summarization model (FES), which is designed
for addressing these two problems and improving faithfulness in abstractive
summarization. For the first problem, we propose to use question-answering (QA)
to examine whether the encoder fully grasps the input document and can answer
the questions on the key information in the input. The QA attention on the
proper input words can also be used to stipulate how the decoder should attend
to the source. For the second problem, we introduce a max-margin loss defined
on the difference between the language and the summarization model, aiming to
prevent the overconfidence of the language model. Extensive experiments on two
benchmark summarization datasets, CNN/DM and XSum, demonstrate that our model
significantly outperforms strong baselines. The evaluation of factual
consistency also shows that our model generates more faithful summaries than
baselines.",https://github.com/iriscxy/FES,-1
DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering,0.315386,"Spoken Question Answering (SQA) is to find the answer from a spoken document
given a question, which is crucial for personal assistants when replying to the
queries from the users. Existing SQA methods all rely on Automatic Speech
Recognition (ASR) transcripts. Not only does ASR need to be trained with
massive annotated data that are time and cost-prohibitive to collect for
low-resourced languages, but more importantly, very often the answers to the
questions include name entities or out-of-vocabulary words that cannot be
recognized correctly. Also, ASR aims to minimize recognition errors equally
over all words, including many function words irrelevant to the SQA task.
Therefore, SQA without ASR transcripts (textless) is always highly desired,
although known to be very difficult.
  This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging
unlabeled data for pre-training and fine-tuned by the SQA downstream task. The
time intervals of spoken answers can be directly predicted from spoken
documents. We also release a new SQA benchmark corpus, NMSQA, for data with
more realistic scenarios. We empirically showed that DUAL yields results
comparable to those obtained by cascading ASR and text QA model and robust to
real-world data. Our code and model will be open-sourced.",None,-1
Instance-Dependent Noisy Label Learning via Graphical Modelling,0.430927,"Noisy labels are unavoidable yet troublesome in the ecosystem of deep
learning because models can easily overfit them. There are many types of label
noise, such as symmetric, asymmetric and instance-dependent noise (IDN), with
IDN being the only type that depends on image information. Such dependence on
image information makes IDN a critical type of label noise to study, given that
labelling mistakes are caused in large part by insufficient or ambiguous
information about the visual classes present in images. Aiming to provide an
effective technique to address IDN, we present a new graphical modelling
approach called InstanceGM, that combines discriminative and generative models.
The main contributions of InstanceGM are: i) the use of the continuous
Bernoulli distribution to train the generative model, offering significant
training advantages, and ii) the exploration of a state-of-the-art noisy-label
discriminative classifier to generate clean labels from instance-dependent
noisy-label samples. InstanceGM is competitive with current noisy-label
learning approaches, particularly in IDN benchmarks using synthetic and
real-world datasets, where our method shows better accuracy than the
competitors in most experiments.",https://github.com/a5507203/IDLN,-1
PixelGame: Infrared small target segmentation as a Nash equilibrium,0.0325022,"A key challenge of infrared small target segmentation (ISTS) is to balance
false negative pixels (FNs) and false positive pixels (FPs). Traditional
methods combine FNs and FPs into a single objective by weighted sum, and the
optimization process is decided by one actor. Minimizing FNs and FPs with the
same strategy leads to antagonistic decisions. To address this problem, we
propose a competitive game framework (pixelGame) from a novel perspective for
ISTS. In pixelGame, FNs and FPs are controlled by different player whose goal
is to minimize their own utility function. FNs-player and FPs-player are
designed with different strategies: One is to minimize FNs and the other is to
minimize FPs. The utility function drives the evolution of the two participants
in competition. We consider the Nash equilibrium of pixelGame as the optimal
solution. In addition, we propose maximum information modulation (MIM) to
highlight the tar-get information. MIM effectively focuses on the salient
region including small targets. Extensive experiments on two standard public
datasets prove the effectiveness of our method. Compared with other
state-of-the-art methods, our method achieves better performance in terms of
F1-measure (F1) and the intersection of union (IoU).",None,-1
Where did you tweet from? Inferring the origin locations of tweets based on contextual information,0.133306,"Public conversations on Twitter comprise many pertinent topics including
disasters, protests, politics, propaganda, sports, climate change,
epidemics/pandemic outbreaks, etc., that can have both regional and global
aspects. Spatial discourse analysis rely on geographical data. However, today
less than 1% of tweets are geotagged; in both cases--point location or bounding
place information. A major issue with tweets is that Twitter users can be at
location A and exchange conversations specific to location B, which we call the
Location A/B problem. The problem is considered solved if location entities can
be classified as either origin locations (Location As) or non-origin locations
(Location Bs). In this work, we propose a simple yet effective framework--the
True Origin Model--to address the problem that uses machine-level natural
language understanding to identify tweets that conceivably contain their origin
location information. The model achieves promising accuracy at country (80%),
state (67%), city (58%), county (56%) and district (64%) levels with support
from a Location Extraction Model as basic as the CoNLL-2003-based RoBERTa. We
employ a tweet contexualizer (locBERT) which is one of the core components of
the proposed model, to investigate multiple tweets' distributions for
understanding Twitter users' tweeting behavior in terms of mentioning origin
and non-origin locations. We also highlight a major concern with the currently
regarded gold standard test set (ground truth) methodology, introduce a new
data set, and identify further research avenues for advancing the area.",None,-1
End-to-End 3D Hand Pose Estimation from Stereo Cameras,0.15172,"This work proposes an end-to-end approach to estimate full 3D hand pose from
stereo cameras. Most existing methods of estimating hand pose from stereo
cameras apply stereo matching to obtain depth map and use depth-based solution
to estimate hand pose. In contrast, we propose to bypass the stereo matching
and directly estimate the 3D hand pose from the stereo image pairs. The
proposed neural network architecture extends from any keypoint predictor to
estimate the sparse disparity of the hand joints. In order to effectively train
the model, we propose a large scale synthetic dataset that is composed of
stereo image pairs and ground truth 3D hand pose annotations. Experiments show
that the proposed approach outperforms the existing methods based on the stereo
depth.",None,-1
MRTNet: Multi-Resolution Temporal Network for Video Sentence Grounding,0.141617,"Given an untrimmed video and natural language query, video sentence grounding
aims to localize the target temporal moment in the video. Existing methods
mainly tackle this task by matching and aligning semantics of the descriptive
sentence and video segments on a single temporal resolution, while neglecting
the temporal consistency of video content in different resolutions. In this
work, we propose a novel multi-resolution temporal video sentence grounding
network: MRTNet, which consists of a multi-modal feature encoder, a
Multi-Resolution Temporal (MRT) module, and a predictor module. MRT module is
an encoder-decoder network, and output features in the decoder part are in
conjunction with Transformers to predict the final start and end timestamps.
Particularly, our MRT module is hot-pluggable, which means it can be seamlessly
incorporated into any anchor-free models. Besides, we utilize a hybrid loss to
supervise cross-modal features in MRT module for more accurate grounding in
three scales: frame-level, clip-level and sequence-level. Extensive experiments
on three prevalent datasets have shown the effectiveness of MRTNet.",None,-1
Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation,0.109194,"We introduce Bi-SimCut: a simple but effective training strategy to boost
neural machine translation (NMT) performance. It consists of two procedures:
bidirectional pretraining and unidirectional finetuning. Both procedures
utilize SimCut, a simple regularization method that forces the consistency
between the output distributions of the original and the cutoff sentence pairs.
Without leveraging extra dataset via back-translation or integrating
large-scale pretrained model, Bi-SimCut achieves strong translation performance
across five translation benchmarks (data sizes range from 160K to 20.2M): BLEU
scores of 31.16 for en -> de and 38.37 for de -> en on the IWSLT14 dataset,
30.78 for en -> de and 35.15 for de -> en on the WMT14 dataset, and 27.17 for
zh -> en on the WMT17 dataset. SimCut is not a new method, but a version of
Cutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be
considered as a perturbation-based method. Given the universality and
simplicity of SimCut and Bi-SimCut, we believe they can serve as strong
baselines for future NMT research.",https://github.com/gpengzhi/Bi-SimCut,-1
Multi-Head Attention Neural Network for Smartphone Invariant Indoor Localization,0.147489,"Smartphones together with RSSI fingerprinting serve as an efficient approach
for delivering a low-cost and high-accuracy indoor localization solution.
However, a few critical challenges have prevented the wide-spread proliferation
of this technology in the public domain. One such critical challenge is device
heterogeneity, i.e., the variation in the RSSI signal characteristics captured
across different smartphone devices. In the real-world, the smartphones or IoT
devices used to capture RSSI fingerprints typically vary across users of an
indoor localization service. Conventional indoor localization solutions may not
be able to cope with device-induced variations which can degrade their
localization accuracy. We propose a multi-head attention neural network-based
indoor localization framework that is resilient to device heterogeneity. An
in-depth analysis of our proposed framework across a variety of indoor
environments demonstrates up to 35% accuracy improvement compared to
state-of-the-art indoor localization techniques.",None,-1
TransLIST: A Transformer-Based Linguistically Informed Sanskrit Tokenizer,0.0447999,"Sanskrit Word Segmentation (SWS) is essential in making digitized texts
available and in deploying downstream tasks. It is, however, non-trivial
because of the sandhi phenomenon that modifies the characters at the word
boundaries, and needs special treatment. Existing lexicon driven approaches for
SWS make use of Sanskrit Heritage Reader, a lexicon-driven shallow parser, to
generate the complete candidate solution space, over which various methods are
applied to produce the most valid solution. However, these approaches fail
while encountering out-of-vocabulary tokens. On the other hand, purely
engineering methods for SWS have made use of recent advances in deep learning,
but cannot make use of the latent word information on availability.
  To mitigate the shortcomings of both families of approaches, we propose
Transformer based Linguistically Informed Sanskrit Tokenizer (TransLIST)
consisting of (1) a module that encodes the character input along with
latent-word information, which takes into account the sandhi phenomenon
specific to SWS and is apt to work with partial or no candidate solutions, (2)
a novel soft-masked attention to prioritize potential candidate words and (3) a
novel path ranking algorithm to rectify the corrupted predictions. Experiments
on the benchmark datasets for SWS show that TransLIST outperforms the current
state-of-the-art system by an average 7.2 points absolute gain in terms of
perfect match (PM) metric. The codebase and datasets are publicly available at
https://github.com/rsingha108/TransLIST",https://github.com/rsingha108/TransLIST,-1
TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation,0.142906,"Knowledge graph embedding (KGE) aims to learn continuous vectors of relations
and entities in knowledge graph. Recently, transition-based KGE methods have
achieved promising performance, where the single relation vector learns to
translate head entity to tail entity. However, this scoring pattern is not
suitable for complex scenarios where the same entity pair has different
relations. Previous models usually focus on the improvement of entity
representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single
relation vector. In this paper, we propose a novel transition-based method,
TranS, for knowledge graph embedding. The single relation vector in traditional
scoring patterns is replaced with synthetic relation representation, which can
solve these issues effectively and efficiently. Experiments on a large
knowledge graph dataset, ogbl-wikikg2, show that our model achieves
state-of-the-art results.",None,-1
Adapters for Enhanced Modeling of Multilingual Knowledge and Text,0.0193742,"Large language models appear to learn facts from the large text corpora they
are trained on. Such facts are encoded implicitly within their many parameters,
making it difficult to verify or manipulate what knowledge has been learned.
Language models have recently been extended to multilingual language models
(MLLMs), enabling knowledge to be learned across hundreds of languages.
Meanwhile, knowledge graphs contain facts in an explicit triple format, which
require careful and costly curation and are only available in a few
high-resource languages, restricting their research and application. To address
these issues, we propose to enhance MLLMs with knowledge from multilingual
knowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks
across many languages, including low-resource ones. Specifically, we introduce
a lightweight adapter set to enhance MLLMs with cross-lingual entity alignment
and facts from MLKGs for many languages. Experiments on common benchmarks show
that such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable
or improved performance for knowledge graph completion and entity alignment
relative to baselines, especially for low-resource languages (for which
knowledge graphs are unavailable); and (2) improved MLLM performance on
language understanding tasks that require multilingual factual knowledge; all
while maintaining performance on other general language tasks.",https://github.com/yifan-h/Multilingual_Space,-1
Spanish Abstract Meaning Representation: Annotation of a General Corpus,0.15209,"The Abstract Meaning Representation (AMR) formalism, designed originally for
English, has been adapted to a number of languages. We build on previous work
proposing the annotation of AMR in Spanish, which resulted in the release of 50
Spanish AMR annotations for the fictional text ""The Little Prince."" In this
work, we present the first sizable, general annotation project for Spanish
Abstract Meaning Representation. Our approach to annotation makes use of
Spanish rolesets from the AnCora-Net lexicon and extends English AMR with
semantic features specific to Spanish. In addition to our guidelines, we
release an annotated corpus (586 annotations total, for 486 unique sentences)
of multiple genres of documents from the ""Abstract Meaning Representation 2.0 -
Four Translations"" sembank. This corpus is commonly used for evaluation of AMR
parsing and generation, but does not include gold AMRs; we hope that providing
gold annotations for this dataset can result in a more complete approach to
cross-lingual AMR parsing. Finally, we perform a disagreement analysis and
discuss the implications of our work on the adaptability of AMR to languages
other than English.",None,-1
Efficient Task-Oriented Dialogue Systems with Response Selection as an Auxiliary Task,0.0360349,"The adoption of pre-trained language models in task-oriented dialogue systems
has resulted in significant enhancements of their text generation abilities.
However, these architectures are slow to use because of the large number of
trainable parameters and can sometimes fail to generate diverse responses. To
address these limitations, we propose two models with auxiliary tasks for
response selection - (1) distinguishing distractors from ground truth responses
and (2) distinguishing synthetic responses from ground truth labels. They
achieve state-of-the-art results on the MultiWOZ 2.1 dataset with combined
scores of 107.5 and 108.3 and outperform a baseline with three times more
parameters. We publish reproducible code and checkpoints and discuss the
effects of applying auxiliary tasks to T5-based architectures.",https://github.com/,-1
Target-Guided Dialogue Response Generation Using Commonsense and Data Augmentation,0.473141,"Target-guided response generation enables dialogue systems to smoothly
transition a conversation from a dialogue context toward a target sentence.
Such control is useful for designing dialogue systems that direct a
conversation toward specific goals, such as creating non-obtrusive
recommendations or introducing new topics in the conversation. In this paper,
we introduce a new technique for target-guided response generation, which first
finds a bridging path of commonsense knowledge concepts between the source and
the target, and then uses the identified bridging path to generate transition
responses. Additionally, we propose techniques to re-purpose existing dialogue
datasets for target-guided generation. Experiments reveal that the proposed
techniques outperform various baselines on this task. Finally, we observe that
the existing automated metrics for this task correlate poorly with human
judgement ratings. We propose a novel evaluation metric that we demonstrate is
more reliable for target-guided response evaluation. Our work generally enables
dialogue system designers to exercise more control over the conversations that
their systems produce.",https://github.com/prakhargupt/target-guided-dialogue-coda,-1
Efficient universal shuffle attack for visual object tracking,0.368957,"Recently, adversarial attacks have been applied in visual object tracking to
deceive deep trackers by injecting imperceptible perturbations into video
frames. However, previous work only generates the video-specific perturbations,
which restricts its application scenarios. In addition, existing attacks are
difficult to implement in reality due to the real-time of tracking and the
re-initialization mechanism. To address these issues, we propose an offline
universal adversarial attack called Efficient Universal Shuffle Attack. It
takes only one perturbation to cause the tracker malfunction on all videos. To
improve the computational efficiency and attack performance, we propose a
greedy gradient strategy and a triple loss to efficiently capture and attack
model-specific feature representations through the gradients. Experimental
results show that EUSA can significantly reduce the performance of
state-of-the-art trackers on OTB2015 and VOT2018.",None,-1
Adaptable Adapters,0.0994431,"State-of-the-art pretrained NLP models contain a hundred million to trillion
parameters. Adapters provide a parameter-efficient alternative for the full
finetuning in which we can only finetune lightweight neural network layers on
top of pretrained weights. Adapter layers are initialized randomly. However,
existing work uses the same adapter architecture -- i.e., the same adapter
layer on top of each layer of the pretrained model -- for every dataset,
regardless of the properties of the dataset or the amount of available training
data. In this work, we introduce adaptable adapters that contain (1) learning
different activation functions for different layers and different input data,
and (2) a learnable switch to select and only use the beneficial adapter
layers. We show that adaptable adapters achieve on-par performances with the
standard adapter architecture while using a considerably smaller number of
adapter layers. In addition, we show that the selected adapter architecture by
adaptable adapters transfers well across different data settings and similar
tasks. We propose to use adaptable adapters for designing efficient and
effective adapter architectures. The resulting adapters (a) contain about 50%
of the learning parameters of the standard adapter and are therefore more
efficient at training and inference, and require less storage space, and (b)
achieve considerably higher performances in low-data settings.",https://github.com/UKPLab/adaptable-adapters,-1
Deep Reinforcement Learning for Exact Combinatorial Optimization: Learning to Branch,0.0195766,"Branch-and-bound is a systematic enumerative method for combinatorial
optimization, where the performance highly relies on the variable selection
strategy. State-of-the-art handcrafted heuristic strategies suffer from
relatively slow inference time for each selection, while the current machine
learning methods require a significant amount of labeled data. We propose a new
approach for solving the data labeling and inference latency issues in
combinatorial optimization based on the use of the reinforcement learning (RL)
paradigm. We use imitation learning to bootstrap an RL agent and then use
Proximal Policy Optimization (PPO) to further explore global optimal actions.
Then, a value network is used to run Monte-Carlo tree search (MCTS) to enhance
the policy network. We evaluate the performance of our method on four different
categories of combinatorial optimization problems and show that our approach
performs strongly compared to the state-of-the-art machine learning and
heuristics based methods.",None,-1
CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding,0.0305444,"Story generation and understanding -- as with all NLG/NLU tasks -- has seen a
surge in neurosymbolic work. Researchers have recognized that, while large
language models (LLMs) have tremendous utility, they can be augmented with
symbolic means to be even better and to make up for any flaws that the neural
networks might have. However, symbolic methods are extremely costly in terms of
the amount of time and expertise needed to create them. In this work, we
capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use
of symbolic methods for tracking the state of stories and aiding in story
understanding. We show that our CoRRPUS system and abstracted prompting
procedures can beat current state-of-the-art structured LLM techniques on
pre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand
engineering. We hope that this work can help highlight the importance of
symbolic representations and specialized prompting for LLMs as these models
require some guidance for performing reasoning tasks properly.",https://github.com/dong-river/CoRRPUS,-1
The Legal Argument Reasoning Task in Civil Procedure,0.0902028,"We present a new NLP task and dataset from the domain of the U.S. civil
procedure. Each instance of the dataset consists of a general introduction to
the case, a particular question, and a possible solution argument, accompanied
by a detailed analysis of why the argument applies in that case. Since the
dataset is based on a book aimed at law students, we believe that it represents
a truly complex task for benchmarking modern legal language models. Our
baseline evaluation shows that fine-tuning a legal transformer provides some
advantage over random baseline models, but our analysis reveals that the actual
ability to infer legal arguments remains a challenging open research question.",https://github.com/trusthlt/legal-argument-reasoning-task,-1
SKIPP'D: a SKy Images and Photovoltaic Power Generation Dataset for Short-term Solar Forecasting,0.274166,"Large-scale integration of photovoltaics (PV) into electricity grids is
challenged by the intermittent nature of solar power. Sky-image-based solar
forecasting using deep learning has been recognized as a promising approach to
predicting the short-term fluctuations. However, there are few publicly
available standardized benchmark datasets for image-based solar forecasting,
which limits the comparison of different forecasting models and the exploration
of forecasting methods. To fill these gaps, we introduce SKIPP'D -- a SKy
Images and Photovoltaic Power Generation Dataset. The dataset contains three
years (2017-2019) of quality-controlled down-sampled sky images and PV power
generation data that is ready-to-use for short-term solar forecasting using
deep learning. In addition, to support the flexibility in research, we provide
the high resolution, high frequency sky images and PV power generation data as
well as the concurrent sky video footage. We also include a code base
containing data processing scripts and baseline model implementations for
researchers to reproduce our previous work and accelerate their research in
solar forecasting.",https://github.com/yuhao-nie/Stanford-solar-forecasting-dataset,-1
What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis,0.257861,"Market sentiment analysis on social media content requires knowledge of both
financial markets and social media jargon, which makes it a challenging task
for human raters. The resulting lack of high-quality labeled data stands in the
way of conventional supervised learning methods. Instead, we approach this
problem using semi-supervised learning with a large language model (LLM). Our
pipeline generates weak financial sentiment labels for Reddit posts with an LLM
and then uses that data to train a small model that can be served in
production. We find that prompting the LLM to produce Chain-of-Thought
summaries and forcing it through several reasoning paths helps generate more
stable and accurate labels, while using a regression loss further improves
distillation quality. With only a handful of prompts, the final model performs
on par with existing supervised models. Though production applications of our
model are limited by ethical considerations, the model's competitive
performance points to the great potential of using LLMs for tasks that
otherwise require skill-intensive annotation.",None,-1
Fault-Aware Design and Training to Enhance DNNs Reliability with Zero-Overhead,0.19807,"Deep Neural Networks (DNNs) enable a wide series of technological
advancements, ranging from clinical imaging, to predictive industrial
maintenance and autonomous driving. However, recent findings indicate that
transient hardware faults may corrupt the models prediction dramatically. For
instance, the radiation-induced misprediction probability can be so high to
impede a safe deployment of DNNs models at scale, urging the need for efficient
and effective hardening solutions. In this work, we propose to tackle the
reliability issue both at training and model design time. First, we show that
vanilla models are highly affected by transient faults, that can induce a
performances drop up to 37%. Hence, we provide three zero-overhead solutions,
based on DNN re-design and re-train, that can improve DNNs reliability to
transient faults up to one order of magnitude. We complement our work with
extensive ablation studies to quantify the gain in performances of each
hardening component.",None,-1
Analyzing Wrap-Up Effects through an Information-Theoretic Lens,0.0691127,"Numerous analyses of reading time (RT) data have been implemented -- all in
an effort to better understand the cognitive processes driving reading
comprehension. However, data measured on words at the end of a sentence -- or
even at the end of a clause -- is often omitted due to the confounding factors
introduced by so-called ""wrap-up effects,"" which manifests as a skewed
distribution of RTs for these words. Consequently, the understanding of the
cognitive processes that might be involved in these wrap-up effects is limited.
In this work, we attempt to learn more about these processes by examining the
relationship between wrap-up effects and information-theoretic quantities, such
as word and context surprisals. We find that the distribution of information in
prior contexts is often predictive of sentence- and clause-final RTs (while not
of sentence-medial RTs). This lends support to several prior hypotheses about
the processes involved in wrap-up effects.",https://github.com/rycolab/wrap-up-effects,-1
Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings,0.122186,"Previous knowledge graph embedding approaches usually map entities to
representations and utilize score functions to predict the target entities, yet
they typically struggle to reason rare or emerging unseen entities. In this
paper, we propose kNN-KGE, a new knowledge graph embedding approach with
pre-trained language models, by linearly interpolating its entity distribution
with k-nearest neighbors. We compute the nearest neighbors based on the
distance in the entity embedding space from the knowledge store. Our approach
can allow rare or emerging entities to be memorized explicitly rather than
implicitly in model parameters. Experimental results demonstrate that our
approach can improve inductive and transductive link prediction results and
yield better performance for low-resource settings with only a few triples,
which might be easier to reason via explicit memory. Code is available at
https://github.com/zjunlp/KNN-KG.",https://github.com/zjunlp/KNN-KG,-1
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,0.241129,"Localizing a semantic parser to support new languages requires effective
cross-lingual generalization. Recent work has found success with
machine-translation or zero-shot methods although these approaches can struggle
to model how native speakers ask questions. We consider how to effectively
leverage minimal annotated examples in new languages for few-shot cross-lingual
semantic parsing. We introduce a first-order meta-learning algorithm to train a
semantic parser with maximal sample efficiency during cross-lingual transfer.
Our algorithm uses high-resource languages to train the parser and
simultaneously optimizes for cross-lingual generalization for lower-resource
languages. Results across six languages on ATIS demonstrate that our
combination of generalization steps yields accurate semantic parsers sampling
$\le$10% of source training data in each new language. Our approach also trains
a competitive model on Spider using English with generalization to Chinese
similarly sampling $\le$10% of training data.",https://github.com/tomsherborne/xgr,-1
Tencent's Multilingual Machine Translation System for WMT22 Large-Scale African Languages,0.175624,"This paper describes Tencent's multilingual machine translation systems for
the WMT22 shared task on Large-Scale Machine Translation Evaluation for African
Languages. We participated in the $\mathbf{constrained}$ translation track in
which only the data and pretrained models provided by the organizer are
allowed. The task is challenging due to three problems, including the absence
of training data for some to-be-evaluated language pairs, the uneven
optimization of language pairs caused by data imbalance, and the curse of
multilinguality. To address these problems, we adopt data augmentation,
distributionally robust optimization, and language family grouping,
respectively, to develop our multilingual neural machine translation (MNMT)
models. Our submissions won the $\mathbf{1st\ place}$ on the blind test sets in
terms of the automatic evaluation metrics. Codes, models, and detailed
competition results are available at
https://github.com/wxjiao/WMT2022-Large-Scale-African.",https://github.com/wxjiao/,-1
"Learning Robotic Navigation from Experience: Principles, Methods, and Recent Results",0.0554155,"Navigation is one of the most heavily studied problems in robotics, and is
conventionally approached as a geometric mapping and planning problem. However,
real-world navigation presents a complex set of physical challenges that defies
simple geometric abstractions. Machine learning offers a promising way to go
beyond geometry and conventional planning, allowing for navigational systems
that make decisions based on actual prior experience. Such systems can reason
about traversability in ways that go beyond geometry, accounting for the
physical outcomes of their actions and exploiting patterns in real-world
environments. They can also improve as more data is collected, potentially
providing a powerful network effect. In this article, we present a general
toolkit for experiential learning of robotic navigation skills that unifies
several recent approaches, describe the underlying design principles, summarize
experimental results from several of our recent papers, and discuss open
problems and directions for future work.",None,-1
Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition,0.388554,"Due to the absence of explicit connectives, implicit discourse relation
recognition (IDRR) remains a challenging task in discourse analysis. The
critical step for IDRR is to learn high-quality discourse relation
representations between two arguments. Recent methods tend to integrate the
whole hierarchical information of senses into discourse relation
representations for multi-level sense recognition. Nevertheless, they
insufficiently incorporate the static hierarchical structure containing all
senses (defined as global hierarchy), and ignore the hierarchical sense label
sequence corresponding to each instance (defined as local hierarchy). For the
purpose of sufficiently exploiting global and local hierarchies of senses to
learn better discourse relation representations, we propose a novel GlObal and
Local Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of
hierarchies with the aid of multi-task learning and contrastive learning.
Experimental results on PDTB 2.0 and PDTB 3.0 datasets demonstrate that our
method remarkably outperforms current state-of-the-art models at all
hierarchical levels. Our code is publicly available at
https://github.com/YJiangcm/GOLF_for_IDRR",https://github.com/YJiangcm/GOLF_for_IDRR,-1
DFA: Dynamic Feature Aggregation for Efficient Video Object Detection,0.116594,"Video object detection is a fundamental yet challenging task in computer
vision. One practical solution is to take advantage of temporal information
from the video and apply feature aggregation to enhance the object features in
each frame. Though effective, those existing methods always suffer from low
inference speeds because they use a fixed number of frames for feature
aggregation regardless of the input frame. Therefore, this paper aims to
improve the inference speed of the current feature aggregation-based video
object detectors while maintaining their performance. To achieve this goal, we
propose a vanilla dynamic aggregation module that adaptively selects the frames
for feature enhancement. Then, we extend the vanilla dynamic aggregation module
to a more effective and reconfigurable deformable version. Finally, we
introduce inplace distillation loss to improve the representations of objects
aggregated with fewer frames. Extensive experimental results validate the
effectiveness and efficiency of our proposed methods: On the ImageNet VID
benchmark, integrated with our proposed methods, FGFA and SELSA can improve the
inference speed by 31% and 76% respectively while getting comparable
performance on accuracy.",https://github.com/open-mmlab/mmtracking,-1
A Multibranch Convolutional Neural Network for Hyperspectral Unmixing,0.151445,"Hyperspectral unmixing remains one of the most challenging tasks in the
analysis of such data. Deep learning has been blooming in the field and proved
to outperform other classic unmixing techniques, and can be effectively
deployed onboard Earth observation satellites equipped with hyperspectral
imagers. In this letter, we follow this research pathway and propose a
multi-branch convolutional neural network that benefits from fusing spectral,
spatial, and spectral-spatial features in the unmixing process. The results of
our experiments, backed up with the ablation study, revealed that our
techniques outperform others from the literature and lead to higher-quality
fractional abundance estimation. Also, we investigated the influence of
reducing the training sets on the capabilities of all algorithms and their
robustness against noise, as capturing large and representative ground-truth
sets is time-consuming and costly in practice, especially in emerging Earth
observation scenarios.",https://gitlab.com/jnalepa/mbhuto,-1
Boosting 3D Object Detection via Object-Focused Image Fusion,0.415918,"3D object detection has achieved remarkable progress by taking point clouds
as the only input. However, point clouds often suffer from incomplete geometric
structures and the lack of semantic information, which makes detectors hard to
accurately classify detected objects. In this work, we focus on how to
effectively utilize object-level information from images to boost the
performance of point-based 3D detector. We present DeMF, a simple yet effective
method to fuse image information into point features. Given a set of point
features and image feature maps, DeMF adaptively aggregates image features by
taking the projected 2D location of the 3D point as reference. We evaluate our
method on the challenging SUN RGB-D dataset, improving state-of-the-art results
by a large margin (+2.1 mAP@0.25 and +2.3mAP@0.5). Code is available at
https://github.com/haoy945/DeMF.",https://github.com/haoy945/DeMF,-1
SVNet: Where SO(3) Equivariance Meets Binarization on Point Cloud Representation,0.0438477,"Efficiency and robustness are increasingly needed for applications on 3D
point clouds, with the ubiquitous use of edge devices in scenarios like
autonomous driving and robotics, which often demand real-time and reliable
responses. The paper tackles the challenge by designing a general framework to
construct 3D learning architectures with SO(3) equivariance and network
binarization. However, a naive combination of equivariant networks and
binarization either causes sub-optimal computational efficiency or geometric
ambiguity. We propose to locate both scalar and vector features in our networks
to avoid both cases. Precisely, the presence of scalar features makes the major
part of the network binarizable, while vector features serve to retain rich
structural information and ensure SO(3) equivariance. The proposed approach can
be applied to general backbones like PointNet and DGCNN. Meanwhile, experiments
on ModelNet40, ShapeNet, and the real-world dataset ScanObjectNN, demonstrated
that the method achieves a great trade-off between efficiency, rotation
robustness, and accuracy. The codes are available at
https://github.com/zhuoinoulu/svnet.",https://github.com/zhuoinoulu/svnet,-1
Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models,0.0966893,"Vision-Language Pre-training (VLP) models have achieved state-of-the-art
performance in numerous cross-modal tasks. Since they are optimized to capture
the statistical properties of intra- and inter-modality, there remains risk to
learn social biases presented in the data as well. In this work, we (1)
introduce a counterfactual-based bias measurement \emph{CounterBias} to
quantify the social bias in VLP models by comparing the [MASK]ed prediction
probabilities of factual and counterfactual samples; (2) construct a novel
VL-Bias dataset including 24K image-text pairs for measuring gender bias in VLP
models, from which we observed that significant gender bias is prevalent in VLP
models; and (3) propose a VLP debiasing method \emph{FairVLP} to minimize the
difference in the [MASK]ed prediction probabilities between factual and
counterfactual image-text pairs for VLP debiasing. Although CounterBias and
FairVLP focus on social bias, they are generalizable to serve as tools and
provide new insights to probe and regularize more knowledge in VLP models.",https://github.com/VL-Bias/VL-Bias,-1
Interpretable Molecular Graph Generation via Monotonic Constraints,0.232702,"Designing molecules with specific properties is a long-lasting research
problem and is central to advancing crucial domains such as drug discovery and
material science. Recent advances in deep graph generative models treat
molecule design as graph generation problems which provide new opportunities
toward the breakthrough of this long-lasting problem. Existing models, however,
have many shortcomings, including poor interpretability and controllability
toward desired molecular properties. This paper focuses on new methodologies
for molecule generation with interpretable and controllable deep generative
models, by proposing new monotonically-regularized graph variational
autoencoders. The proposed models learn to represent the molecules with latent
variables and then learn the correspondence between them and molecule
properties parameterized by polynomial functions. To further improve the
intepretability and controllability of molecule generation towards desired
properties, we derive new objectives which further enforce monotonicity of the
relation between some latent variables and target molecule properties such as
toxicity and clogP. Extensive experimental evaluation demonstrates the
superiority of the proposed framework on accuracy, novelty, disentanglement,
and control towards desired molecular properties. The code is open-source at
https://anonymous.4open.science/r/MDVAE-FD2C.",None,-1
The Art of Prompting: Event Detection based on Type Specific Prompts,0.129878,"We compare various forms of prompts to represent event types and develop a
unified framework to incorporate the event type specific prompts for
supervised, few-shot, and zero-shot event detection. The experimental results
demonstrate that a well-defined and comprehensive event type prompt can
significantly improve the performance of event detection, especially when the
annotated data is scarce (few-shot event detection) or not available (zero-shot
event detection). By leveraging the semantics of event types, our unified
framework shows up to 24.3\% F-score gain over the previous state-of-the-art
baselines.",None,-1
CLIPascene: Scene Sketching with Different Types and Levels of Abstraction,0.33935,"In this paper, we present a method for converting a given scene image into a
sketch using different types and multiple levels of abstraction. We distinguish
between two types of abstraction. The first considers the fidelity of the
sketch, varying its representation from a more precise portrayal of the input
to a looser depiction. The second is defined by the visual simplicity of the
sketch, moving from a detailed depiction to a sparse sketch. Using an explicit
disentanglement into two abstraction axes -- and multiple levels for each one
-- provides users additional control over selecting the desired sketch based on
their personal goals and preferences. To form a sketch at a given level of
fidelity and simplification, we train two MLP networks. The first network
learns the desired placement of strokes, while the second network learns to
gradually remove strokes from the sketch without harming its recognizability
and semantics. Our approach is able to generate sketches of complex scenes
including those with complex backgrounds (e.g., natural and urban settings) and
subjects (e.g., animals and people) while depicting gradual abstractions of the
input scene in terms of fidelity and simplicity.",None,-1
Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers,0.0295777,"Existing Math Word Problem (MWP) solvers have achieved high accuracy on
benchmark datasets. However, prior works have shown that such solvers do not
generalize well and rely on superficial cues to achieve high performance. In
this paper, we first conduct experiments to showcase that this behaviour is
mainly associated with the limited size and diversity present in existing MWP
datasets. Next, we propose several data augmentation techniques broadly
categorized into Substitution and Paraphrasing based methods. By deploying
these methods we increase the size of existing datasets by five folds.
Extensive experiments on two benchmark datasets across three state-of-the-art
MWP solvers show that proposed methods increase the generalization and
robustness of existing solvers. On average, proposed methods significantly
increase the state-of-the-art results by over five percentage points on
benchmark datasets. Further, the solvers trained on the augmented dataset
perform comparatively better on the challenge test set. We also show the
effectiveness of proposed techniques through ablation studies and verify the
quality of augmented samples through human evaluation.",https://github.com/kevivk/MWP-Augmentation,-1
Moving Window Regression: A Novel Approach to Ordinal Regression,0.253524,"A novel ordinal regression algorithm, called moving window regression (MWR),
is proposed in this paper. First, we propose the notion of relative rank
($\rho$-rank), which is a new order representation scheme for input and
reference instances. Second, we develop global and local relative regressors
($\rho$-regressors) to predict $\rho$-ranks within entire and specific rank
ranges, respectively. Third, we refine an initial rank estimate iteratively by
selecting two reference instances to form a search window and then estimating
the $\rho$-rank within the window. Extensive experiments results show that the
proposed algorithm achieves the state-of-the-art performances on various
benchmark datasets for facial age estimation and historical color image
classification. The codes are available at https://github.com/nhshin-mcl/MWR.",None,-1
SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller,0.0830159,"In this paper, we propose a new task of sub-event generation for an unseen
process to evaluate the understanding of the coherence of sub-event actions and
objects. To solve the problem, we design SubeventWriter, a sub-event sequence
generation framework with a coherence controller. Given an unseen process, the
framework can iteratively construct the sub-event sequence by generating one
sub-event at each iteration. We also design a very effective coherence
controller to decode more coherent sub-events. As our extensive experiments and
analysis indicate, SubeventWriter can generate more reliable and meaningful
sub-event sequences for unseen processes.",https://github.com/HKUST-KnowComp/SubeventWriter,-1
Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics,0.0,"Recent work incorporates pre-trained word embeddings such as BERT embeddings
into Neural Topic Models (NTMs), generating highly coherent topics. However,
with high-quality contextualized document representations, do we really need
sophisticated neural models to obtain coherent and interpretable topics? In
this paper, we conduct thorough experiments showing that directly clustering
high-quality sentence embeddings with an appropriate word selecting method can
generate more coherent and diverse topics than NTMs, achieving also higher
efficiency and simplicity.",https://github.com/hyintell/topicx,-1
Learning Local Displacements for Point Cloud Completion,0.221068,"We propose a novel approach aimed at object and semantic scene completion
from a partial scan represented as a 3D point cloud. Our architecture relies on
three novel layers that are used successively within an encoder-decoder
structure and specifically developed for the task at hand. The first one
carries out feature extraction by matching the point features to a set of
pre-trained local descriptors. Then, to avoid losing individual descriptors as
part of standard operations such as max-pooling, we propose an alternative
neighbor-pooling operation that relies on adopting the feature vectors with the
highest activations. Finally, up-sampling in the decoder modifies our feature
extraction in order to increase the output dimension. While this model is
already able to achieve competitive results with the state of the art, we
further propose a way to increase the versatility of our approach to process
point clouds. To this aim, we introduce a second model that assembles our
layers within a transformer architecture. We evaluate both architectures on
object and indoor scene completion tasks, achieving state-of-the-art
performance.",None,-1
Modeling Label Correlations for Ultra-Fine Entity Typing with Neural Pairwise Conditional Random Field,0.0281878,"Ultra-fine entity typing (UFET) aims to predict a wide range of type phrases
that correctly describe the categories of a given entity mention in a sentence.
Most recent works infer each entity type independently, ignoring the
correlations between types, e.g., when an entity is inferred as a president, it
should also be a politician and a leader. To this end, we use an undirected
graphical model called pairwise conditional random field (PCRF) to formulate
the UFET problem, in which the type variables are not only unarily influenced
by the input but also pairwisely relate to all the other type variables. We use
various modern backbones for entity typing to compute unary potentials, and
derive pairwise potentials from type phrase representations that both capture
prior semantic information and facilitate accelerated inference. We use
mean-field variational inference for efficient type inference on very large
type sets and unfold it as a neural network module to enable end-to-end
training. Experiments on UFET show that the Neural-PCRF consistently
outperforms its backbones with little cost and results in a competitive
performance against cross-encoder based SOTA while being thousands of times
faster. We also find Neural- PCRF effective on a widely used fine-grained
entity typing dataset with a smaller type set. We pack Neural-PCRF as a network
module that can be plugged onto multi-label type classifiers with ease and
release it in https://github.com/modelscope/adaseq/tree/master/examples/NPCRF.",https://github.com/modelscope/adaseq/tree/master/examples/NPCRF,-1
Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer,0.0162192,"In Neural Machine Translation (NMT), each token prediction is conditioned on
the source sentence and the target prefix (what has been previously translated
at a decoding step). However, previous work on interpretability in NMT has
mainly focused solely on source sentence tokens' attributions. Therefore, we
lack a full understanding of the influences of every input token (source
sentence and target prefix) in the model predictions. In this work, we propose
an interpretability method that tracks input tokens' attributions for both
contexts. Our method, which can be extended to any encoder-decoder
Transformer-based model, allows us to better comprehend the inner workings of
current NMT models. We apply the proposed method to both bilingual and
multilingual Transformers and present insights into their behaviour.",https://github.com/mt-upc/,-1
An Online Approach to Solve the Dynamic Vehicle Routing Problem with Stochastic Trip Requests for Paratransit Services,0.071687,"Many transit agencies operating paratransit and microtransit services have to
respond to trip requests that arrive in real-time, which entails solving hard
combinatorial and sequential decision-making problems under uncertainty. To
avoid decisions that lead to significant inefficiency in the long term,
vehicles should be allocated to requests by optimizing a non-myopic utility
function or by batching requests together and optimizing a myopic utility
function. While the former approach is typically offline, the latter can be
performed online. We point out two major issues with such approaches when
applied to paratransit services in practice. First, it is difficult to batch
paratransit requests together as they are temporally sparse. Second, the
environment in which transit agencies operate changes dynamically (e.g.,
traffic conditions), causing estimates that are learned offline to become
stale. To address these challenges, we propose a fully online approach to solve
the dynamic vehicle routing problem (DVRP) with time windows and stochastic
trip requests that is robust to changing environmental dynamics by
construction. We focus on scenarios where requests are relatively sparse - our
problem is motivated by applications to paratransit services. We formulate DVRP
as a Markov decision process and use Monte Carlo tree search to evaluate
actions for any given state. Accounting for stochastic requests while
optimizing a non-myopic utility function is computationally challenging;
indeed, the action space for such a problem is intractably large in practice.
To tackle the large action space, we leverage the structure of the problem to
design heuristics that can sample promising actions for the tree search. Our
experiments using real-world data from our partner agency show that the
proposed approach outperforms existing state-of-the-art approaches both in
terms of performance and robustness.",https://github.com/smarttransit-ai/iccps-2022-paratransit-public,-1
SC6D: Symmetry-agnostic and Correspondence-free 6D Object Pose Estimation,0.133818,"This paper presents an efficient symmetry-agnostic and correspondence-free
framework, referred to as SC6D, for 6D object pose estimation from a single
monocular RGB image. SC6D requires neither the 3D CAD model of the object nor
any prior knowledge of the symmetries. The pose estimation is decomposed into
three sub-tasks: a) object 3D rotation representation learning and matching; b)
estimation of the 2D location of the object center; and c) scale-invariant
distance estimation (the translation along the z-axis) via classification. SC6D
is evaluated on three benchmark datasets, T-LESS, YCB-V, and ITODD, and results
in state-of-the-art performance on the T-LESS dataset. Moreover, SC6D is
computationally much more efficient than the previous state-of-the-art method
SurfEmb. The implementation and pre-trained models are publicly available at
https://github.com/dingdingcai/SC6D-pose.",None,-1
IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for Indian Languages,0.146728,"The rapid growth of machine translation (MT) systems has necessitated
comprehensive studies to meta-evaluate evaluation metrics being used, which
enables a better selection of metrics that best reflect MT quality.
Unfortunately, most of the research focuses on high-resource languages, mainly
English, the observations for which may not always apply to other languages.
Indian languages, having over a billion speakers, are linguistically different
from English, and to date, there has not been a systematic study of evaluating
MT systems from English into Indian languages. In this paper, we fill this gap
by creating an MQM dataset consisting of 7000 fine-grained annotations,
spanning 5 Indian languages and 7 MT systems, and use it to establish
correlations between annotator scores and scores obtained using existing
automatic metrics. Our results show that pre-trained metrics, such as COMET,
have the highest correlations with annotator scores. Additionally, we find that
the metrics do not adequately capture fluency-based errors in Indian languages,
and there is a need to develop metrics focused on Indian languages. We hope
that our dataset and analysis will help promote further research in this area.",https://github.com/AI4Bharat/IndicMT-Eval,-1
Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction,0.14687,"Multimodal named entity recognition and relation extraction (MNER and MRE) is
a fundamental and crucial branch in information extraction. However, existing
approaches for MNER and MRE usually suffer from error sensitivity when
irrelevant object images incorporated in texts. To deal with these issues, we
propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for
visual-enhanced entity and relation extraction, aiming to achieve more
effective and robust performance. Specifically, we regard visual representation
as pluggable visual prefix to guide the textual representation for error
insensitive forecasting decision. We further propose a dynamic gated
aggregation strategy to achieve hierarchical multi-scaled visual features as
visual prefix for fusion. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our method, and achieve state-of-the-art
performance. Code is available in https://github.com/zjunlp/HVPNeT.",https://github.com/zjunlp/HVPNeT,-1
A dynamic programming algorithm for span-based nested named-entity recognition in O(n^2),0.0506623,"Span-based nested named-entity recognition (NER) has a cubic-time complexity
using a variant of the CYK algorithm. We show that by adding a supplementary
structural constraint on the search space, nested NER has a quadratic-time
complexity, that is the same asymptotic complexity than the non-nested case.
The proposed algorithm covers a large part of three standard English benchmarks
and delivers comparable experimental results.",None,-1
Learning to Reverse DNNs from AI Programs Automatically,0.0975399,"With the privatization deployment of DNNs on edge devices, the security of
on-device DNNs has raised significant concern. To quantify the model leakage
risk of on-device DNNs automatically, we propose NNReverse, the first
learning-based method which can reverse DNNs from AI programs without domain
knowledge. NNReverse trains a representation model to represent the semantics
of binary code for DNN layers. By searching the most similar function in our
database, NNReverse infers the layer type of a given function's binary code. To
represent assembly instructions semantics precisely, NNReverse proposes a more
fine-grained embedding model to represent the textual and structural-semantic
of assembly functions.",None,-1
The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization,0.146852,"We present a novel benchmark and associated evaluation metrics for assessing
the performance of text anonymization methods. Text anonymization, defined as
the task of editing a text document to prevent the disclosure of personal
information, currently suffers from a shortage of privacy-oriented annotated
text resources, making it difficult to properly evaluate the level of privacy
protection offered by various anonymization methods. This paper presents TAB
(Text Anonymization Benchmark), a new, open-source annotated corpus developed
to address this shortage. The corpus comprises 1,268 English-language court
cases from the European Court of Human Rights (ECHR) enriched with
comprehensive annotations about the personal information appearing in each
document, including their semantic category, identifier type, confidential
attributes, and co-reference relations. Compared to previous work, the TAB
corpus is designed to go beyond traditional de-identification (which is limited
to the detection of predefined semantic categories), and explicitly marks which
text spans ought to be masked in order to conceal the identity of the person to
be protected. Along with presenting the corpus and its annotation layers, we
also propose a set of evaluation metrics that are specifically tailored towards
measuring the performance of text anonymization, both in terms of privacy
protection and utility preservation. We illustrate the use of the benchmark and
the proposed metrics by assessing the empirical performance of several baseline
text anonymization models. The full corpus along with its privacy-oriented
annotation guidelines, evaluation scripts and baseline models are available on:
https://github.com/NorskRegnesentral/text-anonymisation-benchmark",https://github.com/NorskRegnesentral/text-anonymization-benchmark,-1
LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification,0.118899,"Weakly supervised text classification methods typically train a deep neural
classifier based on pseudo-labels. The quality of pseudo-labels is crucial to
final performance but they are inevitably noisy due to their heuristic nature,
so selecting the correct ones has a huge potential for performance boost. One
straightforward solution is to select samples based on the softmax probability
scores in the neural classifier corresponding to their pseudo-labels. However,
we show through our experiments that such solutions are ineffective and
unstable due to the erroneously high-confidence predictions from poorly
calibrated models. Recent studies on the memorization effects of deep neural
models suggest that these models first memorize training samples with clean
labels and then those with noisy labels. Inspired by this observation, we
propose a novel pseudo-label selection method LOPS that takes learning order of
samples into consideration. We hypothesize that the learning order reflects the
probability of wrong annotation in terms of ranking, and therefore, propose to
select the samples that are learnt earlier. LOPS can be viewed as a strong
performance-boost plug-in to most of existing weakly-supervised text
classification methods, as confirmed in extensive experiments on four
real-world datasets.",https://github.com/dheeraj7596/LOPS,-1
Peano: Learning Formal Mathematical Reasoning,0.319545,"General mathematical reasoning is computationally undecidable, but humans
routinely solve new problems. Moreover, discoveries developed over centuries
are taught to subsequent generations quickly. What structure enables this, and
how might that inform automated mathematical reasoning? We posit that central
to both puzzles is the structure of procedural abstractions underlying
mathematics. We explore this idea in a case study on 5 sections of beginning
algebra on the Khan Academy platform. To define a computational foundation, we
introduce Peano, a theorem-proving environment where the set of valid actions
at any point is finite. We use Peano to formalize introductory algebra problems
and axioms, obtaining well-defined search problems. We observe existing
reinforcement learning methods for symbolic reasoning to be insufficient to
solve harder problems. Adding the ability to induce reusable abstractions
(""tactics"") from its own solutions allows an agent to make steady progress,
solving all problems. Furthermore, these abstractions induce an order to the
problems, seen at random during training. The recovered order has significant
agreement with the expert-designed Khan Academy curriculum, and
second-generation agents trained on the recovered curriculum learn
significantly faster. These results illustrate the synergistic role of
abstractions and curricula in the cultural transmission of mathematics.",https://github.com/gpoesia/peano,-1
Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models,0.0611207,"We investigate what kind of structural knowledge learned in neural network
encoders is transferable to processing natural language. We design artificial
languages with structural properties that mimic natural language, pretrain
encoders on the data, and see how much performance the encoder exhibits on
downstream tasks in natural language. Our experimental results show that
pretraining with an artificial language with a nesting dependency structure
provides some knowledge transferable to natural language. A follow-up probing
analysis indicates that its success in the transfer is related to the amount of
encoded contextual information and what is transferred is the knowledge of
position-aware context dependence of language. Our results provide insights
into how neural network encoders process human languages and the source of
cross-lingual transferability of recent multilingual language models.",https://github.com/moses-smt/mosesdecoder,-1
Word-Level Fine-Grained Story Visualization,0.268599,"Story visualization aims to generate a sequence of images to narrate each
sentence in a multi-sentence story with a global consistency across dynamic
scenes and characters. Current works still struggle with output images' quality
and consistency, and rely on additional semantic information or auxiliary
captioning networks. To address these challenges, we first introduce a new
sentence representation, which incorporates word information from all story
sentences to mitigate the inconsistency problem. Then, we propose a new
discriminator with fusion features and further extend the spatial attention to
improve image quality and story consistency. Extensive experiments on different
datasets and human evaluation demonstrate the superior performance of our
approach, compared to state-of-the-art methods, neither using segmentation
masks nor auxiliary captioning networks.",https://github.com/mrlibw/Word-Level-Story-Visualization,-1
CrossRE: A Cross-Domain Dataset for Relation Extraction,0.211341,"Relation Extraction (RE) has attracted increasing attention, but current RE
evaluation is limited to in-domain evaluation setups. Little is known on how
well a RE system fares in challenging, but realistic out-of-distribution
evaluation setups. To address this gap, we propose CrossRE, a new,
freely-available cross-domain benchmark for RE, which comprises six distinct
text domains and includes multi-label annotations. An additional innovation is
that we release meta-data collected during annotation, to include explanations
and flags of difficult instances. We provide an empirical evaluation with a
state-of-the-art model for relation classification. As the meta-data enables us
to shed new light on the state-of-the-art model, we provide a comprehensive
analysis on the impact of difficult cases and find correlations between model
and human annotations. Overall, our empirical investigation highlights the
difficulty of cross-domain RE. We release our dataset, to spur more research in
this direction.",https://github.com/mainlp/CrossRE,-1
SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems,0.611943,"We design deep neural networks (DNNs) and corresponding networks' splittings
to distribute DNNs' workload to camera sensors and a centralized aggregator on
head mounted devices to meet system performance targets in inference accuracy
and latency under the given hardware resource constraints. To achieve an
optimal balance among computation, communication, and performance, a
split-aware neural architecture search framework, SplitNets, is introduced to
conduct model designing, splitting, and communication reduction simultaneously.
We further extend the framework to multi-view systems for learning to fuse
inputs from multiple camera sensors with optimal performance and systemic
efficiency. We validate SplitNets for single-view system on ImageNet as well as
multi-view system on 3D classification, and show that the SplitNets framework
achieves state-of-the-art (SOTA) performance and system latency compared with
existing approaches.",None,-1
Cost-Effective Online Contextual Model Selection,0.0434777,"How can we collect the most useful labels to learn a model selection policy,
when presented with arbitrary heterogeneous data streams? In this paper, we
formulate this task as an online contextual active model selection problem,
where at each round the learner receives an unlabeled data point along with a
context. The goal is to output the best model for any given context without
obtaining an excessive amount of labels. In particular, we focus on the task of
selecting pre-trained classifiers, and propose a contextual active model
selection algorithm (CAMS), which relies on a novel uncertainty sampling query
criterion defined on a given policy class for adaptive model selection. In
comparison to prior art, our algorithm does not assume a globally optimal
model. We provide rigorous theoretical analysis for the regret and query
complexity under both adversarial and stochastic settings. Our experiments on
several benchmark classification datasets demonstrate the algorithm's
effectiveness in terms of both regret and query complexity. Notably, to achieve
the same accuracy, CAMS incurs less than 10% of the label cost when compared to
the best online model selection baselines on CIFAR10.",None,-1
Streaming Radiance Fields for 3D Video Synthesis,0.291926,"We present an explicit-grid based method for efficiently reconstructing
streaming radiance fields for novel view synthesis of real world dynamic
scenes. Instead of training a single model that combines all the frames, we
formulate the dynamic modeling problem with an incremental learning paradigm in
which per-frame model difference is trained to complement the adaption of a
base model on the current frame. By exploiting the simple yet effective tuning
strategy with narrow bands, the proposed method realizes a feasible framework
for handling video sequences on-the-fly with high training efficiency. The
storage overhead induced by using explicit grid representations can be
significantly reduced through the use of model difference based compression. We
also introduce an efficient strategy to further accelerate model optimization
for each frame. Experiments on challenging video sequences demonstrate that our
approach is capable of achieving a training speed of 15 seconds per-frame with
competitive rendering quality, which attains $1000 \times$ speedup over the
state-of-the-art implicit methods. Code is available at
https://github.com/AlgoHunt/StreamRF.",https://github.com/AlgoHunt/StreamRF,-1
Fine-grained Semantic Alignment Network for Weakly Supervised Temporal Language Grounding,0.370486,"Temporal language grounding (TLG) aims to localize a video segment in an
untrimmed video based on a natural language description. To alleviate the
expensive cost of manual annotations for temporal boundary labels, we are
dedicated to the weakly supervised setting, where only video-level descriptions
are provided for training. Most of the existing weakly supervised methods
generate a candidate segment set and learn cross-modal alignment through a
MIL-based framework. However, the temporal structure of the video as well as
the complicated semantics in the sentence are lost during the learning. In this
work, we propose a novel candidate-free framework: Fine-grained Semantic
Alignment Network (FSAN), for weakly supervised TLG. Instead of view the
sentence and candidate moments as a whole, FSAN learns token-by-clip
cross-modal semantic alignment by an iterative cross-modal interaction module,
generates a fine-grained cross-modal semantic alignment map, and performs
grounding directly on top of the map. Extensive experiments are conducted on
two widely-used benchmarks: ActivityNet-Captions, and DiDeMo, where our FSAN
achieves state-of-the-art performance.",None,-1
Event-based Monocular Dense Depth Estimation with Recurrent Transformers,0.0816537,"Event cameras, offering high temporal resolutions and high dynamic ranges,
have brought a new perspective to address common challenges (e.g., motion blur
and low light) in monocular depth estimation. However, how to effectively
exploit the sparse spatial information and rich temporal cues from asynchronous
events remains a challenging endeavor. To this end, we propose a novel
event-based monocular depth estimator with recurrent transformers, namely
EReFormer, which is the first pure transformer with a recursive mechanism to
process continuous event streams. Technically, for spatial modeling, a novel
transformer-based encoder-decoder with a spatial transformer fusion module is
presented, having better global context information modeling capabilities than
CNN-based methods. For temporal modeling, we design a gate recurrent vision
transformer unit that introduces a recursive mechanism into transformers,
improving temporal modeling capabilities while alleviating the expensive GPU
memory cost. The experimental results show that our EReFormer outperforms
state-of-the-art methods by a margin on both synthetic and real-world datasets.
We hope that our work will attract further research to develop stunning
transformers in the event-based vision community. Our open-source code can be
found in the supplemental material.",None,-1
"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",0.128493,"We introduce RoMQA, the first benchmark for robust, multi-evidence,
multi-answer question answering (QA). RoMQA contains clusters of questions that
are derived from related constraints mined from the Wikidata knowledge graph.
RoMQA evaluates robustness of QA models to varying constraints by measuring
worst-case performance within each question cluster. Compared to prior QA
datasets, RoMQA has more human-written questions that require reasoning over
more evidence text and have, on average, many more correct answers. In
addition, human annotators rate RoMQA questions as more natural or likely to be
asked by people. We evaluate state-of-the-art large language models in
zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is
challenging: zero-shot and few-shot models perform similarly to naive
baselines, while supervised retrieval methods perform well below gold evidence
upper bounds. Moreover, existing models are not robust to variations in
question constraints, but can be made more robust by tuning on clusters of
related questions. Our results show that RoMQA is a challenging benchmark for
large language models, and provides a quantifiable test to build more robust QA
methods.",https://github.com/facebookresearch/romqa,-1
Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations,0.183813,"Although large language models can be prompted for both zero- and few-shot
learning, performance drops significantly when no demonstrations are available.
In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap
by constructing pseudo-demonstrations for a given test input using a raw text
corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the
nearest neighbors to the test input from the corpus and pairing them with
random task labels, and (2) applying a set of techniques to reduce the amount
of direct copying the model does from the resulting demonstrations. Evaluation
on nine classification datasets shows that Z-ICL outperforms previous zero-shot
methods by a significant margin, and is on par with in-context learning with
labeled training data in the few-shot setting. Overall, Z-ICL provides a
significantly higher estimate of the zero-shot performance levels of a model,
and supports future efforts to develop better pseudo-demonstrations that
further improve zero-shot results.",https://github.com/alrope123/z-icl,-1
Audio-driven Neural Gesture Reenactment with Video Motion Graphs,0.0978823,"Human speech is often accompanied by body gestures including arm and hand
gestures. We present a method that reenacts a high-quality video with gestures
matching a target speech audio. The key idea of our method is to split and
re-assemble clips from a reference video through a novel video motion graph
encoding valid transitions between clips. To seamlessly connect different clips
in the reenactment, we propose a pose-aware video blending network which
synthesizes video frames around the stitched frames between two clips.
Moreover, we developed an audio-based gesture searching algorithm to find the
optimal order of the reenacted frames. Our system generates reenactments that
are consistent with both the audio rhythms and the speech content. We evaluate
our synthesized video quality quantitatively, qualitatively, and with user
studies, demonstrating that our method produces videos of much higher quality
and consistency with the target audio compared to previous work and baselines.",https://yzhou359.github.io/video_reenact,-1
C2F-TCN: A Framework for Semi and Fully Supervised Temporal Action Segmentation,0.0803722,"Temporal action segmentation tags action labels for every frame in an input
untrimmed video containing multiple actions in a sequence. For the task of
temporal action segmentation, we propose an encoder-decoder-style architecture
named C2F-TCN featuring a ""coarse-to-fine"" ensemble of decoder outputs. The
C2F-TCN framework is enhanced with a novel model agnostic temporal feature
augmentation strategy formed by the computationally inexpensive strategy of the
stochastic max-pooling of segments. It produces more accurate and
well-calibrated supervised results on three benchmark action segmentation
datasets. We show that the architecture is flexible for both supervised and
representation learning. In line with this, we present a novel unsupervised way
to learn frame-wise representation from C2F-TCN. Our unsupervised learning
approach hinges on the clustering capabilities of the input features and the
formation of multi-resolution features from the decoder's implicit structure.
Further, we provide the first semi-supervised temporal action segmentation
results by merging representation learning with conventional supervised
learning. Our semi-supervised learning scheme, called
``Iterative-Contrastive-Classify (ICC)'', progressively improves in performance
with more labeled data. The ICC semi-supervised learning in C2F-TCN, with 40%
labeled videos, performs similar to fully supervised counterparts.",None,-1
StegaNeRF: Embedding Invisible Information within Neural Radiance Fields,0.210526,"Recent advances in neural rendering imply a future of widespread visual data
distributions through sharing NeRF model weights. However, while common visual
data (images and videos) have standard approaches to embed ownership or
copyright information explicitly or subtly, the problem remains unexplored for
the emerging NeRF format. We present StegaNeRF, a method for steganographic
information embedding in NeRF renderings. We design an optimization framework
allowing accurate hidden information extractions from images rendered by NeRF,
while preserving its original visual quality. We perform experimental
evaluations of our method under several potential deployment scenarios, and we
further discuss the insights discovered through our analysis. StegaNeRF
signifies an initial exploration into the novel problem of instilling
customizable, imperceptible, and recoverable information to NeRF renderings,
with minimal impact to rendered images. Project page:
https://xggnet.github.io/StegaNeRF/.",None,-1
Mapping Husserlian phenomenology onto active inference,0.0832276,"Phenomenology is the rigorous descriptive study of conscious experience.
Recent attempts to formalize Husserlian phenomenology provide us with a
mathematical model of perception as a function of prior knowledge and
expectation. In this paper, we re-examine elements of Husserlian phenomenology
through the lens of active inference. In doing so, we aim to advance the
project of computational phenomenology, as recently outlined by proponents of
active inference. We propose that key aspects of Husserl's descriptions of
consciousness can be mapped onto aspects of the generative models associated
with the active inference approach. We first briefly review active inference.
We then discuss Husserl's phenomenology, with a focus on time consciousness.
Finally, we present our mapping from Husserlian phenomenology to active
inference.",None,-1
Efficient yet Competitive Speech Translation: FBK@IWSLT2022,0.0904627,"The primary goal of this FBK's systems submission to the IWSLT 2022 offline
and simultaneous speech translation tasks is to reduce model training costs
without sacrificing translation quality. As such, we first question the need of
ASR pre-training, showing that it is not essential to achieve competitive
results. Second, we focus on data filtering, showing that a simple method that
looks at the ratio between source and target characters yields a quality
improvement of 1 BLEU. Third, we compare different methods to reduce the
detrimental effect of the audio segmentation mismatch between training data
manually segmented at sentence level and inference data that is automatically
segmented. Towards the same goal of training cost reduction, we participate in
the simultaneous task with the same model trained for offline ST. The
effectiveness of our lightweight training strategy is shown by the high score
obtained on the MuST-C en-de corpus (26.7 BLEU) and is confirmed in
high-resource data conditions by a 1.6 BLEU improvement on the IWSLT2020 test
set over last year's winning system.",https://github.com/hlt-mt/FBK-fairseq,-1
Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model,0.119734,"The recently released NLLB-200 is a set of multilingual Neural Machine
Translation models that cover 202 languages. The largest model is based on a
Mixture of Experts architecture and achieves SoTA results across many language
pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just
for inference. In this work, we propose a pruning method that enables the
removal of up to 80% of experts without further finetuning and with a
negligible loss in translation quality, which makes it feasible to run the
model on a single 32GB GPU. Further analysis suggests that our pruning metrics
can identify language-specific experts.",None,-1
Specialized Re-Ranking: A Novel Retrieval-Verification Framework for Cloth Changing Person Re-Identification,0.0998202,"Cloth changing person re-identification(Re-ID) can work under more
complicated scenarios with higher security than normal Re-ID and biometric
techniques and is therefore extremely valuable in applications. Meanwhile,
higher flexibility in appearance always leads to more similar-looking confusing
images, which is the weakness of the widely used retrieval methods. In this
work, we shed light on how to handle these similar images. Specifically, we
propose a novel retrieval-verification framework. Given an image, the retrieval
module can search for similar images quickly. Our proposed verification network
will then compare the input image and the candidate images by contrasting those
local details and give a similarity score. An innovative ranking strategy is
also introduced to take a good balance between retrieval and verification
results. Comprehensive experiments are conducted to show the effectiveness of
our framework and its capability in improving the state-of-the-art methods
remarkably on both synthetic and realistic datasets.",None,-1
Approximate Differentiable Rendering with Algebraic Surfaces,0.419636,"Differentiable renderers provide a direct mathematical link between an
object's 3D representation and images of that object. In this work, we develop
an approximate differentiable renderer for a compact, interpretable
representation, which we call Fuzzy Metaballs. Our approximate renderer focuses
on rendering shapes via depth maps and silhouettes. It sacrifices fidelity for
utility, producing fast runtimes and high-quality gradient information that can
be used to solve vision tasks. Compared to mesh-based differentiable renderers,
our method has forward passes that are 5x faster and backwards passes that are
30x faster. The depth maps and silhouette images generated by our method are
smooth and defined everywhere. In our evaluation of differentiable renderers
for pose estimation, we show that our method is the only one comparable to
classic techniques. In shape from silhouette, our method performs well using
only gradient descent and a per-pixel loss, without any surrogate losses or
regularization. These reconstructions work well even on natural video sequences
with segmentation artifacts. Project page:
https://leonidk.github.io/fuzzy-metaballs",None,-1
DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta,0.864665,"Learning to generate new images for a novel category based on only a few
images, named as few-shot image generation, has attracted increasing research
interest. Several state-of-the-art works have yielded impressive results, but
the diversity is still limited. In this work, we propose a novel Delta
Generative Adversarial Network (DeltaGAN), which consists of a reconstruction
subnetwork and a generation subnetwork. The reconstruction subnetwork captures
intra-category transformation, i.e., delta, between same-category pairs. The
generation subnetwork generates sample-specific delta for an input image, which
is combined with this input image to generate a new image within the same
category. Besides, an adversarial delta matching loss is designed to link the
above two subnetworks together. Extensive experiments on six benchmark datasets
demonstrate the effectiveness of our proposed method. Our code is available at
https://github.com/bcmi/DeltaGAN-Few-Shot-Image-Generation.",None,-1
Compositional Law Parsing with Latent Random Functions,0.0400735,"Human cognition has compositionality. We understand a scene by decomposing
the scene into different concepts (e.g., shape and position of an object) and
learning the respective laws of these concepts, which may be either natural
(e.g., laws of motion) or man-made (e.g., laws of a game). The automatic
parsing of these laws indicates the model's ability to understand the scene,
which makes law parsing play a central role in many visual tasks. This paper
proposes a deep latent variable model for Compositional LAw Parsing (CLAP),
which achieves the human-like compositionality ability through an
encoding-decoding architecture to represent concepts in the scene as latent
variables. CLAP employs concept-specific latent random functions instantiated
with Neural Processes to capture the law of concepts. Our experimental results
demonstrate that CLAP outperforms the baseline methods in multiple visual tasks
such as intuitive physics, abstract visual reasoning, and scene representation.
The law manipulation experiments illustrate CLAP's interpretability by
modifying specific latent random functions on samples. For example, CLAP learns
the laws of position-changing and appearance constancy from the moving balls in
a scene, making it possible to exchange laws between samples or compose
existing laws into novel laws.",https://github.com/FudanVI/generative-abstract-reasoning/tree/main/clap,-1
Algorithmic progress in computer vision,0.022614,"We investigate algorithmic progress in image classification on ImageNet,
perhaps the most well-known test bed for computer vision. We estimate a model,
informed by work on neural scaling laws, and infer a decomposition of progress
into the scaling of compute, data, and algorithms. Using Shapley values to
attribute performance improvements, we find that algorithmic improvements have
been roughly as important as the scaling of compute for progress computer
vision. Our estimates indicate that algorithmic innovations mostly take the
form of compute-augmenting algorithmic advances (which enable researchers to
get better performance from less compute), not data-augmenting algorithmic
advances. We find that compute-augmenting algorithmic advances are made at a
pace more than twice as fast as the rate usually associated with Moore's law.
In particular, we estimate that compute-augmenting innovations halve compute
requirements every nine months (95\% confidence interval: 4 to 25 months).",None,-1
BILP-Q: Quantum Coalition Structure Generation,0.285889,"Quantum AI is an emerging field that uses quantum computing to solve typical
complex problems in AI. In this work, we propose BILP-Q, the first-ever general
quantum approach for solving the Coalition Structure Generation problem (CSGP),
which is notably NP-hard. In particular, we reformulate the CSGP in terms of a
Quadratic Binary Combinatorial Optimization (QUBO) problem to leverage existing
quantum algorithms (e.g., QAOA) to obtain the best coalition structure. Thus,
we perform a comparative analysis in terms of time complexity between the
proposed quantum approach and the most popular classical baselines.
Furthermore, we consider standard benchmark distributions for coalition values
to test the BILP-Q on small-scale experiments using the IBM Qiskit environment.
Finally, since QUBO problems can be solved operating with quantum annealing, we
run BILP-Q on medium-size problems using a real quantum annealer (D-Wave).",https://github.com/supreethmv/BILP-Q,-1
Continual Sequence Generation with Adaptive Compositional Modules,0.0582647,"Continual learning is essential for real-world deployment when there is a
need to quickly adapt the model to new tasks without forgetting knowledge of
old tasks. Existing work on continual sequence generation either always reuses
existing parameters to learn new tasks, which is vulnerable to catastrophic
forgetting on dissimilar tasks, or blindly adds new parameters for every new
task, which could prevent knowledge sharing between similar tasks. To get the
best of both worlds, in this work, we propose continual sequence generation
with adaptive compositional modules to adaptively add modules in transformer
architectures and compose both old and new modules for new tasks. We also
incorporate pseudo experience replay to facilitate knowledge transfer in those
shared modules. Experiment results on various sequences of generation tasks
show that our framework can adaptively add modules or reuse modules based on
task similarity, outperforming state-of-the-art baselines in terms of both
performance and parameter efficiency. We make our code public at
https://github.com/GT-SALT/Adaptive-Compositional-Modules.",https://github.com/GT-SALT/Adaptive-Compositional-Modules,-1
Multielement polynomial chaos Kriging-based metamodelling for Bayesian inference of non-smooth systems,0.0,"This paper presents a surrogate modelling technique based on domain
partitioning for Bayesian parameter inference of highly nonlinear engineering
models. In order to alleviate the computational burden typically involved in
Bayesian inference applications, a multielement Polynomial Chaos Expansion
based Kriging metamodel is proposed. The developed surrogate model combines in
a piecewise function an array of local Polynomial Chaos based Kriging
metamodels constructed on a finite set of non-overlapping subdomains of the
stochastic input space. Therewith, the presence of non-smoothness in the
response of the forward model (e.g.~ nonlinearities and sparseness) can be
reproduced by the proposed metamodel with minimum computational costs owing to
its local adaptation capabilities. The model parameter inference is conducted
through a Markov chain Monte Carlo approach comprising adaptive exploration and
delayed rejection. The efficiency and accuracy of the proposed approach are
validated through two case studies, including an analytical benchmark and a
numerical case study. The latter relates the partial differential equation
governing the hydrogen diffusion phenomenon of metallic materials in Thermal
Desorption Spectroscopy tests.",None,-1
Heart rate estimation in intense exercise videos,0.102459,"Estimating heart rate from video allows non-contact health monitoring with
applications in patient care, human interaction, and sports. Existing work can
robustly measure heart rate under some degree of motion by face tracking.
However, this is not always possible in unconstrained settings, as the face
might be occluded or even outside the camera. Here, we present IntensePhysio: a
challenging video heart rate estimation dataset with realistic face occlusions,
severe subject motion, and ample heart rate variation. To ensure heart rate
variation in a realistic setting we record each subject for around 1-2 hours.
The subject is exercising (at a moderate to high intensity) on a cycling
ergometer with an attached video camera and is given no instructions regarding
positioning or movement. We have 11 subjects, and approximately 20 total hours
of video. We show that the existing remote photo-plethysmography methods have
difficulty in estimating heart rate in this setting. In addition, we present
IBIS-CNN, a new baseline using spatio-temporal superpixels, which improves on
existing models by eliminating the need for a visible face/face tracking. We
will make the code and data publically available soon.",https://github.com/ynapolean/IBIS-CNN,-1
Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection,0.37889,"With the vigorous development of computer vision, oriented object detection
has gradually been featured. In this paper, a novel differentiable angle coder
named phase-shifting coder (PSC) is proposed to accurately predict the
orientation of objects, along with a dual-frequency version (PSCD). By mapping
the rotational periodicity of different cycles into the phase of different
frequencies, we provide a unified framework for various periodic fuzzy problems
caused by rotational symmetry in oriented object detection. Upon such a
framework, common problems in oriented object detection such as boundary
discontinuity and square-like problems are elegantly solved in a unified form.
Visual analysis and experiments on three datasets prove the effectiveness and
the potentiality of our approach. When facing scenarios requiring high-quality
bounding boxes, the proposed methods are expected to give a competitive
performance. The codes are publicly available at
https://github.com/open-mmlab/mmrotate.",https://github.com/open-mmlab/mmrotate,-1
Unsupervised Change Detection Based on Image Reconstruction Loss,0.0524009,"To train the change detector, bi-temporal images taken at different times in
the same area are used. However, collecting labeled bi-temporal images is
expensive and time consuming. To solve this problem, various unsupervised
change detection methods have been proposed, but they still require unlabeled
bi-temporal images. In this paper, we propose unsupervised change detection
based on image reconstruction loss using only unlabeled single temporal single
image. The image reconstruction model is trained to reconstruct the original
source image by receiving the source image and the photometrically transformed
source image as a pair. During inference, the model receives bi-temporal images
as the input, and tries to reconstruct one of the inputs. The changed region
between bi-temporal images shows high reconstruction loss. Our change detector
showed significant performance in various change detection benchmark datasets
even though only a single temporal single source image was used. The code and
trained models will be publicly available for reproducibility.",https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix,-1
Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching,0.263674,"Ontology Matching (OM) plays an important role in many domains such as
bioinformatics and the Semantic Web, and its research is becoming increasingly
popular, especially with the application of machine learning (ML) techniques.
Although the Ontology Alignment Evaluation Initiative (OAEI) represents an
impressive effort for the systematic evaluation of OM systems, it still suffers
from several limitations including limited evaluation of subsumption mappings,
suboptimal reference mappings, and limited support for the evaluation of
ML-based systems. To tackle these limitations, we introduce five new biomedical
OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes
both equivalence and subsumption matching; the quality of reference mappings is
ensured by human curation, ontology pruning, etc.; and a comprehensive
evaluation framework is proposed to measure OM performance from various
perspectives for both ML-based and non-ML-based OM systems. We report
evaluation results for OM systems of different types to demonstrate the usage
of these resources, all of which are publicly available as part of the new
BioML track at OAEI 2022.",https://github.com/KRR-Oxford/DeepOnto,-1
Improving the Adversarial Robustness of NLP Models by Information Bottleneck,0.101661,"Existing studies have demonstrated that adversarial examples can be directly
attributed to the presence of non-robust features, which are highly predictive,
but can be easily manipulated by adversaries to fool NLP models. In this study,
we explore the feasibility of capturing task-specific robust features, while
eliminating the non-robust ones by using the information bottleneck theory.
Through extensive experiments, we show that the models trained with our
information bottleneck-based method are able to achieve a significant
improvement in robust accuracy, exceeding performances of all the previously
reported defense methods while suffering almost no performance drop in clean
accuracy on SST-2, AGNEWS and IMDB datasets.",https://github.com/zhangcen456/IB,-1
Reliable Face Morphing Attack Detection in On-The-Fly Border Control Scenario with Variation in Image Resolution and Capture Distance,0.0789711,"Face Recognition Systems (FRS) are vulnerable to various attacks performed
directly and indirectly. Among these attacks, face morphing attacks are highly
potential in deceiving automatic FRS and human observers and indicate a severe
security threat, especially in the border control scenario. This work presents
a face morphing attack detection, especially in the On-The-Fly (OTF) Automatic
Border Control (ABC) scenario. We present a novel Differential-MAD (D-MAD)
algorithm based on the spherical interpolation and hierarchical fusion of deep
features computed from six different pre-trained deep Convolutional Neural
Networks (CNNs). Extensive experiments are carried out on the newly generated
face morphing dataset (SCFace-Morph) based on the publicly available SCFace
dataset by considering the real-life scenario of Automatic Border Control (ABC)
gates. Experimental protocols are designed to benchmark the proposed and
state-of-the-art (SOTA) D-MAD techniques for different camera resolutions and
capture distances. Obtained results have indicated the superior performance of
the proposed D-MAD method compared to the existing methods.",None,-1
CounTR: Transformer-based Generalised Visual Counting,0.245512,"In this paper, we consider the problem of generalised visual object counting,
with the goal of developing a computational model for counting the number of
objects from arbitrary semantic categories, using arbitrary number of
""exemplars"", i.e. zero-shot or few-shot counting. To this end, we make the
following four contributions: (1) We introduce a novel transformer-based
architecture for generalised visual object counting, termed as Counting
Transformer (CounTR), which explicitly capture the similarity between image
patches or with given ""exemplars"" with the attention mechanism;(2) We adopt a
two-stage training regime, that first pre-trains the model with self-supervised
learning, and followed by supervised fine-tuning;(3) We propose a simple,
scalable pipeline for synthesizing training images with a large number of
instances or that from different semantic categories, explicitly forcing the
model to make use of the given ""exemplars"";(4) We conduct thorough ablation
studies on the large-scale counting benchmark, e.g. FSC-147, and demonstrate
state-of-the-art performance on both zero and few-shot settings.",https://verg-avesta.github.io/CounTR_Webpage/,-1
ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts,0.455351,"This work introduces a new multi-task, parameter-efficient language model
(LM) tuning method that learns to transfer knowledge across different tasks via
a mixture of soft prompts-small prefix embedding vectors pre-trained for
different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt
Tuning), obtains source prompts as encodings of large-scale source tasks into a
small number of parameters and trains an attention module to interpolate the
source prompts and a newly initialized target prompt for every instance in the
target task. During training, only the target task prompt and the attention
weights, which are shared between tasks in multi-task training, are updated,
while the original LM and source prompts are intact. ATTEMPT is highly
parameter-efficient (e.g., updates 2,300 times fewer parameters than full
fine-tuning) while achieving high task performance using knowledge from
high-resource tasks. Moreover, it is modular using pre-trained soft prompts,
and can flexibly add or remove source prompts for effective knowledge transfer.
Our experimental results across 21 diverse NLP datasets show that ATTEMPT
significantly outperforms prompt tuning and outperforms or matches fully
fine-tuned or other parameter-efficient tuning approaches that use over ten
times more parameters. Finally, ATTEMPT outperforms previous work in few-shot
learning settings.",https://github.com/AkariAsai/ATTEMPT,-1
Can Pre-trained Language Models Interpret Similes as Smart as Human?,0.0326747,"Simile interpretation is a crucial task in natural language processing.
Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art
performance on many tasks. However, it remains under-explored whether PLMs can
interpret similes or not. In this paper, we investigate the ability of PLMs in
simile interpretation by designing a novel task named Simile Property Probing,
i.e., to let the PLMs infer the shared properties of similes. We construct our
simile property probing datasets from both general textual corpora and
human-designed questions, containing 1,633 examples covering seven main
categories. Our empirical study based on the constructed datasets shows that
PLMs can infer similes' shared properties while still underperforming humans.
To bridge the gap with human performance, we additionally design a
knowledge-enhanced training objective by incorporating the simile knowledge
into PLMs via knowledge embedding methods. Our method results in a gain of
8.58% in the probing task and 1.37% in the downstream task of sentiment
classification. The datasets and code are publicly available at
https://github.com/Abbey4799/PLMs-Interpret-Simile.",https://github.com/Abbey4799/PLMs-Interpret-Simile,-1
Deep-Attack over the Deep Reinforcement Learning,0.114301,"Recent adversarial attack developments have made reinforcement learning more
vulnerable, and different approaches exist to deploy attacks against it, where
the key is how to choose the right timing of the attack. Some work tries to
design an attack evaluation function to select critical points that will be
attacked if the value is greater than a certain threshold. This approach makes
it difficult to find the right place to deploy an attack without considering
the long-term impact. In addition, there is a lack of appropriate indicators of
assessment during attacks. To make the attacks more intelligent as well as to
remedy the existing problems, we propose the reinforcement learning-based
attacking framework by considering the effectiveness and stealthy
spontaneously, while we also propose a new metric to evaluate the performance
of the attack model in these two aspects. Experimental results show the
effectiveness of our proposed model and the goodness of our proposed evaluation
metric. Furthermore, we validate the transferability of the model, and also its
robustness under the adversarial training.",None,-1
Compilable Neural Code Generation with Compiler Feedback,0.335773,"Automatically generating compilable programs with (or without) natural
language descriptions has always been a touchstone problem for computational
linguistics and automated software engineering. Existing deep-learning
approaches model code generation as text generation, either constrained by
grammar structures in decoder, or driven by pre-trained language models on
large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of
them account for compilability of the generated programs. To improve
compilability of the generated programs, this paper proposes COMPCODER, a
three-stage pipeline utilizing compiler feedback for compilable code
generation, including language model fine-tuning, compilability reinforcement,
and compilability discrimination. Comprehensive experiments on two code
generation tasks demonstrate the effectiveness of our proposed approach,
improving the success rate of compilation from 44.18 to 89.18 in code
completion on average and from 70.3 to 96.2 in text-to-code generation,
respectively, when comparing with the state-of-the-art CodeGPT.",None,-1
On the Importance of Asymmetry for Siamese Representation Learning,0.140062,"Many recent self-supervised frameworks for visual representation learning are
based on certain forms of Siamese networks. Such networks are conceptually
symmetric with two parallel encoders, but often practically asymmetric as
numerous mechanisms are devised to break the symmetry. In this work, we conduct
a formal study on the importance of asymmetry by explicitly distinguishing the
two encoders within the network -- one produces source encodings and the other
targets. Our key insight is keeping a relatively lower variance in target than
source generally benefits learning. This is empirically justified by our
results from five case studies covering different variance-oriented designs,
and is aligned with our preliminary theoretical analysis on the baseline.
Moreover, we find the improvements from asymmetric designs generalize well to
longer training schedules, multiple other frameworks and newer backbones.
Finally, the combined effect of several asymmetric designs achieves a
state-of-the-art accuracy on ImageNet linear probing and competitive results on
downstream transfer. We hope our exploration will inspire more research in
exploiting asymmetry for Siamese representation learning.",https://github.com/facebookresearch/asym-siam,-1
Ranking-Enhanced Unsupervised Sentence Representation Learning,0.21262,"Unsupervised sentence representation learning has progressed through
contrastive learning and data augmentation methods such as dropout masking.
Despite this progress, sentence encoders are still limited to using only an
input sentence when predicting its semantic vector. In this work, we show that
the semantic meaning of a sentence is also determined by nearest-neighbor
sentences that are similar to the input sentence. Based on this finding, we
propose a novel unsupervised sentence encoder, RankEncoder. RankEncoder
predicts the semantic vector of an input sentence by leveraging its
relationship with other sentences in an external corpus, as well as the input
sentence itself. We evaluate RankEncoder on semantic textual benchmark
datasets. From the experimental results, we verify that 1) RankEncoder achieves
80.07% Spearman's correlation, a 1.1% absolute improvement compared to the
previous state-of-the-art performance, 2) RankEncoder is universally applicable
to existing unsupervised sentence embedding methods, and 3) RankEncoder is
specifically effective for predicting the similarity scores of similar sentence
pairs.",https://github.com/yeonsw/RankEncoder.git,-1
Guided Unsupervised Learning by Subaperture Decomposition for Ocean SAR Image Retrieval,0.00712934,"Spaceborne synthetic aperture radar (SAR) can provide accurate images of the
ocean surface roughness day-or-night in nearly all weather conditions, being an
unique asset for many geophysical applications. Considering the huge amount of
data daily acquired by satellites, automated techniques for physical features
extraction are needed. Even if supervised deep learning methods attain
state-of-the-art results, they require great amount of labeled data, which are
difficult and excessively expensive to acquire for ocean SAR imagery. To this
end, we use the subaperture decomposition (SD) algorithm to enhance the
unsupervised learning retrieval on the ocean surface, empowering ocean
researchers to search into large ocean databases. We empirically prove that SD
improve the retrieval precision with over 20% for an unsupervised transformer
auto-encoder network. Moreover, we show that SD brings important performance
boost when Doppler centroid images are used as input data, leading the way to
new unsupervised physics guided retrieval algorithms.",None,-1
GRS: Combining Generation and Revision in Unsupervised Sentence Simplification,0.0419955,"We propose GRS: an unsupervised approach to sentence simplification that
combines text generation and text revision. We start with an iterative
framework in which an input sentence is revised using explicit edit operations,
and add paraphrasing as a new edit operation. This allows us to combine the
advantages of generative and revision-based approaches: paraphrasing captures
complex edit operations, and the use of explicit edit operations in an
iterative manner provides controllability and interpretability. We demonstrate
these advantages of GRS compared to existing methods on the Newsela and ASSET
datasets.",https://github.com/imohammad12/GRS,-1
Forming Effective Human-AI Teams: Building Machine Learning Models that Complement the Capabilities of Multiple Experts,0.570898,"Machine learning (ML) models are increasingly being used in application
domains that often involve working together with human experts. In this
context, it can be advantageous to defer certain instances to a single human
expert when they are difficult to predict for the ML model. While previous work
has focused on scenarios with one distinct human expert, in many real-world
situations several human experts with varying capabilities may be available. In
this work, we propose an approach that trains a classification model to
complement the capabilities of multiple human experts. By jointly training the
classifier together with an allocation system, the classifier learns to
accurately predict those instances that are difficult for the human experts,
while the allocation system learns to pass each instance to the most suitable
team member -- either the classifier or one of the human experts. We evaluate
our proposed approach in multiple experiments on public datasets with
""synthetic"" experts and a real-world medical dataset annotated by multiple
radiologists. Our approach outperforms prior work and is more accurate than the
best human expert or a classifier. Furthermore, it is flexibly adaptable to
teams of varying sizes and different levels of expert diversity.",https://github.com/ptrckhmmr/human-ai-teams,-1
The AI Index 2022 Annual Report,0.866615,"Welcome to the fifth edition of the AI Index Report! The latest edition
includes data from a broad set of academic, private, and nonprofit
organizations as well as more self-collected data and original analysis than
any previous editions, including an expanded technical performance chapter, a
new survey of robotics researchers around the world, data on global AI
legislation records in 25 countries, and a new chapter with an in-depth
analysis of technical AI ethics metrics.
  The AI Index Report tracks, collates, distills, and visualizes data related
to artificial intelligence. Its mission is to provide unbiased, rigorously
vetted, and globally sourced data for policymakers, researchers, executives,
journalists, and the general public to develop a more thorough and nuanced
understanding of the complex field of AI. The report aims to be the world's
most credible and authoritative source for data and insights about AI.",None,-1
Restoring Vision in Hazy Weather with Hierarchical Contrastive Learning,0.0777391,"Image restoration under hazy weather condition, which is called single image
dehazing, has been of significant interest for various computer vision
applications. In recent years, deep learning-based methods have achieved
success. However, existing image dehazing methods typically neglect the
hierarchy of features in the neural network and fail to exploit their
relationships fully. To this end, we propose an effective image dehazing method
named Hierarchical Contrastive Dehazing (HCD), which is based on feature fusion
and contrastive learning strategies. HCD consists of a hierarchical dehazing
network (HDN) and a novel hierarchical contrastive loss (HCL). Specifically,
the core design in the HDN is a hierarchical interaction module, which utilizes
multi-scale activation to revise the feature responses hierarchically. To
cooperate with the training of HDN, we propose HCL which performs contrastive
learning on hierarchically paired exemplars, facilitating haze removal.
Extensive experiments on public datasets, RESIDE, HazeRD, and DENSE-HAZE,
demonstrate that HCD quantitatively outperforms the state-of-the-art methods in
terms of PSNR, SSIM and achieves better visual quality.",None,-1
A Screen-Shooting Resilient Document Image Watermarking Scheme using Deep Neural Network,0.118642,"With the advent of the screen-reading era, the confidential documents
displayed on the screen can be easily captured by a camera without leaving any
traces. Thus, this paper proposes a novel screen-shooting resilient
watermarking scheme for document image using deep neural network. By applying
this scheme, when the watermarked image is displayed on the screen and captured
by a camera, the watermark can be still extracted from the captured
photographs. Specifically, our scheme is an end-to-end neural network with an
encoder to embed watermark and a decoder to extract watermark. During the
training process, a distortion layer between encoder and decoder is added to
simulate the distortions introduced by screen-shooting process in real scenes,
such as camera distortion, shooting distortion, light source distortion.
Besides, an embedding strength adjustment strategy is designed to improve the
visual quality of the watermarked image with little loss of extraction
accuracy. The experimental results show that the scheme has higher robustness
and visual quality than other three recent state-of-the-arts. Specially, even
if the shooting distances and angles are in extreme, our scheme can also obtain
high extraction accuracy.",https://github.com/gslxr/Screen-Shooting-Resilient-Document-Image-Watermarking,-1
Multilevel Hierarchical Network with Multiscale Sampling for Video Question Answering,0.208124,"Video question answering (VideoQA) is challenging given its multimodal
combination of visual understanding and natural language processing. While most
existing approaches ignore the visual appearance-motion information at
different temporal scales, it is unknown how to incorporate the multilevel
processing capacity of a deep learning model with such multiscale information.
Targeting these issues, this paper proposes a novel Multilevel Hierarchical
Network (MHN) with multiscale sampling for VideoQA. MHN comprises two modules,
namely Recurrent Multimodal Interaction (RMI) and Parallel Visual Reasoning
(PVR). With a multiscale sampling, RMI iterates the interaction of
appearance-motion information at each scale and the question embeddings to
build the multilevel question-guided visual representations. Thereon, with a
shared transformer encoder, PVR infers the visual cues at each level in
parallel to fit with answering different question types that may rely on the
visual information at relevant levels. Through extensive experiments on three
VideoQA datasets, we demonstrate improved performances than previous
state-of-the-arts and justify the effectiveness of each part of our method.",None,-1
Prototypical Verbalizer for Prompt-based Few-shot Tuning,0.360483,"Prompt-based tuning for pre-trained language models (PLMs) has shown its
effectiveness in few-shot learning. Typically, prompt-based tuning wraps the
input text into a cloze question. To make predictions, the model maps the
output words to labels via a verbalizer, which is either manually designed or
automatically built. However, manual verbalizers heavily depend on
domain-specific prior knowledge and human efforts, while finding appropriate
label words automatically still remains challenging.In this work, we propose
the prototypical verbalizer (ProtoVerb) which is built directly from training
data. Specifically, ProtoVerb learns prototype vectors as verbalizers by
contrastive learning. In this way, the prototypes summarize training instances
and are able to enclose rich class-level semantics. We conduct experiments on
both topic classification and entity typing tasks, and the results demonstrate
that ProtoVerb significantly outperforms current automatic verbalizers,
especially when training data is extremely scarce. More surprisingly, ProtoVerb
consistently boosts prompt-based tuning even on untuned PLMs, indicating an
elegant non-tuning way to utilize PLMs. Our codes are avaliable at
https://github.com/thunlp/OpenPrompt.",https://github.com/thunlp/OpenPrompt,-1
SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model,0.971035,"Data-driven speech processing models usually perform well with a large amount
of text supervision, but collecting transcribed speech data is costly.
Therefore, we propose SpeechCLIP, a novel framework bridging speech and text
through images to enhance speech models without transcriptions. We leverage
state-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images
and spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior
state-of-the-art on image-speech retrieval and performs zero-shot speech-text
retrieval without direct supervision from transcriptions. Moreover, SpeechCLIP
can directly retrieve semantically related keywords from speech.",https://github.com/atosystem/SpeechCLIP,-1
Visual Fault Detection of Multi-scale Key Components in Freight Trains,0.0768158,"Fault detection for key components in the braking system of freight trains is
critical for ensuring railway transportation safety. Despite the frequently
employed methods based on deep learning, these fault detectors are highly
reliant on hardware resources and are complex to implement. In addition, no
train fault detectors consider the drop in accuracy induced by scale variation
of fault parts. This paper proposes a lightweight anchor-free framework to
solve the above problems. Specifically, to reduce the amount of computation and
model size, we introduce a lightweight backbone and adopt an anchor-free method
for localization and regression. To improve detection accuracy for multi-scale
parts, we design a feature pyramid network to generate rectangular layers of
different sizes to map parts with similar aspect ratios. Experiments on four
fault datasets show that our framework achieves 98.44% accuracy while the model
size is only 22.5 MB, outperforming state-of-the-art detectors.",None,-1
Knowledge Sharing via Domain Adaptation in Customs Fraud Detection,0.018393,"Knowledge of the changing traffic is critical in risk management. Customs
offices worldwide have traditionally relied on local resources to accumulate
knowledge and detect tax fraud. This naturally poses countries with weak
infrastructure to become tax havens of potentially illicit trades. The current
paper proposes DAS, a memory bank platform to facilitate knowledge sharing
across multi-national customs administrations to support each other. We propose
a domain adaptation method to share transferable knowledge of frauds as
prototypes while safeguarding the local trade information. Data encompassing
over 8 million import declarations have been used to test the feasibility of
this new system, which shows that participating countries may benefit up to
2-11 times in fraud detection with the help of shared knowledge. We discuss
implications for substantial tax revenue potential and strengthened policy
against illicit trades.",None,-1
Structured Pruning Adapters,0.0708719,"Adapters are a parameter-efficient alternative to fine-tuning, which augment
a frozen base network to learn new tasks. Yet, the inference of the adapted
model is often slower than the corresponding fine-tuned model. To improve on
this, we propose Structured Pruning Adapters (SPAs), a family of compressing,
task-switching network adapters, that accelerate and specialize networks using
tiny parameter sets and structured pruning. Specifically, we propose a
channel-based SPA and evaluate it with a suite of pruning methods on multiple
computer vision benchmarks. Compared to regular structured pruning with
fine-tuning, our channel-SPAs improve accuracy by 6.9% on average while using
half the parameters at 90% pruned weights. Alternatively, they can learn
adaptations with 17x fewer parameters at 70% pruning with 1.6% lower accuracy.
Similarly, our block-SPA requires far fewer parameters than pruning with
fine-tuning. Our experimental code and Python library of adapters are available
at github.com/lukashedegaard/structured-pruning-adapters.",None,-1
Speaker adaptation for Wav2vec2 based dysarthric ASR,0.0872816,"Dysarthric speech recognition has posed major challenges due to lack of
training data and heavy mismatch in speaker characteristics. Recent ASR systems
have benefited from readily available pretrained models such as wav2vec2 to
improve the recognition performance. Speaker adaptation using fMLLR and
xvectors have provided major gains for dysarthric speech with very little
adaptation data. However, integration of wav2vec2 with fMLLR features or
xvectors during wav2vec2 finetuning is yet to be explored. In this work, we
propose a simple adaptation network for fine-tuning wav2vec2 using fMLLR
features. The adaptation network is also flexible to handle other speaker
adaptive features such as xvectors. Experimental analysis show steady
improvements using our proposed approach across all impairment severity levels
and attains 57.72\% WER for high severity in UASpeech dataset. We also
performed experiments on German dataset to substantiate the consistency of our
proposed approach across diverse domains.",None,-1
Towards Domain Generalization in Object Detection,0.07751,"Despite the striking performance achieved by modern detectors when training
and test data are sampled from the same or similar distribution, the
generalization ability of detectors under unknown distribution shifts remains
hardly studied. Recently several works discussed the detectors' adaptation
ability to a specific target domain which are not readily applicable in
real-world applications since detectors may encounter various environments or
situations while pre-collecting all of them before training is inconceivable.
In this paper, we study the critical problem, domain generalization in object
detection (DGOD), where detectors are trained with source domains and evaluated
on unknown target domains. To thoroughly evaluate detectors under unknown
distribution shifts, we formulate the DGOD problem and propose a comprehensive
evaluation benchmark to fill the vacancy. Moreover, we propose a novel method
named Region Aware Proposal reweighTing (RAPT) to eliminate dependence within
RoI features. Extensive experiments demonstrate that current DG methods fail to
address the DGOD problem and our method outperforms other state-of-the-art
counterparts.",None,-1
A Multi-turn Machine Reading Comprehension Framework with Rethink Mechanism for Emotion-Cause Pair Extraction,0.284494,"Emotion-cause pair extraction (ECPE) is an emerging task in emotion cause
analysis, which extracts potential emotion-cause pairs from an emotional
document. Most recent studies use end-to-end methods to tackle the ECPE task.
However, these methods either suffer from a label sparsity problem or fail to
model complicated relations between emotions and causes. Furthermore, they all
do not consider explicit semantic information of clauses. To this end, we
transform the ECPE task into a document-level machine reading comprehension
(MRC) task and propose a Multi-turn MRC framework with Rethink mechanism
(MM-R). Our framework can model complicated relations between emotions and
causes while avoiding generating the pairing matrix (the leading cause of the
label sparsity problem). Besides, the multi-turn structure can fuse explicit
semantic information flow between emotions and causes. Extensive experiments on
the benchmark emotion cause corpus demonstrate the effectiveness of our
proposed framework, which outperforms existing state-of-the-art methods.",https://github.com/zhoucz97/ECPE-MM-R,-1
Recurrent Encoder-Decoder Networks for Vessel Trajectory Prediction with Uncertainty Estimation,0.190083,"Recent deep learning methods for vessel trajectory prediction are able to
learn complex maritime patterns from historical Automatic Identification System
(AIS) data and accurately predict sequences of future vessel positions with a
prediction horizon of several hours. However, in maritime surveillance
applications, reliably quantifying the prediction uncertainty can be as
important as obtaining high accuracy. This paper extends deep learning
frameworks for trajectory prediction tasks by exploring how recurrent
encoder-decoder neural networks can be tasked not only to predict but also to
yield a corresponding prediction uncertainty via Bayesian modeling of epistemic
and aleatoric uncertainties. We compare the prediction performance of two
different models based on labeled or unlabeled input data to highlight how
uncertainty quantification and accuracy can be improved by using, if available,
additional information on the intention of the ship (e.g., its planned
destination).",None,-1
Automatic Detection of Entity-Manipulated Text using Factual Knowledge,0.0684608,"In this work, we focus on the problem of distinguishing a human written news
article from a news article that is created by manipulating entities in a human
written news article (e.g., replacing entities with factually incorrect
entities). Such manipulated articles can mislead the reader by posing as a
human written news article. We propose a neural network based detector that
detects manipulated news articles by reasoning about the facts mentioned in the
article. Our proposed detector exploits factual knowledge via graph
convolutional neural network along with the textual information in the news
article. We also create challenging datasets for this task by considering
various strategies to generate the new replacement entity (e.g., entity
generation from GPT-2). In all the settings, our proposed model either matches
or outperforms the state-of-the-art detector in terms of accuracy. Our code and
data are available at https://github.com/UBC-NLP/manipulated_entity_detection.",https://github.com/UBC-NLP/manipulated_entity_detection,-1
Exploring Continuous Integrate-and-Fire for Adaptive Simultaneous Speech Translation,0.224883,"Simultaneous speech translation (SimulST) is a challenging task aiming to
translate streaming speech before the complete input is observed. A SimulST
system generally includes two components: the pre-decision that aggregates the
speech information and the policy that decides to read or write. While recent
works had proposed various strategies to improve the pre-decision, they mainly
adopt the fixed wait-k policy, leaving the adaptive policies rarely explored.
This paper proposes to model the adaptive policy by adapting the Continuous
Integrate-and-Fire (CIF). Compared with monotonic multihead attention (MMA),
our method has the advantage of simpler computation, superior quality at low
latency, and better generalization to long utterances. We conduct experiments
on the MuST-C V2 dataset and show the effectiveness of our approach.",https://github.com/George0828Zhang/simulst,-1
Multi-hop Evidence Retrieval for Cross-document Relation Extraction,0.232925,"Relation Extraction (RE) has been extended to cross-document scenarios
because many relations are not simply described in a single document. This
inevitably brings the challenge of efficient open-space evidence retrieval to
support the inference of cross-document relations, along with the challenge of
multi-hop reasoning on top of entities and evidence scattered in an open set of
documents. To combat these challenges, we propose MR.COD (Multi-hop evidence
retrieval for Cross-document relation extraction), which is a multi-hop
evidence retrieval method based on evidence path mining and ranking. We explore
multiple variants of retrievers to show evidence retrieval is essential in
cross-document RE. We also propose a contextual dense retriever for this
setting. Experiments on CodRED show that evidence retrieval with MR.COD
effectively acquires crossdocument evidence and boosts end-to-end RE
performance in both closed and open settings.",https://github.com/luka-group/MrCoD,-1
Jointformer: Single-Frame Lifting Transformer with Error Prediction and Refinement for 3D Human Pose Estimation,0.864665,"Monocular 3D human pose estimation technologies have the potential to greatly
increase the availability of human movement data. The best-performing models
for single-image 2D-3D lifting use graph convolutional networks (GCNs) that
typically require some manual input to define the relationships between
different body joints. We propose a novel transformer-based approach that uses
the more generalised self-attention mechanism to learn these relationships
within a sequence of tokens representing joints. We find that the use of
intermediate supervision, as well as residual connections between the stacked
encoders benefits performance. We also suggest that using error prediction as
part of a multi-task learning framework improves performance by allowing the
network to compensate for its confidence level. We perform extensive ablation
studies to show that each of our contributions increases performance.
Furthermore, we show that our approach outperforms the recent state of the art
for single-frame 3D human pose estimation by a large margin. Our code and
trained models are made publicly available on Github.",None,-1
End-to-end Multilingual Coreference Resolution with Mention Head Prediction,0.0388096,"This paper describes our approach to the CRAC 2022 Shared Task on
Multilingual Coreference Resolution. Our model is based on a state-of-the-art
end-to-end coreference resolution system. Apart from joined multilingual
training, we improved our results with mention head prediction. We also tried
to integrate dependency information into our model. Our system ended up in
$3^{rd}$ place. Moreover, we reached the best performance on two datasets out
of 13.",https://github.com/ufal/corefud-scorer,-1
Segmenting Moving Objects via an Object-Centric Layered Representation,0.0727911,"The objective of this paper is a model that is able to discover, track and
segment multiple moving objects in a video. We make four contributions: First,
we introduce an object-centric segmentation model with a depth-ordered layer
representation. This is implemented using a variant of the transformer
architecture that ingests optical flow, where each query vector specifies an
object and its layer for the entire video. The model can effectively discover
multiple moving objects and handle mutual occlusions; Second, we introduce a
scalable pipeline for generating multi-object synthetic training data via layer
compositions, that is used to train the proposed model, significantly reducing
the requirements for labour-intensive annotations, and supporting Sim2Real
generalisation; Third, we conduct thorough ablation studies, showing that the
model is able to learn object permanence and temporal shape consistency, and is
able to predict amodal segmentation masks; Fourth, we evaluate our model,
trained only on synthetic data, on standard video segmentation benchmarks,
DAVIS, MoCA, SegTrack, FBMS-59, and achieve state-of-the-art performance among
existing methods that do not rely on any manual annotations. With test-time
adaptation, we observe further performance boosts.",None,-1
Towards Trustworthy Multi-label Sewer Defect Classification via Evidential Deep Learning,0.0117241,"An automatic vision-based sewer inspection plays a key role of sewage system
in a modern city. Recent advances focus on utilizing deep learning model to
realize the sewer inspection system, benefiting from the capability of
data-driven feature representation. However, the inherent uncertainty of sewer
defects is ignored, resulting in the missed detection of serious unknown sewer
defect categories. In this paper, we propose a trustworthy multi-label sewer
defect classification (TMSDC) method, which can quantify the uncertainty of
sewer defect prediction via evidential deep learning. Meanwhile, a novel expert
base rate assignment (EBRA) is proposed to introduce the expert knowledge for
describing reliable evidences in practical situations. Experimental results
demonstrate the effectiveness of TMSDC and the superior capability of
uncertainty estimation is achieved on the latest public benchmark.",None,-1
Better Smatch = Better Parser? AMR evaluation is not so simple anymore,0.100618,"Recently, astonishing advances have been observed in AMR parsing, as measured
by the structural Smatch metric. In fact, today's systems achieve performance
levels that seem to surpass estimates of human inter annotator agreement (IAA).
Therefore, it is unclear how well Smatch (still) relates to human estimates of
parse quality, as in this situation potentially fine-grained errors of similar
weight may impact the AMR's meaning to different degrees.
  We conduct an analysis of two popular and strong AMR parsers that --
according to Smatch -- reach quality levels on par with human IAA, and assess
how human quality ratings relate to Smatch and other AMR metrics. Our main
findings are: i) While high Smatch scores indicate otherwise, we find that AMR
parsing is far from being solved: we frequently find structurally small, but
semantically unacceptable errors that substantially distort sentence meaning.
ii) Considering high-performance parsers, better Smatch scores may not
necessarily indicate consistently better parsing quality. To obtain a
meaningful and comprehensive assessment of quality differences of parse(r)s, we
recommend augmenting evaluations with macro statistics, use of additional
metrics, and more human analysis.",https://github.com/Heidelberg-nlp/AMRParseEval,-1
1Cademy @ Causal News Corpus 2022: Enhance Causal Span Detection via Beam-Search-based Position Selector,0.0505763,"In this paper, we present our approach and empirical observations for
Cause-Effect Signal Span Detection -- Subtask 2 of Shared task
3~\cite{tan-etal-2022-event} at CASE 2022. The shared task aims to extract the
cause, effect, and signal spans from a given causal sentence. We model the task
as a reading comprehension (RC) problem and apply a token-level RC-based span
prediction paradigm to the task as the baseline. We explore different training
objectives to fine-tune the model, as well as data augmentation (DA) tricks
based on the language model (LM) for performance improvement. Additionally, we
propose an efficient beam-search post-processing strategy to due with the
drawbacks of span detection to obtain a further performance gain. Our approach
achieves an average $F_1$ score of 54.15 and ranks \textbf{$1^{st}$} in the
CASE competition. Our code is available at
\url{https://github.com/Gzhang-umich/1CademyTeamOfCASE}.",https://github.com/Gzhang-umich/1CademyTeamOfCASE,-1
READ: Large-Scale Neural Scene Rendering for Autonomous Driving,0.345589,"Synthesizing free-view photo-realistic images is an important task in
multimedia. With the development of advanced driver assistance systems~(ADAS)
and their applications in autonomous vehicles, experimenting with different
scenarios becomes a challenge. Although the photo-realistic street scenes can
be synthesized by image-to-image translation methods, which cannot produce
coherent scenes due to the lack of 3D information. In this paper, a large-scale
neural rendering method is proposed to synthesize the autonomous driving
scene~(READ), which makes it possible to synthesize large-scale driving
scenarios on a PC through a variety of sampling schemes. In order to represent
driving scenarios, we propose an {\omega} rendering network to learn neural
descriptors from sparse point clouds. Our model can not only synthesize
realistic driving scenes but also stitch and edit driving scenes. Experiments
show that our model performs well in large-scale driving scenarios.",https://github.com/JOP-Lee/READ-Large-Scale-Neural-,-1
MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation,0.312068,"Responding with multi-modal content has been recognized as an essential
capability for an intelligent conversational agent. In this paper, we introduce
the MMDialog dataset to better facilitate multi-modal conversation. MMDialog is
composed of a curated set of 1.08 million real-world dialogues with 1.53
million unique images across 4,184 topics. MMDialog has two main and unique
advantages. First, it is the largest multi-modal conversation dataset by the
number of dialogues by 88x. Second, it contains massive topics to generalize
the open-domain. To build engaging dialogue system with this dataset, we
propose and normalize two response producing tasks based on retrieval and
generative scenarios. In addition, we build two baselines for above tasks with
state-of-the-art techniques and report their experimental performance. We also
propose a novel evaluation metric MM-Relevance to measure the multi-modal
responses. Our dataset and scripts are available in
https://github.com/victorsungo/MMDialog.",https://github.com/victorsungo/MMDialog,-1
Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training,0.387102,"In cross-lingual speech synthesis, the speech in various languages can be
synthesized for a monoglot speaker. Normally, only the data of monoglot
speakers are available for model training, thus the speaker similarity is
relatively low between the synthesized cross-lingual speech and the native
language recordings. Based on the multilingual transformer text-to-speech
model, this paper studies a multi-task learning framework to improve the
cross-lingual speaker similarity. To further improve the speaker similarity,
joint training with a speaker classifier is proposed. Here, a scheme similar to
parallel scheduled sampling is proposed to train the transformer model
efficiently to avoid breaking the parallel training mechanism when introducing
joint training. By using multi-task learning and speaker classifier joint
training, in subjective and objective evaluations, the cross-lingual speaker
similarity can be consistently improved for both the seen and unseen speakers
in the training set.",None,-1
How stable are Transferability Metrics evaluations?,0.104761,"Transferability metrics is a maturing field with increasing interest, which
aims at providing heuristics for selecting the most suitable source models to
transfer to a given target dataset, without fine-tuning them all. However,
existing works rely on custom experimental setups which differ across papers,
leading to inconsistent conclusions about which transferability metrics work
best. In this paper we conduct a large-scale study by systematically
constructing a broad range of 715k experimental setup variations. We discover
that even small variations to an experimental setup lead to different
conclusions about the superiority of a transferability metric over another.
Then we propose better evaluations by aggregating across many experiments,
enabling to reach more stable conclusions. As a result, we reveal the
superiority of LogME at selecting good source datasets to transfer from in a
semantic segmentation scenario, NLEEP at selecting good source architectures in
an image classification scenario, and GBC at determining which target task
benefits most from a given source model. Yet, no single transferability metric
works best in all scenarios.",https://github.com/google-research/google-research/tree/master/stable transfer,-1
Edge-enhanced Feature Distillation Network for Efficient Super-Resolution,0.290183,"With the recently massive development in convolution neural networks,
numerous lightweight CNN-based image super-resolution methods have been
proposed for practical deployments on edge devices. However, most existing
methods focus on one specific aspect: network or loss design, which leads to
the difficulty of minimizing the model size. To address the issue, we conclude
block devising, architecture searching, and loss design to obtain a more
efficient SR structure. In this paper, we proposed an edge-enhanced feature
distillation network, named EFDN, to preserve the high-frequency information
under constrained resources. In detail, we build an edge-enhanced convolution
block based on the existing reparameterization methods. Meanwhile, we propose
edge-enhanced gradient loss to calibrate the reparameterized path training.
Experimental results show that our edge-enhanced strategies preserve the edge
and significantly improve the final restoration quality. Code is available at
https://github.com/icandle/EFDN.",https://github.com/icandle/EFDN,-1
A Memory Transformer Network for Incremental Learning,0.0479936,"We study class-incremental learning, a training setup in which new classes of
data are observed over time for the model to learn from. Despite the
straightforward problem formulation, the naive application of classification
models to class-incremental learning results in the ""catastrophic forgetting""
of previously seen classes. One of the most successful existing methods has
been the use of a memory of exemplars, which overcomes the issue of
catastrophic forgetting by saving a subset of past data into a memory bank and
utilizing it to prevent forgetting when training future tasks. In our paper, we
propose to enhance the utilization of this memory bank: we not only use it as a
source of additional training data like existing works but also integrate it in
the prediction process explicitly.Our method, the Memory Transformer Network
(MTN), learns how to combine and aggregate the information from the nearest
neighbors in the memory with a transformer to make more accurate predictions.
We conduct extensive experiments and ablations to evaluate our approach. We
show that MTN achieves state-of-the-art performance on the challenging
ImageNet-1k and Google-Landmarks-1k incremental learning benchmarks.",None,-1
Distilling Inter-Class Distance for Semantic Segmentation,0.355336,"Knowledge distillation is widely adopted in semantic segmentation to reduce
the computation cost.The previous knowledge distillation methods for semantic
segmentation focus on pixel-wise feature alignment and intra-class feature
variation distillation, neglecting to transfer the knowledge of the inter-class
distance in the feature space, which is important for semantic segmentation. To
address this issue, we propose an Inter-class Distance Distillation (IDD)
method to transfer the inter-class distance in the feature space from the
teacher network to the student network. Furthermore, semantic segmentation is a
position-dependent task,thus we exploit a position information distillation
module to help the student network encode more position information. Extensive
experiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show
that our method is helpful to improve the accuracy of semantic segmentation
models and achieves the state-of-the-art performance. E.g. it boosts the
benchmark model(""PSPNet+ResNet18"") by 7.50% in accuracy on the Cityscapes
dataset.",None,-1
Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection,0.011882,"Pre-training large neural language models, such as BERT, has led to
impressive gains on many natural language processing (NLP) tasks. Although this
method has proven to be effective for many domains, it might not always provide
desirable benefits. In this paper, we study the effects of hateful pre-training
on low-resource hate speech classification tasks. While previous studies on the
English language have emphasized its importance, we aim to augment their
observations with some non-obvious insights. We evaluate different variations
of tweet-based BERT models pre-trained on hateful, non-hateful, and mixed
subsets of a 40M tweet dataset. This evaluation is carried out for the Indian
languages Hindi and Marathi. This paper is empirical evidence that hateful
pre-training is not the best pre-training option for hate speech detection. We
show that pre-training on non-hateful text from the target domain provides
similar or better results. Further, we introduce HindTweetBERT and
MahaTweetBERT, the first publicly available BERT models pre-trained on Hindi
and Marathi tweets, respectively. We show that they provide state-of-the-art
performance on hate speech classification tasks. We also release hateful BERT
for the two languages and a gold hate speech evaluation benchmark HateEval-Hi
and HateEval-Mr consisting of manually labeled 2000 tweets each. The models and
data are available at https://github.com/l3cube-pune/MarathiNLP .",https://github.com/l3cube-pune/MarathiNLP,-1
Robust Category-Level 6D Pose Estimation with Coarse-to-Fine Rendering of Neural Features,0.0814703,"We consider the problem of category-level 6D pose estimation from a single
RGB image. Our approach represents an object category as a cuboid mesh and
learns a generative model of the neural feature activations at each mesh vertex
to perform pose estimation through differentiable rendering. A common problem
of rendering-based approaches is that they rely on bounding box proposals,
which do not convey information about the 3D rotation of the object and are not
reliable when objects are partially occluded. Instead, we introduce a
coarse-to-fine optimization strategy that utilizes the rendering process to
estimate a sparse set of 6D object proposals, which are subsequently refined
with gradient-based optimization. The key to enabling the convergence of our
approach is a neural feature representation that is trained to be scale- and
rotation-invariant using contrastive learning. Our experiments demonstrate an
enhanced category-level 6D pose estimation performance compared to prior work,
particularly under strong partial occlusion.",None,-1
Circular Pythagorean fuzzy sets and applications to multi-criteria decision making,0.0558528,"In this paper, we introduce the concept of circular Pythagorean fuzzy set
(value) (C-PFS(V)) as a new generalization of both circular intuitionistic
fuzzy sets (C-IFSs) proposed by Atannassov and Pythagorean fuzzy sets (PFSs)
proposed by Yager. A circular Pythagorean fuzzy set is represented by a circle
that represents the membership degree and the non-membership degree and whose
center consists of non-negative real numbers $\mu$ and $\nu$ with the condition
$\mu^2+\nu^2\leq 1$. A C-PFS models the fuzziness of the uncertain information
more properly thanks to its structure that allows modelling the information
with points of a circle of a certain center and a radius. Therefore, a C-PFS
lets decision makers to evaluate objects in a larger and more flexible region
and thus more sensitive decisions can be made. After defining the concept of
C-PFS we define some fundamental set operations between C-PFSs and propose some
algebraic operations between C-PFVs via general $t$-norms and $t$-conorms. By
utilizing these algebraic operations, we introduce some weighted aggregation
operators to transform input values represented by C-PFVs to a single output
value. Then to determine the degree of similarity between C-PFVs we define a
cosine similarity measure based on radius. Furthermore, we develop a method to
transform a collection of Pythagorean fuzzy values to a PFS. Finally, a method
is given to solve multi-criteria decision making problems in circular
Pythagorean fuzzy environment and the proposed method is practiced to a problem
about selecting the best photovoltaic cell from the literature. We also study
the comparison analysis and time complexity of the proposed method.",None,-1
MnTTS2: An Open-Source Multi-Speaker Mongolian Text-to-Speech Synthesis Dataset,0.198442,"Text-to-Speech (TTS) synthesis for low-resource languages is an attractive
research issue in academia and industry nowadays. Mongolian is the official
language of the Inner Mongolia Autonomous Region and a representative
low-resource language spoken by over 10 million people worldwide. However,
there is a relative lack of open-source datasets for Mongolian TTS. Therefore,
we make public an open-source multi-speaker Mongolian TTS dataset, named
MnTTS2, for the benefit of related researchers. In this work, we prepare the
transcription from various topics and invite three professional Mongolian
announcers to form a three-speaker TTS dataset, in which each announcer records
10 hours of speeches in Mongolian, resulting 30 hours in total. Furthermore, we
build the baseline system based on the state-of-the-art FastSpeech2 model and
HiFi-GAN vocoder. The experimental results suggest that the constructed MnTTS2
dataset is sufficient to build robust multi-speaker TTS models for real-world
applications. The MnTTS2 dataset, training recipe, and pretrained models are
released at: \url{https://github.com/ssmlkl/MnTTS2}",https://github.com/ssmlkl/MnTTS2,-1
Object Localization under Single Coarse Point Supervision,0.0895485,"Point-based object localization (POL), which pursues high-performance object
sensing under low-cost data annotation, has attracted increased attention.
However, the point annotation mode inevitably introduces semantic variance for
the inconsistency of annotated points. Existing POL methods heavily reply on
accurate key-point annotations which are difficult to define. In this study, we
propose a POL method using coarse point annotations, relaxing the supervision
signals from accurate key points to freely spotted points. To this end, we
propose a coarse point refinement (CPR) approach, which to our best knowledge
is the first attempt to alleviate semantic variance from the perspective of
algorithm. CPR constructs point bags, selects semantic-correlated points, and
produces semantic center points through multiple instance learning (MIL). In
this way, CPR defines a weakly supervised evolution procedure, which ensures
training high-performance object localizer under coarse point supervision.
Experimental results on COCO, DOTA and our proposed SeaPerson dataset validate
the effectiveness of the CPR approach. The dataset and code will be available
at https://github.com/ucas-vg/PointTinyBenchmark/.",https://github.com/ucas-vg/PointTinyBenchmark/,-1
Adaptive Testing of Computer Vision Models,0.0710588,"Vision models often fail systematically on groups of data that share common
semantic characteristics (e.g., rare objects or unusual scenes), but
identifying these failure modes is a challenge. We introduce AdaVision, an
interactive process for testing vision models which helps users identify and
fix coherent failure modes. Given a natural language description of a coherent
group, AdaVision retrieves relevant images from LAION-5B with CLIP. The user
then labels a small amount of data for model correctness, which is used in
successive retrieval rounds to hill-climb towards high-error regions, refining
the group definition. Once a group is saturated, AdaVision uses GPT-3 to
suggest new group descriptions for the user to explore. We demonstrate the
usefulness and generality of AdaVision in user studies, where users find major
bugs in state-of-the-art classification, object detection, and image captioning
models. These user-discovered groups have failure rates 2-3x higher than those
surfaced by automatic error clustering methods. Finally, finetuning on examples
found with AdaVision fixes the discovered bugs when evaluated on unseen
examples, without degrading in-distribution accuracy, and while also improving
performance on out-of-distribution datasets.",https://github.com/i-gao/adavision,-1
Narrowing the Gap: Improved Detector Training with Noisy Location Annotations,0.048653,"Deep learning methods require massive of annotated data for optimizing
parameters. For example, datasets attached with accurate bounding box
annotations are essential for modern object detection tasks. However, labeling
with such pixel-wise accuracy is laborious and time-consuming, and elaborate
labeling procedures are indispensable for reducing man-made noise, involving
annotation review and acceptance testing. In this paper, we focus on the impact
of noisy location annotations on the performance of object detection approaches
and aim to, on the user side, reduce the adverse effect of the noise. First,
noticeable performance degradation is experimentally observed for both
one-stage and two-stage detectors when noise is introduced to the bounding box
annotations. For instance, our synthesized noise results in performance
decrease from 38.9% AP to 33.6% AP for FCOS detector on COCO test split, and
37.8%AP to 33.7%AP for Faster R-CNN. Second, a self-correction technique based
on a Bayesian filter for prediction ensemble is proposed to better exploit the
noisy location annotations following a Teacher-Student learning paradigm.
Experiments for both synthesized and real-world scenarios consistently
demonstrate the effectiveness of our approach, e.g., our method increases the
degraded performance of the FCOS detector from 33.6% AP to 35.6% AP on COCO.",https://github.com/wangsr126/NDet,-1
Deep Learning Opacity in Scientific Discovery,0.0968793,"Philosophers have recently focused on critical, epistemological challenges
that arise from the opacity of deep neural networks. One might conclude from
this literature that doing good science with opaque models is exceptionally
challenging, if not impossible. Yet, this is hard to square with the recent
boom in optimism for AI in science alongside a flood of recent scientific
breakthroughs driven by AI methods. In this paper, I argue that the disconnect
between philosophical pessimism and scientific optimism is driven by a failure
to examine how AI is actually used in science. I show that, in order to
understand the epistemic justification for AI-powered breakthroughs,
philosophers must examine the role played by deep learning as part of a wider
process of discovery. The philosophical distinction between the 'context of
discovery' and the 'context of justification' is helpful in this regard. I
demonstrate the importance of attending to this distinction with two cases
drawn from the scientific literature, and show that epistemic opacity need not
diminish AI's capacity to lead scientists to significant and justifiable
breakthroughs.",None,-1
Learning logic programs by combining programs,0.0506654,"The goal of inductive logic programming is to induce a logic program (a set
of logical rules) that generalises training examples. Inducing programs with
many rules and literals is a major challenge. To tackle this challenge, we
introduce an approach where we learn small non-separable programs and combine
them. We implement our approach in a constraint-driven ILP system. Our approach
can learn optimal and recursive programs and perform predicate invention. Our
experiments on multiple domains, including game playing and program synthesis,
show that our approach can drastically outperform existing approaches in terms
of predictive accuracies and learning times, sometimes reducing learning times
from over an hour to a few seconds.",https://github.com/logic-and-learning-lab/ecai23-combo,-1
Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients,0.113789,"Deep neural networks for 3D point cloud classification, such as PointNet,
have been demonstrated to be vulnerable to adversarial attacks. Current
adversarial defenders often learn to denoise the (attacked) point clouds by
reconstruction, and then feed them to the classifiers as input. In contrast to
the literature, we propose a family of robust structured declarative
classifiers for point cloud classification, where the internal constrained
optimization mechanism can effectively defend adversarial attacks through
implicit gradients. Such classifiers can be formulated using a bilevel
optimization framework. We further propose an effective and efficient
instantiation of our approach, namely, Lattice Point Classifier (LPC), based on
structured sparse coding in the permutohedral lattice and 2D convolutional
neural networks (CNNs) that is end-to-end trainable. We demonstrate
state-of-the-art robust point cloud classification performance on ModelNet40
and ScanNet under seven different attackers. For instance, we achieve 89.51%
and 83.16% test accuracy on each dataset under the recent JGBA attacker that
outperforms DUP-Net and IF-Defense with PointNet by ~70%. Demo code is
available at https://zhang-vislab.github.io.",https://zhang-vislab.github.io,-1
SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models,0.169273,"Vision-language models such as CLIP are pretrained on large volumes of
internet sourced image and text pairs, and have been shown to sometimes exhibit
impressive zero- and low-shot image classification performance. However, due to
their size, fine-tuning these models on new datasets can be prohibitively
expensive, both in terms of the supervision and compute required. To combat
this, a series of light-weight adaptation methods have been proposed to
efficiently adapt such models when limited supervision is available. In this
work, we show that while effective on internet-style datasets, even those
remedies under-deliver on classification tasks with images that differ
significantly from those commonly found online. To address this issue, we
present a new approach called SVL-Adapter that combines the complementary
strengths of both vision-language pretraining and self-supervised
representation learning. We report an average classification accuracy
improvement of 10% in the low-shot setting when compared to existing methods,
on a set of challenging visual classification tasks. Further, we present a
fully automatic way of selecting an important blending hyperparameter for our
model that does not require any held-out labeled validation data. Code for our
project is available here: https://github.com/omipan/svl_adapter.",https://github.com/omipan/svl_adapter,-1
Face recognition with small and large size databases,0.0288317,"This paper presents experimental results using the ORL (40 people) and FERET
(994 people) databases. The ORL database can be useful for securing
applications where few users attempting to access are expected. This is the
case, for instance, of a PDA or PC where the password is the face of the user.
On the other hand, the FERET database is useful for studying those situations
where the number of authorized users is around a thousand people.",None,-1
Graph Reasoning Transformer for Image Parsing,0.0703366,"Capturing the long-range dependencies has empirically proven to be effective
on a wide range of computer vision tasks. The progressive advances on this
topic have been made through the employment of the transformer framework with
the help of the multi-head attention mechanism. However, the attention-based
image patch interaction potentially suffers from problems of redundant
interactions of intra-class patches and unoriented interactions of inter-class
patches. In this paper, we propose a novel Graph Reasoning Transformer (GReaT)
for image parsing to enable image patches to interact following a relation
reasoning pattern. Specifically, the linearly embedded image patches are first
projected into the graph space, where each node represents the implicit visual
center for a cluster of image patches and each edge reflects the relation
weight between two adjacent nodes. After that, global relation reasoning is
performed on this graph accordingly. Finally, all nodes including the relation
information are mapped back into the original space for subsequent processes.
Compared to the conventional transformer, GReaT has higher interaction
efficiency and a more purposeful interaction pattern. Experiments are carried
out on the challenging Cityscapes and ADE20K datasets. Results show that GReaT
achieves consistent performance gains with slight computational overheads on
the state-of-the-art transformer baselines.",https://github.com/open-mmlab/mmsegmentation,-1
Emergent Quantized Communication,0.0,"The field of emergent communication aims to understand the characteristics of
communication as it emerges from artificial agents solving tasks that require
information exchange. Communication with discrete messages is considered a
desired characteristic, for both scientific and applied reasons. However,
training a multi-agent system with discrete communication is not
straightforward, requiring either reinforcement learning algorithms or relaxing
the discreteness requirement via a continuous approximation such as the
Gumbel-softmax. Both these solutions result in poor performance compared to
fully continuous communication. In this work, we propose an alternative
approach to achieve discrete communication -- quantization of communicated
messages. Using message quantization allows us to train the model end-to-end,
achieving superior performance in multiple setups. Moreover, quantization is a
natural framework that runs the gamut from continuous to discrete
communication. Thus, it sets the ground for a broader view of multi-agent
communication in the deep learning era.",None,-1
Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention for Social-Text Classification,0.429274,"Social media has become the fulcrum of all forms of communication.
Classifying social texts such as fake news, rumour, sarcasm, etc. has gained
significant attention. The surface-level signals expressed by a social-text
itself may not be adequate for such tasks; therefore, recent methods attempted
to incorporate other intrinsic signals such as user behavior and the underlying
graph structure. Oftentimes, the `public wisdom' expressed through the
comments/replies to a social-text acts as a surrogate of crowd-sourced view and
may provide us with complementary signals. State-of-the-art methods on
social-text classification tend to ignore such a rich hierarchical signal.
Here, we propose Hyphen, a discourse-aware hyperbolic spectral co-attention
network. Hyphen is a fusion of hyperbolic graph representation learning with a
novel Fourier co-attention mechanism in an attempt to generalise the
social-text classification tasks by incorporating public discourse. We parse
public discourse as an Abstract Meaning Representation (AMR) graph and use the
powerful hyperbolic geometric representation to model graphs with hierarchical
structure. Finally, we equip it with a novel Fourier co-attention mechanism to
capture the correlation between the source post and public discourse. Extensive
experiments on four different social-text classification tasks, namely
detecting fake news, hate speech, rumour, and sarcasm, show that Hyphen
generalises well, and achieves state-of-the-art results on ten benchmark
datasets. We also employ a sentence-level fact-checked and annotated dataset to
evaluate how Hyphen is capable of producing explanations as analogous evidence
to the final prediction.",https://github.com/LCS2-IIITD/Hyphen,-1
Magpie: Automatically Tuning Static Parameters for Distributed File Systems using Deep Reinforcement Learning,0.00614937,"Distributed file systems are widely used nowadays, yet using their default
configurations is often not optimal. At the same time, tuning configuration
parameters is typically challenging and time-consuming. It demands expertise
and tuning operations can also be expensive. This is especially the case for
static parameters, where changes take effect only after a restart of the system
or workloads. We propose a novel approach, Magpie, which utilizes deep
reinforcement learning to tune static parameters by strategically exploring and
exploiting configuration parameter spaces. To boost the tuning of the static
parameters, our method employs both server and client metrics of distributed
file systems to understand the relationship between static parameters and
performance. Our empirical evaluation results show that Magpie can noticeably
improve the performance of the distributed file system Lustre, where our
approach on average achieves 91.8% throughput gains against default
configuration after tuning towards single performance indicator optimization,
while it reaches 39.7% more throughput gains against the baseline.",https://github.com/dos-group/magpie,-1
SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis,0.876241,"We propose MINT, a new Multilingual INTimacy analysis dataset covering 13,372
tweets in 10 languages including English, French, Spanish, Italian, Portuguese,
Korean, Dutch, Chinese, Hindi, and Arabic. We benchmarked a list of popular
multilingual pre-trained language models. The dataset is released along with
the SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis
(https://sites.google.com/umich.edu/semeval-2023-tweet-intimacy).",None,-1
Fast Point Cloud Generation with Straight Flows,0.256306,"Diffusion models have emerged as a powerful tool for point cloud generation.
A key component that drives the impressive performance for generating
high-quality samples from noise is iteratively denoise for thousands of steps.
While beneficial, the complexity of learning steps has limited its applications
to many 3D real-world. To address this limitation, we propose Point Straight
Flow (PSF), a model that exhibits impressive performance using one step. Our
idea is based on the reformulation of the standard diffusion model, which
optimizes the curvy learning trajectory into a straight path. Further, we
develop a distillation strategy to shorten the straight path into one step
without a performance loss, enabling applications to 3D real-world with latency
constraints. We perform evaluations on multiple 3D tasks and find that our PSF
performs comparably to the standard diffusion model, outperforming other
efficient 3D point cloud generation methods. On real-world applications such as
point cloud completion and training-free text-guided generation in a
low-latency setup, PSF performs favorably.",None,-1
Estimating and Explaining Model Performance When Both Covariates and Labels Shift,0.101001,"Deployed machine learning (ML) models often encounter new user data that
differs from their training data. Therefore, estimating how well a given model
might perform on the new data is an important step toward reliable ML
applications. This is very challenging, however, as the data distribution can
change in flexible ways, and we may not have any labels on the new data, which
is often the case in monitoring settings. In this paper, we propose a new
distribution shift model, Sparse Joint Shift (SJS), which considers the joint
shift of both labels and a few features. This unifies and generalizes several
existing shift models including label shift and sparse covariate shift, where
only marginal feature or label distribution shifts are considered. We describe
mathematical conditions under which SJS is identifiable. We further propose
SEES, an algorithmic framework to characterize the distribution shift under SJS
and to estimate a model's performance on new data without any labels. We
conduct extensive experiments on several real-world datasets with various ML
models. Across different datasets and distribution shifts, SEES achieves
significant (up to an order of magnitude) shift estimation error improvements
over existing approaches.",None,-1
Self-Normalized Density Map (SNDM) for Counting Microbiological Objects,0.0806549,"The statistical properties of the density map (DM) approach to counting
microbiological objects on images are studied in detail. The DM is given by
U$^2$-Net. Two statistical methods for deep neural networks are utilized: the
bootstrap and the Monte Carlo (MC) dropout. The detailed analysis of the
uncertainties for the DM predictions leads to a deeper understanding of the DM
model's deficiencies. Based on our investigation, we propose a
self-normalization module in the network. The improved network model, called
\textit{Self-Normalized Density Map} (SNDM), can correct its output density map
by itself to accurately predict the total number of objects in the image. The
SNDM architecture outperforms the original model. Moreover, both statistical
frameworks -- bootstrap and MC dropout -- have consistent statistical results
for SNDM, which were not observed in the original model. The SNDM efficiency is
comparable with the detector-base models, such as Faster and Cascade R-CNN
detectors.",None,-1
TaSPM: Targeted Sequential Pattern Mining,0.140369,"Sequential pattern mining (SPM) is an important technique of pattern mining,
which has many applications in reality. Although many efficient sequential
pattern mining algorithms have been proposed, there are few studies can focus
on target sequences. Targeted querying sequential patterns can not only reduce
the number of sequences generated by SPM, but also improve the efficiency of
users in performing pattern analysis. The current algorithms available on
targeted sequence querying are based on specific scenarios and cannot be
generalized to other applications. In this paper, we formulate the problem of
targeted sequential pattern mining and propose a generic framework namely
TaSPM, based on the fast CM-SPAM algorithm. What's more, to improve the
efficiency of TaSPM on large-scale datasets and multiple-items-based sequence
datasets, we propose several pruning strategies to reduce meaningless
operations in mining processes. Totally four pruning strategies are designed in
TaSPM, and hence it can terminate unnecessary pattern extensions quickly and
achieve better performance. Finally, we conduct extensive experiments on
different datasets to compare the existing SPM algorithms with TaSPM.
Experiments show that the novel targeted mining algorithm TaSPM can achieve
faster running time and less memory consumption.",None,-1
Model-Free Opponent Shaping,0.240624,"In general-sum games, the interaction of self-interested learning agents
commonly leads to collectively worst-case outcomes, such as defect-defect in
the iterated prisoner's dilemma (IPD). To overcome this, some methods, such as
Learning with Opponent-Learning Awareness (LOLA), shape their opponents'
learning process. However, these methods are myopic since only a small number
of steps can be anticipated, are asymmetric since they treat other agents as
naive learners, and require the use of higher-order derivatives, which are
calculated through white-box access to an opponent's differentiable learning
algorithm. To address these issues, we propose Model-Free Opponent Shaping
(M-FOS). M-FOS learns in a meta-game in which each meta-step is an episode of
the underlying inner game. The meta-state consists of the inner policies, and
the meta-policy produces a new inner policy to be used in the next episode.
M-FOS then uses generic model-free optimisation methods to learn meta-policies
that accomplish long-horizon opponent shaping. Empirically, M-FOS
near-optimally exploits naive learners and other, more sophisticated algorithms
from the literature. For example, to the best of our knowledge, it is the first
method to learn the well-known Zero-Determinant (ZD) extortion strategy in the
IPD. In the same settings, M-FOS leads to socially optimal outcomes under
meta-self-play. Finally, we show that M-FOS can be scaled to high-dimensional
settings.",https://github.com/luchris429/Model-Free-Opponent-Shaping,-1
Better Few-Shot Relation Extraction with Label Prompt Dropout,0.170024,"Few-shot relation extraction aims to learn to identify the relation between
two entities based on very limited training examples. Recent efforts found that
textual labels (i.e., relation names and relation descriptions) could be
extremely useful for learning class representations, which will benefit the
few-shot learning task. However, what is the best way to leverage such label
information in the learning process is an important research question. Existing
works largely assume such textual labels are always present during both
learning and prediction. In this work, we argue that such approaches may not
always lead to optimal results. Instead, we present a novel approach called
label prompt dropout, which randomly removes label descriptions in the learning
process. Our experiments show that our approach is able to lead to improved
class representations, yielding significantly better results on the few-shot
relation extraction task.",https://github.com/jzhang38/LPD,-1
Asynchronous Actor-Critic for Multi-Agent Reinforcement Learning,0.073907,"Synchronizing decisions across multiple agents in realistic settings is
problematic since it requires agents to wait for other agents to terminate and
communicate about termination reliably. Ideally, agents should learn and
execute asynchronously instead. Such asynchronous methods also allow temporally
extended actions that can take different amounts of time based on the situation
and action executed. Unfortunately, current policy gradient methods are not
applicable in asynchronous settings, as they assume that agents synchronously
reason about action selection at every time step. To allow asynchronous
learning and decision-making, we formulate a set of asynchronous multi-agent
actor-critic methods that allow agents to directly optimize asynchronous
policies in three standard training paradigms: decentralized learning,
centralized learning, and centralized training for decentralized execution.
Empirical results (in simulation and hardware) in a variety of realistic
domains demonstrate the superiority of our approaches in large multi-agent
problems and validate the effectiveness of our algorithms for learning
high-quality and asynchronous solutions.",https://github.com/mgualti/PointCloudsPython,-1
Training Language Models with Language Feedback,0.22071,"Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level summarization
ability.",None,150035
RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection,0.0,"This paper presents a Refinement Pyramid Transformer (RePFormer) for robust
facial landmark detection. Most facial landmark detectors focus on learning
representative image features. However, these CNN-based feature representations
are not robust enough to handle complex real-world scenarios due to ignoring
the internal structure of landmarks, as well as the relations between landmarks
and context. In this work, we formulate the facial landmark detection task as
refining landmark queries along pyramid memories. Specifically, a pyramid
transformer head (PTH) is introduced to build both homologous relations among
landmarks and heterologous relations between landmarks and cross-scale
contexts. Besides, a dynamic landmark refinement (DLR) module is designed to
decompose the landmark regression into an end-to-end refinement procedure,
where the dynamically aggregated queries are transformed to residual
coordinates predictions. Extensive experimental results on four facial landmark
detection benchmarks and their various subsets demonstrate the superior
performance and high robustness of our framework.",None,51979
The Neural Race Reduction: Dynamics of Abstraction in Gated Networks,0.0579237,"Our theoretical understanding of deep learning has not kept pace with its
empirical success. While network architecture is known to be critical, we do
not yet understand its effect on learned representations and network behavior,
or how this architecture should reflect task structure.In this work, we begin
to address this gap by introducing the Gated Deep Linear Network framework that
schematizes how pathways of information flow impact learning dynamics within an
architecture. Crucially, because of the gating, these networks can compute
nonlinear functions of their input. We derive an exact reduction and, for
certain cases, exact solutions to the dynamics of learning. Our analysis
demonstrates that the learning dynamics in structured networks can be
conceptualized as a neural race with an implicit bias towards shared
representations, which then govern the model's ability to systematically
generalize, multi-task, and transfer. We validate our key insights on
naturalistic datasets and with relaxed assumptions. Taken together, our work
gives rise to general hypotheses relating neural architecture to learning and
provides a mathematical approach towards understanding the design of more
complex architectures and the role of modularity and compositionality in
solving real-world problems. The code and results are available at
https://www.saxelab.org/gated-dln .",None,1844
A Deep-Discrete Learning Framework for Spherical Surface Registration,0.0584814,"Cortical surface registration is a fundamental tool for neuroimaging analysis
that has been shown to improve the alignment of functional regions relative to
volumetric approaches. Classically, image registration is performed by
optimizing a complex objective similarity function, leading to long run times.
This contributes to a convention for aligning all data to a global average
reference frame that poorly reflects the underlying cortical heterogeneity. In
this paper, we propose a novel unsupervised learning-based framework that
converts registration to a multi-label classification problem, where each point
in a low-resolution control grid deforms to one of fixed, finite number of
endpoints. This is learned using a spherical geometric deep learning
architecture, in an end-to-end unsupervised way, with regularization imposed
using a deep Conditional Random Field (CRF). Experiments show that our proposed
framework performs competitively, in terms of similarity and areal distortion,
relative to the most popular classical surface registration algorithms and
generates smoother deformations than other learning-based surface registration
methods, even in subjects with atypical cortical morphology.",https://github.com/ThomasYeoLab/CBIG,10746
Non-Autoregressive Machine Translation: It's Not as Fast as it Seems,0.140265,"Efficient machine translation models are commercially important as they can
increase inference speeds, and reduce costs and carbon emissions. Recently,
there has been much interest in non-autoregressive (NAR) models, which promise
faster translation. In parallel to the research on NAR models, there have been
successful attempts to create optimized autoregressive models as part of the
WMT shared task on efficient translation. In this paper, we point out flaws in
the evaluation methodology present in the literature on NAR models and we
provide a fair comparison between a state-of-the-art NAR model and the
autoregressive submissions to the shared task. We make the case for consistent
evaluation of NAR models, and also for the importance of comparing NAR models
with other widely used methods for improving efficiency. We run experiments
with a connectionist-temporal-classification-based (CTC) NAR model implemented
in C++ and compare it with AR models using wall clock times. Our results show
that, although NAR models are faster on GPUs, with small batch sizes, they are
almost always slower under more realistic usage conditions. We call for more
realistic and extensive evaluation of NAR models in future work.",https://github.com/jindrahelcl/marian-dev,23570
On the Paradox of Learning to Reason from Data,0.0416266,"Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be
trained end-to-end to solve logical reasoning problems presented in natural
language? We attempt to answer this question in a confined problem space where
there exists a set of parameters that perfectly simulates logical reasoning. We
make observations that seem to contradict each other: BERT attains near-perfect
accuracy on in-distribution test examples while failing to generalize to other
data distributions over the exact same problem space. Our study provides an
explanation for this paradox: instead of learning to emulate the correct
reasoning function, BERT has in fact learned statistical features that
inherently exist in logical reasoning problems. We also show that it is
infeasible to jointly remove statistical features from data, illustrating the
difficulty of learning to reason in general. Our result naturally extends to
other neural models and unveils the fundamental difference between learning to
reason and learning to achieve high performance on NLP benchmarks using
statistical features.",https://github.com/joshuacnf/paradox-learning2reason,34559
Controlling Bias Exposure for Fair Interpretable Predictions,0.101849,"Recent work on reducing bias in NLP models usually focuses on protecting or
isolating information related to a sensitive attribute (like gender or race).
However, when sensitive information is semantically entangled with the task
information of the input, e.g., gender information is predictive for a
profession, a fair trade-off between task performance and bias mitigation is
difficult to achieve. Existing approaches perform this trade-off by eliminating
bias information from the latent space, lacking control over how much bias is
necessarily required to be removed. We argue that a favorable debiasing method
should use sensitive information 'fairly', rather than blindly eliminating it
(Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we
provide a novel debiasing algorithm by adjusting the predictive model's belief
to (1) ignore the sensitive information if it is not useful for the task; (2)
use sensitive information minimally as necessary for the prediction (while also
incurring a penalty). Experimental results on two text classification tasks
(influenced by gender) and an open-ended generation task (influenced by race)
indicate that our model achieves a desirable trade-off between debiasing and
task performance along with producing debiased rationales as evidence.",https://github.com/ZexueHe/interpretable_debiasing,30843
InducT-GCN: Inductive Graph Convolutional Networks for Text Classification,0.0701222,"Text classification aims to assign labels to textual units by making use of
global information. Recent studies have applied graph neural network (GNN) to
capture the global word co-occurrence in a corpus. Existing approaches require
that all the nodes (training and test) in a graph are present during training,
which are transductive and do not naturally generalise to unseen nodes. To make
those models inductive, they use extra resources, like pretrained word
embedding. However, high-quality resource is not always available and hard to
train. Under the extreme settings with no extra resource and limited amount of
training set, can we still learn an inductive graph-based text classification
model? In this paper, we introduce a novel inductive graph-based text
classification framework, InducT-GCN (InducTive Graph Convolutional Networks
for Text classification). Compared to transductive models that require test
documents in training, we construct a graph based on the statistics of training
documents only and represent document vectors with a weighted sum of word
vectors. We then conduct one-directional GCN propagation during testing. Across
five text classification benchmarks, our InducT-GCN outperformed
state-of-the-art methods that are either transductive in nature or pre-trained
additional resources. We also conducted scalability testing by gradually
increasing the data size and revealed that our InducT-GCN can reduce the time
and space complexity. The code is available on:
https://github.com/usydnlp/InductTGCN.",https://github.com/usydnlp/InductTGCN,3559
Improving Multilingual Neural Machine Translation System for Indic Languages,0.0371719,"Machine Translation System (MTS) serves as an effective tool for
communication by translating text or speech from one language to another
language. The need of an efficient translation system becomes obvious in a
large multilingual environment like India, where English and a set of Indian
Languages (ILs) are officially used. In contrast with English, ILs are still
entreated as low-resource languages due to unavailability of corpora. In order
to address such asymmetric nature, multilingual neural machine translation
(MNMT) system evolves as an ideal approach in this direction. In this paper, we
propose a MNMT system to address the issues related to low-resource language
translation. Our model comprises of two MNMT systems i.e. for English-Indic
(one-to-many) and the other for Indic-English (many-to-one) with a shared
encoder-decoder containing 15 language pairs (30 translation directions). Since
most of IL pairs have scanty amount of parallel corpora, not sufficient for
training any machine translation model. We explore various augmentation
strategies to improve overall translation quality through the proposed model. A
state-of-the-art transformer architecture is used to realize the proposed
model. Trials over a good amount of data reveal its superiority over the
conventional models. In addition, the paper addresses the use of language
relationships (in terms of dialect, script, etc.), particularly about the role
of high-resource languages of the same family in boosting the performance of
low-resource languages. Moreover, the experimental results also show the
advantage of backtranslation and domain adaptation for ILs to enhance the
translation quality of both source and target languages. Using all these key
approaches, our proposed model emerges to be more efficient than the baseline
model in terms of evaluation metrics i.e BLEU (BiLingual Evaluation Understudy)
score for a set of ILs.",None,50
Fine-Grained Object Classification via Self-Supervised Pose Alignment,0.418873,"Semantic patterns of fine-grained objects are determined by subtle appearance
difference of local parts, which thus inspires a number of part-based methods.
However, due to uncontrollable object poses in images, distinctive details
carried by local regions can be spatially distributed or even self-occluded,
leading to a large variation on object representation. For discounting pose
variations, this paper proposes to learn a novel graph based object
representation to reveal a global configuration of local parts for
self-supervised pose alignment across classes, which is employed as an
auxiliary feature regularization on a deep representation learning
network.Moreover, a coarse-to-fine supervision together with the proposed
pose-insensitive constraint on shallow-to-deep sub-networks encourages
discriminative features in a curriculum learning manner. We evaluate our method
on three popular fine-grained object classification benchmarks, consistently
achieving the state-of-the-art performance. Source codes are available at
https://github.com/yangxh11/P2P-Net.",https://github.com/yangxh11/P2P-Net,15319
Agile Maneuvers in Legged Robots: a Predictive Control Approach,0.0954752,"Planning and execution of agile locomotion maneuvers have been a longstanding
challenge in legged robotics. It requires to derive motion plans and local
feedback policies in real-time to handle the nonholonomy of the kinetic
momenta. To achieve so, we propose a hybrid predictive controller that
considers the robot's actuation limits and full-body dynamics. It combines the
feedback policies with tactile information to locally predict future actions.
It converges within a few milliseconds thanks to a feasibility-driven approach.
Our predictive controller enables ANYmal robots to generate agile maneuvers in
realistic scenarios. A crucial element is to track the local feedback policies
as, in contrast to whole-body control, they achieve the desired angular
momentum. To the best of our knowledge, our predictive controller is the first
to handle actuation limits, generate agile locomotion maneuvers, and execute
optimal feedback policies for low level torque control without the use of a
separate whole-body controller.",None,4536
Does Simultaneous Speech Translation need Simultaneous Models?,0.03624,"In simultaneous speech translation (SimulST), finding the best trade-off
between high translation quality and low latency is a challenging task. To meet
the latency constraints posed by the different application scenarios, multiple
dedicated SimulST models are usually trained and maintained, generating high
computational costs. In this paper, motivated by the increased social and
environmental impact caused by these costs, we investigate whether a single
model trained offline can serve not only the offline but also the simultaneous
task without the need for any additional training or adaptation. Experiments on
en->{de, es} indicate that, aside from facilitating the adoption of
well-established offline techniques and architectures without affecting
latency, the offline solution achieves similar or better translation quality
compared to the same model trained in simultaneous settings, as well as being
competitive with the SimulST state of the art.",https://github.com/hlt-mt/,9038
How would Stance Detection Techniques Evolve after the Launch of ChatGPT?,0.854876,"Stance detection refers to the task of extracting the standpoint (Favor,
Against or Neither) towards a target in given texts. Such research gains
increasing attention with the proliferation of social media contents. The
conventional framework of handling stance detection is converting it into text
classification tasks. Deep learning models have already replaced rule-based
models and traditional machine learning models in solving such problems.
Current deep neural networks are facing two main challenges which are
insufficient labeled data and information in social media posts and the
unexplainable nature of deep learning models. A new pre-trained language model
chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our
experiments show that ChatGPT can achieve SOTA or similar performance for
commonly used datasets including SemEval-2016 and P-Stance. At the same time,
ChatGPT can provide explanation for its own prediction, which is beyond the
capability of any existing model. The explanations for the cases it cannot
provide classification results are especially useful. ChatGPT has the potential
to be the best AI model for stance detection tasks in NLP, or at least change
the research paradigm of this field. ChatGPT also opens up the possibility of
building explanatory AI for stance detection.",None,648
WinoDict: Probing language models for in-context word acquisition,0.0583544,"We introduce a new in-context learning paradigm to measure Large Language
Models' (LLMs) ability to learn novel words during inference. In particular, we
rewrite Winograd-style co-reference resolution problems by replacing the key
concept word with a synthetic but plausible word that the model must understand
to complete the task. Solving this task requires the model to make use of the
dictionary definition of the new word given in the prompt. This benchmark
addresses word acquisition, one important aspect of the diachronic degradation
known to afflict LLMs. As LLMs are frozen in time at the moment they are
trained, they are normally unable to reflect the way language changes over
time. We show that the accuracy of LLMs compared to the original Winograd tasks
decreases radically in our benchmark, thus identifying a limitation of current
models and providing a benchmark to measure future improvements in LLMs ability
to do in-context learning.",https://github.com/google-research/language/tree/master/language/wino_dict,2175
SQuId: Measuring Speech Naturalness in Many Languages,0.196008,"Much of text-to-speech research relies on human evaluation, which incurs
heavy costs and slows down the development process. The problem is particularly
acute in heavily multilingual applications, where recruiting and polling judges
can take weeks. We introduce SQuId (Speech Quality Identification), a
multilingual naturalness prediction model trained on over a million ratings and
tested in 65 locales-the largest effort of this type to date. The main insight
is that training one model on many locales consistently outperforms mono-locale
baselines. We present our task, the model, and show that it outperforms a
competitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then
demonstrate the effectiveness of cross-locale transfer during fine-tuning and
highlight its effect on zero-shot locales, i.e., locales for which there is no
fine-tuning data. Through a series of analyses, we highlight the role of
non-linguistic effects such as sound artifacts in cross-locale transfer.
Finally, we present the effect of our design decision, e.g., model size,
pre-training diversity, and language rebalancing with several ablation
experiments.",None,10439
MiniViT: Compressing Vision Transformers with Weight Multiplexing,0.967283,"Vision Transformer (ViT) models have recently drawn much attention in
computer vision due to their high model capability. However, ViT models suffer
from huge number of parameters, restricting their applicability on devices with
limited memory. To alleviate this problem, we propose MiniViT, a new
compression framework, which achieves parameter reduction in vision
transformers while retaining the same performance. The central idea of MiniViT
is to multiplex the weights of consecutive transformer blocks. More
specifically, we make the weights shared across layers, while imposing a
transformation on the weights to increase diversity. Weight distillation over
self-attention is also applied to transfer knowledge from large-scale ViT
models to weight-multiplexed compact models. Comprehensive experiments
demonstrate the efficacy of MiniViT, showing that it can reduce the size of the
pre-trained Swin-B transformer by 48\%, while achieving an increase of 1.0\% in
Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters,
MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters,
without seriously compromising the performance. Finally, we verify the
transferability of MiniViT by reporting its performance on downstream
benchmarks. Code and models are available at here.",None,18607
PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation,0.160137,"In this paper, we propose a new deep learning-based method for estimating
room layout given a pair of 360 panoramas. Our system, called Position-aware
Stereo Merging Network or PSMNet, is an end-to-end joint layout-pose estimator.
PSMNet consists of a Stereo Pano Pose (SP2) transformer and a novel
Cross-Perspective Projection (CP2) layer. The stereo-view SP2 transformer is
used to implicitly infer correspondences between views, and can handle noisy
poses. The pose-aware CP2 layer is designed to render features from the
adjacent view to the anchor (reference) view, in order to perform view fusion
and estimate the visible layout. Our experiments and analysis validate our
method, which significantly outperforms the state-of-the-art layout estimators,
especially for large and complex room spaces.",None,28982
4D-StOP: Panoptic Segmentation of 4D LiDAR using Spatio-temporal Object Proposal Generation and Aggregation,0.638811,"In this work, we present a new paradigm, called 4D-StOP, to tackle the task
of 4D Panoptic LiDAR Segmentation. 4D-StOP first generates spatio-temporal
proposals using voting-based center predictions, where each point in the 4D
volume votes for a corresponding center. These tracklet proposals are further
aggregated using learned geometric features. The tracklet aggregation method
effectively generates a video-level 4D scene representation over the entire
space-time volume. This is in contrast to existing end-to-end trainable
state-of-the-art approaches which use spatio-temporal embeddings that are
represented by Gaussian probability distributions. Our voting-based tracklet
generation method followed by geometric feature-based aggregation generates
significantly improved panoptic LiDAR segmentation quality when compared to
modeling the entire 4D volume using Gaussian probability distributions. 4D-StOP
achieves a new state-of-the-art when applied to the SemanticKITTI test dataset
with a score of 63.9 LSTQ, which is a large (+7%) improvement compared to
current best-performing end-to-end trainable methods. The code and pre-trained
models are available at: https://github.com/LarsKreuzberg/4D-StOP.",https://github.com/LarsKreuzberg/4D-StOP,35042
A Self-Guided Framework for Radiology Report Generation,0.323976,"Automatic radiology report generation is essential to computer-aided
diagnosis. Through the success of image captioning, medical report generation
has been achievable. However, the lack of annotated disease labels is still the
bottleneck of this area. In addition, the image-text data bias problem and
complex sentences make it more difficult to generate accurate reports. To
address these gaps, we pre-sent a self-guided framework (SGF), a suite of
unsupervised and supervised deep learning methods to mimic the process of human
learning and writing. In detail, our framework obtains the domain knowledge
from medical reports with-out extra disease labels and guides itself to extract
fined-grain visual features as-sociated with the text. Moreover, SGF
successfully improves the accuracy and length of medical report generation by
incorporating a similarity comparison mechanism that imitates the process of
human self-improvement through compar-ative practice. Extensive experiments
demonstrate the utility of our SGF in the majority of cases, showing its
superior performance over state-of-the-art meth-ods. Our results highlight the
capacity of the proposed framework to distinguish fined-grained visual details
between words and verify its advantage in generating medical reports.",None,2284
VoLux-GAN: A Generative Model for 3D Face Synthesis with HDRI Relighting,0.0928042,"We propose VoLux-GAN, a generative framework to synthesize 3D-aware faces
with convincing relighting. Our main contribution is a volumetric HDRI
relighting method that can efficiently accumulate albedo, diffuse and specular
lighting contributions along each 3D ray for any desired HDR environmental map.
Additionally, we show the importance of supervising the image decomposition
process using multiple discriminators. In particular, we propose a data
augmentation technique that leverages recent advances in single image portrait
relighting to enforce consistent geometry, albedo, diffuse and specular
components. Multiple experiments and comparisons with other generative
frameworks show how our model is a step forward towards photorealistic
relightable 3D generative models.",None,10370
"Event Causality Identification with Causal News Corpus -- Shared Task 3, CASE 2022",0.230731,"The Event Causality Identification Shared Task of CASE 2022 involved two
subtasks working on the Causal News Corpus. Subtask 1 required participants to
predict if a sentence contains a causal relation or not. This is a supervised
binary classification task. Subtask 2 required participants to identify the
Cause, Effect and Signal spans per causal sentence. This could be seen as a
supervised sequence labeling task. For both subtasks, participants uploaded
their predictions for a held-out test set, and ranking was done based on binary
F1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes
the work of the 17 teams that submitted their results to our competition and 12
system description papers that were received. The best F1 scores achieved for
Subtask 1 and 2 were 86.19% and 54.15%, respectively. All the top-performing
approaches involved pre-trained language models fine-tuned to the targeted
task. We further discuss these approaches and analyze errors across
participants' systems in this paper.",https://github.com/tanfiona/UniCausal,3459
Bridging POMDPs and Bayesian decision making for robust maintenance planning under model uncertainty: An application to railway systems,0.0878442,"Structural Health Monitoring (SHM) describes a process for inferring
quantifiable metrics of structural condition, which can serve as input to
support decisions on the operation and maintenance of infrastructure assets.
Given the long lifespan of critical structures, this problem can be cast as a
sequential decision making problem over prescribed horizons. Partially
Observable Markov Decision Processes (POMDPs) offer a formal framework to solve
the underlying optimal planning task. However, two issues can undermine the
POMDP solutions. Firstly, the need for a model that can adequately describe the
evolution of the structural condition under deterioration or corrective actions
and, secondly, the non-trivial task of recovery of the observation process
parameters from available monitoring data. Despite these potential challenges,
the adopted POMDP models do not typically account for uncertainty on model
parameters, leading to solutions which can be unrealistically confident. In
this work, we address both key issues. We present a framework to estimate POMDP
transition and observation model parameters directly from available data, via
Markov Chain Monte Carlo (MCMC) sampling of a Hidden Markov Model (HMM)
conditioned on actions. The MCMC inference estimates distributions of the
involved model parameters. We then form and solve the POMDP problem by
exploiting the inferred distributions, to derive solutions that are robust to
model uncertainty. We successfully apply our approach on maintenance planning
for railway track assets on the basis of a ""fractal value"" indicator, which is
computed from actual railway monitoring data.",http://github.com/google/jax,9608
Multiple Instance Neuroimage Transformer,0.0140595,"For the first time, we propose using a multiple instance learning based
convolution-free transformer model, called Multiple Instance Neuroimage
Transformer (MINiT), for the classification of T1weighted (T1w) MRIs. We first
present several variants of transformer models adopted for neuroimages. These
models extract non-overlapping 3D blocks from the input volume and perform
multi-headed self-attention on a sequence of their linear projections. MINiT,
on the other hand, treats each of the non-overlapping 3D blocks of the input
MRI as its own instance, splitting it further into non-overlapping 3D patches,
on which multi-headed self-attention is computed. As a proof-of-concept, we
evaluate the efficacy of our model by training it to identify sex from T1w-MRIs
of two public datasets: Adolescent Brain Cognitive Development (ABCD) and the
National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA).
The learned attention maps highlight voxels contributing to identifying sex
differences in brain morphometry. The code is available at
https://github.com/singlaayush/MINIT.",None,14913
GIFS: Neural Implicit Function for General Shape Representation,0.473087,"Recent development of neural implicit function has shown tremendous success
on high-quality 3D shape reconstruction. However, most works divide the space
into inside and outside of the shape, which limits their representing power to
single-layer and watertight shapes. This limitation leads to tedious data
processing (converting non-watertight raw data to watertight) as well as the
incapability of representing general object shapes in the real world. In this
work, we propose a novel method to represent general shapes including
non-watertight shapes and shapes with multi-layer surfaces. We introduce
General Implicit Function for 3D Shape (GIFS), which models the relationships
between every two points instead of the relationships between points and
surfaces. Instead of dividing 3D space into predefined inside-outside regions,
GIFS encodes whether two points are separated by any surface. Experiments on
ShapeNet show that GIFS outperforms previous state-of-the-art methods in terms
of reconstruction quality, rendering efficiency, and visual fidelity. Project
page is available at https://jianglongye.com/gifs .",https://jianglongye.com/gifs,25908
On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,0.192512,"Many recent studies on large-scale language models have reported successful
in-context zero- and few-shot learning ability. However, the in-depth analysis
of when in-context learning occurs is still lacking. For example, it is unknown
how in-context learning performance changes as the training corpus varies.
Here, we investigate the effects of the source and size of the pretraining
corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From
our in-depth investigation, we introduce the following observations: (1)
in-context learning performance heavily depends on the corpus domain source,
and the size of the pretraining corpus does not necessarily determine the
emergence of in-context learning, (2) in-context learning ability can emerge
when a language model is trained on a combination of multiple corpora, even
when each corpus does not result in in-context learning on its own, (3)
pretraining with a corpus related to a downstream task does not always
guarantee the competitive in-context learning performance of the downstream
task, especially in the few-shot setting, and (4) the relationship between
language modeling (measured in perplexity) and in-context learning does not
always correlate: e.g., low perplexity does not always imply high in-context
few-shot learning performance.",None,150035
Atari-5: Distilling the Arcade Learning Environment down to Five Games,0.0160244,"The Arcade Learning Environment (ALE) has become an essential benchmark for
assessing the performance of reinforcement learning algorithms. However, the
computational cost of generating results on the entire 57-game dataset limits
ALE's use and makes the reproducibility of many results infeasible. We propose
a novel solution to this problem in the form of a principled methodology for
selecting small but representative subsets of environments within a benchmark
suite. We applied our method to identify a subset of five ALE games, called
Atari-5, which produces 57-game median score estimates within 10% of their true
values. Extending the subset to 10-games recovers 80% of the variance for
log-scores for all games within the 57-game set. We show this level of
compression is possible due to a high degree of correlation between many of the
games in ALE.",https://github.com/maitchison/Atari-5,10193
Deep Surrogate Assisted Generation of Environments,0.0741055,"Recent progress in reinforcement learning (RL) has started producing
generally capable agents that can solve a distribution of complex environments.
These agents are typically tested on fixed, human-authored environments. On the
other hand, quality diversity (QD) optimization has been proven to be an
effective component of environment generation algorithms, which can generate
collections of high-quality environments that are diverse in the resulting
agent behaviors. However, these algorithms require potentially expensive
simulations of agents on newly generated environments. We propose Deep
Surrogate Assisted Generation of Environments (DSAGE), a sample-efficient QD
environment generation algorithm that maintains a deep surrogate model for
predicting agent behaviors in new environments. Results in two benchmark
domains show that DSAGE significantly outperforms existing QD environment
generation algorithms in discovering collections of environments that elicit
diverse behaviors of a state-of-the-art RL agent and a planning agent. Our
source code and videos are available at https://dsagepaper.github.io/.",https://dsagepaper.github.io/,3297
Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint,0.344497,"Active learning is a promising alternative to alleviate the issue of high
annotation cost in the computer vision tasks by consciously selecting more
informative samples to label. Active learning for object detection is more
challenging and existing efforts on it are relatively rare. In this paper, we
propose a novel hybrid approach to address this problem, where the
instance-level uncertainty and diversity are jointly considered in a bottom-up
manner. To balance the computational complexity, the proposed approach is
designed as a two-stage procedure. At the first stage, an Entropy-based
Non-Maximum Suppression (ENMS) is presented to estimate the uncertainty of
every image, which performs NMS according to the entropy in the feature space
to remove predictions with redundant information gains. At the second stage, a
diverse prototype (DivProto) strategy is explored to ensure the diversity
across images by progressively converting it into the intra-class and
inter-class diversities of the entropy-based class-specific prototypes.
Extensive experiments are conducted on MS COCO and Pascal VOC, and the proposed
approach achieves state of the art results and significantly outperforms the
other counterparts, highlighting its superiority.",https://github.com/facebookresearch/maskrcnn-benchmark,347
Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents,0.0789382,"At the heart of improving conversational AI is the open problem of how to
evaluate conversations. Issues with automatic metrics are well known (Liu et
al., 2016, arXiv:1603.08023), with human evaluations still considered the gold
standard. Unfortunately, how to perform human evaluations is also an open
problem: differing data collection methods have varying levels of human
agreement and statistical sensitivity, resulting in differing amounts of human
annotation hours and labor costs. In this work we compare five different
crowdworker-based human evaluation methods and find that different methods are
best depending on the types of models compared, with no clear winner across the
board. While this highlights the open problems in the area, our analysis leads
to advice of when to use which one, and possible future directions.",None,7769
CrowdFormer: Weakly-supervised Crowd counting with Improved Generalizability,0.297121,"Convolutional neural networks (CNNs) have dominated the field of computer
vision for nearly a decade due to their strong ability to learn local features.
However, due to their limited receptive field, CNNs fail to model the global
context. On the other hand, transformer, an attention-based architecture can
model the global context easily. Despite this, there are limited studies that
investigate the effectiveness of transformers in crowd counting. In addition,
the majority of the existing crowd counting methods are based on the regression
of density maps which requires point-level annotation of each person present in
the scene. This annotation task is laborious and also error-prone. This has led
to increased focus on weakly-supervised crowd counting methods which require
only the count-level annotations. In this paper, we propose a weakly-supervised
method for crowd counting using a pyramid vision transformer. We have conducted
extensive evaluations to validate the effectiveness of the proposed method. Our
method is comparable to the state-of-the-art on the benchmark crowd datasets.
More importantly, it shows remarkable generalizability.",None,2627
Redwood: Using Collision Detection to Grow a Large-Scale Intent Classification Dataset,0.0944497,"Dialog systems must be capable of incorporating new skills via updates over
time in order to reflect new use cases or deployment scenarios. Similarly,
developers of such ML-driven systems need to be able to add new training data
to an already-existing dataset to support these new skills. In intent
classification systems, problems can arise if training data for a new skill's
intent overlaps semantically with an already-existing intent. We call such
cases collisions. This paper introduces the task of intent collision detection
between multiple datasets for the purposes of growing a system's skillset. We
introduce several methods for detecting collisions, and evaluate our methods on
real datasets that exhibit collisions. To highlight the need for intent
collision detection, we show that model performance suffers if new data is
added in such a way that does not arbitrate colliding intents. Finally, we use
collision detection to construct and benchmark a new dataset, Redwood, which is
composed of 451 ntent categories from 13 original intent classification
datasets, making it the largest publicly available intent classification
benchmark.",https://github.com/gxlarson/redwood,1652
Self-supervised models of audio effectively explain human cortical responses to speech,0.215203,"Self-supervised language models are very effective at predicting high-level
cortical responses during language comprehension. However, the best current
models of lower-level auditory processing in the human brain rely on either
hand-constructed acoustic filters or representations from supervised audio
neural networks. In this work, we capitalize on the progress of self-supervised
speech representation learning (SSL) to create new state-of-the-art models of
the human auditory system. Compared against acoustic baselines, phonemic
features, and supervised models, representations from the middle layers of
self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently
yield the best prediction performance for fMRI recordings within the auditory
cortex (AC). Brain areas involved in low-level auditory processing exhibit a
preference for earlier SSL model layers, whereas higher-level semantic areas
prefer later layers. We show that these trends are due to the models' ability
to encode information at multiple linguistic levels (acoustic, phonetic, and
lexical) along their representation depth. Overall, these results show that
self-supervised models effectively capture the hierarchy of information
relevant to different stages of speech processing in human cortex.",None,589
mGPT: Few-Shot Learners Go Multilingual,0.286719,"Recent studies report that autoregressive language models can successfully
solve many NLP tasks via zero- and few-shot learning paradigms, which opens up
new possibilities for using the pre-trained language models. This paper
introduces two autoregressive GPT-like models with 1.3 billion and 13 billion
parameters trained on 60 languages from 25 language families using Wikipedia
and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using
GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron
frameworks allow us to parallelize the training and inference steps
effectively. The resulting models show performance on par with the recently
released XGLM models by Facebook, covering more languages and enhancing NLP
possibilities for low resource languages of CIS countries and Russian small
nations. We detail the motivation for the choices of the architecture design,
thoroughly describe the data preparation pipeline, and train five small
versions of the model to choose the most optimal multilingual tokenization
strategy. We measure the model perplexity in all covered languages and evaluate
it on the wide spectre of multilingual tasks, including classification,
generative, sequence labeling and knowledge probing. The models were evaluated
with the zero-shot and few-shot methods. Furthermore, we compared the
classification tasks with the state-of-the-art multilingual model XGLM. source
code and the mGPT XL model are publicly released.",https://github.com/ai-forever/mgpt,338
Multi-task Learning for Cross-Lingual Sentiment Analysis,0.132072,"This paper presents a cross-lingual sentiment analysis of news articles using
zero-shot and few-shot learning. The study aims to classify the Croatian news
articles with positive, negative, and neutral sentiments using the Slovene
dataset. The system is based on a trilingual BERT-based model trained in three
languages: English, Slovene, Croatian. The paper analyses different setups
using datasets in two languages and proposes a simple multi-task model to
perform sentiment classification. The evaluation is performed using the
few-shot and zero-shot scenarios in single-task and multi-task experiments for
Croatian and Slovene.",https://github.com/cleopatra-itn/SentimentAnalyserSLHRNews,1490
Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost,0.0755199,"State-of-the-art NLP systems represent inputs with word embeddings, but these
are brittle when faced with Out-of-Vocabulary (OOV) words. To address this
issue, we follow the principle of mimick-like models to generate vectors for
unseen words, by learning the behavior of pre-trained embeddings using only the
surface form of words. We present a simple contrastive learning framework,
LOVE, which extends the word representation of an existing pre-trained language
model (such as BERT), and makes it robust to OOV with few additional
parameters. Extensive evaluations demonstrate that our lightweight model
achieves similar or even better performances than prior competitors, both on
original datasets and on corrupted variants. Moreover, it can be used in a
plug-and-play fashion with FastText and BERT, where it significantly improves
their robustness.",https://github.com/tigerchen52/LOVE,129114
A context-aware knowledge transferring strategy for CTC-based ASR,0.130547,"Non-autoregressive automatic speech recognition (ASR) modeling has received
increasing attention recently because of its fast decoding speed and superior
performance. Among representatives, methods based on the connectionist temporal
classification (CTC) are still a dominating stream. However, the theoretically
inherent flaw, the assumption of independence between tokens, creates a
performance barrier for the school of works. To mitigate the challenge, we
propose a context-aware knowledge transferring strategy, consisting of a
knowledge transferring module and a context-aware training strategy, for
CTC-based ASR. The former is designed to distill linguistic information from a
pre-trained language model, and the latter is framed to modulate the
limitations caused by the conditional independence assumption. As a result, a
knowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is
presented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2
datasets demonstrate the effectiveness of the proposed method.",None,810
Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity,0.117484,"This paper analyzes three formal models of Transformer encoders that differ
in the form of their self-attention mechanism: unique hard attention (UHAT);
generalized unique hard attention (GUHAT), which generalizes UHAT; and
averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers,
viewed as string acceptors, can only recognize formal languages in the
complexity class AC$^0$, the class of languages recognizable by families of
Boolean circuits of constant depth and polynomial size. This upper bound
subsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages
or the PARITY language, since those languages are outside AC$^0$ (Furst et al.,
1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are
recognizable by AHAT networks, implying that AHAT can recognize languages that
UHAT and GUHAT cannot.",None,20886
Sufficient Statistic Memory Approximate Message Passing,0.0458069,"Approximate message passing (AMP) type algorithms have been widely used in
the signal reconstruction of certain large random linear systems. A key feature
of the AMP-type algorithms is that their dynamics can be correctly described by
state evolution. However, state evolution does not necessarily guarantee the
convergence of iterative algorithms. To solve the convergence problem of
AMP-type algorithms in principle, this paper proposes a memory AMP (MAMP) under
a sufficient statistic condition, named sufficient statistic MAMP (SS-MAMP). We
show that the covariance matrices of SS-MAMP are L-banded and convergent. Given
an arbitrary MAMP, we can construct the SS-MAMP by damping, which not only
ensures the convergence, but also preserves the orthogonality, i.e., its
dynamics can be correctly described by state evolution.",None,-1
Medical Image Captioning via Generative Pretrained Transformers,0.097783,"The automatic clinical caption generation problem is referred to as proposed
model combining the analysis of frontal chest X-Ray scans with structured
patient information from the radiology records. We combine two language models,
the Show-Attend-Tell and the GPT-3, to generate comprehensive and descriptive
radiology records. The proposed combination of these models generates a textual
summary with the essential information about pathologies found, their location,
and the 2D heatmaps localizing each pathology on the original X-Ray scans. The
proposed model is tested on two medical datasets, the Open-I, MIMIC-CXR, and
the general-purpose MS-COCO. The results measured with the natural language
assessment metrics prove their efficient applicability to the chest X-Ray image
captioning.",None,-1
Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization,0.159083,"In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute
human pose estimation with calibrated camera. Accurate and generalizable
absolute 3D human pose estimation from monocular 2D pose input is an ill-posed
problem. To address this challenge, we convert the input from pixel space to 3D
normalized rays. This conversion makes our approach robust to camera intrinsic
parameter changes. To deal with the in-the-wild camera extrinsic parameter
variations, Ray3D explicitly takes the camera extrinsic parameters as an input
and jointly models the distribution between the 3D pose rays and camera
extrinsic parameters. This novel network design is the key to the outstanding
generalizability of Ray3D approach. To have a comprehensive understanding of
how the camera intrinsic and extrinsic parameter variations affect the accuracy
of absolute 3D key-point localization, we conduct in-depth systematic
experiments on three single person 3D benchmarks as well as one synthetic
benchmark. These experiments demonstrate that our method significantly
outperforms existing state-of-the-art models. Our code and the synthetic
dataset are available at https://github.com/YxZhxn/Ray3D .",https://github.com/YxZhxn/Ray3D,-1
Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings,0.145973,"Although contextualized embeddings generated from large-scale pre-trained
models perform well in many tasks, traditional static embeddings (e.g.,
Skip-gram, Word2Vec) still play an important role in low-resource and
lightweight settings due to their low computational cost, ease of deployment,
and stability. In this paper, we aim to improve word embeddings by 1)
incorporating more contextual information from existing pre-trained models into
the Skip-gram framework, which we call Context-to-Vec; 2) proposing a
post-processing retrofitting method for static embeddings independent of
training by employing priori synonym knowledge and weighted vector
distribution. Through extrinsic and intrinsic tasks, our methods are well
proven to outperform the baselines by a large margin.",https://github.com/binbinjiang/Context2Vector,70015
Attention Based Neural Networks for Wireless Channel Estimation,0.0682463,"In this paper, we deploy the self-attention mechanism to achieve improved
channel estimation for orthogonal frequency-division multiplexing waveforms in
the downlink. Specifically, we propose a new hybrid encoder-decoder structure
(called HA02) for the first time which exploits the attention mechanism to
focus on the most important input information. In particular, we implement a
transformer encoder block as the encoder to achieve the sparsity in the input
features and a residual neural network as the decoder respectively, inspired by
the success of the attention mechanism. Using 3GPP channel models, our
simulations show superior estimation performance compared with other candidate
neural network methods for channel estimation.",None,18255
PIC4rl-gym: a ROS2 modular framework for Robots Autonomous Navigation with Deep Reinforcement Learning,0.240054,"Learning agents can optimize standard autonomous navigation improving
flexibility, efficiency, and computational cost of the system by adopting a
wide variety of approaches. This work introduces the \textit{PIC4rl-gym}, a
fundamental modular framework to enhance navigation and learning research by
mixing ROS2 and Gazebo, the standard tools of the robotics community, with Deep
Reinforcement Learning (DRL). The paper describes the whole structure of the
PIC4rl-gym, which fully integrates DRL agent's training and testing in several
indoor and outdoor navigation scenarios and tasks. A modular approach is
adopted to easily customize the simulation by selecting new platforms, sensors,
or models. We demonstrate the potential of our novel gym by benchmarking the
resulting policies, trained for different navigation tasks, with a complete set
of metrics.",https://github.com/PIC4SeR/PIC4rl_gym,3782
Semantic Enhanced Text-to-SQL Parsing via Iteratively Learning Schema Linking Graph,0.147112,"The generalizability to new databases is of vital importance to Text-to-SQL
systems which aim to parse human utterances into SQL statements. Existing works
achieve this goal by leveraging the exact matching method to identify the
lexical matching between the question words and the schema items. However,
these methods fail in other challenging scenarios, such as the synonym
substitution in which the surface form differs between the corresponding
question words and schema items. In this paper, we propose a framework named
ISESL-SQL to iteratively build a semantic enhanced schema-linking graph between
question tokens and database schemas. First, we extract a schema linking graph
from PLMs through a probing procedure in an unsupervised manner. Then the
schema linking graph is further optimized during the training process through a
deep graph learning method. Meanwhile, we also design an auxiliary task called
graph regularization to improve the schema information mentioned in the
schema-linking graph. Extensive experiments on three benchmarks demonstrate
that ISESL-SQL could consistently outperform the baselines and further
investigations show its generalizability and robustness.",https://github.com/THU-BPM/ISESL-SQL,7471
Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition,0.351796,"This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech
Recognition (AVSR) system. To this end, we propose Visual Context-driven Audio
Feature Enhancement module (V-CAFE) to enhance the input noisy audio speech
with a help of audio-visual correspondence. The proposed V-CAFE is designed to
capture the transition of lip movements, namely visual context and to generate
a noise reduction mask by considering the obtained visual context. Through
context-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be
refined for mask generation. The noisy representations are masked out with the
noise reduction mask resulting in enhanced audio features. The enhanced audio
features are fused with the visual features and taken to an encoder-decoder
model composed of Conformer and Transformer for speech recognition. We show the
proposed end-to-end AVSR with the V-CAFE can further improve the
noise-robustness of AVSR. The effectiveness of the proposed method is evaluated
in noisy speech recognition and overlapped speech recognition experiments using
the two largest audio-visual datasets, LRS2 and LRS3.",None,11033
Efficient Long Sequence Modeling via State Space Augmented Transformer,0.230885,"Transformer models have achieved superior performance in various natural
language processing tasks. However, the quadratic computational cost of the
attention mechanism limits its practicality for long sequences. There are
existing attention variants that improve the computational efficiency, but they
have limited ability to effectively compute global information. In parallel to
Transformer models, state space models (SSMs) are tailored for long sequences,
but they are not flexible enough to capture complicated local information. We
propose SPADE, short for $\underline{\textbf{S}}$tate
s$\underline{\textbf{P}}$ace
$\underline{\textbf{A}}$ugmente$\underline{\textbf{D}}$
Transform$\underline{\textbf{E}}$r. Specifically, we augment a SSM into the
bottom layer of SPADE, and we employ efficient local attention methods for the
other layers. The SSM augments global information, which complements the lack
of long-range dependency issue in local attention methods. Experimental results
on the Long Range Arena benchmark and language modeling tasks demonstrate the
effectiveness of the proposed method. To further demonstrate the scalability of
SPADE, we pre-train large encoder-decoder models and present fine-tuning
results on natural language understanding and natural language generation
tasks.",https://github.com/microsoft/EfficientLongSequenceModeling,82331
"When classifying grammatical role, BERT doesn't care about word order... except when it matters",0.0917404,"Because meaning can often be inferred from lexical semantics alone, word
order is often a redundant cue in natural language. For example, the words
chopped, chef, and onion are more likely used to convey ""The chef chopped the
onion,"" not ""The onion chopped the chef."" Recent work has shown large language
models to be surprisingly word order invariant, but crucially has largely
considered natural prototypical inputs, where compositional meaning mostly
matches lexical expectations. To overcome this confound, we probe grammatical
role representation in English BERT and GPT-2, on instances where lexical
expectations are not sufficient, and word order knowledge is necessary for
correct classification. Such non-prototypical instances are naturally occurring
English sentences with inanimate subjects or animate objects, or sentences
where we systematically swap the arguments to make sentences like ""The onion
chopped the chef"". We find that, while early layer embeddings are largely
lexical, word order is in fact crucial in defining the later-layer
representations of words in semantically non-prototypical positions. Our
experiments isolate the effect of word order on the contextualization process,
and highlight how models use context in the uncommon, but critical, instances
where it matters.",https://github.com/toizzy/except-when-it-matters,4825
Do Bayesian Neural Networks Need To Be Fully Stochastic?,0.0784904,"We investigate the benefit of treating all the parameters in a Bayesian
neural network stochastically and find compelling theoretical and empirical
evidence that this standard construction may be unnecessary. To this end, we
prove that expressive predictive distributions require only small amounts of
stochasticity. In particular, partially stochastic networks with only $n$
stochastic biases are universal probabilistic predictors for $n$-dimensional
predictive problems. In empirical investigations, we find no systematic benefit
of full stochasticity across four different inference modalities and eight
datasets; partially stochastic networks can match and sometimes even outperform
fully stochastic networks, despite their reduced memory costs.",https://github.com/IntelLabs/bayesian-torch,2733
On the Super-exponential Quantum Speedup of Equivariant Quantum Machine Learning Algorithms with SU($d$) Symmetry,0.257,"We introduce a framework of the equivariant convolutional algorithms which is
tailored for a number of machine-learning tasks on physical systems with
arbitrary SU($d$) symmetries. It allows us to enhance a natural model of
quantum computation--permutational quantum computing (PQC) [Quantum Inf.
Comput., 10, 470-497 (2010)] --and defines a more powerful model: PQC+. While
PQC was shown to be effectively classically simulatable, we exhibit a problem
which can be efficiently solved on PQC+ machine, whereas the best known
classical algorithms runs in $O(n!n^2)$ time, thus providing strong evidence
against PQC+ being classically simulatable. We further discuss practical
quantum machine learning algorithms which can be carried out in the paradigm of
PQC+.",None,13616
Data-Efficient Backdoor Attacks,0.0790679,"Recent studies have proven that deep neural networks are vulnerable to
backdoor attacks. Specifically, by mixing a small number of poisoned samples
into the training set, the behavior of the trained model can be maliciously
controlled. Existing attack methods construct such adversaries by randomly
selecting some clean data from the benign set and then embedding a trigger into
them. However, this selection strategy ignores the fact that each poisoned
sample contributes inequally to the backdoor injection, which reduces the
efficiency of poisoning. In this paper, we formulate improving the poisoned
data efficiency by the selection as an optimization problem and propose a
Filtering-and-Updating Strategy (FUS) to solve it. The experimental results on
CIFAR-10 and ImageNet-10 indicate that the proposed method is effective: the
same attack success rate can be achieved with only 47% to 75% of the poisoned
sample volume compared to the random selection strategy. More importantly, the
adversaries selected according to one setting can generalize well to other
settings, exhibiting strong transferability. The prototype code of our method
is now available at https://github.com/xpf/Data-Efficient-Backdoor-Attacks.",https://github.com/xpf/Data-Efﬁcient-Backdoor-Attacks,175520
Recognising the importance of preference change: A call for a coordinated multidisciplinary research effort in the age of AI,0.124548,"As artificial intelligence becomes more powerful and a ubiquitous presence in
daily life, it is imperative to understand and manage the impact of AI systems
on our lives and decisions. Modern ML systems often change user behavior (e.g.
personalized recommender systems learn user preferences to deliver
recommendations that change online behavior). An externality of behavior change
is preference change. This article argues for the establishment of a
multidisciplinary endeavor focused on understanding how AI systems change
preference: Preference Science. We operationalize preference to incorporate
concepts from various disciplines, outlining the importance of meta-preferences
and preference-change preferences, and proposing a preliminary framework for
how preferences change. We draw a distinction between preference change,
permissible preference change, and outright preference manipulation. A
diversity of disciplines contribute unique insights to this framework.",None,2326
"CVM-Cervix: A Hybrid Cervical Pap-Smear Image Classification Framework Using CNN, Visual Transformer and Multilayer Perceptron",0.912505,"Cervical cancer is the seventh most common cancer among all the cancers
worldwide and the fourth most common cancer among women. Cervical cytopathology
image classification is an important method to diagnose cervical cancer. Manual
screening of cytopathology images is time-consuming and error-prone. The
emergence of the automatic computer-aided diagnosis system solves this problem.
This paper proposes a framework called CVM-Cervix based on deep learning to
perform cervical cell classification tasks. It can analyze pap slides quickly
and accurately. CVM-Cervix first proposes a Convolutional Neural Network module
and a Visual Transformer module for local and global feature extraction
respectively, then a Multilayer Perceptron module is designed to fuse the local
and global features for the final classification. Experimental results show the
effectiveness and potential of the proposed CVM-Cervix in the field of cervical
Pap smear image classification. In addition, according to the practical needs
of clinical work, we perform a lightweight post-processing to compress the
model.",None,14255
Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,0.0411115,"Language models have been shown to perform better with an increase in scale
on a wide variety of tasks via the in-context learning paradigm. In this paper,
we investigate the hypothesis that the ability of a large language model to
in-context learn-perform a task is not uniformly spread across all of its
underlying components. Using a 66 billion parameter language model (OPT-66B)
across a diverse set of 14 downstream tasks, we find this is indeed the case:
$\sim$70% of attention heads and $\sim$20% of feed forward networks can be
removed with minimal decline in task performance. We find substantial overlap
in the set of attention heads (un)important for in-context learning across
tasks and number of in-context examples. We also address our hypothesis through
a task-agnostic lens, finding that a small set of attention heads in OPT-66B
score highly on their ability to perform primitive induction operations
associated with in-context learning, namely, prefix matching and copying. These
induction heads overlap with task-specific important heads, reinforcing
arguments by Olsson et al. (arXiv:2209.11895) regarding induction head
generality to more sophisticated behaviors associated with in-context learning.
Overall, our study provides several insights that indicate large language
models may be under-trained for in-context learning and opens up questions on
how to pre-train language models to more effectively perform in-context
learning.",github.com/amazon-science/llm-interpret,42346
CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization,0.230085,"The quest for seeking health information has swamped the web with consumers'
health-related questions. Generally, consumers use overly descriptive and
peripheral information to express their medical condition or other healthcare
needs, contributing to the challenges of natural language understanding. One
way to address this challenge is to summarize the questions and distill the key
information of the original question. To address this issue, we introduce a new
dataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health
questions and corresponding summaries. The dataset is derived from the
community question-answering forum and therefore provides a valuable resource
for understanding consumer health-related posts on social media. We benchmark
the dataset on multiple state-of-the-art summarization models to show the
effectiveness of the dataset.",https://github.com/shwetanlp/Yahoo-CHQ-Summ,14142
Learning to Adapt Domain Shifts of Moral Values via Instance Weighting,0.148904,"Classifying moral values in user-generated text from social media is critical
in understanding community cultures and interpreting user behaviors of social
movements. Moral values and language usage can change across the social
movements; however, text classifiers are usually trained in source domains of
existing social movements and tested in target domains of new social issues
without considering the variations. In this study, we examine domain shifts of
moral values and language usage, quantify the effects of domain shifts on the
morality classification task, and propose a neural adaptation framework via
instance weighting to improve cross-domain classification tasks. The
quantification analysis suggests a strong correlation between morality shifts,
language usage, and classification performance. We evaluate the neural
adaptation framework on a public Twitter data across 7 social movements and
gain classification improvements up to 12.1\%. Finally, we release a new data
of the COVID-19 vaccine labeled with moral values and evaluate our approach on
the new target domain. For the case study of the COVID-19 vaccine, our
adaptation framework achieves up to 5.26\% improvements over neural baselines.",None,12040
SciNLI: A Corpus for Natural Language Inference on Scientific Text,0.101996,"Existing Natural Language Inference (NLI) datasets, while being instrumental
in the advancement of Natural Language Understanding (NLU) research, are not
related to scientific text. In this paper, we introduce SciNLI, a large dataset
for NLI that captures the formality in scientific text and contains 107,412
sentence pairs extracted from scholarly papers on NLP and computational
linguistics. Given that the text used in scientific literature differs vastly
from the text used in everyday language both in terms of vocabulary and
sentence structure, our dataset is well suited to serve as a benchmark for the
evaluation of scientific NLU models. Our experiments show that SciNLI is harder
to classify than the existing NLI datasets. Our best performing model with
XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23%
showing that there is substantial room for improvement.",https://github.com/msadat3/SciNLI,6491
Efficient Visual Tracking via Hierarchical Cross-Attention Transformer,0.261071,"In recent years, target tracking has made great progress in accuracy. This
development is mainly attributed to powerful networks (such as transformers)
and additional modules (such as online update and refinement modules). However,
less attention has been paid to tracking speed. Most state-of-the-art trackers
are satisfied with the real-time speed on powerful GPUs. However, practical
applications necessitate higher requirements for tracking speed, especially
when edge platforms with limited resources are used. In this work, we present
an efficient tracking method via a hierarchical cross-attention transformer
named HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on
the edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our
HCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100,
UAV123, and VOT2020. Code and models are available at
https://github.com/chenxin-dlut/HCAT.",https://github.com/chenxin-dlut/HCAT,48744
RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs,0.762052,"Blind face restoration is to recover a high-quality face image from unknown
degradations. As face image contains abundant contextual information, we
propose a method, RestoreFormer, which explores fully-spatial attentions to
model contextual information and surpasses existing works that use local
operators. RestoreFormer has several benefits compared to prior arts. First,
unlike the conventional multi-head self-attention in previous Vision
Transformers (ViTs), RestoreFormer incorporates a multi-head cross-attention
layer to learn fully-spatial interactions between corrupted queries and
high-quality key-value pairs. Second, the key-value pairs in ResotreFormer are
sampled from a reconstruction-oriented high-quality dictionary, whose elements
are rich in high-quality facial features specifically aimed for face
reconstruction, leading to superior restoration results. Third, RestoreFormer
outperforms advanced state-of-the-art methods on one synthetic dataset and
three real-world datasets, as well as produces images with better visual
quality.",https://github.com/wzhouxiff/RestoreFormer.git,53489
TripleRE: Knowledge Graph Embeddings via Tripled Relation Vectors,0.864665,"Translation-based knowledge graph embedding has been one of the most
important branches for knowledge representation learning since TransE came out.
Although many translation-based approaches have achieved some progress in
recent years, the performance was still unsatisfactory. This paper proposes a
novel knowledge graph embedding method named TripleRE with two versions. The
first version of TripleRE creatively divide the relationship vector into three
parts. The second version takes advantage of the concept of residual and
achieves better performance. In addition, attempts on using NodePiece to encode
entities achieved promising results in reducing the parametric size, and solved
the problems of scalability. Experiments show that our approach achieved
state-of-the-art performance on the large-scale knowledge graph dataset, and
competitive performance on other datasets.",https://github.com/ZJULearning/TransAt,360
Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing,0.0281878,"The explosion of e-commerce has caused the need for processing and analysis
of product titles, like entity typing in product titles. However, the rapid
activity in e-commerce has led to the rapid emergence of new entities, which is
difficult to be solved by general entity typing. Besides, product titles in
e-commerce have very different language styles from text data in general
domain. In order to handle new entities in product titles and address the
special language styles problem of product titles in e-commerce domain, we
propose our textual entailment model with continuous prompt tuning based
hypotheses and fusion embeddings for e-commerce entity typing. First, we
reformulate the entity typing task into a textual entailment problem to handle
new entities that are not present during training. Second, we design a model to
automatically generate textual entailment hypotheses using a continuous prompt
tuning method, which can generate better textual entailment hypotheses without
manual design. Third, we utilize the fusion embeddings of BERT embedding and
CharacterBERT embedding with a two-layer MLP classifier to solve the problem
that the language styles of product titles in e-commerce are different from
that of general domain. To analyze the effect of each contribution, we compare
the performance of entity typing and textual entailment model, and conduct
ablation studies on continuous prompt tuning and fusion embeddings. We also
evaluate the impact of different prompt template initialization for the
continuous prompt tuning. We show our proposed model improves the average F1
score by around 2% compared to the baseline BERT entity typing model.",None,197861
PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch,0.864665,"Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a local region. Patch attacks can be highly effective in a
variety of tasks and physically realizable via attachment (e.g. a sticker) to
the real-world objects. Despite the diversity in attack patterns, adversarial
patches tend to be highly textured and different in appearance from natural
images. We exploit this property and present PatchZero, a general defense
pipeline against white-box adversarial patches without retraining the
downstream classifier or detector. Specifically, our defense detects
adversaries at the pixel-level and ""zeros out"" the patch region by repainting
with mean pixel values. We further design a two-stage adversarial training
scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA
defense performance on the image classification (ImageNet, RESISC45), object
detection (PASCAL VOC), and video classification (UCF101) tasks with little
degradation in benign performance. In addition, PatchZero transfers to
different patch shapes and attack types.",https://github.com/Trusted-AI/adversarial-robustness-toolbox,36875
Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions,0.0505034,"Role-oriented dialogue summarization is to generate summaries for different
roles in the dialogue, e.g., merchants and consumers. Existing methods handle
this task by summarizing each role's content separately and thus are prone to
ignore the information from other roles. However, we believe that other roles'
content could benefit the quality of summaries, such as the omitted information
mentioned by other roles. Therefore, we propose a novel role interaction
enhanced method for role-oriented dialogue summarization. It adopts cross
attention and decoder self-attention interactions to interactively acquire
other roles' critical information. The cross attention interaction aims to
select other roles' critical dialogue utterances, while the decoder
self-attention interaction aims to obtain key information from other roles'
summaries. Experimental results have shown that our proposed method
significantly outperforms strong baselines on two public role-oriented dialogue
summarization datasets. Extensive analyses have demonstrated that other roles'
content could help generate summaries with more complete semantics and correct
topic structures.",https://github.com/xiaolinAndy/RODS,15943
LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs,0.268584,"Recent advance in 2D CNNs has revealed that large kernels are important.
However, when directly applying large convolutional kernels in 3D CNNs, severe
difficulties are met, where those successful module designs in 2D become
surprisingly ineffective on 3D networks, including the popular depth-wise
convolution. To address this vital challenge, we instead propose the
spatial-wise partition convolution and its large-kernel module. As a result, it
avoids the optimization and efficiency issues of naive 3D large kernels. Our
large-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D
tasks of semantic segmentation and object detection. It achieves 73.9% mIoU on
the ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection
benchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance
further boosts to 74.2% NDS with a simple multi-modal fusion. In addition,
LargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object
detection. For the first time, we show that large kernels are feasible and
essential for 3D visual tasks.",https://github.com/dvlab-research/LargeKernel3D,306651
Smoothing Entailment Graphs with Language Models,0.20607,"The diversity and Zipfian frequency distribution of natural language
predicates in corpora leads to sparsity in Entailment Graphs (EGs) built by
Open Relation Extraction (ORE). EGs are computationally efficient and
explainable models of natural language inference, but as symbolic models, they
fail if a novel premise or hypothesis vertex is missing at test-time. We
present theory and methodology for overcoming such sparsity in symbolic models.
First, we introduce a theory of optimal smoothing of EGs by constructing
transitive chains. We then demonstrate an efficient, open-domain, and
unsupervised smoothing method using an off-the-shelf Language Model to find
approximations of missing premise predicates. This improves recall by 25.1 and
16.3 percentage points on two difficult directional entailment datasets, while
raising average precision and maintaining model explainability. Further, in a
QA task we show that EG smoothing is most useful for answering questions with
lesser supporting text, where missing premise predicates are more costly.
Finally, controlled experiments with WordNet confirm our theory and show that
hypothesis smoothing is difficult, but possible in principle.",https://github.com/nighttime/EntGraph,26355
An attention mechanism based convolutional network for satellite precipitation downscaling over China,0.267941,"Precipitation is a key part of hydrological circulation and is a sensitive
indicator of climate change. The Integrated Multi-satellitE Retrievals for the
Global Precipitation Measurement (GPM) mission (IMERG) datasets are widely used
for global and regional precipitation investigations. However, their local
application is limited by the relatively coarse spatial resolution. Therefore,
in this paper, an attention mechanism based convolutional network (AMCN) is
proposed to downscale GPM IMERG monthly precipitation data. The proposed method
is an end-to-end network, which consists of a global cross-attention module, a
multi-factor cross-attention module, and a residual convolutional module,
comprehensively considering the potential relationships between precipitation
and complicated surface characteristics. In addition, a degradation loss
function based on low-resolution precipitation is designed to physically
constrain the network training, to improve the robustness of the proposed
network under different time and scale variations. The experiments demonstrate
that the proposed network significantly outperforms three baseline methods.
Finally, a geographic difference analysis method is introduced to further
improve the downscaled results by incorporating in-situ measurements for
high-quality and fine-scale precipitation estimation.",None,-1
Computational historical linguistics and language diversity in South Asia,0.0912411,"South Asia is home to a plethora of languages, many of which severely lack
access to new language technologies. This linguistic diversity also results in
a research environment conducive to the study of comparative, contact, and
historical linguistics -- fields which necessitate the gathering of extensive
data from many languages. We claim that data scatteredness (rather than
scarcity) is the primary obstacle in the development of South Asian language
technology, and suggest that the study of language history is uniquely aligned
with surmounting this obstacle. We review recent developments in and at the
intersection of South Asian NLP and historical-comparative linguistics,
describing our and others' current efforts in this area. We also offer new
strategies towards breaking the data barrier.",None,-1
Semantic Similarity Computing Model Based on Multi Model Fine-Grained Nonlinear Fusion,0.147768,"Natural language processing (NLP) task has achieved excellent performance in
many fields, including semantic understanding, automatic summarization, image
recognition and so on. However, most of the neural network models for NLP
extract the text in a fine-grained way, which is not conducive to grasp the
meaning of the text from a global perspective. To alleviate the problem, the
combination of the traditional statistical method and deep learning model as
well as a novel model based on multi model nonlinear fusion are proposed in
this paper. The model uses the Jaccard coefficient based on part of speech,
Term Frequency-Inverse Document Frequency (TF-IDF) and word2vec-CNN algorithm
to measure the similarity of sentences respectively. According to the
calculation accuracy of each model, the normalized weight coefficient is
obtained and the calculation results are compared. The weighted vector is input
into the fully connected neural network to give the final classification
results. As a result, the statistical sentence similarity evaluation algorithm
reduces the granularity of feature extraction, so it can grasp the sentence
features globally. Experimental results show that the matching of sentence
similarity calculation method based on multi model nonlinear fusion is 84%, and
the F1 value of the model is 75%.",None,18646
Region-Aware Face Swapping,0.395715,"This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to
achieve identity-consistent harmonious high-resolution face generation in a
local-global manner: \textbf{1)} Local Facial Region-Aware (FRA) branch
augments local identity-relevant features by introducing the Transformer to
effectively model misaligned cross-scale semantic interaction. \textbf{2)}
Global Source Feature-Adaptive (SFA) branch further complements global
identity-relevant cues for generating identity-consistent swapped faces.
Besides, we propose a \textit{Face Mask Predictor} (FMP) module incorporated
with StyleGAN2 to predict identity-relevant soft facial masks in an
unsupervised manner that is more practical for generating harmonious
high-resolution faces. Abundant experiments qualitatively and quantitatively
demonstrate the superiority of our method for generating more
identity-consistent high-resolution swapped faces over SOTA methods, \eg,
obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\uparrow$.",None,6531
A new Hyper-heuristic based on Adaptive Simulated Annealing and Reinforcement Learning for the Capacitated Electric Vehicle Routing Problem,0.213073,"Electric vehicles (EVs) have been adopted in urban areas to reduce
environmental pollution and global warming as a result of the increasing number
of freight vehicles. However, there are still deficiencies in routing the
trajectories of last-mile logistics that continue to impact social and economic
sustainability. For that reason, in this paper, a hyper-heuristic (HH) approach
called Hyper-heuristic Adaptive Simulated Annealing with Reinforcement Learning
(HHASA$_{RL}$) is proposed. It is composed of a multi-armed bandit method and
the self-adaptive Simulated Annealing (SA) metaheuristic algorithm for solving
the problem called Capacitated Electric Vehicle Routing Problem (CEVRP). Due to
the limited number of charging stations and the travel range of EVs, the EVs
must require battery recharging moments in advance and reduce travel times and
costs. The HH implemented improves multiple minimum best-known solutions and
obtains the best mean values for some high-dimensional instances for the
proposed benchmark for the IEEE WCCI2020 competition.",None,4806
Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems,0.0631089,"Do all instances need inference through the big models for a correct
prediction? Perhaps not; some instances are easy and can be answered correctly
by even small capacity models. This provides opportunities for improving the
computational efficiency of systems. In this work, we present an explorative
study on 'model cascading', a simple technique that utilizes a collection of
models of varying capacities to accurately yet efficiently output predictions.
Through comprehensive experiments in multiple task settings that differ in the
number of models available for cascading (K value), we show that cascading
improves both the computational efficiency and the prediction accuracy. For
instance, in K=3 setting, cascading saves up to 88.93% computation cost and
consistently achieves superior prediction accuracy with an improvement of up to
2.18%. We also study the impact of introducing additional models in the cascade
and show that it further increases the efficiency improvements. Finally, we
hope that our work will facilitate development of efficient NLP systems making
their widespread adoption in real-world applications possible.",None,-1
Whodunit? Learning to Contrast for Authorship Attribution,0.233312,"Authorship attribution is the task of identifying the author of a given text.
The key is finding representations that can differentiate between authors.
Existing approaches typically use manually designed features that capture a
dataset's content and style, but these approaches are dataset-dependent and
yield inconsistent performance across corpora. In this work, we propose
\textit{learning} author-specific representations by fine-tuning pre-trained
generic language representations with a contrastive objective (Contra-X). We
show that Contra-X learns representations that form highly separable clusters
for different authors. It advances the state-of-the-art on multiple human and
machine authorship attribution benchmarks, enabling improvements of up to 6.8%
over cross-entropy fine-tuning. However, we find that Contra-X improves overall
accuracy at the cost of sacrificing performance for some authors. Resolving
this tension will be an important direction for future work. To the best of our
knowledge, we are the first to integrate contrastive learning with pre-trained
language model fine-tuning for authorship attribution.",https://github.com/BoAi01/Contra-X.git,-1
Motion Policy Networks,0.278777,"Collision-free motion generation in unknown environments is a core building
block for robot manipulation. Generating such motions is challenging due to
multiple objectives; not only should the solutions be optimal, the motion
generator itself must be fast enough for real-time performance and reliable
enough for practical deployment. A wide variety of methods have been proposed
ranging from local controllers to global planners, often being combined to
offset their shortcomings. We present an end-to-end neural model called Motion
Policy Networks (M$\pi$Nets) to generate collision-free, smooth motion from
just a single depth camera observation. M$\pi$Nets are trained on over 3
million motion planning problems in over 500,000 environments. Our experiments
show that M$\pi$Nets are significantly faster than global planners while
exhibiting the reactivity needed to deal with dynamic scenes. They are 46%
better than prior neural planners and more robust than local control policies.
Despite being only trained in simulation, M$\pi$Nets transfer well to the real
robot with noisy partial point clouds. Code and data are publicly available at
https://mpinets.github.io.",https://mpinets.github.io,-1
Improving Chinese Story Generation via Awareness of Syntactic Dependencies and Semantics,0.153284,"Story generation aims to generate a long narrative conditioned on a given
input. In spite of the success of prior works with the application of
pre-trained models, current neural models for Chinese stories still struggle to
generate high-quality long text narratives. We hypothesise that this stems from
ambiguity in syntactically parsing the Chinese language, which does not have
explicit delimiters for word segmentation. Consequently, neural models suffer
from the inefficient capturing of features in Chinese narratives. In this
paper, we present a new generation framework that enhances the feature
capturing mechanism by informing the generation model of dependencies between
words and additionally augmenting the semantic representation learning through
synonym denoising training. We conduct a range of experiments, and the results
demonstrate that our framework outperforms the state-of-the-art Chinese
generation models on all evaluation metrics, demonstrating the benefits of
enhanced dependency and semantic representation learning.",https://github.com/hehedaozuiteng/Chinese-Story-Generation,-1
When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture,0.0524453,"Vision Transformers (ViTs) have recently achieved competitive performance in
broad vision tasks. Unfortunately, on popular threat models, naturally trained
ViTs are shown to provide no more adversarial robustness than convolutional
neural networks (CNNs). Adversarial training is still required for ViTs to
defend against such adversarial attacks. In this paper, we provide the first
and comprehensive study on the adversarial training recipe of ViTs via
extensive evaluation of various training techniques across benchmark datasets.
We find that pre-training and SGD optimizer are necessary for ViTs' adversarial
training. Further considering ViT as a new type of model architecture, we
investigate its adversarial robustness from the perspective of its unique
architectural components. We find, when randomly masking gradients from some
attention blocks or masking perturbations on some patches during adversarial
training, the adversarial robustness of ViTs can be remarkably improved, which
may potentially open up a line of work to explore the architectural information
inside the newly designed models like ViTs. Our code is available at
https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers.",https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers,-1
Unifying and Boosting Gradient-Based Training-Free Neural Architecture Search,0.0880127,"Neural architecture search (NAS) has gained immense popularity owing to its
ability to automate neural architecture design. A number of training-free
metrics are recently proposed to realize NAS without training, hence making NAS
more scalable. Despite their competitive empirical performances, a unified
theoretical understanding of these training-free metrics is lacking. As a
consequence, (a) the relationships among these metrics are unclear, (b) there
is no theoretical interpretation for their empirical performances, and (c)
there may exist untapped potential in existing training-free NAS, which
probably can be unveiled through a unified theoretical understanding. To this
end, this paper presents a unified theoretical analysis of gradient-based
training-free NAS, which allows us to (a) theoretically study their
relationships, (b) theoretically guarantee their generalization performances,
and (c) exploit our unified theoretical understanding to develop a novel
framework named hybrid NAS (HNAS) which consistently boosts training-free NAS
in a principled way. Remarkably, HNAS can enjoy the advantages of both
training-free (i.e., the superior search efficiency) and training-based (i.e.,
the remarkable search effectiveness) NAS, which we have demonstrated through
extensive experiments.",https://github.com/fmfn/BayesianOptimization,-1
Procedural Image Programs for Representation Learning,0.22317,"Learning image representations using synthetic data allows training neural
networks without some of the concerns associated with real images, such as
privacy and bias. Existing work focuses on a handful of curated generative
processes which require expert knowledge to design, making it hard to scale up.
To overcome this, we propose training with a large dataset of twenty-one
thousand programs, each one generating a diverse set of synthetic images. These
programs are short code snippets, which are easy to modify and fast to execute
using OpenGL. The proposed dataset can be used for both supervised and
unsupervised representation learning, and reduces the gap between pre-training
with real and procedurally generated images by 38%.",https://github.com/mbaradad/shaders21k,-1
DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation,0.0886643,"This paper proposes a simple baseline framework for video-based 2D/3D human
pose estimation that can achieve 10 times efficiency improvement over existing
works without any performance degradation, named DeciWatch. Unlike current
solutions that estimate each frame in a video, DeciWatch introduces a simple
yet effective sample-denoise-recover framework that only watches sparsely
sampled frames, taking advantage of the continuity of human motions and the
lightweight pose representation. Specifically, DeciWatch uniformly samples less
than 10% video frames for detailed estimation, denoises the estimated 2D/3D
poses with an efficient Transformer architecture, and then accurately recovers
the rest of the frames using another Transformer-based network. Comprehensive
experimental results on three video-based human pose estimation and body mesh
recovery tasks with four datasets validate the efficiency and effectiveness of
DeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.",https://github.com/cure-lab/DeciWatch,-1
BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets,0.0525518,"The free flow of information has been accelerated by the rapid development of
social media technology. There has been a significant social and psychological
impact on the population due to the outbreak of Coronavirus disease (COVID-19).
The COVID-19 pandemic is one of the current events being discussed on social
media platforms. In order to safeguard societies from this pandemic, studying
people's emotions on social media is crucial. As a result of their particular
characteristics, sentiment analysis of texts like tweets remains challenging.
Sentiment analysis is a powerful text analysis tool. It automatically detects
and analyzes opinions and emotions from unstructured data. Texts from a wide
range of sources are examined by a sentiment analysis tool, which extracts
meaning from them, including emails, surveys, reviews, social media posts, and
web articles. To evaluate sentiments, natural language processing (NLP) and
machine learning techniques are used, which assign weights to entities, topics,
themes, and categories in sentences or phrases. Machine learning tools learn
how to detect sentiment without human intervention by examining examples of
emotions in text. In a pandemic situation, analyzing social media texts to
uncover sentimental trends can be very helpful in gaining a better
understanding of society's needs and predicting future trends. We intend to
study society's perception of the COVID-19 pandemic through social media using
state-of-the-art BERT and Deep CNN models. The superiority of BERT models over
other deep models in sentiment analysis is evident and can be concluded from
the comparison of the various research studies mentioned in this article.",None,-1
"Perceive, Interact, Predict: Learning Dynamic and Static Clues for End-to-End Motion Prediction",0.418725,"Motion prediction is highly relevant to the perception of dynamic objects and
static map elements in the scenarios of autonomous driving. In this work, we
propose PIP, the first end-to-end Transformer-based framework which jointly and
interactively performs online mapping, object detection and motion prediction.
PIP leverages map queries, agent queries and mode queries to encode the
instance-wise information of map elements, agents and motion intentions,
respectively. Based on the unified query representation, a differentiable
multi-task interaction scheme is proposed to exploit the correlation between
perception and prediction. Even without human-annotated HD map or agent's
historical tracking trajectory as guidance information, PIP realizes end-to-end
multi-agent motion prediction and achieves better performance than
tracking-based and HD-map-based methods. PIP provides comprehensive high-level
information of the driving scene (vectorized static map and dynamic objects
with motion information), and contributes to the downstream planning and
control. Code and models will be released for facilitating further research.",None,-1
Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning,0.361327,"Recent research shows synthetic data as a source of supervision helps
pretrained language models (PLM) transfer learning to new target tasks/domains.
However, this idea is less explored for spatial language. We provide two new
data resources on multiple spatial language processing tasks. The first dataset
is synthesized for transfer learning on spatial question answering (SQA) and
spatial role labeling (SpRL). Compared to previous SQA datasets, we include a
larger variety of spatial relation types and spatial expressions. Our data
generation process is easily extendable with new spatial expression lexicons.
The second one is a real-world SQA dataset with human-generated questions built
on an existing corpus with SPRL annotations. This dataset can be used to
evaluate spatial language processing models in realistic situations. We show
pretraining with automatically generated data significantly improves the SOTA
results on several SQA and SPRL benchmarks, particularly when the training data
in the target domain is small.",https://github.com/HLR/SpaRTUN,-1
Not always about you: Prioritizing community needs when developing endangered language technology,0.22662,"Languages are classified as low-resource when they lack the quantity of data
necessary for training statistical and machine learning tools and models.
Causes of resource scarcity vary but can include poor access to technology for
developing these resources, a relatively small population of speakers, or a
lack of urgency for collecting such resources in bilingual populations where
the second language is high-resource. As a result, the languages described as
low-resource in the literature are as different as Finnish on the one hand,
with millions of speakers using it in every imaginable domain, and Seneca, with
only a small-handful of fluent speakers using the language primarily in a
restricted domain. While issues stemming from the lack of resources necessary
to train models unite this disparate group of languages, many other issues cut
across the divide between widely-spoken low resource languages and endangered
languages. In this position paper, we discuss the unique technological,
cultural, practical, and ethical challenges that researchers and indigenous
speech community members face when working together to develop language
technology to support endangered language documentation and revitalization. We
report the perspectives of language teachers, Master Speakers and elders from
indigenous communities, as well as the point of view of academics. We describe
an ongoing fruitful collaboration and make recommendations for future
partnerships between academic researchers and language community stakeholders.",None,-1
What do we Really Know about State of the Art NER?,0.16738,"Named Entity Recognition (NER) is a well researched NLP task and is widely
used in real world NLP scenarios. NER research typically focuses on the
creation of new ways of training NER, with relatively less emphasis on
resources and evaluation. Further, state of the art (SOTA) NER models, trained
on standard datasets, typically report only a single performance measure
(F-score) and we don't really know how well they do for different entity types
and genres of text, or how robust are they to new, unseen entities. In this
paper, we perform a broad evaluation of NER using a popular dataset, that takes
into consideration various text genres and sources constituting the dataset at
hand. Additionally, we generate six new adversarial test sets through small
perturbations in the original test set, replacing select entities while
retaining the context. We also train and test our models on randomly generated
train/dev/test splits followed by an experiment where the models are trained on
a select set of genres but tested genres not seen in training. These
comprehensive evaluation strategies were performed using three SOTA NER models.
Based on our results, we recommend some useful reporting practices for NER
researchers, that could help in providing a better understanding of a SOTA
model's performance in future.",https://github.com/nishkalavallabhi/SOTANER/,-1
Comparative layer-wise analysis of self-supervised speech models,0.678831,"Many self-supervised speech models, varying in their pre-training objective,
input modality, and pre-training data, have been proposed in the last few
years. Despite impressive successes on downstream tasks, we still have a
limited understanding of the properties encoded by the models and the
differences across models. In this work, we examine the intermediate
representations for a variety of recent models. Specifically, we measure
acoustic, phonetic, and word-level properties encoded in individual layers,
using a lightweight analysis tool based on canonical correlation analysis
(CCA). We find that these properties evolve across layers differently depending
on the model, and the variations relate to the choice of pre-training
objective. We further investigate the utility of our analyses for downstream
tasks by comparing the property trends with performance on speech recognition
and spoken language understanding tasks. We discover that CCA trends provide
reliable guidance to choose layers of interest for downstream tasks and that
single-layer performance often matches or improves upon using all layers,
suggesting implications for more efficient use of pre-trained models.",https://github.com/ankitapasad/layerwise-analysis/,-1
PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects,0.440829,"Object pose estimation is crucial for robotic applications and augmented
reality. Beyond instance level 6D object pose estimation methods, estimating
category-level pose and shape has become a promising trend. As such, a new
research field needs to be supported by well-designed datasets. To provide a
benchmark with high-quality ground truth annotations to the community, we
introduce a multimodal dataset for category-level object pose estimation with
photometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high
quality 3D models of household objects over 8 categories including highly
reflective, transparent and symmetric objects. We developed a novel
robot-supported multi-modal (RGB, depth, polarisation) data acquisition and
annotation process. It ensures sub-millimeter accuracy of the pose for opaque
textured, shiny and transparent objects, no motion blur and perfect camera
synchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and
monocular RGB methods are evaluated on the challenging scenes of PhoCaL.",None,-1
Generating natural images with direct Patch Distributions Matching,0.180484,"Many traditional computer vision algorithms generate realistic images by
requiring that each patch in the generated image be similar to a patch in a
training image and vice versa. Recently, this classical approach has been
replaced by adversarial training with a patch discriminator. The adversarial
approach avoids the computational burden of finding nearest neighbors of
patches but often requires very long training times and may fail to match the
distribution of patches. In this paper we leverage the recently developed
Sliced Wasserstein Distance and develop an algorithm that explicitly and
efficiently minimizes the distance between patch distributions in two images.
Our method is conceptually simple, requires no training and can be implemented
in a few lines of codes. On a number of image generation tasks we show that our
results are often superior to single-image-GANs, require no training, and can
generate high quality images in a few seconds. Our implementation is available
at https://github.com/ariel415el/GPDM",https://github.com/ariel415el/GPDM,-1
A Systematic Evaluation of Response Selection for Open Domain Dialogue,0.025251,"Recent progress on neural approaches for language processing has triggered a
resurgence of interest on building intelligent open-domain chatbots. However,
even the state-of-the-art neural chatbots cannot produce satisfying responses
for every turn in a dialog. A practical solution is to generate multiple
response candidates for the same context, and then perform response
ranking/selection to determine which candidate is the best. Previous work in
response selection typically trains response rankers using synthetic data that
is formed from existing dialogs by using a ground truth response as the single
appropriate response and constructing inappropriate responses via random
selection or using adversarial methods. In this work, we curated a dataset
where responses from multiple response generators produced for the same dialog
context are manually annotated as appropriate (positive) and inappropriate
(negative). We argue that such training data better matches the actual use case
examples, enabling the models to learn to rank responses effectively. With this
new dataset, we conduct a systematic evaluation of state-of-the-art methods for
response selection, and demonstrate that both strategies of using multiple
positive candidates and using manually verified hard negative candidates can
bring in significant performance improvement in comparison to using the
adversarial training data, e.g., increase of 3% and 13% in Recall@1 score,
respectively.",https://github.com/golsun/DialogRPT/,-1
DARER: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition,0.069665,"The task of joint dialog sentiment classification (DSC) and act recognition
(DAR) aims to simultaneously predict the sentiment label and act label for each
utterance in a dialog. In this paper, we put forward a new framework which
models the explicit dependencies via integrating \textit{prediction-level
interactions} other than semantics-level interactions, more consistent with
human intuition. Besides, we propose a speaker-aware temporal graph (SATG) and
a dual-task relational temporal graph (DRTG) to introduce \textit{temporal
relations} into dialog understanding and dual-task reasoning. To implement our
framework, we propose a novel model dubbed DARER, which first generates the
context-, speaker- and temporal-sensitive utterance representations via
modeling SATG, then conducts recurrent dual-task relational reasoning on DRTG,
in which process the estimated label distributions act as key clues in
prediction-level interactions. Experiment results show that DARER outperforms
existing models by large margins while requiring much less computation resource
and costing less training time. Remarkably, on DSC task in Mastodon, DARER
gains a relative improvement of about 25% over previous best model in terms of
F1, with less than 50% parameters and about only 60% required GPU memory.",https://github.com/XingBowen714/DARER,-1
ALTO: A Large-Scale Dataset for UAV Visual Place Recognition and Localization,0.0296726,"We present the ALTO dataset, a vision-focused dataset for the development and
benchmarking of Visual Place Recognition and Localization methods for Unmanned
Aerial Vehicles. The dataset is composed of two long (approximately 150km and
260km) trajectories flown by a helicopter over Ohio and Pennsylvania, and it
includes high precision GPS-INS ground truth location data, high precision
accelerometer readings, laser altimeter readings, and RGB downward facing
camera imagery. In addition, we provide reference imagery over the flight
paths, which makes this dataset suitable for VPR benchmarking and other tasks
common in Localization, such as image registration and visual odometry. To the
author's knowledge, this is the largest real-world aerial-vehicle dataset of
this kind. Our dataset is available at https://github.com/MetaSLAM/ALTO.",https://github.com/MetaSLAM/ALTO,-1
rPPG-Toolbox: Deep Remote PPG Toolbox,0.0509952,"Camera-based physiological measurement is a fast growing field of computer
vision. Remote photoplethysmography (rPPG) utilizes imaging devices (e.g.,
cameras) to measure the peripheral blood volume pulse (BVP) via
photoplethysmography, and enables cardiac measurement via webcams and
smartphones. However, the task is non-trivial with important pre-processing,
modeling, and post-processing steps required to obtain state-of-the-art
results. Replication of results and benchmarking of new models is critical for
scientific progress; however, as with many other applications of deep learning,
reliable codebases are not easy to find or use. We present a comprehensive
toolbox, rPPG-Toolbox, that contains unsupervised and supervised rPPG models
with support for public benchmark datasets, data augmentation, and systematic
evaluation: \url{https://github.com/ubicomplab/rPPG-Toolbox}",https://github.com/ubicomplab/rPPG-Toolbox,-1
Sequential Manipulation Planning on Scene Graph,0.124577,"We devise a 3D scene graph representation, contact graph+ (cg+), for
efficient sequential task planning. Augmented with predicate-like attributes,
this contact graph-based representation abstracts scene layouts with succinct
geometric information and valid robot-scene interactions. Goal configurations,
naturally specified on contact graphs, can be produced by a genetic algorithm
with a stochastic optimization method. A task plan is then initialized by
computing the Graph Editing Distance (GED) between the initial contact graphs
and the goal configurations, which generates graph edit operations
corresponding to possible robot actions. We finalize the task plan by imposing
constraints to regulate the temporal feasibility of graph edit operations,
ensuring valid task and motion correspondences. In a series of simulations and
experiments, robots successfully complete complex sequential object
rearrangement tasks that are difficult to specify using conventional planning
language like Planning Domain Definition Language (PDDL), demonstrating the
high feasibility and potential of robot sequential task planning on contact
graph.",https://sites.google.com/view/planning-on-graph,-1
Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning,0.480341,"Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions
formed from seen state and object during training. Since the same state may be
various in the visual appearance while entangled with different objects, CZSL
is still a challenging task. Some methods recognize state and object with two
trained classifiers, ignoring the impact of the interaction between object and
state; the other methods try to learn the joint representation of the
state-object compositions, leading to the domain gap between seen and unseen
composition sets. In this paper, we propose a novel Siamese Contrastive
Embedding Network (SCEN) (Code: https://github.com/XDUxyLi/SCEN-master) for
unseen composition recognition. Considering the entanglement between state and
object, we embed the visual feature into a Siamese Contrastive Space to capture
prototypes of them separately, alleviating the interaction between state and
object. In addition, we design a State Transition Module (STM) to increase the
diversity of training compositions, improving the robustness of the recognition
model. Extensive experiments indicate that our method significantly outperforms
the state-of-the-art approaches on three challenging benchmark datasets,
including the recent proposed C-QGA dataset.",https://github.com/XDUxyLi/SCEN-master,-1
Probing Speech Emotion Recognition Transformers for Linguistic Knowledge,0.0725412,"Large, pre-trained neural networks consisting of self-attention layers
(transformers) have recently achieved state-of-the-art results on several
speech emotion recognition (SER) datasets. These models are typically
pre-trained in self-supervised manner with the goal to improve automatic speech
recognition performance -- and thus, to understand linguistic information. In
this work, we investigate the extent in which this information is exploited
during SER fine-tuning. Using a reproducible methodology based on open-source
tools, we synthesise prosodically neutral speech utterances while varying the
sentiment of the text. Valence predictions of the transformer model are very
reactive to positive and negative sentiment content, as well as negations, but
not to intensifiers or reducers, while none of those linguistic features impact
arousal or dominance. These findings show that transformers can successfully
leverage linguistic information to improve their valence predictions, and that
linguistic analysis should be included in their testing.",https://github.com/espnet/espnet,-1
Sentiment-Aware Automatic Speech Recognition pre-training for enhanced Speech Emotion Recognition,0.182329,"We propose a novel multi-task pre-training method for Speech Emotion
Recognition (SER). We pre-train SER model simultaneously on Automatic Speech
Recognition (ASR) and sentiment classification tasks to make the acoustic ASR
model more ``emotion aware''. We generate targets for the sentiment
classification using text-to-sentiment model trained on publicly available
data. Finally, we fine-tune the acoustic ASR on emotion annotated speech data.
We evaluated the proposed approach on the MSP-Podcast dataset, where we
achieved the best reported concordance correlation coefficient (CCC) of 0.41
for valence prediction.",None,-1
Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes,0.0908746,"We present a novel approach to efficiently compute tight non-convex
enclosures of the image through neural networks with ReLU, sigmoid, or
hyperbolic tangent activation functions. In particular, we abstract the
input-output relation of each neuron by a polynomial approximation, which is
evaluated in a set-based manner using polynomial zonotopes. While our approach
can also can be beneficial for open-loop neural network verification, our main
application is reachability analysis of neural network controlled systems,
where polynomial zonotopes are able to capture the non-convexity caused by the
neural network as well as the system dynamics. This results in a superior
performance compared to other methods, as we demonstrate on various benchmarks.",None,-1
Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models,0.187229,"Self-supervised learning (SSL) is seen as a very promising approach with high
performance for several speech downstream tasks. Since the parameters of SSL
models are generally so large that training and inference require a lot of
memory and computational cost, it is desirable to produce compact SSL models
without a significant performance degradation by applying compression methods
such as knowledge distillation (KD). Although the KD approach is able to shrink
the depth and/or width of SSL model structures, there has been little research
on how varying the depth and width impacts the internal representation of the
small-footprint model. This paper provides an empirical study that addresses
the question. We investigate the performance on SUPERB while varying the
structure and KD methods so as to keep the number of parameters constant; this
allows us to analyze the contribution of the representation introduced by
varying the model architecture. Experiments demonstrate that a certain depth is
essential for solving content-oriented tasks (e.g. automatic speech
recognition) accurately, whereas a certain width is necessary for achieving
high performance on several speaker-oriented tasks (e.g. speaker
identification). Based on these observations, we identify, for SUPERB, a more
compressed model with better performance than previous studies.",https://github.com/s3prl/s3prl,-1
Detector-Free Weakly Supervised Group Activity Recognition,0.345982,"Group activity recognition is the task of understanding the activity
conducted by a group of people as a whole in a multi-person video. Existing
models for this task are often impractical in that they demand ground-truth
bounding box labels of actors even in testing or rely on off-the-shelf object
detectors. Motivated by this, we propose a novel model for group activity
recognition that depends neither on bounding box labels nor on object detector.
Our model based on Transformer localizes and encodes partial contexts of a
group activity by leveraging the attention mechanism, and represents a video
clip as a set of partial context embeddings. The embedding vectors are then
aggregated to form a single group representation that reflects the entire
context of an activity while capturing temporal evolution of each partial
context. Our method achieves outstanding performance on two benchmarks,
Volleyball and NBA datasets, surpassing not only the state of the art trained
with the same level of supervision, but also some of existing models relying on
stronger supervision.",https://github.com/JacobYuan7/DIN_GAR,-1
Towards Involving End-users in Interactive Human-in-the-loop AI Fairness,0.202484,"Ensuring fairness in artificial intelligence (AI) is important to counteract
bias and discrimination in far-reaching applications. Recent work has started
to investigate how humans judge fairness and how to support machine learning
(ML) experts in making their AI models fairer. Drawing inspiration from an
Explainable AI (XAI) approach called \emph{explanatory debugging} used in
interactive machine learning, our work explores designing interpretable and
interactive human-in-the-loop interfaces that allow ordinary end-users without
any technical or domain background to identify potential fairness issues and
possibly fix them in the context of loan decisions. Through workshops with
end-users, we co-designed and implemented a prototype system that allowed
end-users to see why predictions were made, and then to change weights on
features to ""debug"" fairness issues. We evaluated the use of this prototype
system through an online study. To investigate the implications of diverse
human values about fairness around the globe, we also explored how cultural
dimensions might play a role in using this prototype. Our results contribute to
the design of interfaces to allow end-users to be involved in judging and
addressing AI fairness through a human-in-the-loop approach.",None,-1
Towards Summary Candidates Fusion,0.230277,"Sequence-to-sequence deep neural models fine-tuned for abstractive
summarization can achieve great performance on datasets with enough human
annotations. Yet, it has been shown that they have not reached their full
potential, with a wide gap between the top beam search output and the oracle
beam. Recently, re-ranking methods have been proposed, to learn to select a
better summary candidate. However, such methods are limited by the summary
quality aspects captured by the first-stage candidates. To bypass this
limitation, we propose a new paradigm in second-stage abstractive summarization
called SummaFusion that fuses several summary candidates to produce a novel
abstractive second-stage summary. Our method works well on several
summarization datasets, improving both the ROUGE scores and qualitative
properties of fused summaries. It is especially good when the candidates to
fuse are worse, such as in the few-shot setup where we set a new
state-of-the-art. We will make our code and checkpoints available at
https://github.com/ntunlp/SummaFusion/.",https://github.com/ntunlp/SummaFusion/,-1
Knowledge Is Flat: A Seq2Seq Generative Framework for Various Knowledge Graph Completion,0.347329,"Knowledge Graph Completion (KGC) has been recently extended to multiple
knowledge graph (KG) structures, initiating new research directions, e.g.
static KGC, temporal KGC and few-shot KGC. Previous works often design KGC
models closely coupled with specific graph structures, which inevitably results
in two drawbacks: 1) structure-specific KGC models are mutually incompatible;
2) existing KGC methods are not adaptable to emerging KGs. In this paper, we
propose KG-S2S, a Seq2Seq generative framework that could tackle different
verbalizable graph structures by unifying the representation of KG facts into
""flat"" text, regardless of their original form. To remedy the KG structure
information loss from the ""flat"" text, we further improve the input
representations of entities and relations, and the inference algorithm in
KG-S2S. Experiments on five benchmarks show that KG-S2S outperforms many
competitive baselines, setting new state-of-the-art performance. Finally, we
analyze KG-S2S's ability on the different relations and the Non-entity
Generations.",https://github.com/chenchens190009/KG-S2S,-1
JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus,0.222885,"Most current machine translation models are mainly trained with parallel
corpora, and their translation accuracy largely depends on the quality and
quantity of the corpora. Although there are billions of parallel sentences for
a few language pairs, effectively dealing with most language pairs is difficult
due to a lack of publicly available parallel corpora. This paper creates a
large parallel corpus for English-Japanese, a language pair for which only
limited resources are available, compared to such resource-rich languages as
English-German. It introduces a new web-based English-Japanese parallel corpus
named JParaCrawl v3.0. Our new corpus contains more than 21 million unique
parallel sentence pairs, which is more than twice as many as the previous
JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new
corpus boosts the accuracy of machine translation models on various domains.
The JParaCrawl v3.0 corpus will eventually be publicly available online for
research purposes.",https://github.com/paracrawl/,-1
Long-tailed Instance Segmentation using Gumbel Optimized Loss,0.429659,"Major advancements have been made in the field of object detection and
segmentation recently. However, when it comes to rare categories, the
state-of-the-art methods fail to detect them, resulting in a significant
performance gap between rare and frequent categories. In this paper, we
identify that Sigmoid or Softmax functions used in deep detectors are a major
reason for low performance and are sub-optimal for long-tailed detection and
segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for
long-tailed detection and segmentation. It aligns with the Gumbel distribution
of rare classes in imbalanced datasets, considering the fact that most classes
in long-tailed detection have low expected probability. The proposed GOL
significantly outperforms the best state-of-the-art method by 1.1% on AP , and
boosts the overall segmentation by 9.0% and detection by 8.0%, particularly
improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS
dataset. Code available at: https://github.com/kostas1515/GOL",https://github.com/kostas1515/GOL,-1
Identifying Moments of Change from Longitudinal User Text,0.295369,"Identifying changes in individuals' behaviour and mood, as observed via
content shared on online platforms, is increasingly gaining importance. Most
research to-date on this topic focuses on either: (a) identifying individuals
at risk or with a certain mental health condition given a batch of posts or (b)
providing equivalent labels at the post level. A disadvantage of such work is
the lack of a strong temporal component and the inability to make longitudinal
assessments following an individual's trajectory and allowing timely
interventions. Here we define a new task, that of identifying moments of change
in individuals on the basis of their shared content online. The changes we
consider are sudden shifts in mood (switches) or gradual mood progression
(escalations). We have created detailed guidelines for capturing moments of
change and a corpus of 500 manually annotated user timelines (18.7K posts). We
have developed a variety of baseline models drawing inspiration from related
tasks and show that the best performance is obtained through context aware
sequential modelling. We also introduce new metrics for capturing rare events
in temporal windows.",None,-1
COIN: Co-Cluster Infomax for Bipartite Graphs,0.0815848,"Bipartite graphs are powerful data structures to model interactions between
two types of nodes, which have been used in a variety of applications, such as
recommender systems, information retrieval, and drug discovery. A fundamental
challenge for bipartite graphs is how to learn informative node embeddings.
Despite the success of recent self-supervised learning methods on bipartite
graphs, their objectives are discriminating instance-wise positive and negative
node pairs, which could contain cluster-level errors. In this paper, we
introduce a novel co-cluster infomax (COIN) framework, which captures the
cluster-level information by maximizing the mutual information of co-clusters.
Different from previous infomax methods which estimate mutual information by
neural networks, COIN could easily calculate mutual information. Besides, COIN
is an end-to-end coclustering method which can be trained jointly with other
objective functions and optimized via back-propagation. Furthermore, we also
provide theoretical analysis for COIN. We theoretically prove that COIN is able
to effectively increase the mutual information of node embeddings and COIN is
upper-bounded by the prior distributions of nodes. We extensively evaluate the
proposed COIN framework on various benchmark datasets and tasks to demonstrate
the effectiveness of COIN.",https://github.com/clhchtcjj/BiNE/tree/master/data/wiki,-1
Constrained Optimization with Dynamic Bound-scaling for Effective NLPBackdoor Defense,0.605026,"We develop a novel optimization method for NLPbackdoor inversion. We leverage
a dynamically reducing temperature coefficient in the softmax function to
provide changing loss landscapes to the optimizer such that the process
gradually focuses on the ground truth trigger, which is denoted as a one-hot
value in a convex hull. Our method also features a temperature rollback
mechanism to step away from local optimals, exploiting the observation that
local optimals can be easily deter-mined in NLP trigger inversion (while not in
general optimization). We evaluate the technique on over 1600 models (with
roughly half of them having injected backdoors) on 3 prevailing NLP tasks, with
4 different backdoor attacks and 7 architectures. Our results show that the
technique is able to effectively and efficiently detect and remove backdoors,
outperforming 4 baseline methods.",None,-1
Convolutional Neural Networks for Image Spam Detection,0.153383,"Spam can be defined as unsolicited bulk email. In an effort to evade
text-based filters, spammers sometimes embed spam text in an image, which is
referred to as image spam. In this research, we consider the problem of image
spam detection, based on image analysis. We apply convolutional neural networks
(CNN) to this problem, we compare the results obtained using CNNs to other
machine learning techniques, and we compare our results to previous related
work. We consider both real-world image spam and challenging image spam-like
datasets. Our results improve on previous work by employing CNNs based on a
novel feature set consisting of a combination of the raw image and Canny edges.",None,-1
A Close Look into the Calibration of Pre-trained Language Models,0.169751,"Pre-trained language models (PLMs) may fail in giving reliable estimates of
their predictive uncertainty. We take a close look into this problem, aiming to
answer two questions: (1) Do PLMs learn to become calibrated in the training
process? (2) How effective are existing calibration methods? For the first
question, we conduct fine-grained control experiments to study the dynamic
change in PLMs' calibration performance in training. We consider six factors as
control variables, including dataset difficulty, available training samples,
training steps, the number of tunable parameters, model scale, and pretraining.
We observe a consistent change in calibration performance across six factors.
We find that PLMs don't learn to become calibrated in training, evidenced by
the continual increase in confidence, no matter whether the predictions are
correct or not. We highlight that our finding somewhat contradicts two
established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining
improves model calibration. Next, we study the effectiveness of existing
calibration methods in mitigating the overconfidence issue. Besides unlearnable
calibration methods (e.g., label smoothing), we adapt and extend two recently
proposed learnable methods that directly collect data to train models to have
reasonable confidence estimations. Experimental results show that learnable
methods significantly reduce PLMs' confidence in wrong predictions. The code is
available at \url{https://github.com/lifan-yuan/PLMCalibration}.",https://github.com/lifan-yuan/PLMCalibration,-1
HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification,0.827891,"Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification due to its complex label hierarchy. Recently, the
pretrained language models (PLM)have been widely adopted in HTC through a
fine-tuning paradigm. However, in this paradigm, there exists a huge gap
between the classification tasks with sophisticated label hierarchy and the
masked language model (MLM) pretraining tasks of PLMs and thus the potentials
of PLMs can not be fully tapped. To bridge the gap, in this paper, we propose
HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label
MLM perspective. Specifically, we construct a dynamic virtual template and
label words that take the form of soft prompts to fuse the label hierarchy
knowledge and introduce a zero-bounded multi-label cross entropy loss to
harmonize the objectives of HTC and MLM. Extensive experiments show HPT
achieves state-of-the-art performances on 3 popular HTC datasets and is adept
at handling the imbalance and low resource situations. Our code is available at
https://github.com/wzh9969/HPT.",https://github.com/wzh9969/HPT,-1
Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization,0.889543,"Vision transformers (ViTs) are emerging with significantly improved accuracy
in computer vision tasks. However, their complex architecture and enormous
computation/storage demand impose urgent needs for new hardware accelerator
design methodology. This work proposes an FPGA-aware automatic ViT acceleration
framework based on the proposed mixed-scheme quantization. To the best of our
knowledge, this is the first FPGA-based ViT acceleration framework exploring
model quantization. Compared with state-of-the-art ViT quantization work
(algorithmic approach only without hardware acceleration), our quantization
achieves 0.47% to 1.36% higher Top-1 accuracy under the same bit-width.
Compared with the 32-bit floating-point baseline FPGA accelerator, our
accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS
vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.",None,-1
Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature,0.465856,"Literary translation is a culturally significant task, but it is bottlenecked
by the small number of qualified literary translators relative to the many
untranslated works published around the world. Machine translation (MT) holds
potential to complement the work of human translators by improving both
training procedures and their overall efficiency. Literary translation is less
constrained than more traditional MT settings since translators must balance
meaning equivalence, readability, and critical interpretability in the target
language. This property, along with the complex discourse-level context present
in literary texts, also makes literary MT more challenging to computationally
model and evaluate. To explore this task, we collect a dataset (Par3) of
non-English language novels in the public domain, each aligned at the paragraph
level to both human and automatic English translations. Using Par3, we discover
that expert literary translators prefer reference human translations over
machine-translated paragraphs at a rate of 84%, while state-of-the-art
automatic MT metrics do not correlate with those preferences. The experts note
that MT outputs contain not only mistranslations, but also discourse-disrupting
errors and stylistic inconsistencies. To address these problems, we train a
post-editing model whose output is preferred over normal MT output at a rate of
69% by experts. We publicly release Par3 at
https://github.com/katherinethai/par3/ to spur future research into literary
MT.",https://github.com/katherinethai/par3/,-1
Spatio-Temporal Dynamic Graph Relation Learning for Urban Metro Flow Prediction,0.164875,"Urban metro flow prediction is of great value for metro operation scheduling,
passenger flow management and personal travel planning. However, it faces two
main challenges. First, different metro stations, e.g. transfer stations and
non-transfer stations, have unique traffic patterns. Second, it is challenging
to model complex spatio-temporal dynamic relation of metro stations. To address
these challenges, we develop a spatio-temporal dynamic graph relational
learning model (STDGRL) to predict urban metro station flow. First, we propose
a spatio-temporal node embedding representation module to capture the traffic
patterns of different stations. Second, we employ a dynamic graph relationship
learning module to learn dynamic spatial relationships between metro stations
without a predefined graph adjacency matrix. Finally, we provide a
transformer-based long-term relationship prediction module for long-term metro
flow prediction. Extensive experiments are conducted based on metro data in
Beijing, Shanghai, Chongqing and Hangzhou. Experimental results show the
advantages of our method beyond 11 baselines for urban metro flow prediction.",None,-1
Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting,0.694871,"In this paper, we propose a novel end-to-end user-defined keyword spotting
method that utilizes linguistically corresponding patterns between speech and
text sequences. Unlike previous approaches requiring speech keyword enrollment,
our method compares input queries with an enrolled text keyword sequence. To
place the audio and text representations within a common latent space, we adopt
an attention-based cross-modal matching approach that is trained in an
end-to-end manner with monotonic matching loss and keyword classification loss.
We also utilize a de-noising loss for the acoustic embedding network to improve
robustness in noisy environments. Additionally, we introduce the LibriPhrase
dataset, a new short-phrase dataset based on LibriSpeech for efficiently
training keyword spotting models. Our proposed method achieves competitive
results on various evaluation sets compared to other single-modal and
cross-modal baselines.",https://github.com/gusrud1103/LibriPhrase.git,-1
Interactive Multi-Class Tiny-Object Detection,0.0681559,"Annotating tens or hundreds of tiny objects in a given image is laborious yet
crucial for a multitude of Computer Vision tasks. Such imagery typically
contains objects from various categories, yet the multi-class interactive
annotation setting for the detection task has thus far been unexplored. To
address these needs, we propose a novel interactive annotation method for
multiple instances of tiny objects from multiple classes, based on a few
point-based user inputs. Our approach, C3Det, relates the full image context
with annotator inputs in a local and global manner via late-fusion and
feature-correlation, respectively. We perform experiments on the Tiny-DOTA and
LCell datasets using both two-stage and one-stage object detection
architectures to verify the efficacy of our approach. Our approach outperforms
existing approaches in interactive annotation, achieving higher mAP with fewer
clicks. Furthermore, we validate the annotation efficiency of our approach in a
user study where it is shown to be 2.85x faster and yield only 0.36x task load
(NASA-TLX, lower is better) compared to manual annotation. The code is
available at
https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection.",https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection,-1
DocEnTr: An End-to-End Document Image Enhancement Transformer,0.621617,"Document images can be affected by many degradation scenarios, which cause
recognition and processing difficulties. In this age of digitization, it is
important to denoise them for proper usage. To address this challenge, we
present a new encoder-decoder architecture based on vision transformers to
enhance both machine-printed and handwritten document images, in an end-to-end
fashion. The encoder operates directly on the pixel patches with their
positional information without the use of any convolutional layers, while the
decoder reconstructs a clean image from the encoded patches. Conducted
experiments show a superiority of the proposed model compared to the state-of
the-art methods on several DIBCO benchmarks. Code and models will be publicly
available at: \url{https://github.com/dali92002/DocEnTR}.",https://github.com/dali92002/DocEnTR,-1
Biometric Signature Verification Using Recurrent Neural Networks,0.612743,"Architectures based on Recurrent Neural Networks (RNNs) have been
successfully applied to many different tasks such as speech or handwriting
recognition with state-of-the-art results. The main contribution of this work
is to analyse the feasibility of RNNs for on-line signature verification in
real practical scenarios. We have considered a system based on Long Short-Term
Memory (LSTM) with a Siamese architecture whose goal is to learn a similarity
metric from pairs of signatures. For the experimental work, the BiosecurID
database comprised of 400 users and 4 separated acquisition sessions are
considered. Our proposed LSTM RNN system has outperformed the results of recent
published works on the BiosecurID benchmark in figures ranging from 17.76% to
28.00% relative verification performance improvement for skilled forgeries.",None,-1
Meta Spatio-Temporal Debiasing for Video Scene Graph Generation,0.694763,"Video scene graph generation (VidSGG) aims to parse the video content into
scene graphs, which involves modeling the spatio-temporal contextual
information in the video. However, due to the long-tailed training data in
datasets, the generalization performance of existing VidSGG models can be
affected by the spatio-temporal conditional bias problem. In this work, from
the perspective of meta-learning, we propose a novel Meta Video Scene Graph
Generation (MVSGG) framework to address such a bias problem. Specifically, to
handle various types of spatio-temporal conditional biases, our framework first
constructs a support set and a group of query sets from the training data,
where the data distribution of each query set is different from that of the
support set w.r.t. a type of conditional bias. Then, by performing a novel meta
training and testing process to optimize the model to obtain good testing
performance on these query sets after training on the support set, our
framework can effectively guide the model to learn to well generalize against
biases. Extensive experiments demonstrate the efficacy of our proposed
framework.",None,-1
Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models,0.0975974,"Recently the prompt-tuning paradigm has attracted significant attention. By
only tuning continuous prompts with a frozen pre-trained language model (PLM),
prompt-tuning takes a step towards deploying a shared frozen PLM to serve
numerous downstream tasks. Although prompt-tuning shows good performance on
certain natural language understanding (NLU) tasks, its effectiveness on
natural language generation (NLG) tasks is still under-explored. In this paper,
we argue that one of the factors hindering the development of prompt-tuning on
NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different
from the pretraining corpus). For example, our preliminary exploration reveals
a large performance gap between prompt-tuning and fine-tuning when unfamiliar
inputs occur frequently in NLG tasks. This motivates us to propose
input-tuning, which fine-tunes both the continuous prompts and the input
representations, leading to a more effective way to adapt unfamiliar inputs to
frozen PLMs. Our proposed input-tuning is conceptually simple and empirically
powerful. Experimental results on seven NLG tasks demonstrate that input-tuning
is significantly and consistently better than prompt-tuning. Furthermore, on
three of these tasks, input-tuning can achieve a comparable or even better
performance than fine-tuning.",None,-1
VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction,0.337467,"The success of the Neural Radiance Fields (NeRF) in novel view synthesis has
inspired researchers to propose neural implicit scene reconstruction. However,
most existing neural implicit reconstruction methods optimize per-scene
parameters and therefore lack generalizability to new scenes. We introduce
VolRecon, a novel generalizable implicit reconstruction method with Signed Ray
Distance Function (SRDF). To reconstruct the scene with fine details and little
noise, VolRecon combines projection features aggregated from multi-view
features, and volume features interpolated from a coarse global feature volume.
Using a ray transformer, we compute SRDF values of sampled points on a ray and
then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by
about 30% in sparse view reconstruction and achieves comparable accuracy as
MVSNet in full view reconstruction. Furthermore, our approach exhibits good
generalization performance on the large-scale ETH3D benchmark.",https://github.com/IVRL/VolRecon/,-1
PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization,0.3437,"Few-shot abstractive summarization has become a challenging task in natural
language generation. To support it, we designed a novel soft prompts
architecture coupled with a prompt pre-training plus fine-tuning paradigm that
is effective and tunes only extremely light parameters. The soft prompts
include continuous input embeddings across an encoder and a decoder to fit the
structure of the generation models. Importantly, a novel inner-prompt placed in
the text is introduced to capture document-level information. The aim is to
devote attention to understanding the document that better prompts the model to
generate document-related content. The first step in the summarization
procedure is to conduct prompt pre-training with self-supervised pseudo-data.
This teaches the model basic summarizing capabilities. The model is then
fine-tuned with few-shot examples. Experimental results on the CNN/DailyMail
and XSum datasets show that our method, with only 0.1% of the parameters,
outperforms full-model tuning where all model parameters are tuned. It also
surpasses Prompt Tuning by a large margin and delivers competitive results
against Prefix-Tuning with 3% of the parameters.",None,-1
From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering,0.224685,"Multi-hop question answering (QA) is a challenging task requiring QA systems
to perform complex reasoning over multiple documents and provide supporting
facts together with the exact answer. Existing works tend to utilize
graph-based reasoning and question decomposition to obtain the reasoning chain,
which inevitably introduces additional complexity and cumulative error to the
system. To address the above issue, we propose a simple yet effective novel
framework, From Easy to Hard (FE2H), to remove distracting information and
obtain better contextual representations for the multi-hop QA task. Inspired by
the iterative document selection process and the progressive learning custom of
humans, FE2H divides both the document selector and reader into two stages
following an easy-to-hard manner. Specifically, we first select the document
most relevant to the question and then utilize the question together with this
document to select other pertinent documents. As for the QA phase, our reader
is first trained on a single-hop QA dataset and then transferred into the
multi-hop QA task. We comprehensively evaluate our model on the popular
multi-hop QA benchmark HotpotQA. Experimental results demonstrate that our
method ourperforms all other methods in the leaderboard of HotpotQA (distractor
setting).",None,-1
HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance,0.240826,"Marker-less monocular 3D human motion capture (MoCap) with scene interactions
is a challenging research topic relevant for extended reality, robotics and
virtual avatar generation. Due to the inherent depth ambiguity of monocular
settings, 3D motions captured with existing methods often contain severe
artefacts such as incorrect body-scene inter-penetrations, jitter and body
floating. To tackle these issues, we propose HULC, a new approach for 3D human
MoCap which is aware of the scene geometry. HULC estimates 3D poses and dense
body-environment surface contacts for improved 3D localisations, as well as the
absolute scale of the subject. Furthermore, we introduce a 3D pose trajectory
optimisation based on a novel pose manifold sampling that resolves erroneous
body-environment inter-penetrations. Although the proposed method requires less
structured inputs compared to existing scene-aware monocular MoCap algorithms,
it produces more physically-plausible poses: HULC significantly and
consistently outperforms the existing approaches in various experiments and on
different metrics. Project page: https://vcai.mpi-inf.mpg.de/projects/HULC/.",None,-1
Mix and Localize: Localizing Sound Sources in Mixtures,0.454825,"We present a method for simultaneously localizing multiple sound sources
within a visual scene. This task requires a model to both group a sound mixture
into individual sources, and to associate them with a visual signal. Our method
jointly solves both tasks at once, using a formulation inspired by the
contrastive random walk of Jabri et al. We create a graph in which images and
separated sounds correspond to nodes, and train a random walker to transition
between nodes from different modalities with high return probability. The
transition probabilities for this walk are determined by an audio-visual
similarity metric that is learned by our model. We show through experiments
with musical instruments and human speech that our model can successfully
localize multiple sounds, outperforming other self-supervised methods. Project
site: https://hxixixh.github.io/mix-and-localize",https://hxixixh.github.io/mix-and-localize,-1
Causal Fairness Analysis,0.156036,"Decision-making systems based on AI and machine learning have been used
throughout a wide range of real-world scenarios, including healthcare, law
enforcement, education, and finance. It is no longer far-fetched to envision a
future where autonomous systems will be driving entire business decisions and,
more broadly, supporting large-scale decision-making infrastructure to solve
society's most challenging problems. Issues of unfairness and discrimination
are pervasive when decisions are being made by humans, and remain (or are
potentially amplified) when decisions are made using machines with little
transparency, accountability, and fairness. In this paper, we introduce a
framework for \textit{causal fairness analysis} with the intent of filling in
this gap, i.e., understanding, modeling, and possibly solving issues of
fairness in decision-making settings. The main insight of our approach will be
to link the quantification of the disparities present on the observed data with
the underlying, and often unobserved, collection of causal mechanisms that
generate the disparity in the first place, challenge we call the Fundamental
Problem of Causal Fairness Analysis (FPCFA). In order to solve the FPCFA, we
study the problem of decomposing variations and empirical measures of fairness
that attribute such variations to structural mechanisms and different units of
the population. Our effort culminates in the Fairness Map, which is the first
systematic attempt to organize and explain the relationship between different
criteria found in the literature. Finally, we study which causal assumptions
are minimally needed for performing causal fairness analysis and propose a
Fairness Cookbook, which allows data scientists to assess the existence of
disparate impact and disparate treatment.",None,-1
Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries,0.218136,"Learning under a continuously changing data distribution with incorrect
labels is a desirable real-world problem yet challenging. A large body of
continual learning (CL) methods, however, assumes data streams with clean
labels, and online learning scenarios under noisy data streams are yet
underexplored. We consider a more practical CL task setup of an online learning
from blurry data stream with corrupted labels, where existing CL methods
struggle. To address the task, we first argue the importance of both diversity
and purity of examples in the episodic memory of continual learning models. To
balance diversity and purity in the episodic memory, we propose a novel
strategy to manage and use the memory by a unified approach of label noise
aware diverse sampling and robust learning with semi-supervised learning. Our
empirical validations on four real-world or synthetic noise datasets (CIFAR10
and 100, mini-WebVision, and Food-101N) exhibit that our method significantly
outperforms prior arts in this realistic and challenging continual learning
scenario. Code and data splits are available in
https://github.com/clovaai/puridiver.",https://github.com/clovaai/puridiver,-1
PolyHope: Two-Level Hope Speech Detection from Tweets,0.243655,"Hope is characterized as openness of spirit toward the future, a desire,
expectation, and wish for something to happen or to be true that remarkably
affects human's state of mind, emotions, behaviors, and decisions. Hope is
usually associated with concepts of desired expectations and
possibility/probability concerning the future. Despite its importance, hope has
rarely been studied as a social media analysis task. This paper presents a hope
speech dataset that classifies each tweet first into ""Hope"" and ""Not Hope"",
then into three fine-grained hope categories: ""Generalized Hope"", ""Realistic
Hope"", and ""Unrealistic Hope"" (along with ""Not Hope""). English tweets in the
first half of 2022 were collected to build this dataset. Furthermore, we
describe our annotation process and guidelines in detail and discuss the
challenges of classifying hope and the limitations of the existing hope speech
detection corpora. In addition, we reported several baselines based on
different learning approaches, such as traditional machine learning, deep
learning, and transformers, to benchmark our dataset. We evaluated our
baselines using weighted-averaged and macro-averaged F1-scores. Observations
show that a strict process for annotator selection and detailed annotation
guidelines enhanced the dataset's quality. This strict annotation process
resulted in promising performance for simple machine learning classifiers with
only bi-grams; however, binary and multiclass hope speech detection results
reveal that contextual embedding models have higher performance in this
dataset.",None,-1
Large-Field Contextual Feature Learning for Glass Detection,0.078477,"Glass is very common in our daily life. Existing computer vision systems
neglect it and thus may have severe consequences, e.g., a robot may crash into
a glass wall. However, sensing the presence of glass is not straightforward.
The key challenge is that arbitrary objects/scenes can appear behind the glass.
In this paper, we propose an important problem of detecting glass surfaces from
a single RGB image. To address this problem, we construct the first large-scale
glass detection dataset (GDD) and propose a novel glass detection network,
called GDNet-B, which explores abundant contextual cues in a large
field-of-view via a novel large-field contextual feature integration (LCFI)
module and integrates both high-level and low-level boundary features with a
boundary feature enhancement (BFE) module. Extensive experiments demonstrate
that our GDNet-B achieves satisfying glass detection results on the images
within and beyond the GDD testing set. We further validate the effectiveness
and generalization capability of our proposed GDNet-B by applying it to other
vision tasks, including mirror segmentation and salient object detection.
Finally, we show the potential applications of glass detection and discuss
possible future research directions.",None,-1
GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields,0.0662565,"We propose GazeNeRF, a 3D-aware method for the task of gaze redirection.
Existing gaze redirection methods operate on 2D images and struggle to generate
3D consistent results. Instead, we build on the intuition that the face region
and eyeballs are separate 3D structures that move in a coordinated yet
independent fashion. Our method leverages recent advancements in conditional
image-based neural radiance fields and proposes a two-stream architecture that
predicts volumetric features for the face and eye regions separately. Rigidly
transforming the eye features via a 3D rotation matrix provides fine-grained
control over the desired gaze angle. The final, redirected image is then
attained via differentiable volume compositing. Our experiments show that this
architecture outperforms naively conditioned NeRF baselines as well as previous
state-of-the-art 2D gaze redirection methods in terms of redirection accuracy
and identity preservation.",https://github.com/zllrunning/face-parsing.PyTorch,-1
Towards Better Chinese-centric Neural Machine Translation for Low-resource Languages,0.267489,"The last decade has witnessed enormous improvements in science and
technology, stimulating the growing demand for economic and cultural exchanges
in various countries. Building a neural machine translation (NMT) system has
become an urgent trend, especially in the low-resource setting. However, recent
work tends to study NMT systems for low-resource languages centered on English,
while few works focus on low-resource NMT systems centered on other languages
such as Chinese. To achieve this, the low-resource multilingual translation
challenge of the 2021 iFLYTEK AI Developer Competition provides the
Chinese-centric multilingual low-resource NMT tasks, where participants are
required to build NMT systems based on the provided low-resource samples. In
this paper, we present the winner competition system that leverages monolingual
word embeddings data enhancement, bilingual curriculum learning, and
contrastive re-ranking. In addition, a new Incomplete-Trust (In-trust) loss
function is proposed to replace the traditional cross-entropy loss when
training. The experimental results demonstrate that the implementation of these
ideas leads better performance than other state-of-the-art methods. All the
experimental codes are released at:
https://github.com/WENGSYX/Low-resource-text-translation.",https://github.com/WENGSYX/Low-resource-text-translation,-1
Annotating Norwegian Language Varieties on Twitter for Part-of-Speech,0.111534,"Norwegian Twitter data poses an interesting challenge for Natural Language
Processing (NLP) tasks. These texts are difficult for models trained on
standardized text in one of the two Norwegian written forms (Bokm{\aa}l and
Nynorsk), as they contain both the typical variation of social media text, as
well as a large amount of dialectal variety. In this paper we present a novel
Norwegian Twitter dataset annotated with POS-tags. We show that models trained
on Universal Dependency (UD) data perform worse when evaluated against this
dataset, and that models trained on Bokm{\aa}l generally perform better than
those trained on Nynorsk. We also see that performance on dialectal tweets is
comparable to the written standards for some models. Finally we perform a
detailed analysis of the errors that models commonly make on this data.",https://github.com/noklesta/,-1
MMNet: Muscle motion-guided network for micro-expression recognition,0.232883,"Facial micro-expressions (MEs) are involuntary facial motions revealing
peoples real feelings and play an important role in the early intervention of
mental illness, the national security, and many human-computer interaction
systems. However, existing micro-expression datasets are limited and usually
pose some challenges for training good classifiers. To model the subtle facial
muscle motions, we propose a robust micro-expression recognition (MER)
framework, namely muscle motion-guided network (MMNet). Specifically, a
continuous attention (CA) block is introduced to focus on modeling local subtle
muscle motion patterns with little identity information, which is different
from most previous methods that directly extract features from complete video
frames with much identity information. Besides, we design a position
calibration (PC) module based on the vision transformer. By adding the position
embeddings of the face generated by PC module at the end of the two branches,
the PC module can help to add position information to facial muscle motion
pattern features for the MER. Extensive experiments on three public
micro-expression datasets demonstrate that our approach outperforms
state-of-the-art methods by a large margin.",https://github.com/muse1998/MMNet,-1
Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction,0.185315,"The DocRED dataset is one of the most popular and widely used benchmarks for
document-level relation extraction (RE). It adopts a recommend-revise
annotation scheme so as to have a large-scale annotated dataset. However, we
find that the annotation of DocRED is incomplete, i.e., false negative samples
are prevalent. We analyze the causes and effects of the overwhelming false
negative problem in the DocRED dataset. To address the shortcoming, we
re-annotate 4,053 documents in the DocRED dataset by adding the missed relation
triples back to the original DocRED. We name our revised DocRED dataset
Re-DocRED. We conduct extensive experiments with state-of-the-art neural models
on both datasets, and the experimental results show that the models trained and
evaluated on our Re-DocRED achieve performance improvements of around 13 F1
points. Moreover, we conduct a comprehensive analysis to identify the potential
areas for further improvement. Our dataset is publicly available at
https://github.com/tonytan48/Re-DocRED.",https://github.com/tonytan48/Re-DocRED,-1
Human Activity Recognition from Wi-Fi CSI Data Using Principal Component-Based Wavelet CNN,0.106452,"Human Activity Recognition (HAR) is an emerging technology with several
applications in surveillance, security, and healthcare sectors. Noninvasive HAR
systems based on Wi-Fi Channel State Information (CSI) signals can be developed
leveraging the quick growth of ubiquitous Wi-Fi technologies, and the
correlation between CSI dynamics and body motions. In this paper, we propose
Principal Component-based Wavelet Convolutional Neural Network (or PCWCNN) -- a
novel approach that offers robustness and efficiency for practical real-time
applications. Our proposed method incorporates two efficient preprocessing
algorithms -- the Principal Component Analysis (PCA) and the Discrete Wavelet
Transform (DWT). We employ an adaptive activity segmentation algorithm that is
accurate and computationally light. Additionally, we used the Wavelet CNN for
classification, which is a deep convolutional network analogous to the
well-studied ResNet and DenseNet networks. We empirically show that our
proposed PCWCNN model performs very well on a real dataset, outperforming
existing approaches.",None,-1
A Distributional Lens for Multi-Aspect Controllable Text Generation,0.498208,"Multi-aspect controllable text generation is a more challenging and practical
task than single-aspect control. Existing methods achieve complex multi-aspect
control by fusing multiple controllers learned from single-aspect, but suffer
from attribute degeneration caused by the mutual interference of these
controllers. To address this, we provide observations on attribute fusion from
a distributional perspective and propose to directly search for the
intersection areas of multiple attribute distributions as their combination for
generation. Our method first estimates the attribute space with an autoencoder
structure. Afterward, we iteratively approach the intersections by jointly
minimizing distances to points representing different attributes. Finally, we
map them to attribute-relevant sentences with a prefix-tuning-based decoder.
Experiments on the three-aspect control task, including sentiment, topic, and
detoxification aspects, reveal that our method outperforms several strong
baselines on attribute relevance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory support for the effectiveness
of our approach.",https://github.com/HappyGu0524/MultiControl,-1
Describing Differences between Text Distributions with Natural Language,0.39926,"How do two distributions of texts differ? Humans are slow at answering this,
since discovering patterns might require tediously reading through hundreds of
samples. We propose to automatically summarize the differences by ""learning a
natural language hypothesis"": given two distributions $D_{0}$ and $D_{1}$, we
search for a description that is more often true for $D_{1}$, e.g., ""is
military-related."" To tackle this problem, we fine-tune GPT-3 to propose
descriptions with the prompt: ""[samples of $D_{0}$] + [samples of $D_{1}$] +
the difference between them is_____."" We then re-rank the descriptions by
checking how often they hold on a larger set of samples with a learned
verifier. On a benchmark of 54 real-world binary classification tasks, while
GPT-3 Curie (13B) only generates a description similar to human annotation 7%
of the time, the performance reaches 61% with fine-tuning and re-ranking, and
our best system using GPT-3 Davinci (175B) reaches 76%. We apply our system to
describe distribution shifts, debug dataset shortcuts, summarize unknown tasks,
and label text clusters, and present analyses based on automatically generated
descriptions.",None,-1
Generate rather than Retrieve: Large Language Models are Strong Context Generators,0.282701,"Knowledge-intensive tasks, such as open-domain question answering (QA),
require access to a large amount of world or domain knowledge. A common
approach for knowledge-intensive tasks is to employ a retrieve-then-read
pipeline that first retrieves a handful of relevant contextual documents from
an external corpus such as Wikipedia and then predicts an answer conditioned on
the retrieved documents. In this paper, we present a novel perspective for
solving knowledge-intensive tasks by replacing document retrievers with large
language model generators. We call our method generate-then-read (GenRead),
which first prompts a large language model to generate contextutal documents
based on a given question, and then reads the generated documents to produce
the final answer. Furthermore, we propose a novel clustering-based prompting
method that selects distinct prompts, resulting in the generated documents that
cover different perspectives, leading to better recall over acceptable answers.
We conduct extensive experiments on three different knowledge-intensive tasks,
including open-domain QA, fact checking, and dialogue system. Notably, GenRead
achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly
outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0
and +3.9, without retrieving any documents from any external knowledge source.
Lastly, we demonstrate the model performance can be further improved by
combining retrieval and generation. Our code and generated documents can be
found at https://github.com/wyu97/GenRead.",https://github.com/wyu97/GenRead,-1
Massively Digitized Power Grid: Opportunities and Challenges of Use-inspired AI,0.0474731,"This article presents a use-inspired perspective of the opportunities and
challenges in a massively digitized power grid. It argues that the intricate
interplay of data availability, computing capability, and artificial
intelligence (AI) algorithm development are the three key factors driving the
adoption of digitized solutions in the power grid. The impact of these three
factors on critical functions of power system operation and planning practices
are reviewed and illustrated with industrial practice case studies. Open
challenges and research opportunities for data, computing, and AI algorithms
are articulated within the context of the power industry's tremendous
decarbonization efforts.",None,-1
Multimodal learning with graphs,0.210374,"Artificial intelligence for graphs has achieved remarkable success in
modeling complex systems, ranging from dynamic networks in biology to
interacting particle systems in physics. However, the increasingly
heterogeneous graph datasets call for multimodal methods that can combine
different inductive biases: the set of assumptions that algorithms use to make
predictions for inputs they have not encountered during training. Learning on
multimodal datasets presents fundamental challenges because the inductive
biases can vary by data modality and graphs might not be explicitly given in
the input. To address these challenges, multimodal graph AI methods combine
different modalities while leveraging cross-modal dependencies using graphs.
Diverse datasets are combined using graphs and fed into sophisticated
multimodal architectures, specified as image-intensive, knowledge-grounded and
language-intensive models. Using this categorization, we introduce a blueprint
for multimodal graph learning, use it to study existing methods and provide
guidelines to design new models.",https://yashaektefaie.github.io/mgl,-1
Boundary-Guided Camouflaged Object Detection,0.356937,"Camouflaged object detection (COD), segmenting objects that are elegantly
blended into their surroundings, is a valuable yet challenging task. Existing
deep-learning methods often fall into the difficulty of accurately identifying
the camouflaged object with complete and fine object structure. To this end, in
this paper, we propose a novel boundary-guided network (BGNet) for camouflaged
object detection. Our method explores valuable and extra object-related edge
semantics to guide representation learning of COD, which forces the model to
generate features that highlight object structure, thereby promoting
camouflaged object detection of accurate boundary localization. Extensive
experiments on three challenging benchmark datasets demonstrate that our BGNet
significantly outperforms the existing 18 state-of-the-art methods under four
widely-used evaluation metrics. Our code is publicly available at:
https://github.com/thograce/BGNet.",None,-1
Stubborn: A Strong Baseline for Indoor Object Navigation,0.552096,"We present a strong baseline that surpasses the performance of previously
published methods on the Habitat Challenge task of navigating to a target
object in indoor environments. Our method is motivated from primary failure
modes of prior state-of-the-art: poor exploration, inaccurate object
identification, and agent getting trapped due to imprecise map construction. We
make three contributions to mitigate these issues: (i) First, we show that
existing map-based methods fail to effectively use semantic clues for
exploration. We present a semantic-agnostic exploration strategy (called
Stubborn) without any learning that surprisingly outperforms prior work. (ii)
We propose a strategy for integrating temporal information to improve object
identification. (iii) Lastly, due to inaccurate depth observation the agent
often gets trapped in small regions. We develop a multi-scale collision map for
obstacle identification that mitigates this issue.",https://github.com/Improbable-AI/Stubborn,-1
Ontology-enhanced Prompt-tuning for Few-shot Learning,0.99755,"Few-shot Learning (FSL) is aimed to make predictions based on a limited
number of samples. Structured data such as knowledge graphs and ontology
libraries has been leveraged to benefit the few-shot setting in various tasks.
However, the priors adopted by the existing methods suffer from challenging
knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder
the performance for few-shot learning. In this study, we explore knowledge
injection for FSL with pre-trained language models and propose
ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the
ontology transformation based on the external knowledge graph to address the
knowledge missing issue, which fulfills and converts structure knowledge to
text. We further introduce span-sensitive knowledge injection via a visible
matrix to select informative knowledge to handle the knowledge noise issue. To
bridge the gap between knowledge and text, we propose a collective training
algorithm to optimize representations jointly. We evaluate our proposed
OntoPrompt in three tasks, including relation extraction, event extraction, and
knowledge graph completion, with eight datasets. Experimental results
demonstrate that our approach can obtain better few-shot performance than
baselines.",None,-1
Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations,0.58123,"We propose an unsupervised method for 3D geometry-aware representation
learning of articulated objects, in which no image-pose pairs or foreground
masks are used for training. Though photorealistic images of articulated
objects can be rendered with explicit pose control through existing 3D neural
representations, these methods require ground truth 3D pose and foreground
masks for training, which are expensive to obtain. We obviate this need by
learning the representations with GAN training. The generator is trained to
produce realistic images of articulated objects from random poses and latent
vectors by adversarial training. To avoid a high computational cost for GAN
training, we propose an efficient neural representation for articulated objects
based on tri-planes and then present a GAN-based framework for its unsupervised
training. Experiments demonstrate the efficiency of our method and show that
GAN-based training enables the learning of controllable 3D representations
without paired supervision.",https://github.com/open-mmlab/mmpose,-1
DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following,0.793902,"Language-guided Embodied AI benchmarks requiring an agent to navigate an
environment and manipulate objects typically allow one-way communication: the
human user gives a natural language command to the agent, and the agent can
only follow the command passively. We present DialFRED, a dialogue-enabled
embodied instruction following benchmark based on the ALFRED benchmark.
DialFRED allows an agent to actively ask questions to the human user; the
additional information in the user's response is used by the agent to better
complete its task. We release a human-annotated dataset with 53K task-relevant
questions and answers and an oracle to answer questions. To solve DialFRED, we
propose a questioner-performer framework wherein the questioner is pre-trained
with the human-annotated data and fine-tuned with reinforcement learning. We
make DialFRED publicly available and encourage researchers to propose and
evaluate their solutions to building dialog-enabled embodied agents.",https://github.com/xfgao/DialFRED,-1
ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver Distraction Detection,0.214594,"Ensuring traffic safety and mitigating accidents in modern driving is of
paramount importance, and computer vision technologies have the potential to
significantly contribute to this goal. This paper presents a multi-modal Vision
Transformer for Driver Distraction Detection (termed ViT-DD), which
incorporates inductive information from training signals related to both
distraction detection and driver emotion recognition. Additionally, a
self-learning algorithm is developed, allowing for the seamless integration of
driver data without emotion labels into the multi-task training process of
ViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existing
state-of-the-art methods for driver distraction detection by 6.5% and 0.9% on
the SFDDD and AUCDD datasets, respectively.",None,-1
The Alberta Plan for AI Research,0.159051,"Herein we describe our approach to artificial intelligence research, which we
call the Alberta Plan. The Alberta Plan is pursued within our research groups
in Alberta and by others who are like minded throughout the world. We welcome
all who would join us in this pursuit.",None,-1
Prompt-Based Metric Learning for Few-Shot NER,0.130367,"Few-shot named entity recognition (NER) targets generalizing to unseen labels
and/or domains with few labeled examples. Existing metric learning methods
compute token-level similarities between query and support sets, but are not
able to fully incorporate label semantics into modeling. To address this issue,
we propose a simple method to largely improve metric learning for NER: 1)
multiple prompt schemas are designed to enhance label semantics; 2) we propose
a novel architecture to effectively combine multiple prompt-based
representations. Empirically, our method achieves new state-of-the-art (SOTA)
results under 16 of the 18 considered settings, substantially outperforming the
previous SOTA by an average of 8.84% and a maximum of 34.51% in relative gains
of micro F1. Our code is available at https://github.com/AChen-qaq/ProML.",https://github.com/AChen-qaq/ProML,-1
Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion Image Manipulation,0.565497,"Fashion attribute editing is a task that aims to convert the semantic
attributes of a given fashion image while preserving the irrelevant regions.
Previous works typically employ conditional GANs where the generator explicitly
learns the target attributes and directly execute the conversion. These
approaches, however, are neither scalable nor generic as they operate only with
few limited attributes and a separate generator is required for each dataset or
attribute set. Inspired by the recent advancement of diffusion models, we
explore the classifier-guided diffusion that leverages the off-the-shelf
diffusion model pretrained on general visual semantics such as Imagenet. In
order to achieve a generic editing pipeline, we pose this as multi-attribute
image manipulation task, where the attribute ranges from item category, fabric,
pattern to collar and neckline. We empirically show that conventional methods
fail in our challenging setting, and study efficient adaptation scheme that
involves recently introduced attention-pooling technique to obtain a
multi-attribute classifier guidance. Based on this, we present a mask-free
fashion attribute editing framework that leverages the classifier logits and
the cross-attention map for manipulation. We empirically demonstrate that our
framework achieves convincing sample quality and attribute alignments.",None,-1
Object Level Depth Reconstruction for Category Level 6D Object Pose Estimation From Monocular RGB Image,0.113915,"Recently, RGBD-based category-level 6D object pose estimation has achieved
promising improvement in performance, however, the requirement of depth
information prohibits broader applications. In order to relieve this problem,
this paper proposes a novel approach named Object Level Depth reconstruction
Network (OLD-Net) taking only RGB images as input for category-level 6D object
pose estimation. We propose to directly predict object-level depth from a
monocular RGB image by deforming the category-level shape prior into
object-level depth and the canonical NOCS representation. Two novel modules
named Normalized Global Position Hints (NGPH) and Shape-aware Decoupled Depth
Reconstruction (SDDR) module are introduced to learn high fidelity object-level
depth and delicate shape representations. At last, the 6D object pose is solved
by aligning the predicted canonical representation with the back-projected
object-level depth. Extensive experiments on the challenging CAMERA25 and
REAL275 datasets indicate that our model, though simple, achieves
state-of-the-art performance.",None,-1
Robust Trajectory Prediction against Adversarial Attacks,0.060549,"Trajectory prediction using deep neural networks (DNNs) is an essential
component of autonomous driving (AD) systems. However, these methods are
vulnerable to adversarial attacks, leading to serious consequences such as
collisions. In this work, we identify two key ingredients to defend trajectory
prediction models against adversarial attacks including (1) designing effective
adversarial training methods and (2) adding domain-specific data augmentation
to mitigate the performance degradation on clean data. We demonstrate that our
method is able to improve the performance by 46% on adversarial data and at the
cost of only 3% performance degradation on clean data, compared to the model
trained with clean data. Additionally, compared to existing robust methods, our
method can improve performance by 21% on adversarial examples and 9% on clean
data. Our robust model is evaluated with a planner to study its downstream
impacts. We demonstrate that our model can significantly reduce the severe
accident rates (e.g., collisions and off-road driving).",https://robustav.github.io/RobustTraj,-1
A simple language-agnostic yet very strong baseline system for hate speech and offensive content identification,0.11415,"For automatically identifying hate speech and offensive content in tweets, a
system based on a classical supervised algorithm only fed with character
n-grams, and thus completely language-agnostic, is proposed by the SATLab team.
After its optimization in terms of the feature weighting and the classifier
parameters, it reached, in the multilingual HASOC 2021 challenge, a medium
performance level in English, the language for which it is easy to develop deep
learning approaches relying on many external linguistic resources, but a far
better level for the two less resourced language, Hindi and Marathi. It ends
even first when performances are averaged over the three tasks in these
languages, outperforming many deep learning approaches. These performances
suggest that it is an interesting reference level to evaluate the benefits of
using more complex approaches such as deep learning or taking into account
complementary resources.",None,-1
CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification,0.532568,"Existing computer vision research in artwork struggles with artwork's
fine-grained attributes recognition and lack of curated annotated datasets due
to their costly creation. To the best of our knowledge, we are one of the first
methods to use CLIP (Contrastive Language-Image Pre-Training) to train a neural
network on a variety of artwork images and text descriptions pairs. CLIP is
able to learn directly from free-form art descriptions, or, if available,
curated fine-grained labels. Model's zero-shot capability allows predicting
accurate natural language description for a given image, without directly
optimizing for the task. Our approach aims to solve 2 challenges: instance
retrieval and fine-grained artwork attribute recognition. We use the iMet
Dataset, which we consider the largest annotated artwork dataset. In this
benchmark we achieved competitive results using only self-supervision.",https://github.com/KeremTurgutlu/clip_art,-1
FJMP: Factorized Joint Multi-Agent Motion Prediction over Learned Directed Acyclic Interaction Graphs,0.195239,"Predicting the future motion of road agents is a critical task in an
autonomous driving pipeline. In this work, we address the problem of generating
a set of scene-level, or joint, future trajectory predictions in multi-agent
driving scenarios. To this end, we propose FJMP, a Factorized Joint Motion
Prediction framework for multi-agent interactive driving scenarios. FJMP models
the future scene interaction dynamics as a sparse directed interaction graph,
where edges denote explicit interactions between agents. We then prune the
graph into a directed acyclic graph (DAG) and decompose the joint prediction
task into a sequence of marginal and conditional predictions according to the
partial ordering of the DAG, where joint future trajectories are decoded using
a directed acyclic graph neural network (DAGNN). We conduct experiments on the
INTERACTION and Argoverse 2 datasets and demonstrate that FJMP produces more
accurate and scene-consistent joint trajectory predictions than non-factorized
approaches, especially on the most interactive and kinematically interesting
agents. FJMP ranks 1st on the multi-agent test leaderboard of the INTERACTION
dataset.",None,-1
Video Prediction by Efficient Transformers,0.0,"Video prediction is a challenging computer vision task that has a wide range
of applications. In this work, we present a new family of Transformer-based
models for video prediction. Firstly, an efficient local spatial-temporal
separation attention mechanism is proposed to reduce the complexity of standard
Transformers. Then, a full autoregressive model, a partial autoregressive model
and a non-autoregressive model are developed based on the new efficient
Transformer. The partial autoregressive model has a similar performance with
the full autoregressive model but a faster inference speed. The
non-autoregressive model not only achieves a faster inference speed but also
mitigates the quality degradation problem of the autoregressive counterparts,
but it requires additional parameters and loss function for learning. Given the
same attention mechanism, we conducted a comprehensive study to compare the
proposed three video prediction variants. Experiments show that the proposed
video prediction models are competitive with more complex state-of-the-art
convolutional-LSTM based models. The source code is available at
https://github.com/XiYe20/VPTR.",https://github.com/XiYe20/VPTR,-1
CITRIS: Causal Identifiability from Temporal Intervened Sequences,0.400436,"Understanding the latent causal factors of a dynamical system from visual
observations is considered a crucial step towards agents reasoning in complex
environments. In this paper, we propose CITRIS, a variational autoencoder
framework that learns causal representations from temporal sequences of images
in which underlying causal factors have possibly been intervened upon. In
contrast to the recent literature, CITRIS exploits temporality and observing
intervention targets to identify scalar and multidimensional causal factors,
such as 3D rotation angles. Furthermore, by introducing a normalizing flow,
CITRIS can be easily extended to leverage and disentangle representations
obtained by already pretrained autoencoders. Extending previous results on
scalar causal factors, we prove identifiability in a more general setting, in
which only some components of a causal factor are affected by interventions. In
experiments on 3D rendered image sequences, CITRIS outperforms previous methods
on recovering the underlying causal variables. Moreover, using pretrained
autoencoders, CITRIS can even generalize to unseen instantiations of causal
factors, opening future research areas in sim-to-real generalization for causal
representation learning.",https://github.com/phlippe/CITRIS,-1
Hyperbolic Image Segmentation,0.127573,"For image segmentation, the current standard is to perform pixel-level
optimization and inference in Euclidean output embedding spaces through linear
hyperplanes. In this work, we show that hyperbolic manifolds provide a valuable
alternative for image segmentation and propose a tractable formulation of
hierarchical pixel-level classification in hyperbolic space. Hyperbolic Image
Segmentation opens up new possibilities and practical benefits for
segmentation, such as uncertainty estimation and boundary information for free,
zero-label generalization, and increased performance in low-dimensional output
embeddings.",https://github.com/MinaGhadimiAtigh/HyperbolicImageSegmentation,-1
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning,0.686992,"Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.",https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt,-1
Crosslingual Generalization through Multitask Finetuning,0.772832,"Multitask prompted finetuning (MTF) has been shown to help large language
models generalize to new tasks in a zero-shot setting, but so far explorations
of MTF have focused on English data and models. We apply MTF to the pretrained
multilingual BLOOM and mT5 model families to produce finetuned variants called
BLOOMZ and mT0. We find finetuning large multilingual language models on
English tasks with English prompts allows for task generalization to
non-English languages that appear only in the pretraining corpus. Finetuning on
multilingual tasks with English prompts further improves performance on English
and non-English tasks leading to various state-of-the-art zero-shot results. We
also investigate finetuning on multilingual tasks with prompts that have been
machine-translated from English to match the language of each dataset. We find
training on these machine-translated prompts leads to better performance on
human-written prompts in the respective languages. Surprisingly, we find models
are capable of zero-shot generalization to tasks in languages they have never
intentionally seen. We conjecture that the models are learning higher-level
capabilities that are both task- and language-agnostic. In addition, we
introduce xP3, a composite of supervised datasets in 46 languages with English
and machine-translated prompts. Our code, datasets and models are freely
available at https://github.com/bigscience-workshop/xmtf.",None,-1
Towards High-Fidelity Single-view Holistic Reconstruction of Indoor Scenes,0.448684,"We present a new framework to reconstruct holistic 3D indoor scenes including
both room background and indoor objects from single-view images. Existing
methods can only produce 3D shapes of indoor objects with limited geometry
quality because of the heavy occlusion of indoor scenes. To solve this, we
propose an instance-aligned implicit function (InstPIFu) for detailed object
reconstruction. Combining with instance-aligned attention module, our method is
empowered to decouple mixed local features toward the occluded instances.
Additionally, unlike previous methods that simply represents the room
background as a 3D bounding box, depth map or a set of planes, we recover the
fine geometry of the background via implicit representation. Extensive
experiments on the SUN RGB-D, Pix3D, 3D-FUTURE, and 3D-FRONT datasets
demonstrate that our method outperforms existing approaches in both background
and foreground object reconstruction. Our code and model will be made publicly
available.",None,-1
Q-ViT: Fully Differentiable Quantization for Vision Transformer,0.0,"In this paper, we propose a fully differentiable quantization method for
vision transformer (ViT) named as Q-ViT, in which both of the quantization
scales and bit-widths are learnable parameters. Specifically, based on our
observation that heads in ViT display different quantization robustness, we
leverage head-wise bit-width to squeeze the size of Q-ViT while preserving
performance. In addition, we propose a novel technique named switchable scale
to resolve the convergence problem in the joint training of quantization scales
and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to
3-bit without heavy performance drop. Moreover, we analyze the quantization
robustness of every architecture component of ViT and show that the Multi-head
Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key
aspects for ViT quantization. This study provides some insights for further
research about ViT quantization. Extensive experiments on different ViT models,
such as DeiT and Swin Transformer show the effectiveness of our quantization
method. In particular, our method outperforms the state-of-the-art uniform
quantization method by 1.5% on DeiT-Tiny.",https://github.com/zhexinli/Q-ViT-DeiT,-1
The Topological BERT: Transforming Attention into Topology for Natural Language Processing,0.086162,"In recent years, the introduction of the Transformer models sparked a
revolution in natural language processing (NLP). BERT was one of the first text
encoders using only the attention mechanism without any recurrent parts to
achieve state-of-the-art results on many NLP tasks.
  This paper introduces a text classifier using topological data analysis. We
use BERT's attention maps transformed into attention graphs as the only input
to that classifier. The model can solve tasks such as distinguishing spam from
ham messages, recognizing whether a sentence is grammatically correct, or
evaluating a movie review as negative or positive. It performs comparably to
the BERT baseline and outperforms it on some tasks.
  Additionally, we propose a new method to reduce the number of BERT's
attention heads considered by the topological classifier, which allows us to
prune the number of heads from 144 down to as few as ten with no reduction in
performance. Our work also shows that the topological model displays higher
robustness against adversarial attacks than the original BERT model, which is
maintained during the pruning process. To the best of our knowledge, this work
is the first to confront topological-based models with adversarial attacks in
the context of NLP.",None,-1
A Multimodal Corpus for Emotion Recognition in Sarcasm,0.0951924,"While sentiment and emotion analysis have been studied extensively, the
relationship between sarcasm and emotion has largely remained unexplored. A
sarcastic expression may have a variety of underlying emotions. For example, ""I
love being ignored"" belies sadness, while ""my mobile is fabulous with a battery
backup of only 15 minutes!"" expresses frustration. Detecting the emotion behind
a sarcastic expression is non-trivial yet an important task. We undertake the
task of detecting the emotion in a sarcastic statement, which to the best of
our knowledge, is hitherto unexplored. We start with the recently released
multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions.
We identify and correct 343 incorrect emotion labels (out of 690). We double
the size of the dataset, label it with emotions along with valence and arousal
which are important indicators of emotional intensity. Finally, we label each
sarcastic utterance with one of the four sarcasm types-Propositional, Embedded,
Likeprefixed and Illocutionary, with the goal of advancing sarcasm detection
research. Exhaustive experimentation with multimodal (text, audio, and video)
fusion models establishes a benchmark for exact emotion recognition in sarcasm
and outperforms the state-of-art sarcasm detection. We release the dataset
enriched with various annotations and the code for research purposes:
https://github.com/apoorva-nunna/MUStARD_Plus_Plus",https://github.com/apoorva-nunna/MUStARD_Plus_Plus,-1
Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing,0.110103,"Generated texts from large pretrained language models have been shown to
exhibit a variety of harmful, human-like biases about various demographics.
These findings prompted large efforts aiming to understand and measure such
effects, with the goal of providing benchmarks that can guide the development
of techniques mitigating these stereotypical associations. However, as recent
research has pointed out, the current benchmarks lack a robust experimental
setup, consequently hindering the inference of meaningful conclusions from
their evaluation metrics. In this paper, we extend these arguments and
demonstrate that existing techniques and benchmarks aiming to measure
stereotypes tend to be inaccurate and consist of a high degree of experimental
noise that severely limits the knowledge we can gain from benchmarking language
models based on them. Accordingly, we propose a new framework for robustly
measuring and quantifying biases exhibited by generative language models.
Finally, we use this framework to investigate GPT-3's occupational gender bias
and propose prompting techniques for mitigating these biases without the need
for fine-tuning.",None,-1
Nonparametric Masked Language Modeling,0.351153,"Existing language models (LMs) predict tokens with a softmax over a finite
vocabulary, which can make it difficult to predict rare tokens or phrases. We
introduce NPM, the first nonparametric masked language model that replaces this
softmax with a nonparametric distribution over every phrase in a reference
corpus. NPM fills in the [MASK] solely from retrieving a token from a text
corpus. We show that NPM can be efficiently trained with a contrastive
objective and an in-batch approximation to full corpus retrieval. Zero-shot
evaluation on 16 tasks including classification, fact probing and question
answering demonstrates that NPM outperforms significantly larger parametric
models, either with or without a retrieve-and-generate approach. It is
particularly better at dealing with rare patterns (word senses or facts) and
predicting rare or nearly unseen words (e.g., non-Latin script). We release the
model and code at github.com/facebookresearch/NPM.",https://github.com/facebookresearch/NPM,-1
Creativity in translation: machine translation as a constraint for literary texts,0.374209,"This article presents the results of a study involving the translation of a
short story by Kurt Vonnegut from English to Catalan and Dutch using three
modalities: machine-translation (MT), post-editing (PE) and translation without
aid (HT). Our aim is to explore creativity, understood to involve novelty and
acceptability, from a quantitative perspective. The results show that HT has
the highest creativity score, followed by PE, and lastly, MT, and this is
unanimous from all reviewers. A neural MT system trained on literary data does
not currently have the necessary capabilities for a creative translation; it
renders literal solutions to translation problems. More importantly, using MT
to post-edit raw output constrains the creativity of translators, resulting in
a poorer translation often not fit for publication, according to experts.",None,-1
Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,0.282364,"Pre-trained language models (LMs) are shown to easily generate toxic
language. In this work, we systematically explore domain-adaptive training to
reduce the toxicity of language models. We conduct this study on three
dimensions: training corpus, model size, and parameter efficiency. For the
training corpus, we propose to leverage the generative power of LMs and
generate nontoxic datasets for domain-adaptive training, which mitigates the
exposure bias and is shown to be more data-efficient than using a curated
pre-training corpus. We demonstrate that the self-generation method
consistently outperforms the existing baselines across various model sizes on
both automatic and human evaluations, even when it uses a 1/3 smaller training
corpus. We then comprehensively study detoxifying LMs with parameter sizes
ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never
been studied before. We find that i) large LMs have similar toxicity levels as
smaller ones given the same pre-training corpus, and ii) large LMs require more
endeavor to detoxify. We also explore parameter-efficient training methods for
detoxification. We demonstrate that adding and training adapter-only layers in
LMs not only saves a lot of parameters but also achieves a better trade-off
between toxicity and perplexity than whole model adaptation for the large-scale
models.",https://github.com/NVIDIA/Megatron-LM/,-1
Learning Appearance-motion Normality for Video Anomaly Detection,0.470494,"Video anomaly detection is a challenging task in the computer vision
community. Most single task-based methods do not consider the independence of
unique spatial and temporal patterns, while two-stream structures lack the
exploration of the correlations. In this paper, we propose spatial-temporal
memories augmented two-stream auto-encoder framework, which learns the
appearance normality and motion normality independently and explores the
correlations via adversarial learning. Specifically, we first design two proxy
tasks to train the two-stream structure to extract appearance and motion
features in isolation. Then, the prototypical features are recorded in the
corresponding spatial and temporal memory pools. Finally, the encoding-decoding
network performs adversarial learning with the discriminator to explore the
correlations between spatial and temporal patterns. Experimental results show
that our framework outperforms the state-of-the-art methods, achieving AUCs of
98.1% and 89.8% on UCSD Ped2 and CUHK Avenue datasets.",None,-1
TAPE: Task-Agnostic Prior Embedding for Image Restoration,0.234432,"Learning a generalized prior for natural image restoration is an important
yet challenging task. Early methods mostly involved handcrafted priors
including normalized sparsity, l_0 gradients, dark channel priors, etc.
Recently, deep neural networks have been used to learn various image priors but
do not guarantee to generalize. In this paper, we propose a novel approach that
embeds a task-agnostic prior into a transformer. Our approach, named
Task-Agnostic Prior Embedding (TAPE), consists of two stages, namely,
task-agnostic pre-training and task-specific fine-tuning, where the first stage
embeds prior knowledge about natural images into the transformer and the second
stage extracts the knowledge to assist downstream image restoration.
Experiments on various types of degradation validate the effectiveness of TAPE.
The image restoration performance in terms of PSNR is improved by as much as
1.45dB and even outperforms task-specific algorithms. More importantly, TAPE
shows the ability of disentangling generalized image priors from degraded
images, which enjoys favorable transfer ability to unknown downstream tasks.",None,-1
Exploring Visual Prompts for Adapting Large-Scale Models,0.982586,"We investigate the efficacy of visual prompting to adapt large-scale models
in vision. Following the recent approach from prompt tuning and adversarial
reprogramming, we learn a single image perturbation such that a frozen model
prompted with this perturbation performs a new task. Through comprehensive
experiments, we demonstrate that visual prompting is particularly effective for
CLIP and robust to distribution shift, achieving performance competitive with
standard linear probes. We further analyze properties of the downstream
dataset, prompt design, and output transformation in regard to adaptation
performance. The surprising effectiveness of visual prompting provides a new
perspective on adapting pre-trained models in vision. Code is available at
http://hjbahng.github.io/visual_prompting .",https://hjbahng.github.io/visual_prompting/,-1
BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment,0.778042,"This work addresses the Burst Super-Resolution (BurstSR) task using a new
architecture, which requires restoring a high-quality image from a sequence of
noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in
BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can
significantly improve the capability of extracting inter-frame information and
reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided
Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin
Transformer Blocks and Groups as our main backbone. More specifically, we
combine optical flows and deformable convolutions, hence our BSRT can handle
misalignment and aggregate the potential texture information in multi-frames
more efficiently. In addition, our Transformer-based structure can capture
long-range dependency to further improve the performance. The evaluation on
both synthetic and real-world tracks demonstrates that our approach achieves a
new state-of-the-art in BurstSR task. Further, our BSRT wins the championship
in the NTIRE2022 Burst Super-Resolution Challenge.",https://github.com/Algolzw/BSRT,-1
Penalized Proximal Policy Optimization for Safe Reinforcement Learning,0.628747,"Safe reinforcement learning aims to learn the optimal policy while satisfying
safety constraints, which is essential in real-world applications. However,
current algorithms still struggle for efficient policy updates with hard
constraint satisfaction. In this paper, we propose Penalized Proximal Policy
Optimization (P3O), which solves the cumbersome constrained policy iteration
via a single minimization of an equivalent unconstrained problem. Specifically,
P3O utilizes a simple-yet-effective penalty function to eliminate cost
constraints and removes the trust-region constraint by the clipped surrogate
objective. We theoretically prove the exactness of the proposed method with a
finite penalty factor and provide a worst-case analysis for approximate error
when evaluated on sample trajectories. Moreover, we extend P3O to more
challenging multi-constraint and multi-agent scenarios which are less studied
in previous work. Extensive experiments show that P3O outperforms
state-of-the-art algorithms with respect to both reward improvement and
constraint satisfaction on a set of constrained locomotive tasks.",https://github.com/openai/safety-starter-agents,-1
Teaching Where to Look: Attention Similarity Knowledge Distillation for Low Resolution Face Recognition,0.314045,"Deep learning has achieved outstanding performance for face recognition
benchmarks, but performance reduces significantly for low resolution (LR)
images. We propose an attention similarity knowledge distillation approach,
which transfers attention maps obtained from a high resolution (HR) network as
a teacher into an LR network as a student to boost LR recognition performance.
Inspired by humans being able to approximate an object's region from an LR
image based on prior knowledge obtained from HR images, we designed the
knowledge distillation loss using the cosine similarity to make the student
network's attention resemble the teacher network's attention. Experiments on
various LR face related benchmarks confirmed the proposed method generally
improved recognition performances on LR settings, outperforming
state-of-the-art results by simply transferring well-constructed attention
maps. The code and pretrained models are publicly available in the
https://github.com/gist-ailab/teaching-where-to-look.",https://github.com/gist-ailab/teaching-where-to-look,-1
Realistic Blur Synthesis for Learning Image Deblurring,0.490688,"Training learning-based deblurring methods demands a tremendous amount of
blurred and sharp image pairs. Unfortunately, existing synthetic datasets are
not realistic enough, and deblurring models trained on them cannot handle real
blurred images effectively. While real datasets have recently been proposed,
they provide limited diversity of scenes and camera settings, and capturing
real datasets for diverse settings is still challenging. To resolve this, this
paper analyzes various factors that introduce differences between real and
synthetic blurred images. To this end, we present RSBlur, a novel dataset with
real blurred images and the corresponding sharp image sequences to enable a
detailed analysis of the difference between real and synthetic blur. With the
dataset, we reveal the effects of different factors in the blur generation
process. Based on the analysis, we also present a novel blur synthesis pipeline
to synthesize more realistic blur. We show that our synthesis pipeline can
improve the deblurring performance on real blurred images.",None,-1
Learning a General Clause-to-Clause Relationships for Enhancing Emotion-Cause Pair Extraction,0.39477,"Emotion-cause pair extraction (ECPE) is an emerging task aiming to extract
potential pairs of emotions and corresponding causes from documents. Previous
approaches have focused on modeling the pair-to-pair relationship and achieved
promising results. However, the clause-to-clause relationship, which
fundamentally symbolizes the underlying structure of a document, has still been
in its research infancy. In this paper, we define a novel clause-to-clause
relationship. To learn it applicably, we propose a general clause-level
encoding model named EA-GAT comprising E-GAT and Activation Sort. E-GAT is
designed to aggregate information from different types of clauses; Activation
Sort leverages the individual emotion/cause prediction and the sort-based
mapping to propel the clause to a more favorable representation. Since EA-GAT
is a clause-level encoding model, it can be broadly integrated with any
previous approach. Experimental results show that our approach has a
significant advantage over all current approaches on the Chinese and English
benchmark corpus, with an average of $2.1\%$ and $1.03\%$.",None,-1
Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis,0.598595,"Recent literature focuses on utilizing the entity information in the
sentence-level relation extraction (RE), but this risks leaking superficial and
spurious clues of relations. As a result, RE still suffers from unintended
entity bias, i.e., the spurious correlation between entity mentions (names) and
relations. Entity bias can mislead the RE models to extract the relations that
do not exist in the text. To combat this issue, some previous work masks the
entity mentions to prevent the RE models from overfitting entity mentions.
However, this strategy degrades the RE performance because it loses the
semantic information of entities. In this paper, we propose the CORE
(Counterfactual Analysis based Relation Extraction) debiasing method that
guides the RE models to focus on the main effects of textual context without
losing the entity information. We first construct a causal graph for RE, which
models the dependencies between variables in RE models. Then, we propose to
conduct counterfactual analysis on our causal graph to distill and mitigate the
entity bias, that captures the causal effects of specific entity mentions in
each instance. Note that our CORE method is model-agnostic to debias existing
RE systems during inference without changing their training processes.
Extensive experimental results demonstrate that our CORE yields significant
gains on both effectiveness and generalization for RE. The source code is
provided at: https://github.com/vanoracai/CoRE.",https://github.com/vanoracai/CoRE,-1
DeepLIIF: An Online Platform for Quantification of Clinical Pathology Slides,0.115666,"In the clinic, resected tissue samples are stained with Hematoxylin-and-Eosin
(H&E) and/or Immunhistochemistry (IHC) stains and presented to the pathologists
on glass slides or as digital scans for diagnosis and assessment of disease
progression. Cell-level quantification, e.g. in IHC protein expression scoring,
can be extremely inefficient and subjective. We present DeepLIIF
(https://deepliif.org), a first free online platform for efficient and
reproducible IHC scoring. DeepLIIF outperforms current state-of-the-art
approaches (relying on manual error-prone annotations) by virtually restaining
clinical IHC slides with more informative multiplex immunofluorescence
staining. Our DeepLIIF cloud-native platform supports (1) more than 150
proprietary/non-proprietary input formats via the Bio-Formats standard, (2)
interactive adjustment, visualization, and downloading of the IHC
quantification results and the accompanying restained images, (3) consumption
of an exposed workflow API programmatically or through interactive plugins for
open source whole slide image viewers such as QuPath/ImageJ, and (4) auto
scaling to efficiently scale GPU resources based on user demand.",https://github.com/nadeemlab/DeepLIIF,-1
A Generalist Framework for Panoptic Segmentation of Images and Videos,0.758526,"Panoptic segmentation assigns semantic and instance ID labels to every pixel
of an image. As permutations of instance IDs are also valid solutions, the task
requires learning of high-dimensional one-to-many mapping. As a result,
state-of-the-art approaches use customized architectures and task-specific loss
functions. We formulate panoptic segmentation as a discrete data generation
problem, without relying on inductive bias of the task. A diffusion model is
proposed to model panoptic masks, with a simple architecture and generic loss
function. By simply adding past predictions as a conditioning signal, our
method is capable of modeling video (in a streaming setting) and thereby learns
to track object instances automatically. With extensive experiments, we
demonstrate that our simple approach can perform competitively to
state-of-the-art specialist methods in similar settings.",https://github.com/google-research/pix2seq,-1
ARST: Auto-Regressive Surgical Transformer for Phase Recognition from Laparoscopic Videos,0.262701,"Phase recognition plays an essential role for surgical workflow analysis in
computer assisted intervention. Transformer, originally proposed for sequential
data modeling in natural language processing, has been successfully applied to
surgical phase recognition. Existing works based on transformer mainly focus on
modeling attention dependency, without introducing auto-regression. In this
work, an Auto-Regressive Surgical Transformer, referred as ARST, is first
proposed for on-line surgical phase recognition from laparoscopic videos,
modeling the inter-phase correlation implicitly by conditional probability
distribution. To reduce inference bias and to enhance phase consistency, we
further develop a consistency constraint inference strategy based on
auto-regression. We conduct comprehensive validations on a well-known public
dataset Cholec80. Experimental results show that our method outperforms the
state-of-the-art methods both quantitatively and qualitatively, and achieves an
inference rate of 66 frames per second (fps).",None,-1
Towards automatic generation of Piping and Instrumentation Diagrams (P&IDs) with Artificial Intelligence,0.367751,"Developing Piping and Instrumentation Diagrams (P&IDs) is a crucial step
during the development of chemical processes. Currently, this is a tedious,
manual, and time-consuming task. We propose a novel, completely data-driven
method for the prediction of control structures. Our methodology is inspired by
end-to-end transformer-based human language translation models. We cast the
control structure prediction as a translation task where Process Flow Diagrams
(PFDs) are translated to P&IDs. To use established transformer-based language
translation models, we represent the P&IDs and PFDs as strings using our
recently proposed SFILES 2.0 notation. Model training is performed in a
transfer learning approach. Firstly, we pre-train our model using generated
P&IDs to learn the grammatical structure of the process diagrams. Thereafter,
the model is fine-tuned leveraging transfer learning on real P&IDs. The model
achieved a top-5 accuracy of 74.8% on 10,000 generated P&IDs and 89.2% on
100,000 generated P&IDs. These promising results show great potential for
AI-assisted process engineering. The tests on a dataset of 312 real P&IDs
indicate the need of a larger P&IDs dataset for industry applications.",None,-1
Streaming Adaptive Submodular Maximization,0.0679227,"Many sequential decision making problems can be formulated as an adaptive
submodular maximization problem. However, most of existing studies in this
field focus on pool-based setting, where one can pick items in any order, and
there have been few studies for the stream-based setting where items arrive in
an arbitrary order and one must immediately decide whether to select an item or
not upon its arrival. In this paper, we introduce a new class of utility
functions, semi-policywise submodular functions. We develop a series of
effective algorithms to maximize a semi-policywise submodular function under
the stream-based setting.",None,-1
Integrating Human-in-the-loop into Swarm Learning for Decentralized Fake News Detection,0.306214,"Social media has become an effective platform to generate and spread fake
news that can mislead people and even distort public opinion. Centralized
methods for fake news detection, however, cannot effectively protect user
privacy during the process of centralized data collection for training models.
Moreover, it cannot fully involve user feedback in the loop of learning
detection models for further enhancing fake news detection. To overcome these
challenges, this paper proposed a novel decentralized method, Human-in-the-loop
Based Swarm Learning (HBSL), to integrate user feedback into the loop of
learning and inference for recognizing fake news without violating user privacy
in a decentralized manner. It consists of distributed nodes that are able to
independently learn and detect fake news on local data. Furthermore, detection
models trained on these nodes can be enhanced through decentralized model
merging. Experimental results demonstrate that the proposed method outperforms
the state-of-the-art decentralized method in regard of detecting fake news on a
benchmark dataset.",None,-1
TEMPERA: Test-Time Prompting via Reinforcement Learning,0.83425,"Careful prompt design is critical to the use of large language models in
zero-shot or few-shot learning. As a consequence, there is a growing interest
in automated methods to design optimal prompts. In this work, we propose
Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to
prior prompt generation methods, TEMPERA can efficiently leverage prior
knowledge, is adaptive to different queries and provides an interpretable
prompt for every query. To achieve this, we design a novel action space that
allows flexible editing of the initial prompts covering a wide set of
commonly-used components like instructions, few-shot exemplars, and
verbalizers. The proposed method achieves significant gains compared with
recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a
variety of tasks including sentiment analysis, topic classification, natural
language inference, and reading comprehension. Our method achieves 5.33x on
average improvement in sample efficiency when compared to the traditional
fine-tuning methods.",https://github.com/tianjunz/TEMPERA,-1
Omnifont Persian OCR System Using Primitives,0.0962347,"In this paper, we introduce a model-based omnifont Persian OCR system. The
system uses a set of 8 primitive elements as structural features for
recognition. First, the scanned document is preprocessed. After normalizing the
preprocessed image, text rows and sub-words are separated and then thinned.
After recognition of dots in sub-words, strokes are extracted and primitive
elements of each sub-word are recognized using the strokes. Finally, the
primitives are compared with a predefined set of character identification
vectors in order to identify sub-word characters. The separation and
recognition steps of the system are concurrent, eliminating unavoidable errors
of independent separation of letters. The system has been tested on documents
with 14 standard Persian fonts in 6 sizes. The achieved precision is 97.06%.",None,-1
Transformers are Sample-Efficient World Models,0.0742307,"Deep reinforcement learning agents are notoriously sample inefficient, which
considerably limits their application to real-world problems. Recently, many
model-based methods have been designed to address this issue, with learning in
the imagination of a world model being one of the most prominent approaches.
However, while virtually unlimited interaction with a simulated environment
sounds appealing, the world model has to be accurate over extended periods of
time. Motivated by the success of Transformers in sequence modeling tasks, we
introduce IRIS, a data-efficient agent that learns in a world model composed of
a discrete autoencoder and an autoregressive Transformer. With the equivalent
of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean
human normalized score of 1.046, and outperforms humans on 10 out of 26 games,
setting a new state of the art for methods without lookahead search. To foster
future research on Transformers and world models for sample-efficient
reinforcement learning, we release our code and models at
https://github.com/eloialonso/iris.",https://github.com/eloialonso/iris,-1
Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021,0.217865,"Human intelligence has the remarkable ability to quickly adapt to new tasks
and environments. Starting from a very young age, humans acquire new skills and
learn how to solve new tasks either by imitating the behavior of others or by
following provided natural language instructions. To facilitate research in
this direction, we propose \emph{IGLU: Interactive Grounded Language
Understanding in a Collaborative Environment}.
  The primary goal of the competition is to approach the problem of how to
build interactive agents that learn to solve a task while provided with
grounded natural language instructions in a collaborative environment.
Understanding the complexity of the challenge, we split it into sub-tasks to
make it feasible for participants.",None,-1
Towards Inter-character Relationship-driven Story Generation,0.0649849,"In this paper, we introduce the task of modeling interpersonal relationships
for story generation. For addressing this task, we propose Relationships as
Latent Variables for Story Generation, (ReLiSt). ReLiSt generates stories
sentence by sentence and has two major components - a relationship selector and
a story continuer. The relationship selector specifies a latent variable to
pick the relationship to exhibit in the next sentence and the story continuer
generates the next sentence while expressing the selected relationship in a
coherent way. Our automatic and human evaluations demonstrate that ReLiSt is
able to generate stories with relationships that are more faithful to desired
relationships while maintaining the content quality. The relationship
assignments to sentences during inference bring interpretability to ReLiSt.",https://github.com/dbamman/book-nlp,-1
Revisiting Discrete Soft Actor-Critic,0.22748,"We study the adaption of soft actor-critic (SAC) from continuous action space
to discrete action space. We revisit vanilla SAC and provide an in-depth
understanding of its Q value underestimation and performance instability issues
when applied to discrete settings. We thereby propose entropy-penalty and
double average Q-learning with Q-clip to address these issues. Extensive
experiments on typical benchmarks with discrete action space, including Atari
games and a large-scale MOBA game, show the efficacy of our proposed method.
Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.",https://github.com/coldsummerday/Revisiting-Discrete-SAC,-1
Sample-Efficient Reinforcement Learning of Partially Observable Markov Games,0.159042,"This paper considers the challenging tasks of Multi-Agent Reinforcement
Learning (MARL) under partial observability, where each agent only sees her own
individual observations and actions that reveal incomplete information about
the underlying state of system. This paper studies these tasks under the
general model of multiplayer general-sum Partially Observable Markov Games
(POMGs), which is significantly larger than the standard model of Imperfect
Information Extensive-Form Games (IIEFGs). We identify a rich subclass of POMGs
-- weakly revealing POMGs -- in which sample-efficient learning is tractable.
In the self-play setting, we prove that a simple algorithm combining optimism
and Maximum Likelihood Estimation (MLE) is sufficient to find approximate Nash
equilibria, correlated equilibria, as well as coarse correlated equilibria of
weakly revealing POMGs, in a polynomial number of samples when the number of
agents is small. In the setting of playing against adversarial opponents, we
show that a variant of our optimistic MLE algorithm is capable of achieving
sublinear regret when being compared against the optimal maximin policies. To
our best knowledge, this work provides the first line of sample-efficient
results for learning POMGs.",None,-1
MoSE: Modality Split and Ensemble for Multimodal Knowledge Graph Completion,0.232213,"Multimodal knowledge graph completion (MKGC) aims to predict missing entities
in MKGs. Previous works usually share relation representation across
modalities. This results in mutual interference between modalities during
training, since for a pair of entities, the relation from one modality probably
contradicts that from another modality. Furthermore, making a unified
prediction based on the shared relation representation treats the input in
different modalities equally, while their importance to the MKGC task should be
different. In this paper, we propose MoSE, a Modality Split representation
learning and Ensemble inference framework for MKGC. Specifically, in the
training phase, we learn modality-split relation embeddings for each modality
instead of a single modality-shared one, which alleviates the modality
interference. Based on these embeddings, in the inference phase, we first make
modality-split predictions and then exploit various ensemble methods to combine
the predictions with different weights, which models the modality importance
dynamically. Experimental results on three KG datasets show that MoSE
outperforms state-of-the-art MKGC methods. Codes are available at
https://github.com/OreOZhao/MoSE4MKGC.",https://github.com/OreOZhao/MoSE4MKGC,-1
iCaps: Iterative Category-level Object Pose and Shape Estimation,0.697521,"This paper proposes a category-level 6D object pose and shape estimation
approach iCaps, which allows tracking 6D poses of unseen objects in a category
and estimating their 3D shapes. We develop a category-level auto-encoder
network using depth images as input, where feature embeddings from the
auto-encoder encode poses of objects in a category. The auto-encoder can be
used in a particle filter framework to estimate and track 6D poses of objects
in a category. By exploiting an implicit shape representation based on signed
distance functions, we build a LatentNet to estimate a latent representation of
the 3D shape given the estimated pose of an object. Then the estimated pose and
shape can be used to update each other in an iterative way. Our category-level
6D object pose and shape estimation pipeline only requires 2D detection and
segmentation for initialization. We evaluate our approach on a publicly
available dataset and demonstrate its effectiveness. In particular, our method
achieves comparably high accuracy on shape estimation.",http://github.com/aerogjy/iCaps,-1
ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering,0.533581,"With the recent advance in large pre-trained language models, researchers
have achieved record performances in NLP tasks that mostly focus on language
pattern matching. The community is experiencing the shift of the challenge from
how to model language to the imitation of complex reasoning abilities like
human beings. In this work, we investigate the application domain of finance
that involves real-world, complex numerical reasoning. We propose a new
large-scale dataset, ConvFinQA, aiming to study the chain of numerical
reasoning in conversational question answering. Our dataset poses great
challenge in modeling long-range, complex numerical reasoning paths in
real-world conversations. We conduct comprehensive experiments and analyses
with both the neural symbolic methods and the prompting-based methods, to
provide insights into the reasoning mechanisms of these two divisions. We
believe our new dataset should serve as a valuable resource to push forward the
exploration of real-world, complex reasoning tasks as the next research focus.
Our dataset and code is publicly available at
https://github.com/czyssrs/ConvFinQA.",https://github.com/czyssrs/ConvFinQA,-1
CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation,0.52123,"We propose Clustering Mask Transformer (CMT-DeepLab), a transformer-based
framework for panoptic segmentation designed around clustering. It rethinks the
existing transformer architectures used in segmentation and detection;
CMT-DeepLab considers the object queries as cluster centers, which fill the
role of grouping the pixels when applied to segmentation. The clustering is
computed with an alternating procedure, by first assigning pixels to the
clusters by their feature affinity, and then updating the cluster centers and
pixel features. Together, these operations comprise the Clustering Mask
Transformer (CMT) layer, which produces cross-attention that is denser and more
consistent with the final segmentation task. CMT-DeepLab improves the
performance over prior art significantly by 4.4% PQ, achieving a new
state-of-the-art of 55.7% PQ on the COCO test-dev set.",None,-1
A Double-Graph Based Framework for Frame Semantic Parsing,0.082765,"Frame semantic parsing is a fundamental NLP task, which consists of three
subtasks: frame identification, argument identification and role
classification. Most previous studies tend to neglect relations between
different subtasks and arguments and pay little attention to ontological frame
knowledge defined in FrameNet. In this paper, we propose a Knowledge-guided
Incremental semantic parser with Double-graph (KID). We first introduce Frame
Knowledge Graph (FKG), a heterogeneous graph containing both frames and FEs
(Frame Elements) built on the frame knowledge so that we can derive
knowledge-enhanced representations for frames and FEs. Besides, we propose
Frame Semantic Graph (FSG) to represent frame semantic structures extracted
from the text with graph structures. In this way, we can transform frame
semantic parsing into an incremental graph construction problem to strengthen
interactions between subtasks and relations between arguments. Our experiments
show that KID outperforms the previous state-of-the-art method by up to 1.7
F1-score on two FrameNet datasets. Our code is availavle at
https://github.com/PKUnlp-icler/KID.",https://github.com/PKUnlp-icler/KID,-1
Patterns of near-crash events in a naturalistic driving dataset: applying rules mining,0.13531,"This study aims to explore the associations between near-crash events and
road geometry and trip features by investigating a naturalistic driving dataset
and a corresponding roadway inventory dataset using an association rule mining
method.",None,-1
Large region targets observation scheduling by multiple satellites using resampling particle swarm optimization,0.148467,"The last decades have witnessed a rapid increase of Earth observation
satellites (EOSs), leading to the increasing complexity of EOSs scheduling. On
account of the widespread applications of large region observation, this paper
aims to address the EOSs observation scheduling problem for large region
targets. A rapid coverage calculation method employing a projection reference
plane and a polygon clipping technique is first developed. We then formulate a
nonlinear integer programming model for the scheduling problem, where the
objective function is calculated based on the developed coverage calculation
method. A greedy initialization-based resampling particle swarm optimization
(GI-RPSO) algorithm is proposed to solve the model. The adopted greedy
initialization strategy and particle resampling method contribute to generating
efficient and effective solutions during the evolution process. In the end,
extensive experiments are conducted to illustrate the effectiveness and
reliability of the proposed method. Compared to the traditional particle swarm
optimization and the widely used greedy algorithm, the proposed GI-RPSO can
improve the scheduling result by 5.42% and 15.86%, respectively.",None,-1
A general-purpose material property data extraction pipeline from large polymer corpora using Natural Language Processing,0.391596,"The ever-increasing number of materials science articles makes it hard to
infer chemistry-structure-property relations from published literature. We used
natural language processing (NLP) methods to automatically extract material
property data from the abstracts of polymer literature. As a component of our
pipeline, we trained MaterialsBERT, a language model, using 2.4 million
materials science abstracts, which outperforms other baseline models in three
out of five named entity recognition datasets when used as the encoder for
text. Using this pipeline, we obtained ~300,000 material property records from
~130,000 abstracts in 60 hours. The extracted data was analyzed for a diverse
range of applications such as fuel cells, supercapacitors, and polymer solar
cells to recover non-trivial insights. The data extracted through our pipeline
is made available through a web platform at https://polymerscholar.org which
can be used to locate material property data recorded in abstracts
conveniently. This work demonstrates the feasibility of an automatic pipeline
that starts from published literature and ends with a complete set of extracted
material property information.",None,-1
Synthetic Distracted Driving (SynDD2) dataset for analyzing distracted behaviors and various gaze zones of a driver,0.207391,"This article presents a synthetic distracted driving (SynDD2 - a continuum of
SynDD1) dataset for machine learning models to detect and analyze drivers'
various distracted behavior and different gaze zones. We collected the data in
a stationary vehicle using three in-vehicle cameras positioned at locations: on
the dashboard, near the rearview mirror, and on the top right-side window
corner. The dataset contains two activity types: distracted activities and gaze
zones for each participant, and each activity type has two sets: without
appearance blocks and with appearance blocks such as wearing a hat or
sunglasses. The order and duration of each activity for each participant are
random. In addition, the dataset contains manual annotations for each activity,
having its start and end time annotated. Researchers could use this dataset to
evaluate the performance of machine learning algorithms to classify various
distracting activities and gaze zones of drivers.",None,-1
Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing,0.0609663,"Entity typing aims at predicting one or more words that describe the type(s)
of a specific mention in a sentence. Due to shortcuts from surface patterns to
annotated entity labels and biased training, existing entity typing models are
subject to the problem of spurious correlations. To comprehensively investigate
the faithfulness and reliability of entity typing methods, we first
systematically define distinct kinds of model biases that are reflected mainly
from spurious correlations. Particularly, we identify six types of existing
model biases, including mention-context bias, lexical overlapping bias, named
entity bias, pronoun bias, dependency bias, and overgeneralization bias. To
mitigate model biases, we then introduce a counterfactual data augmentation
method. By augmenting the original training set with their debiased
counterparts, models are forced to fully comprehend sentences and discover the
fundamental cues for entity typing, rather than relying on spurious
correlations for shortcuts. Experimental results on the UFET dataset show our
counterfactual data augmentation approach helps improve generalization of
different entity typing models with consistently better performance on both the
original and debiased test sets.",https://github.com/luka-group/DiagnoseET,-1
Evaluating and Inducing Personality in Pre-trained Language Models,0.156256,"Standardized and quantified evaluation of machine behaviors is a crux of
understanding LLMs. In this study, we draw inspiration from psychometric
studies by leveraging human personality theory as a tool for studying machine
behaviors. Originating as a philosophical quest for human behaviors, the study
of personality delves into how individuals differ in thinking, feeling, and
behaving. Toward building and understanding human-like social machines, we are
motivated to ask: Can we assess machine behaviors by leveraging human
psychometric tests in a principled and quantitative manner? If so, can we
induce a specific personality in LLMs? To answer these questions, we introduce
the Machine Personality Inventory (MPI) tool for studying machine behaviors;
MPI follows standardized personality tests, built upon the Big Five Personality
Factors (Big Five) theory and personality assessment inventories. By
systematically evaluating LLMs with MPI, we provide the first piece of evidence
demonstrating the efficacy of MPI in studying LLMs behaviors. We further devise
a Personality Prompting (P^2) method to induce LLMs with specific personalities
in a controllable way, capable of producing diverse and verifiable behaviors.
We hope this work sheds light on future studies by adopting personality as the
essential indicator for various downstream tasks, and could further motivate
research into equally intriguing human-like machine behaviors.",None,-1
Real-time Online Video Detection with Temporal Smoothing Transformers,0.202654,"Streaming video recognition reasons about objects and their actions in every
frame of a video. A good streaming recognition model captures both long-term
dynamics and short-term changes of video. Unfortunately, in most existing
methods, the computational complexity grows linearly or quadratically with the
length of the considered dynamics. This issue is particularly pronounced in
transformer-based architectures. To address this issue, we reformulate the
cross-attention in a video transformer through the lens of kernel and apply two
kinds of temporal smoothing kernel: A box kernel or a Laplace kernel. The
resulting streaming attention reuses much of the computation from frame to
frame, and only requires a constant time update each frame. Based on this idea,
we build TeSTra, a Temporal Smoothing Transformer, that takes in arbitrarily
long inputs with constant caching and computing overhead. Specifically, it runs
$6\times$ faster than equivalent sliding-window based transformers with 2,048
frames in a streaming setting. Furthermore, thanks to the increased temporal
span, TeSTra achieves state-of-the-art results on THUMOS'14 and
EPIC-Kitchen-100, two standard online action detection and action anticipation
datasets. A real-time version of TeSTra outperforms all but one prior
approaches on the THUMOS'14 dataset.",https://github.com/zhaoyue-zephyrus/TeSTra/,-1
Automated Isovist Computation for Minecraft,0.142191,"Procedural content generation for games is a growing trend in both research
and industry, even though there is no consensus of how good content looks, nor
how to automatically evaluate it. A number of metrics have been developed in
the past, usually focused on the artifact as a whole, and mostly lacking
grounding in human experience. In this study we develop a new set of automated
metrics, motivated by ideas from architecture, namely isovists and space
syntax, which have a track record of capturing human experience of space. These
metrics can be computed for a specific game state, from the player's
perspective, and take into account their embodiment in the game world. We show
how to apply those metrics to the 3d blockworld of Minecraft. We use a dataset
of generated settlements from the GDMC Settlement Generation Challenge in
Minecraft and establish several rank-based correlations between the isovist
properties and the rating human judges gave those settelements. We also produce
a range of heat maps that demonstrate the location based applicability of the
approach, which allows for development of those metrics as measures for a game
experience at a specific time and space.",None,-1
LISA: Learning Implicit Shape and Appearance of Hands,0.734463,"This paper proposes a do-it-all neural model of human hands, named LISA. The
model can capture accurate hand shape and appearance, generalize to arbitrary
hand subjects, provide dense surface correspondences, be reconstructed from
images in the wild and easily animated. We train LISA by minimizing the shape
and appearance losses on a large set of multi-view RGB image sequences
annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand
local coordinate, our model predicts the color and the signed distance with
respect to each hand bone independently, and then combines the per-bone
predictions using predicted skinning weights. The shape, color and pose
representations are disentangled by design, allowing to estimate or animate
only selected parameters. We experimentally demonstrate that LISA can
accurately reconstruct a dynamic hand from monocular or multi-view sequences,
achieving a noticeably higher quality of reconstructed hand shapes compared to
baseline approaches. Project page:
https://www.iri.upc.edu/people/ecorona/lisa/.",https://www.iri.upc.edu/people/ecorona/lisa/,-1
DIMBA: Discretely Masked Black-Box Attack in Single Object Tracking,0.0924791,"The adversarial attack can force a CNN-based model to produce an incorrect
output by craftily manipulating human-imperceptible input. Exploring such
perturbations can help us gain a deeper understanding of the vulnerability of
neural networks, and provide robustness to deep learning against miscellaneous
adversaries. Despite extensive studies focusing on the robustness of image,
audio, and NLP, works on adversarial examples of visual object tracking --
especially in a black-box manner -- are quite lacking. In this paper, we
propose a novel adversarial attack method to generate noises for single object
tracking under black-box settings, where perturbations are merely added on
initial frames of tracking sequences, which is difficult to be noticed from the
perspective of a whole video clip. Specifically, we divide our algorithm into
three components and exploit reinforcement learning for localizing important
frame patches precisely while reducing unnecessary computational queries
overhead. Compared to existing techniques, our method requires fewer queries on
initialized frames of a video to manipulate competitive or even better attack
performance. We test our algorithm in both long-term and short-term datasets,
including OTB100, VOT2018, UAV123, and LaSOT. Extensive experiments demonstrate
the effectiveness of our method on three mainstream types of trackers:
discrimination, Siamese-based, and reinforcement learning-based trackers.",None,-1
Learning Probabilities of Causation from Finite Population Data,0.238181,"This paper deals with the problem of learning the probabilities of causation
of subpopulations given finite population data. The tight bounds of three basic
probabilities of causation, the probability of necessity and sufficiency (PNS),
the probability of sufficiency (PS), and the probability of necessity (PN),
were derived by Tian and Pearl. However, obtaining the bounds for each
subpopulation requires experimental and observational distributions of each
subpopulation, which is usually impractical to estimate given finite population
data. We propose a machine learning model that helps to learn the bounds of the
probabilities of causation for subpopulations given finite population data. We
further show by a simulated study that the machine learning model is able to
learn the bounds of PNS for 32768 subpopulations with only knowing roughly 500
of them from the finite population data.",None,-1
JuryGCN: Quantifying Jackknife Uncertainty on Graph Convolutional Networks,0.0893711,"Graph Convolutional Network (GCN) has exhibited strong empirical performance
in many real-world applications. The vast majority of existing works on GCN
primarily focus on the accuracy while ignoring how confident or uncertain a GCN
is with respect to its predictions. Despite being a cornerstone of trustworthy
graph mining, uncertainty quantification on GCN has not been well studied and
the scarce existing efforts either fail to provide deterministic quantification
or have to change the training procedure of GCN by introducing additional
parameters or architectures. In this paper, we propose the first
frequentist-based approach named JuryGCN in quantifying the uncertainty of GCN,
where the key idea is to quantify the uncertainty of a node as the width of
confidence interval by a jackknife estimator. Moreover, we leverage the
influence functions to estimate the change in GCN parameters without
re-training to scale up the computation. The proposed JuryGCN is capable of
quantifying uncertainty deterministically without modifying the GCN
architecture or introducing additional parameters. We perform extensive
experimental evaluation on real-world datasets in the tasks of both active
learning and semi-supervised node classification, which demonstrate the
efficacy of the proposed method.",None,-1
Monte Carlo Tree Search based Variable Selection for High Dimensional Bayesian Optimization,0.216425,"Bayesian optimization (BO) is a class of popular methods for expensive
black-box optimization, and has been widely applied to many scenarios. However,
BO suffers from the curse of dimensionality, and scaling it to high-dimensional
problems is still a challenge. In this paper, we propose a variable selection
method MCTS-VS based on Monte Carlo tree search (MCTS), to iteratively select
and optimize a subset of variables. That is, MCTS-VS constructs a
low-dimensional subspace via MCTS and optimizes in the subspace with any BO
algorithm. We give a theoretical analysis of the general variable selection
method to reveal how it can work. Experiments on high-dimensional synthetic
functions and real-world problems (i.e., NAS-bench problems and MuJoCo
locomotion tasks) show that MCTS-VS equipped with a proper BO optimizer can
achieve state-of-the-art performance.",https://github.com/lamda-bbo/MCTS-VS,-1
Beyond the Return: Off-policy Function Estimation under User-specified Error-measuring Distributions,0.0586005,"Off-policy evaluation often refers to two related tasks: estimating the
expected return of a policy and estimating its value function (or other
functions of interest, such as density ratios). While recent works on
marginalized importance sampling (MIS) show that the former can enjoy provable
guarantees under realizable function approximation, the latter is only known to
be feasible under much stronger assumptions such as prohibitively expressive
discriminators. In this work, we provide guarantees for off-policy function
estimation under only realizability, by imposing proper regularization on the
MIS objectives. Compared to commonly used regularization in MIS, our
regularizer is much more flexible and can account for an arbitrary
user-specified distribution, under which the learned function will be close to
the groundtruth. We provide exact characterization of the optimal dual solution
that needs to be realized by the discriminator class, which determines the
data-coverage assumption in the case of value-function learning. As another
surprising observation, the regularizer can be altered to relax the
data-coverage requirement, and completely eliminate it in the ideal case with
strong side information.",None,-1
Patch-wise Contrastive Style Learning for Instagram Filter Removal,0.0603154,"Image-level corruptions and perturbations degrade the performance of CNNs on
different downstream vision tasks. Social media filters are one of the most
common resources of various corruptions and perturbations for real-world visual
analysis applications. The negative effects of these distractive factors can be
alleviated by recovering the original images with their pure style for the
inference of the downstream vision tasks. Assuming these filters substantially
inject a piece of additional style information to the social media images, we
can formulate the problem of recovering the original versions as a reverse
style transfer problem. We introduce Contrastive Instagram Filter Removal
Network (CIFR), which enhances this idea for Instagram filter removal by
employing a novel multi-layer patch-wise contrastive style learning mechanism.
Experiments show our proposed strategy produces better qualitative and
quantitative results than the previous studies. Moreover, we present the
results of our additional experiments for proposed architecture within
different settings. Finally, we present the inference outputs and quantitative
comparison of filtered and recovered images on localization and segmentation
tasks to encourage the main motivation for this problem.",https://github.com/birdortyedi/cifr-pytorch,-1
Instance-Specific Image Goal Navigation: Training Embodied Agents to Find Object Instances,0.242267,"We consider the problem of embodied visual navigation given an image-goal
(ImageNav) where an agent is initialized in an unfamiliar environment and
tasked with navigating to a location 'described' by an image. Unlike related
navigation tasks, ImageNav does not have a standardized task definition which
makes comparison across methods difficult. Further, existing formulations have
two problematic properties; (1) image-goals are sampled from random locations
which can lead to ambiguity (e.g., looking at walls), and (2) image-goals match
the camera specification and embodiment of the agent; this rigidity is limiting
when considering user-driven downstream applications. We present the
Instance-specific ImageNav task (InstanceImageNav) to address these
limitations. Specifically, the goal image is 'focused' on some particular
object instance in the scene and is taken with camera parameters independent of
the agent. We instantiate InstanceImageNav in the Habitat Simulator using
scenes from the Habitat-Matterport3D dataset (HM3D) and release a standardized
benchmark to measure community progress.",https://github.com/facebookresearch/habitat-lab,-1
Disentangling Identity and Pose for Facial Expression Recognition,0.0501211,"Facial expression recognition (FER) is a challenging problem because the
expression component is always entangled with other irrelevant factors, such as
identity and head pose. In this work, we propose an identity and pose
disentangled facial expression recognition (IPD-FER) model to learn more
discriminative feature representation. We regard the holistic facial
representation as the combination of identity, pose and expression. These three
components are encoded with different encoders. For identity encoder, a well
pre-trained face recognition model is utilized and fixed during training, which
alleviates the restriction on specific expression training data in previous
works and makes the disentanglement practicable on in-the-wild datasets. At the
same time, the pose and expression encoder are optimized with corresponding
labels. Combining identity and pose feature, a neutral face of input individual
should be generated by the decoder. When expression feature is added, the input
image should be reconstructed. By comparing the difference between synthesized
neutral and expressional images of the same individual, the expression
component is further disentangled from identity and pose. Experimental results
verify the effectiveness of our method on both lab-controlled and in-the-wild
databases and we achieve state-of-the-art recognition performance.",https://github.com/WIKI2020/FacePose,-1
BigColor: Colorization using a Generative Color Prior for Natural Images,0.0442489,"For realistic and vivid colorization, generative priors have recently been
exploited. However, such generative priors often fail for in-the-wild complex
images due to their limited representation space. In this paper, we propose
BigColor, a novel colorization approach that provides vivid colorization for
diverse in-the-wild images with complex structures. While previous generative
priors are trained to synthesize both image structures and colors, we learn a
generative color prior to focus on color synthesis given the spatial structure
of an image. In this way, we reduce the burden of synthesizing image structures
from the generative prior and expand its representation space to cover diverse
images. To this end, we propose a BigGAN-inspired encoder-generator network
that uses a spatial feature map instead of a spatially-flattened BigGAN latent
code, resulting in an enlarged representation space. Our method enables robust
colorization for diverse inputs in a single forward pass, supports arbitrary
input resolutions, and provides multi-modal colorization results. We
demonstrate that BigColor significantly outperforms existing methods especially
on in-the-wild images with complex structures.",https://github.com/jantic/DeOldify,-1
TimeLMs: Diachronic Language Models from Twitter,0.99188,"Despite its importance, the time variable has been largely neglected in the
NLP and language model literature. In this paper, we present TimeLMs, a set of
language models specialized on diachronic Twitter data. We show that a
continual learning strategy contributes to enhancing Twitter-based language
models' capacity to deal with future and out-of-distribution tweets, while
making them competitive with standardized and more monolithic benchmarks. We
also perform a number of qualitative analyses showing how they cope with trends
and peaks in activity involving specific named entities or concept drift.",https://github.com/cardiffnlp/timelms,-1
Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness,0.0507472,"Data-driven predictive solutions predominant in commercial applications tend
to suffer from biases and stereotypes, which raises equity concerns. Prediction
models may discover, use, or amplify spurious correlations based on gender or
other protected personal characteristics, thus discriminating against
marginalized groups. Mitigating gender bias has become an important research
focus in natural language processing (NLP) and is an area where annotated
corpora are available. Data augmentation reduces gender bias by adding
counterfactual examples to the training dataset. In this work, we show that
some of the examples in the augmented dataset can be not important or even
harmful for fairness. We hence propose a general method for pruning both the
factual and counterfactual examples to maximize the model's fairness as
measured by the demographic parity, equality of opportunity, and equality of
odds. The fairness achieved by our method surpasses that of data augmentation
on three text classification datasets, using no more than half of the examples
in the augmented dataset. Our experiments are conducted using models of varying
sizes and pre-training settings.",https://github.com/Garrett-R/gender bender,-1
Uncertainty-aware deep learning methods for robust diabetic retinopathy classification,0.407944,"Automatic classification of diabetic retinopathy from retinal images has been
widely studied using deep neural networks with impressive results. However,
there is a clinical need for estimation of the uncertainty in the
classifications, a shortcoming of modern neural networks. Recently, approximate
Bayesian deep learning methods have been proposed for the task but the studies
have only considered the binary referable/non-referable diabetic retinopathy
classification applied to benchmark datasets. We present novel results by
systematically investigating a clinical dataset and a clinically relevant
5-class classification scheme, in addition to benchmark datasets and the binary
classification scheme. Moreover, we derive a connection between uncertainty
measures and classifier risk, from which we develop a new uncertainty measure.
We observe that the previously proposed entropy-based uncertainty measure
generalizes to the clinical dataset on the binary classification scheme but not
on the 5-class scheme, whereas our new uncertainty measure generalizes to the
latter case.",None,-1
Multi-View Document Representation Learning for Open-Domain Dense Retrieval,0.496007,"Dense retrieval has achieved impressive advances in first-stage retrieval
from a large-scale document collection, which is built on bi-encoder
architecture to produce single vector representation of query and document.
However, a document can usually answer multiple potential queries from
different views. So the single vector representation of a document is hard to
match with multi-view queries, and faces a semantic mismatch problem. This
paper proposes a multi-view document representation learning framework, aiming
to produce multi-view embeddings to represent documents and enforce them to
align with different queries. First, we propose a simple yet effective method
of generating multiple embeddings through viewers. Second, to prevent
multi-view embeddings from collapsing to the same one, we further propose a
global-local loss with annealed temperature to encourage the multiple viewers
to better align with different potential queries. Experiments show our method
outperforms recent works and achieves state-of-the-art results.",None,-1
"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",0.741303,"The past decade has witnessed dramatic gains in natural language processing
and an unprecedented scaling of large language models. These developments have
been accelerated by the advent of few-shot techniques such as chain of thought
(CoT) prompting. Specifically, CoT pushes the performance of large language
models in a few-shot setup by augmenting the prompts with intermediate steps.
Despite impressive results across various tasks, the reasons behind their
success have not been explored. This work uses counterfactual prompting to
develop a deeper understanding of CoT-based few-shot prompting mechanisms in
large language models. We first systematically identify and define the key
components of a prompt: symbols, patterns, and text. Then, we devise and
conduct an exhaustive set of experiments across four different tasks, by
querying the model with counterfactual prompts where only one of these
components is altered. Our experiments across three models (PaLM, GPT-3, and
CODEX) reveal several surprising findings and brings into question the
conventional wisdom around few-shot prompting. First, the presence of factual
patterns in a prompt is practically immaterial to the success of CoT. Second,
our results conclude that the primary role of intermediate steps may not be to
facilitate learning how to solve a task. The intermediate steps are rather a
beacon for the model to realize what symbols to replicate in the output to form
a factual answer. Further, text imbues patterns with commonsense knowledge and
meaning. Our empirical and qualitative analysis reveals that a symbiotic
relationship between text and patterns explains the success of few-shot
prompting: text helps extract commonsense from the question to help patterns,
and patterns enforce task understanding and direct text generation.",https://github.com/google-research/google-research/tree/master/l2da/learned2design,-1
RuCoLA: Russian Corpus of Linguistic Acceptability,0.456741,"Linguistic acceptability (LA) attracts the attention of the research
community due to its many uses, such as testing the grammatical knowledge of
language models and filtering implausible texts with acceptability classifiers.
However, the application scope of LA in languages other than English is limited
due to the lack of high-quality resources. To this end, we introduce the
Russian Corpus of Linguistic Acceptability (RuCoLA), built from the ground up
under the well-established binary LA approach. RuCoLA consists of $9.8$k
in-domain sentences from linguistic publications and $3.6$k out-of-domain
sentences produced by generative models. The out-of-domain set is created to
facilitate the practical use of acceptability for improving language
generation. Our paper describes the data collection protocol and presents a
fine-grained analysis of acceptability classification experiments with a range
of baseline approaches. In particular, we demonstrate that the most widely used
language models still fall behind humans by a large margin, especially when
detecting morphological and semantic errors. We release RuCoLA, the code of
experiments, and a public leaderboard (rucola-benchmark.com) to assess the
linguistic competence of language models for Russian.",https://github.com/RussianNLP/RuCoLA,-1
The Internet of Senses: Building on Semantic Communications and Edge Intelligence,0.127002,"The Internet of Senses (IoS) holds the promise of flawless telepresence-style
communication for all human `receptors' and therefore blurs the difference of
virtual and real environments. We commence by highlighting the compelling use
cases empowered by the IoS and also the key network requirements. We then
elaborate on how the emerging semantic communications and Artificial
Intelligence (AI)/Machine Learning (ML) paradigms along with 6G technologies
may satisfy the requirements of IoS use cases. On one hand, semantic
communications can be applied for extracting meaningful and significant
information and hence efficiently exploit the resources and for harnessing a
priori information at the receiver to satisfy IoS requirements. On the other
hand, AI/ML facilitates frugal network resource management by making use of the
enormous amount of data generated in IoS edge nodes and devices, as well as by
optimizing the IoS performance via intelligent agents. However, the intelligent
agents deployed at the edge are not completely aware of each others' decisions
and the environments of each other, hence they operate in a partially rather
than fully observable environment. Therefore, we present a case study of
Partially Observable Markov Decision Processes (POMDP) for improving the User
Equipment (UE) throughput and energy consumption, as they are imperative for
IoS use cases, using Reinforcement Learning for astutely activating and
deactivating the component carriers in carrier aggregation. Finally, we outline
the challenges and open issues of IoS implementations and employing semantic
communications, edge intelligence as well as learning under partial
observability in the IoS context.",None,-1
Hyperbolic Vision Transformers: Combining Improvements in Metric Learning,0.370638,"Metric learning aims to learn a highly discriminative model encouraging the
embeddings of similar classes to be close in the chosen metrics and pushed
apart for dissimilar ones. The common recipe is to use an encoder to extract
embeddings and a distance-based loss function to match the representations --
usually, the Euclidean distance is utilized. An emerging interest in learning
hyperbolic data embeddings suggests that hyperbolic geometry can be beneficial
for natural data. Following this line of work, we propose a new
hyperbolic-based model for metric learning. At the core of our method is a
vision transformer with output embeddings mapped to hyperbolic space. These
embeddings are directly optimized using modified pairwise cross-entropy loss.
We evaluate the proposed model with six different formulations on four datasets
achieving the new state-of-the-art performance. The source code is available at
https://github.com/htdt/hyp_metric.",https://github.com/htdt/hyp_metric,-1
RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks,0.095892,"Temporal/spatial receptive fields of models play an important role in
sequential/spatial tasks. Large receptive fields facilitate long-term
relations, while small receptive fields help to capture the local details.
Existing methods construct models with hand-designed receptive fields in
layers. Can we effectively search for receptive field combinations to replace
hand-designed patterns? To answer this question, we propose to find better
receptive field combinations through a global-to-local search scheme. Our
search scheme exploits both global search to find the coarse combinations and
local search to get the refined receptive field combinations further. The
global search finds possible coarse combinations other than human-designed
patterns. On top of the global search, we propose an expectation-guided
iterative local search scheme to refine combinations effectively. Our RF-Next
models, plugging receptive field search to various models, boost the
performance on many tasks, e.g., temporal action segmentation, object
detection, instance segmentation, and speech synthesis. The source code is
publicly available on http://mmcheng.net/rfnext.",http://mmcheng.net/rfnext,-1
Is a Modular Architecture Enough?,0.0992929,"Inspired from human cognition, machine learning systems are gradually
revealing advantages of sparser and more modular architectures. Recent work
demonstrates that not only do some modular architectures generalize well, but
they also lead to better out-of-distribution generalization, scaling
properties, learning speed, and interpretability. A key intuition behind the
success of such systems is that the data generating system for most real-world
settings is considered to consist of sparsely interacting parts, and endowing
models with similar inductive biases will be helpful. However, the field has
been lacking in a rigorous quantitative assessment of such systems because
these real-world data distributions are complex and unknown. In this work, we
provide a thorough assessment of common modular architectures, through the lens
of simple and known modular data distributions. We highlight the benefits of
modularity and sparsity and reveal insights on the challenges faced while
optimizing modular systems. In doing so, we propose evaluation metrics that
highlight the benefits of modularity, the regimes in which these benefits are
substantial, as well as the sub-optimality of current end-to-end learned
modular systems as opposed to their claimed potential.",https://github.com/sarthmit/Mod_Arch,-1
Bayes risk CTC: Controllable CTC alignment in Sequence-to-Sequence tasks,0.116065,"Sequence-to-Sequence (seq2seq) tasks transcribe the input sequence to a
target sequence. The Connectionist Temporal Classification (CTC) criterion is
widely used in multiple seq2seq tasks. Besides predicting the target sequence,
a side product of CTC is to predict the alignment, which is the most probable
input-long sequence that specifies a hard aligning relationship between the
input and target units. As there are multiple potential aligning sequences
(called paths) that are equally considered in CTC formulation, the choice of
which path will be most probable and become the predicted alignment is always
uncertain. In addition, it is usually observed that the alignment predicted by
vanilla CTC will drift compared with its reference and rarely provides
practical functionalities. Thus, the motivation of this work is to make the CTC
alignment prediction controllable and thus equip CTC with extra
functionalities. The Bayes risk CTC (BRCTC) criterion is then proposed in this
work, in which a customizable Bayes risk function is adopted to enforce the
desired characteristics of the predicted alignment. With the risk function, the
BRCTC is a general framework to adopt some customizable preference over the
paths in order to concentrate the posterior into a particular subset of the
paths. In applications, we explore one particular preference which yields
models with the down-sampling ability and reduced inference costs. By using
BRCTC with another preference for early emissions, we obtain an improved
performance-latency trade-off for online models. Experimentally, the proposed
BRCTC reduces the inference cost of offline models by up to 47% without
performance degradation and cuts down the overall latency of online systems to
an unseen level.",https://github.com/k2-fsa/k2,-1
Unbiased Heterogeneous Scene Graph Generation with Relation-aware Message Passing Neural Network,0.122307,"Recent scene graph generation (SGG) frameworks have focused on learning
complex relationships among multiple objects in an image. Thanks to the nature
of the message passing neural network (MPNN) that models high-order
interactions between objects and their neighboring objects, they are dominant
representation learning modules for SGG. However, existing MPNN-based
frameworks assume the scene graph as a homogeneous graph, which restricts the
context-awareness of visual relations between objects. That is, they overlook
the fact that the relations tend to be highly dependent on the objects with
which the relations are associated. In this paper, we propose an unbiased
heterogeneous scene graph generation (HetSGG) framework that captures
relation-aware context using message passing neural networks. We devise a novel
message passing layer, called relation-aware message passing neural network
(RMP), that aggregates the contextual information of an image considering the
predicate type between objects. Our extensive evaluations demonstrate that
HetSGG outperforms state-of-the-art methods, especially outperforming on tail
predicate classes.",https://github.com/KanghoonYoon/hetsgg-torch,-1
Label-enhanced Prototypical Network with Contrastive Learning for Multi-label Few-shot Aspect Category Detection,0.575907,"Multi-label aspect category detection allows a given review sentence to
contain multiple aspect categories, which is shown to be more practical in
sentiment analysis and attracting increasing attention. As annotating large
amounts of data is time-consuming and labor-intensive, data scarcity occurs
frequently in real-world scenarios, which motivates multi-label few-shot aspect
category detection. However, research on this problem is still in infancy and
few methods are available. In this paper, we propose a novel label-enhanced
prototypical network (LPN) for multi-label few-shot aspect category detection.
The highlights of LPN can be summarized as follows. First, it leverages label
description as auxiliary knowledge to learn more discriminative prototypes,
which can retain aspect-relevant information while eliminating the harmful
effect caused by irrelevant aspects. Second, it integrates with contrastive
learning, which encourages that the sentences with the same aspect label are
pulled together in embedding space while simultaneously pushing apart the
sentences with different aspect labels. In addition, it introduces an adaptive
multi-label inference module to predict the aspect count in the sentence, which
is simple yet effective. Extensive experimental results on three datasets
demonstrate that our proposed model LPN can consistently achieve
state-of-the-art performance.",None,-1
DuAT: Dual-Aggregation Transformer Network for Medical Image Segmentation,0.0635764,"Transformer-based models have been widely demonstrated to be successful in
computer vision tasks by modelling long-range dependencies and capturing global
representations. However, they are often dominated by features of large
patterns leading to the loss of local details (e.g., boundaries and small
objects), which are critical in medical image segmentation. To alleviate this
problem, we propose a Dual-Aggregation Transformer Network called DuAT, which
is characterized by two innovative designs, namely, the Global-to-Local Spatial
Aggregation (GLSA) and Selective Boundary Aggregation (SBA) modules. The GLSA
has the ability to aggregate and represent both global and local spatial
features, which are beneficial for locating large and small objects,
respectively. The SBA module is used to aggregate the boundary characteristic
from low-level features and semantic information from high-level features for
better preserving boundary details and locating the re-calibration objects.
Extensive experiments in six benchmark datasets demonstrate that our proposed
model outperforms state-of-the-art methods in the segmentation of skin lesion
images, and polyps in colonoscopy images. In addition, our approach is more
robust than existing methods in various challenging situations such as small
object segmentation and ambiguous object boundaries.",None,-1
HiSMatch: Historical Structure Matching based Temporal Knowledge Graph Reasoning,0.0558385,"A Temporal Knowledge Graph (TKG) is a sequence of KGs with respective
timestamps, which adopts quadruples in the form of (\emph{subject},
\emph{relation}, \emph{object}, \emph{timestamp}) to describe dynamic facts.
TKG reasoning has facilitated many real-world applications via answering such
queries as (\emph{query entity}, \emph{query relation}, \emph{?}, \emph{future
timestamp}) about future. This is actually a matching task between a query and
candidate entities based on their historical structures, which reflect
behavioral trends of the entities at different timestamps. In addition, recent
KGs provide background knowledge of all the entities, which is also helpful for
the matching. Thus, in this paper, we propose the \textbf{Hi}storical
\textbf{S}tructure \textbf{Match}ing (\textbf{HiSMatch}) model. It applies two
structure encoders to capture the semantic information contained in the
historical structures of the query and candidate entities. Besides, it adopts
another encoder to integrate the background knowledge into the model. TKG
reasoning experiments on six benchmark datasets demonstrate the significant
improvement of the proposed HiSMatch model, with up to 5.6\% performance
improvement in MRR, compared to the state-of-the-art baselines.",None,-1
DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization,0.837716,"Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve
state-of-the-art performance on many generative NLP tasks. However, such models
pose a great challenge in resource-constrained scenarios owing to their large
memory requirements and high latency. To alleviate this issue, we propose to
jointly distill and quantize the model, where knowledge is transferred from the
full-precision teacher model to the quantized and distilled low-precision
student model. Empirical analyses show that, despite the challenging nature of
generative tasks, we were able to achieve a 16.5x model footprint compression
ratio with little performance drop relative to the full-precision counterparts
on multiple summarization and QA datasets. We further pushed the limit of
compression ratio to 27.7x and presented the performance-efficiency trade-off
for generative tasks using pre-trained models. To the best of our knowledge,
this is the first work aiming to effectively distill and quantize
sequence-to-sequence pre-trained models for language generation tasks.",https://www.github.com/amazon-research/dq-bart/,-1
Robust Lottery Tickets for Pre-trained Language Models,0.203663,"Recent works on Lottery Ticket Hypothesis have shown that pre-trained
language models (PLMs) contain smaller matching subnetworks(winning tickets)
which are capable of reaching accuracy comparable to the original models.
However, these tickets are proved to be notrobust to adversarial examples, and
even worse than their PLM counterparts. To address this problem, we propose a
novel method based on learning binary weight masks to identify robust tickets
hidden in the original PLMs. Since the loss is not differentiable for the
binary mask, we assign the hard concrete distribution to the masks and
encourage their sparsity using a smoothing approximation of L0
regularization.Furthermore, we design an adversarial loss objective to guide
the search for robust tickets and ensure that the tickets perform well bothin
accuracy and robustness. Experimental results show the significant improvement
of the proposed method over previous work on adversarial robustness evaluation.",https://github.com/ruizheng20/robust_ticket,-1
Gradient-based Uncertainty for Monocular Depth Estimation,0.0,"In monocular depth estimation, disturbances in the image context, like moving
objects or reflecting materials, can easily lead to erroneous predictions. For
that reason, uncertainty estimates for each pixel are necessary, in particular
for safety-critical applications such as automated driving. We propose a post
hoc uncertainty estimation approach for an already trained and thus fixed depth
estimation model, represented by a deep neural network. The uncertainty is
estimated with the gradients which are extracted with an auxiliary loss
function. To avoid relying on ground-truth information for the loss definition,
we present an auxiliary loss function based on the correspondence of the depth
prediction for an image and its horizontally flipped counterpart. Our approach
achieves state-of-the-art uncertainty estimation results on the KITTI and NYU
Depth V2 benchmarks without the need to retrain the neural network. Models and
code are publicly available at https://github.com/jhornauer/GrUMoDepth.",https://github.com/jhornauer/GrUMoDepth,-1
On the Explainability of Natural Language Processing Deep Models,0.0347693,"While there has been a recent explosion of work on ExplainableAI ExAI on deep
models that operate on imagery and tabular data, textual datasets present new
challenges to the ExAI community. Such challenges can be attributed to the lack
of input structure in textual data, the use of word embeddings that add to the
opacity of the models and the difficulty of the visualization of the inner
workings of deep models when they are trained on textual data.
  Lately, methods have been developed to address the aforementioned challenges
and present satisfactory explanations on Natural Language Processing (NLP)
models. However, such methods are yet to be studied in a comprehensive
framework where common challenges are properly stated and rigorous evaluation
practices and metrics are proposed. Motivated to democratize ExAI methods in
the NLP field, we present in this work a survey that studies model-agnostic as
well as model-specific explainability methods on NLP models. Such methods can
either develop inherently interpretable NLP models or operate on pre-trained
models in a post-hoc manner. We make this distinction and we further decompose
the methods into three categories according to what they explain: (1) word
embeddings (input-level), (2) inner workings of NLP models (processing-level)
and (3) models' decisions (output-level). We also detail the different
evaluation approaches interpretability methods in the NLP field. Finally, we
present a case-study on the well-known neural machine translation in an
appendix and we propose promising future research directions for ExAI in the
NLP field.",None,-1
Spiking Neural Networks for Frame-based and Event-based Single Object Localization,0.104314,"Spiking neural networks have shown much promise as an energy-efficient
alternative to artificial neural networks. However, understanding the impacts
of sensor noises and input encodings on the network activity and performance
remains difficult with common neuromorphic vision baselines like
classification. Therefore, we propose a spiking neural network approach for
single object localization trained using surrogate gradient descent, for frame-
and event-based sensors. We compare our method with similar artificial neural
networks and show that our model has competitive/better performance in
accuracy, robustness against various corruptions, and has lower energy
consumption. Moreover, we study the impact of neural coding schemes for static
images in accuracy, robustness, and energy efficiency. Our observations differ
importantly from previous studies on bio-plausible learning rules, which helps
in the design of surrogate gradient trained architectures, and offers insight
to design priorities in future neuromorphic technologies in terms of noise
characteristics and data encoding methods.",https://github.com/hendrycks/robustness,-1
Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization,0.115058,"Neural architecture search (NAS) and network pruning are widely studied
efficient AI techniques, but not yet perfect. NAS performs exhaustive candidate
architecture search, incurring tremendous search cost. Though (structured)
pruning can simply shrink model dimension, it remains unclear how to decide the
per-layer sparsity automatically and optimally. In this work, we revisit the
problem of layer-width optimization and propose Pruning-as-Search (PaS), an
end-to-end channel pruning method to search out desired sub-network
automatically and efficiently. Specifically, we add a depth-wise binary
convolution to learn pruning policies directly through gradient descent. By
combining the structural reparameterization and PaS, we successfully searched
out a new family of VGG-like and lightweight networks, which enable the
flexibility of arbitrary width with respect to each layer instead of each
stage. Experimental results show that our proposed architecture outperforms
prior arts by around $1.0\%$ top-1 accuracy under similar inference speed on
ImageNet-1000 classification task. Furthermore, we demonstrate the
effectiveness of our width search on complex tasks including instance
segmentation and image translation. Code and models are released.",None,-1
LingYi: Medical Conversational Question Answering System based on Multi-modal Knowledge Graphs,0.221048,"The medical conversational system can relieve the burden of doctors and
improve the efficiency of healthcare, especially during the pandemic. This
paper presents a medical conversational question answering (CQA) system based
on the multi-modal knowledge graph, namely ""LingYi"", which is designed as a
pipeline framework to maintain high flexibility. Our system utilizes automated
medical procedures including medical triage, consultation, image-text drug
recommendation and record. To conduct knowledge-grounded dialogues with
patients, we first construct a Chinese Medical Multi-Modal Knowledge Graph
(CM3KG) and collect a large-scale Chinese Medical CQA (CMCQA) dataset. Compared
with the other existing medical question-answering systems, our system adopts
several state-of-the-art technologies including medical entity disambiguation
and medical dialogue generation, which is more friendly to provide medical
services to patients. In addition, we have open-sourced our codes which contain
back-end models and front-end web pages at https://github.com/WENGSYX/LingYi.
The datasets including CM3KG at https://github.com/WENGSYX/CM3KG and CMCQA at
https://github.com/WENGSYX/CMCQA are also released to further promote future
research.",https://github.com/WENGSYX/LingYi,-1
One-Shot Adaptation of GAN in Just One CLIP,0.12596,"There are many recent research efforts to fine-tune a pre-trained generator
with a few target images to generate images of a novel domain. Unfortunately,
these methods often suffer from overfitting or under-fitting when fine-tuned
with a single target image. To address this, here we present a novel
single-shot GAN adaptation method through unified CLIP space manipulations.
Specifically, our model employs a two-step training strategy: reference image
search in the source generator using a CLIP-guided latent optimization,
followed by generator fine-tuning with a novel loss function that imposes CLIP
space consistency between the source and adapted generators. To further improve
the adapted model to produce spatially consistent samples with respect to the
source generator, we also propose contrastive regularization for patchwise
relationships in the CLIP space. Experimental results show that our model
generates diverse outputs with the target texture and outperforms the baseline
models both qualitatively and quantitatively. Furthermore, we show that our
CLIP space manipulation strategy allows more effective attribute editing.",https://github.com/cyclomon/OneshotCLIP,-1
Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET,0.718922,"Neural metrics have achieved impressive correlation with human judgements in
the evaluation of machine translation systems, but before we can safely
optimise towards such metrics, we should be aware of (and ideally eliminate)
biases toward bad translations that receive high scores. Our experiments show
that sample-based Minimum Bayes Risk decoding can be used to explore and
quantify such weaknesses. When applying this strategy to COMET for en-de and
de-en, we find that COMET models are not sensitive enough to discrepancies in
numbers and named entities. We further show that these biases are hard to fully
remove by simply training on additional synthetic data and release our code and
data for facilitating further experiments.",https://github.com/Unbabel/COMET,-1
Semi-Supervised Formality Style Transfer with Consistency Training,0.115217,"Formality style transfer (FST) is a task that involves paraphrasing an
informal sentence into a formal one without altering its meaning. To address
the data-scarcity problem of existing parallel datasets, previous studies tend
to adopt a cycle-reconstruction scheme to utilize additional unlabeled data,
where the FST model mainly benefits from target-side unlabeled sentences. In
this work, we propose a simple yet effective semi-supervised framework to
better utilize source-side unlabeled sentences based on consistency training.
Specifically, our approach augments pseudo-parallel data obtained from a
source-side informal sentence by enforcing the model to generate similar
outputs for its perturbed version. Moreover, we empirically examined the
effects of various data perturbation methods and propose effective data
filtering strategies to improve our framework. Experimental results on the
GYAFC benchmark demonstrate that our approach can achieve state-of-the-art
results, even with less than 40% of the parallel data.",https://github.com/Aolius/semi-fst,-1
Using Ontologies for the Formalization and Recognition of Criticality for Automated Driving,0.0,"Knowledge representation and reasoning has a long history of examining how
knowledge can be formalized, interpreted, and semantically analyzed by
machines. In the area of automated vehicles, recent advances suggest the
ability to formalize and leverage relevant knowledge as a key enabler in
handling the inherently open and complex context of the traffic world. This
paper demonstrates ontologies to be a powerful tool for a) modeling and
formalization of and b) reasoning about factors associated with criticality in
the environment of automated vehicles. For this, we leverage the well-known
6-Layer Model to create a formal representation of the environmental context.
Within this representation, an ontology models domain knowledge as logical
axioms, enabling deduction on the presence of critical factors within traffic
scenes and scenarios. For executing automated analyses, a joint description
logic and rule reasoner is used in combination with an a-priori predicate
augmentation. We elaborate on the modular approach, present a publicly
available implementation, and evaluate the method by means of a large-scale
drone data set of urban traffic scenarios.",None,-1
SATr: Slice Attention with Transformer for Universal Lesion Detection,0.165258,"Universal Lesion Detection (ULD) in computed tomography plays an essential
role in computer-aided diagnosis. Promising ULD results have been reported by
multi-slice-input detection approaches which model 3D context from multiple
adjacent CT slices, but such methods still experience difficulty in obtaining a
global representation among different slices and within each individual slice
since they only use convolution-based fusion operations. In this paper, we
propose a novel Slice Attention Transformer (SATr) block which can be easily
plugged into convolution-based ULD backbones to form hybrid network structures.
Such newly formed hybrid backbones can better model long-distance feature
dependency via the cascaded self-attention modules in the Transformer block
while still holding a strong power of modeling local features with the
convolutional operations in the original backbone. Experiments with five
state-of-the-art methods show that the proposed SATr block can provide an
almost free boost to lesion detection accuracy without extra hyperparameters or
special network designs.",https://github.com/jacobgil/pytorch-grad-cam,-1
Space-based gravitational wave signal detection and extraction with deep neural network,0.120935,"Space-based gravitational wave (GW) detectors will be able to observe signals
from sources that are otherwise nearly impossible from current ground-based
detection. Consequently, the well established signal detection method, matched
filtering, will require a complex template bank, leading to a computational
cost that is too expensive in practice. Here, we develop a high-accuracy GW
signal detection and extraction method for all space-based GW sources. As a
proof of concept, we show that a science-driven and uniform multi-stage
self-attention-based deep neural network can identify synthetic signals that
are submerged in Gaussian noise. Our method exhibits a detection rate exceeding
99% in identifying signals from various sources, with the signal-to-noise ratio
at 50, at a false alarm rate of 1%. while obtaining at least 95% similarity
compared with target signals. We further demonstrate the interpretability and
strong generalization behavior for several extended scenarios.",https://github.com/AI-HPC-Research-Team/space_signal_detection_1,-1
Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text,0.0643997,"As text generated by large language models proliferates, it becomes vital to
understand how humans engage with such text, and whether or not they are able
to detect when the text they are reading did not originate with a human writer.
Prior work on human detection of generated text focuses on the case where an
entire passage is either human-written or machine-generated. In this paper, we
study a more realistic setting where text begins as human-written and
transitions to being generated by state-of-the-art neural language models. We
show that, while annotators often struggle at this task, there is substantial
variance in annotator skill and that given proper incentives, annotators can
improve at this task over time. Furthermore, we conduct a detailed comparison
study and analyze how a variety of variables (model size, decoding strategy,
fine-tuning, prompt genre, etc.) affect human detection performance. Finally,
we collect error annotations from our participants and use them to show that
certain textual genres influence models to make different types of errors and
that certain sentence-level features correlate highly with annotator selection.
We release the RoFT dataset: a collection of over 21,000 human annotations
paired with error classifications to encourage future work in human detection
and evaluation of generated text.",https://github.com/liamdugan/human-detection,-1
Attribute-based Representations for Accurate and Interpretable Video Anomaly Detection,0.178238,"Video anomaly detection (VAD) is a challenging computer vision task with many
practical applications. As anomalies are inherently ambiguous, it is essential
for users to understand the reasoning behind a system's decision in order to
determine if the rationale is sound. In this paper, we propose a simple but
highly effective method that pushes the boundaries of VAD accuracy and
interpretability using attribute-based representations. Our method represents
every object by its velocity and pose. The anomaly scores are computed using a
density-based approach. Surprisingly, we find that this simple representation
is sufficient to achieve state-of-the-art performance in ShanghaiTech, the
largest and most complex VAD dataset. Combining our interpretable
attribute-based representations with implicit, deep representation yields
state-of-the-art performance with a $99.1\%, 93.3\%$, and $85.9\%$ AUROC on
Ped2, Avenue, and ShanghaiTech, respectively. Our method is accurate,
interpretable, and easy to implement.",https://github.com/talreiss/Accurate-Interpretable-VAD,-1
EA$^2$E: Improving Consistency with Event Awareness for Document-Level Argument Extraction,0.165339,"Events are inter-related in documents. Motivated by the
one-sense-per-discourse theory, we hypothesize that a participant tends to play
consistent roles across multiple events in the same document. However recent
work on document-level event argument extraction models each individual event
in isolation and therefore causes inconsistency among extracted arguments
across events, which will further cause discrepancy for downstream applications
such as event knowledge base population, question answering, and hypothesis
generation. In this work, we formulate event argument consistency as the
constraints from event-event relations under the document-level setting. To
improve consistency we introduce the Event-Aware Argument Extraction (EA$^2$E)
model with augmented context for training and inference. Experiment results on
WIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA$^2$E
compared to baseline methods.",https://github.com/ZQS1943/DOCIE,-1
Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2021,0.245648,"Automatic detection of fake news is a highly important task in the
contemporary world. This study reports the 2nd shared task called
UrduFake@FIRE2021 on identifying fake news detection in Urdu. The goal of the
shared task is to motivate the community to come up with efficient methods for
solving this vital problem, particularly for the Urdu language. The task is
posed as a binary classification problem to label a given news article as a
real or a fake news article. The organizers provide a dataset comprising news
in five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and
(v) Business, split into training and testing sets. The training set contains
1300 annotated news articles -- 750 real news, 550 fake news, while the testing
set contains 300 news articles -- 200 real, 100 fake news. 34 teams from 7
different countries (China, Egypt, Israel, India, Mexico, Pakistan, and UAE)
registered to participate in the UrduFake@FIRE2021 shared task. Out of those,
18 teams submitted their experimental results, and 11 of those submitted their
technical reports, which is substantially higher compared to the UrduFake
shared task in 2020 when only 6 teams submitted their technical reports. The
technical reports submitted by the participants demonstrated different data
representation techniques ranging from count-based BoW features to word vector
embeddings as well as the use of numerous machine learning algorithms ranging
from traditional SVM to various neural network architectures including
Transformers such as BERT and RoBERTa. In this year's competition, the best
performing system obtained an F1-macro score of 0.679, which is lower than the
past year's best result of 0.907 F1-macro. Admittedly, while training sets from
the past and the current years overlap to a large extent, the testing set
provided this year is completely different.",https://github.com/MaazAmjad/Urdu-Fake-news-detection-FIRE2021,-1
Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning,0.0985062,"Novel category discovery aims at adapting models trained on known categories
to novel categories. Previous works only focus on the scenario where known and
novel categories are of the same granularity. In this paper, we investigate a
new practical scenario called Fine-grained Category Discovery under
Coarse-grained supervision (FCDC). FCDC aims at discovering fine-grained
categories with only coarse-grained labeled data, which can adapt models to
categories of different granularity from known ones and reduce significant
labeling cost. It is also a challenging task since supervised training on
coarse-grained categories tends to focus on inter-class distance (distance
between coarse-grained classes) but ignore intra-class distance (distance
between fine-grained sub-classes) which is essential for separating
fine-grained categories. Considering most current methods cannot transfer
knowledge from coarse-grained level to fine-grained level, we propose a
hierarchical weighted self-contrastive network by building a novel weighted
self-contrastive module and combining it with supervised learning in a
hierarchical manner. Extensive experiments on public datasets show both
effectiveness and efficiency of our model over compared methods. Code and data
are available at https://github.com/Lackel/Hierarchical_Weighted_SCL.",https://github.com/Lackel/Hierarchical_Weighted_SCL,-1
Quantitative AI Risk Assessments: Opportunities and Challenges,0.371023,"Although AI-based systems are increasingly being leveraged to provide value
to organizations, individuals, and society, significant attendant risks have
been identified. These risks have led to proposed regulations, litigation, and
general societal concerns.
  As with any promising technology, organizations want to benefit from the
positive capabilities of AI technology while reducing the risks. The best way
to reduce risks is to implement comprehensive AI lifecycle governance where
policies and procedures are described and enforced during the design,
development, deployment, and monitoring of an AI system. While support for
comprehensive governance is beginning to emerge, organizations often need to
identify the risks of deploying an already-built model without knowledge of how
it was constructed or access to its original developers.
  Such an assessment will quantitatively assess the risks of an existing model
in a manner analogous to how a home inspector might assess the energy
efficiency of an already-built home or a physician might assess overall patient
health based on a battery of tests. This paper explores the concept of a
quantitative AI Risk Assessment, exploring the opportunities, challenges, and
potential impacts of such an approach, and discussing how it might improve AI
regulations.",None,-1
Sar Ship Detection based on Swin Transformer and Feature Enhancement Feature Pyramid Network,0.291995,"With the booming of Convolutional Neural Networks (CNNs), CNNs such as VGG-16
and ResNet-50 widely serve as backbone in SAR ship detection. However, CNN
based backbone is hard to model long-range dependencies, and causes the lack of
enough high-quality semantic information in feature maps of shallow layers,
which leads to poor detection performance in complicated background and
small-sized ships cases. To address these problems, we propose a SAR ship
detection method based on Swin Transformer and Feature Enhancement Feature
Pyramid Network (FEFPN). Swin Transformer serves as backbone to model
long-range dependencies and generates hierarchical features maps. FEFPN is
proposed to further improve the quality of feature maps by gradually enhancing
the semantic information of feature maps at all levels, especially feature maps
in shallow layers. Experiments conducted on SAR ship detection dataset (SSDD)
reveal the advantage of our proposed methods.",None,-1
PSP-HDRI$+$: A Synthetic Dataset Generator for Pre-Training of Human-Centric Computer Vision Models,0.265571,"We introduce a new synthetic data generator PSP-HDRI$+$ that proves to be a
superior pre-training alternative to ImageNet and other large-scale synthetic
data counterparts. We demonstrate that pre-training with our synthetic data
will yield a more general model that performs better than alternatives even
when tested on out-of-distribution (OOD) sets. Furthermore, using ablation
studies guided by person keypoint estimation metrics with an off-the-shelf
model architecture, we show how to manipulate our synthetic data generator to
further improve model performance.",https://github.com/Unity-Technologies/PeopleSansPeople,-1
OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction,0.322264,"Learning how humans manipulate objects requires machines to acquire knowledge
from two perspectives: one for understanding object affordances and the other
for learning human's interactions based on the affordances. Even though these
two knowledge bases are crucial, we find that current databases lack a
comprehensive awareness of them. In this work, we propose a multi-modal and
rich-annotated knowledge repository, OakInk, for visual and cognitive
understanding of hand-object interactions. We start to collect 1,800 common
household objects and annotate their affordances to construct the first
knowledge base: Oak. Given the affordance, we record rich human interactions
with 100 selected objects in Oak. Finally, we transfer the interactions on the
100 recorded objects to their virtual counterparts through a novel method:
Tink. The recorded and transferred hand-object interactions constitute the
second knowledge base: Ink. As a result, OakInk contains 50,000 distinct
affordance-aware and intent-oriented hand-object interactions. We benchmark
OakInk on pose estimation and grasp generation tasks. Moreover, we propose two
practical applications of OakInk: intent-based interaction generation and
handover generation. Our datasets and source code are publicly available at
https://github.com/lixiny/OakInk.",https://github.com/lixiny/OakInk,-1
Communication Beyond Transmitting Bits: Semantics-Guided Source and Channel Coding,0.557329,"Classical communication paradigms focus on accurately transmitting bits over
a noisy channel, and Shannon theory provides a fundamental theoretical limit on
the rate of reliable communications. In this approach, bits are treated
equally, and the communication system is oblivious to what meaning these bits
convey or how they would be used. Future communications towards intelligence
and conciseness will predictably play a dominant role, and the proliferation of
connected intelligent agents requires a radical rethinking of coded
transmission paradigm to support the new communication morphology on the
horizon. The recent concept of ""semantic communications"" offers a promising
research direction. Injecting semantic guidance into the coded transmission
design to achieve semantics-aware communications shows great potential for
further breakthrough in effectiveness and reliability. This article sheds light
on semantics-guided source and channel coding as a transmission paradigm of
semantic communications, which exploits both data semantics diversity and
wireless channel diversity together to boost the whole system performance. We
present the general system architecture and key techniques, and indicate some
open issues on this topic.",None,-1
NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,0.335746,"Given the ubiquitous nature of numbers in text, reasoning with numbers to
perform simple calculations is an important skill of AI systems. While many
datasets and models have been developed to this end, state-of-the-art AI
systems are brittle; failing to perform the underlying mathematical reasoning
when they appear in a slightly different scenario. Drawing inspiration from
GLUE that was proposed in the context of natural language understanding, we
propose NumGLUE, a multi-task benchmark that evaluates the performance of AI
systems on eight different tasks, that at their core require simple arithmetic
understanding. We show that this benchmark is far from being solved with neural
models including state-of-the-art large-scale language models performing
significantly worse than humans (lower by 46.4%). Further, NumGLUE promotes
sharing knowledge across tasks, especially those with limited training data as
evidenced by the superior performance (average gain of 3.4% on each task) when
a model is jointly trained on all the tasks as opposed to task-specific
modeling. Finally, we hope that NumGLUE will encourage systems that perform
robust and general arithmetic reasoning within language, a first step towards
being able to perform more complex mathematical reasoning.",None,-1
Semantic properties of English nominal pluralization: Insights from word embeddings,0.120653,"Semantic differentiation of nominal pluralization is grammaticalized in many
languages. For example, plural markers may only be relevant for human nouns.
English does not appear to make such distinctions. Using distributional
semantics, we show that English nominal pluralization exhibits semantic
clusters. For instance, pluralization of fruit words is more similar to one
another and less similar to pluralization of other semantic classes. Therefore,
reduction of the meaning shift in plural formation to the addition of an
abstract plural meaning is too simplistic. A semantically informed method,
called CosClassAvg, is introduced that outperforms pluralization methods in
distributional semantics which assume plural formation amounts to the addition
of a fixed plural vector. In comparison with our approach, a method from
compositional distributional semantics, called FRACSS, predicted plural vectors
that were more similar to the corpus-extracted plural vectors in terms of
direction but not vector length. A modeling study reveals that the observed
difference between the two predicted semantic spaces by CosClassAvg and FRACSS
carries over to how well a computational model of the listener can understand
previously unencountered plural forms. Mappings from word forms, represented
with triphone vectors, to predicted semantic vectors are more productive when
CosClassAvg-generated semantic vectors are employed as gold standard vectors
instead of FRACSS-generated vectors.",None,-1
Zero-Shot On-the-Fly Event Schema Induction,0.18885,"What are the events involved in a pandemic outbreak? What steps should be
taken when planning a wedding? The answers to these questions can be found by
collecting many documents on the complex event of interest, extracting relevant
information, and analyzing it. We present a new approach in which large
language models are utilized to generate source documents that allow
predicting, given a high-level event definition, the specific events,
arguments, and relations between them to construct a schema that describes the
complex event in its entirety. Using our model, complete schemas on any topic
can be generated on-the-fly without any manual data collection, i.e., in a
zero-shot manner. Moreover, we develop efficient methods to extract pertinent
information from texts and demonstrate in a series of experiments that these
schemas are considered to be more complete than human-curated ones in the
majority of examined scenarios. Finally, we show that this framework is
comparable in performance with previous supervised schema induction methods
that rely on collecting real texts while being more general and flexible
without the need for a predefined ontology.",None,-1
Memory-free Online Change-point Detection: A Novel Neural Network Approach,0.0794887,"Change-point detection (CPD), which detects abrupt changes in the data
distribution, is recognized as one of the most significant tasks in time series
analysis. Despite the extensive literature on offline CPD, unsupervised online
CPD still suffers from major challenges, including scalability, hyperparameter
tuning, and learning constraints. To mitigate some of these challenges, in this
paper, we propose a novel deep learning approach for unsupervised online CPD
from multi-dimensional time series, named Adaptive LSTM-Autoencoder
Change-Point Detection (ALACPD). ALACPD exploits an LSTM-autoencoder-based
neural network to perform unsupervised online CPD. It continuously adapts to
the incoming samples without keeping the previously received input, thus being
memory-free. We perform an extensive evaluation on several real-world time
series CPD benchmarks. We show that ALACPD, on average, ranks first among
state-of-the-art CPD algorithms in terms of quality of the time series
segmentation, and it is on par with the best performer in terms of the accuracy
of the estimated change-points. The implementation of ALACPD is available
online on Github\footnote{\url{https://github.com/zahraatashgahi/ALACPD}}.",https://github.com/zahraatashgahi/ALACPD,-1
NICGSlowDown: Evaluating the Efficiency Robustness of Neural Image Caption Generation Models,0.0778631,"Neural image caption generation (NICG) models have received massive attention
from the research community due to their excellent performance in visual
understanding. Existing work focuses on improving NICG model accuracy while
efficiency is less explored. However, many real-world applications require
real-time feedback, which highly relies on the efficiency of NICG models.
Recent research observed that the efficiency of NICG models could vary for
different inputs. This observation brings in a new attack surface of NICG
models, i.e., An adversary might be able to slightly change inputs to cause the
NICG models to consume more computational resources. To further understand such
efficiency-oriented threats, we propose a new attack approach, NICGSlowDown, to
evaluate the efficiency robustness of NICG models. Our experimental results
show that NICGSlowDown can generate images with human-unnoticeable
perturbations that will increase the NICG model latency up to 483.86%. We hope
this research could raise the community's concern about the efficiency
robustness of NICG models.",https://github.com/NICGSlowDown,-1
MAESTRO: Matched Speech Text Representations through Modality Matching,0.99355,"We present Maestro, a self-supervised training method to unify
representations learnt from speech and text modalities. Self-supervised
learning from speech signals aims to learn the latent structure inherent in the
signal, while self-supervised learning from text attempts to capture lexical
information. Learning aligned representations from unpaired speech and text
sequences is a challenging task. Previous work either implicitly enforced the
representations learnt from these two modalities to be aligned in the latent
space through multitasking and parameter sharing or explicitly through
conversion of modalities via speech synthesis. While the former suffers from
interference between the two modalities, the latter introduces additional
complexity. In this paper, we propose Maestro, a novel algorithm to learn
unified representations from both these modalities simultaneously that can
transfer to diverse downstream tasks such as Automated Speech Recognition (ASR)
and Speech Translation (ST). Maestro learns unified representations through
sequence alignment, duration prediction and matching embeddings in the learned
space through an aligned masked-language model loss. We establish a new
state-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 8% relative
reduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)
and 21 languages to English multilingual ST on CoVoST 2 with an improvement of
2.8 BLEU averaged over 21 languages.",None,-1
Improving Rare Word Recognition with LM-aware MWER Training,0.326143,"Language models (LMs) significantly improve the recognition accuracy of
end-to-end (E2E) models on words rarely seen during training, when used in
either the shallow fusion or the rescoring setups. In this work, we introduce
LMs in the learning of hybrid autoregressive transducer (HAT) models in the
discriminative training framework, to mitigate the training versus inference
gap regarding the use of LMs. For the shallow fusion setup, we use LMs during
both hypotheses generation and loss computation, and the LM-aware MWER-trained
model achieves 10\% relative improvement over the model trained with standard
MWER on voice search test sets containing rare words. For the rescoring setup,
we learn a small neural module to generate per-token fusion weights in a
data-dependent manner. This model achieves the same rescoring WER as regular
MWER-trained model, but without the need for sweeping fusion weights.",None,-1
Provable Defense against Backdoor Policies in Reinforcement Learning,0.176077,"We propose a provable defense mechanism against backdoor policies in
reinforcement learning under subspace trigger assumption. A backdoor policy is
a security threat where an adversary publishes a seemingly well-behaved policy
which in fact allows hidden triggers. During deployment, the adversary can
modify observed states in a particular way to trigger unexpected actions and
harm the agent. We assume the agent does not have the resources to re-train a
good policy. Instead, our defense mechanism sanitizes the backdoor policy by
projecting observed states to a 'safe subspace', estimated from a small number
of interactions with a clean (non-triggered) environment. Our sanitized policy
achieves $\epsilon$ approximate optimality in the presence of triggers,
provided the number of clean interactions is $O\left(\frac{D}{(1-\gamma)^4
\epsilon^2}\right)$ where $\gamma$ is the discounting factor and $D$ is the
dimension of state space. Empirically, we show that our sanitization defense
performs well on two Atari game environments.",https://github.com/skbharti/Provable-Defense-in-RL,-1
MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering,0.468576,"Visual language data such as plots, charts, and infographics are ubiquitous
in the human world. However, state-of-the-art vision-language models do not
perform well on these data. We propose MatCha (Math reasoning and Chart
derendering pretraining) to enhance visual language models' capabilities in
jointly modeling charts/plots and language data. Specifically, we propose
several pretraining tasks that cover plot deconstruction and numerical
reasoning which are the key capabilities in visual language modeling.
  We perform the MatCha pretraining starting from Pix2Struct, a recently
proposed image-to-text visual language model. On standard benchmarks such as
PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as
much as nearly 20%. We also examine how well MatCha pretraining transfers to
domains such as screenshots, textbook diagrams, and document figures and
observe overall improvement, verifying the usefulness of MatCha pretraining on
broader visual language tasks.",https://github.com/google-research/google-research/tree/master/deplot,-1
Improving End-to-End Contextual Speech Recognition with Fine-Grained Contextual Knowledge Selection,0.909282,"Nowadays, most methods in end-to-end contextual speech recognition bias the
recognition process towards contextual knowledge. Since all-neural contextual
biasing methods rely on phrase-level contextual modeling and attention-based
relevance modeling, they may encounter confusion between similar
context-specific phrases, which hurts predictions at the token level. In this
work, we focus on mitigating confusion problems with fine-grained contextual
knowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge
to reduce the uncertainty of token predictions. Specifically, we first apply
phrase selection to narrow the range of phrase candidates, and then conduct
token attention on the tokens in the selected phrase candidates. Moreover, we
re-normalize the attention weights of most relevant phrases in inference to
obtain more focused phrase-level contextual representations, and inject
position information to better discriminate phrases or tokens. On LibriSpeech
and an in-house 160,000-hour dataset, we explore the proposed methods based on
a controllable all-neural biasing method, collaborative decoding (ColDec). The
proposed methods provide at most 6.1% relative word error rate reduction on
LibriSpeech and 16.4% relative character error rate reduction on the in-house
dataset over ColDec.",https://github.com/MingLunHan/CIF-ColDec,-1
Deformable Butterfly: A Highly Structured and Sparse Linear Transform,0.093142,"We introduce a new kind of linear transform named Deformable Butterfly
(DeBut) that generalizes the conventional butterfly matrices and can be adapted
to various input-output dimensions. It inherits the fine-to-coarse-grained
learnable hierarchy of traditional butterflies and when deployed to neural
networks, the prominent structures and sparsity in a DeBut layer constitutes a
new way for network compression. We apply DeBut as a drop-in replacement of
standard fully connected and convolutional layers, and demonstrate its
superiority in homogenizing a neural network and rendering it favorable
properties such as light weight and low inference complexity, without
compromising accuracy. The natural complexity-accuracy tradeoff arising from
the myriad deformations of a DeBut layer also opens up new rooms for analytical
and practical research. The codes and Appendix are publicly available at:
https://github.com/ruilin0212/DeBut.",https://github.com/ruilin0212/DeBut,-1
Acquiring and Modelling Abstract Commonsense Knowledge via Conceptualization,0.0358678,"Conceptualization, or viewing entities and situations as instances of
abstract concepts in mind and making inferences based on that, is a vital
component in human intelligence for commonsense reasoning. Although recent
artificial intelligence has made progress in acquiring and modelling
commonsense, attributed to large neural language models and commonsense
knowledge graphs (CKGs), conceptualization is yet to thoroughly be introduced,
making current approaches ineffective to cover knowledge about countless
diverse entities and situations in the real world. To address the problem, we
thoroughly study the possible role of conceptualization in commonsense
reasoning, and formulate a framework to replicate human conceptual induction
from acquiring abstract knowledge about abstract concepts. Aided by the
taxonomy Probase, we develop tools for contextualized conceptualization on
ATOMIC, a large-scale human annotated CKG. We annotate a dataset for the
validity of conceptualizations for ATOMIC on both event and triple level,
develop a series of heuristic rules based on linguistic features, and train a
set of neural models, so as to generate and verify abstract knowledge. Based on
these components, a pipeline to acquire abstract knowledge is built. A large
abstract CKG upon ATOMIC is then induced, ready to be instantiated to infer
about unseen entities or situations. Furthermore, experiments find directly
augmenting data with abstract triples to be helpful in commonsense modelling.",https://github.com/HKUST-KnowComp/atomic-conceptualization,-1
Holistic Interaction Transformer Network for Action Detection,0.262206,"Actions are about how we interact with the environment, including other
people, objects, and ourselves. In this paper, we propose a novel multi-modal
Holistic Interaction Transformer Network (HIT) that leverages the largely
ignored, but critical hand and pose information essential to most human
actions. The proposed ""HIT"" network is a comprehensive bi-modal framework that
comprises an RGB stream and a pose stream. Each of them separately models
person, object, and hand interactions. Within each sub-network, an
Intra-Modality Aggregation module (IMA) is introduced that selectively merges
individual interaction units. The resulting features from each modality are
then glued using an Attentive Fusion Mechanism (AFM). Finally, we extract cues
from the temporal context to better classify the occurring actions using cached
memory. Our method significantly outperforms previous approaches on the J-HMDB,
UCF101-24, and MultiSports datasets. We also achieve competitive results on
AVA. The code will be available at https://github.com/joslefaure/HIT.",https://github.com/joslefaure/HIT,-1
Combining Lipschitz and RBF Surrogate Models for High-dimensional Computationally Expensive Problems,0.116432,"Standard evolutionary optimization algorithms assume that the evaluation of
the objective and constraint functions is straightforward and computationally
cheap. However, in many real-world optimization problems, these evaluations
involve computationally expensive numerical simulations or physical
experiments. Surrogate-assisted evolutionary algorithms (SAEAs) have recently
gained increased attention for their performance in solving these types of
problems. The main idea of SAEAs is the integration of an evolutionary
algorithm with a selected surrogate model that approximates the computationally
expensive function. In this paper, we propose a surrogate model based on a
Lipschitz underestimation and use it to develop a differential evolution-based
algorithm. The algorithm, called Lipschitz Surrogate-assisted Differential
Evolution (LSADE), utilizes the Lipschitz-based surrogate model, along with a
standard radial basis function surrogate model and a local search procedure.
The experimental results on seven benchmark functions of dimensions 30, 50,
100, and 200 show that the proposed LSADE algorithm is competitive compared
with the state-of-the-art algorithms under a limited computational budget,
being especially effective for the very complicated benchmark functions in high
dimensions.",https://github.com/JakubKudela89/LSADE,-1
Mixing Up Contrastive Learning: Self-Supervised Representation Learning for Time Series,0.113482,"The lack of labeled data is a key challenge for learning useful
representation from time series data. However, an unsupervised representation
framework that is capable of producing high quality representations could be of
great value. It is key to enabling transfer learning, which is especially
beneficial for medical applications, where there is an abundance of data but
labeling is costly and time consuming. We propose an unsupervised contrastive
learning framework that is motivated from the perspective of label smoothing.
The proposed approach uses a novel contrastive loss that naturally exploits a
data augmentation scheme in which new samples are generated by mixing two data
samples with a mixing component. The task in the proposed framework is to
predict the mixing component, which is utilized as soft targets in the loss
function. Experiments demonstrate the framework's superior performance compared
to other representation learning approaches on both univariate and multivariate
time series and illustrate its benefits for transfer learning for clinical time
series.",None,-1
Detecting Methane Plumes using PRISMA: Deep Learning Model and Data Augmentation,0.367364,"The new generation of hyperspectral imagers, such as PRISMA, has improved
significantly our detection capability of methane (CH4) plumes from space at
high spatial resolution (30m). We present here a complete framework to identify
CH4 plumes using images from the PRISMA satellite mission and a deep learning
model able to detect plumes over large areas. To compensate for the relative
scarcity of PRISMA images, we trained our model by transposing high resolution
plumes from Sentinel-2 to PRISMA. Our methodology thus avoids computationally
expensive synthetic plume generation from Large Eddy Simulations by generating
a broad and realistic training database, and paves the way for large-scale
detection of methane plumes using future hyperspectral sensors (EnMAP, EMIT,
CarbonMapper).",None,-1
Semi-Supervised Single-View 3D Reconstruction via Prototype Shape Priors,0.116221,"The performance of existing single-view 3D reconstruction methods heavily
relies on large-scale 3D annotations. However, such annotations are tedious and
expensive to collect. Semi-supervised learning serves as an alternative way to
mitigate the need for manual labels, but remains unexplored in 3D
reconstruction. Inspired by the recent success of semi-supervised image
classification tasks, we propose SSP3D, a semi-supervised framework for 3D
reconstruction. In particular, we introduce an attention-guided prototype shape
prior module for guiding realistic object reconstruction. We further introduce
a discriminator-guided module to incentivize better shape generation, as well
as a regularizer to tolerate noisy training samples. On the ShapeNet benchmark,
the proposed approach outperforms previous supervised methods by clear margins
under various labeling ratios, (i.e., 1%, 5% , 10% and 20%). Moreover, our
approach also performs well when transferring to real-world Pix3D datasets
under labeling ratios of 10%. We also demonstrate our method could transfer to
novel categories with few novel supervised data. Experiments on the popular
ShapeNet dataset show that our method outperforms the zero-shot baseline by
over 12% and we also perform rigorous ablations and analysis to validate our
approach.",https://github.com/ChenHsing/SSP3D,-1
On the Relation between Sensitivity and Accuracy in In-context Learning,0.0371692,"In-context learning (ICL) suffers from oversensitivity to the prompt, making
it unreliable in real-world scenarios. We study the sensitivity of ICL with
respect to multiple perturbation types. First, we find that label bias obscures
the true sensitivity, and therefore prior work may have significantly
underestimated ICL sensitivity. Second, we observe a strong negative
correlation between ICL sensitivity and accuracy: predictions sensitive to
perturbations are less likely to be correct. Motivated by these findings, we
propose \textsc{SenSel}, a few-shot selective prediction method that abstains
from sensitive predictions. Experiments on ten classification datasets show
that \textsc{SenSel} consistently outperforms two commonly used
confidence-based and entropy-based baselines on abstention decisions.",https://github.com/yandachen/ICLSensitivity,-1
Understanding Influence Functions and Datamodels via Harmonic Analysis,0.0422594,"Influence functions estimate effect of individual data points on predictions
of the model on test data and were adapted to deep learning in Koh and Liang
[2017]. They have been used for detecting data poisoning, detecting helpful and
harmful examples, influence of groups of datapoints, etc. Recently, Ilyas et
al. [2022] introduced a linear regression method they termed datamodels to
predict the effect of training points on outputs on test data. The current
paper seeks to provide a better theoretical understanding of such interesting
empirical phenomena. The primary tool is harmonic analysis and the idea of
noise stability. Contributions include: (a) Exact characterization of the
learnt datamodel in terms of Fourier coefficients. (b) An efficient method to
estimate the residual error and quality of the optimum linear datamodel without
having to train the datamodel. (c) New insights into when influences of groups
of datapoints may or may not add up linearly.",https://github.com/libffcv/ffcv/,-1
Physics-aware Differentiable Discrete Codesign for Diffractive Optical Neural Networks,0.228352,"Diffractive optical neural networks (DONNs) have attracted lots of attention
as they bring significant advantages in terms of power efficiency, parallelism,
and computational speed compared with conventional deep neural networks (DNNs),
which have intrinsic limitations when implemented on digital platforms.
However, inversely mapping algorithm-trained physical model parameters onto
real-world optical devices with discrete values is a non-trivial task as
existing optical devices have non-unified discrete levels and non-monotonic
properties. This work proposes a novel device-to-system hardware-software
codesign framework, which enables efficient physics-aware training of DONNs
w.r.t arbitrary experimental measured optical devices across layers.
Specifically, Gumbel-Softmax is employed to enable differentiable discrete
mapping from real-world device parameters into the forward function of DONNs,
where the physical parameters in DONNs can be trained by simply minimizing the
loss function of the ML task. The results have demonstrated that our proposed
framework offers significant advantages over conventional quantization-based
methods, especially with low-precision optical devices. Finally, the proposed
algorithm is fully verified with physical experimental optical systems in
low-precision settings.",None,-1
Constrained Dynamic Movement Primitives for Safe Learning of Motor Skills,0.147097,"Dynamic movement primitives are widely used for learning skills which can be
demonstrated to a robot by a skilled human or controller. While their
generalization capabilities and simple formulation make them very appealing to
use, they possess no strong guarantees to satisfy operational safety
constraints for a task. In this paper, we present constrained dynamic movement
primitives (CDMP) which can allow for constraint satisfaction in the robot
workspace. We present a formulation of a non-linear optimization to perturb the
DMP forcing weights regressed by locally-weighted regression to admit a Zeroing
Barrier Function (ZBF), which certifies workspace constraint satisfaction. We
demonstrate the proposed CDMP under different constraints on the end-effector
movement such as obstacle avoidance and workspace constraints on a physical
robot. A video showing the implementation of the proposed algorithm using
different manipulators in different environments could be found here
https://youtu.be/hJegJJkJfys.",None,-1
GMF: General Multimodal Fusion Framework for Correspondence Outlier Rejection,0.112098,"Rejecting correspondence outliers enables to boost the correspondence
quality, which is a critical step in achieving high point cloud registration
accuracy. The current state-of-the-art correspondence outlier rejection methods
only utilize the structure features of the correspondences. However, texture
information is critical to reject the correspondence outliers in our human
vision system. In this paper, we propose General Multimodal Fusion (GMF) to
learn to reject the correspondence outliers by leveraging both the structure
and texture information. Specifically, two cross-attention-based fusion layers
are proposed to fuse the texture information from paired images and structure
information from point correspondences. Moreover, we propose a convolutional
position encoding layer to enhance the difference between Tokens and enable the
encoding feature pay attention to neighbor information. Our position encoding
layer will make the cross-attention operation integrate both local and global
information. Experiments on multiple datasets(3DMatch, 3DLoMatch, KITTI) and
recent state-of-the-art models (3DRegNet, DGR, PointDSC) prove that our GMF
achieves wide generalization ability and consistently improves the point cloud
registration accuracy. Furthermore, several ablation studies demonstrate the
robustness of the proposed GMF on different loss functions, lighting conditions
and noises.The code is available at https://github.com/XiaoshuiHuang/GMF.",https://github.com/XiaoshuiHuang/GMF,-1
BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric,0.0503552,"End-to-End speech-to-speech translation (S2ST) is generally evaluated with
text-based metrics. This means that generated speech has to be automatically
transcribed, making the evaluation dependent on the availability and quality of
automatic speech recognition (ASR) systems. In this paper, we propose a
text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the
dependency on ASR systems. BLASER leverages a multilingual multimodal encoder
to directly encode the speech segments for source input, translation output and
reference into a shared embedding space and computes a score of the translation
quality that can be used as a proxy to human evaluation. To evaluate our
approach, we construct training and evaluation sets from more than 40k human
annotations covering seven language directions. The best results of BLASER are
achieved by training with supervision from human rating scores. We show that
when evaluated at the sentence level, BLASER correlates significantly better
with human judgment compared to ASR-dependent metrics including ASR-SENTBLEU in
all translation directions and ASR-COMET in five of them. Our analysis shows
combining speech and text as inputs to BLASER does not increase the correlation
with human scores, but best correlations are achieved when using speech, which
motivates the goal of our research. Moreover, we show that using ASR for
references is detrimental for text-based metrics.",https://github.com/facebookresearch/stopes,-1
Discovering Transferable Forensic Features for CNN-generated Images Detection,0.23843,"Visual counterfeits are increasingly causing an existential conundrum in
mainstream media with rapid evolution in neural image synthesis methods. Though
detection of such counterfeits has been a taxing problem in the image forensics
community, a recent class of forensic detectors -- universal detectors -- are
able to surprisingly spot counterfeit images regardless of generator
architectures, loss functions, training datasets, and resolutions. This
intriguing property suggests the possible existence of transferable forensic
features (T-FF) in universal detectors. In this work, we conduct the first
analytical study to discover and understand T-FF in universal detectors. Our
contributions are 2-fold: 1) We propose a novel forensic feature relevance
statistic (FF-RS) to quantify and discover T-FF in universal detectors and, 2)
Our qualitative and quantitative investigations uncover an unexpected finding:
color is a critical T-FF in universal detectors. Code and models are available
at https://keshik6.github.io/transferable-forensic-features/",None,-1
TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,0.105113,"Recently, the structural reading comprehension (SRC) task on web pages has
attracted increasing research interests. Although previous SRC work has
leveraged extra information such as HTML tags or XPaths, the informative
topology of web pages is not effectively exploited. In this work, we propose a
Topological Information Enhanced model (TIE), which transforms the token-level
task into a tag-level task by introducing a two-stage process (i.e. node
locating and answer refining). Based on that, TIE integrates Graph Attention
Network (GAT) and Pre-trained Language Model (PLM) to leverage the topological
information of both logical structures and spatial structures. Experimental
results demonstrate that our model outperforms strong baselines and achieves
state-of-the-art performances on the web-based SRC benchmark WebSRC at the time
of writing. The code of TIE will be publicly available at
https://github.com/X-LANCE/TIE.",https://github.com/X-LANCE/TIE,-1
On the State of the Art in Authorship Attribution and Authorship Verification,0.277266,"Despite decades of research on authorship attribution (AA) and authorship
verification (AV), inconsistent dataset splits/filtering and mismatched
evaluation methods make it difficult to assess the state of the art. In this
paper, we present a survey of the fields, resolve points of confusion,
introduce Valla that standardizes and benchmarks AA/AV datasets and metrics,
provide a large-scale empirical evaluation, and provide apples-to-apples
comparisons between existing methods. We evaluate eight promising methods on
fifteen datasets (including distribution-shifted challenge sets) and introduce
a new large-scale dataset based on texts archived by Project Gutenberg.
Surprisingly, we find that a traditional Ngram-based model performs best on 5
(of 7) AA tasks, achieving an average macro-accuracy of $76.50\%$ (compared to
$66.71\%$ for a BERT-based model). However, on the two AA datasets with the
greatest number of words per author, as well as on the AV datasets, BERT-based
models perform best. While AV methods are easily applied to AA, they are seldom
included as baselines in AA papers. We show that through the application of
hard-negative mining, AV methods are competitive alternatives to AA methods.
Valla and all experiment code can be found here:
https://github.com/JacobTyo/Valla",https://github.com/JacobTyo/Valla,-1
Occluded Person Re-Identification via Relational Adaptive Feature Correction Learning,0.081925,"Occluded person re-identification (Re-ID) in images captured by multiple
cameras is challenging because the target person is occluded by pedestrians or
objects, especially in crowded scenes. In addition to the processes performed
during holistic person Re-ID, occluded person Re-ID involves the removal of
obstacles and the detection of partially visible body parts. Most existing
methods utilize the off-the-shelf pose or parsing networks as pseudo labels,
which are prone to error. To address these issues, we propose a novel Occlusion
Correction Network (OCNet) that corrects features through relational-weight
learning and obtains diverse and representative features without using external
networks. In addition, we present a simple concept of a center feature in order
to provide an intuitive solution to pedestrian occlusion scenarios.
Furthermore, we suggest the idea of Separation Loss (SL) for focusing on
different parts between global features and part features. We conduct extensive
experiments on five challenging benchmark datasets for occluded and holistic
Re-ID tasks to demonstrate that our method achieves superior performance to
state-of-the-art methods especially on occluded scene.",None,-1
A Simple and Powerful Global Optimization for Unsupervised Video Object Segmentation,0.124055,"We propose a simple, yet powerful approach for unsupervised object
segmentation in videos. We introduce an objective function whose minimum
represents the mask of the main salient object over the input sequence. It only
relies on independent image features and optical flows, which can be obtained
using off-the-shelf self-supervised methods. It scales with the length of the
sequence with no need for superpixels or sparsification, and it generalizes to
different datasets without any specific training. This objective function can
actually be derived from a form of spectral clustering applied to the entire
video. Our method achieves on-par performance with the state of the art on
standard benchmarks (DAVIS2016, SegTrack-v2, FBMS59), while being conceptually
and practically much simpler. Code is available at
https://ponimatkin.github.io/ssl-vos.",https://ponimatkin.github.io/ssl-vos,-1
What Do Compressed Multilingual Machine Translation Models Forget?,0.131039,"Recently, very large pre-trained models achieve state-of-the-art results in
various natural language processing (NLP) tasks, but their size makes it more
challenging to apply them in resource-constrained environments. Compression
techniques allow to drastically reduce the size of the models and therefore
their inference time with negligible impact on top-tier metrics. However, the
general performance averaged across multiple tasks and/or languages may hide a
drastic performance drop on under-represented features, which could result in
the amplification of biases encoded by the models. In this work, we assess the
impact of compression methods on Multilingual Neural Machine Translation models
(MNMT) for various language groups, gender, and semantic biases by extensive
analysis of compressed models on different machine translation benchmarks, i.e.
FLORES-101, MT-Gender, and DiBiMT. We show that the performance of
under-represented languages drops significantly, while the average BLEU metric
only slightly decreases. Interestingly, the removal of noisy memorization with
compression leads to a significant improvement for some medium-resource
languages. Finally, we demonstrate that compression amplifies intrinsic gender
and semantic biases, even in high-resource languages. Code:
https://github.com/alirezamshi/bias-compressedMT",https://github.com/alirezamshi/bias-compressedMT,-1
A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,0.0811393,"Deep reinforcement learning is a promising approach to learning policies in
uncontrolled environments that do not require domain knowledge. Unfortunately,
due to sample inefficiency, deep RL applications have primarily focused on
simulated environments. In this work, we demonstrate that the recent
advancements in machine learning algorithms and libraries combined with a
carefully tuned robot controller lead to learning quadruped locomotion in only
20 minutes in the real world. We evaluate our approach on several indoor and
outdoor terrains which are known to be challenging for classical model-based
controllers. We observe the robot to be able to learn walking gait consistently
on all of these terrains. Finally, we evaluate our design decisions in a
simulated environment.",None,-1
CrowdMLP: Weakly-Supervised Crowd Counting via Multi-Granularity MLP,0.497148,"Existing state-of-the-art crowd counting algorithms rely excessively on
location-level annotations, which are burdensome to acquire. When only
count-level (weak) supervisory signals are available, it is arduous and
error-prone to regress total counts due to the lack of explicit spatial
constraints. To address this issue, a novel and efficient counter (referred to
as CrowdMLP) is presented, which probes into modelling global dependencies of
embeddings and regressing total counts by devising a multi-granularity MLP
regressor. In specific, a locally-focused pre-trained frontend is cascaded to
extract crude feature maps with intrinsic spatial cues, which prevent the model
from collapsing into trivial outcomes. The crude embeddings, along with raw
crowd scenes, are tokenized at different granularity levels. The
multi-granularity MLP then proceeds to mix tokens at the dimensions of
cardinality, channel, and spatial for mining global information. An effective
proxy task, namely Split-Counting, is also proposed to evade the barrier of
limited samples and the shortage of spatial hints in a self-supervised manner.
Extensive experiments demonstrate that CrowdMLP significantly outperforms
existing weakly-supervised counting algorithms and performs on par with
state-of-the-art location-level supervised approaches.",None,-1
A Proposal for Foley Sound Synthesis Challenge,0.113389,"""Foley"" refers to sound effects that are added to multimedia during
post-production to enhance its perceived acoustic properties, e.g., by
simulating the sounds of footsteps, ambient environmental sounds, or visible
objects on the screen. While foley is traditionally produced by foley artists,
there is increasing interest in automatic or machine-assisted techniques
building upon recent advances in sound synthesis and generative models. To
foster more participation in this growing research area, we propose a challenge
for automatic foley synthesis. Through case studies on successful previous
challenges in audio and machine learning, we set the goals of the proposed
challenge: rigorous, unified, and efficient evaluation of different foley
synthesis systems, with an overarching goal of drawing active participation
from the research community. We outline the details and design considerations
of a foley sound synthesis challenge, including task definition, dataset
requirements, and evaluation criteria.",None,-1
"Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System",0.684501,"Humans excel at continually learning from an ever-changing environment
whereas it remains a challenge for deep neural networks which exhibit
catastrophic forgetting. The complementary learning system (CLS) theory
suggests that the interplay between rapid instance-based learning and slow
structured learning in the brain is crucial for accumulating and retaining
knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)
method which maintains short-term and long-term semantic memories that interact
with the episodic memory. Our method employs an effective replay mechanism
whereby new knowledge is acquired while aligning the decision boundaries with
the semantic memories. CLS-ER does not utilize the task boundaries or make any
assumption about the distribution of the data which makes it versatile and
suited for ""general continual learning"". Our approach achieves state-of-the-art
performance on standard benchmarks as well as more realistic general continual
learning settings.",https://github.com/NeurAI-Lab/CLS-ER,-1
Structural Bias for Aspect Sentiment Triplet Extraction,0.0748588,"Structural bias has recently been exploited for aspect sentiment triplet
extraction (ASTE) and led to improved performance. On the other hand, it is
recognized that explicitly incorporating structural bias would have a negative
impact on efficiency, whereas pretrained language models (PLMs) can already
capture implicit structures. Thus, a natural question arises: Is structural
bias still a necessity in the context of PLMs? To answer the question, we
propose to address the efficiency issues by using an adapter to integrate
structural bias in the PLM and using a cheap-to-compute relative position
structure in place of the syntactic dependency structure. Benchmarking
evaluation is conducted on the SemEval datasets. The results show that our
proposed structural adapter is beneficial to PLMs and achieves state-of-the-art
performance over a range of strong baselines, yet with a light parameter demand
and low latency. Meanwhile, we give rise to the concern that the current
evaluation default with data of small scale is under-confident. Consequently,
we release a large-scale dataset for ASTE. The results on the new dataset hint
that the structural adapter is confidently effective and efficient to a large
scale. Overall, we draw the conclusion that structural bias shall still be a
necessity even with PLMs.",https://github.com/GeneZC/StructBias,-1
Multimodal Transformer for Nursing Activity Recognition,0.252385,"In an aging population, elderly patient safety is a primary concern at
hospitals and nursing homes, which demands for increased nurse care. By
performing nurse activity recognition, we can not only make sure that all
patients get an equal desired care, but it can also free nurses from manual
documentation of activities they perform, leading to a fair and safe place of
care for the elderly. In this work, we present a multimodal transformer-based
network, which extracts features from skeletal joints and acceleration data,
and fuses them to perform nurse activity recognition. Our method achieves
state-of-the-art performance of 81.8% accuracy on the benchmark dataset
available for nurse activity recognition from the Nurse Care Activity
Recognition Challenge. We perform ablation studies to show that our fusion
model is better than single modality transformer variants (using only
acceleration or skeleton joints data). Our solution also outperforms
state-of-the-art ST-GCN, GRU and other classical hand-crafted-feature-based
classifier solutions by a margin of 1.6%, on the NCRC dataset. Code is
available at \url{https://github.com/Momilijaz96/MMT_for_NCRC}.",https://github.com/Momilijaz96/MMT_for_NCRC,-1
End-to-end model for named entity recognition from speech without paired training data,0.0616857,"Recent works showed that end-to-end neural approaches tend to become very
popular for spoken language understanding (SLU). Through the term end-to-end,
one considers the use of a single model optimized to extract semantic
information directly from the speech signal. A major issue for such models is
the lack of paired audio and textual data with semantic annotation. In this
paper, we propose an approach to build an end-to-end neural model to extract
semantic information in a scenario in which zero paired audio data is
available. Our approach is based on the use of an external model trained to
generate a sequence of vectorial representations from text. These
representations mimic the hidden representations that could be generated inside
an end-to-end automatic speech recognition (ASR) model by processing a speech
signal. An SLU neural module is then trained using these representations as
input and the annotated text as output. Last, the SLU module replaces the top
layers of the ASR model to achieve the construction of the end-to-end model.
Our experiments on named entity recognition, carried out on the QUAERO corpus,
show that this approach is very promising, getting better results than a
comparable cascade approach or than the use of synthetic voices.",https://github.com/mdhaffar/Named-Entity-Recognition,-1
Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,0.704122,"End-to-end speech-to-speech translation (S2ST) without relying on
intermediate text representations is a rapidly emerging frontier of research.
Recent works have demonstrated that the performance of such direct S2ST systems
is approaching that of conventional cascade S2ST when trained on comparable
datasets. However, in practice, the performance of direct S2ST is bounded by
the availability of paired S2ST training data. In this work, we explore
multiple approaches for leveraging much more widely available unsupervised and
weakly-supervised speech and text data to improve the performance of direct
S2ST based on Translatotron 2. With our most effective approaches, the average
translation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is
improved by +13.6 BLEU (or +113% relatively), as compared to the previous
state-of-the-art trained without additional data. The improvements on
low-resource language are even more significant (+398% relatively on average).
Our comparative studies suggest future research directions for S2ST and speech
representation learning.",None,-1
Exemplar Free Class Agnostic Counting,0.190636,"We tackle the task of Class Agnostic Counting, which aims to count objects in
a novel object category at test time without any access to labeled training
data for that category. All previous class agnostic counting methods cannot
work in a fully automated setting, and require computationally expensive test
time adaptation. To address these challenges, we propose a visual counter which
operates in a fully automated setting and does not require any test time
adaptation. Our proposed approach first identifies exemplars from repeating
objects in an image, and then counts the repeating objects. We propose a novel
region proposal network for identifying the exemplars. After identifying the
exemplars, we obtain the corresponding count by using a density estimation
based Visual Counter. We evaluate our proposed approach on FSC-147 dataset, and
show that it achieves superior performance compared to the existing approaches.",None,-1
Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection,0.240239,"We study the selection of transfer languages for automatic abusive language
detection. Instead of preparing a dataset for every language, we demonstrate
the effectiveness of cross-lingual transfer learning for zero-shot abusive
language detection. This way we can use existing data from higher-resource
languages to build better detection systems for low-resource languages. Our
datasets are from seven different languages from three language families. We
measure the distance between the languages using several language similarity
measures, especially by quantifying the World Atlas of Language Structures. We
show that there is a correlation between linguistic similarity and classifier
performance. This discovery allows us to choose an optimal transfer language
for zero shot abusive language detection.",None,-1
Generalizing to the Future: Mitigating Entity Bias in Fake News Detection,0.203824,"The wide dissemination of fake news is increasingly threatening both
individuals and society. Fake news detection aims to train a model on the past
news and detect fake news of the future. Though great efforts have been made,
existing fake news detection methods overlooked the unintended entity bias in
the real-world data, which seriously influences models' generalization ability
to future data. For example, 97\% of news pieces in 2010-2017 containing the
entity `Donald Trump' are real in our data, but the percentage falls down to
merely 33\% in 2018. This would lead the model trained on the former set to
hardly generalize to the latter, as it tends to predict news pieces about
`Donald Trump' as real for lower training loss. In this paper, we propose an
entity debiasing framework (\textbf{ENDEF}) which generalizes fake news
detection models to the future data by mitigating entity bias from a
cause-effect perspective. Based on the causal graph among entities, news
contents, and news veracity, we separately model the contribution of each cause
(entities and contents) during training. In the inference stage, we remove the
direct effect of the entities to mitigate entity bias. Extensive offline
experiments on the English and Chinese datasets demonstrate that the proposed
framework can largely improve the performance of base fake news detectors, and
online tests verify its superiority in practice. To the best of our knowledge,
this is the first work to explicitly improve the generalization ability of fake
news detection models to the future data. The code has been released at
https://github.com/ICTMCG/ENDEF-SIGIR2022.",https://github.com/ICTMCG/ENDEF-SIGIR2022,-1
A Framework for Multi-stage Bonus Allocation in meal delivery Platform,0.241812,"Online meal delivery is undergoing explosive growth, as this service is
becoming increasingly popular. A meal delivery platform aims to provide
excellent and stable services for customers and restaurants. However, in
reality, several hundred thousand orders are canceled per day in the Meituan
meal delivery platform since they are not accepted by the crowd soucing
drivers. The cancellation of the orders is incredibly detrimental to the
customer's repurchase rate and the reputation of the Meituan meal delivery
platform. To solve this problem, a certain amount of specific funds is provided
by Meituan's business managers to encourage the crowdsourcing drivers to accept
more orders. To make better use of the funds, in this work, we propose a
framework to deal with the multi-stage bonus allocation problem for a meal
delivery platform. The objective of this framework is to maximize the number of
accepted orders within a limited bonus budget. This framework consists of a
semi-black-box acceptance probability model, a Lagrangian dual-based dynamic
programming algorithm, and an online allocation algorithm. The semi-black-box
acceptance probability model is employed to forecast the relationship between
the bonus allocated to order and its acceptance probability, the Lagrangian
dual-based dynamic programming algorithm aims to calculate the empirical
Lagrangian multiplier for each allocation stage offline based on the historical
data set, and the online allocation algorithm uses the results attained in the
offline part to calculate a proper delivery bonus for each order. To verify the
effectiveness and efficiency of our framework, both offline experiments on a
real-world data set and online A/B tests on the Meituan meal delivery platform
are conducted. Our results show that using the proposed framework, the total
order cancellations can be decreased by more than 25\% in reality.",None,-1
Ultrahyperbolic Knowledge Graph Embeddings,0.250377,"Recent knowledge graph (KG) embeddings have been advanced by hyperbolic
geometry due to its superior capability for representing hierarchies. The
topological structures of real-world KGs, however, are rather heterogeneous,
i.e., a KG is composed of multiple distinct hierarchies and non-hierarchical
graph structures. Therefore, a homogeneous (either Euclidean or hyperbolic)
geometry is not sufficient for fairly representing such heterogeneous
structures. To capture the topological heterogeneity of KGs, we present an
ultrahyperbolic KG embedding (UltraE) in an ultrahyperbolic (or
pseudo-Riemannian) manifold that seamlessly interleaves hyperbolic and
spherical manifolds. In particular, we model each relation as a
pseudo-orthogonal transformation that preserves the pseudo-Riemannian bilinear
form. The pseudo-orthogonal transformation is decomposed into various operators
(i.e., circular rotations, reflections and hyperbolic rotations), allowing for
simultaneously modeling heterogeneous structures as well as complex relational
patterns. Experimental results on three standard KGs show that UltraE
outperforms previous Euclidean- and hyperbolic-based approaches.",None,-1
BITE: Textual Backdoor Attacks with Iterative Trigger Injection,0.16467,"Backdoor attacks have become an emerging threat to NLP systems. By providing
poisoned training data, the adversary can embed a ""backdoor"" into the victim
model, which allows input instances satisfying certain textual patterns (e.g.,
containing a keyword) to be predicted as a target label of the adversary's
choice. In this paper, we demonstrate that it is possible to design a backdoor
attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a
high attack success rate). We propose BITE, a backdoor attack that poisons the
training data to establish strong correlations between the target label and a
set of ""trigger words"". These trigger words are iteratively identified and
injected into the target-label instances through natural word-level
perturbations. The poisoned training data instruct the victim model to predict
the target label on inputs containing trigger words, forming the backdoor.
Experiments on four text classification datasets show that our proposed attack
is significantly more effective than baseline methods while maintaining decent
stealthiness, raising alarm on the usage of untrusted training data. We further
propose a defense method named DeBITE based on potential trigger word removal,
which outperforms existing methods in defending against BITE and generalizes
well to handling other backdoor attacks.",https://github.com/INK-USC/BITE,-1
Adversarial Contrastive Learning for Evidence-aware Fake News Detection with Graph Neural Networks,0.171328,"The prevalence and perniciousness of fake news have been a critical issue on
the Internet, which stimulates the development of automatic fake news detection
in turn. In this paper, we focus on evidence-based fake news detection, where
several evidences are utilized to probe the veracity of news (i.e., a claim).
Most previous methods first employ sequential models to embed the semantic
information and then capture the claim-evidence interaction based on attention
mechanisms. Despite their effectiveness, they still suffer from three
weaknesses. Firstly, sequential models fail to integrate the relevant
information that is scattered far apart in evidences. Secondly, they
underestimate much redundant information in evidences may be useless or
harmful. Thirdly, insufficient data utilization limits the separability and
reliability of representations captured by the model. To solve these problems,
we propose a unified Graph-based sEmantic structure mining framework with
ConTRAstive Learning, namely GETRAL in short. Specifically, we first model
claims and evidences as graph-structured data to capture the long-distance
semantic dependency. Consequently, we reduce information redundancy by
performing graph structure learning. Then the fine-grained semantic
representations are fed into the claim-evidence interaction module for
predictions. Finally, an adversarial contrastive learning module is applied to
make full use of data and strengthen representation learning. Comprehensive
experiments have demonstrated the superiority of GETRAL over the
state-of-the-arts and validated the efficacy of semantic mining with graph
structure and contrastive learning.",https://github.com/CRIPAC-DIG/GETRAL,-1
"Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",0.233482,"The size of pretrained models is increasing, and so is their performance on a
variety of NLP tasks. However, as their memorization capacity grows, they might
pick up more social biases. In this work, we examine the connection between
model size and its gender bias (specifically, occupational gender bias). We
measure bias in three masked language model families (RoBERTa, DeBERTa, and T5)
in two setups: directly using prompt based method, and using a downstream task
(Winogender). We find on the one hand that larger models receive higher bias
scores on the former task, but when evaluated on the latter, they make fewer
gender errors. To examine these potentially conflicting results, we carefully
investigate the behavior of the different models on Winogender. We find that
while larger models outperform smaller ones, the probability that their
mistakes are caused by gender bias is higher. Moreover, we find that the
proportion of stereotypical errors compared to anti-stereotypical ones grows
with the model size. Our findings highlight the potential risks that can arise
from increasing model size.",https://github.com/schwartz-lab-NLP/model_size_and_gender_bias,-1
Exemplar-free Online Continual Learning,0.212164,"Targeted for real world scenarios, online continual learning aims to learn
new tasks from sequentially available data under the condition that each data
is observed only once by the learner. Though recent works have made remarkable
achievements by storing part of learned task data as exemplars for knowledge
replay, the performance is greatly relied on the size of stored exemplars while
the storage consumption is a significant constraint in continual learning. In
addition, storing exemplars may not always be feasible for certain applications
due to privacy concerns. In this work, we propose a novel exemplar-free method
by leveraging nearest-class-mean (NCM) classifier where the class mean is
estimated during training phase on all data seen so far through online mean
update criteria. We focus on image classification task and conduct extensive
experiments on benchmark datasets including CIFAR-100 and Food-1k. The results
demonstrate that our method without using any exemplar outperforms
state-of-the-art exemplar-based approaches with large margins under standard
protocol (20 exemplars per class) and is able to achieve competitive
performance even with larger exemplar size (100 exemplars per class).",None,-1
Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation,0.214846,"Neural networks tend to gradually forget the previously learned knowledge
when learning multiple tasks sequentially from dynamic data distributions. This
problem is called \textit{catastrophic forgetting}, which is a fundamental
challenge in the continual learning of neural networks. In this work, we
observe that catastrophic forgetting not only occurs in continual learning but
also affects the traditional static training. Neural networks, especially
neural machine translation models, suffer from catastrophic forgetting even if
they learn from a static training set. To be specific, the final model pays
imbalanced attention to training samples, where recently exposed samples
attract more attention than earlier samples. The underlying cause is that
training samples do not get balanced training in each model update, so we name
this problem \textit{imbalanced training}. To alleviate this problem, we
propose Complementary Online Knowledge Distillation (COKD), which uses
dynamically updated teacher models trained on specific data orders to
iteratively provide complementary knowledge to the student model. Experimental
results on multiple machine translation tasks show that our method successfully
alleviates the problem of imbalanced training and achieves substantial
improvements over strong baseline systems.",https://github.com/ictnlp/COKD,-1
Class-Specific Semantic Reconstruction for Open Set Recognition,0.279375,"Open set recognition enables deep neural networks (DNNs) to identify samples
of unknown classes, while maintaining high classification accuracy on samples
of known classes. Existing methods basing on auto-encoder (AE) and prototype
learning show great potential in handling this challenging task. In this study,
we propose a novel method, called Class-Specific Semantic Reconstruction
(CSSR), that integrates the power of AE and prototype learning. Specifically,
CSSR replaces prototype points with manifolds represented by class-specific
AEs. Unlike conventional prototype-based methods, CSSR models each known class
on an individual AE manifold, and measures class belongingness through AE's
reconstruction error. Class-specific AEs are plugged into the top of the DNN
backbone and reconstruct the semantic representations learned by the DNN
instead of the raw image. Through end-to-end learning, the DNN and the AEs
boost each other to learn both discriminative and representative information.
The results of experiments conducted on multiple datasets show that the
proposed method achieves outstanding performance in both close and open set
recognition and is sufficiently simple and flexible to incorporate into
existing frameworks.",None,-1
DigNet: Digging Clues from Local-Global Interactive Graph for Aspect-level Sentiment Classification,0.0803003,"In aspect-level sentiment classification (ASC), state-of-the-art models
encode either syntax graph or relation graph to capture the local syntactic
information or global relational information. Despite the advantages of syntax
and relation graphs, they have respective shortages which are neglected,
limiting the representation power in the graph modeling process. To resolve
their limitations, we design a novel local-global interactive graph, which
marries their advantages by stitching the two graphs via interactive edges. To
model this local-global interactive graph, we propose a novel neural network
termed DigNet, whose core module is the stacked local-global interactive (LGI)
layers performing two processes: intra-graph message passing and cross-graph
message passing. In this way, the local syntactic and global relational
information can be reconciled as a whole in understanding the aspect-level
sentiment. Concretely, we design two variants of local-global interactive
graphs with different kinds of interactive edges and three variants of LGI
layers. We conduct experiments on several public benchmark datasets and the
results show that we outperform previous best scores by 3\%, 2.32\%, and 6.33\%
in terms of Macro-F1 on Lap14, Res14, and Res15 datasets, respectively,
confirming the effectiveness and superiority of the proposed local-global
interactive graph and DigNet.",None,-1
Human Interpretation of Saliency-based Explanation Over Text,0.126648,"While a lot of research in explainable AI focuses on producing effective
explanations, less work is devoted to the question of how people understand and
interpret the explanation. In this work, we focus on this question through a
study of saliency-based explanations over textual data. Feature-attribution
explanations of text models aim to communicate which parts of the input text
were more influential than others towards the model decision. Many current
explanation methods, such as gradient-based or Shapley value-based methods,
provide measures of importance which are well-understood mathematically. But
how does a person receiving the explanation (the explainee) comprehend it? And
does their understanding match what the explanation attempted to communicate?
We empirically investigate the effect of various factors of the input, the
feature-attribution explanation, and visualization procedure, on laypeople's
interpretation of the explanation. We query crowdworkers for their
interpretation on tasks in English and German, and fit a GAMM model to their
responses considering the factors of interest. We find that people often
mis-interpret the explanations: superficial and unrelated factors, such as word
length, influence the explainees' importance assignment despite the explanation
communicating importance directly. We then show that some of this distortion
can be attenuated: we propose a method to adjust saliencies based on model
estimates of over- and under-perception, and explore bar charts as an
alternative to heatmap saliency visualization. We find that both approaches can
attenuate the distorting effect of specific factors, leading to
better-calibrated understanding of the explanation.",None,-1
ConvMAE: Masked Convolution Meets Masked Autoencoders,0.950213,"Vision Transformers (ViT) become widely-adopted architectures for various
vision tasks. Masked auto-encoding for feature pretraining and multi-scale
hybrid convolution-transformer architectures can further unleash the potentials
of ViT, leading to state-of-the-art performances on image classification,
detection and semantic segmentation. In this paper, our ConvMAE framework
demonstrates that multi-scale hybrid convolution-transformer can learn more
discriminative representations via the mask auto-encoding scheme. However,
directly using the original masking strategy leads to the heavy computational
cost and pretraining-finetuning discrepancy. To tackle the issue, we adopt the
masked convolution to prevent information leakage in the convolution blocks. A
simple block-wise masking strategy is proposed to ensure computational
efficiency. We also propose to more directly supervise the multi-scale features
of the encoder to boost multi-scale features. Based on our pretrained ConvMAE
models, ConvMAE-Base improves ImageNet-1K finetuning accuracy by 1.4% compared
with MAE-Base. On object detection, ConvMAE-Base finetuned for only 25 epochs
surpasses MAE-Base fined-tuned for 100 epochs by 2.9% box AP and 2.2% mask AP
respectively. Code and pretrained models are available at
https://github.com/Alpha-VL/ConvMAE.",https://github.com/Alpha-VL/ConvMAE,-1
Exploration via Elliptical Episodic Bonuses,0.115092,"In recent years, a number of reinforcement learning (RL) methods have been
proposed to explore complex environments which differ across episodes. In this
work, we show that the effectiveness of these methods critically relies on a
count-based episodic term in their exploration bonus. As a result, despite
their success in relatively simple, noise-free settings, these methods fall
short in more realistic scenarios where the state space is vast and prone to
noise. To address this limitation, we introduce Exploration via Elliptical
Episodic Bonuses (E3B), a new method which extends count-based episodic bonuses
to continuous state spaces and encourages an agent to explore states that are
diverse under a learned embedding within each episode. The embedding is learned
using an inverse dynamics model in order to capture controllable aspects of the
environment. Our method sets a new state-of-the-art across 16 challenging tasks
from the MiniHack suite, without requiring task-specific inductive biases. E3B
also matches existing methods on sparse reward, pixel-based VizDoom
environments, and outperforms existing methods in reward-free exploration on
Habitat, demonstrating that it can scale to high-dimensional pixel-based
observations and realistic environments.",https://github.com/facebookresearch/e3b,-1
Prosodic Alignment for off-screen automatic dubbing,0.0580691,"The goal of automatic dubbing is to perform speech-to-speech translation
while achieving audiovisual coherence. This entails isochrony, i.e.,
translating the original speech by also matching its prosodic structure into
phrases and pauses, especially when the speaker's mouth is visible. In previous
work, we introduced a prosodic alignment model to address isochrone or
on-screen dubbing. In this work, we extend the prosodic alignment model to also
address off-screen dubbing that requires less stringent synchronization
constraints. We conduct experiments on four dubbing directions - English to
French, Italian, German and Spanish - on a publicly available collection of TED
Talks and on publicly available YouTube videos. Empirical results show that
compared to our previous work the extended prosodic alignment model provides
significantly better subjective viewing experience on videos in which on-screen
and off-screen automatic dubbing is applied for sentences with speakers mouth
visible and not visible, respectively.",None,-1
Spiking Graph Convolutional Networks,0.100817,"Graph Convolutional Networks (GCNs) achieve an impressive performance due to
the remarkable representation ability in learning the graph information.
However, GCNs, when implemented on a deep network, require expensive
computation power, making them difficult to be deployed on battery-powered
devices. In contrast, Spiking Neural Networks (SNNs), which perform a
bio-fidelity inference process, offer an energy-efficient neural architecture.
In this work, we propose SpikingGCN, an end-to-end framework that aims to
integrate the embedding of GCNs with the biofidelity characteristics of SNNs.
The original graph data are encoded into spike trains based on the
incorporation of graph convolution. We further model biological information
processing by utilizing a fully connected layer combined with neuron nodes. In
a wide range of scenarios (e.g. citation networks, image graph classification,
and recommender systems), our experimental results show that the proposed
method could gain competitive performance against state-of-the-art approaches.
Furthermore, we show that SpikingGCN on a neuromorphic chip can bring a clear
advantage of energy efficiency into graph data analysis, which demonstrates its
great potential to construct environment-friendly machine learning models.",https://github.com/ZulunZhu/SpikingGCN.git,-1
On the Effect of Anticipation on Reading Times,0.0479955,"Over the past two decades, numerous studies have demonstrated how less
predictable (i.e., higher surprisal) words take more time to read. In general,
these studies have implicitly assumed the reading process is purely responsive:
Readers observe a new word and allocate time to process it as required. We
argue that prior results are also compatible with a reading process that is at
least partially anticipatory: Readers could make predictions about a future
word and allocate time to process it based on their expectation. In this work,
we operationalize this anticipation as a word's contextual entropy. We assess
the effect of anticipation on reading by comparing how well surprisal and
contextual entropy predict reading times on four naturalistic reading datasets:
two self-paced and two eye-tracking. Experimentally, across datasets and
analyses, we find substantial evidence for effects of contextual entropy over
surprisal on a word's reading time (RT): in fact, entropy is sometimes better
than surprisal in predicting a word's RT. Spillover effects, however, are
generally not captured by entropy, but only by surprisal. Further, we
hypothesize four cognitive mechanisms through which contextual entropy could
impact RTs -- three of which we are able to design experiments to analyze.
Overall, our results support a view of reading that is not just responsive, but
also anticipatory.",https://github.com/rycolab/anticipation-on-reading-times,-1
Global Convergence of Localized Policy Iteration in Networked Multi-Agent Reinforcement Learning,0.228046,"We study a multi-agent reinforcement learning (MARL) problem where the agents
interact over a given network. The goal of the agents is to cooperatively
maximize the average of their entropy-regularized long-term rewards. To
overcome the curse of dimensionality and to reduce communication, we propose a
Localized Policy Iteration (LPI) algorithm that provably learns a
near-globally-optimal policy using only local information. In particular, we
show that, despite restricting each agent's attention to only its $\kappa$-hop
neighborhood, the agents are able to learn a policy with an optimality gap that
decays polynomially in $\kappa$. In addition, we show the finite-sample
convergence of LPI to the global optimal policy, which explicitly captures the
trade-off between optimality and computational complexity in choosing $\kappa$.
Numerical simulations demonstrate the effectiveness of LPI.",None,-1
Non-rigid Point Cloud Registration with Neural Deformation Pyramid,0.359229,"Non-rigid point cloud registration is a key component in many computer vision
and computer graphics applications. The high complexity of the unknown
non-rigid motion make this task a challenging problem. In this paper, we break
down this problem via hierarchical motion decomposition. Our method called
Neural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid
architecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP),
takes as input a sinusoidally encoded 3D point and outputs its motion
increments from the previous level. The sinusoidal function starts with a low
input frequency and gradually increases when the pyramid level goes down. This
allows a multi-level rigid to nonrigid motion decomposition and also speeds up
the solving by 50 times compared to the existing MLP-based approach. Our method
achieves advanced partialto-partial non-rigid point cloud registration results
on the 4DMatch/4DLoMatch benchmark under both no-learned and supervised
settings.",https://github.com/rabbityl/DeformationPyramid,-1
Large Language Models are few(1)-shot Table Reasoners,0.678523,"Recent literature has shown that large language models (LLMs) are generally
excellent few-shot reasoners to solve text reasoning tasks. However, the
capability of LLMs on table reasoning tasks is yet to be explored. In this
paper, we aim at understanding how well LLMs can perform table-related tasks
with few-shot in-context learning. Specifically, we evaluated LLMs on popular
table QA and fact verification datasets like WikiTableQuestion, FetaQA,
TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning
over table structures, though these models are not pre-trained on any table
corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very
strong performance with only a 1-shot demonstration, even on par with some SoTA
models. We show that LLMs are even more competent at generating comprehensive
long-form answers on FetaQA than tuned T5-large. We further manually studied
the reasoning chains elicited from LLMs and found that these reasoning chains
are highly consistent with the underlying semantic form. We believe that LLMs
can serve as a simple yet generic baseline for future research. The code and
data are released in https://github.com/wenhuchen/TableCoT.",https://github.com/wenhuchen/TableCoT,-1
Unfooling Perturbation-Based Post Hoc Explainers,0.129027,"Monumental advancements in artificial intelligence (AI) have lured the
interest of doctors, lenders, judges, and other professionals. While these
high-stakes decision-makers are optimistic about the technology, those familiar
with AI systems are wary about the lack of transparency of its decision-making
processes. Perturbation-based post hoc explainers offer a model agnostic means
of interpreting these systems while only requiring query-level access. However,
recent work demonstrates that these explainers can be fooled adversarially.
This discovery has adverse implications for auditors, regulators, and other
sentinels. With this in mind, several natural questions arise - how can we
audit these black box systems? And how can we ascertain that the auditee is
complying with the audit in good faith? In this work, we rigorously formalize
this problem and devise a defense against adversarial attacks on
perturbation-based explainers. We propose algorithms for the detection
(CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by our
novel conditional anomaly detection approach, KNN-CAD. We demonstrate that our
approach successfully detects whether a black box system adversarially conceals
its decision-making process and mitigates the adversarial attack on real-world
data for the prevalent explainers, LIME and SHAP.",https://github.com/craymichael/unfooling,-1
ST-MoE: Designing Stable and Transferable Sparse Expert Models,0.699852,"Scale has opened new frontiers in natural language processing -- but at a
high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have
been proposed as an energy efficient path to even larger and more capable
language models. But advancing the state-of-the-art across a broad set of
natural language tasks has been hindered by training instabilities and
uncertain quality during fine-tuning. Our work focuses on these issues and acts
as a design guide. We conclude by scaling a sparse model to 269B parameters,
with a computational cost comparable to a 32B dense encoder-decoder Transformer
(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,
a sparse model achieves state-of-the-art performance in transfer learning,
across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC
Challenge), summarization (XSum, CNN-DM), closed book question answering
(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,
ANLI R3).",https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py,-1
Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation,0.165255,"Sign language gloss translation aims to translate the sign glosses into
spoken language texts, which is challenging due to the scarcity of labeled
gloss-text parallel data. Back translation (BT), which generates
pseudo-parallel data by translating in-domain spoken language texts into sign
glosses, has been applied to alleviate the data scarcity problem. However, the
lack of large-scale high-quality domain spoken language text data limits the
effect of BT. In this paper, to overcome the limitation, we propose a Prompt
based domain text Generation (PGEN) approach to produce the large-scale
in-domain spoken language text data. Specifically, PGEN randomly concatenates
sentences from the original in-domain spoken language text data as prompts to
induce a pre-trained language model (i.e., GPT-2) to generate spoken language
texts in a similar style. Experimental results on three benchmarks of sign
language gloss translation in varied languages demonstrate that BT with spoken
language texts generated by PGEN significantly outperforms the compared
methods. In addition, as the scale of spoken language texts generated by PGEN
increases, the BT technique can achieve further improvements, demonstrating the
effectiveness of our approach. We release the code and data for facilitating
future research in this field.",https://github.com/Atrewin/PGen,-1
Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining,0.162265,"Deep visuomotor policy learning, which aims to map raw visual observation to
action, achieves promising results in control tasks such as robotic
manipulation and autonomous driving. However, it requires a huge number of
online interactions with the training environment, which limits its real-world
application. Compared to the popular unsupervised feature learning for visual
recognition, feature pretraining for visuomotor control tasks is much less
explored. In this work, we aim to pretrain policy representations for driving
tasks by watching hours-long uncurated YouTube videos. Specifically, we train
an inverse dynamic model with a small amount of labeled data and use it to
predict action labels for all the YouTube video frames. A new contrastive
policy pretraining method is then developed to learn action-conditioned
features from the video frames with pseudo action labels. Experiments show that
the resulting action-conditioned features obtain substantial improvements for
the downstream reinforcement learning and imitation learning tasks,
outperforming the weights pretrained from previous unsupervised learning
methods and ImageNet pretrained weight. Code, model weights, and data are
available at: https://metadriverse.github.io/ACO.",https://metadriverse.github.io/ACO,-1
Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization,0.575584,"Neural abstractive summarization models are prone to generate summaries which
are factually inconsistent with their source documents. Previous work has
introduced the task of recognizing such factual inconsistency as a downstream
application of natural language inference (NLI). However, state-of-the-art NLI
models perform poorly in this context due to their inability to generalize to
the target task. In this work, we show that NLI models can be effective for
this task when the training data is augmented with high-quality task-oriented
examples. We introduce Falsesum, a data generation pipeline leveraging a
controllable text generation model to perturb human-annotated summaries,
introducing varying types of factual inconsistencies. Unlike previously
introduced document-level NLI datasets, our generated dataset contains examples
that are diverse and inconsistent yet plausible. We show that models trained on
a Falsesum-augmented NLI dataset improve the state-of-the-art performance
across four benchmarks for detecting factual inconsistency in summarization.
  The code to obtain the dataset is available online at
https://github.com/joshbambrick/Falsesum",https://github.com/joshbambrick/Falsesum,-1
Danish Airs and Grounds: A Dataset for Aerial-to-Street-Level Place Recognition and Localization,0.284934,"Place recognition and visual localization are particularly challenging in
wide baseline configurations. In this paper, we contribute with the
\emph{Danish Airs and Grounds} (DAG) dataset, a large collection of
street-level and aerial images targeting such cases. Its main challenge lies in
the extreme viewing-angle difference between query and reference images with
consequent changes in illumination and perspective. The dataset is larger and
more diverse than current publicly available data, including more than 50 km of
road in urban, suburban and rural areas. All images are associated with
accurate 6-DoF metadata that allows the benchmarking of visual localization
methods.
  We also propose a map-to-image re-localization pipeline, that first estimates
a dense 3D reconstruction from the aerial images and then matches query
street-level images to street-level renderings of the 3D model. The dataset can
be downloaded at: https://frederikwarburg.github.io/DAG",None,-1
Near-Optimal Multi-Agent Learning for Safe Coverage Control,0.0621594,"In multi-agent coverage control problems, agents navigate their environment
to reach locations that maximize the coverage of some density. In practice, the
density is rarely known $\textit{a priori}$, further complicating the original
NP-hard problem. Moreover, in many applications, agents cannot visit arbitrary
locations due to $\textit{a priori}$ unknown safety constraints. In this paper,
we aim to efficiently learn the density to approximately solve the coverage
problem while preserving the agents' safety. We first propose a conditionally
linear submodular coverage function that facilitates theoretical analysis.
Utilizing this structure, we develop MacOpt, a novel algorithm that efficiently
trades off the exploration-exploitation dilemma due to partial observability,
and show that it achieves sublinear regret. Next, we extend results on
single-agent safe exploration to our multi-agent setting and propose SafeMac
for safe coverage and exploration. We analyze SafeMac and give first of its
kind results: near optimal coverage in finite time while provably guaranteeing
safety. We extensively evaluate our algorithms on synthetic and real problems,
including a bio-diversity monitoring task under safety constraints, where
SafeMac outperforms competing methods.",https://github.com/manish-pra/SafeMaC,-1
Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition,0.355953,"The research and applications of multimodal emotion recognition have become
increasingly popular recently. However, multimodal emotion recognition faces
the challenge of lack of data. To solve this problem, we propose to use
transfer learning which leverages state-of-the-art pre-trained models including
wav2vec 2.0 and BERT for this task. Multi-level fusion approaches including
coattention-based early fusion and late fusion with the models trained on both
embeddings are explored. Also, a multi-granularity framework which extracts not
only frame-level speech embeddings but also segment-level embeddings including
phone, syllable and word-level speech embeddings is proposed to further boost
the performance. By combining our coattention-based early fusion model and late
fusion model with the multi-granularity feature extraction framework, we obtain
result that outperforms best baseline approaches by 1.3% unweighted accuracy
(UA) on the IEMOCAP dataset.",None,-1
Extractive Summarization of Legal Decisions using Multi-task Learning and Maximal Marginal Relevance,0.0815721,"Summarizing legal decisions requires the expertise of law practitioners,
which is both time- and cost-intensive. This paper presents techniques for
extractive summarization of legal decisions in a low-resource setting using
limited expert annotated data. We test a set of models that locate relevant
content using a sequential model and tackle redundancy by leveraging maximal
marginal relevance to compose summaries. We also demonstrate an implicit
approach to help train our proposed models generate more informative summaries.
Our multi-task learning model variant leverages rhetorical role identification
as an auxiliary task to further improve the summarizer. We perform extensive
experiments on datasets containing legal decisions from the US Board of
Veterans' Appeals and conduct quantitative and expert-ranked evaluations of our
models. Our results show that the proposed approaches can achieve ROUGE scores
vis-\`a-vis expert extracted summaries that match those achieved by
inter-annotator comparison.",https://github.com/LLTLab/VetClaims-JSON,-1
Disentangling Architecture and Training for Optical Flow,0.0651508,"How important are training details and datasets to recent optical flow models
like RAFT? And do they generalize? To explore these questions, rather than
develop a new model, we revisit three prominent models, PWC-Net, IRR-PWC and
RAFT, with a common set of modern training techniques and datasets, and observe
significant performance gains, demonstrating the importance and generality of
these training details. Our newly trained PWC-Net and IRR-PWC models show
surprisingly large improvements, up to 30% versus original published results on
Sintel and KITTI 2015 benchmarks. They outperform the more recent Flow1D on
KITTI 2015 while being 3x faster during inference. Our newly trained RAFT
achieves an Fl-all score of 4.31% on KITTI 2015, more accurate than all
published optical flow methods at the time of writing. Our results demonstrate
the benefits of separating the contributions of models, training techniques and
datasets when analyzing performance gains of optical flow methods. Our source
code will be publicly available.",https://autoflow-google.github.io,-1
Acceptability Judgements via Examining the Topology of Attention Maps,0.0734063,"The role of the attention mechanism in encoding linguistic knowledge has
received special interest in NLP. However, the ability of the attention heads
to judge the grammatical acceptability of a sentence has been underexplored.
This paper approaches the paradigm of acceptability judgments with topological
data analysis (TDA), showing that the geometric properties of the attention
graph can be efficiently exploited for two standard practices in linguistics:
binary judgments and linguistic minimal pairs. Topological features enhance the
BERT-based acceptability classifier scores by $8$%-$24$% on CoLA in three
languages (English, Italian, and Swedish). By revealing the topological
discrepancy between attention maps of minimal pairs, we achieve the human-level
performance on the BLiMP benchmark, outperforming nine statistical and
Transformer LM baselines. At the same time, TDA provides the foundation for
analyzing the linguistic functions of attention heads and interpreting the
correspondence between the graph features and grammatical phenomena.",https://github.com/danchern97/tda4la,-1
Shape-Guided Diffusion with Inside-Outside Attention,0.163747,"We introduce precise object silhouette as a new form of user control in
text-to-image diffusion models, which we dub Shape-Guided Diffusion. Our
training-free method uses an Inside-Outside Attention mechanism during the
inversion and generation process to apply a shape constraint to the cross- and
self-attention maps. Our mechanism designates which spatial region is the
object (inside) vs. background (outside) then associates edits to the correct
region. We demonstrate the efficacy of our method on the shape-guided editing
task, where the model must replace an object according to a text prompt and
object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and
achieve SOTA results in shape faithfulness without a degradation in text
alignment or image realism according to both automatic metrics and annotator
ratings. Our data and code will be made available at
https://shape-guided-diffusion.github.io.",https://shape-guided-diffusion.github.io,-1
QuadTree Attention for Vision Transformers,0.809951,"Transformers have been successful in many vision tasks, thanks to their
capability of capturing long-range dependency. However, their quadratic
computational complexity poses a major obstacle for applying them to vision
tasks requiring dense predictions, such as object detection, feature matching,
stereo, etc. We introduce QuadTree Attention, which reduces the computational
complexity from quadratic to linear. Our quadtree transformer builds token
pyramids and computes attention in a coarse-to-fine manner. At each level, the
top K patches with the highest attention scores are selected, such that at the
next level, attention is only evaluated within the relevant regions
corresponding to these top K patches. We demonstrate that quadtree attention
achieves state-of-the-art performance in various vision tasks, e.g. with 4.0%
improvement in feature matching on ScanNet, about 50% flops reduction in stereo
matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification,
1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on
semantic segmentation over previous state-of-the-art transformers. The codes
are available at https://github.com/Tangshitao/QuadtreeAttention.",https://github.com/Tangshitao/QuadtreeAttention,-1
PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition,0.903028,"3D Point cloud is becoming a critical data representation in many real-world
applications like autonomous driving, robotics, and medical imaging. Although
the success of deep learning further accelerates the adoption of 3D point
clouds in the physical world, deep learning is notorious for its vulnerability
to adversarial attacks. In this work, we first identify that the
state-of-the-art empirical defense, adversarial training, has a major
limitation in applying to 3D point cloud models due to gradient obfuscation. We
further propose PointDP, a purification strategy that leverages diffusion
models to defend against 3D adversarial attacks. We extensively evaluate
PointDP on six representative 3D point cloud architectures, and leverage 10+
strong and adaptive attacks to demonstrate its lower-bound robustness. Our
evaluation shows that PointDP achieves significantly better robustness than
state-of-the-art purification methods under strong attacks. Results of
certified defenses on randomized smoothing combined with PointDP will be
included in the near future.",None,-1
Improving Chinese Spelling Check by Character Pronunciation Prediction: The Effects of Adaptivity and Granularity,0.146998,"Chinese spelling check (CSC) is a fundamental NLP task that detects and
corrects spelling errors in Chinese texts. As most of these spelling errors are
caused by phonetic similarity, effectively modeling the pronunciation of
Chinese characters is a key factor for CSC. In this paper, we consider
introducing an auxiliary task of Chinese pronunciation prediction (CPP) to
improve CSC, and, for the first time, systematically discuss the adaptivity and
granularity of this auxiliary task. We propose SCOPE which builds on top of a
shared encoder two parallel decoders, one for the primary CSC task and the
other for a fine-grained auxiliary CPP task, with a novel adaptive weighting
scheme to balance the two tasks. In addition, we design a delicate iterative
correction strategy for further improvements during inference. Empirical
evaluation shows that SCOPE achieves new state-of-the-art on three CSC
benchmarks, demonstrating the effectiveness and superiority of the auxiliary
CPP task. Comprehensive ablation studies further verify the positive effects of
adaptivity and granularity of the task. Code and data used in this paper are
publicly available at https://github.com/jiahaozhenbang/SCOPE.",https://github.com/jiahaozhenbang/SCOPE,-1
Chunk-based Nearest Neighbor Machine Translation,0.187785,"Semi-parametric models, which augment generation with retrieval, have led to
impressive results in language modeling and machine translation, due to their
ability to retrieve fine-grained information from a datastore of examples. One
of the most prominent approaches, $k$NN-MT, exhibits strong domain adaptation
capabilities by retrieving tokens from domain-specific datastores
\citep{khandelwal2020nearest}. However, $k$NN-MT requires an expensive
retrieval operation for every single generated token, leading to a very low
decoding speed (around 8 times slower than a parametric model). In this paper,
we introduce a \textit{chunk-based} $k$NN-MT model which retrieves chunks of
tokens from the datastore, instead of a single token. We propose several
strategies for incorporating the retrieved chunks into the generation process,
and for selecting the steps at which the model needs to search for neighbors in
the datastore. Experiments on machine translation in two settings, static and
``on-the-fly'' domain adaptation, show that the chunk-based $k$NN-MT model
leads to significant speed-ups (up to 4 times) with only a small drop in
translation quality.",https://github.com/deep-spin/chunk-based_knn-mt,-1
Exploiting Transformation Invariance and Equivariance for Self-supervised Sound Localisation,0.606661,"We present a simple yet effective self-supervised framework for audio-visual
representation learning, to localize the sound source in videos. To understand
what enables to learn useful representations, we systematically investigate the
effects of data augmentations, and reveal that (1) composition of data
augmentations plays a critical role, i.e. explicitly encouraging the
audio-visual representations to be invariant to various transformations~({\em
transformation invariance}); (2) enforcing geometric consistency substantially
improves the quality of learned representations, i.e. the detected sound source
should follow the same transformation applied on input video frames~({\em
transformation equivariance}). Extensive experiments demonstrate that our model
significantly outperforms previous methods on two sound localization
benchmarks, namely, Flickr-SoundNet and VGG-Sound. Additionally, we also
evaluate audio retrieval and cross-modal retrieval tasks. In both cases, our
self-supervised models demonstrate superior retrieval performances, even
competitive with the supervised approach in audio retrieval. This reveals the
proposed framework learns strong multi-modal representations that are
beneficial to sound localisation and generalization to further applications.
\textit{All codes will be available}.",https://jinxiang-liu.github.io/SSL-TIE/,-1
AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning,0.134064,"Biomedical named entity recognition (BioNER) seeks to automatically recognize
biomedical entities in natural language text, serving as a necessary foundation
for downstream text mining tasks and applications such as information
extraction and question answering. Manually labeling training data for the
BioNER task is costly, however, due to the significant domain expertise
required for accurate annotation. The resulting data scarcity causes current
BioNER approaches to be prone to overfitting, to suffer from limited
generalizability, and to address a single entity type at a time (e.g., gene or
disease). We therefore propose a novel all-in-one (AIO) scheme that uses
external data from existing annotated resources to enhance the accuracy and
stability of BioNER models. We further present AIONER, a general-purpose BioNER
tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER
on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and
compares favorably to other state-of-the-art approaches such as multi-task
learning. We further demonstrate the practical utility of AIONER in three
independent tasks to recognize entity types not previously seen in training
data, as well as the advantages of AIONER over existing methods for processing
biomedical text at a large scale (e.g., the entire PubMed data).",None,-1
AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models,0.0406415,"The objective of pre-trained language models is to learn contextual
representations of textual data. Pre-trained language models have become
mainstream in natural language processing and code modeling. Using probes, a
technique to study the linguistic properties of hidden vector spaces, previous
works have shown that these pre-trained language models encode simple
linguistic properties in their hidden representations. However, none of the
previous work assessed whether these models encode the whole grammatical
structure of a programming language. In this paper, we prove the existence of a
syntactic subspace, lying in the hidden representations of pre-trained language
models, which contain the syntactic information of the programming language. We
show that this subspace can be extracted from the models' representations and
define a novel probing method, the AST-Probe, that enables recovering the whole
abstract syntax tree (AST) of an input code snippet. In our experimentations,
we show that this syntactic subspace exists in five state-of-the-art
pre-trained language models. In addition, we highlight that the middle layers
of the models are the ones that encode most of the AST information. Finally, we
estimate the optimal size of this syntactic subspace and show that its
dimension is substantially lower than those of the models' representation
spaces. This suggests that pre-trained language models use a small part of
their representation spaces to encode syntactic information of the programming
languages.",https://doi.org/10.5281/zenodo.7032076,-1
PoissonMat: Remodeling Matrix Factorization using Poisson Distribution and Solving the Cold Start Problem without Input Data,0.864665,"Matrix Factorization is one of the most successful recommender system
techniques over the past decade. However, the classic probabilistic theory
framework for matrix factorization is modeled using normal distributions. To
find better probabilistic models, algorithms such as RankMat, ZeroMat and
DotMat have been invented in recent years. In this paper, we model the user
rating behavior in recommender system as a Poisson process, and design an
algorithm that relies on no input data to solve the recommendation problem and
the cold start issue at the same time. We prove the superiority of our
algorithm in comparison with matrix factorization, random placement, Zipf
placement, ZeroMat, DotMat, etc.",None,-1
OCR Improves Machine Translation for Low-Resource Languages,0.096457,"We aim to investigate the performance of current OCR systems on low resource
languages and low resource scripts. We introduce and make publicly available a
novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with
noise, for 60 low-resource languages in low resource scripts. We evaluate
state-of-the-art OCR systems on our benchmark and analyse most common errors.
We show that OCR monolingual data is a valuable resource that can increase
performance of Machine Translation models, when used in backtranslation. We
then perform an ablation study to investigate how OCR errors impact Machine
Translation performance and determine what is the minimum level of OCR quality
needed for the monolingual data to be useful for Machine Translation.",https://github.com/facebookresearch/flores,-1
"Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data",0.0228275,"Automatic player identification is an essential and complex task in sports
video analysis. Different strategies have been devised over the years, but
identification based on jersey numbers is one of the most common approaches
given its versatility and relative simplicity. However, automatic detection of
jersey numbers is still challenging due to changing camera angles, low video
resolution, small object size in wide-range shots and transient changes in the
player's posture and movement. In this paper we present a novel approach for
jersey number identification in a small, highly imbalanced dataset from the
Seattle Seahawks practice videos. Our results indicate that simple models can
achieve an acceptable performance on the jersey number detection task and that
synthetic data can improve the performance dramatically (accuracy increase of
~9% overall, ~18% on low frequency numbers) making our approach achieve state
of the art results.",None,-1
Cyberbullying detection across social media platforms via platform-aware adversarial encoding,0.276659,"Despite the increasing interest in cyberbullying detection, existing efforts
have largely been limited to experiments on a single platform and their
generalisability across different social media platforms have received less
attention. We propose XP-CB, a novel cross-platform framework based on
Transformers and adversarial learning. XP-CB can enhance a Transformer
leveraging unlabelled data from the source and target platforms to come up with
a common representation while preventing platform-specific training. To
validate our proposed framework, we experiment on cyberbullying datasets from
three different platforms through six cross-platform configurations, showing
its effectiveness with both BERT and RoBERTa as the underlying Transformer
models.",None,-1
Adversarial Masking for Self-Supervised Learning,0.490751,"We propose ADIOS, a masked image model (MIM) framework for self-supervised
learning, which simultaneously learns a masking function and an image encoder
using an adversarial objective. The image encoder is trained to minimise the
distance between representations of the original and that of a masked image.
The masking function, conversely, aims at maximising this distance. ADIOS
consistently improves on state-of-the-art self-supervised learning (SSL)
methods on a variety of tasks and datasets -- including classification on
ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and
iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao
et al., 2021) -- while generating semantically meaningful masks. Unlike modern
MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch
tokenisation construction of Vision Transformers, and can be implemented with
convolutional backbones. We further demonstrate that the masks learned by ADIOS
are more effective in improving representation learning of SSL methods than
masking schemes used in popular MIM models. Code is available at
https://github.com/YugeTen/adios.",None,-1
Improving the fusion of acoustic and text representations in RNN-T,0.136875,"The recurrent neural network transducer (RNN-T) has recently become the
mainstream end-to-end approach for streaming automatic speech recognition
(ASR). To estimate the output distributions over subword units, RNN-T uses a
fully connected layer as the joint network to fuse the acoustic representations
extracted using the acoustic encoder with the text representations obtained
using the prediction network based on the previous subword units. In this
paper, we propose to use gating, bilinear pooling, and a combination of them in
the joint network to produce more expressive representations to feed into the
output layer. A regularisation method is also proposed to enable better
acoustic encoder training by reducing the gradients back-propagated into the
prediction network at the beginning of RNN-T training. Experimental results on
a multilingual ASR setting for voice search over nine languages show that the
joint use of the proposed methods can result in 4%--5% relative word error rate
reductions with only a few million extra parameters.",None,-1
MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning,0.00815827,"The deep learning community has witnessed an exponentially growing interest
in self-supervised learning (SSL). However, it still remains unexplored how to
build a framework for learning useful representations of raw music waveforms in
a self-supervised manner. In this work, we design Music2Vec, a framework
exploring different SSL algorithmic components and tricks for music audio
recordings. Our model achieves comparable results to the state-of-the-art
(SOTA) music SSL model Jukebox, despite being significantly smaller with less
than 2% of parameters of the latter. The model will be released on
Huggingface(Please refer to: https://huggingface.co/m-a-p/music2vec-v1)",https://huggingface.co/m-a-p/music2vec-v1,-1
Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors,0.210853,"Video game testing requires game-specific knowledge as well as common sense
reasoning about the events in the game. While AI-driven agents can satisfy the
first requirement, it is not yet possible to meet the second requirement
automatically. Therefore, video game testing often still relies on manual
testing, and human testers are required to play the game thoroughly to detect
bugs. As a result, it is challenging to fully automate game testing. In this
study, we explore the possibility of leveraging the zero-shot capabilities of
large language models for video game bug detection. By formulating the bug
detection problem as a question-answering task, we show that large language
models can identify which event is buggy in a sequence of textual descriptions
of events from a game. To this end, we introduce the GameBugDescriptions
benchmark dataset, which consists of 167 buggy gameplay videos and a total of
334 question-answer pairs across 8 games. We extensively evaluate the
performance of six models across the OPT and InstructGPT large language model
families on our benchmark dataset. Our results show promising results for
employing language models to detect video game bugs. With the proper prompting
technique, we could achieve an accuracy of 70.66%, and on some video games, up
to 78.94%. Our code, evaluation data and the benchmark can be found on
https://asgaardlab.github.io/LLMxBugs",https://github.com/facebookresearch/metaseq/,-1
RCL: Recurrent Continuous Localization for Temporal Action Detection,0.242714,"Temporal representation is the cornerstone of modern action detection
techniques. State-of-the-art methods mostly rely on a dense anchoring scheme,
where anchors are sampled uniformly over the temporal domain with a discretized
grid, and then regress the accurate boundaries. In this paper, we revisit this
foundational stage and introduce Recurrent Continuous Localization (RCL), which
learns a fully continuous anchoring representation. Specifically, the proposed
representation builds upon an explicit model conditioned with video embeddings
and temporal coordinates, which ensure the capability of detecting segments
with arbitrary length. To optimize the continuous representation, we develop an
effective scale-invariant sampling strategy and recurrently refine the
prediction in subsequent iterations. Our continuous anchoring scheme is fully
differentiable, allowing to be seamlessly integrated into existing detectors,
e.g., BMN and G-TAD. Extensive experiments on two benchmarks demonstrate that
our continuous representation steadily surpasses other discretized counterparts
by ~2% mAP. As a result, RCL achieves 52.92% mAP@0.5 on THUMOS14 and 37.65% mAP
on ActivtiyNet v1.3, outperforming all existing single-model detectors.",None,-1
Revisiting Neural Scaling Laws in Language and Vision,0.262991,"The remarkable progress in deep learning in recent years is largely driven by
improvements in scale, where bigger models are trained on larger datasets for
longer schedules. To predict the benefit of scale empirically, we argue for a
more rigorous methodology based on the extrapolation loss, instead of reporting
the best-fitting (interpolating) parameters. We then present a recipe for
estimating scaling law parameters reliably from learning curves. We demonstrate
that it extrapolates more accurately than previous methods in a wide range of
architecture families across several domains, including image classification,
neural machine translation (NMT) and language modeling, in addition to tasks
from the BIG-Bench evaluation benchmark. Finally, we release a benchmark
dataset comprising of 90 evaluation tasks to facilitate research in this
domain.",https://github.com/google-research/revisiting_neural_scaling_laws,-1
Adaptive Local-Component-aware Graph Convolutional Network for One-shot Skeleton-based Action Recognition,0.0236498,"Skeleton-based action recognition receives increasing attention because the
skeleton representations reduce the amount of training data by eliminating
visual information irrelevant to actions. To further improve the sample
efficiency, meta-learning-based one-shot learning solutions were developed for
skeleton-based action recognition. These methods find the nearest neighbor
according to the similarity between instance-level global average embedding.
However, such measurement holds unstable representativity due to inadequate
generalized learning on local invariant and noisy features, while intuitively,
more fine-grained recognition usually relies on determining key local body
movements. To address this limitation, we present the Adaptive
Local-Component-aware Graph Convolutional Network, which replaces the
comparison metric with a focused sum of similarity measurements on aligned
local embedding of action-critical spatial/temporal segments. Comprehensive
one-shot experiments on the public benchmark of NTU-RGB+D 120 indicate that our
method provides a stronger representation than the global embedding and helps
our model reach state-of-the-art.",None,-1
uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers,0.276262,"The task of Chinese Spelling Check (CSC) is aiming to detect and correct
spelling errors that can be found in the text. While manually annotating a
high-quality dataset is expensive and time-consuming, thus the scale of the
training dataset is usually very small (e.g., SIGHAN15 only contains 2339
samples for training), therefore supervised-learning based models usually
suffer the data sparsity limitation and over-fitting issue, especially in the
era of big language models. In this paper, we are dedicated to investigating
the \textbf{unsupervised} paradigm to address the CSC problem and we propose a
framework named \textbf{uChecker} to conduct unsupervised spelling error
detection and correction. Masked pretrained language models such as BERT are
introduced as the backbone model considering their powerful language diagnosis
capability. Benefiting from the various and flexible MASKing operations, we
propose a Confusionset-guided masking strategy to fine-train the masked
language model to further improve the performance of unsupervised detection and
correction. Experimental results on standard datasets demonstrate the
effectiveness of our proposed model uChecker in terms of character-level and
sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of spelling
error detection and correction respectively.",https://github.com/iqiyi/FASPell,-1
Multi-Event-Camera Depth Estimation and Outlier Rejection by Refocused Events Fusion,0.268437,"Event cameras are bio-inspired sensors that offer advantages over traditional
cameras. They operate asynchronously, sampling the scene at microsecond
resolution and producing a stream of brightness changes. This unconventional
output has sparked novel computer vision methods to unlock the camera's
potential. Here, the problem of event-based stereo 3D reconstruction for SLAM
is considered. Most event-based stereo methods attempt to exploit the high
temporal resolution of the camera and the simultaneity of events across cameras
to establish matches and estimate depth. By contrast, this work investigates
how to estimate depth without explicit data association by fusing Disparity
Space Images (DSIs) originated in efficient monocular methods. Fusion theory is
developed and applied to design multi-camera 3D reconstruction algorithms that
produce state-of-the-art results, as confirmed by comparisons with four
baseline methods and tests on a variety of available datasets.",https://github.com/tub-rip/dvs mcemvs,-1
Learning Perception-Aware Agile Flight in Cluttered Environments,0.517162,"Recently, neural control policies have outperformed existing model-based
planning-and-control methods for autonomously navigating quadrotors through
cluttered environments in minimum time. However, they are not perception aware,
a crucial requirement in vision-based navigation due to the camera's limited
field of view and the underactuated nature of a quadrotor. We propose a
learning-based system that achieves perception-aware, agile flight in cluttered
environments. Our method combines imitation learning with reinforcement
learning (RL) by leveraging a privileged learning-by-cheating framework. Using
RL, we first train a perception-aware teacher policy with full-state
information to fly in minimum time through cluttered environments. Then, we use
imitation learning to distill its knowledge into a vision-based student policy
that only perceives the environment via a camera. Our approach tightly couples
perception and control, showing a significant advantage in computation speed
(10 times faster) and success rate. We demonstrate the closed-loop control
performance using hardware-in-the-loop simulation.",None,-1
Denoising Diffusion Error Correction Codes,0.216446,"Error correction code (ECC) is an integral part of the physical communication
layer, ensuring reliable data transfer over noisy channels. Recently, neural
decoders have demonstrated their advantage over classical decoding techniques.
However, recent state-of-the-art neural decoders suffer from high complexity
and lack the important iterative scheme characteristic of many legacy decoders.
In this work, we propose to employ denoising diffusion models for the soft
decoding of linear codes at arbitrary block lengths. Our framework models the
forward channel corruption as a series of diffusion steps that can be reversed
iteratively. Three contributions are made: (i) a diffusion process suitable for
the decoding setting is introduced, (ii) the neural diffusion decoder is
conditioned on the number of parity errors, which indicates the level of
corruption at a given step, (iii) a line search procedure based on the code's
syndrome obtains the optimal reverse diffusion step size. The proposed approach
demonstrates the power of diffusion models for ECC and is able to achieve state
of the art accuracy, outperforming the other neural decoders by sizable
margins, even for a single reverse diffusion step.",None,-1
"Interventions, Where and How? Experimental Design for Causal Models at Scale",0.0423797,"Causal discovery from observational and interventional data is challenging
due to limited data and non-identifiability: factors that introduce uncertainty
in estimating the underlying structural causal model (SCM). Selecting
experiments (interventions) based on the uncertainty arising from both factors
can expedite the identification of the SCM. Existing methods in experimental
design for causal discovery from limited data either rely on linear assumptions
for the SCM or select only the intervention target. This work incorporates
recent advances in Bayesian causal discovery into the Bayesian optimal
experimental design framework, allowing for active causal discovery of large,
nonlinear SCMs while selecting both the interventional target and the value. We
demonstrate the performance of the proposed method on synthetic graphs
(Erdos-R\`enyi, Scale Free) for both linear and nonlinear SCMs as well as on
the \emph{in-silico} single-cell gene regulatory network dataset, DREAM.",https://github.com/yannadani/cbed,-1
BoxGraph: Semantic Place Recognition and Pose Estimation from 3D LiDAR,0.108624,"This paper is about extremely robust and lightweight localisation using LiDAR
point clouds based on instance segmentation and graph matching. We model 3D
point clouds as fully-connected graphs of semantically identified components
where each vertex corresponds to an object instance and encodes its shape.
Optimal vertex association across graphs allows for full 6-Degree-of-Freedom
(DoF) pose estimation and place recognition by measuring similarity. This
representation is very concise, condensing the size of maps by a factor of 25
against the state-of-the-art, requiring only 3kB to represent a 1.4MB laser
scan. We verify the efficacy of our system on the SemanticKITTI dataset, where
we achieve a new state-of-the-art in place recognition, with an average of
88.4% recall at 100% precision where the next closest competitor follows with
64.9%. We also show accurate metric pose estimation performance - estimating
6-DoF pose with median errors of 10 cm and 0.33 deg.",None,-1
GaIA: Graphical Information Gain based Attention Network for Weakly Supervised Point Cloud Semantic Segmentation,0.110652,"While point cloud semantic segmentation is a significant task in 3D scene
understanding, this task demands a time-consuming process of fully annotating
labels. To address this problem, recent studies adopt a weakly supervised
learning approach under the sparse annotation. Different from the existing
studies, this study aims to reduce the epistemic uncertainty measured by the
entropy for a precise semantic segmentation. We propose the graphical
information gain based attention network called GaIA, which alleviates the
entropy of each point based on the reliable information. The graphical
information gain discriminates the reliable point by employing relative entropy
between target point and its neighborhoods. We further introduce anchor-based
additive angular margin loss, ArcPoint. The ArcPoint optimizes the unlabeled
points containing high entropy towards semantically similar classes of the
labeled points on hypersphere space. Experimental results on S3DIS and
ScanNet-v2 datasets demonstrate our framework outperforms the existing weakly
supervised methods. We have released GaIA at https://github.com/Karel911/GaIA.",https://github.com/Karel911/GaIA,-1
Towards Understanding Mixture of Experts in Deep Learning,0.218993,"The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by
a router, has achieved great success in deep learning. However, the
understanding of such architecture remains elusive. In this paper, we formally
study how the MoE layer improves the performance of neural network learning and
why the mixture model will not collapse into a single model. Our empirical
results suggest that the cluster structure of the underlying problem and the
non-linearity of the expert are pivotal to the success of MoE. To further
understand this, we consider a challenging classification problem with
intrinsic cluster structures, which is hard to learn using a single expert. Yet
with the MoE layer, by choosing the experts as two-layer nonlinear
convolutional neural networks (CNNs), we show that the problem can be learned
successfully. Furthermore, our theory shows that the router can learn the
cluster-center features, which helps divide the input complex problem into
simpler linear classification sub-problems that individual experts can conquer.
To our knowledge, this is the first result towards formally understanding the
mechanism of the MoE layer for deep learning.",https://github.com/TheophileBlard/french-sentiment-analysis-with-bert,-1
HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator,0.507052,"Video prediction is an important yet challenging problem; burdened with the
tasks of generating future frames and learning environment dynamics. Recently,
autoregressive latent video models have proved to be a powerful video
prediction tool, by separating the video prediction into two sub-problems:
pre-training an image generator model, followed by learning an autoregressive
prediction model in the latent space of the image generator. However,
successfully generating high-fidelity and high-resolution videos has yet to be
seen. In this work, we investigate how to train an autoregressive latent video
prediction model capable of predicting high-fidelity future frames with minimal
modification to existing models, and produce high-resolution (256x256) videos.
Specifically, we scale up prior models by employing a high-fidelity image
generator (VQ-GAN) with a causal transformer model, and introduce additional
techniques of top-k sampling and data augmentation to further improve video
prediction quality. Despite the simplicity, the proposed method achieves
competitive performance to state-of-the-art approaches on standard video
prediction benchmarks with fewer parameters, and enables high-resolution video
prediction on complex and large-scale datasets. Videos are available at
https://sites.google.com/view/harp-videos/home.",None,-1
MotionBERT: A Unified Perspective on Learning Human Motion Representations,0.175834,"We present a unified perspective on tackling various human-centric video
tasks by learning human motion representations from large-scale and
heterogeneous data resources. Specifically, we propose a pretraining stage in
which a motion encoder is trained to recover the underlying 3D motion from
noisy partial 2D observations. The motion representations acquired in this way
incorporate geometric, kinematic, and physical knowledge about human motion,
which can be easily transferred to multiple downstream tasks. We implement the
motion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer)
neural network. It could capture long-range spatio-temporal relationships among
the skeletal joints comprehensively and adaptively, exemplified by the lowest
3D pose estimation error so far when trained from scratch. Furthermore, our
proposed framework achieves state-of-the-art performance on all three
downstream tasks by simply finetuning the pretrained motion encoder with a
simple regression head (1-2 layers), which demonstrates the versatility of the
learned motion representations. Code and models are available at
https://motionbert.github.io/",https://motionbert.github.io/,-1
OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering,0.295996,"The information in tables can be an important complement to text, making
table-based question answering (QA) systems of great value. The intrinsic
complexity of handling tables often adds an extra burden to both model design
and data annotation. In this paper, we aim to develop a simple table-based QA
model with minimal annotation effort. Motivated by the fact that table-based QA
requires both alignment between questions and tables and the ability to perform
complicated reasoning over multiple table elements, we propose an omnivorous
pretraining approach that consumes both natural and synthetic data to endow
models with these respective abilities. Specifically, given freely available
tables, we leverage retrieval to pair them with relevant natural sentences for
mask-based pretraining, and synthesize NL questions by converting SQL sampled
from tables for pretraining with a QA loss. We perform extensive experiments in
both few-shot and full settings, and the results clearly demonstrate the
superiority of our model OmniTab, with the best multitasking approach achieving
an absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively,
also establishing a new state-of-the-art on WikiTableQuestions. Detailed
ablations and analyses reveal different characteristics of natural and
synthetic data, shedding light on future directions in omnivorous pretraining.
Code, pretraining data, and pretrained models are available at
https://github.com/jzbjyb/OmniTab.",https://github.com/jzbjyb/OmniTab,-1
Fast Object Placement Assessment,0.265177,"Object placement assessment (OPA) aims to predict the rationality score of a
composite image in terms of the placement (e.g., scale, location) of inserted
foreground object. However, given a pair of scaled foreground and background,
to enumerate all the reasonable locations, existing OPA model needs to place
the foreground at each location on the background and pass the obtained
composite image through the model one at a time, which is very time-consuming.
In this work, we investigate a new task named as fast OPA. Specifically,
provided with a scaled foreground and a background, we only pass them through
the model once and predict the rationality scores for all locations. To
accomplish this task, we propose a pioneering fast OPA model with several
innovations (i.e., foreground dynamic filter, background prior transfer, and
composite feature mimicking) to bridge the performance gap between slow OPA
model and fast OPA model. Extensive experiments on OPA dataset show that our
proposed fast OPA model performs on par with slow OPA model but runs
significantly faster.",None,-1
DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection,0.440646,"Graph Anomaly Detection (GAD) has recently become a hot research spot due to
its practicability and theoretical value. Since GAD emphasizes the application
and the rarity of anomalous samples, enriching the varieties of its datasets is
fundamental work. Thus, this paper present DGraph, a real-world dynamic graph
in the finance domain. DGraph overcomes many limitations of current GAD
datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth
nodes. We provide a comprehensive observation of DGraph, revealing that
anomalous nodes and normal nodes generally have different structures, neighbor
distribution, and temporal dynamics. Moreover, it suggests that unlabeled nodes
are also essential for detecting fraudsters. Furthermore, we conduct extensive
experiments on DGraph. Observation and experiments demonstrate that DGraph is
propulsive to advance GAD research and enable in-depth exploration of anomalous
nodes.",https://github.com/hxttkl/DGraph_Experiments,-1
DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation,0.242177,"Natural language processing (NLP) algorithms have become very successful, but
they still struggle when applied to out-of-distribution examples. In this paper
we propose a controllable generation approach in order to deal with this domain
adaptation (DA) challenge. Given an input text example, our DoCoGen algorithm
generates a domain-counterfactual textual example (D-con) - that is similar to
the original in all aspects, including the task label, but its domain is
changed to a desired one. Importantly, DoCoGen is trained using only unlabeled
examples from multiple domains - no NLP task labels or parallel pairs of
textual examples and their domain-counterfactuals are required. We show that
DoCoGen can generate coherent counterfactuals consisting of multiple sentences.
We use the D-cons generated by DoCoGen to augment a sentiment classifier and a
multi-label intent classifier in 20 and 78 DA setups, respectively, where
source-domain labeled data is scarce. Our model outperforms strong baselines
and improves the accuracy of a state-of-the-art unsupervised DA algorithm.",https://github.com/nitaytech/DoCoGen,-1
Linear Convergence of Natural Policy Gradient Methods with Log-Linear Policies,0.117665,"We consider infinite-horizon discounted Markov decision processes and study
the convergence rates of the natural policy gradient (NPG) and the Q-NPG
methods with the log-linear policy class. Using the compatible function
approximation framework, both methods with log-linear policies can be written
as inexact versions of the policy mirror descent (PMD) method. We show that
both methods attain linear convergence rates and
$\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexities using a simple,
non-adaptive geometrically increasing step size, without resorting to entropy
or other strongly convex regularization. Lastly, as a byproduct, we obtain
sublinear convergence rates for both methods with arbitrary constant step size.",None,-1
Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning,0.0813961,"Advances in artificial intelligence often stem from the development of new
environments that abstract real-world situations into a form where research can
be done conveniently. This paper contributes such an environment based on ideas
inspired by elementary Microeconomics. Agents learn to produce resources in a
spatially complex world, trade them with one another, and consume those that
they prefer. We show that the emergent production, consumption, and pricing
behaviors respond to environmental conditions in the directions predicted by
supply and demand shifts in Microeconomics. We also demonstrate settings where
the agents' emergent prices for goods vary over space, reflecting the local
abundance of goods. After the price disparities emerge, some agents then
discover a niche of transporting goods between regions with different
prevailing prices -- a profitable strategy because they can buy goods where
they are cheap and sell them where they are expensive. Finally, in a series of
ablation experiments, we investigate how choices in the environmental rewards,
bartering actions, agent architecture, and ability to consume tradable goods
can either aid or inhibit the emergence of this economic behavior. This work is
part of the environment development branch of a research program that aims to
build human-like artificial general intelligence through multi-agent
interactions in simulated societies. By exploring which environment features
are needed for the basic phenomena of elementary microeconomics to emerge
automatically from learning, we arrive at an environment that differs from
those studied in prior multi-agent reinforcement learning work along several
dimensions. For example, the model incorporates heterogeneous tastes and
physical abilities, and agents negotiate with one another as a grounded form of
communication.",https://github.com/deepmind/meltingpot,-1
Housekeep: Tidying Virtual Households using Commonsense Reasoning,0.493202,"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the
home for embodied AI. In Housekeep, an embodied agent must tidy a house by
rearranging misplaced objects without explicit instructions specifying which
objects need to be rearranged. Instead, the agent must learn from and is
evaluated against human preferences of which objects belong where in a tidy
house. Specifically, we collect a dataset of where humans typically place
objects in tidy and untidy houses constituting 1799 objects, 268 object
categories, 585 placements, and 105 rooms. Next, we propose a modular baseline
approach for Housekeep that integrates planning, exploration, and navigation.
It leverages a fine-tuned large language model (LLM) trained on an internet
text corpus for effective planning. We show that our baseline agent generalizes
to rearranging unseen objects in unknown environments. See our webpage for more
details: https://yashkant.github.io/housekeep/",None,-1
Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue,0.18425,"Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question
regarding a given video and dialogue context. Despite the recent success of
multi-modal reasoning to generate answer sentences, existing dialogue systems
still suffer from a text hallucination problem, which denotes indiscriminate
text-copying from input texts without an understanding of the question. This is
due to learning spurious correlations from the fact that answer sentences in
the dataset usually include the words of input texts, thus the VGD system
excessively relies on copying words from input texts by hoping those words to
overlap with ground-truth texts. Hence, we design Text Hallucination Mitigating
(THAM) framework, which incorporates Text Hallucination Regularization (THR)
loss derived from the proposed information-theoretic text hallucination
measurement approach. Applying THAM with current dialogue systems validates the
effectiveness on VGD benchmarks (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows
enhanced interpretability.",https://github.com/dialogtekgeek/DSTC8-AVSD_ofﬁcial,-1
Structured access: an emerging paradigm for safe AI deployment,0.0630847,"Structured access is an emerging paradigm for the safe deployment of
artificial intelligence (AI). Instead of openly disseminating AI systems,
developers facilitate controlled, arm's length interactions with their AI
systems. The aim is to prevent dangerous AI capabilities from being widely
accessible, whilst preserving access to AI capabilities that can be used
safely. The developer must both restrict how the AI system can be used, and
prevent the user from circumventing these restrictions through modification or
reverse engineering of the AI system. Structured access is most effective when
implemented through cloud-based AI services, rather than disseminating AI
software that runs locally on users' hardware. Cloud-based interfaces provide
the AI developer greater scope for controlling how the AI system is used, and
for protecting against unauthorized modifications to the system's design. This
chapter expands the discussion of ""publication norms"" in the AI community,
which to date has focused on the question of how the informational content of
AI research projects should be disseminated (e.g., code and models). Although
this is an important question, there are limits to what can be achieved through
the control of information flows. Structured access views AI software not only
as information that can be shared but also as a tool with which users can have
arm's length interactions. There are early examples of structured access being
practiced by AI developers, but there is much room for further development,
both in the functionality of cloud-based interfaces and in the wider
institutional framework.",None,-1
Roadmap for Cybersecurity in Autonomous Vehicles,0.141675,"Autonomous vehicles are on the horizon and will be transforming
transportation safety and comfort. These vehicles will be connected to various
external systems and utilize advanced embedded systems to perceive their
environment and make intelligent decisions. However, this increased
connectivity makes these vehicles vulnerable to various cyber-attacks that can
have catastrophic effects. Attacks on automotive systems are already on the
rise in today's vehicles and are expected to become more commonplace in future
autonomous vehicles. Thus, there is a need to strengthen cybersecurity in
future autonomous vehicles. In this article, we discuss major automotive
cyber-attacks over the past decade and present state-of-the-art solutions that
leverage artificial intelligence (AI). We propose a roadmap towards building
secure autonomous vehicles and highlight key open challenges that need to be
addressed.",None,-1
Cross-Modality High-Frequency Transformer for MR Image Super-Resolution,0.36427,"Improving the resolution of magnetic resonance (MR) image data is critical to
computer-aided diagnosis and brain function analysis. Higher resolution helps
to capture more detailed content, but typically induces to lower
signal-to-noise ratio and longer scanning time. To this end, MR image
super-resolution has become a widely-interested topic in recent times. Existing
works establish extensive deep models with the conventional architectures based
on convolutional neural networks (CNN). In this work, to further advance this
research field, we make an early effort to build a Transformer-based MR image
super-resolution framework, with careful designs on exploring valuable domain
prior knowledge. Specifically, we consider two-fold domain priors including the
high-frequency structure prior and the inter-modality context prior, and
establish a novel Transformer architecture, called Cross-modality
high-frequency Transformer (Cohf-T), to introduce such priors into
super-resolving the low-resolution (LR) MR images. Experiments on two datasets
indicate that Cohf-T achieves new state-of-the-art performance.",None,-1
Simple Techniques Work Surprisingly Well for Neural Network Test Prioritization and Active Learning (Replicability Study),0.496294,"Test Input Prioritizers (TIP) for Deep Neural Networks (DNN) are an important
technique to handle the typically very large test datasets efficiently, saving
computation and labeling costs. This is particularly true for large-scale,
deployed systems, where inputs observed in production are recorded to serve as
potential test or training data for the next versions of the system. Feng et.
al. propose DeepGini, a very fast and simple TIP, and show that it outperforms
more elaborate techniques such as neuron- and surprise coverage. In a
large-scale study (4 case studies, 8 test datasets, 32'200 trained models) we
verify their findings. However, we also find that other comparable or even
simpler baselines from the field of uncertainty quantification, such as the
predicted softmax likelihood or the entropy of the predicted softmax
likelihoods perform equally well as DeepGini.",https://github.com/testingautomated-usi/simple-tip,-1
Towards Sequence-Level Training for Visual Tracking,0.120183,"Despite the extensive adoption of machine learning on the task of visual
object tracking, recent learning-based approaches have largely overlooked the
fact that visual tracking is a sequence-level task in its nature; they rely
heavily on frame-level training, which inevitably induces inconsistency between
training and testing in terms of both data distributions and task objectives.
This work introduces a sequence-level training strategy for visual tracking
based on reinforcement learning and discusses how a sequence-level design of
data sampling, learning objectives, and data augmentation can improve the
accuracy and robustness of tracking algorithms. Our experiments on standard
benchmarks including LaSOT, TrackingNet, and GOT-10k demonstrate that four
representative tracking models, SiamRPN++, SiamAttn, TransT, and TrDiMP,
consistently improve by incorporating the proposed methods in training without
modifying architectures.",https://github.com/byminji/SLTtrack,-1
Neural-Symbolic Entangled Framework for Complex Query Answering,0.64732,"Answering complex queries over knowledge graphs (KG) is an important yet
challenging task because of the KG incompleteness issue and cascading errors
during reasoning. Recent query embedding (QE) approaches to embed the entities
and relations in a KG and the first-order logic (FOL) queries into a low
dimensional space, answering queries by dense similarity search. However,
previous works mainly concentrate on the target answers, ignoring intermediate
entities' usefulness, which is essential for relieving the cascading error
problem in logical query answering. In addition, these methods are usually
designed with their own geometric or distributional embeddings to handle
logical operators like union, intersection, and negation, with the sacrifice of
the accuracy of the basic operator - projection, and they could not absorb
other embedding methods to their models. In this work, we propose a Neural and
Symbolic Entangled framework (ENeSy) for complex query answering, which enables
the neural and symbolic reasoning to enhance each other to alleviate the
cascading error and KG incompleteness. The projection operator in ENeSy could
be any embedding method with the capability of link prediction, and the other
FOL operators are handled without parameters. With both neural and symbolic
reasoning results contained, ENeSy answers queries in ensembles. ENeSy achieves
the SOTA performance on several benchmarks, especially in the setting of the
training model only with the link prediction task.",None,-1
BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate Speech in Different Social Contexts,0.18733,"Social media platforms and online streaming services have spawned a new breed
of Hate Speech (HS). Due to the massive amount of user-generated content on
these sites, modern machine learning techniques are found to be feasible and
cost-effective to tackle this problem. However, linguistically diverse datasets
covering different social contexts in which offensive language is typically
used are required to train generalizable models. In this paper, we identify the
shortcomings of existing Bangla HS datasets and introduce a large manually
labeled dataset BD-SHS that includes HS in different social contexts. The
labeling criteria were prepared following a hierarchical annotation process,
which is the first of its kind in Bangla HS to the best of our knowledge. The
dataset includes more than 50,200 offensive comments crawled from online social
networking sites and is at least 60% larger than any existing Bangla HS
datasets. We present the benchmark result of our dataset by training different
NLP models resulting in the best one achieving an F1-score of 91.0%. In our
experiments, we found that a word embedding trained exclusively using 1.47
million comments from social media and streaming sites consistently resulted in
better modeling of HS detection in comparison to other pre-trained embeddings.
Our dataset and all accompanying codes is publicly available at
github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media",https://github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media,-1
Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences,0.376681,"We propose Probabilistic Warp Consistency, a weakly-supervised learning
objective for semantic matching. Our approach directly supervises the dense
matching scores predicted by the network, encoded as a conditional probability
distribution. We first construct an image triplet by applying a known warp to
one of the images in a pair depicting different instances of the same object
class. Our probabilistic learning objectives are then derived using the
constraints arising from the resulting image triplet. We further account for
occlusion and background clutter present in real image pairs by extending our
probabilistic output space with a learnable unmatched state. To supervise it,
we design an objective between image pairs depicting different object classes.
We validate our method by applying it to four recent semantic matching
architectures. Our weakly-supervised approach sets a new state-of-the-art on
four challenging semantic matching benchmarks. Lastly, we demonstrate that our
objective also brings substantial improvements in the strongly-supervised
regime, when combined with keypoint annotations.",https://github.com/PruneTruong/DenseMatching,-1
Optimal estimation of Gaussian DAG models,0.0375777,"We study the optimal sample complexity of learning a Gaussian directed
acyclic graph (DAG) from observational data. Our main results establish the
minimax optimal sample complexity for learning the structure of a linear
Gaussian DAG model in two settings of interest: 1) Under equal variances
without knowledge of the true ordering, and 2) For general linear models given
knowledge of the ordering. In both cases the sample complexity is $n\asymp
q\log(d/q)$, where $q$ is the maximum number of parents and $d$ is the number
of nodes. We further make comparisons with the classical problem of learning
(undirected) Gaussian graphical models, showing that under the equal variance
assumption, these two problems share the same optimal sample complexity. In
other words, at least for Gaussian models with equal error variances, learning
a directed graphical model is statistically no more difficult than learning an
undirected graphical model. Our results also extend to more general
identification assumptions as well as subgaussian errors.",https://github.com/WY-Chen/EqVarDAG/blob/master/R/EqVarDAG_HD_TD.R,-1
FedX: Unsupervised Federated Learning with Cross Knowledge Distillation,0.317423,"This paper presents FedX, an unsupervised federated learning framework. Our
model learns unbiased representation from decentralized and heterogeneous local
data. It employs a two-sided knowledge distillation with contrastive learning
as a core component, allowing the federated system to function without
requiring clients to share any data features. Furthermore, its adaptable
architecture can be used as an add-on module for existing unsupervised
algorithms in federated settings. Experiments show that our model improves
performance significantly (1.58--5.52pp) on five unsupervised algorithms.",https://github.com/Sungwon-Han/FEDX,-1
Self-Supervised Equivariant Learning for Oriented Keypoint Detection,0.123275,"Detecting robust keypoints from an image is an integral part of many computer
vision problems, and the characteristic orientation and scale of keypoints play
an important role for keypoint description and matching. Existing
learning-based methods for keypoint detection rely on standard
translation-equivariant CNNs but often fail to detect reliable keypoints
against geometric variations. To learn to detect robust oriented keypoints, we
introduce a self-supervised learning framework using rotation-equivariant CNNs.
We propose a dense orientation alignment loss by an image pair generated by
synthetic transformations for training a histogram-based orientation map. Our
method outperforms the previous methods on an image matching benchmark and a
camera pose estimation benchmark.",None,-1
Linear Leaky-Integrate-and-Fire Neuron Model Based Spiking Neural Networks and Its Mapping Relationship to Deep Neural Networks,0.0843602,"Spiking neural networks (SNNs) are brain-inspired machine learning algorithms
with merits such as biological plausibility and unsupervised learning
capability. Previous works have shown that converting Artificial Neural
Networks (ANNs) into SNNs is a practical and efficient approach for
implementing an SNN. However, the basic principle and theoretical groundwork
are lacking for training a non-accuracy-loss SNN. This paper establishes a
precise mathematical mapping between the biological parameters of the Linear
Leaky-Integrate-and-Fire model (LIF)/SNNs and the parameters of ReLU-AN/Deep
Neural Networks (DNNs). Such mapping relationship is analytically proven under
certain conditions and demonstrated by simulation and real data experiments. It
can serve as the theoretical basis for the potential combination of the
respective merits of the two categories of neural networks.",None,-1
FvOR: Robust Joint Shape and Pose Optimization for Few-view Object Reconstruction,0.148293,"Reconstructing an accurate 3D object model from a few image observations
remains a challenging problem in computer vision. State-of-the-art approaches
typically assume accurate camera poses as input, which could be difficult to
obtain in realistic settings. In this paper, we present FvOR, a learning-based
object reconstruction method that predicts accurate 3D models given a few
images with noisy input poses. The core of our approach is a fast and robust
multi-view reconstruction algorithm to jointly refine 3D geometry and camera
pose estimation using learnable neural network modules. We provide a thorough
benchmark of state-of-the-art approaches for this problem on ShapeNet. Our
approach achieves best-in-class results. It is also two orders of magnitude
faster than the recent optimization-based approach IDR. Our code is released at
\url{https://github.com/zhenpeiyang/FvOR/}",https://github.com/zhenpeiyang/FvOR/,-1
Asynchronous Optimisation for Event-based Visual Odometry,0.0758846,"Event cameras open up new possibilities for robotic perception due to their
low latency and high dynamic range. On the other hand, developing effective
event-based vision algorithms that fully exploit the beneficial properties of
event cameras remains work in progress. In this paper, we focus on event-based
visual odometry (VO). While existing event-driven VO pipelines have adopted
continuous-time representations to asynchronously process event data, they
either assume a known map, restrict the camera to planar trajectories, or
integrate other sensors into the system. Towards map-free event-only monocular
VO in SE(3), we propose an asynchronous structure-from-motion optimisation
back-end. Our formulation is underpinned by a principled joint optimisation
problem involving non-parametric Gaussian Process motion modelling and
incremental maximum a posteriori inference. A high-performance incremental
computation engine is employed to reason about the camera trajectory with every
incoming event. We demonstrate the robustness of our asynchronous back-end in
comparison to frame-based methods which depend on accurate temporal
accumulation of measurements.",None,-1
DICE: Data-Efficient Clinical Event Extraction with Generative Models,0.283114,"Event extraction for the clinical domain is an under-explored research area.
The lack of training data along with the high volume of domain-specific
terminologies with vague entity boundaries makes the task especially
challenging. In this paper, we introduce DICE, a robust and data-efficient
generative model for clinical event extraction. DICE frames event extraction as
a conditional generation problem and introduces a contrastive learning
objective to accurately decide the boundaries of biomedical mentions. DICE also
trains an auxiliary mention identification task jointly with event extraction
tasks to better identify entity mention boundaries, and further introduces
special markers to incorporate identified entity mentions as trigger and
argument candidates for their respective tasks. To benchmark clinical event
extraction, we compose MACCROBAT-EE, the first clinical event extraction
dataset with argument annotation, based on an existing clinical information
extraction dataset MACCROBAT. Our experiments demonstrate state-of-the-art
performances of DICE for clinical and news domain event extraction, especially
under low data settings.",None,-1
Understanding Zero-Shot Adversarial Robustness for Large-Scale Models,0.397348,"Pretrained large-scale vision-language models like CLIP have exhibited strong
generalization over unseen tasks. Yet imperceptible adversarial perturbations
can significantly reduce CLIP's performance on new tasks. In this work, we
identify and explore the problem of \emph{adapting large-scale models for
zero-shot adversarial robustness}. We first identify two key factors during
model adaption -- training losses and adaptation methods -- that affect the
model's zero-shot adversarial robustness. We then propose a text-guided
contrastive adversarial training loss, which aligns the text embeddings and the
adversarial visual features with contrastive learning on a small set of
training data. We apply this training loss to two adaption methods, model
finetuning and visual prompt tuning. We find that visual prompt tuning is more
effective in the absence of texts, while finetuning wins in the existence of
text guidance. Overall, our approach significantly improves the zero-shot
adversarial robustness over CLIP, seeing an average improvement of over 31
points over ImageNet and 15 zero-shot datasets. We hope this work can shed
light on understanding the zero-shot adversarial robustness of large-scale
models.",https://github.com/cvlab-columbia/ZSRobust4FoundationModel,-1
Towards Stroke Patients' Upper-limb Automatic Motor Assessment Using Smartwatches,0.0996696,"Assessing the physical condition in rehabilitation scenarios is a challenging
problem, since it involves Human Activity Recognition (HAR) and kinematic
analysis methods. In addition, the difficulties increase in unconstrained
rehabilitation scenarios, which are much closer to the real use cases. In
particular, our aim is to design an upper-limb assessment pipeline for stroke
patients using smartwatches. We focus on the HAR task, as it is the first part
of the assessing pipeline. Our main target is to automatically detect and
recognize four key movements inspired by the Fugl-Meyer assessment scale, which
are performed in both constrained and unconstrained scenarios. In addition to
the application protocol and dataset, we propose two detection and
classification baseline methods. We believe that the proposed framework,
dataset and baseline results will serve to foster this research field.",None,-1
Optical tracking in team sports,0.0329527,"Sports analysis has gained paramount importance for coaches, scouts, and
fans. Recently, computer vision researchers have taken on the challenge of
collecting the necessary data by proposing several methods of automatic player
and ball tracking. Building on the gathered tracking data, data miners are able
to perform quantitative analysis on the performance of players and teams. With
this survey, our goal is to provide a basic understanding for quantitative data
analysts about the process of creating the input data and the characteristics
thereof. Thus, we summarize the recent methods of optical tracking by providing
a comprehensive taxonomy of conventional and deep learning methods, separately.
Moreover, we discuss the preprocessing steps of tracking, the most common
challenges in this domain, and the application of tracking data to sports
teams. Finally, we compare the methods by their cost and limitations, and
conclude the work by highlighting potential future research directions.",None,-1
"Graph Neural Networks Meet Wireless Communications: Motivation, Applications, and Future Directions",0.0229648,"As an efficient graph analytical tool, graph neural networks (GNNs) have
special properties that are particularly fit for the characteristics and
requirements of wireless communications, exhibiting good potential for the
advancement of next-generation wireless communications. This article aims to
provide a comprehensive overview of the interplay between GNNs and wireless
communications, including GNNs for wireless communications (GNN4Com) and
wireless communications for GNNs (Com4GNN). In particular, we discuss GNN4Com
based on how graphical models are constructed and introduce Com4GNN with
corresponding incentives. We also highlight potential research directions to
promote future research endeavors for GNNs in wireless communications.",None,-1
Unsupervised Image-to-Image Translation with Generative Prior,0.17508,"Unsupervised image-to-image translation aims to learn the translation between
two visual domains without paired data. Despite the recent progress in image
translation models, it remains challenging to build mappings between complex
domains with drastic visual discrepancies. In this work, we present a novel
framework, Generative Prior-guided UNsupervised Image-to-image Translation
(GP-UNIT), to improve the overall quality and applicability of the translation
algorithm. Our key insight is to leverage the generative prior from pre-trained
class-conditional GANs (e.g., BigGAN) to learn rich content correspondences
across various domains. We propose a novel coarse-to-fine scheme: we first
distill the generative prior to capture a robust coarse-level content
representation that can link objects at an abstract semantic level, based on
which fine-level content features are adaptively learned for more accurate
multi-level content correspondences. Extensive experiments demonstrate the
superiority of our versatile framework over state-of-the-art methods in robust,
high-quality and diversified translations, even for challenging and distant
domains.",https://github.com/williamyang1991/GP-UNIT,-1
Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction,0.203006,"When applying multi-instance learning (MIL) to make predictions for bags of
instances, the prediction accuracy of an instance often depends on not only the
instance itself but also its context in the corresponding bag. From the
viewpoint of causal inference, such bag contextual prior works as a confounder
and may result in model robustness and interpretability issues. Focusing on
this problem, we propose a novel interventional multi-instance learning (IMIL)
framework to achieve deconfounded instance-level prediction. Unlike traditional
likelihood-based strategies, we design an Expectation-Maximization (EM)
algorithm based on causal intervention, providing a robust instance selection
in the training phase and suppressing the bias caused by the bag contextual
prior. Experiments on pathological image analysis demonstrate that our IMIL
method substantially reduces false positives and outperforms state-of-the-art
MIL methods.",None,-1
ViTOL: Vision Transformer for Weakly Supervised Object Localization,0.208594,"Weakly supervised object localization (WSOL) aims at predicting object
locations in an image using only image-level category labels. Common challenges
that image classification models encounter when localizing objects are, (a)
they tend to look at the most discriminative features in an image that confines
the localization map to a very small region, (b) the localization maps are
class agnostic, and the models highlight objects of multiple classes in the
same image and, (c) the localization performance is affected by background
noise. To alleviate the above challenges we introduce the following simple
changes through our proposed method ViTOL. We leverage the vision-based
transformer for self-attention and introduce a patch-based attention dropout
layer (p-ADL) to increase the coverage of the localization map and a gradient
attention rollout mechanism to generate class-dependent attention maps. We
conduct extensive quantitative, qualitative and ablation experiments on the
ImageNet-1K and CUB datasets. We achieve state-of-the-art MaxBoxAcc-V2
localization scores of 70.47% and 73.17% on the two datasets respectively. Code
is available on https://github.com/Saurav-31/ViTOL",https://github.com/Saurav-31/ViTOL,-1
SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models,0.193259,"Recent research showed promising results on combining pretrained language
models (LMs) with canonical utterance for few-shot semantic parsing. The
canonical utterance is often lengthy and complex due to the compositional
structure of formal languages. Learning to generate such canonical utterance
requires significant amount of data to reach high performance. Fine-tuning with
only few-shot samples, the LMs can easily forget pretrained knowledge, overfit
spurious biases, and suffer from compositionally out-of-distribution
generalization errors. To tackle these issues, we propose a novel few-shot
semantic parsing method -- SeqZero. SeqZero decomposes the problem into a
sequence of sub-problems, which correspond to the sub-clauses of the formal
language. Based on the decomposition, the LMs only need to generate short
answers using prompts for predicting sub-clauses. Thus, SeqZero avoids
generating a long canonical utterance at once. Moreover, SeqZero employs not
only a few-shot model but also a zero-shot model to alleviate the overfitting.
In particular, SeqZero brings out the merits from both models via ensemble
equipped with our proposed constrained rescaling. SeqZero achieves SOTA
performance of BART-based models on GeoQuery and EcommerceQuery, which are two
few-shot datasets with compositional data split.",https://github.com/amzn/SeqZero,-1
Deepfake Video Detection with Spatiotemporal Dropout Transformer,0.719901,"While the abuse of deepfake technology has caused serious concerns recently,
how to detect deepfake videos is still a challenge due to the high
photo-realistic synthesis of each frame. Existing image-level approaches often
focus on single frame and ignore the spatiotemporal cues hidden in deepfake
videos, resulting in poor generalization and robustness. The key of a
video-level detector is to fully exploit the spatiotemporal inconsistency
distributed in local facial regions across different frames in deepfake videos.
Inspired by that, this paper proposes a simple yet effective patch-level
approach to facilitate deepfake video detection via spatiotemporal dropout
transformer. The approach reorganizes each input video into bag of patches that
is then fed into a vision transformer to achieve robust representation.
Specifically, a spatiotemporal dropout operation is proposed to fully explore
patch-level spatiotemporal cues and serve as effective data augmentation to
further enhance model's robustness and generalization ability. The operation is
flexible and can be easily plugged into existing vision transformers. Extensive
experiments demonstrate the effectiveness of our approach against 25
state-of-the-arts with impressive robustness, generalizability, and
representation ability.",https://github.com/MarekKowalski/FaceSwap/,-1
Learning to Execute Actions or Ask Clarification Questions,0.732715,"Collaborative tasks are ubiquitous activities where a form of communication
is required in order to reach a joint goal. Collaborative building is one of
such tasks. We wish to develop an intelligent builder agent in a simulated
building environment (Minecraft) that can build whatever users wish to build by
just talking to the agent. In order to achieve this goal, such agents need to
be able to take the initiative by asking clarification questions when further
information is needed. Existing works on Minecraft Corpus Dataset only learn to
execute instructions neglecting the importance of asking for clarifications. In
this paper, we extend the Minecraft Corpus Dataset by annotating all builder
utterances into eight types, including clarification questions, and propose a
new builder agent model capable of determining when to ask or execute
instructions. Experimental results show that our model achieves
state-of-the-art performance on the collaborative building task with a
substantial improvement. We also define two new tasks, the learning to ask task
and the joint learning task. The latter consists of solving both collaborating
building and learning to ask tasks jointly.",https://github.com/ZhengxiangShi/LearnToAsk,-1
"Artificial Intelligence for Cybersecurity: Threats, Attacks and Mitigation",0.0300923,"With the advent of the digital era, every day-to-day task is automated due to
technological advances. However, technology has yet to provide people with
enough tools and safeguards. As the internet connects more-and-more devices
around the globe, the question of securing the connected devices grows at an
even spiral rate. Data thefts, identity thefts, fraudulent transactions,
password compromises, and system breaches are becoming regular everyday news.
The surging menace of cyber-attacks got a jolt from the recent advancements in
Artificial Intelligence. AI is being applied in almost every field of different
sciences and engineering. The intervention of AI not only automates a
particular task but also improves efficiency by many folds. So it is evident
that such a scrumptious spread would be very appetizing to cybercriminals. Thus
the conventional cyber threats and attacks are now ``intelligent"" threats. This
article discusses cybersecurity and cyber threats along with both conventional
and intelligent ways of defense against cyber-attacks. Furthermore finally, end
the discussion with the potential prospects of the future of AI in
cybersecurity.",None,-1
"CounterGeDi: A controllable approach to generate polite, detoxified and emotional counterspeech",0.662506,"Recently, many studies have tried to create generation models to assist
counter speakers by providing counterspeech suggestions for combating the
explosive proliferation of online hate. However, since these suggestions are
from a vanilla generation model, they might not include the appropriate
properties required to counter a particular hate speech instance. In this
paper, we propose CounterGeDi - an ensemble of generative discriminators (GeDi)
to guide the generation of a DialoGPT model toward more polite, detoxified, and
emotionally laden counterspeech. We generate counterspeech using three datasets
and observe significant improvement across different attribute scores. The
politeness and detoxification scores increased by around 15% and 6%
respectively, while the emotion in the counterspeech increased by at least 10%
across all the datasets. We also experiment with triple-attribute control and
observe significant improvement over single attribute results when combining
complementing attributes, e.g., politeness, joyfulness and detoxification. In
all these experiments, the relevancy of the generated text does not deteriorate
due to the application of these controls",https://github.com/hate-alert/CounterGEDI,-1
High-Res Facial Appearance Capture from Polarized Smartphone Images,0.0635075,"We propose a novel method for high-quality facial texture reconstruction from
RGB images using a novel capturing routine based on a single smartphone which
we equip with an inexpensive polarization foil. Specifically, we turn the
flashlight into a polarized light source and add a polarization filter on top
of the camera. Leveraging this setup, we capture the face of a subject with
cross-polarized and parallel-polarized light. For each subject, we record two
short sequences in a dark environment under flash illumination with different
light polarization using the modified smartphone. Based on these observations,
we reconstruct an explicit surface mesh of the face using structure from
motion. We then exploit the camera and light co-location within a
differentiable renderer to optimize the facial textures using an
analysis-by-synthesis approach. Our method optimizes for high-resolution normal
textures, diffuse albedo, and specular albedo using a coarse-to-fine
optimization scheme. We show that the optimized textures can be used in a
standard rendering pipeline to synthesize high-quality photo-realistic 3D
digital humans in novel environments.",None,-1
Efficient Knowledge Distillation from Model Checkpoints,0.150972,"Knowledge distillation is an effective approach to learn compact models
(students) with the supervision of large and strong models (teachers). As
empirically there exists a strong correlation between the performance of
teacher and student models, it is commonly believed that a high performing
teacher is preferred. Consequently, practitioners tend to use a well trained
network or an ensemble of them as the teacher. In this paper, we make an
intriguing observation that an intermediate model, i.e., a checkpoint in the
middle of the training procedure, often serves as a better teacher compared to
the fully converged model, although the former has much lower accuracy. More
surprisingly, a weak snapshot ensemble of several intermediate models from a
same training trajectory can outperform a strong ensemble of independently
trained and fully converged models, when they are used as teachers. We show
that this phenomenon can be partially explained by the information bottleneck
principle: the feature representations of intermediate models can have higher
mutual information regarding the input, and thus contain more ""dark knowledge""
for effective distillation. We further propose an optimal intermediate teacher
selection algorithm based on maximizing the total task-related mutual
information. Experiments verify its effectiveness and applicability.",https://github.com/LeapLabTHU/CheckpointKD,-1
EnvEdit: Environment Editing for Vision-and-Language Navigation,0.333922,"In Vision-and-Language Navigation (VLN), an agent needs to navigate through
the environment based on natural language instructions. Due to limited
available data for agent training and finite diversity in navigation
environments, it is challenging for the agent to generalize to new, unseen
environments. To address this problem, we propose EnvEdit, a data augmentation
method that creates new environments by editing existing environments, which
are used to train a more generalizable agent. Our augmented environments can
differ from the seen environments in three diverse aspects: style, object
appearance, and object classes. Training on these edit-augmented environments
prevents the agent from overfitting to existing environments and helps
generalize better to new, unseen environments. Empirically, on both the
Room-to-Room and the multi-lingual Room-Across-Room datasets, we show that our
proposed EnvEdit method gets significant improvements in all metrics on both
pre-trained and non-pre-trained VLN agents, and achieves the new
state-of-the-art on the test leaderboard. We further ensemble the VLN agents
augmented on different edited environments and show that these edit methods are
complementary. Code and data are available at
https://github.com/jialuli-luka/EnvEdit",https://github.com/jialuli-luka/EnvEdit,-1
RELIC: Retrieving Evidence for Literary Claims,0.134016,"Humanities scholars commonly provide evidence for claims that they make about
a work of literature (e.g., a novel) in the form of quotations from the work.
We collect a large-scale dataset (RELiC) of 78K literary quotations and
surrounding critical analysis and use it to formulate the novel task of
literary evidence retrieval, in which models are given an excerpt of literary
analysis surrounding a masked quotation and asked to retrieve the quoted
passage from the set of all passages in the work. Solving this retrieval task
requires a deep understanding of complex literary and linguistic phenomena,
which proves challenging to methods that overwhelmingly rely on lexical and
semantic similarity matching. We implement a RoBERTa-based dense passage
retriever for this task that outperforms existing pretrained information
retrieval baselines; however, experiments and analysis by human domain experts
indicate that there is substantial room for improvement over our dense
retriever.",None,-1
Transformer-based Entity Typing in Knowledge Graphs,0.0609663,"We investigate the knowledge graph entity typing task which aims at inferring
plausible entity types. In this paper, we propose a novel Transformer-based
Entity Typing (TET) approach, effectively encoding the content of neighbors of
an entity. More precisely, TET is composed of three different mechanisms: a
local transformer allowing to infer missing types of an entity by independently
encoding the information provided by each of its neighbors; a global
transformer aggregating the information of all neighbors of an entity into a
single long sequence to reason about more complex entity types; and a context
transformer integrating neighbors content based on their contribution to the
type inference through information exchange between neighbor pairs.
Furthermore, TET uses information about class membership of types to
semantically strengthen the representation of an entity. Experiments on two
real-world datasets demonstrate the superior performance of TET compared to the
state-of-the-art.",https://github.com/zhiweihu1103/ET-TET,-1
Surgical Skill Assessment via Video Semantic Aggregation,0.279587,"Automated video-based assessment of surgical skills is a promising task in
assisting young surgical trainees, especially in poor-resource areas. Existing
works often resort to a CNN-LSTM joint framework that models long-term
relationships by LSTMs on spatially pooled short-term CNN features. However,
this practice would inevitably neglect the difference among semantic concepts
such as tools, tissues, and background in the spatial dimension, impeding the
subsequent temporal relationship modeling. In this paper, we propose a novel
skill assessment framework, Video Semantic Aggregation (ViSA), which discovers
different semantic parts and aggregates them across spatiotemporal dimensions.
The explicit discovery of semantic parts provides an explanatory visualization
that helps understand the neural network's decisions. It also enables us to
further incorporate auxiliary information such as the kinematic data to improve
representation learning and performance. The experiments on two datasets show
the competitiveness of ViSA compared to state-of-the-art methods. Source code
is available at: bit.ly/MICCAI2022ViSA.",None,-1
On Improving Summarization Factual Consistency from Natural Language Feedback,0.121746,"Despite the recent progress in language generation models, their outputs may
not always meet user expectations. In this work, we study whether informational
feedback in natural language can be leveraged to improve generation quality and
user preference alignment. To this end, we consider factual consistency in
summarization, the quality that the summary should only contain information
supported by the input documents, as the user-expected preference. We collect a
high-quality dataset, DeFacto, containing human demonstrations and
informational natural language feedback consisting of corrective instructions,
edited summaries, and explanations with respect to the factual consistency of
the summary. Using our dataset, we study three natural language generation
tasks: (1) editing a summary by following the human feedback, (2) generating
human feedback for editing the original summary, and (3) revising the initial
summary to correct factual errors by generating both the human feedback and
edited summary. We show that DeFacto can provide factually consistent
human-edited summaries and further insights into summarization factual
consistency thanks to its informational natural language feedback. We further
demonstrate that fine-tuned language models can leverage our dataset to improve
the summary factual consistency, while large language models lack the zero-shot
learning ability in our proposed tasks that require controllable text
generation.",https://github.com/microsoft/DeFacto,-1
Modeling Information Change in Science Communication with Semantically Matched Paraphrases,0.12695,"Whether the media faithfully communicate scientific information has long been
a core issue to the science community. Automatically identifying paraphrased
scientific findings could enable large-scale tracking and analysis of
information changes in the science communication process, but this requires
systems to understand the similarity between scientific information across
multiple domains. To this end, we present the SCIENTIFIC PARAPHRASE AND
INFORMATION CHANGE DATASET (SPICED), the first paraphrase dataset of scientific
findings annotated for degree of information change. SPICED contains 6,000
scientific finding pairs extracted from news stories, social media discussions,
and full texts of original papers. We demonstrate that SPICED poses a
challenging task and that models trained on SPICED improve downstream
performance on evidence retrieval for fact checking of real-world scientific
claims. Finally, we show that models trained on SPICED can reveal large-scale
trends in the degrees to which people and organizations faithfully communicate
new scientific findings. Data, code, and pre-trained models are available at
http://www.copenlu.com/publication/2022_emnlp_wright/.",http://www.copenlu.com/publication/2022_emnlp_wright/,-1
PolarMOT: How Far Can Geometric Relations Take Us in 3D Multi-Object Tracking?,0.226632,"Most (3D) multi-object tracking methods rely on appearance-based cues for
data association. By contrast, we investigate how far we can get by only
encoding geometric relationships between objects in 3D space as cues for
data-driven data association. We encode 3D detections as nodes in a graph,
where spatial and temporal pairwise relations among objects are encoded via
localized polar coordinates on graph edges. This representation makes our
geometric relations invariant to global transformations and smooth trajectory
changes, especially under non-holonomic motion. This allows our graph neural
network to learn to effectively encode temporal and spatial interactions and
fully leverage contextual and motion cues to obtain final scene interpretation
by posing data association as edge classification. We establish a new
state-of-the-art on nuScenes dataset and, more importantly, show that our
method, PolarMOT, generalizes remarkably well across different locations
(Boston, Singapore, Karlsruhe) and datasets (nuScenes and KITTI).",None,-1
Confident Adaptive Language Modeling,0.861375,"Recent advances in Transformer-based large language models (LLMs) have led to
significant performance improvements across many tasks. These gains come with a
drastic increase in the models' size, potentially leading to slow and costly
use at inference time. In practice, however, the series of generations made by
LLMs is composed of varying levels of difficulty. While certain predictions
truly benefit from the models' full capacity, other continuations are more
trivial and can be solved with reduced compute. In this work, we introduce
Confident Adaptive Language Modeling (CALM), a framework for dynamically
allocating different amounts of compute per input and generation timestep.
Early exit decoding involves several challenges that we address here, such as:
(1) what confidence measure to use; (2) connecting sequence-level constraints
to local per-token exit decisions; and (3) attending back to missing hidden
representations due to early exits in previous tokens. Through theoretical
analysis and empirical experiments on three diverse text generation tasks, we
demonstrate the efficacy of our framework in reducing compute -- potential
speedup of up to $\times 3$ -- while provably maintaining high performance.",None,-1
UnCommonSense: Informative Negative Knowledge about Everyday Concepts,0.0787629,"Commonsense knowledge about everyday concepts is an important asset for AI
applications, such as question answering and chatbots. Recently, we have seen
an increasing interest in the construction of structured commonsense knowledge
bases (CSKBs). An important part of human commonsense is about properties that
do not apply to concepts, yet existing CSKBs only store positive statements.
Moreover, since CSKBs operate under the open-world assumption, absent
statements are considered to have unknown truth rather than being invalid. This
paper presents the UNCOMMONSENSE framework for materializing informative
negative commonsense statements. Given a target concept, comparable concepts
are identified in the CSKB, for which a local closed-world assumption is
postulated. This way, positive statements about comparable concepts that are
absent for the target concept become seeds for negative statement candidates.
The large set of candidates is then scrutinized, pruned and ranked by
informativeness. Intrinsic and extrinsic evaluations show that our method
significantly outperforms the state-of-the-art. A large dataset of informative
negations is released as a resource for future research.",https://github.com/tsafavi/NegatER,-1
Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction,0.10044,"Recently, prompt-tuning has attracted growing interests in event argument
extraction (EAE). However, the existing prompt-tuning methods have not achieved
satisfactory performance due to the lack of consideration of entity
information. In this paper, we propose a bi-directional iterative prompt-tuning
method for EAE, where the EAE task is treated as a cloze-style task to take
full advantage of entity information and pre-trained language models (PLMs).
Furthermore, our method explores event argument interactions by introducing the
argument roles of contextual entities into prompt construction. Since template
and verbalizer are two crucial components in a cloze-style prompt, we propose
to utilize the role label semantic knowledge to construct a semantic verbalizer
and design three kinds of templates for the EAE task. Experiments on the ACE
2005 English dataset with standard and low-resource settings show that the
proposed method significantly outperforms the peer state-of-the-art methods.
Our code is available at https://github.com/HustMinsLab/BIP.",https://github.com/HustMinsLab/BIP,-1
TAD: Transfer Learning-based Multi-Adversarial Detection of Evasion Attacks against Network Intrusion Detection Systems,0.0830618,"Nowadays, intrusion detection systems based on deep learning deliver
state-of-the-art performance. However, recent research has shown that specially
crafted perturbations, called adversarial examples, are capable of
significantly reducing the performance of these intrusion detection systems.
The objective of this paper is to design an efficient transfer learning-based
adversarial detector and then to assess the effectiveness of using multiple
strategically placed adversarial detectors compared to a single adversarial
detector for intrusion detection systems. In our experiments, we implement
existing state-of-the-art models for intrusion detection. We then attack those
models with a set of chosen evasion attacks. In an attempt to detect those
adversarial attacks, we design and implement multiple transfer learning-based
adversarial detectors, each receiving a subset of the information passed
through the IDS. By combining their respective decisions, we illustrate that
combining multiple detectors can further improve the detectability of
adversarial traffic compared to a single detector in the case of a parallel IDS
design.",None,-1
"""I'm sorry to hear that"": Finding New Biases in Language Models with a Holistic Descriptor Dataset",0.940286,"As language models grow in popularity, it becomes increasingly important to
clearly measure all possible markers of demographic identity in order to avoid
perpetuating existing societal harms. Many datasets for measuring bias
currently exist, but they are restricted in their coverage of demographic axes
and are commonly used with preset bias tests that presuppose which types of
biases models can exhibit. In this work, we present a new, more inclusive bias
measurement dataset, HolisticBias, which includes nearly 600 descriptor terms
across 13 different demographic axes. HolisticBias was assembled in a
participatory process including experts and community members with lived
experience of these terms. These descriptors combine with a set of bias
measurement templates to produce over 450,000 unique sentence prompts, which we
use to explore, identify, and reduce novel forms of bias in several generative
models. We demonstrate that HolisticBias is effective at measuring previously
undetectable biases in token likelihoods from language models, as well as in an
offensiveness classifier. We will invite additions and amendments to the
dataset, which we hope will serve as a basis for more easy-to-use and
standardized methods for evaluating bias in NLP models.",https://github.com/facebookresearch/ResponsibleNLP/tree/main/holistic_bias,-1
Learning Representations for Hyper-Relational Knowledge Graphs,0.0283368,"Knowledge graphs (KGs) have gained prominence for their ability to learn
representations for uni-relational facts. Recently, research has focused on
modeling hyper-relational facts, which move beyond the restriction of
uni-relational facts and allow us to represent more complex and real-world
information. However, existing approaches for learning representations on
hyper-relational KGs majorly focus on enhancing the communication from
qualifiers to base triples while overlooking the flow of information from base
triple to qualifiers. This can lead to suboptimal qualifier representations,
especially when a large amount of qualifiers are presented. It motivates us to
design a framework that utilizes multiple aggregators to learn representations
for hyper-relational facts: one from the perspective of the base triple and the
other one from the perspective of the qualifiers. Experiments demonstrate the
effectiveness of our framework for hyper-relational knowledge graph completion
across multiple datasets. Furthermore, we conduct an ablation study that
validates the importance of the various components in our framework. The code
to reproduce our results can be found at
\url{https://github.com/HarryShomer/QUAD}.",https://github.com/HarryShomer/QUAD,-1
SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues,0.511527,"Dialogue systems are usually categorized into two types, open-domain and
task-oriented. The first one focuses on chatting with users and making them
engage in the conversations, where selecting a proper topic to fit the dialogue
context is essential for a successful dialogue. The other one focuses on a
specific task instead of casual talks, e.g., finding a movie on Friday night,
or playing a song. These two directions have been studied separately due to
their different purposes. However, how smoothly transitioning from social
chatting to task-oriented dialogues is important for triggering business
opportunities, and there is no public data focusing on such scenarios. Hence,
this paper focuses on investigating the conversations starting from open-domain
social chatting and then gradually transitioning to task-oriented purposes, and
releases a large-scale dataset with detailed annotations for encouraging this
research direction. To achieve this goal, this paper proposes a framework to
automatically generate many dialogues without human involvement, in which any
powerful open-domain dialogue generation model can be easily leveraged. The
human evaluation shows that our generated dialogue data has a natural flow at a
reasonable quality, showing that our released data has a great potential of
guiding future research directions and commercial activities. Furthermore, the
released models allow researchers to automatically generate unlimited dialogues
in the target scenarios, which can greatly benefit semi-supervised and
unsupervised approaches.",https://github.com/MiuLab/SalesBot,-1
"Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability",0.204773,"Large-scale pre-training has been proven to be crucial for various computer
vision tasks. However, with the increase of pre-training data amount, model
architecture amount, and the private/inaccessible data, it is not very
efficient or possible to pre-train all the model architectures on large-scale
datasets. In this work, we investigate an alternative strategy for
pre-training, namely Knowledge Distillation as Efficient Pre-training (KDEP),
aiming to efficiently transfer the learned feature representation from existing
pre-trained models to new student models for future downstream tasks. We
observe that existing Knowledge Distillation (KD) methods are unsuitable
towards pre-training since they normally distill the logits that are going to
be discarded when transferred to downstream tasks. To resolve this problem, we
propose a feature-based KD method with non-parametric feature dimension
aligning. Notably, our method performs comparably with supervised pre-training
counterparts in 3 downstream tasks and 9 downstream datasets requiring 10x less
data and 5x less pre-training time. Code is available at
https://github.com/CVMI-Lab/KDEP.",https://github.com/CVMI-Lab/KDEP,-1
NeRF-RPN: A general framework for object detection in NeRFs,0.228223,"This paper presents the first significant object detection framework,
NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model,
NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting
a novel voxel representation that incorporates multi-scale 3D neural volumetric
features, we demonstrate it is possible to regress the 3D bounding boxes of
objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN
is a general framework and can be applied to detect objects without class
labels. We experimented NeRF-RPN with various backbone architectures, RPN head
designs and loss functions. All of them can be trained in an end-to-end manner
to estimate high quality 3D bounding boxes. To facilitate future research in
object detection for NeRF, we built a new benchmark dataset which consists of
both synthetic and real-world data with careful labeling and clean up. Code and
dataset are available at https://github.com/lyclyc52/NeRF_RPN.",https://github.com/lyclyc52/NeRF_RPN,-1
Character-Aware Models Improve Visual Text Rendering,0.702044,"Current image generation models struggle to reliably produce well-formed
visual text. In this paper, we investigate a key contributing factor: popular
text-to-image models lack character-level input features, making it much harder
to predict a word's visual makeup as a series of glyphs. To quantify this
effect, we conduct a series of experiments comparing character-aware vs.
character-blind text encoders. In the text-only domain, we find that
character-aware models provide large gains on a novel spelling task
(WikiSpell). Applying our learnings to the visual domain, we train a suite of
image generation models, and show that character-aware variants outperform
their character-blind counterparts across a range of novel text rendering tasks
(our DrawText benchmark). Our models set a much higher state-of-the-art on
visual spelling, with 30+ point accuracy gains over competitors on rare words,
despite training on far fewer examples.",None,-1
BlobGAN: Spatially Disentangled Scene Representations,0.864665,"We propose an unsupervised, mid-level representation for a generative model
of scenes. The representation is mid-level in that it is neither per-pixel nor
per-image; rather, scenes are modeled as a collection of spatial, depth-ordered
""blobs"" of features. Blobs are differentiably placed onto a feature grid that
is decoded into an image by a generative adversarial network. Due to the
spatial uniformity of blobs and the locality inherent to convolution, our
network learns to associate different blobs with different entities in a scene
and to arrange these blobs to capture scene layout. We demonstrate this
emergent behavior by showing that, despite training without any supervision,
our method enables applications such as easy manipulation of objects within a
scene (e.g., moving, removing, and restyling furniture), creation of feasible
scenes given constraints (e.g., plausible rooms with drawers at a particular
location), and parsing of real-world images into constituent parts. On a
challenging multi-category dataset of indoor scenes, BlobGAN outperforms
StyleGAN2 in image quality as measured by FID. See our project page for video
results and interactive demo: https://www.dave.ml/blobgan",None,-1
Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations,0.366963,"In this paper, we show that recent advances in self-supervised feature
learning enable unsupervised object discovery and semantic segmentation with a
performance that matches the state of the field on supervised semantic
segmentation 10 years ago. We propose a methodology based on unsupervised
saliency masks and self-supervised feature clustering to kickstart object
discovery followed by training a semantic segmentation network on pseudo-labels
to bootstrap the system on images with multiple objects. We present results on
PASCAL VOC that go far beyond the current state of the art (50.0 mIoU), and we
report for the first time results on MS COCO for the whole set of 81 classes:
our method discovers 34 categories with more than $20\%$ IoU, while obtaining
an average IoU of 19.6 for all 81 categories.",https://github.com/zadaianchuk/comus,-1
Co-evolving morphology and control of soft robots using a single genome,0.149106,"When simulating soft robots, both their morphology and their controllers play
important roles in task performance. This paper introduces a new method to
co-evolve these two components in the same process. We do that by using the
hyperNEAT algorithm to generate two separate neural networks in one pass, one
responsible for the design of the robot body structure and the other for the
control of the robot.
  The key difference between our method and most existing approaches is that it
does not treat the development of the morphology and the controller as separate
processes. Similar to nature, our method derives both the ""brain"" and the
""body"" of an agent from a single genome and develops them together. While our
approach is more realistic and doesn't require an arbitrary separation of
processes during evolution, it also makes the problem more complex because the
search space for this single genome becomes larger and any mutation to the
genome affects ""brain"" and the ""body"" at the same time.
  Additionally, we present a new speciation function that takes into
consideration both the genotypic distance, as is the standard for NEAT, and the
similarity between robot bodies. By using this function, agents with very
different bodies are more likely to be in different species, this allows robots
with different morphologies to have more specialized controllers since they
won't crossover with other robots that are too different from them.
  We evaluate the presented methods on four tasks and observe that even if the
search space was larger, having a single genome makes the evolution process
converge faster when compared to having separated genomes for body and control.
The agents in our population also show morphologies with a high degree of
regularity and controllers capable of coordinating the voxels to produce the
necessary movements.",https://github.com/fhtanaka/SGR,-1
A Unified View of Masked Image Modeling,0.390452,"Masked image modeling has demonstrated great potential to eliminate the
label-hungry problem of training large-scale vision Transformers, achieving
impressive performance on various downstream tasks. In this work, we propose a
unified view of masked image modeling after revisiting existing methods. Under
the unified view, we introduce a simple yet effective method, termed as
MaskDistill, which reconstructs normalized semantic features from teacher
models at the masked positions, conditioning on corrupted input images.
Experimental results on image classification and semantic segmentation show
that MaskDistill achieves comparable or superior performance than
state-of-the-art methods. When using the huge vision Transformer and
pretraining 300 epochs, MaskDistill obtains 88.3% fine-tuning top-1 accuracy on
ImageNet-1k (224 size) and 58.8% semantic segmentation mIoU metric on ADE20k
(512 size). The code and pretrained models will be available at
https://aka.ms/unimim.",https://aka.ms/unimim,-1
Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech,0.8427,"We introduce a generic, language-independent method to collect a large
percentage of offensive and hate tweets regardless of their topics or genres.
We harness the extralinguistic information embedded in the emojis to collect a
large number of offensive tweets. We apply the proposed method on Arabic tweets
and compare it with English tweets - analysing key cultural differences. We
observed a constant usage of these emojis to represent offensiveness throughout
different timespans on Twitter. We manually annotate and publicly release the
largest Arabic dataset for offensive, fine-grained hate speech, vulgar and
violence content. Furthermore, we benchmark the dataset for detecting
offensiveness and hate speech using different transformer architectures and
perform in-depth linguistic analysis. We evaluate our models on external
datasets - a Twitter dataset collected using a completely different method, and
a multi-platform dataset containing comments from Twitter, YouTube and
Facebook, for assessing generalization capability. Competitive results on these
datasets suggest that the data collected using our method captures universal
characteristics of offensive language. Our findings also highlight the common
words used in offensive communications, common targets for hate speech,
specific patterns in violence tweets; and pinpoint common classification errors
that can be attributed to limitations of NLP models. We observe that even
state-of-the-art transformer models may fail to take into account culture,
background and context or understand nuances present in real-world data such as
sarcasm.",None,-1
Red-Teaming the Stable Diffusion Safety Filter,0.833032,"Stable Diffusion is a recent open-source image generation model comparable to
proprietary models such as DALLE, Imagen, or Parti. Stable Diffusion comes with
a safety filter that aims to prevent generating explicit images. Unfortunately,
the filter is obfuscated and poorly documented. This makes it hard for users to
prevent misuse in their applications, and to understand the filter's
limitations and improve it. We first show that it is easy to generate
disturbing content that bypasses the safety filter. We then reverse-engineer
the filter and find that while it aims to prevent sexual content, it ignores
violence, gore, and other similarly disturbing content. Based on our analysis,
we argue safety measures in future model releases should strive to be fully
open and properly documented to stimulate security contributions from the
community.",https://github.com/huggingface/diffusers/blob/84b9df5/src/diffusers/pipelines/stable_diffusion/safety_checker.py,-1
Learning to Decompose Visual Features with Latent Textual Prompts,0.106112,"Recent advances in pre-training vision-language models like CLIP have shown
great potential in learning transferable visual representations. Nonetheless,
for downstream inference, CLIP-like models suffer from either 1) degraded
accuracy and robustness in the case of inaccurate text descriptions during
retrieval-based inference (the challenge for zero-shot protocol); or 2)
breaking the well-established vision-language alignment (the challenge for
linear probing). To address them, we propose Decomposed Feature Prompting
(DeFo). DeFo leverages a flexible number of learnable embeddings as textual
input while maintaining the vision-language dual-model architecture, which
enables the model to learn decomposed visual features with the help of
feature-level textual prompts. We further use an additional linear layer to
perform classification, allowing a scalable size of language inputs. Our
empirical study shows DeFo's significance in improving the vision-language
models. For example, DeFo obtains 73.2% test accuracy on ImageNet with a
ResNet-50 backbone without tuning any pretrained weights of both the vision and
language encoder, outperforming zero-shot CLIP by a large margin of 15.0%, and
outperforming state-of-the-art vision-language prompt tuning method by 7.6%.",None,-1
The Best Decisions Are Not the Best Advice: Making Adherence-Aware Recommendations,0.0995293,"Many high-stake decisions follow an expert-in-loop structure in that a human
operator receives recommendations from an algorithm but is the ultimate
decision maker. Hence, the algorithm's recommendation may differ from the
actual decision implemented in practice. However, most algorithmic
recommendations are obtained by solving an optimization problem that assumes
recommendations will be perfectly implemented. We propose an adherence-aware
optimization framework to capture the dichotomy between the recommended and the
implemented policy and analyze the impact of partial adherence on the optimal
recommendation. We show that overlooking the partial adherence phenomenon, as
is currently being done by most recommendation engines, can lead to arbitrarily
severe performance deterioration, compared with both the current human baseline
performance and what is expected by the recommendation algorithm. Our framework
also provides useful tools to analyze the structure and to compute optimal
recommendation policies that are naturally immune against such human
deviations, and are guaranteed to improve upon the baseline policy.",None,-1
Are GAN-based Morphs Threatening Face Recognition?,0.234356,"Morphing attacks are a threat to biometric systems where the biometric
reference in an identity document can be altered. This form of attack presents
an important issue in applications relying on identity documents such as border
security or access control. Research in generation of face morphs and their
detection is developing rapidly, however very few datasets with morphing
attacks and open-source detection toolkits are publicly available. This paper
bridges this gap by providing two datasets and the corresponding code for four
types of morphing attacks: two that rely on facial landmarks based on OpenCV
and FaceMorpher, and two that use StyleGAN 2 to generate synthetic morphs. We
also conduct extensive experiments to assess the vulnerability of four
state-of-the-art face recognition systems, including FaceNet, VGG-Face,
ArcFace, and ISV. Surprisingly, the experiments demonstrate that, although
visually more appealing, morphs based on StyleGAN 2 do not pose a significant
threat to the state to face recognition systems, as these morphs were
outmatched by the simple morphs that are based facial landmarks.",None,-1
Data Splits and Metrics for Method Benchmarking on Surgical Action Triplet Datasets,0.305754,"In addition to generating data and annotations, devising sensible data
splitting strategies and evaluation metrics is essential for the creation of a
benchmark dataset. This practice ensures consensus on the usage of the data,
homogeneous assessment, and uniform comparison of research methods on the
dataset. This study focuses on CholecT50, which is a 50 video surgical dataset
that formalizes surgical activities as triplets of <instrument, verb, target>.
In this paper, we introduce the standard splits for the CholecT50 and CholecT45
datasets and show how they compare with existing use of the dataset. CholecT45
is the first public release of 45 videos of CholecT50 dataset. We also develop
a metrics library, ivtmetrics, for model evaluation on surgical triplets.
Furthermore, we conduct a benchmark study by reproducing baseline methods in
the most predominantly used deep learning frameworks (PyTorch and TensorFlow)
to evaluate them using the proposed data splits and metrics and release them
publicly to support future research. The proposed data splits and evaluation
metrics will enable global tracking of research progress on the dataset and
facilitate optimal model selection for further deployment.",https://github.com/CAMMA-public,-1
Memorizing Transformers,0.205199,"Language models typically need to be trained or finetuned in order to acquire
new knowledge, which involves updating their weights. We instead envision
language models that can simply read and memorize new data at inference time,
thus acquiring new knowledge immediately. In this work, we extend language
models with the ability to memorize the internal representations of past
inputs. We demonstrate that an approximate kNN lookup into a non-differentiable
memory of recent (key, value) pairs improves language modeling across various
benchmarks and tasks, including generic webtext (C4), math papers (arXiv),
books (PG-19), code (Github), as well as formal theorems (Isabelle). We show
that the performance steadily improves when we increase the size of memory up
to 262K tokens. On benchmarks including code and mathematics, we find that the
model is capable of making use of newly defined functions and theorems during
test time.",None,-1
How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning,0.749536,"To avoid collapse in self-supervised learning (SSL), a contrastive loss is
widely used but often requires a large number of negative samples. Without
negative samples yet achieving competitive performance, a recent work has
attracted significant attention for providing a minimalist simple Siamese
(SimSiam) method to avoid collapse. However, the reason for how it avoids
collapse without negative samples remains not fully clear and our investigation
starts by revisiting the explanatory claims in the original SimSiam. After
refuting their claims, we introduce vector decomposition for analyzing the
collapse based on the gradient analysis of the $l_2$-normalized representation
vector. This yields a unified perspective on how negative samples and SimSiam
alleviate collapse. Such a unified perspective comes timely for understanding
the recent progress in SSL.",None,-1
MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid,0.516775,"Multi-modal entity alignment (MMEA) aims to discover identical entities
across different knowledge graphs (KGs) whose entities are associated with
relevant images. However, current MMEA algorithms rely on KG-level modality
fusion strategies for multi-modal entity representation, which ignores the
variations of modality preferences of different entities, thus compromising
robustness against noise in modalities such as blurry images and relations.
This paper introduces MEAformer, a multi-modal entity alignment transformer
approach for meta modality hybrid, which dynamically predicts the mutual
correlation coefficients among modalities for more fine-grained entity-level
modality fusion and alignment. Experimental results demonstrate that our model
not only achieves SOTA performance in multiple training scenarios, including
supervised, unsupervised, iterative, and low-resource settings, but also has a
limited number of parameters, efficient runtime, and interpretability. Our code
is available at https://github.com/zjukg/MEAformer.",https://github.com/zjukg/MEAformer,-1
Finding and Listing Front-door Adjustment Sets,0.400944,"Identifying the effects of new interventions from data is a significant
challenge found across a wide range of the empirical sciences. A well-known
strategy for identifying such effects is Pearl's front-door (FD) criterion
(Pearl, 1995). The definition of the FD criterion is declarative, only allowing
one to decide whether a specific set satisfies the criterion. In this paper, we
present algorithms for finding and enumerating possible sets satisfying the FD
criterion in a given causal diagram. These results are useful in facilitating
the practical applications of the FD criterion for causal effects estimation
and helping scientists to select estimands with desired properties, e.g., based
on cost, feasibility of measurement, or statistical power.",https://github.com/CausalAILab/FrontdoorAdjustmentSets,-1
Learning Deep Sensorimotor Policies for Vision-based Autonomous Drone Racing,0.0698072,"Autonomous drones can operate in remote and unstructured environments,
enabling various real-world applications. However, the lack of effective
vision-based algorithms has been a stumbling block to achieving this goal.
Existing systems often require hand-engineered components for state estimation,
planning, and control. Such a sequential design involves laborious tuning,
human heuristics, and compounding delays and errors. This paper tackles the
vision-based autonomous-drone-racing problem by learning deep sensorimotor
policies. We use contrastive learning to extract robust feature representations
from the input images and leverage a two-stage learning-by-cheating framework
for training a neural network policy. The resulting policy directly infers
control commands with feature representations learned from raw images, forgoing
the need for globally-consistent state estimation, trajectory planning, and
handcrafted control design. Our experimental results indicate that our
vision-based policy can achieve the same level of racing performance as the
state-based policy while being robust against different visual disturbances and
distractors. We believe this work serves as a stepping-stone toward developing
intelligent vision-based autonomous systems that control the drone purely from
image inputs, like human pilots.",None,-1
Unsupervised Scene Sketch to Photo Synthesis,0.043996,"Sketches make an intuitive and powerful visual expression as they are fast
executed freehand drawings. We present a method for synthesizing realistic
photos from scene sketches. Without the need for sketch and photo pairs, our
framework directly learns from readily available large-scale photo datasets in
an unsupervised manner. To this end, we introduce a standardization module that
provides pseudo sketch-photo pairs during training by converting photos and
sketches to a standardized domain, i.e. the edge map. The reduced domain gap
between sketch and photo also allows us to disentangle them into two
components: holistic scene structures and low-level visual styles such as color
and texture. Taking this advantage, we synthesize a photo-realistic image by
combining the structure of a sketch and the visual style of a reference photo.
Extensive experimental results on perceptual similarity metrics and human
perceptual studies show the proposed method could generate realistic photos
with high fidelity from scene sketches and outperform state-of-the-art photo
synthesis baselines. We also demonstrate that our framework facilitates a
controllable manipulation of photo synthesis by editing strokes of
corresponding sketches, delivering more fine-grained details than previous
approaches that rely on region-level editing.",None,-1
"F-coref: Fast, Accurate and Easy to Use Coreference Resolution",0.237657,"We introduce fastcoref, a python package for fast, accurate, and easy-to-use
English coreference resolution. The package is pip-installable, and allows two
modes: an accurate mode based on the LingMess architecture, providing
state-of-the-art coreference accuracy, and a substantially faster model,
F-coref, which is the focus of this work. F-coref allows to process 2.8K
OntoNotes documents in 25 seconds on a V100 GPU (compared to 6 minutes for the
LingMess model, and to 12 minutes of the popular AllenNLP coreference model)
with only a modest drop in accuracy. The fast speed is achieved through a
combination of distillation of a compact model from the LingMess model, and an
efficient batching implementation using a technique we call leftover batching.
Our code is available at https://github.com/shon-otmazgin/fastcoref",https://github.com/shon-otmazgin/fastcoref,-1
Trustworthy Social Bias Measurement,0.0788316,"How do we design measures of social bias that we trust? While prior work has
introduced several measures, no measure has gained widespread trust: instead,
mounting evidence argues we should distrust these measures. In this work, we
design bias measures that warrant trust based on the cross-disciplinary theory
of measurement modeling. To combat the frequently fuzzy treatment of social
bias in NLP, we explicitly define social bias, grounded in principles drawn
from social science research. We operationalize our definition by proposing a
general bias measurement framework DivDist, which we use to instantiate 5
concrete bias measures. To validate our measures, we propose a rigorous testing
protocol with 8 testing criteria (e.g. predictive validity: do measures predict
biases in US employment?). Through our testing, we demonstrate considerable
evidence to trust our measures, showing they overcome conceptual, technical,
and empirical deficiencies present in prior measures.",https://github.com/rishibommasani/BiasMeasures,-1
HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation,0.0431224,"Language models with the Transformers structure have shown great performance
in natural language processing. However, there still poses problems when
fine-tuning pre-trained language models on downstream tasks, such as
over-fitting or representation collapse. In this work, we propose HyPe, a
simple yet effective fine-tuning technique to alleviate such problems by
perturbing hidden representations of Transformers layers. Unlike previous works
that only add noise to inputs or parameters, we argue that the hidden
representations of Transformers layers convey more diverse and meaningful
language information. Therefore, making the Transformers layers more robust to
hidden representation perturbations can further benefit the fine-tuning of PLMs
en bloc. We conduct extensive experiments and analyses on GLUE and other
natural language inference datasets. Results demonstrate that HyPe outperforms
vanilla fine-tuning and enhances generalization of hidden representations from
different layers. In addition, HyPe acquires negligible computational
overheads, and is better than and compatible with previous state-of-the-art
fine-tuning techniques.",https://github.com/Yuanhy1997/HyPe,-1
Content and Style Aware Generation of Text-line Images for Handwriting Recognition,0.296808,"Handwritten Text Recognition has achieved an impressive performance in public
benchmarks. However, due to the high inter- and intra-class variability between
handwriting styles, such recognizers need to be trained using huge volumes of
manually labeled training data. To alleviate this labor-consuming problem,
synthetic data produced with TrueType fonts has been often used in the training
loop to gain volume and augment the handwriting style variability. However,
there is a significant style bias between synthetic and real data which hinders
the improvement of recognition performance. To deal with such limitations, we
propose a generative method for handwritten text-line images, which is
conditioned on both visual appearance and textual content. Our method is able
to produce long text-line samples with diverse handwriting styles. Once
properly trained, our method can also be adapted to new target data by only
accessing unlabeled text-line images to mimic handwritten styles and produce
images with any textual content. Extensive experiments have been done on making
use of the generated samples to boost Handwritten Text Recognition performance.
Both qualitative and quantitative results demonstrate that the proposed
approach outperforms the current state of the art.",https://github.com/kaonashi-tyc/zi2zi,-1
Prompting Is Programming: A Query Language for Large Language Models,0.0846439,"Large language models have demonstrated outstanding performance on a wide
range of tasks such as question answering and code generation. On a high level,
given an input, a language model can be used to automatically complete the
sequence in a statistically-likely way. Based on this, users prompt these
models with language instructions or examples, to implement a variety of
downstream tasks. Advanced prompting methods can even imply interaction between
the language model, a user, and external tools such as calculators. However, to
obtain state-of-the-art performance or adapt language models for specific
tasks, complex task- and model-specific programs have to be implemented, which
may still require ad-hoc interaction.
  Based on this, we present the novel idea of Language Model Programming (LMP).
LMP generalizes language model prompting from pure text prompts to an intuitive
combination of text prompting and scripting. Additionally, LMP allows
constraints to be specified over the language model output. This enables easy
adaption to many tasks while abstracting language model internals and providing
high-level semantics.
  To enable LMP, we implement LMQL(short for Language Model Query Language),
which leverages the constraints and control flow from an LMP prompt to generate
an efficient inference procedure that minimizes the number of expensive calls
to the underlying language model.
  We show that LMQL can capture a wide range of state-of-the-art prompting
methods in an intuitive way, especially facilitating interactive flows that are
challenging to implement with existing high-level APIs. Our evaluation shows
that we retain or increase the accuracy on several downstream tasks, while also
significantly reducing the required amount of computation or cost in the case
of pay-to-use APIs (26-85% cost savings).",https://github.com/eth-sri/lmql,-1
Egocentric Human-Object Interaction Detection Exploiting Synthetic Data,0.236801,"We consider the problem of detecting Egocentric HumanObject Interactions
(EHOIs) in industrial contexts. Since collecting and labeling large amounts of
real images is challenging, we propose a pipeline and a tool to generate
photo-realistic synthetic First Person Vision (FPV) images automatically
labeled for EHOI detection in a specific industrial scenario. To tackle the
problem of EHOI detection, we propose a method that detects the hands, the
objects in the scene, and determines which objects are currently involved in an
interaction. We compare the performance of our method with a set of
state-of-the-art baselines. Results show that using a synthetic dataset
improves the performance of an EHOI detection system, especially when few real
data are available. To encourage research on this topic, we publicly release
the proposed dataset at the following url:
https://iplab.dmi.unict.it/EHOI_SYNTH/.",https://github.com/cocodataset/cocoapi,-1
Path-Aware Graph Attention for HD Maps in Motion Prediction,0.0842119,"The success of motion prediction for autonomous driving relies on integration
of information from the HD maps. As maps are naturally graph-structured,
investigation on graph neural networks (GNNs) for encoding HD maps is
burgeoning in recent years. However, unlike many other applications where GNNs
have been straightforwardly deployed, HD maps are heterogeneous graphs where
vertices (lanes) are connected by edges (lane-lane interaction relationships)
of various nature, and most graph-based models are not designed to understand
the variety of edge types which provide crucial cues for predicting how the
agents would travel the lanes. To overcome this challenge, we propose
Path-Aware Graph Attention, a novel attention architecture that infers the
attention between two vertices by parsing the sequence of edges forming the
paths that connect them. Our analysis illustrates how the proposed attention
mechanism can facilitate learning in a didactic problem where existing graph
networks like GCN struggle. By improving map encoding, the proposed model
surpasses previous state of the art on the Argoverse Motion Forecasting
dataset, and won the first place in the 2021 Argoverse Motion Forecasting
Competition.",None,-1
Deanthropomorphising NLP: Can a Language Model Be Conscious?,0.0232849,"This work is intended as a voice in the discussion over previous claims that
a pretrained large language model (LLM) based on the Transformer model
architecture can be sentient. Such claims have been made concerning the LaMDA
model and also concerning the current wave of LLM-powered chatbots, such as
ChatGPT. This claim, if confirmed, would have serious ramifications in the
Natural Language Processing (NLP) community due to wide-spread use of similar
models. However, here we take the position that such a large language model
cannot be sentient, or conscious, and that LaMDA in particular exhibits no
advances over other similar models that would qualify it. We justify this by
analysing the Transformer architecture through Integrated Information Theory of
consciousness. We see the claims of sentience as part of a wider tendency to
use anthropomorphic language in NLP reporting. Regardless of the veracity of
the claims, we consider this an opportune moment to take stock of progress in
language modelling and consider the ethical implications of the task. In order
to make this work helpful for readers outside the NLP community, we also
present the necessary background in language modelling.",None,-1
Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion,0.553027,"We present a framework for modeling interactional communication in dyadic
conversations: given multimodal inputs of a speaker, we autoregressively output
multiple possibilities of corresponding listener motion. We combine the motion
and speech audio of the speaker using a motion-audio cross attention
transformer. Furthermore, we enable non-deterministic prediction by learning a
discrete latent representation of realistic listener motion with a novel
motion-encoding VQ-VAE. Our method organically captures the multimodal and
non-deterministic nature of nonverbal dyadic interactions. Moreover, it
produces realistic 3D listener facial motion synchronous with the speaker (see
video). We demonstrate that our method outperforms baselines qualitatively and
quantitatively via a rich suite of experiments. To facilitate this line of
research, we introduce a novel and large in-the-wild dataset of dyadic
conversations. Code, data, and videos available at
https://evonneng.github.io/learning2listen/.",None,-1
Speech Resources in the Tamasheq Language,0.0363371,"In this paper we present two datasets for Tamasheq, a developing language
mainly spoken in Mali and Niger. These two datasets were made available for the
IWSLT 2022 low-resource speech translation track, and they consist of
collections of radio recordings from daily broadcast news in Niger (Studio
Kalangou) and Mali (Studio Tamani). We share (i) a massive amount of unlabeled
audio data (671 hours) in five languages: French from Niger, Fulfulde, Hausa,
Tamasheq and Zarma, and (ii) a smaller 17 hours parallel corpus of audio
recordings in Tamasheq, with utterance-level translations in the French
language. All this data is shared under the Creative Commons BY-NC-ND 3.0
license. We hope these resources will inspire the speech community to develop
and benchmark models using the Tamasheq language.",https://github.com/mzboito/IWSLT2022 Tamasheq data.,-1
Enhanced Bi-directional Motion Estimation for Video Frame Interpolation,0.42862,"We present a novel simple yet effective algorithm for motion-based video
frame interpolation. Existing motion-based interpolation methods typically rely
on a pre-trained optical flow model or a U-Net based pyramid network for motion
estimation, which either suffer from large model size or limited capacity in
handling complex and large motion cases. In this work, by carefully integrating
intermediateoriented forward-warping, lightweight feature encoder, and
correlation volume into a pyramid recurrent framework, we derive a compact
model to simultaneously estimate the bidirectional motion between input frames.
It is 15 times smaller in size than PWC-Net, yet enables more reliable and
flexible handling of challenging motion cases. Based on estimated
bi-directional motion, we forward-warp input frames and their context features
to intermediate frame, and employ a synthesis network to estimate the
intermediate frame from warped representations. Our method achieves excellent
performance on a broad range of video frame interpolation benchmarks. Code and
trained models are available at \url{https://github.com/srcn-ivl/EBME}.",https://github.com/srcn-ivl/EBME,-1
Jury Learning: Integrating Dissenting Voices into Machine Learning Models,0.0590877,"Whose labels should a machine learning (ML) algorithm learn to emulate? For
ML tasks ranging from online comment toxicity to misinformation detection to
medical diagnosis, different groups in society may have irreconcilable
disagreements about ground truth labels. Supervised ML today resolves these
label disagreements implicitly using majority vote, which overrides minority
groups' labels. We introduce jury learning, a supervised ML approach that
resolves these disagreements explicitly through the metaphor of a jury:
defining which people or groups, in what proportion, determine the classifier's
prediction. For example, a jury learning model for online toxicity might
centrally feature women and Black jurors, who are commonly targets of online
harassment. To enable jury learning, we contribute a deep learning architecture
that models every annotator in a dataset, samples from annotators' models to
populate the jury, then runs inference to classify. Our architecture enables
juries that dynamically adapt their composition, explore counterfactuals, and
visualize dissent.",None,-1
AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning,0.811671,"Standard fine-tuning of large pre-trained language models (PLMs) for
downstream tasks requires updating hundreds of millions to billions of
parameters, and storing a large copy of the PLM weights for every task
resulting in increased cost for storing, sharing and serving the models. To
address this, parameter-efficient fine-tuning (PEFT) techniques were introduced
where small trainable components are injected in the PLM and updated during
fine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of
adaptation modules -- given the underlying PEFT method of choice -- introduced
in each Transformer layer while keeping most of the PLM weights frozen. For
instance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture
of low rank decomposition matrices like LoRA to improve downstream task
performance over the corresponding PEFT methods for fully supervised and
few-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the
same computational cost and the number of tunable parameters as the underlying
PEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix
outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for
both NLU and NLG tasks.",None,-1
Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering,0.405219,"Retrieval augmented language models have recently become the standard for
knowledge intensive tasks. Rather than relying purely on latent semantics
within the parameters of large neural models, these methods enlist a
semi-parametric memory to encode an index of knowledge for the model to
retrieve over. Most prior work has employed text passages as the unit of
knowledge, which has high coverage at the cost of interpretability,
controllability, and efficiency. The opposite properties arise in other methods
which have instead relied on knowledge base (KB) facts. At the same time, more
recent work has demonstrated the effectiveness of storing and retrieving from
an index of Q-A pairs derived from text \citep{lewis2021paq}. This approach
yields a high coverage knowledge representation that maintains KB-like
properties due to its representations being more atomic units of information.
In this work we push this line of research further by proposing a
question-answer augmented encoder-decoder model and accompanying pretraining
strategy. This yields an end-to-end system that not only outperforms prior QA
retrieval methods on single-hop QA tasks but also enables compositional
reasoning, as demonstrated by strong performance on two multi-hop QA datasets.
Together, these methods improve the ability to interpret and control the model
while narrowing the performance gap with passage retrieval systems.",https://github.com/google-research/language/,-1
ImageArg: A Multi-modal Tweet Dataset for Image Persuasiveness Mining,0.0169105,"The growing interest in developing corpora of persuasive texts has promoted
applications in automated systems, e.g., debating and essay scoring systems;
however, there is little prior work mining image persuasiveness from an
argumentative perspective. To expand persuasiveness mining into a multi-modal
realm, we present a multi-modal dataset, ImageArg, consisting of annotations of
image persuasiveness in tweets. The annotations are based on a persuasion
taxonomy we developed to explore image functionalities and the means of
persuasion. We benchmark image persuasiveness tasks on ImageArg using
widely-used multi-modal learning methods. The experimental results show that
our dataset offers a useful resource for this rich and challenging topic, and
there is ample room for modeling improvement.",https://github.com/MeiqiGuo/ArgMining2022-ImageArg,-1
Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence,0.0279591,"The logical negation property (LNP), which implies generating different
predictions for semantically opposite inputs, is an important property that a
trustworthy language model must satisfy. However, much recent evidence shows
that large-size pre-trained language models (PLMs) do not satisfy this
property. In this paper, we perform experiments using probing tasks to assess
PLM's LNP understanding. Unlike previous studies that only examined negation
expressions, we expand the boundary of the investigation to lexical semantics.
Through experiments, we observe that PLMs violate the LNP frequently. To
alleviate the issue, we propose a novel intermediate training task, names
meaning-matching, designed to directly learn a meaning-text correspondence,
instead of relying on the distributional hypothesis. Through multiple
experiments, we find that the task enables PLMs to learn lexical semantic
information. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm
that it is a safe intermediate task that guarantees a similar or better
performance of downstream tasks. Finally, we observe that our proposed approach
outperforms our previous counterparts despite its time and resource efficiency.",https://github.com/MJ-Jang/beyond-distributional,-1
An Empirical Study on Cross-X Transfer for Legal Judgment Prediction,0.239222,"Cross-lingual transfer learning has proven useful in a variety of Natural
Language Processing (NLP) tasks, but it is understudied in the context of legal
NLP, and not at all in Legal Judgment Prediction (LJP). We explore transfer
learning techniques on LJP using the trilingual Swiss-Judgment-Prediction
dataset, including cases written in three languages. We find that cross-lingual
transfer improves the overall results across languages, especially when we use
adapter-based fine-tuning. Finally, we further improve the model's performance
by augmenting the training dataset with machine-translated versions of the
original documents, using a 3x larger training corpus. Further on, we perform
an analysis exploring the effect of cross-domain and cross-regional transfer,
i.e., train a model across domains (legal areas), or regions. We find that in
both settings (legal areas, origin regions), models trained across all groups
perform overall better, while they also have improved results in the worst-case
scenarios. Finally, we report improved results when we ambitiously apply
cross-jurisdiction transfer, where we further augment our dataset with Indian
legal cases.",https://github.com/UKPLab/EasyNMT,-1
Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models,0.175929,"Neural network models have achieved state-of-the-art performance on
grapheme-to-phoneme (G2P) conversion. However, their performance relies on
large-scale pronunciation dictionaries, which may not be available for a lot of
languages. Inspired by the success of the pre-trained language model BERT, this
paper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which
is built by self-supervised training on a large, language-specific word list
with only grapheme information. Furthermore, two approaches are developed to
incorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e.,
fine-tuning GBERT or fusing GBERT into the Transformer model by attention.
Experimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean
datasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our
GBERT-based G2P models under both medium-resource and low-resource data
conditions.",https://github.com/ldong1111/GraphemeBERT,-1
HLDC: Hindi Legal Documents Corpus,0.57676,"Many populous countries including India are burdened with a considerable
backlog of legal cases. Development of automated systems that could process
legal documents and augment legal practitioners can mitigate this. However,
there is a dearth of high-quality corpora that is needed to develop such
data-driven systems. The problem gets even more pronounced in the case of low
resource languages such as Hindi. In this resource paper, we introduce the
Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents
in Hindi. Documents are cleaned and structured to enable the development of
downstream applications. Further, as a use-case for the corpus, we introduce
the task of bail prediction. We experiment with a battery of models and propose
a Multi-Task Learning (MTL) based model for the same. MTL models use
summarization as an auxiliary task along with bail prediction as the main task.
Experiments with different models are indicative of the need for further
research in this area. We release the corpus and model implementation code with
this paper: https://github.com/Exploration-Lab/HLDC",https://github.com/Exploration-Lab/HLDC,-1
HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations,0.245799,"Recently, various response generation models for two-party conversations have
achieved impressive improvements, but less effort has been paid to multi-party
conversations (MPCs) which are more practical and complicated. Compared with a
two-party conversation where a dialogue context is a sequence of utterances,
building a response generation model for MPCs is more challenging, since there
exist complicated context structures and the generated responses heavily rely
on both interlocutors (i.e., speaker and addressee) and history utterances. To
address these challenges, we present HeterMPC, a heterogeneous graph-based
neural network for response generation in MPCs which models the semantics of
utterances and interlocutors simultaneously with two types of nodes in a graph.
Besides, we also design six types of meta relations with
node-edge-type-dependent parameters to characterize the heterogeneous
interactions within the graph. Through multi-hop updating, HeterMPC can
adequately utilize the structural knowledge of conversations for response
generation. Experimental results on the Ubuntu Internet Relay Chat (IRC)
channel benchmark show that HeterMPC outperforms various baseline models for
response generation in MPCs.",https://github.com/lxchtan/HeterMPC,-1
Blur Interpolation Transformer for Real-World Motion from Blur,0.231473,"This paper studies the challenging problem of recovering motion from blur,
also known as joint deblurring and interpolation or blur temporal
super-resolution. The challenges are twofold: 1) the current methods still
leave considerable room for improvement in terms of visual quality even on the
synthetic dataset, and 2) poor generalization to real-world data. To this end,
we propose a blur interpolation transformer (BiT) to effectively unravel the
underlying temporal correlation encoded in blur. Based on multi-scale residual
Swin transformer blocks, we introduce dual-end temporal supervision and
temporally symmetric ensembling strategies to generate effective features for
time-varying motion rendering. In addition, we design a hybrid camera system to
collect the first real-world dataset of one-to-many blur-sharp video pairs.
Experimental results show that BiT has a significant gain over the
state-of-the-art methods on the public dataset Adobe240. Besides, the proposed
real-world dataset effectively helps the model generalize well to real blurry
scenarios. Code and data are available at https://github.com/zzh-tech/BiT.",https://github.com/zzh-tech/BiT,-1
"Video Question Answering: Datasets, Algorithms and Challenges",0.468091,"Video Question Answering (VideoQA) aims to answer natural language questions
according to the given videos. It has earned increasing attention with recent
research trends in joint vision and language understanding. Yet, compared with
ImageQA, VideoQA is largely underexplored and progresses slowly. Although
different algorithms have continually been proposed and shown success on
different VideoQA datasets, we find that there lacks a meaningful survey to
categorize them, which seriously impedes its advancements. This paper thus
provides a clear taxonomy and comprehensive analyses to VideoQA, focusing on
the datasets, algorithms, and unique challenges. We then point out the research
trend of studying beyond factoid QA to inference QA towards the cognition of
video contents, Finally, we conclude some promising directions for future
exploration.",https://github.com/VRU-NExT/VideoQA,-1
ROAD-R: The Autonomous Driving Dataset with Logical Requirements,0.146871,"Neural networks have proven to be very powerful at computer vision tasks.
However, they often exhibit unexpected behaviours, violating known requirements
expressing background knowledge. This calls for models (i) able to learn from
the requirements, and (ii) guaranteed to be compliant with the requirements
themselves. Unfortunately, the development of such models is hampered by the
lack of datasets equipped with formally specified requirements. In this paper,
we introduce the ROad event Awareness Dataset with logical Requirements
(ROAD-R), the first publicly available dataset for autonomous driving with
requirements expressed as logical constraints. Given ROAD-R, we show that
current state-of-the-art models often violate its logical constraints, and that
it is possible to exploit them to create models that (i) have a better
performance, and (ii) are guaranteed to be compliant with the requirements
themselves.",https://github.com/gurkirt/road-dataset,-1
Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection,0.193983,"Language models increasingly rely on massive web dumps for diverse text data.
However, these sources are rife with undesirable content. As such, resources
like Wikipedia, books, and newswire often serve as anchors for automatically
selecting web text most suitable for language modeling, a process typically
referred to as quality filtering. Using a new dataset of U.S. high school
newspaper articles -- written by students from across the country -- we
investigate whose language is preferred by the quality filter used for GPT-3.
We find that newspapers from larger schools, located in wealthier, educated,
and urban ZIP codes are more likely to be classified as high quality. We then
demonstrate that the filter's measurement of quality is unaligned with other
sensible metrics, such as factuality or literary acclaim. We argue that
privileging any corpus as high quality entails a language ideology, and more
care is needed to construct training corpora for language models, with better
transparency and justification for the inclusion or exclusion of various texts.",https://github.com/kernelmachine/quality-filter,-1
FisheyeHDK: Hyperbolic Deformable Kernel Learning for Ultra-Wide Field-of-View Image Recognition,0.535519,"Conventional convolution neural networks (CNNs) trained on narrow
Field-of-View (FoV) images are the state-of-the-art approaches for object
recognition tasks. Some methods proposed the adaptation of CNNs to ultra-wide
FoV images by learning deformable kernels. However, they are limited by the
Euclidean geometry and their accuracy degrades under strong distortions caused
by fisheye projections. In this work, we demonstrate that learning the shape of
convolution kernels in non-Euclidean spaces is better than existing deformable
kernel methods. In particular, we propose a new approach that learns deformable
kernel parameters (positions) in hyperbolic space. FisheyeHDK is a hybrid CNN
architecture combining hyperbolic and Euclidean convolution layers for
positions and features learning. First, we provide an intuition of hyperbolic
space for wide FoV images. Using synthetic distortion profiles, we demonstrate
the effectiveness of our approach. We select two datasets - Cityscapes and
BDD100K 2020 - of perspective images which we transform to fisheye equivalents
at different scaling factors (analog to focal lengths). Finally, we provide an
experiment on data collected by a real fisheye camera. Validations and
experiments show that our approach improves existing deformable kernel methods
for CNN adaptation on fisheye images.",None,-1
CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic Surface Representation via Neural Homeomorphism,0.685297,"While neural representations for static 3D shapes are widely studied,
representations for deformable surfaces are limited to be template-dependent or
lack efficiency. We introduce Canonical Deformation Coordinate Space (CaDeX), a
unified representation of both shape and nonrigid motion. Our key insight is
the factorization of the deformation between frames by continuous bijective
canonical maps (homeomorphisms) and their inverses that go through a learned
canonical shape. Our novel deformation representation and its implementation
are simple, efficient, and guarantee cycle consistency, topology preservation,
and, if needed, volume conservation. Our modelling of the learned canonical
shapes provides a flexible and stable space for shape prior learning. We
demonstrate state-of-the-art performance in modelling a wide range of
deformable geometries: human bodies, animal bodies, and articulated objects.",None,-1
Generative Multiplane Images: Making a 2D GAN 3D-Aware,0.118129,"What is really needed to make an existing 2D GAN 3D-aware? To answer this
question, we modify a classical GAN, i.e., StyleGANv2, as little as possible.
We find that only two modifications are absolutely necessary: 1) a multiplane
image style generator branch which produces a set of alpha maps conditioned on
their depth; 2) a pose-conditioned discriminator. We refer to the generated
output as a 'generative multiplane image' (GMPI) and emphasize that its
renderings are not only high-quality but also guaranteed to be view-consistent,
which makes GMPIs different from many prior works. Importantly, the number of
alpha maps can be dynamically adjusted and can differ between training and
inference, alleviating memory concerns and enabling fast training of GMPIs in
less than half a day at a resolution of $1024^2$. Our findings are consistent
across three challenging and common high-resolution datasets, including FFHQ,
AFHQv2, and MetFaces.",https://github.com/apple/ml-gmpi,-1
Diverse Plausible 360-Degree Image Outpainting for Efficient 3DCG Background Creation,0.596141,"We address the problem of generating a 360-degree image from a single image
with a narrow field of view by estimating its surroundings. Previous methods
suffered from overfitting to the training resolution and deterministic
generation. This paper proposes a completion method using a transformer for
scene modeling and novel methods to improve the properties of a 360-degree
image on the output image. Specifically, we use CompletionNets with a
transformer to perform diverse completions and AdjustmentNet to match color,
stitching, and resolution with an input image, enabling inference at any
resolution. To improve the properties of a 360-degree image on an output image,
we also propose WS-perceptual loss and circular inference. Thorough experiments
show that our method outperforms state-of-the-art (SOTA) methods both
qualitatively and quantitatively. For example, compared to SOTA methods, our
method completes images 16 times larger in resolution and achieves 1.7 times
lower Frechet inception distance (FID). Furthermore, we propose a pipeline that
uses the completion results for lighting and background of 3DCG scenes. Our
plausible background completion enables perceptually natural results in the
application of inserting virtual objects with specular surfaces.",None,-1
OpenTAL: Towards Open Set Temporal Action Localization,0.997521,"Temporal Action Localization (TAL) has experienced remarkable success under
the supervised learning paradigm. However, existing TAL methods are rooted in
the closed set assumption, which cannot handle the inevitable unknown actions
in open-world scenarios. In this paper, we, for the first time, step toward the
Open Set TAL (OSTAL) problem and propose a general framework OpenTAL based on
Evidential Deep Learning (EDL). Specifically, the OpenTAL consists of
uncertainty-aware action classification, actionness prediction, and temporal
location regression. With the proposed importance-balanced EDL method,
classification uncertainty is learned by collecting categorical evidence
majorly from important samples. To distinguish the unknown actions from
background video frames, the actionness is learned by the positive-unlabeled
learning. The classification uncertainty is further calibrated by leveraging
the guidance from the temporal localization quality. The OpenTAL is general to
enable existing TAL models for open set scenarios, and experimental results on
THUMOS14 and ActivityNet1.3 benchmarks show the effectiveness of our method.
The code and pre-trained models are released at
https://www.rit.edu/actionlab/opental.",https://www.rit.edu/actionlab/opental,-1
Positive-Unlabeled Learning with Adversarial Data Augmentation for Knowledge Graph Completion,0.406371,"Most real-world knowledge graphs (KG) are far from complete and
comprehensive. This problem has motivated efforts in predicting the most
plausible missing facts to complete a given KG, i.e., knowledge graph
completion (KGC). However, existing KGC methods suffer from two main issues, 1)
the false negative issue, i.e., the sampled negative training instances may
include potential true facts; and 2) the data sparsity issue, i.e., true facts
account for only a tiny part of all possible facts. To this end, we propose
positive-unlabeled learning with adversarial data augmentation (PUDA) for KGC.
In particular, PUDA tailors positive-unlabeled risk estimator for the KGC task
to deal with the false negative issue. Furthermore, to address the data
sparsity issue, PUDA achieves a data augmentation strategy by unifying
adversarial training and positive-unlabeled learning under the
positive-unlabeled minimax game. Extensive experimental results on real-world
benchmark datasets demonstrate the effectiveness and compatibility of our
proposed method.",https://github.com/lilv98/PUDA-IJCAI22,-1
Interactive Segmentation of Radiance Fields,0.343502,"Radiance Fields (RF) are popular to represent casually-captured scenes for
new view synthesis and several applications beyond it. Mixed reality on
personal spaces needs understanding and manipulating scenes represented as RFs,
with semantic segmentation of objects as an important step. Prior segmentation
efforts show promise but don't scale to complex objects with diverse
appearance. We present the ISRF method to interactively segment objects with
fine structure and appearance. Nearest neighbor feature matching using
distilled semantic features identifies high-confidence seed regions. Bilateral
search in a joint spatio-semantic space grows the region to recover accurate
segmentation. We show state-of-the-art results of segmenting objects from RFs
and compositing them to another scene, changing appearance, etc., and an
interactive segmentation tool that others can use.
  Project Page: https://rahul-goel.github.io/isrf/",https://rahul-goel.github.io/isrf/,-1
Fingerprint Liveness Detection Based on Quality Measures,0.430426,"A new fingerprint parameterization for liveness detection based on quality
measures is presented. The novel feature set is used in a complete liveness
detection system and tested on the development set of the LivDET competition,
comprising over 4,500 real and fake images acquired with three different
optical sensors. The proposed solution proves to be robust to the multi-sensor
scenario, and presents an overall rate of 93% of correctly classified samples.
Furthermore, the liveness detection method presented has the added advantage
over previously studied techniques of needing just one image from a finger to
decide whether it is real or fake.",None,-1
Probabilistic Representations for Video Contrastive Learning,0.225751,"This paper presents Probabilistic Video Contrastive Learning, a
self-supervised representation learning method that bridges contrastive
learning with probabilistic representation. We hypothesize that the clips
composing the video have different distributions in short-term duration, but
can represent the complicated and sophisticated video distribution through
combination in a common embedding space. Thus, the proposed method represents
video clips as normal distributions and combines them into a Mixture of
Gaussians to model the whole video distribution. By sampling embeddings from
the whole video distribution, we can circumvent the careful sampling strategy
or transformations to generate augmented views of the clips, unlike previous
deterministic methods that have mainly focused on such sample generation
strategies for contrastive learning. We further propose a stochastic
contrastive loss to learn proper video distributions and handle the inherent
uncertainty from the nature of the raw video. Experimental results verify that
our probabilistic embedding stands as a state-of-the-art video representation
learning for action recognition and video retrieval on the most popular
benchmarks, including UCF101 and HMDB51.",None,-1
Write It Like You See It: Detectable Differences in Clinical Notes By Race Lead To Differential Model Recommendations,0.317181,"Clinical notes are becoming an increasingly important data source for machine
learning (ML) applications in healthcare. Prior research has shown that
deploying ML models can perpetuate existing biases against racial minorities,
as bias can be implicitly embedded in data. In this study, we investigate the
level of implicit race information available to ML models and human experts and
the implications of model-detectable differences in clinical notes. Our work
makes three key contributions. First, we find that models can identify patient
self-reported race from clinical notes even when the notes are stripped of
explicit indicators of race. Second, we determine that human experts are not
able to accurately predict patient race from the same redacted clinical notes.
Finally, we demonstrate the potential harm of this implicit information in a
simulation study, and show that models trained on these race-redacted clinical
notes can still perpetuate existing biases in clinical treatment decisions.",None,-1
RobustLoc: Robust Camera Pose Regression in Challenging Driving Environments,0.11597,"Camera relocalization has various applications in autonomous driving.
Previous camera pose regression models consider only ideal scenarios where
there is little environmental perturbation. To deal with challenging driving
environments that may have changing seasons, weather, illumination, and the
presence of unstable objects, we propose RobustLoc, which derives its
robustness against perturbations from neural differential equations. Our model
uses a convolutional neural network to extract feature maps from multi-view
images, a robust neural differential equation diffusion block module to diffuse
information interactively, and a branched pose decoder with multi-layer
training to estimate the vehicle poses. Experiments demonstrate that RobustLoc
surpasses current state-of-the-art camera pose regression models and achieves
robust performance in various environments. Our code is released at:
https://github.com/sijieaaa/RobustLoc",https://github.com/sijieaaa/RobustLoc,-1
Mildly Conservative Q-Learning for Offline Reinforcement Learning,0.595894,"Offline reinforcement learning (RL) defines the task of learning from a
static logged dataset without continually interacting with the environment. The
distribution shift between the learned policy and the behavior policy makes it
necessary for the value function to stay conservative such that
out-of-distribution (OOD) actions will not be severely overestimated. However,
existing approaches, penalizing the unseen actions or regularizing with the
behavior policy, are too pessimistic, which suppresses the generalization of
the value function and hinders the performance improvement. This paper explores
mild but enough conservatism for offline learning while not harming
generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD
actions are actively trained by assigning them proper pseudo Q values. We
theoretically show that MCQ induces a policy that behaves at least as well as
the behavior policy and no erroneous overestimation will occur for OOD actions.
Experimental results on the D4RL benchmarks demonstrate that MCQ achieves
remarkable performance compared with prior work. Furthermore, MCQ shows
superior generalization ability when transferring from offline to online, and
significantly outperforms baselines. Our code is publicly available at
https://github.com/dmksjfl/MCQ.",https://github.com/dmksjfl/MCQ,-1
The Self-Optimal-Transport Feature Transform,0.18752,"The Self-Optimal-Transport (SOT) feature transform is designed to upgrade the
set of features of a data instance to facilitate downstream matching or
grouping related tasks. The transformed set encodes a rich representation of
high order relations between the instance features. Distances between
transformed features capture their direct original similarity and their third
party agreement regarding similarity to other features in the set. A particular
min-cost-max-flow fractional matching problem, whose entropy regularized
version can be approximated by an optimal transport (OT) optimization, results
in our transductive transform which is efficient, differentiable, equivariant,
parameterless and probabilistically interpretable. Empirically, the transform
is highly effective and flexible in its use, consistently improving networks it
is inserted into, in a variety of tasks and training schemes. We demonstrate
its merits through the problem of unsupervised clustering and its efficiency
and wide applicability for few-shot-classification, with state-of-the-art
results, and large-scale person re-identification.",None,-1
SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels,0.611011,"Deep neural networks are prone to overfitting noisy labels, resulting in poor
generalization performance. To overcome this problem, we present a simple and
effective method self-ensemble label correction (SELC) to progressively correct
noisy labels and refine the model. We look deeper into the memorization
behavior in training with noisy labels and observe that the network outputs are
reliable in the early stage. To retain this reliable knowledge, SELC uses
ensemble predictions formed by an exponential moving average of network outputs
to update the original noisy labels. We show that training with SELC refines
the model by gradually reducing supervision from noisy labels and increasing
supervision from ensemble predictions. Despite its simplicity, compared with
many state-of-the-art methods, SELC obtains more promising and stable results
in the presence of class-conditional, instance-dependent, and real-world label
noise. The code is available at https://github.com/MacLLL/SELC.",https://github.com/MacLLL/SELC,-1
A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling,0.0952316,"Document-level relation extraction (RE) aims to identify relations between
entities across multiple sentences. Most previous methods focused on
document-level RE under full supervision. However, in real-world scenario, it
is expensive and difficult to completely label all relations in a document
because the number of entity pairs in document-level RE grows quadratically
with the number of entities. To solve the common incomplete labeling problem,
we propose a unified positive-unlabeled learning framework - shift and squared
ranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled
(PU) learning on document-level RE for the first time. Considering that labeled
data of a dataset may lead to prior shift of unlabeled data, we introduce a PU
learning under prior shift of training data. Also, using none-class score as an
adaptive threshold, we propose squared ranking loss and prove its Bayesian
consistency with multi-label ranking metrics. Extensive experiments demonstrate
that our method achieves an improvement of about 14 F1 points relative to the
previous baseline with incomplete labeling. In addition, it outperforms
previous state-of-the-art results under both fully supervised and extremely
unlabeled settings as well.",https://github.com/www-Ye/SSR-PU,-1
AutoField: Automating Feature Selection in Deep Recommender Systems,0.0914157,"Feature quality has an impactful effect on recommendation performance.
Thereby, feature selection is a critical process in developing deep
learning-based recommender systems. Most existing deep recommender systems,
however, focus on designing sophisticated neural networks, while neglecting the
feature selection process. Typically, they just feed all possible features into
their proposed deep architectures, or select important features manually by
human experts. The former leads to non-trivial embedding parameters and extra
inference time, while the latter requires plenty of expert knowledge and human
labor effort. In this work, we propose an AutoML framework that can adaptively
select the essential feature fields in an automatic manner. Specifically, we
first design a differentiable controller network, which is capable of
automatically adjusting the probability of selecting a particular feature
field; then, only selected feature fields are utilized to retrain the deep
recommendation model. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our framework. We conduct further experiments
to investigate its properties, including the transferability, key components,
and parameter sensitivity.",https://github.com/rixwew/pytorch-fm,-1
Spectral Adversarial Training for Robust Graph Neural Network,0.0258498,"Recent studies demonstrate that Graph Neural Networks (GNNs) are vulnerable
to slight but adversarially designed perturbations, known as adversarial
examples. To address this issue, robust training methods against adversarial
examples have received considerable attention in the literature.
\emph{Adversarial Training (AT)} is a successful approach to learning a robust
model using adversarially perturbed training samples. Existing AT methods on
GNNs typically construct adversarial perturbations in terms of graph structures
or node features. However, they are less effective and fraught with challenges
on graph data due to the discreteness of graph structure and the relationships
between connected examples. In this work, we seek to address these challenges
and propose Spectral Adversarial Training (SAT), a simple yet effective
adversarial training approach for GNNs. SAT first adopts a low-rank
approximation of the graph structure based on spectral decomposition, and then
constructs adversarial perturbations in the spectral domain rather than
directly manipulating the original graph structure. To investigate its
effectiveness, we employ SAT on three widely used GNNs. Experimental results on
four public graph datasets demonstrate that SAT significantly improves the
robustness of GNNs against adversarial attacks without sacrificing
classification accuracy and training efficiency.",https://github.com/EdisonLeeeee/SAT,-1
Adapting Pretrained Text-to-Text Models for Long Text Sequences,0.0987368,"We present an empirical study of adapting an existing pretrained text-to-text
model for long-sequence inputs. Through a comprehensive study along three axes
of the pretraining pipeline -- model architecture, optimization objective, and
pretraining corpus, we propose an effective recipe to build long-context models
from existing short-context models. Specifically, we replace the full attention
in transformers with pooling-augmented blockwise attention, and pretrain the
model with a masked-span prediction task with spans of varying length. In terms
of the pretraining corpus, we find that using randomly concatenated
short-documents from a large open-domain corpus results in better performance
than using existing long document corpora which are typically limited in their
domain coverage. With these findings, we build a long-context model that
achieves competitive performance on long-text QA tasks and establishes the new
state of the art on five long-text summarization datasets, often outperforming
previous methods with larger model sizes. Our code has been released at
https://github.com/facebookresearch/bart_ls.",https://github.com/facebookresearch/bart_ls,-1
SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds,0.598635,"With the recent availability and affordability of commercial depth sensors
and 3D scanners, an increasing number of 3D (i.e., RGBD, point cloud) datasets
have been publicized to facilitate research in 3D computer vision. However,
existing datasets either cover relatively small areas or have limited semantic
annotations. Fine-grained understanding of urban-scale 3D scenes is still in
its infancy. In this paper, we introduce SensatUrban, an urban-scale UAV
photogrammetry point cloud dataset consisting of nearly three billion points
collected from three UK cities, covering 7.6 km^2. Each point in the dataset
has been labelled with fine-grained semantic annotations, resulting in a
dataset that is three times the size of the previous existing largest
photogrammetric point cloud dataset. In addition to the more commonly
encountered categories such as road and vegetation, urban-level categories
including rail, bridge, and river are also included in our dataset. Based on
this dataset, we further build a benchmark to evaluate the performance of
state-of-the-art segmentation algorithms. In particular, we provide a
comprehensive analysis and identify several key challenges limiting urban-scale
point cloud understanding. The dataset is available at
http://point-cloud-analysis.cs.ox.ac.uk.",None,-1
Self-Supervision Can Be a Good Few-Shot Learner,0.0647113,"Existing few-shot learning (FSL) methods rely on training with a large
labeled dataset, which prevents them from leveraging abundant unlabeled data.
From an information-theoretic perspective, we propose an effective unsupervised
FSL method, learning representations with self-supervision. Following the
InfoMax principle, our method learns comprehensive representations by capturing
the intrinsic structure of the data. Specifically, we maximize the mutual
information (MI) of instances and their representations with a low-bias MI
estimator to perform self-supervised pre-training. Rather than supervised
pre-training focusing on the discriminable features of the seen classes, our
self-supervised model has less bias toward the seen classes, resulting in
better generalization for unseen classes. We explain that supervised
pre-training and self-supervised pre-training are actually maximizing different
MI objectives. Extensive experiments are further conducted to analyze their FSL
performance with various training settings. Surprisingly, the results show that
self-supervised pre-training can outperform supervised pre-training under the
appropriate conditions. Compared with state-of-the-art FSL methods, our
approach achieves comparable performance on widely used FSL benchmarks without
any labels of the base classes.",None,-1
DEMETR: Diagnosing Evaluation Metrics for Translation,0.234536,"While machine translation evaluation metrics based on string overlap (e.g.,
BLEU) have their limitations, their computations are transparent: the BLEU
score assigned to a particular candidate translation can be traced back to the
presence or absence of certain words. The operations of newer learned metrics
(e.g., BLEURT, COMET), which leverage pretrained language models to achieve
higher correlations with human quality judgments than BLEU, are opaque in
comparison. In this paper, we shed light on the behavior of these learned
metrics by creating DEMETR, a diagnostic dataset with 31K English examples
(translated from 10 source languages) for evaluating the sensitivity of MT
evaluation metrics to 35 different linguistic perturbations spanning semantic,
syntactic, and morphological error categories. All perturbations were carefully
designed to form minimal pairs with the actual translation (i.e., differ in
only one aspect). We find that learned metrics perform substantially better
than string-based metrics on DEMETR. Additionally, learned metrics differ in
their sensitivity to various phenomena (e.g., BERTScore is sensitive to
untranslated words but relatively insensitive to gender manipulation, while
COMET is much more sensitive to word repetition than to aspectual changes). We
publicly release DEMETR to spur more informed future development of machine
translation evaluation metrics",https://github.com/marzenakrp/demetr,-1
HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics,0.118172,"We propose a method that leverages graph neural networks, multi-level message
passing, and unsupervised training to enable real-time prediction of realistic
clothing dynamics. Whereas existing methods based on linear blend skinning must
be trained for specific garments, our method is agnostic to body shape and
applies to tight-fitting garments as well as loose, free-flowing clothing. Our
method furthermore handles changes in topology (e.g., garments with buttons or
zippers) and material properties at inference time. As one key contribution, we
propose a hierarchical message-passing scheme that efficiently propagates stiff
stretching modes while preserving local detail. We empirically show that our
method outperforms strong baselines quantitatively and that its results are
perceived as more realistic than state-of-the-art methods.",None,-1
Spatiotemporal Costmap Inference for MPC via Deep Inverse Reinforcement Learning,0.392335,"It can be difficult to autonomously produce driver behavior so that it
appears natural to other traffic participants. Through Inverse Reinforcement
Learning (IRL), we can automate this process by learning the underlying reward
function from human demonstrations. We propose a new IRL algorithm that learns
a goal-conditioned spatiotemporal reward function. The resulting costmap is
used by Model Predictive Controllers (MPCs) to perform a task without any
hand-designing or hand-tuning of the cost function. We evaluate our proposed
Goal-conditioned SpatioTemporal Zeroing Maximum Entropy Deep IRL (GSTZ)-MEDIRL
framework together with MPC in the CARLA simulator for autonomous driving, lane
keeping, and lane changing tasks in a challenging dense traffic highway
scenario. Our proposed methods show higher success rates compared to other
baseline methods including behavior cloning, state-of-the-art RL policies, and
MPC with a learning-based behavior prediction model.",None,-1
VideoDex: Learning Dexterity from Internet Videos,0.0240481,"To build general robotic agents that can operate in many environments, it is
often imperative for the robot to collect experience in the real world.
However, this is often not feasible due to safety, time, and hardware
restrictions. We thus propose leveraging the next best thing as real-world
experience: internet videos of humans using their hands. Visual priors, such as
visual features, are often learned from videos, but we believe that more
information from videos can be utilized as a stronger prior. We build a
learning algorithm, VideoDex, that leverages visual, action, and physical
priors from human video datasets to guide robot behavior. These actions and
physical priors in the neural network dictate the typical human behavior for a
particular robot task. We test our approach on a robot arm and dexterous
hand-based system and show strong results on various manipulation tasks,
outperforming various state-of-the-art methods. Videos at
https://video-dex.github.io",https://github.com/AGI-Labs/robot_baselines,-1
Exploring Patch-wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks,0.348596,"Recently, contrastive learning-based image translation methods have been
proposed, which contrasts different spatial locations to enhance the spatial
correspondence. However, the methods often ignore the diverse semantic relation
within the images. To address this, here we propose a novel semantic relation
consistency (SRC) regularization along with the decoupled contrastive learning,
which utilize the diverse semantics by focusing on the heterogeneous semantics
between the image patches of a single image. To further improve the
performance, we present a hard negative mining by exploiting the semantic
relation. We verified our method for three tasks: single-modal and multi-modal
image translations, and GAN compression task for image translation.
Experimental results confirmed the state-of-art performance of our method in
all the three tasks.",https://github.com/mit-han-lab/gan-compression,-1
Unifying Short and Long-Term Tracking with Graph Hierarchies,0.0780758,"Tracking objects over long videos effectively means solving a spectrum of
problems, from short-term association for un-occluded objects to long-term
association for objects that are occluded and then reappear in the scene.
Methods tackling these two tasks are often disjoint and crafted for specific
scenarios, and top-performing approaches are often a mix of techniques, which
yields engineering-heavy solutions that lack generality. In this work, we
question the need for hybrid approaches and introduce SUSHI, a unified and
scalable multi-object tracker. Our approach processes long clips by splitting
them into a hierarchy of subclips, which enables high scalability. We leverage
graph neural networks to process all levels of the hierarchy, which makes our
model unified across temporal scales and highly general. As a result, we obtain
significant improvements over state-of-the-art on four diverse datasets. Our
code and models are available at bit.ly/sushi-mot.",None,-1
Realistic Defocus Blur for Multiplane Computer-Generated Holography,0.304498,"This paper introduces a new multiplane CGH computation method to reconstruct
artefact-free high-quality holograms with natural-looking defocus blur. Our
method introduces a new targeting scheme and a new loss function. While the
targeting scheme accounts for defocused parts of the scene at each depth plane,
the new loss function analyzes focused and defocused parts separately in
reconstructed images. Our method support phase-only CGH calculations using
various iterative (e.g., Gerchberg-Saxton, Gradient Descent) and non-iterative
(e.g., Double Phase) CGH techniques. We achieve our best image quality using a
modified gradient descent-based optimization recipe where we introduce a
constraint inspired by the double phase method. We validate our method
experimentally using our proof-of-concept holographic display, comparing
various algorithms, including multi-depth scenes with sparse and dense
contents.",None,-1
Fusing Convolutional Neural Network and Geometric Constraint for Image-based Indoor Localization,0.0368981,"This paper proposes a new image-based localization framework that explicitly
localizes the camera/robot by fusing Convolutional Neural Network (CNN) and
sequential images' geometric constraints. The camera is localized using a
single or few observed images and training images with 6-degree-of-freedom pose
labels. A Siamese network structure is adopted to train an image descriptor
network, and the visually similar candidate image in the training set is
retrieved to localize the testing image geometrically. Meanwhile, a
probabilistic motion model predicts the pose based on a constant velocity
assumption. The two estimated poses are finally fused using their uncertainties
to yield an accurate pose prediction. This method leverages the geometric
uncertainty and is applicable in indoor scenarios predominated by diffuse
illumination. Experiments on simulation and real data sets demonstrate the
efficiency of our proposed method. The results further show that combining the
CNN-based framework with geometric constraint achieves better accuracy when
compared with CNN-only methods, especially when the training data size is
small.",None,-1
Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries,0.355674,"Knowledge graph (KG) embeddings have been a mainstream approach for reasoning
over incomplete KGs. However, limited by their inherently shallow and static
architectures, they can hardly deal with the rising focus on complex logical
queries, which comprise logical operators, imputed edges, multiple source
entities, and unknown intermediate entities. In this work, we present the
Knowledge Graph Transformer (kgTransformer) with masked pre-training and
fine-tuning strategies. We design a KG triple transformation method to enable
Transformer to handle KGs, which is further strengthened by the
Mixture-of-Experts (MoE) sparse activation. We then formulate the complex
logical queries as masked prediction and introduce a two-stage masked
pre-training strategy to improve transferability and generalizability.
Extensive experiments on two benchmarks demonstrate that kgTransformer can
consistently outperform both KG embedding-based baselines and advanced encoders
on nine in-domain and out-of-domain reasoning tasks. Additionally,
kgTransformer can reason with explainability via providing the full reasoning
paths to interpret given answers.",https://github.com/THUDM/kgTransformer,-1
The four-fifths rule is not disparate impact: a woeful tale of epistemic trespassing in algorithmic fairness,0.36815,"Computer scientists are trained to create abstractions that simplify and
generalize. However, a premature abstraction that omits crucial contextual
details creates the risk of epistemic trespassing, by falsely asserting its
relevance into other contexts. We study how the field of responsible AI has
created an imperfect synecdoche by abstracting the four-fifths rule (a.k.a. the
4/5 rule or 80% rule), a single part of disparate impact discrimination law,
into the disparate impact metric. This metric incorrectly introduces a new
deontic nuance and new potentials for ethical harms that were absent in the
original 4/5 rule. We also survey how the field has amplified the potential for
harm in codifying the 4/5 rule into popular AI fairness software toolkits. The
harmful erasure of legal nuances is a wake-up call for computer scientists to
self-critically re-evaluate the abstractions they create and use, particularly
in the interdisciplinary field of AI ethics.",None,-1
Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control,0.602108,"Abstractive summarization systems leveraging pre-training language models
have achieved superior results on benchmark datasets. However, such models have
been shown to be more prone to hallucinate facts that are unfaithful to the
input context. In this paper, we propose a method to remedy entity-level
extrinsic hallucinations with Entity Coverage Control (ECC). We first compute
entity coverage precision and prepend the corresponding control code for each
training example, which implicitly guides the model to recognize faithfulness
contents in the training phase. We further extend our method via intermediate
fine-tuning on large but noisy data extracted from Wikipedia to unlock
zero-shot summarization. We show that the proposed method leads to more
faithful and salient abstractive summarization in supervised fine-tuning and
zero-shot settings according to our experimental results on three benchmark
datasets XSum, Pubmed, and SAMSum of very different domains and styles.",None,-1
Training Robots without Robots: Deep Imitation Learning for Master-to-Robot Policy Transfer,0.081307,"Deep imitation learning is promising for robot manipulation because it only
requires demonstration samples. In this study, deep imitation learning is
applied to tasks that require force feedback. However, existing demonstration
methods have deficiencies; bilateral teleoperation requires a complex control
scheme and is expensive, and kinesthetic teaching suffers from visual
distractions from human intervention. This research proposes a new
master-to-robot (M2R) policy transfer system that does not require robots for
teaching force feedback-based manipulation tasks. The human directly
demonstrates a task using a controller. This controller resembles the kinematic
parameters of the robot arm and uses the same end-effector with force/torque
(F/T) sensors to measure the force feedback. Using this controller, the
operator can feel force feedback without a bilateral system. The proposed
method can overcome domain gaps between the master and robot using gaze-based
imitation learning and a simple calibration method. Furthermore, a Transformer
is applied to infer policy from F/T sensory input. The proposed system was
evaluated on a bottle-cap-opening task that requires force feedback.",None,-1
Machine Learning Methods in Solving the Boolean Satisfiability Problem,0.150833,"This paper reviews the recent literature on solving the Boolean
satisfiability problem (SAT), an archetypal NP-complete problem, with the help
of machine learning techniques. Despite the great success of modern SAT solvers
to solve large industrial instances, the design of handcrafted heuristics is
time-consuming and empirical. Under the circumstances, the flexible and
expressive machine learning methods provide a proper alternative to solve this
long-standing problem. We examine the evolving ML-SAT solvers from naive
classifiers with handcrafted features to the emerging end-to-end SAT solvers
such as NeuroSAT, as well as recent progress on combinations of existing CDCL
and local search solvers with machine learning methods. Overall, solving SAT
with machine learning is a promising yet challenging research topic. We
conclude the limitations of current works and suggest possible future
directions.",None,-1
SpanProto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition,0.289136,"Few-shot Named Entity Recognition (NER) aims to identify named entities with
very little annotated data. Previous methods solve this problem based on
token-wise classification, which ignores the information of entity boundaries,
and inevitably the performance is affected by the massive non-entity tokens. To
this end, we propose a seminal span-based prototypical network (SpanProto) that
tackles few-shot NER via a two-stage approach, including span extraction and
mention classification. In the span extraction stage, we transform the
sequential tags into a global boundary matrix, enabling the model to focus on
the explicit boundary information. For mention classification, we leverage
prototypical learning to capture the semantic representations for each labeled
span and make the model better adapt to novel-class entities. To further
improve the model performance, we split out the false positives generated by
the span extractor but not labeled in the current episode set, and then present
a margin-based loss to separate them from each prototype region. Experiments
over multiple benchmarks demonstrate that our model outperforms strong
baselines by a large margin.",https://github.com/alibaba/EasyNLP,-1
LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning,0.349023,"Recently, deep learning models have made great progress in MWP solving on
answer accuracy. However, they are uninterpretable since they mainly rely on
shallow heuristics to achieve high performance without understanding and
reasoning the grounded math logic. To address this issue and make a step
towards interpretable MWP solving, we first construct a high-quality MWP
dataset named InterMWP which consists of 11,495 MWPs and annotates
interpretable logical formulas based on algebraic knowledge as the grounded
linguistic logic of each solution equation. Different from existing MWP
datasets, our InterMWP benchmark asks for a solver to not only output the
solution expressions but also predict the corresponding logical formulas. We
further propose a novel approach with logical prompt and interpretation
generation, called LogicSolver. For each MWP, our LogicSolver first retrieves
some highly-correlated algebraic knowledge and then passes them to the backbone
model as prompts to improve the semantic representations of MWPs. With these
improved semantic representations, our LogicSolver generates corresponding
solution expressions and interpretable knowledge formulas in accord with the
generated solution expressions, simultaneously. Experimental results show that
our LogicSolver has stronger logical formula-based interpretability than
baselines while achieving higher answer accuracy with the help of logical
prompts, simultaneously. The source code and dataset is available at
https://github.com/yangzhch6/InterMWP.",https://github.com/yangzhch6/InterMWP,-1
Optimal Representations for Covariate Shift,0.389606,"Machine learning systems often experience a distribution shift between
training and testing. In this paper, we introduce a simple variational
objective whose optima are exactly the set of all representations on which risk
minimizers are guaranteed to be robust to any distribution shift that preserves
the Bayes predictor, e.g., covariate shifts. Our objective has two components.
First, a representation must remain discriminative for the task, i.e., some
predictor must be able to simultaneously minimize the source and target risk.
Second, the representation's marginal support needs to be the same across
source and target. We make this practical by designing self-supervised
objectives that only use unlabelled data and augmentations to train robust
representations. Our objectives give insights into the robustness of CLIP, and
further improve CLIP's representations to achieve SOTA results on DomainBed.",https://github.com/ryoungj/optdom,-1
DFNet: Enhance Absolute Pose Regression with Direct Feature Matching,0.853611,"We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. By incorporating
exposure-adaptive novel view synthesis, our method successfully addresses
photometric distortions in outdoor environments that existing photometric-based
methods fail to handle. With domain-invariant feature matching, our solution
improves pose regression accuracy using semi-supervised learning on unlabeled
data. In particular, the pipeline consists of two components: Novel View
Synthesizer and DFNet. The former synthesizes novel views compensating for
changes in exposure and the latter regresses camera poses and extracts robust
features that close the domain gap between real images and synthetic ones.
Furthermore, we introduce an online synthetic data generation scheme. We show
that these approaches effectively enhance camera pose estimation both in indoor
and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by
outperforming existing single-image APR methods by as much as 56%, comparable
to 3D structure-based methods.",None,-1
Efficient Training of Language Models to Fill in the Middle,0.990984,"We show that autoregressive language models can learn to infill text after we
apply a straightforward transformation to the dataset, which simply moves a
span of text from the middle of a document to its end. While this data
augmentation has garnered much interest in recent years, we provide extensive
evidence that training models with a large fraction of data transformed in this
way does not harm the original left-to-right generative capability, as measured
by perplexity and sampling evaluations across a wide range of scales. Given the
usefulness, simplicity, and efficiency of training models to fill-in-the-middle
(FIM), we suggest that future autoregressive language models be trained with
FIM by default. To this end, we run a series of ablations on key
hyperparameters, such as the data transformation frequency, the structure of
the transformation, and the method of selecting the infill span. We use these
ablations to prescribe strong default settings and best practices to train FIM
models. We have released our best infilling model trained with best practices
in our API, and release our infilling benchmarks to aid future research.",None,-1
A Holistic Framework for Analyzing the COVID-19 Vaccine Debate,0.399349,"The Covid-19 pandemic has led to infodemic of low quality information leading
to poor health decisions. Combating the outcomes of this infodemic is not only
a question of identifying false claims, but also reasoning about the decisions
individuals make. In this work we propose a holistic analysis framework
connecting stance and reason analysis, and fine-grained entity level moral
sentiment analysis. We study how to model the dependencies between the
different level of analysis and incorporate human insights into the learning
process. Experiments show that our framework provides reliable predictions even
in the low-supervision settings.",https://gitlab.com/mlpacheco/covid-moral-foundations,-1
Fully Automated End-to-End Fake Audio Detection,0.12663,"The existing fake audio detection systems often rely on expert experience to
design the acoustic features or manually design the hyperparameters of the
network structure. However, artificial adjustment of the parameters can have a
relatively obvious influence on the results. It is almost impossible to
manually set the best set of parameters. Therefore this paper proposes a fully
automated end-toend fake audio detection method. We first use wav2vec
pre-trained model to obtain a high-level representation of the speech.
Furthermore, for the network structure, we use a modified version of the
differentiable architecture search (DARTS) named light-DARTS. It learns deep
speech representations while automatically learning and optimizing complex
neural structures consisting of convolutional operations and residual blocks.
The experimental results on the ASVspoof 2019 LA dataset show that our proposed
system achieves an equal error rate (EER) of 1.08%, which outperforms the
state-of-the-art single system.",None,-1
Outliers Dimensions that Disrupt Transformers Are Driven by Frequency,0.113438,"While Transformer-based language models are generally very robust to pruning,
there is the recently discovered outlier phenomenon: disabling only 48 out of
110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We
replicate the original evidence for the outlier phenomenon and we link it to
the geometry of the embedding space. We find that in both BERT and RoBERTa the
magnitude of hidden state coefficients corresponding to outlier dimensions
correlates with the frequency of encoded tokens in pre-training data, and it
also contributes to the ""vertical"" self-attention pattern enabling the model to
focus on the special tokens. This explains the drop in performance from
disabling the outliers, and it suggests that to decrease anisotropicity in
future models we need pre-training schemas that would better take into account
the skewed token distributions.",https://github.com/gpucce/outliersvsfreq/,-1
Ask Me Anything: A simple strategy for prompting language models,0.411621,"Large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt that demonstrates how to perform the task and
no additional training. Prompting is a brittle process wherein small
modifications to the prompt can cause large variations in the model
predictions, and therefore significant effort is dedicated towards designing a
painstakingly ""perfect prompt"" for a task. To mitigate the high degree of
effort involved in prompt-design, we instead ask whether producing multiple
effective, yet imperfect, prompts and aggregating them can lead to a high
quality prompting strategy. Our observations motivate our proposed prompting
method, ASK ME ANYTHING (AMA). We first develop an understanding of the
effective prompt formats, finding that question-answering (QA) prompts, which
encourage open-ended generation (""Who went to the park?"") tend to outperform
those that restrict the model outputs (""John went to the park. Output True or
False.""). Our approach recursively uses the LLM itself to transform task inputs
to the effective QA format. We apply the collected prompts to obtain several
noisy votes for the input's true label. We find that the prompts can have very
different accuracies and complex dependencies and thus propose to use weak
supervision, a procedure for combining the noisy predictions, to produce the
final predictions for the inputs. We evaluate AMA across open-source model
families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B
parameters), demonstrating an average performance lift of 10.2% over the
few-shot baseline. This simple strategy enables the open-source GPT-J-6B model
to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular
benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms
few-shot GPT3-175B. We release our code here:
https://github.com/HazyResearch/ama_prompting",None,-1
The Isabelle ENIGMA,0.171002,"We significantly improve the performance of the E automated theorem prover on
the Isabelle Sledgehammer problems by combining learning and theorem proving in
several ways. In particular, we develop targeted versions of the ENIGMA
guidance for the Isabelle problems, targeted versions of neural premise
selection, and targeted strategies for E. The methods are trained in several
iterations over hundreds of thousands untyped and typed first-order problems
extracted from Isabelle. Our final best single-strategy ENIGMA and premise
selection system improves the best previous version of E by 25.3% in 15
seconds, outperforming also all other previous ATP and SMT systems.",https://github.com/ai4reason/ATP_Proofs,-1
AI-Aided Integrated Terrestrial and Non-Terrestrial 6G Solutions for Sustainable Maritime Networking,0.457816,"The maritime industry is experiencing a technological revolution that affects
shipbuilding, operation of both seagoing and inland vessels, cargo management,
and working practices in harbors. This ongoing transformation is driven by the
ambition to make the ecosystem more sustainable and cost-efficient.
Digitalization and automation help achieve these goals by transforming shipping
and cruising into a much more cost- and energy-efficient, and decarbonized
industry segment. The key enablers in these processes are always-available
connectivity and content delivery services, which can not only aid shipping
companies in improving their operational efficiency and reducing carbon
emissions but also contribute to enhanced crew welfare and passenger
experience. Due to recent advancements in integrating high-capacity and
ultra-reliable terrestrial and non-terrestrial networking technologies,
ubiquitous maritime connectivity is becoming a reality. To cope with the
increased complexity of managing these integrated systems, this article
advocates the use of artificial intelligence and machine learning-based
approaches to meet the service requirements and energy efficiency targets in
various maritime communications scenarios.",None,-1
WaveY-Net: Physics-augmented deep learning for high-speed electromagnetic simulation and optimization,0.186061,"The calculation of electromagnetic field distributions within structured
media is central to the optimization and validation of photonic devices. We
introduce WaveY-Net, a hybrid data- and physics-augmented convolutional neural
network that can predict electromagnetic field distributions with ultra fast
speeds and high accuracy for entire classes of dielectric photonic structures.
This accuracy is achieved by training the neural network to learn only the
magnetic near-field distributions of a system and to use a discrete formalism
of Maxwell's equations in two ways: as physical constraints in the loss
function and as a means to calculate the electric fields from the magnetic
fields. As a model system, we construct a surrogate simulator for periodic
silicon nanostructure arrays and show that the high speed simulator can be
directly and effectively used in the local and global freeform optimization of
metagratings. We anticipate that physics-augmented networks will serve as a
viable Maxwell simulator replacement for many classes of photonic systems,
transforming the way they are designed.",None,-1
The Arabic Ontology -- An Arabic Wordnet with Ontologically Clean Content,0.63486,"We present a formal Arabic wordnet built on the basis of a carefully designed
ontology hereby referred to as the Arabic Ontology. The ontology provides a
formal representation of the concepts that the Arabic terms convey, and its
content was built with ontological analysis in mind, and benchmarked to
scientific advances and rigorous knowledge sources as much as this is possible,
rather than to only speakers' beliefs as lexicons typically are. A
comprehensive evaluation was conducted thereby demonstrating that the current
version of the top-levels of the ontology can top the majority of the Arabic
meanings. The ontology consists currently of about 1,300 well-investigated
concepts in addition to 11,000 concepts that are partially validated. The
ontology is accessible and searchable through a lexicographic search engine
(https://ontology.birzeit.edu) that also includes about 150 Arabic-multilingual
lexicons, and which are being mapped and enriched using the ontology. The
ontology is fully mapped with Princeton WordNet, Wikidata, and other resources.",None,-1
Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors,0.224079,"We present a method for inferring diverse 3D models of human-object
interactions from images. Reasoning about how humans interact with objects in
complex scenes from a single 2D image is a challenging task given ambiguities
arising from the loss of information through projection. In addition, modeling
3D interactions requires the generalization ability towards diverse object
categories and interaction types. We propose an action-conditioned modeling of
interactions that allows us to infer diverse 3D arrangements of humans and
objects without supervision on contact regions or 3D scene geometry. Our method
extracts high-level commonsense knowledge from large language models (such as
GPT-3), and applies them to perform 3D reasoning of human-object interactions.
Our key insight is priors extracted from large language models can help in
reasoning about human-object contacts from textural prompts only. We
quantitatively evaluate the inferred 3D models on a large human-object
interaction dataset and show how our method leads to better 3D reconstructions.
We further qualitatively evaluate the effectiveness of our method on real
images and demonstrate its generalizability towards interaction types and
object categories.",https://github.com/open-mmlab/mmpose,-1
Neural Face Identification in a 2D Wireframe Projection of a Manifold Object,0.0555682,"In computer-aided design (CAD) systems, 2D line drawings are commonly used to
illustrate 3D object designs. To reconstruct the 3D models depicted by a single
2D line drawing, an important key is finding the edge loops in the line drawing
which correspond to the actual faces of the 3D object. In this paper, we
approach the classical problem of face identification from a novel data-driven
point of view. We cast it as a sequence generation problem: starting from an
arbitrary edge, we adopt a variant of the popular Transformer model to predict
the edges associated with the same face in a natural order. This allows us to
avoid searching the space of all possible edge loops with various hand-crafted
rules and heuristics as most existing methods do, deal with challenging cases
such as curved surfaces and nested edge loops, and leverage additional cues
such as face types. We further discuss how possibly imperfect predictions can
be used for 3D object reconstruction.",https://github.com/tpaviot/pythonocc,-1
CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion,0.506304,"Knowledge graphs store a large number of factual triples while they are still
incomplete, inevitably. The previous knowledge graph completion (KGC) models
predict missing links between entities merely relying on fact-view data,
ignoring the valuable commonsense knowledge. The previous knowledge graph
embedding (KGE) techniques suffer from invalid negative sampling and the
uncertainty of fact-view link prediction, limiting KGC's performance. To
address the above challenges, we propose a novel and scalable Commonsense-Aware
Knowledge Embedding (CAKE) framework to automatically extract commonsense from
factual triples with entity concepts. The generated commonsense augments
effective self-supervision to facilitate both high-quality negative sampling
(NS) and joint commonsense and fact-view link prediction. Experimental results
on the KGC task demonstrate that assembling our framework could enhance the
performance of the original KGE models, and the proposed commonsense-aware NS
module is superior to other NS techniques. Besides, our proposed framework
could be easily adaptive to various KGE models and explain the predicted
results.",https://github.com/ngl567/CAKE,-1
Sparse Probabilistic Circuits via Pruning and Growing,0.13412,"Probabilistic circuits (PCs) are a tractable representation of probability
distributions allowing for exact and efficient computation of likelihoods and
marginals. There has been significant recent progress on improving the scale
and expressiveness of PCs. However, PC training performance plateaus as model
size increases. We discover that most capacity in existing large PC structures
is wasted: fully-connected parameter layers are only sparsely used. We propose
two operations: pruning and growing, that exploit the sparsity of PC
structures. Specifically, the pruning operation removes unimportant
sub-networks of the PC for model compression and comes with theoretical
guarantees. The growing operation increases model capacity by increasing the
size of the latent space. By alternatingly applying pruning and growing, we
increase the capacity that is meaningfully used, allowing us to significantly
scale up PC learning. Empirically, our learner achieves state-of-the-art
likelihoods on MNIST-family image datasets and on Penn Tree Bank language data
compared to other PC learners and less tractable deep generative models such as
flow-based models and variational autoencoders (VAEs).",https://github.com/UCLA-StarAI/SparsePC,-1
Human-Object Interaction Detection via Disentangled Transformer,0.0592369,"Human-Object Interaction Detection tackles the problem of joint localization
and classification of human object interactions. Existing HOI transformers
either adopt a single decoder for triplet prediction, or utilize two parallel
decoders to detect individual objects and interactions separately, and compose
triplets by a matching process. In contrast, we decouple the triplet prediction
into human-object pair detection and interaction classification. Our main
motivation is that detecting the human-object instances and classifying
interactions accurately needs to learn representations that focus on different
regions. To this end, we present Disentangled Transformer, where both encoder
and decoder are disentangled to facilitate learning of two sub-tasks. To
associate the predictions of disentangled decoders, we first generate a unified
representation for HOI triplets with a base decoder, and then utilize it as
input feature of each disentangled decoder. Extensive experiments show that our
method outperforms prior work on two public HOI benchmarks by a sizeable
margin. Code will be available.",https://github.com/facebookresearch/detectron2,-1
Transformer Quality in Linear Time,0.233874,"We revisit the design choices in Transformers, and propose methods to address
their weaknesses in handling long sequences. First, we propose a simple layer
named gated attention unit, which allows the use of a weaker single-head
attention with minimal quality loss. We then propose a linear approximation
method complementary to this new layer, which is accelerator-friendly and
highly competitive in quality. The resulting model, named FLASH, matches the
perplexity of improved Transformers over both short (512) and long (8K) context
lengths, achieving training speedups of up to 4.9$\times$ on Wiki-40B and
12.1$\times$ on PG-19 for auto-regressive language modeling, and 4.8$\times$ on
C4 for masked language modeling.",None,22641
Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting,0.100494,"This paper presents work on novel machine translation (MT) systems between
spoken and signed languages, where signed languages are represented in
SignWriting, a sign language writing system. Our work seeks to address the lack
of out-of-the-box support for signed languages in current MT systems and is
based on the SignBank dataset, which contains pairs of spoken language text and
SignWriting content. We introduce novel methods to parse, factorize, decode,
and evaluate SignWriting, leveraging ideas from neural factored MT. In a
bilingual setup--translating from American Sign Language to (American)
English--our method achieves over 30 BLEU, while in two multilingual
setups--translating in both directions between spoken languages and signed
languages--we achieve over 20 BLEU. We find that common MT techniques used to
improve spoken language translation similarly affect the performance of sign
language translation. These findings validate our use of an intermediate text
representation for signed languages to include them in natural language
processing research.",https://github.com/sign-language-processing/sign-language-processing.github.io,1923
Soft Diffusion: Score Matching for General Corruptions,0.392099,"We define a broader family of corruption processes that generalizes
previously known diffusion models. To reverse these general diffusions, we
propose a new objective called Soft Score Matching that provably learns the
score function for any linear corruption process and yields state of the art
results for CelebA. Soft Score Matching incorporates the degradation process in
the network. Our new loss trains the model to predict a clean image,
\textit{that after corruption}, matches the diffused observation. We show that
our objective learns the gradient of the likelihood under suitable regularity
conditions for a family of corruption processes. We further develop a
principled way to select the corruption levels for general diffusion processes
and a novel sampling method that we call Momentum Sampler. We show
experimentally that our framework works for general linear corruption
processes, such as Gaussian blur and masking. We achieve state-of-the-art FID
score $1.85$ on CelebA-64, outperforming all previous linear diffusion models.
We also show significant computational benefits compared to vanilla denoising
diffusion.",None,28054
cosFormer: Rethinking Softmax in Attention,0.998054,"Transformer has shown great successes in natural language processing,
computer vision, and audio processing. As one of its core components, the
softmax attention helps to capture long-range dependencies yet prohibits its
scale-up due to the quadratic space and time complexity to the sequence length.
Kernel methods are often adopted to reduce the complexity by approximating the
softmax operator. Nevertheless, due to the approximation errors, their
performances vary in different tasks/corpus and suffer crucial performance
drops when compared with the vanilla softmax attention. In this paper, we
propose a linear transformer called cosFormer that can achieve comparable or
better accuracy to the vanilla transformer in both casual and cross attentions.
cosFormer is based on two key properties of softmax attention: i).
non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme
that can concentrate the distribution of the attention matrix. As its linear
substitute, cosFormer fulfills these properties with a linear operator and a
cosine-based distance re-weighting mechanism. Extensive experiments on language
modeling and text understanding tasks demonstrate the effectiveness of our
method. We further examine our method on long sequences and achieve
state-of-the-art performance on the Long-Range Arena benchmark. The source code
is available at https://github.com/OpenNLPLab/cosFormer.",None,2443
Exploring Target Representations for Masked Autoencoders,0.166271,"Masked autoencoders have become popular training paradigms for
self-supervised visual representation learning. These models randomly mask a
portion of the input and reconstruct the masked portion according to the target
representations. In this paper, we first show that a careful choice of the
target representation is unnecessary for learning good representations, since
different targets tend to derive similarly behaved models. Driven by this
observation, we propose a multi-stage masked distillation pipeline and use a
randomly initialized model as the teacher, enabling us to effectively train
high-capacity models without any efforts to carefully design target
representations. Interestingly, we further explore using teachers of larger
capacity, obtaining distilled students with remarkable transferring ability. On
different tasks of classification, transfer learning, object detection, and
semantic segmentation, the proposed method to perform masked knowledge
distillation with bootstrapped teachers (dBOT) outperforms previous
self-supervised methods by nontrivial margins. We hope our findings, as well as
the proposed method, could motivate people to rethink the roles of target
representations in pre-training masked autoencoders.The code and pre-trained
models are publicly available at https://github.com/liuxingbin/dbot.",https://github.com/liuxingbin/dbot,28838
Intelligent problem-solving as integrated hierarchical reinforcement learning,0.348196,"According to cognitive psychology and related disciplines, the development of
complex problem-solving behaviour in biological agents depends on hierarchical
cognitive mechanisms. Hierarchical reinforcement learning is a promising
computational approach that may eventually yield comparable problem-solving
behaviour in artificial agents and robots. However, to date the problem-solving
abilities of many human and non-human animals are clearly superior to those of
artificial systems. Here, we propose steps to integrate biologically inspired
hierarchical mechanisms to enable advanced problem-solving skills in artificial
agents. Therefore, we first review the literature in cognitive psychology to
highlight the importance of compositional abstraction and predictive
processing. Then we relate the gained insights with contemporary hierarchical
reinforcement learning methods. Interestingly, our results suggest that all
identified cognitive mechanisms have been implemented individually in isolated
computational architectures, raising the question of why there exists no single
unifying architecture that integrates them. As our final contribution, we
address this question by providing an integrative perspective on the
computational challenges to develop such a unifying architecture. We expect our
results to guide the development of more sophisticated cognitively inspired
hierarchical machine learning architectures.",None,13187
Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles,0.791707,"Video Anomaly Detection (VAD) is an important topic in computer vision.
Motivated by the recent advances in self-supervised learning, this paper
addresses VAD by solving an intuitive yet challenging pretext task, i.e.,
spatio-temporal jigsaw puzzles, which is cast as a multi-label fine-grained
classification problem. Our method exhibits several advantages over existing
works: 1) the spatio-temporal jigsaw puzzles are decoupled in terms of spatial
and temporal dimensions, responsible for capturing highly discriminative
appearance and motion features, respectively; 2) full permutations are used to
provide abundant jigsaw puzzles covering various difficulty levels, allowing
the network to distinguish subtle spatio-temporal differences between normal
and abnormal events; and 3) the pretext task is tackled in an end-to-end manner
without relying on any pre-trained models. Our method outperforms
state-of-the-art counterparts on three public benchmarks. Especially on
ShanghaiTech Campus, the result is superior to reconstruction and
prediction-based methods by a large margin.",https://github.com/wizyoung/YOLOv3,29064
Questions Are All You Need to Train a Dense Passage Retriever,0.706879,"We introduce ART, a new corpus-level autoencoding approach for training dense
retrieval models that does not require any labeled training data. Dense
retrieval is a central challenge for open-domain tasks, such as Open QA, where
state-of-the-art methods typically require large supervised datasets with
custom hard-negative mining and denoising of positive examples. ART, in
contrast, only requires access to unpaired inputs and outputs (e.g. questions
and potential answer documents). It uses a new document-retrieval autoencoding
scheme, where (1) an input question is used to retrieve a set of evidence
documents, and (2) the documents are then used to compute the probability of
reconstructing the original question. Training for retrieval based on question
reconstruction enables effective unsupervised learning of both document and
question encoders, which can be later incorporated into complete Open QA
systems without any further finetuning. Extensive experiments demonstrate that
ART obtains state-of-the-art results on multiple QA retrieval benchmarks with
only generic initialization from a pre-trained language model, removing the
need for labeled data and task-specific losses.",https://github.com/DevSinghSachan/art,104111
The Role of ImageNet Classes in Fréchet Inception Distance,0.964328,"Fr\'echet Inception Distance (FID) is the primary metric for ranking models
in data-driven generative modeling. While remarkably successful, the metric is
known to sometimes disagree with human judgement. We investigate a root cause
of these discrepancies, and visualize what FID ""looks at"" in generated images.
We show that the feature space that FID is (typically) computed in is so close
to the ImageNet classifications that aligning the histograms of Top-$N$
classifications between sets of generated and real images can reduce FID
substantially -- without actually improving the quality of results. Thus, we
conclude that FID is prone to intentional or accidental distortions. As a
practical example of an accidental distortion, we discuss a case where an
ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while
being worse in terms of human evaluation.",https://github.com/kynkaat/role-of-imagenet-classes-in-fid,44899
BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis,0.527198,"Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to align aspects and corresponding sentiments for
aspect-specific sentiment polarity inference. It is challenging because a
sentence may contain multiple aspects or complicated (e.g., conditional,
coordinating, or adversative) relations. Recently, exploiting dependency syntax
information with graph neural networks has been the most popular trend. Despite
its success, methods that heavily rely on the dependency tree pose challenges
in accurately modeling the alignment of the aspects and their words indicative
of sentiment, since the dependency tree may provide noisy signals of unrelated
associations (e.g., the ""conj"" relation between ""great"" and ""dreadful"" in
Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax
aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully
exploits the syntax information (e.g., phrase segmentation and hierarchical
structure) of the constituent tree of a sentence to model the sentiment-aware
context of every single aspect (called intra-context) and the sentiment
relations across aspects (called inter-context) for learning. Experiments on
four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the
state-of-the-art methods consistently.",https://github.com/yzhangcs/parser,147980
Hyperspherical Consistency Regularization,0.224419,"Recent advances in contrastive learning have enlightened diverse applications
across various semi-supervised fields. Jointly training supervised learning and
unsupervised learning with a shared feature encoder becomes a common scheme.
Though it benefits from taking advantage of both feature-dependent information
from self-supervised learning and label-dependent information from supervised
learning, this scheme remains suffering from bias of the classifier. In this
work, we systematically explore the relationship between self-supervised
learning and supervised learning, and study how self-supervised learning helps
robust data-efficient deep learning. We propose hyperspherical consistency
regularization (HCR), a simple yet effective plug-and-play method, to
regularize the classifier using feature-dependent information and thus avoid
bias from labels. Specifically, HCR first projects logits from the classifier
and feature projections from the projection head on the respective hypersphere,
then it enforces data points on hyperspheres to have similar structures by
minimizing binary cross entropy of pairwise distances' similarity metrics.
Extensive experiments on semi-supervised and weakly-supervised learning
demonstrate the effectiveness of our method, by showing superior performance
with HCR.",None,70015
Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance,0.0812181,"This paper presents a novel end-to-end method for the problem of
skeleton-based unsupervised human action recognition. We propose a new
architecture with a convolutional autoencoder that uses graph Laplacian
regularization to model the skeletal geometry across the temporal dynamics of
actions. Our approach is robust towards viewpoint variations by including a
self-supervised gradient reverse layer that ensures generalization across
camera views. The proposed method is validated on NTU-60 and NTU-120
large-scale datasets in which it outperforms all prior unsupervised
skeleton-based approaches on the cross-subject, cross-view, and cross-setup
protocols. Although unsupervised, our learnable representation allows our
method even to surpass a few supervised skeleton-based action recognition
methods. The code is available in:
www.github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian",www.github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian,5556
mSLAM: Massively multilingual joint pre-training for speech and text,0.955329,"We present mSLAM, a multilingual Speech and LAnguage Model that learns
cross-lingual cross-modal representations of speech and text by pre-training
jointly on large amounts of unlabeled speech and text in multiple languages.
mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on
character-level text, along with Connectionist Temporal Classification (CTC)
losses on paired speech and transcript data, to learn a single model capable of
learning from and representing both speech and text signals in a shared
representation space. We evaluate mSLAM on several downstream speech
understanding tasks and find that joint pre-training with text improves quality
on speech translation, speech intent classification and speech language-ID
while being competitive on multilingual ASR, when compared against speech-only
pre-training. Our speech translation model demonstrates zero-shot text
translation without seeing any text translation data, providing evidence for
cross-modal alignment of representations. mSLAM also benefits from multi-modal
fine-tuning, further improving the quality of speech translation by directly
leveraging text translation data during the fine-tuning process. Our empirical
analysis highlights several opportunities and challenges arising from
large-scale multimodal pre-training, suggesting directions for future research.",None,18269
Visual Programming: Compositional visual reasoning without training,0.999999,"We present VISPROG, a neuro-symbolic approach to solving complex and
compositional visual tasks given natural language instructions. VISPROG avoids
the need for any task-specific training. Instead, it uses the in-context
learning ability of large language models to generate python-like modular
programs, which are then executed to get both the solution and a comprehensive
and interpretable rationale. Each line of the generated program may invoke one
of several off-the-shelf computer vision models, image processing routines, or
python functions to produce intermediate outputs that may be consumed by
subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4
diverse tasks - compositional visual question answering, zero-shot reasoning on
image pairs, factual knowledge object tagging, and language-guided image
editing. We believe neuro-symbolic approaches like VISPROG are an exciting
avenue to easily and effectively expand the scope of AI systems to serve the
long tail of complex tasks that people may wish to perform.",None,10523
UC-OWOD: Unknown-Classified Open World Object Detection,0.601024,"Open World Object Detection (OWOD) is a challenging computer vision problem
that requires detecting unknown objects and gradually learning the identified
unknown classes. However, it cannot distinguish unknown instances as multiple
unknown classes. In this work, we propose a novel OWOD problem called
Unknown-Classified Open World Object Detection (UC-OWOD). UC-OWOD aims to
detect unknown instances and classify them into different unknown classes.
Besides, we formulate the problem and devise a two-stage object detector to
solve UC-OWOD. First, unknown label-aware proposal and unknown-discriminative
classification head are used to detect known and unknown objects. Then,
similarity-based unknown classification and unknown clustering refinement
modules are constructed to distinguish multiple unknown classes. Moreover, two
novel evaluation protocols are designed to evaluate unknown-class detection.
Abundant experiments and visualizations prove the effectiveness of the proposed
method. Code is available at https://github.com/JohnWuzh/UC-OWOD.",https://github.com/JohnWuzh/UC-OWOD,11164
Offline RL Policies Should be Trained to be Adaptive,0.395708,"Offline RL algorithms must account for the fact that the dataset they are
provided may leave many facets of the environment unknown. The most common way
to approach this challenge is to employ pessimistic or conservative methods,
which avoid behaviors that are too dissimilar from those in the training
dataset. However, relying exclusively on conservatism has drawbacks:
performance is sensitive to the exact degree of conservatism, and conservative
objectives can recover highly suboptimal policies. In this work, we propose
that offline RL methods should instead be adaptive in the presence of
uncertainty. We show that acting optimally in offline RL in a Bayesian sense
involves solving an implicit POMDP. As a result, optimal policies for offline
RL must be adaptive, depending not just on the current state but rather all the
transitions seen so far during evaluation.We present a model-free algorithm for
approximating this optimal adaptive policy, and demonstrate the efficacy of
learning such adaptive policies in offline RL benchmarks.",https://github.com/snu-mllab/EDAC.git,136864
Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,0.221344,"Human evaluation is the foundation upon which the evaluation of both
summarization systems and automatic metrics rests. However, existing human
evaluation studies for summarization either exhibit a low inter-annotator
agreement or have insufficient scale, and an in-depth analysis of human
evaluation is lacking. Therefore, we address the shortcomings of existing
summarization evaluation along the following axes: (1) We propose a modified
summarization salience protocol, Atomic Content Units (ACUs), which is based on
fine-grained semantic units and allows for a high inter-annotator agreement.
(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large
human evaluation dataset consisting of 22,000 summary-level annotations over 28
top-performing systems on three datasets. (3) We conduct a comparative study of
four human evaluation protocols, underscoring potential confounding factors in
evaluation setups. (4) We evaluate 50 automatic metrics and their variants
using the collected human annotations across evaluation protocols and
demonstrate how our benchmark leads to more statistically stable and
significant results. The metrics we benchmarked include recent methods based on
large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings
have important implications for evaluating LLMs, as we show that LLMs adjusted
by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,
which is affected by the annotators' prior, input-agnostic preferences, calling
for more robust, targeted evaluation methods.",https://github.com/Yale-LILY/ROSE,41181
TweetNLP: Cutting-Edge Natural Language Processing for Social Media,0.267422,"In this paper we present TweetNLP, an integrated platform for Natural
Language Processing (NLP) in social media. TweetNLP supports a diverse set of
NLP tasks, including generic focus areas such as sentiment analysis and named
entity recognition, as well as social media-specific tasks such as emoji
prediction and offensive language identification. Task-specific systems are
powered by reasonably-sized Transformer-based language models specialized on
social media text (in particular, Twitter) which can be run without the need
for dedicated hardware or cloud services. The main contributions of TweetNLP
are: (1) an integrated Python library for a modern toolkit supporting social
media analysis using our various task-specific models adapted to the social
domain; (2) an interactive online demo for codeless experimentation using our
models; and (3) a tutorial covering a wide variety of typical social media
applications.",https://github.com/cardiffnlp/tweetnlp,5590
Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition,0.180669,"Subject-invariant facial action unit (AU) recognition remains challenging for
the reason that the data distribution varies among subjects. In this paper, we
propose a causal inference framework for subject-invariant facial action unit
recognition. To illustrate the causal effect existing in AU recognition task,
we formulate the causalities among facial images, subjects, latent AU semantic
relations, and estimated AU occurrence probabilities via a structural causal
model. By constructing such a causal diagram, we clarify the causal effect
among variables and propose a plug-in causal intervention module, CIS, to
deconfound the confounder \emph{Subject} in the causal diagram. Extensive
experiments conducted on two commonly used AU benchmark datasets, BP4D and
DISFA, show the effectiveness of our CIS, and the model with CIS inserted,
CISNet, has achieved state-of-the-art performance.",None,250
Cross-modal Contrastive Learning for Speech Translation,0.669684,"How can we learn unified representations for spoken utterances and their
written text? Learning similar representations for semantically similar speech
and text is important for speech translation. To this end, we propose ConST, a
cross-modal contrastive learning method for end-to-end speech-to-text
translation. We evaluate ConST and a variety of previous baselines on a popular
benchmark MuST-C. Experiments show that the proposed ConST consistently
outperforms the previous methods on, and achieves an average BLEU of 29.4. The
analysis further verifies that ConST indeed closes the representation gap of
different modalities -- its learned representation improves the accuracy of
cross-modal speech-text retrieval from 4% to 88%. Code and models are available
at https://github.com/ReneeYe/ConST.",https://github.com/ReneeYe/ConST,3502
RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation,0.297934,"Category-level object pose estimation aims to predict the 6D pose as well as
the 3D metric size of arbitrary objects from a known set of categories. Recent
methods harness shape prior adaptation to map the observed point cloud into the
canonical space and apply Umeyama algorithm to recover the pose and size.
However, their shape prior integration strategy boosts pose estimation
indirectly, which leads to insufficient pose-sensitive feature extraction and
slow inference speed. To tackle this problem, in this paper, we propose a novel
geometry-guided Residual Object Bounding Box Projection network RBP-Pose that
jointly predicts object pose and residual vectors describing the displacements
from the shape-prior-indicated object surface projections on the bounding box
towards the real surface projections. Such definition of residual vectors is
inherently zero-mean and relatively small, and explicitly encapsulates spatial
cues of the 3D object for robust and accurate pose regression. We enforce
geometry-aware consistency terms to align the predicted pose and residual
vectors to further boost performance.",https://github.com/lolrudy/RBP_Pose,22417
MagicPony: Learning Articulated 3D Animals in the Wild,0.406881,"We consider the problem of predicting the 3D shape, articulation, viewpoint,
texture, and lighting of an articulated animal like a horse given a single test
image as input. We present a new method, dubbed MagicPony, that learns this
predictor purely from in-the-wild single-view images of the object category,
with minimal assumptions about the topology of deformation. At its core is an
implicit-explicit representation of articulated shape and appearance, combining
the strengths of neural fields and meshes. In order to help the model
understand an object's shape and pose, we distil the knowledge captured by an
off-the-shelf self-supervised vision transformer and fuse it into the 3D model.
To overcome local optima in viewpoint estimation, we further introduce a new
viewpoint sampling scheme that comes at no additional training cost. MagicPony
outperforms prior work on this challenging task and demonstrates excellent
generalisation in reconstructing art, despite the fact that it is only trained
on real images.",https://3dmagicpony.github.io/,96096
Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning,0.0929713,"Controlled automated story generation seeks to generate natural language
stories satisfying constraints from natural language critiques or preferences.
Existing methods to control for story preference utilize prompt engineering
which is labor intensive and often inconsistent. They may also use
logit-manipulation methods which require annotated datasets to exist for the
desired attributes. To address these issues, we first train a contrastive
bi-encoder model to align stories with corresponding human critiques, named
CARP, building a general purpose preference model. This is subsequently used as
a reward function to fine-tune a generative language model via reinforcement
learning. However, simply fine-tuning a generative language model with a
contrastive reward model does not always reliably result in a story generation
system capable of generating stories that meet user preferences. To increase
story generation robustness we further fine-tune the contrastive reward model
using a prompt-learning technique. A human participant study is then conducted
comparing generations from our full system, ablations, and two baselines. We
show that the full fine-tuning pipeline results in a story generator preferred
over a LLM 20x as large as well as logit-based methods. This motivates the use
of contrastive learning for general purpose human preference modeling.",https://github.com/lvwerra/trl/,19820
C-VTON: Context-Driven Image-Based Virtual Try-On Network,0.468031,"Image-based virtual try-on techniques have shown great promise for enhancing
the user-experience and improving customer satisfaction on fashion-oriented
e-commerce platforms. However, existing techniques are currently still limited
in the quality of the try-on results they are able to produce from input images
of diverse characteristics. In this work, we propose a Context-Driven Virtual
Try-On Network (C-VTON) that addresses these limitations and convincingly
transfers selected clothing items to the target subjects even under challenging
pose configurations and in the presence of self-occlusions. At the core of the
C-VTON pipeline are: (i) a geometric matching procedure that efficiently aligns
the target clothing with the pose of the person in the input images, and (ii) a
powerful image generator that utilizes various types of contextual information
when synthesizing the final try-on result. C-VTON is evaluated in rigorous
experiments on the VITON and MPV datasets and in comparison to state-of-the-art
techniques from the literature. Experimental results show that the proposed
approach is able to produce photo-realistic and visually convincing results and
significantly improves on the existing state-of-the-art.",https://github.com/benquick123/C-VTON,3927
Understanding Iterative Revision from Human-Written Text,0.267414,"Writing is, by nature, a strategic, adaptive, and more importantly, an
iterative process. A crucial part of writing is editing and revising the text.
Previous works on text revision have focused on defining edit intention
taxonomies within a single domain or developing computational models with a
single level of edit granularity, such as sentence-level edits, which differ
from human's revision cycles. This work describes IteraTeR: the first
large-scale, multi-domain, edit-intention annotated corpus of iteratively
revised text. In particular, IteraTeR is collected based on a new framework to
comprehensively model the iterative text revisions that generalize to various
domains of formal writing, edit intentions, revision depths, and granularities.
When we incorporate our annotated edit intentions, both generative and
edit-based text revision models significantly improve automatic evaluations.
Through our work, we better understand the text revision process, making vital
connections between edit intentions and writing quality, enabling the creation
of diverse corpora to support computational modeling of iterative text
revisions.",https://github.com/vipulraheja/IteraTeR,1339
Models and Datasets for Cross-Lingual Summarisation,0.388606,"We present a cross-lingual summarisation corpus with long documents in a
source language associated with multi-sentence summaries in a target language.
The corpus covers twelve language pairs and directions for four European
languages, namely Czech, English, French and German, and the methodology for
its creation can be applied to several other languages. We derive cross-lingual
document-summary instances from Wikipedia by combining lead paragraphs and
articles' bodies from language aligned Wikipedia titles. We analyse the
proposed cross-lingual summarisation task with automatic metrics and validate
it with a human study. To illustrate the utility of our dataset we report
experiments with multi-lingual pre-trained models in supervised, zero- and
few-shot, and out-of-domain scenarios.",https://github.com/lauhaide/clads,1727
From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer,0.671794,"Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC.",https://github.com/zjunlp/PromptKG/tree/main/research/GenKGC,8507
OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers,0.160982,"We present OSFormer, the first one-stage transformer framework for
camouflaged instance segmentation (CIS). OSFormer is based on two key designs.
First, we design a location-sensing transformer (LST) to obtain the location
label and instance-aware parameters by introducing the location-guided queries
and the blend-convolution feedforward network. Second, we develop a
coarse-to-fine fusion (CFF) to merge diverse context information from the LST
encoder and CNN backbone. Coupling these two components enables OSFormer to
efficiently blend local features and long-range context dependencies for
predicting camouflaged instances. Compared with two-stage frameworks, our
OSFormer reaches 41% AP and achieves good convergence efficiency without
requiring enormous training data, i.e., only 3,040 samples under 60 epochs.
Code link: https://github.com/PJLallen/OSFormer.",https://github.com/PJLallen/OSFormer,220689
TensorIR: An Abstraction for Automatic Tensorized Program Optimization,0.192128,"Deploying deep learning models on various devices has become an important
topic. The wave of hardware specialization brings a diverse set of acceleration
primitives for multi-dimensional tensor computations. These new acceleration
primitives, along with the emerging machine learning models, bring tremendous
engineering challenges. In this paper, we present TensorIR, a compiler
abstraction for optimizing programs with these tensor computation primitives.
TensorIR generalizes the loop nest representation used in existing machine
learning compilers to bring tensor computation as the first-class citizen.
Finally, we build an end-to-end framework on top of our abstraction to
automatically optimize deep learning models for given tensor computation
primitives. Experimental results show that TensorIR compilation automatically
uses the tensor computation primitives for given hardware backends and delivers
performance that is competitive to state-of-art hand-optimized systems across
platforms.",None,59013
Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality,0.974503,"We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground.",None,20526
VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension,0.183473,"One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on
Vietnamese Language and Speech Processing (VLSP 2021). This task attracted 77
participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the challenge, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% in F1-score and 67.43% in Exact Match on the
private test set. The Vietnamese MRC systems proposed by the top 3 teams use
XLM-RoBERTa, a powerful pre-trained language model based on the transformer
architecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further
explore the Vietnamese machine reading comprehension task and related tasks
such as question answering, question generation, and natural language
inference.",https://github.com/google-research/bert/blob/master/run_squad.py,2352
"A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets",0.542205,"In recent years, interest has arisen in using machine learning to improve the
efficiency of automatic medical consultation and enhance patient experience. In
this article, we propose two frameworks to support automatic medical
consultation, namely doctor-patient dialogue understanding and task-oriented
interaction. We create a new large medical dialogue dataset with multi-level
finegrained annotations and establish five independent tasks, including named
entity recognition, dialogue act classification, symptom label inference,
medical report generation and diagnosis-oriented dialogue policy. We report a
set of benchmark results for each task, which shows the usability of the
dataset and sets a baseline for future studies. Both code and data is available
from https://github.com/lemuria-wchen/imcs21.",https://github.com/lemuria-wchen/imcs21,21857
Few-shot Named Entity Recognition with Self-describing Networks,0.149295,"Few-shot NER needs to effectively capture information from limited instances
and transfer useful knowledge from external resources. In this paper, we
propose a self-describing mechanism for few-shot NER, which can effectively
leverage illustrative instances and precisely transfer knowledge from external
resources by describing both entity types and mentions using a universal
concept set. Specifically, we design Self-describing Networks (SDNet), a
Seq2Seq generation model which can universally describe mentions using
concepts, automatically map novel entity types to concepts, and adaptively
recognize entities on-demand. We pre-train SDNet with large-scale corpus, and
conduct experiments on 8 benchmarks from different domains. Experiments show
that SDNet achieves competitive performances on all benchmarks and achieves the
new state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and
robustness.",https://github.com/chen700564/sdnet,4816
Rethinking Graph Convolutional Networks in Knowledge Graph Completion,0.0802254,"Graph convolutional networks (GCNs) -- which are effective in modeling graph
structures -- have been increasingly popular in knowledge graph completion
(KGC). GCN-based KGC models first use GCNs to generate expressive entity
representations and then use knowledge graph embedding (KGE) models to capture
the interactions among entities and relations. However, many GCN-based KGC
models fail to outperform state-of-the-art KGE models though introducing
additional computational complexity. This phenomenon motivates us to explore
the real effect of GCNs in KGC. Therefore, in this paper, we build upon
representative GCN-based KGC models and introduce variants to find which factor
of GCNs is critical in KGC. Surprisingly, we observe from experiments that the
graph structure modeling in GCNs does not have a significant impact on the
performance of KGC models, which is in contrast to the common belief. Instead,
the transformations for entity representations are responsible for the
performance improvements. Based on the observation, we propose a simple yet
effective framework named LTE-KGE, which equips existing KGE models with
linearly transformed entity embeddings. Experiments demonstrate that LTE-KGE
models lead to similar performance improvements with GCN-based KGC methods,
while being more computationally efficient. These results suggest that existing
GCNs are unnecessary for KGC, and novel GCN-based KGC models should count on
more ablation studies to validate their effectiveness. The code of all the
experiments is available on GitHub at https://github.com/MIRALab-USTC/GCN4KGC.",https://github.com/MIRALab-USTC/GCN4KGC,2225
QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars,0.401363,"Real-time tracking of human body motion is crucial for interactive and
immersive experiences in AR/VR. However, very limited sensor data about the
body is available from standalone wearable devices such as HMDs (Head Mounted
Devices) or AR glasses. In this work, we present a reinforcement learning
framework that takes in sparse signals from an HMD and two controllers, and
simulates plausible and physically valid full body motions. Using high quality
full body motion as dense supervision during training, a simple policy network
can learn to output appropriate torques for the character to balance, walk, and
jog, while closely following the input signals. Our results demonstrate
surprisingly similar leg motions to ground truth without any observations of
the lower body, even when the input is only the 6D transformations of the HMD.
We also show that a single policy can be robust to diverse locomotion styles,
different body sizes, and novel environments.",None,912
CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,0.764101,"Constructing benchmarks that test the abilities of modern natural language
understanding models is difficult - pre-trained language models exploit
artifacts in benchmarks to achieve human parity, but still fail on adversarial
examples and make errors that demonstrate a lack of common sense. In this work,
we propose gamification as a framework for data construction. The goal of
players in the game is to compose questions that mislead a rival AI while using
specific phrases for extra points. The game environment leads to enhanced user
engagement and simultaneously gives the game designer control over the
collected data, allowing us to collect high-quality data at scale. Using our
method we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and
demonstrate its difficulty for models that are orders-of-magnitude larger than
the AI used in the game itself. Our best baseline, the T5-based Unicorn with
11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3
(52.9%) in a few-shot inference setup. Both score well below human performance
which is at 94.1%.",https://github.com/allenai/twentyquestions,46374
ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval,0.589126,"Neural retrievers based on pre-trained language models (PLMs), such as
dual-encoders, have achieved promising performance on the task of open-domain
question answering (QA). Their effectiveness can further reach new
state-of-the-arts by incorporating cross-architecture knowledge distillation.
However, most of the existing studies just directly apply conventional
distillation methods. They fail to consider the particular situation where the
teacher and student have different structures. In this paper, we propose a
novel distillation method that significantly advances cross-architecture
distillation for dual-encoders. Our method 1) introduces a self on-the-fly
distillation method that can effectively distill late interaction (i.e.,
ColBERT) to vanilla dual-encoder, and 2) incorporates a cascade distillation
process to further improve the performance with a cross-encoder teacher.
Extensive experiments are conducted to validate that our proposed solution
outperforms strong baselines and establish a new state-of-the-art on
open-domain QA benchmarks.",None,15024
Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,1.0,"Chain-of-thought prompting has demonstrated remarkable performance on various
natural language reasoning tasks. However, it tends to perform poorly on tasks
which requires solving problems harder than the exemplars shown in the prompts.
To overcome this challenge of easy-to-hard generalization, we propose a novel
prompting strategy, least-to-most prompting. The key idea in this strategy is
to break down a complex problem into a series of simpler subproblems and then
solve them in sequence. Solving each subproblem is facilitated by the answers
to previously solved subproblems. Our experimental results on tasks related to
symbolic manipulation, compositional generalization, and math reasoning reveal
that least-to-most prompting is capable of generalizing to more difficult
problems than those seen in the prompts. A notable finding is that when the
GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve
the compositional generalization benchmark SCAN in any split (including length
split) with an accuracy of at least 99% using just 14 exemplars, compared to
only 16% accuracy with chain-of-thought prompting. This is particularly
noteworthy because neural-symbolic models in the literature that specialize in
solving SCAN are trained on the entire training set containing over 15,000
examples. We have included prompts for all the tasks in the Appendix.",None,42280
Rethinking Performance Gains in Image Dehazing Networks,0.344541,"Image dehazing is an active topic in low-level vision, and many image
dehazing networks have been proposed with the rapid development of deep
learning. Although these networks' pipelines work fine, the key mechanism to
improving image dehazing performance remains unclear. For this reason, we do
not target to propose a dehazing network with fancy modules; rather, we make
minimal modifications to popular U-Net to obtain a compact dehazing network.
Specifically, we swap out the convolutional blocks in U-Net for residual blocks
with the gating mechanism, fuse the feature maps of main paths and skip
connections using the selective kernel, and call the resulting U-Net variant
gUNet. As a result, with a significantly reduced overhead, gUNet is superior to
state-of-the-art methods on multiple image dehazing datasets. Finally, we
verify these key designs to the performance gain of image dehazing networks
through extensive ablation studies.",https://github.com/IDKiro/gUNet,6873
Strategy Synthesis for Zero-Sum Neuro-Symbolic Concurrent Stochastic Games,0.424889,"Neuro-symbolic approaches to artificial intelligence, which combine neural
networks with classical symbolic techniques, are growing in prominence,
necessitating formal approaches to reason about their correctness. We propose a
novel modelling formalism called neuro-symbolic concurrent stochastic games
(NS-CSGs), which comprise two probabilistic finite-state agents interacting in
a shared continuous-state environment. Each agent observes the environment
using a neural perception mechanism, which converts inputs such as images into
symbolic percepts, and makes decisions symbolically. We focus on the class of
NS-CSGs with Borel state spaces and prove the existence and measurability of
the value function for zero-sum discounted cumulative rewards under
piecewise-constant restrictions on the components of this class of models. To
compute values and synthesise strategies, we present, for the first time,
practical value iteration (VI) and policy iteration (PI) algorithms to solve
this new subclass of continuous-state CSGs. These require a finite
decomposition of the environment induced by the neural perception mechanisms of
the agents and rely on finite abstract representations of value functions and
strategies closed under VI or PI. First, we introduce a Borel measurable
piecewise-constant (B-PWC) representation of value functions, extend minimax
backups to this representation and propose a value iteration algorithm called
B-PWC VI. Second, we introduce two novel representations for the value
functions and strategies, constant-piecewise-linear (CON-PWL) and
constant-piecewise-constant (CON-PWC) respectively, and propose
Minimax-action-free PI by extending a recent PI method based on alternating
player choices for finite state spaces to Borel state spaces, which does not
require normal-form games to be solved.",None,16419
Towards a Cleaner Document-Oriented Multilingual Crawled Corpus,0.69908,"The need for raw large raw corpora has dramatically increased in recent years
with the introduction of transfer learning and semi-supervised learning methods
to Natural Language Processing. And while there have been some recent attempts
to manually curate the amount of data necessary to train large language models,
the main way to obtain this data is still through automatic web crawling. In
this paper we take the existing multilingual web corpus OSCAR and its pipeline
Ungoliant that extracts and classifies data from Common Crawl at the line
level, and propose a set of improvements and automatic annotations in order to
produce a new document-oriented version of OSCAR that could prove more suitable
to pre-train large generative language models as well as hopefully other
applications in Natural Language Processing and Digital Humanities.",https://github.com/oscar-corpus/oscar-tools,9732
CM3: A Causal Masked Multimodal Model of the Internet,0.988314,"We introduce CM3, a family of causally masked generative models trained over
a large corpus of structured multi-modal documents that can contain both text
and image tokens. Our new causally masked approach generates tokens left to
right while also masking out a small number of long token spans that are
generated at the end of the string, instead of their original positions. The
casual masking object provides a type of hybrid of the more common causal and
masked language models, by enabling full generative modeling while also
providing bidirectional context when generating the masked spans. We train
causally masked language-image models on large-scale web and Wikipedia
articles, where each document contains all of the text, hypertext markup,
hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they
appear in the original HTML source (before masking). The resulting CM3 models
can generate rich structured, multi-modal outputs while conditioning on
arbitrary masked document contexts, and thereby implicitly learn a wide range
of text, image, and cross modal tasks. They can be prompted to recover, in a
zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM.
We set the new state-of-the-art in zero-shot summarization, entity linking, and
entity disambiguation while maintaining competitive performance in the
fine-tuning setting. We can generate images unconditionally, conditioned on
text (like DALL-E) and do captioning all in a zero-shot setting with a single
model.",https://github.com/facebookresearch/GENRE,104111
"GO-Surf: Neural Feature Grid Optimization for Fast, High-Fidelity RGB-D Surface Reconstruction",0.863321,"We present GO-Surf, a direct feature grid optimization method for accurate
and fast surface reconstruction from RGB-D sequences. We model the underlying
scene with a learned hierarchical feature voxel grid that encapsulates
multi-level geometric and appearance local information. Feature vectors are
directly optimized such that after being tri-linearly interpolated, decoded by
two shallow MLPs into signed distance and radiance values, and rendered via
surface volume rendering, the discrepancy between synthesized and observed
RGB/depth values is minimized. Our supervision signals -- RGB, depth and
approximate SDF -- can be obtained directly from input images without any need
for fusion or post-processing. We formulate a novel SDF gradient regularization
term that encourages surface smoothness and hole filling while maintaining high
frequency details. GO-Surf can optimize sequences of $1$-$2$K frames in
$15$-$45$ minutes, a speedup of $\times60$ over NeuralRGB-D, the most related
approach based on an MLP representation, while maintaining on par performance
on standard benchmarks. Project page: https://jingwenwang95.github.io/go_surf/",https://jingwenwang95.github.io/go_surf,6103
Third Time's the Charm? Image and Video Editing with StyleGAN3,0.99293,"StyleGAN is arguably one of the most intriguing and well-studied generative
models, demonstrating impressive performance in image generation, inversion,
and manipulation. In this work, we explore the recent StyleGAN3 architecture,
compare it to its predecessor, and investigate its unique advantages, as well
as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained
on unaligned data, one can still use aligned data for training, without
hindering the ability to generate unaligned imagery. Next, our analysis of the
disentanglement of the different latent spaces of StyleGAN3 indicates that the
commonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts,
underscoring the benefits of using the StyleSpace for fine-grained editing.
Considering image inversion, we observe that existing encoder-based techniques
struggle when trained on unaligned data. We therefore propose an encoding
scheme trained solely on aligned data, yet can still invert unaligned images.
Finally, we introduce a novel video inversion and editing workflow that
leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce
texture sticking and expand the field of view of the edited video.",https://yuval-alaluf.github.io/stylegan3-editing/,62088
Revisiting End-to-End Speech-to-Text Translation From Scratch,0.259694,"End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining
its encoder and/or decoder using source transcripts via speech recognition or
text translation tasks, without which translation performance drops
substantially. However, transcripts are not always available, and how
significant such pretraining is for E2E ST has rarely been studied in the
literature. In this paper, we revisit this question and explore the extent to
which the quality of E2E ST trained on speech-translation pairs alone can be
improved. We reexamine several techniques proven beneficial to ST previously,
and offer a set of best practices that biases a Transformer-based E2E ST system
toward training from scratch. Besides, we propose parameterized distance
penalty to facilitate the modeling of locality in the self-attention model for
speech. On four benchmarks covering 23 languages, our experiments show that,
without using any transcripts or pretraining, the proposed system reaches and
even outperforms previous studies adopting pretraining, although the gap
remains in (extremely) low-resource settings. Finally, we discuss neural
acoustic feature modeling, where a neural model is designed to extract acoustic
features from raw speech signals directly, with the goal to simplify inductive
biases and add freedom to the model in describing speech. For the first time,
we demonstrate its feasibility and show encouraging results on ST tasks.",None,21498
Anticipative Feature Fusion Transformer for Multi-Modal Action Anticipation,0.448647,"Although human action anticipation is a task which is inherently multi-modal,
state-of-the-art methods on well known action anticipation datasets leverage
this data by applying ensemble methods and averaging scores of unimodal
anticipation networks. In this work we introduce transformer based modality
fusion techniques, which unify multi-modal data at an early stage. Our
Anticipative Feature Fusion Transformer (AFFT) proves to be superior to popular
score fusion approaches and presents state-of-the-art results outperforming
previous methods on EpicKitchens-100 and EGTEA Gaze+. Our model is easily
extensible and allows for adding new modalities without architectural changes.
Consequently, we extracted audio features on EpicKitchens-100 which we add to
the set of commonly used features in the community.",https://github.com/zeyun-zhong/AFFT,21427
UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture,0.297793,"We present UnrealEgo, i.e., a new large-scale naturalistic dataset for
egocentric 3D human pose estimation. UnrealEgo is based on an advanced concept
of eyeglasses equipped with two fisheye cameras that can be used in
unconstrained environments. We design their virtual prototype and attach them
to 3D human models for stereo view capture. We next generate a large corpus of
human motions. As a consequence, UnrealEgo is the first dataset to provide
in-the-wild stereo images with the largest variety of motions among existing
egocentric datasets. Furthermore, we propose a new benchmark method with a
simple but effective idea of devising a 2D keypoint estimation module for
stereo inputs to improve 3D human pose estimation. The extensive experiments
show that our approach outperforms the previous state-of-the-art methods
qualitatively and quantitatively. UnrealEgo and our source codes are available
on our project web page.",None,42457
The Stack: 3 TB of permissively licensed source code,0.877409,"Large Language Models (LLMs) play an ever-increasing role in the field of
Artificial Intelligence (AI)--not only for natural language processing but also
for code understanding and generation. To stimulate open and responsible
research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting
of permissively licensed source code in 30 programming languages. We describe
how we collect the full dataset, construct a permissively licensed subset,
present a data governance plan, discuss limitations, and show promising results
on text2code benchmarks by training 350M-parameter decoders on different Python
subsets. We find that (1) near-deduplicating the data significantly boosts
performance across all experiments, and (2) it is possible to match previously
reported HumanEval and MBPP performance using only permissively licensed data.
We make the dataset available at https://hf.co/BigCode, provide a tool called
""Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers
to search The Stack for copies of their code, and provide a process for code to
be removed from the dataset by following the instructions at
https://www.bigcode-project.org/docs/about/the-stack/.",None,84607
Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations,0.5027,"We present Neural Feature Fusion Fields (N3F), a method that improves dense
2D image feature extractors when the latter are applied to the analysis of
multiple images reconstructible as a 3D scene. Given an image feature
extractor, for example pre-trained using self-supervision, N3F uses it as a
teacher to learn a student network defined in 3D space. The 3D student network
is similar to a neural radiance field that distills said features and can be
trained with the usual differentiable rendering machinery. As a consequence,
N3F is readily applicable to most neural rendering formulations, including
vanilla NeRF and its extensions to complex dynamic scenes. We show that our
method not only enables semantic understanding in the context of scene-specific
neural fields without the use of manual labels, but also consistently improves
over the self-supervised 2D baselines. This is demonstrated by considering
various tasks, such as 2D object retrieval, 3D segmentation, and scene editing,
in diverse sequences, including long egocentric videos in the EPIC-KITCHENS
benchmark.",None,96096
Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs,0.132687,"Most existing deep neural networks (DNNs) are easily disturbed by slight
noise. However, there are few researches on physical attacks by deploying
lighting equipment. The light-based physical attacks has excellent covertness,
which brings great security risks to many vision-based applications (such as
self-driving). Therefore, we propose a light-based physical attack, called
adversarial laser spot (AdvLS), which optimizes the physical parameters of
laser spots through genetic algorithm to perform physical attacks. It realizes
robust and covert physical attack by using low-cost laser equipment. As far as
we know, AdvLS is the first light-based physical attack that perform physical
attacks in the daytime. A large number of experiments in the digital and
physical environments show that AdvLS has excellent robustness and covertness.
In addition, through in-depth analysis of the experimental data, we find that
the adversarial perturbations generated by AdvLS have superior adversarial
attack migration. The experimental results show that AdvLS impose serious
interference to advanced DNNs, we call for the attention of the proposed AdvLS.
The code of AdvLS is available at: https://github.com/ChengYinHu/AdvLS",https://github.com/ChengYinHu/AdvLS,9817
Multilingual Machine Translation with Hyper-Adapters,0.0549896,"Multilingual machine translation suffers from negative interference across
languages. A common solution is to relax parameter sharing with
language-specific modules like adapters. However, adapters of related languages
are unable to transfer information, and their total number of parameters
becomes prohibitively expensive as the number of languages grows. In this work,
we overcome these drawbacks using hyper-adapters -- hyper-networks that
generate adapters from language and layer embeddings. While past work had poor
results when scaling hyper-networks, we propose a rescaling fix that
significantly improves convergence and enables training larger hyper-networks.
We find that hyper-adapters are more parameter efficient than regular adapters,
reaching the same performance with up to 12 times less parameters. When using
the same number of parameters and FLOPS, our approach consistently outperforms
regular adapters. Also, hyper-adapters converge faster than alternative
approaches and scale better than regular dense networks. Our analysis shows
that hyper-adapters learn to encode language relatedness, enabling positive
transfer across languages.",https://github.com/cbaziotis/fairseq,9766
"""I think this is the most disruptive technology"": Exploring Sentiments of ChatGPT Early Adopters using Twitter Data",0.968952,"Large language models have recently attracted significant attention due to
their impressive performance on a variety of tasks. ChatGPT developed by OpenAI
is one such implementation of a large, pre-trained language model that has
gained immense popularity among early adopters, where certain users go to the
extent of characterizing it as a disruptive technology in many domains.
Understanding such early adopters' sentiments is important because it can
provide insights into the potential success or failure of the technology, as
well as its strengths and weaknesses. In this paper, we conduct a mixed-method
study using 10,732 tweets from early ChatGPT users. We first use topic
modelling to identify the main topics and then perform an in-depth qualitative
sentiment analysis of each topic. Our results show that the majority of the
early adopters have expressed overwhelmingly positive sentiments related to
topics such as Disruptions to software development, Entertainment and
exercising creativity. Only a limited percentage of users expressed concerns
about issues such as the potential for misuse of ChatGPT, especially regarding
topics such as Impact on educational aspects. We discuss these findings by
providing specific examples for each topic and then detail implications related
to addressing these concerns for both researchers and users.",None,440
MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors,0.296033,"In this paper, we propose MOTRv2, a simple yet effective pipeline to
bootstrap end-to-end multi-object tracking with a pretrained object detector.
Existing end-to-end methods, MOTR and TrackFormer are inferior to their
tracking-by-detection counterparts mainly due to their poor detection
performance. We aim to improve MOTR by elegantly incorporating an extra object
detector. We first adopt the anchor formulation of queries and then use an
extra object detector to generate proposals as anchors, providing detection
prior to MOTR. The simple modification greatly eases the conflict between joint
learning detection and association tasks in MOTR. MOTRv2 keeps the query
propogation feature and scales well on large-scale benchmarks. MOTRv2 ranks the
1st place (73.4% HOTA on DanceTrack) in the 1st Multiple People Tracking in
Group Dance Challenge. Moreover, MOTRv2 reaches state-of-the-art performance on
the BDD100K dataset. We hope this simple and effective pipeline can provide
some new insights to the end-to-end MOT community. Code is available at
\url{https://github.com/megvii-research/MOTRv2}.",https://github.com/megvii-research/MOTRv2,2415
Rationale-Augmented Ensembles in Language Models,0.868289,"Recent research has shown that rationales, or step-by-step chains of thought,
can be used to improve performance in multi-step reasoning tasks. We reconsider
rationale-augmented prompting for few-shot in-context learning, where (input ->
output) prompts are expanded to (input, rationale -> output) prompts. For
rationale-augmented prompting we demonstrate how existing approaches, which
rely on manual prompt engineering, are subject to sub-optimal rationales that
may harm performance. To mitigate this brittleness, we propose a unified
framework of rationale-augmented ensembles, where we identify rationale
sampling in the output space as the key component to robustly improve
performance. This framework is general and can easily be extended to common
natural language processing tasks, even those that do not traditionally
leverage intermediate steps, such as question answering, word sense
disambiguation, and sentiment analysis. We demonstrate that rationale-augmented
ensembles achieve more accurate and interpretable results than existing
prompting approaches--including standard prompting without rationales and
rationale-based chain-of-thought prompting--while simultaneously improving
interpretability of model predictions through the associated rationales.",None,42280
Neural Cloth Simulation,0.190283,"We present a general framework for the garment animation problem through
unsupervised deep learning inspired in physically based simulation. Existing
trends in the literature already explore this possibility. Nonetheless, these
approaches do not handle cloth dynamics. Here, we propose the first methodology
able to learn realistic cloth dynamics unsupervisedly, and henceforth, a
general formulation for neural cloth simulation. The key to achieve this is to
adapt an existing optimization scheme for motion from simulation based
methodologies to deep learning. Then, analyzing the nature of the problem, we
devise an architecture able to automatically disentangle static and dynamic
cloth subspaces by design. We will show how this improves model performance.
Additionally, this opens the possibility of a novel motion augmentation
technique that greatly improves generalization. Finally, we show it also allows
to control the level of motion in the predictions. This is a useful, never seen
before, tool for artists. We provide of detailed analysis of the problem to
establish the bases of neural cloth simulation and guide future research into
the specifics of this domain.",None,16698
Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks,0.271236,"We introduce camouflaged data poisoning attacks, a new attack vector that
arises in the context of machine unlearning and other settings when model
retraining may be induced. An adversary first adds a few carefully crafted
points to the training dataset such that the impact on the model's predictions
is minimal. The adversary subsequently triggers a request to remove a subset of
the introduced points at which point the attack is unleashed and the model's
predictions are negatively affected. In particular, we consider clean-label
targeted attacks (in which the goal is to cause the model to misclassify a
specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof.
This attack is realized by constructing camouflage datapoints that mask the
effect of a poisoned dataset.",https://github.com/Jimmy-di/camouﬂage-poisoning,4022
A case for using rotation invariant features in state of the art feature matchers,0.0886065,"The aim of this paper is to demonstrate that a state of the art feature
matcher (LoFTR) can be made more robust to rotations by simply replacing the
backbone CNN with a steerable CNN which is equivariant to translations and
image rotations. It is experimentally shown that this boost is obtained without
reducing performance on ordinary illumination and viewpoint matching sequences.",https://github.com/georg-/se2-loftr,8663
Demystifying Prompts in Language Models via Perplexity Estimation,0.978255,"Language models can be prompted to perform a wide variety of zero- and
few-shot learning problems. However, performance varies significantly with the
choice of prompt, and we do not yet understand why this happens or how to pick
the best prompts. In this work, we analyze the factors that contribute to this
variance and establish a new empirical hypothesis: the performance of a prompt
is coupled with the extent to which the model is familiar with the language it
contains. Over a wide range of tasks, we show that the lower the perplexity of
the prompt is, the better the prompt is able to perform the task. As a result,
we devise a method for creating prompts: (1) automatically extend a small seed
set of manually written prompts by paraphrasing using GPT3 and backtranslation
and (2) choose the lowest perplexity prompts to get significant gains in
performance.",https://github.com/bigscience-workshop/promptsource,104111
CSL: A Large-scale Chinese Scientific Literature Dataset,0.851738,"Scientific literature serves as a high-quality corpus, supporting a lot of
Natural Language Processing (NLP) research. However, existing datasets are
centered around the English language, which restricts the development of
Chinese scientific NLP. In this work, we present CSL, a large-scale Chinese
Scientific Literature dataset, which contains the titles, abstracts, keywords
and academic fields of 396k papers. To our knowledge, CSL is the first
scientific document dataset in Chinese. The CSL can serve as a Chinese corpus.
Also, this semi-structured data is a natural annotation that can constitute
many supervised NLP tasks. Based on CSL, we present a benchmark to evaluate the
performance of models across scientific domain tasks, i.e., summarization,
keyword generation and text classification. We analyze the behavior of existing
text-to-text models on the evaluation tasks and reveal the challenges for
Chinese scientific NLP tasks, which provides a valuable reference for future
research. Data and code are available at https://github.com/ydli-ai/CSL",https://github.com/ydli-ai/CSL,57782
Ditto: Building Digital Twins of Articulated Objects from Interaction,0.431631,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto",https://ut-austin-rpl.github.io/Ditto/,18716
Selective Annotation Makes Language Models Better Few-Shot Learners,0.20881,"Many recent approaches to natural language tasks are built on the remarkable
abilities of large language models. Large language models can perform
in-context learning, where they learn a new task from a few task
demonstrations, without any parameter updates. This work examines the
implications of in-context learning for the creation of datasets for new
natural language tasks. Departing from recent in-context learning methods, we
formulate an annotation-efficient, two-step framework: selective annotation
that chooses a pool of examples to annotate from unlabeled data in advance,
followed by prompt retrieval that retrieves task examples from the annotated
pool at test time. Based on this framework, we propose an unsupervised,
graph-based selective annotation method, voke-k, to select diverse,
representative examples to annotate. Extensive experiments on 10 datasets
(covering classification, commonsense reasoning, dialogue, and text/code
generation) demonstrate that our selective annotation method improves the task
performance by a large margin. On average, vote-k achieves a 12.9%/11.4%
relative gain under an annotation budget of 18/100, as compared to randomly
selecting examples to annotate. Compared to state-of-the-art supervised
finetuning approaches, it yields similar performance with 10-100x less
annotation cost across 10 tasks. We further analyze the effectiveness of our
framework in various scenarios: language models with varying sizes, alternative
selective annotation methods, and cases where there is a test data domain
shift. We hope that our studies will serve as a basis for data annotations as
large language models are increasingly applied to new tasks. Our code is
available at https://github.com/HKUNLP/icl-selective-annotation.",None,104111
Inference and dynamic decision-making for deteriorating systems with probabilistic dependencies through Bayesian networks and deep reinforcement learning,0.0351326,"In the context of modern environmental and societal concerns, there is an
increasing demand for methods able to identify management strategies for civil
engineering systems, minimizing structural failure risks while optimally
planning inspection and maintenance (I&M) processes. Most available methods
simplify the I&M decision problem to the component level due to the
computational complexity associated with global optimization methodologies
under joint system-level state descriptions. In this paper, we propose an
efficient algorithmic framework for inference and decision-making under
uncertainty for engineering systems exposed to deteriorating environments,
providing optimal management strategies directly at the system level. In our
approach, the decision problem is formulated as a factored partially observable
Markov decision process, whose dynamics are encoded in Bayesian network
conditional structures. The methodology can handle environments under equal or
general, unequal deterioration correlations among components, through Gaussian
hierarchical structures and dynamic Bayesian networks. In terms of policy
optimization, we adopt a deep decentralized multi-agent actor-critic (DDMAC)
reinforcement learning approach, in which the policies are approximated by
actor neural networks guided by a critic network. By including deterioration
dependence in the simulated environment, and by formulating the cost model at
the system level, DDMAC policies intrinsically consider the underlying
system-effects. This is demonstrated through numerical experiments conducted
for both a 9-out-of-10 system and a steel frame under fatigue deterioration.
Results demonstrate that DDMAC policies offer substantial benefits when
compared to state-of-the-art heuristic approaches. The inherent consideration
of system-effects by DDMAC strategies is also interpreted based on the learned
policies.",None,622
Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation,0.149114,"Continual relation extraction (CRE) aims to continually learn new relations
from a class-incremental data stream. CRE model usually suffers from
catastrophic forgetting problem, i.e., the performance of old relations
seriously degrades when the model learns new relations. Most previous work
attributes catastrophic forgetting to the corruption of the learned
representations as new relations come, with an implicit assumption that the CRE
models have adequately learned the old relations. In this paper, through
empirical studies we argue that this assumption may not hold, and an important
reason for catastrophic forgetting is that the learned representations do not
have good robustness against the appearance of analogous relations in the
subsequent learning process. To address this issue, we encourage the model to
learn more precise and robust representations through a simple yet effective
adversarial class augmentation mechanism (ACA), which is easy to implement and
model-agnostic. Experimental results show that ACA can consistently improve the
performance of state-of-the-art CRE models on two popular benchmarks.",https://github.com/Wangpeiyi9979/ACA,10429
DiGamma: Domain-aware Genetic Algorithm for HW-Mapping Co-optimization for DNN Accelerators,0.73134,"The design of DNN accelerators includes two key parts: HW resource
configuration and mapping strategy. Intensive research has been conducted to
optimize each of them independently. Unfortunately, optimizing for both
together is extremely challenging due to the extremely large cross-coupled
search space. To address this, in this paper, we propose a HW-Mapping
co-optimization framework, an efficient encoding of the immense design space
constructed by HW and Mapping, and a domain-aware genetic algorithm, named
DiGamma, with specialized operators for improving search efficiency. We
evaluate DiGamma with seven popular DNNs models with different properties. Our
evaluations show DiGamma can achieve (geomean) 3.0x and 10.0x speedup,
comparing to the best-performing baseline optimization algorithms, in edge and
cloud settings.",https://github.com/maestro-project/digamma,16433
Real-time Online Multi-Object Tracking in Compressed Domain,0.271508,"Recent online Multi-Object Tracking (MOT) methods have achieved desirable
tracking performance. However, the tracking speed of most existing methods is
rather slow. Inspired from the fact that the adjacent frames are highly
relevant and redundant, we divide the frames into key and non-key frames
respectively and track objects in the compressed domain. For the key frames,
the RGB images are restored for detection and data association. To make data
association more reliable, an appearance Convolutional Neural Network (CNN)
which can be jointly trained with the detector is proposed. For the non-key
frames, the objects are directly propagated by a tracking CNN based on the
motion information provided in the compressed domain. Compared with the
state-of-the-art online MOT methods,our tracker is about 6x faster while
maintaining a comparable tracking performance.",None,59177
"M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval",0.0117991,"This work investigates the use of large-scale, English-only pre-trained
models (CLIP and HuBERT) for multilingual image-speech retrieval. For
non-English image-speech retrieval, we outperform the current state-of-the-art
performance by a wide margin both when training separate models for each
language, and with a single model which processes speech in all three
languages. We identify key differences in model behavior and performance
between English and non-English settings, attributable to the English-only
pre-training of CLIP and HuBERT, and investigate how fine-tuning the
pre-trained models impacts these differences. Finally, we show that our models
can be used for mono- and cross-lingual speech-text retrieval and cross-lingual
speech-speech retrieval, despite never having seen any parallel speech-text or
speech-speech data during training.",None,9523
"Intelligent Computing: The Latest Advances, Challenges and Future",0.143315,"Computing is a critical driving force in the development of human
civilization. In recent years, we have witnessed the emergence of intelligent
computing, a new computing paradigm that is reshaping traditional computing and
promoting digital revolution in the era of big data, artificial intelligence
and internet-of-things with new computing theories, architectures, methods,
systems, and applications. Intelligent computing has greatly broadened the
scope of computing, extending it from traditional computing on data to
increasingly diverse computing paradigms such as perceptual intelligence,
cognitive intelligence, autonomous intelligence, and human-computer fusion
intelligence. Intelligence and computing have undergone paths of different
evolution and development for a long time but have become increasingly
intertwined in recent years: intelligent computing is not only
intelligence-oriented but also intelligence-driven. Such cross-fertilization
has prompted the emergence and rapid advancement of intelligent computing.
Intelligent computing is still in its infancy and an abundance of innovations
in the theories, systems, and applications of intelligent computing are
expected to occur soon. We present the first comprehensive survey of literature
on intelligent computing, covering its theory fundamentals, the technological
fusion of intelligence and computing, important applications, challenges, and
future perspectives. We believe that this survey is highly timely and will
provide a comprehensive reference and cast valuable insights into intelligent
computing for academic and industrial researchers and practitioners.",None,50644
Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation,0.366949,"Document-level Relation Extraction (DocRE) is a more challenging task
compared to its sentence-level counterpart. It aims to extract relations from
multiple sentences at once. In this paper, we propose a semi-supervised
framework for DocRE with three novel components. Firstly, we use an axial
attention module for learning the interdependency among entity-pairs, which
improves the performance on two-hop relations. Secondly, we propose an adaptive
focal loss to tackle the class imbalance problem of DocRE. Lastly, we use
knowledge distillation to overcome the differences between human annotated data
and distantly supervised data. We conducted experiments on two DocRE datasets.
Our model consistently outperforms strong baselines and its performance exceeds
the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.
Our code and data will be released at https://github.com/tonytan48/KD-DocRE.",https://github.com/tonytan48/KD-DocRE,18583
Spatial Transformer Network on Skeleton-based Gait Recognition,0.198598,"Skeleton-based gait recognition models usually suffer from the robustness
problem, as the Rank-1 accuracy varies from 90\% in normal walking cases to
70\% in walking with coats cases. In this work, we propose a state-of-the-art
robust skeleton-based gait recognition model called Gait-TR, which is based on
the combination of spatial transformer frameworks and temporal convolutional
networks. Gait-TR achieves substantial improvements over other skeleton-based
gait models with higher accuracy and better robustness on the well-known gait
dataset CASIA-B. Particularly in walking with coats cases, Gait-TR get a 90\%
Rank-1 gait recognition accuracy rate, which is higher than the best result of
silhouette-based models, which usually have higher accuracy than the
silhouette-based gait recognition models. Moreover, our experiment on CASIA-B
shows that the spatial transformer can extract gait features from the human
skeleton better than the widely used graph convolutional network.",None,17341
Table-based Fact Verification with Self-adaptive Mixture of Experts,0.103312,"The table-based fact verification task has recently gained widespread
attention and yet remains to be a very challenging problem. It inherently
requires informative reasoning over natural language together with different
numerical and logical reasoning on tables (e.g., count, superlative,
comparative). Considering that, we exploit mixture-of-experts and present in
this paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE).
Specifically, we have developed a mixture-of-experts neural network to
recognize and execute different types of reasoning -- the network is composed
of multiple experts, each handling a specific part of the semantics for
reasoning, whereas a management module is applied to decide the contribution of
each expert network to the verification result. A self-adaptive method is
developed to teach the management module combining results of different experts
more efficiently without external knowledge. The experimental results
illustrate that our framework achieves 85.1% accuracy on the benchmark dataset
TabFact, comparable with the previous state-of-the-art models. We hope our
framework can serve as a new baseline for table-based verification. Our code is
available at https://github.com/THUMLP/SaMoE.",https://github.com/THUMLP/SaMoE,2921
DraftRec: Personalized Draft Recommendation for Winning in Multi-Player Online Battle Arena Games,0.214734,"This paper presents a personalized character recommendation system for
Multiplayer Online Battle Arena (MOBA) games which are considered as one of the
most popular online video game genres around the world. When playing MOBA
games, players go through a draft stage, where they alternately select a
virtual character to play. When drafting, players select characters by not only
considering their character preferences, but also the synergy and competence of
their team's character combination. However, the complexity of drafting induces
difficulties for beginners to choose the appropriate characters based on the
characters of their team while considering their own champion preferences. To
alleviate this problem, we propose DraftRec, a novel hierarchical model which
recommends characters by considering each player's champion preferences and the
interaction between the players. DraftRec consists of two networks: the player
network and the match network. The player network captures the individual
player's champion preference, and the match network integrates the complex
relationship between the players and their respective champions. We train and
evaluate our model from a manually collected 280,000 matches of League of
Legends and a publicly available 50,000 matches of Dota2. Empirically, our
method achieved state-of-the-art performance in character recommendation and
match outcome prediction task. Furthermore, a comprehensive user survey
confirms that DraftRec provides convincing and satisfying recommendations. Our
code and dataset are available at https://github.com/dojeon-ai/DraftRec.",https://github.com/dojeon-ai/DraftRec,12146
CORE: Simple and Effective Session-based Recommendation within Consistent Representation Space,0.786109,"Session-based Recommendation (SBR) refers to the task of predicting the next
item based on short-term user behaviors within an anonymous session. However,
session embedding learned by a non-linear encoder is usually not in the same
representation space as item embeddings, resulting in the inconsistent
prediction issue while recommending items. To address this issue, we propose a
simple and effective framework named CORE, which can unify the representation
space for both the encoding and decoding processes. Firstly, we design a
representation-consistent encoder that takes the linear combination of input
item embeddings as session embedding, guaranteeing that sessions and items are
in the same representation space. Besides, we propose a robust distance
measuring method to prevent overfitting of embeddings in the consistent
representation space. Extensive experiments conducted on five public real-world
datasets demonstrate the effectiveness and efficiency of the proposed method.
The code is available at: https://github.com/RUCAIBox/CORE.",https://github.com/RUCAIBox/CORE,19116
Offline RL for Natural Language Generation with Implicit Language Q Learning,0.586294,"Large language models distill broad knowledge from text corpora. However,
they can be inconsistent when it comes to completing user specified tasks. This
issue can be addressed by finetuning such models via supervised learning on
curated datasets, or via reinforcement learning. In this work, we propose a
novel offline RL method, implicit language Q-learning (ILQL), designed for use
on language models, that combines both the flexible utility maximization
framework of RL algorithms with the ability of supervised learning to leverage
previously collected data, as well as its simplicity and stability. Our method
employs a combination of value conservatism alongside an implicit dataset
support constraint in learning value functions, which are then used to guide
language model generations towards maximizing user-specified utility functions.
In addition to empirically validating ILQL, we present a detailed empirical
analysis of situations where offline RL can be useful in natural language
generation settings, demonstrating how it can be a more effective utility
optimizer than prior approaches for end-to-end dialogue, and how it can
effectively optimize high variance reward functions based on subjective
judgement, such as whether to label a comment as toxic or not.",None,136864
Faithful Reasoning Using Large Language Models,0.86867,"Although contemporary large language models (LMs) demonstrate impressive
question-answering capabilities, their answers are typically the product of a
single call to the model. This entails an unwelcome degree of opacity and
compromises performance, especially on problems that are inherently multi-step.
To address these limitations, we show how LMs can be made to perform faithful
multi-step reasoning via a process whose causal structure mirrors the
underlying logical structure of the problem. Our approach works by chaining
together reasoning steps, where each step results from calls to two fine-tuned
LMs, one for selection and one for inference, to produce a valid reasoning
trace. Our method carries out a beam search through the space of reasoning
traces to improve reasoning quality. We demonstrate the effectiveness of our
model on multi-step logical deduction and scientific question-answering,
showing that it outperforms baselines on final answer accuracy, and generates
humanly interpretable reasoning traces whose validity can be checked by the
user.",None,13655
Semiconductor Defect Detection by Hybrid Classical-Quantum Deep Learning,0.532493,"With the rapid development of artificial intelligence and autonomous driving
technology, the demand for semiconductors is projected to rise substantially.
However, the massive expansion of semiconductor manufacturing and the
development of new technology will bring many defect wafers. If these defect
wafers have not been correctly inspected, the ineffective semiconductor
processing on these defect wafers will cause additional impact to our
environment, such as excessive carbon dioxide emission and energy consumption.
In this paper, we utilize the information processing advantages of quantum
computing to promote the defect learning defect review (DLDR). We propose a
classical-quantum hybrid algorithm for deep learning on near-term quantum
processors. By tuning parameters implemented on it, quantum circuit driven by
our framework learns a given DLDR task, include of wafer defect map
classification, defect pattern classification, and hotspot detection. In
addition, we explore parametrized quantum circuits with different
expressibility and entangling capacities. These results can be used to build a
future roadmap to develop circuit-based quantum deep learning for semiconductor
defect detection.",None,10320
ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework,0.511731,"In this paper, a computation efficient regression framework is presented for
estimating the 6D pose of rigid objects from a single RGB-D image, which is
applicable to handling symmetric objects. This framework is designed in a
simple architecture that efficiently extracts point-wise features from RGB-D
data using a fully convolutional network, called XYZNet, and directly regresses
the 6D pose without any post refinement. In the case of symmetric object, one
object has multiple ground-truth poses, and this one-to-many relationship may
lead to estimation ambiguity. In order to solve this ambiguity problem, we
design a symmetry-invariant pose distance metric, called average (maximum)
grouped primitives distance or A(M)GPD. The proposed A(M)GPD loss can make the
regression network converge to the correct state, i.e., all minima in the
A(M)GPD loss surface are mapped to the correct poses. Extensive experiments on
YCB-Video and T-LESS datasets demonstrate the proposed framework's
substantially superior performance in top accuracy and low computational cost.",https://github.com/GANWANSHUI/ES6D.git,12636
SeqOT: A Spatial-Temporal Transformer Network for Place Recognition Using Sequential LiDAR Data,0.552902,"Place recognition is an important component for autonomous vehicles to
achieve loop closing or global localization. In this paper, we tackle the
problem of place recognition based on sequential 3D LiDAR scans obtained by an
onboard LiDAR sensor. We propose a transformer-based network named SeqOT to
exploit the temporal and spatial information provided by sequential range
images generated from the LiDAR data. It uses multi-scale transformers to
generate a global descriptor for each sequence of LiDAR range images in an
end-to-end fashion. During online operation, our SeqOT finds similar places by
matching such descriptors between the current query sequence and those stored
in the map. We evaluate our approach on four datasets collected with different
types of LiDAR sensors in different environments. The experimental results show
that our method outperforms the state-of-the-art LiDAR-based place recognition
methods and generalizes well across different environments. Furthermore, our
method operates online faster than the frame rate of the sensor. The
implementation of our method is released as open source at:
https://github.com/BIT-MJY/SeqOT.",https://github.com/BIT-MJY/SeqOT,108
M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction,0.774122,"Predicting future motions of road participants is an important task for
driving autonomously in urban scenes. Existing models excel at predicting
marginal trajectories for single agents, yet it remains an open question to
jointly predict scene compliant trajectories over multiple agents. The
challenge is due to exponentially increasing prediction space as a function of
the number of agents. In this work, we exploit the underlying relations between
interacting agents and decouple the joint prediction problem into marginal
prediction problems. Our proposed approach M2I first classifies interacting
agents as pairs of influencers and reactors, and then leverages a marginal
prediction model and a conditional prediction model to predict trajectories for
the influencers and reactors, respectively. The predictions from interacting
agents are combined and selected according to their joint likelihoods.
Experiments show that our simple but effective approach achieves
state-of-the-art performance on the Waymo Open Motion Dataset interactive
prediction benchmark.",None,18787
TAFNet: A Three-Stream Adaptive Fusion Network for RGB-T Crowd Counting,0.843613,"In this paper, we propose a three-stream adaptive fusion network named
TAFNet, which uses paired RGB and thermal images for crowd counting.
Specifically, TAFNet is divided into one main stream and two auxiliary streams.
We combine a pair of RGB and thermal images to constitute the input of main
stream. Two auxiliary streams respectively exploit RGB image and thermal image
to extract modality-specific features. Besides, we propose an Information
Improvement Module (IIM) to fuse the modality-specific features into the main
stream adaptively. Experiment results on RGBT-CC dataset show that our method
achieves more than 20% improvement on mean average error and root mean squared
error compared with state-of-the-art method. The source code will be publicly
available at https://github.com/TANGHAIHAN/TAFNet.",https://github.com/TANGHAIHAN/TAFNet,9219
Anomaly Detection via Reverse Distillation from One-Class Embedding,0.559367,"Knowledge distillation (KD) achieves promising results on the challenging
problem of unsupervised anomaly detection (AD).The representation discrepancy
of anomalies in the teacher-student (T-S) model provides essential evidence for
AD. However, using similar or identical architectures to build the teacher and
student models in previous studies hinders the diversity of anomalous
representations. To tackle this problem, we propose a novel T-S model
consisting of a teacher encoder and a student decoder and introduce a simple
yet effective ""reverse distillation"" paradigm accordingly. Instead of receiving
raw images directly, the student network takes teacher model's one-class
embedding as input and targets to restore the teacher's multiscale
representations. Inherently, knowledge distillation in this study starts from
abstract, high-level presentations to low-level features. In addition, we
introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S
model. The obtained compact embedding effectively preserves essential
information on normal patterns, but abandons anomaly perturbations. Extensive
experimentation on AD and one-class novelty detection benchmarks shows that our
method surpasses SOTA performance, demonstrating our proposed approach's
effectiveness and generalizability.",None,431
Deep Transformer Q-Networks for Partially Observable Reinforcement Learning,0.0,"Real-world reinforcement learning tasks often involve some form of partial
observability where the observations only give a partial or noisy view of the
true state of the world. Such tasks typically require some form of memory,
where the agent has access to multiple past observations, in order to perform
well. One popular way to incorporate memory is by using a recurrent neural
network to access the agent's history. However, recurrent neural networks in
reinforcement learning are often fragile and difficult to train, susceptible to
catastrophic forgetting and sometimes fail completely as a result. In this
work, we propose Deep Transformer Q-Networks (DTQN), a novel architecture
utilizing transformers and self-attention to encode an agent's history. DTQN is
designed modularly, and we compare results against several modifications to our
base model. Our experiments demonstrate the transformer can solve partially
observable tasks faster and more stably than previous recurrent approaches.",https://github.com/kevslinger/DTQN,7971
Learning Progressive Modality-shared Transformers for Effective Visible-Infrared Person Re-identification,0.708943,"Visible-Infrared Person Re-Identification (VI-ReID) is a challenging
retrieval task under complex modality changes. Existing methods usually focus
on extracting discriminative visual features while ignoring the reliability and
commonality of visual features between different modalities. In this paper, we
propose a novel deep learning framework named Progressive Modality-shared
Transformer (PMT) for effective VI-ReID. To reduce the negative effect of
modality gaps, we first take the gray-scale images as an auxiliary modality and
propose a progressive learning strategy. Then, we propose a Modality-Shared
Enhancement Loss (MSEL) to guide the model to explore more reliable identity
information from modality-shared features. Finally, to cope with the problem of
large intra-class differences and small inter-class differences, we propose a
Discriminative Center Loss (DCL) combined with the MSEL to further improve the
discrimination of reliable features. Extensive experiments on SYSU-MM01 and
RegDB datasets show that our proposed framework performs better than most
state-of-the-art methods. For model reproduction, we release the source code at
https://github.com/hulu88/PMT.",https://github.com/hulu88/PMT,370
Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild,0.158744,"Previous methods for dynamic facial expression in the wild are mainly based
on Convolutional Neural Networks (CNNs), whose local operations ignore the
long-range dependencies in videos. To solve this problem, we propose the
spatio-temporal Transformer (STT) to capture discriminative features within
each frame and model contextual relationships among frames. Spatio-temporal
dependencies are captured and integrated by our unified Transformer.
Specifically, given an image sequence consisting of multiple frames as input,
we utilize the CNN backbone to translate each frame into a visual feature
sequence. Subsequently, the spatial attention and the temporal attention within
each block are jointly applied for learning spatio-temporal representations at
the sequence level. In addition, we propose the compact softmax cross entropy
loss to further encourage the learned features have the minimum intra-class
distance and the maximum inter-class distance. Experiments on two in-the-wild
dynamic facial expression datasets (i.e., DFEW and AFEW) indicate that our
method provides an effective way to make use of the spatial and temporal
dependencies for dynamic facial expression recognition. The source code and the
training logs will be made publicly available.",None,36241
Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment,0.614641,"Training a generative adversarial network (GAN) with limited data has been a
challenging task. A feasible solution is to start with a GAN well-trained on a
large scale source domain and adapt it to the target domain with a few samples,
termed as few shot generative model adaption. However, existing methods are
prone to model overfitting and collapse in extremely few shot setting (less
than 10). To solve this problem, we propose a relaxed spatial structural
alignment method to calibrate the target generative models during the adaption.
We design a cross-domain spatial structural consistency loss comprising the
self-correlation and disturbance correlation consistency loss. It helps align
the spatial structural information between the synthesis image pairs of the
source and target domains. To relax the cross-domain alignment, we compress the
original latent space of generative models to a subspace. Image pairs generated
from the subspace are pulled closer. Qualitative and quantitative experiments
show that our method consistently surpasses the state-of-the-art methods in few
shot setting.",https://github.com/StevenShaw1999/RSSA,18126
DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation,0.288338,"In many real-world scenarios, we often deal with streaming data that is
sequentially collected over time. Due to the non-stationary nature of the
environment, the streaming data distribution may change in unpredictable ways,
which is known as concept drift. To handle concept drift, previous methods
first detect when/where the concept drift happens and then adapt models to fit
the distribution of the latest data. However, there are still many cases that
some underlying factors of environment evolution are predictable, making it
possible to model the future concept drift trend of the streaming data, while
such cases are not fully explored in previous work.
  In this paper, we propose a novel method DDG-DA, that can effectively
forecast the evolution of data distribution and improve the performance of
models. Specifically, we first train a predictor to estimate the future data
distribution, then leverage it to generate training samples, and finally train
models on the generated data. We conduct experiments on three real-world tasks
(forecasting on stock price trend, electricity load and solar irradiance) and
obtain significant improvement on multiple widely-used models.",https://github.com/Microsoft/qlib/tree/main/examples/benchmarks%20dynamic/DDG-DA,8108
Monotonic Differentiable Sorting Networks,0.141674,"Differentiable sorting algorithms allow training with sorting and ranking
supervision, where only the ordering or ranking of samples is known. Various
methods have been proposed to address this challenge, ranging from optimal
transport-based differentiable Sinkhorn sorting algorithms to making classic
sorting networks differentiable. One problem of current differentiable sorting
methods is that they are non-monotonic. To address this issue, we propose a
novel relaxation of conditional swap operations that guarantees monotonicity in
differentiable sorting networks. We introduce a family of sigmoid functions and
prove that they produce differentiable sorting networks that are monotonic.
Monotonicity ensures that the gradients always have the correct sign, which is
an advantage in gradient-based optimization. We demonstrate that monotonic
differentiable sorting networks improve upon previous differentiable sorting
methods.",https://github.com/Felix-Petersen/diffsort,13631
Multi-Game Decision Transformers,0.967645,"A longstanding goal of the field of AI is a method for learning a highly
capable, generalist agent from diverse experience. In the subfields of vision
and language, this was largely achieved by scaling up transformer-based models
and training them on large, diverse datasets. Motivated by this progress, we
investigate whether the same strategy can be used to produce generalist
reinforcement learning agents. Specifically, we show that a single
transformer-based model - with a single set of weights - trained purely offline
can play a suite of up to 46 Atari games simultaneously at close-to-human
performance. When trained and evaluated appropriately, we find that the same
trends observed in language and vision hold, including scaling of performance
with model size and rapid adaptation to new games via fine-tuning. We compare
several approaches in this multi-game setting, such as online and offline RL
methods and behavioral cloning, and find that our Multi-Game Decision
Transformer models offer the best scalability and performance. We release the
pre-trained models and code to encourage further research in this direction.",https://github.com/deepmind,34890
Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors,0.848092,"The main question this work aims at answering is: ""can morphing attack
detection (MAD) solutions be successfully developed based on synthetic data?"".
Towards that, this work introduces the first synthetic-based MAD development
dataset, namely the Synthetic Morphing Attack Detection Development dataset
(SMDD). This dataset is utilized successfully to train three MAD backbones
where it proved to lead to high MAD performance, even on completely unknown
attack types. Additionally, an essential aspect of this work is the detailed
legal analyses of the challenges of using and sharing real biometric data,
rendering our proposed SMDD dataset extremely essential. The SMDD dataset,
consisting of 30,000 attack and 50,000 bona fide samples, is publicly available
for research purposes.",https://github.com/naserdamer/SMDD-Synthetic-Face-Morphing-Attack-Detection-Development-dataset,4711
Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches,0.251237,"Deep learning has substantially boosted the performance of Monocular Depth
Estimation (MDE), a critical component in fully vision-based autonomous driving
(AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack
against learning-based MDE. In particular, we use an optimization-based method
to systematically generate stealthy physical-object-oriented adversarial
patches to attack depth estimation. We balance the stealth and effectiveness of
our attack with object-oriented adversarial design, sensitive region
localization, and natural style camouflage. Using real-world driving scenarios,
we evaluate our attack on concurrent MDE models and a representative downstream
task for AD (i.e., 3D object detection). Experimental results show that our
method can generate stealthy, effective, and robust adversarial patches for
different target objects and models and achieves more than 6 meters mean depth
estimation error and 93% attack success rate (ASR) in object detection with a
patch of 1/9 of the vehicle's rear area. Field tests on three different driving
routes with a real vehicle indicate that we cause over 6 meters mean depth
estimation error and reduce the object detection rate from 90.70% to 5.16% in
continuous video frames.",None,15242
Leveraging Social Influence based on Users Activity Centers for Point-of-Interest Recommendation,0.0528053,"Recommender Systems (RSs) aim to model and predict the user preference while
interacting with items, such as Points of Interest (POIs). These systems face
several challenges, such as data sparsity, limiting their effectiveness. In
this paper, we address this problem by incorporating social, geographical, and
temporal information into the Matrix Factorization (MF) technique. To this end,
we model social influence based on two factors: similarities between users in
terms of common check-ins and the friendships between them. We introduce two
levels of friendship based on explicit friendship networks and high check-in
overlap between users. We base our friendship algorithm on users' geographical
activity centers. The results show that our proposed model outperforms the
state-of-the-art on two real-world datasets. More specifically, our ablation
study shows that the social model improves the performance of our proposed POI
recommendation system by 31% and 14% on the Gowalla and Yelp datasets in terms
of Precision@10, respectively.",https://github.com/Seyedhosseinzadeh/SUCP,12
BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation,0.999976,"Vision-Language Pre-training (VLP) has advanced the performance for many
vision-language tasks. However, most existing pre-trained models only excel in
either understanding-based tasks or generation-based tasks. Furthermore,
performance improvement has been largely achieved by scaling up the dataset
with noisy image-text pairs collected from the web, which is a suboptimal
source of supervision. In this paper, we propose BLIP, a new VLP framework
which transfers flexibly to both vision-language understanding and generation
tasks. BLIP effectively utilizes the noisy web data by bootstrapping the
captions, where a captioner generates synthetic captions and a filter removes
the noisy ones. We achieve state-of-the-art results on a wide range of
vision-language tasks, such as image-text retrieval (+2.7% in average
recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score).
BLIP also demonstrates strong generalization ability when directly transferred
to video-language tasks in a zero-shot manner. Code, models, and datasets are
released at https://github.com/salesforce/BLIP.",https://github.com/salesforce/BLIP,41181
Few-shot Learning with Noisy Labels,0.149947,"Few-shot learning (FSL) methods typically assume clean support sets with
accurately labeled samples when training on novel classes. This assumption can
often be unrealistic: support sets, no matter how small, can still include
mislabeled samples. Robustness to label noise is therefore essential for FSL
methods to be practical, but this problem surprisingly remains largely
unexplored. To address mislabeled samples in FSL settings, we make several
technical contributions. (1) We offer simple, yet effective, feature
aggregation methods, improving the prototypes used by ProtoNet, a popular FSL
technique. (2) We describe a novel Transformer model for Noisy Few-Shot
Learning (TraNFS). TraNFS leverages a transformer's attention mechanism to
weigh mislabeled versus correct samples. (3) Finally, we extensively test these
methods on noisy versions of MiniImageNet and TieredImageNet. Our results show
that TraNFS is on-par with leading FSL methods on clean support sets, yet
outperforms them, by far, in the presence of label noise.",None,16626
Simple Open-Vocabulary Object Detection with Vision Transformers,0.824939,"Combining simple architectures with large-scale pre-training has led to
massive improvements in image classification. For object detection,
pre-training and scaling approaches are less well established, especially in
the long-tailed and open-vocabulary setting, where training data is relatively
scarce. In this paper, we propose a strong recipe for transferring image-text
models to open-vocabulary object detection. We use a standard Vision
Transformer architecture with minimal modifications, contrastive image-text
pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling
properties of this setup shows that increasing image-level pre-training and
model size yield consistent improvements on the downstream detection task. We
provide the adaptation strategies and regularizations needed to attain very
strong performance on zero-shot text-conditioned and one-shot image-conditioned
object detection. Code and models are available on GitHub.",https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit,82737
Event Transformer. A sparse-aware solution for efficient event data processing,0.696371,"Event cameras are sensors of great interest for many applications that run in
low-resource and challenging environments. They log sparse illumination changes
with high temporal resolution and high dynamic range, while they present
minimal power consumption. However, top-performing methods often ignore
specific event-data properties, leading to the development of generic but
computationally expensive algorithms. Efforts toward efficient solutions
usually do not achieve top-accuracy results for complex tasks. This work
proposes a novel framework, Event Transformer (EvT), that effectively takes
advantage of event-data properties to be highly efficient and accurate. We
introduce a new patch-based event representation and a compact transformer-like
architecture to process it. EvT is evaluated on different event-based
benchmarks for action and gesture recognition. Evaluation results show better
or comparable accuracy to the state-of-the-art while requiring significantly
less computation resources, which makes EvT able to work with minimal latency
both on GPU and CPU.",https://github.com/AlbertoSabater/EventTransformer,6081
CIRCLe: Color Invariant Representation Learning for Unbiased Classification of Skin Lesions,0.159289,"While deep learning based approaches have demonstrated expert-level
performance in dermatological diagnosis tasks, they have also been shown to
exhibit biases toward certain demographic attributes, particularly skin types
(e.g., light versus dark), a fairness concern that must be addressed. We
propose CIRCLe, a skin color invariant deep representation learning method for
improving fairness in skin lesion classification. CIRCLe is trained to classify
images by utilizing a regularization loss that encourages images with the same
diagnosis but different skin types to have similar latent representations.
Through extensive evaluation and ablation studies, we demonstrate CIRCLe's
superior performance over the state-of-the-art when evaluated on 16k+ images
spanning 6 Fitzpatrick skin types and 114 diseases, using classification
accuracy, equal opportunity difference (for light versus dark groups), and
normalized accuracy range, a new measure we propose to assess fairness on
multiple skin type groups.",https://github.com/arezou-pakzad/CIRCLe,13990
GazeOnce: Real-Time Multi-Person Gaze Estimation,0.290627,"Appearance-based gaze estimation aims to predict the 3D eye gaze direction
from a single image. While recent deep learning-based approaches have
demonstrated excellent performance, they usually assume one calibrated face in
each input image and cannot output multi-person gaze in real time. However,
simultaneous gaze estimation for multiple people in the wild is necessary for
real-world applications. In this paper, we propose the first one-stage
end-to-end gaze estimation method, GazeOnce, which is capable of simultaneously
predicting gaze directions for multiple faces (>10) in an image. In addition,
we design a sophisticated data generation pipeline and propose a new dataset,
MPSGaze, which contains full images of multiple people with 3D gaze ground
truth. Experimental results demonstrate that our unified framework not only
offers a faster speed, but also provides a lower gaze estimation error compared
with state-of-the-art methods. This technique can be useful in real-time
applications with multiple users.",None,2832
News Summarization and Evaluation in the Era of GPT-3,0.991461,"The recent success of prompting large language models like GPT-3 has led to a
paradigm shift in NLP research. In this paper, we study its impact on text
summarization, focusing on the classic benchmark domain of news summarization.
First, we investigate how GPT-3 compares against fine-tuned models trained on
large summarization datasets. We show that not only do humans overwhelmingly
prefer GPT-3 summaries, prompted using only a task description, but these also
do not suffer from common dataset-specific issues such as poor factuality.
Next, we study what this means for evaluation, particularly the role of gold
standard test sets. Our experiments show that both reference-based and
reference-free automatic metrics cannot reliably evaluate GPT-3 summaries.
Finally, we evaluate models on a setting beyond generic summarization,
specifically keyword-based summarization, and show how dominant fine-tuning
approaches compare to prompting.
  To support further research, we release: (a) a corpus of 10K generated
summaries from fine-tuned and prompt-based models across 4 standard
summarization benchmarks, (b) 1K human preference judgments comparing different
systems for generic- and keyword-based summarization.",None,5490
NICO++: Towards Better Benchmarking for Domain Generalization,0.189952,"Despite the remarkable performance that modern deep neural networks have
achieved on independent and identically distributed (I.I.D.) data, they can
crash under distribution shifts. Most current evaluation methods for domain
generalization (DG) adopt the leave-one-out strategy as a compromise on the
limited number of domains. We propose a large-scale benchmark with extensive
labeled domains named NICO++ along with more rational evaluation methods for
comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose
two metrics to quantify covariate shift and concept shift, respectively. Two
novel generalization bounds from the perspective of data construction are
proposed to prove that limited concept shift and significant covariate shift
favor the evaluation capability for generalization. Through extensive
experiments, NICO++ shows its superior evaluation capability compared with
current DG datasets and its contribution in alleviating unfairness caused by
the leak of oracle knowledge in model selection.",https://github.com/xxgege/NICO-plus,51525
Learning Equivariant Segmentation with Instance-Unique Querying,0.157033,"Prevalent state-of-the-art instance segmentation methods fall into a
query-based scheme, in which instance masks are derived by querying the image
feature using a set of instance-aware embeddings. In this work, we devise a new
training framework that boosts query-based models through discriminative query
embedding learning. It explores two essential properties, namely dataset-level
uniqueness and transformation equivariance, of the relation between queries and
instances. First, our algorithm uses the queries to retrieve the corresponding
instances from the whole training dataset, instead of only searching within
individual scenes. As querying instances across scenes is more challenging, the
segmenters are forced to learn more discriminative queries for effective
instance separation. Second, our algorithm encourages both image (instance)
representations and queries to be equivariant against geometric
transformations, leading to more robust, instance-query matching. On top of
four famous, query-based models ($i.e.,$ CondInst, SOLOv2, SOTR, and
Mask2Former), our training algorithm provides significant performance gains
($e.g.,$ +1.6 - 3.2 AP) on COCO dataset. In addition, our algorithm promotes
the performance of SOLOv2 by 2.7 AP, on LVISv1 dataset.",https://github.com/JamesLiang819/Instance_Unique_Querying,1818
ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification,0.230164,"Generating new events given context with correlated ones plays a crucial role
in many event-centric reasoning tasks. Existing works either limit their scope
to specific scenarios or overlook event-level correlations. In this paper, we
propose to pre-train a general Correlation-aware context-to-Event Transformer
(ClarET) for event-centric reasoning. To achieve this, we propose three novel
event-centric objectives, i.e., whole event recovering, contrastive
event-correlation encoding and prompt-based event locating, which highlight
event-level correlations with effective training. The proposed ClarET is
applicable to a wide range of event-centric reasoning scenarios, considering
its versatility of (i) event-correlation types (e.g., causal, temporal,
contrast), (ii) application formulations (i.e., generation and classification),
and (iii) reasoning types (e.g., abductive, counterfactual and ending
reasoning). Empirical fine-tuning results, as well as zero- and few-shot
learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4
reasoning types with diverse event correlations), verify its effectiveness and
generalization ability.",https://github.com/yczhou001/ClarET,20961
Quark: Controllable Text Generation with Reinforced Unlearning,0.382432,"Large-scale language models often learn behaviors that are misaligned with
user expectations. Generated text may contain offensive or toxic language,
contain significant repetition, or be of a different sentiment than desired by
the user. We consider the task of unlearning these misalignments by fine-tuning
the language model on signals of what not to do. We introduce Quantized Reward
Konditioning (Quark), an algorithm for optimizing a reward function that
quantifies an (un)wanted property, while not straying too far from the original
model. Quark alternates between (i) collecting samples with the current
language model, (ii) sorting them into quantiles based on reward, with each
quantile identified by a reward token prepended to the language model's input,
and (iii) using a standard language modeling loss on samples from each quantile
conditioned on its reward token, while remaining nearby the original language
model via a KL-divergence penalty. By conditioning on a high-reward token at
generation time, the model generates text that exhibits less of the unwanted
property. For unlearning toxicity, negative sentiment, and repetition, our
experiments show that Quark outperforms both strong baselines and
state-of-the-art reinforcement learning methods like PPO (Schulman et al.
2017), while relying only on standard language modeling primitives.",https://github.com/GXimingLu/Quark,46374
Omnivore: A Single Model for Many Visual Modalities,0.999928,"Prior work has studied different visual modalities in isolation and developed
separate architectures for recognition of images, videos, and 3D data. Instead,
in this paper, we propose a single model which excels at classifying images,
videos, and single-view 3D data using exactly the same model parameters. Our
'Omnivore' model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities.
Omnivore is simple to train, uses off-the-shelf standard datasets, and performs
at-par or better than modality-specific models of the same size. A single
Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN
RGB-D. After finetuning, our models outperform prior work on a variety of
vision tasks and generalize across modalities. Omnivore's shared visual
representation naturally enables cross-modal recognition without access to
correspondences between modalities. We hope our results motivate researchers to
model visual modalities together.",None,111734
$\mathcal{X}$-Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing,0.140208,"This paper presents a generic probabilistic framework for estimating the
statistical dependency and finding the anatomical correspondences among an
arbitrary number of medical images. The method builds on a novel formulation of
the $N$-dimensional joint intensity distribution by representing the common
anatomy as latent variables and estimating the appearance model with
nonparametric estimators. Through connection to maximum likelihood and the
expectation-maximization algorithm, an information\hyp{}theoretic metric called
$\mathcal{X}$-metric and a co-registration algorithm named $\mathcal{X}$-CoReg
are induced, allowing groupwise registration of the $N$ observed images with
computational complexity of $\mathcal{O}(N)$. Moreover, the method naturally
extends for a weakly-supervised scenario where anatomical labels of certain
images are provided. This leads to a combined\hyp{}computing framework
implemented with deep learning, which performs registration and segmentation
simultaneously and collaboratively in an end-to-end fashion. Extensive
experiments were conducted to demonstrate the versatility and applicability of
our model, including multimodal groupwise registration, motion correction for
dynamic contrast enhanced magnetic resonance images, and deep combined
computing for multimodal medical images. Results show the superiority of our
method in various applications in terms of both accuracy and efficiency,
highlighting the advantage of the proposed representation of the imaging
process.",https://zmiclab.github.io/projects.html,5501
Voxel Field Fusion for 3D Object Detection,0.370128,"In this work, we present a conceptually simple yet effective framework for
cross-modality 3D object detection, named voxel field fusion. The proposed
approach aims to maintain cross-modality consistency by representing and fusing
augmented image features as a ray in the voxel field. To this end, the
learnable sampler is first designed to sample vital features from the image
plane that are projected to the voxel grid in a point-to-ray manner, which
maintains the consistency in feature representation with spatial context. In
addition, ray-wise fusion is conducted to fuse features with the supplemental
context in the constructed voxel field. We further develop mixed augmentor to
align feature-variant transformations, which bridges the modality gap in data
augmentation. The proposed framework is demonstrated to achieve consistent
gains in various benchmarks and outperforms previous fusion-based methods on
KITTI and nuScenes datasets. Code is made available at
https://github.com/dvlab-research/VFF.",https://github.com/dvlab-research/VFF.1,76411
Towards Human-Agent Communication via the Information Bottleneck Principle,0.0895413,"Emergent communication research often focuses on optimizing task-specific
utility as a driver for communication. However, human languages appear to
evolve under pressure to efficiently compress meanings into communication
signals by optimizing the Information Bottleneck tradeoff between
informativeness and complexity. In this work, we study how trading off these
three factors -- utility, informativeness, and complexity -- shapes emergent
communication, including compared to human communication. To this end, we
propose Vector-Quantized Variational Information Bottleneck (VQ-VIB), a method
for training neural agents to compress inputs into discrete signals embedded in
a continuous space. We train agents via VQ-VIB and compare their performance to
previously proposed neural architectures in grounded environments and in a
Lewis reference game. Across all neural architectures and settings, taking into
account communicative informativeness benefits communication convergence rates,
and penalizing communicative complexity leads to human-like lexicon sizes while
maintaining high utility. Additionally, we find that VQ-VIB outperforms other
discrete communication methods. This work demonstrates how fundamental
principles that are believed to characterize human language evolution may
inform emergent communication in artificial agents.",None,24775
SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient object detection,0.981684,"Convolutional neural networks (CNNs) are good at extracting contexture
features within certain receptive fields, while transformers can model the
global long-range dependency features. By absorbing the advantage of
transformer and the merit of CNN, Swin Transformer shows strong feature
representation ability. Based on it, we propose a cross-modality fusion model
SwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin
Transformer to extract the hierarchical features, boosted by attention
mechanism to bridge the gap between two modalities, and guided by edge
information to sharp the contour of salient object. To be specific, two-stream
Swin Transformer encoder first extracts multi-modality features, and then
spatial alignment and channel re-calibration module is presented to optimize
intra-level cross-modality features. To clarify the fuzzy boundary, edge-guided
decoder achieves inter-level cross-modality fusion under the guidance of edge
features. The proposed model outperforms the state-of-the-art models on RGB-D
and RGB-T datasets, showing that it provides more insight into the
cross-modality complementarity task.",https://github.com/liuzywen/SwinNet,5263
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,0.184984,"This work presents a detailed linguistic analysis into why larger
Transformer-based pre-trained language models with more parameters and lower
perplexity nonetheless yield surprisal estimates that are less predictive of
human reading times. First, regression analyses show a strictly monotonic,
positive log-linear relationship between perplexity and fit to reading times
for the more recently released five GPT-Neo variants and eight OPT variants on
two separate datasets, replicating earlier results limited to just GPT-2 (Oh et
al., 2022). Subsequently, analysis of residual errors reveals a systematic
deviation of the larger variants, such as underpredicting reading times of
named entities and making compensatory overpredictions for reading times of
function words such as modals and conjunctions. These results suggest that the
propensity of larger Transformer-based models to 'memorize' sequences during
training makes their surprisal estimates diverge from humanlike expectations,
which warrants caution in using pre-trained language models to study human
language processing.",https://github.com/byungdoh/llm_surprisal,176
3D Common Corruptions and Data Augmentation,0.610361,"We introduce a set of image transformations that can be used as corruptions
to evaluate the robustness of models as well as data augmentation mechanisms
for training neural networks. The primary distinction of the proposed
transformations is that, unlike existing approaches such as Common Corruptions,
the geometry of the scene is incorporated in the transformations -- thus
leading to corruptions that are more likely to occur in the real world. We also
introduce a set of semantic corruptions (e.g. natural object occlusions). We
show these transformations are `efficient' (can be computed on-the-fly),
`extendable' (can be applied on most image datasets), expose vulnerability of
existing models, and can effectively make models more robust when employed as
`3D data augmentation' mechanisms. The evaluations on several tasks and
datasets suggest incorporating 3D information into benchmarking and training
opens up a promising direction for robustness research.",https://github.com/facebookresearch/detectron2,19756
EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance,0.401518,"Although current neural text-to-speech (TTS) models are able to generate
high-quality speech, intensity controllable emotional TTS is still a
challenging task. Most existing methods need external optimizations for
intensity calculation, leading to suboptimal results or degraded quality. In
this paper, we propose EmoDiff, a diffusion-based TTS model where emotion
intensity can be manipulated by a proposed soft-label guidance technique
derived from classifier guidance. Specifically, instead of being guided with a
one-hot vector for the specified emotion, EmoDiff is guided with a soft label
where the value of the specified emotion and \textit{Neutral} is set to
$\alpha$ and $1-\alpha$ respectively. The $\alpha$ here represents the emotion
intensity and can be chosen from 0 to 1. Our experiments show that EmoDiff can
precisely control the emotion intensity while maintaining high voice quality.
Moreover, diverse speech with specified emotion intensity can be generated by
sampling in the reverse denoising process.",None,8854
Ham2Pose: Animating Sign Language Notation into Pose Sequences,0.467542,"Translating spoken languages into Sign languages is necessary for open
communication between the hearing and hearing-impaired communities. To achieve
this goal, we propose the first method for animating a text written in
HamNoSys, a lexical Sign language notation, into signed pose sequences. As
HamNoSys is universal by design, our proposed method offers a generic solution
invariant to the target Sign language. Our method gradually generates pose
predictions using transformer encoders that create meaningful representations
of the text and poses while considering their spatial and temporal information.
We use weak supervision for the training process and show that our method
succeeds in learning from partial and inaccurate data. Additionally, we offer a
new distance measurement that considers missing keypoints, to measure the
distance between pose sequences using DTW-MJE. We validate its correctness
using AUTSL, a large-scale Sign language dataset, show that it measures the
distance between pose sequences more accurately than existing measurements, and
use it to assess the quality of our generated pose sequences. Code for the data
pre-processing, the model, and the distance measurement is publicly released
for future research.",https://github.com/AmitMY/pose-format,3241
Dataset Distillation by Matching Training Trajectories,0.964366,"Dataset distillation is the task of synthesizing a small dataset such that a
model trained on the synthetic set will match the test accuracy of the model
trained on the full dataset. In this paper, we propose a new formulation that
optimizes our distilled data to guide networks to a similar state as those
trained on real data across many training steps. Given a network, we train it
for several iterations on our distilled data and optimize the distilled data
with respect to the distance between the synthetically trained parameters and
the parameters trained on real data. To efficiently obtain the initial and
target network parameters for large-scale datasets, we pre-compute and store
training trajectories of expert networks trained on the real dataset. Our
method handily outperforms existing methods and also allows us to distill
higher-resolution visual data.",None,133896
Equivariant Self-Supervision for Musical Tempo Estimation,0.0590595,"Self-supervised methods have emerged as a promising avenue for representation
learning in the recent years since they alleviate the need for labeled
datasets, which are scarce and expensive to acquire. Contrastive methods are a
popular choice for self-supervision in the audio domain, and typically provide
a learning signal by forcing the model to be invariant to some transformations
of the input. These methods, however, require measures such as negative
sampling or some form of regularisation to be taken to prevent the model from
collapsing on trivial solutions. In this work, instead of invariance, we
propose to use equivariance as a self-supervision signal to learn audio tempo
representations from unlabelled data. We derive a simple loss function that
prevents the network from collapsing on a trivial solution during training,
without requiring any form of regularisation or negative sampling. Our
experiments show that it is possible to learn meaningful representations for
tempo estimation by solely relying on equivariant self-supervision, achieving
performance comparable with supervised methods on several benchmarks. As an
added benefit, our method only requires moderate compute resources and
therefore remains accessible to a wide research community.",https://github.com/Quint-e/equivariant-self-supervision-tempo,152
SGPT: GPT Sentence Embeddings for Semantic Search,0.508077,"Decoder transformers have continued increasing in scale reaching hundreds of
billions of parameters. Due to their scale the same decoder sets
state-of-the-art results on various language tasks via prompting or
fine-tuning. Yet, these large foundation models remain unusable for the related
fields of semantic search and sentence embeddings. This prevents possibly new
state-of-the-art results and forces organizations to train and maintain
separate models. To this end, we propose SGPT to use decoders for sentence
embeddings and semantic search via prompting or fine-tuning. At 5.8 billion
parameters SGPT improves on the previously best sentence embeddings by a margin
of 7% and outperforms a concurrent method with 175 billion parameters as
measured on the BEIR search benchmark. Code, models and result files are freely
available at https://github.com/Muennighoff/sgpt.",https://github.com/Muennighoff/sgpt,4781
TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter,0.178343,"Pre-trained language models (PLMs) are fundamental for natural language
processing applications. Most existing PLMs are not tailored to the noisy
user-generated text on social media, and the pre-training does not factor in
the valuable social engagement logs available in a social network. We present
TwHIN-BERT, a multilingual language model productionized at Twitter, trained on
in-domain data from the popular social network. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages,
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on various multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We open-source TwHIN-BERT and our
curated hashtag prediction and social engagement benchmark datasets to the
research community.",None,255449
"Less Data, More Knowledge: Building Next Generation Semantic Communication Networks",0.443997,"Semantic communication is viewed as a revolutionary paradigm that can
potentially transform how we design and operate wireless communication systems.
However, despite a recent surge of research activities in this area, the
research landscape remains limited. In this tutorial, we present the first
rigorous vision of a scalable end-to-end semantic communication network that is
founded on novel concepts from artificial intelligence (AI), causal reasoning,
and communication theory. We first discuss how the design of semantic
communication networks requires a move from data-driven networks towards
knowledge-driven ones. Subsequently, we highlight the necessity of creating
semantic representations of data that satisfy the key properties of minimalism,
generalizability, and efficiency so as to do more with less. We then explain
how those representations can form the basis a so-called semantic language. By
using semantic representation and languages, we show that the traditional
transmitter and receiver now become a teacher and apprentice. Then, we define
the concept of reasoning by investigating the fundamentals of causal
representation learning and their role in designing semantic communication
networks. We demonstrate that reasoning faculties are majorly characterized by
the ability to capture causal and associational relationships in datastreams.
For such reasoning-driven networks, we propose novel and essential semantic
communication metrics that include new ""reasoning capacity"" measures that could
go beyond Shannon's bound to capture the convergence of computing and
communication. Finally, we explain how semantic communications can be scaled to
large-scale networks (6G and beyond). In a nutshell, we expect this tutorial to
provide a comprehensive reference on how to properly build, analyze, and deploy
future semantic communication networks.",None,70013
Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging,0.0982916,"Training vision or language models on large datasets can take days, if not
weeks. We show that averaging the weights of the k latest checkpoints, each
collected at the end of an epoch, can speed up the training progression in
terms of loss and accuracy by dozens of epochs, corresponding to time savings
up to ~68 and ~30 GPU hours when training a ResNet50 on ImageNet and
RoBERTa-Base model on WikiText-103, respectively. We also provide the code and
model checkpoint trajectory to reproduce the results and facilitate research on
reusing historical weights for faster convergence.",https://github.com/jeankaddour/lawa,632
"The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",0.111428,"Construction Grammar (CxG) is a paradigm from cognitive linguistics
emphasising the connection between syntax and semantics. Rather than rules that
operate on lexical items, it posits constructions as the central building
blocks of language, i.e., linguistic units of different granularity that
combine syntax and semantics. As a first step towards assessing the
compatibility of CxG with the syntactic and semantic knowledge demonstrated by
state-of-the-art pretrained language models (PLMs), we present an investigation
of their capability to classify and understand one of the most commonly studied
constructions, the English comparative correlative (CC). We conduct experiments
examining the classification accuracy of a syntactic probe on the one hand and
the models' behaviour in a semantic application task on the other, with BERT,
RoBERTa, and DeBERTa as the example PLMs. Our results show that all three
investigated PLMs are able to recognise the structure of the CC but fail to use
its meaning. While human-like performance of PLMs on many NLP tasks has been
alleged, this indicates that PLMs still suffer from substantial shortcomings in
central domains of linguistic knowledge.",https://github.com/LeonieWeissweiler/ComparativeCorrelative,76971
Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models,0.566874,"Image restoration under adverse weather conditions has been of significant
interest for various computer vision applications. Recent successful methods
rely on the current progress in deep neural network architectural designs
(e.g., with vision transformers). Motivated by the recent progress achieved
with state-of-the-art conditional generative models, we present a novel
patch-based image restoration algorithm based on denoising diffusion
probabilistic models. Our patch-based diffusion modeling approach enables
size-agnostic image restoration by using a guided denoising process with
smoothed noise estimates across overlapping patches during inference. We
empirically evaluate our model on benchmark datasets for image desnowing,
combined deraining and dehazing, and raindrop removal. We demonstrate our
approach to achieve state-of-the-art performances on both weather-specific and
multi-weather image restoration, and experimentally show strong generalization
to real-world test images.",https://github.com/IGITUGraz/WeatherDiffusion,6767
Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,0.902191,"Large language models (LLMs) have shown remarkable reasoning capabilities
given chain-of-thought prompts (examples with intermediate reasoning steps).
Existing benchmarks measure reasoning ability indirectly, by evaluating
accuracy on downstream tasks such as mathematical reasoning. However, it is
unclear how these models obtain the answers and whether they rely on simple
heuristics rather than the generated chain-of-thought. To enable systematic
exploration of the reasoning ability of LLMs, we present a new synthetic
question-answering dataset called PrOntoQA, where each example is generated
from a synthetic world model represented in first-order logic. This allows us
to parse the generated chain-of-thought into symbolic proofs for formal
analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite
capable of making correct individual deduction steps, and so are generally
capable of reasoning, even in fictional contexts. However, they have difficulty
with proof planning: When multiple valid deduction steps are available, they
are not able to systematically explore the different options.",None,84287
Masked Generative Distillation,0.989691,"Knowledge distillation has been applied to various tasks successfully. The
current distillation algorithm usually improves students' performance by
imitating the output of the teacher. This paper shows that teachers can also
improve students' representation power by guiding students' feature recovery.
From this point of view, we propose Masked Generative Distillation (MGD), which
is simple: we mask random pixels of the student's feature and force it to
generate the teacher's full feature through a simple block. MGD is a truly
general feature-based distillation method, which can be utilized on various
tasks, including image classification, object detection, semantic segmentation
and instance segmentation. We experiment on different models with extensive
datasets and the results show that all the students achieve excellent
improvements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1
accuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP,
SOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on
ResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at
https://github.com/yzd-v/MGD.",https://github.com/yzd-v/MGD,5005
Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning,0.119566,"In this paper, we study the named entity recognition (NER) problem under
distant supervision. Due to the incompleteness of the external dictionaries
and/or knowledge bases, such distantly annotated training data usually suffer
from a high false negative rate. To this end, we formulate the Distantly
Supervised NER (DS-NER) problem via Multi-class Positive and Unlabeled (MPU)
learning and propose a theoretically and practically novel CONFidence-based MPU
(Conf-MPU) approach. To handle the incomplete annotations, Conf-MPU consists of
two steps. First, a confidence score is estimated for each token of being an
entity token. Then, the proposed Conf-MPU risk estimation is applied to train a
multi-class classifier for the NER task. Thorough experiments on two benchmark
datasets labeled by various external knowledge demonstrate the superiority of
the proposed Conf-MPU over existing DS-NER methods.",https://github.com/shangjingbo1226/AutoNER,3269
Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation,0.52901,"Scene flow estimation, which extracts point-wise motion between scenes, is
becoming a crucial task in many computer vision tasks. However, all of the
existing estimation methods utilize only the unidirectional features,
restricting the accuracy and generality. This paper presents a novel scene flow
estimation architecture using bidirectional flow embedding layers. The proposed
bidirectional layer learns features along both forward and backward directions,
enhancing the estimation performance. In addition, hierarchical feature
extraction and warping improve the performance and reduce computational
overhead. Experimental results show that the proposed architecture achieved a
new state-of-the-art record by outperforming other approaches with large margin
in both FlyingThings3D and KITTI benchmarks. Codes are available at
https://github.com/cwc1260/BiFlow.",https://github.com/cwc1260/BiFlow,1348
Balancing Discriminability and Transferability for Source-Free Domain Adaptation,0.18218,"Conventional domain adaptation (DA) techniques aim to improve domain
transferability by learning domain-invariant representations; while
concurrently preserving the task-discriminability knowledge gathered from the
labeled source data. However, the requirement of simultaneous access to labeled
source and unlabeled target renders them unsuitable for the challenging
source-free DA setting. The trivial solution of realizing an effective original
to generic domain mapping improves transferability but degrades task
discriminability. Upon analyzing the hurdles from both theoretical and
empirical standpoints, we derive novel insights to show that a mixup between
original and corresponding translated generic samples enhances the
discriminability-transferability trade-off while duly respecting the
privacy-oriented source-free setting. A simple but effective realization of the
proposed insights on top of the existing source-free DA approaches yields
state-of-the-art performance with faster convergence. Beyond single-source, we
also outperform multi-source prior-arts across both classification and semantic
segmentation benchmarks.",https://github.com/iver56/audiomentations,13740
NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages,0.323579,"Natural language processing (NLP) has a significant impact on society via
technologies such as machine translation and search engines. Despite its
success, NLP technology is only widely available for high-resource languages
such as English and Chinese, while it remains inaccessible to many languages
due to the unavailability of data resources and benchmarks. In this work, we
focus on developing resources for languages in Indonesia. Despite being the
second most linguistically diverse country, most languages in Indonesia are
categorized as endangered and some are even extinct. We develop the first-ever
parallel resource for 10 low-resource languages in Indonesia. Our resource
includes datasets, a multi-task benchmark, and lexicons, as well as a parallel
Indonesian-English dataset. We provide extensive analyses and describe the
challenges when creating such resources. We hope that our work can spark NLP
research on Indonesian and other underrepresented languages.",https://github.com/andria009/IndonesianSentimentLexicon,32819
TRUST XAI: Model-Agnostic Explanations for AI With a Case Study on IIoT Security,0.62827,"Despite AI's significant growth, its ""black box"" nature creates challenges in
generating adequate trust. Thus, it is seldom utilized as a standalone unit in
IoT high-risk applications, such as critical industrial infrastructures,
medical systems, and financial applications, etc. Explainable AI (XAI) has
emerged to help with this problem. However, designing appropriately fast and
accurate XAI is still challenging, especially in numerical applications. Here,
we propose a universal XAI model named Transparency Relying Upon Statistical
Theory (TRUST), which is model-agnostic, high-performing, and suitable for
numerical applications. Simply put, TRUST XAI models the statistical behavior
of the AI's outputs in an AI-based system. Factor analysis is used to transform
the input features into a new set of latent variables. We use mutual
information to rank these variables and pick only the most influential ones on
the AI's outputs and call them ""representatives"" of the classes. Then we use
multi-modal Gaussian distributions to determine the likelihood of any new
sample belonging to each class. We demonstrate the effectiveness of TRUST in a
case study on cybersecurity of the industrial Internet of things (IIoT) using
three different cybersecurity datasets. As IIoT is a prominent application that
deals with numerical data. The results show that TRUST XAI provides
explanations for new random samples with an average success rate of 98%.
Compared with LIME, a popular XAI model, TRUST is shown to be superior in the
context of performance, speed, and the method of explainability. In the end, we
also show how TRUST is explained to the user.",None,7250
Training Language Models with Memory Augmentation,0.757998,"Recent work has improved language models (LMs) remarkably by equipping them
with a non-parametric memory component. However, most existing approaches only
introduce mem-ories at testing time or represent them using a separately
trained encoder, resulting in suboptimal training of the language model. In
this work, we present TRIME, a novel yet simple training approach designed for
training LMs with memory augmentation. Our approach uses a training objective
that directly takes in-batch examples as accessible memory. We also present new
methods for memory construction and data batching, which are used for adapting
to different sets of memories--local, long-term, and external memory--at
testing time. We evaluate TRIME on multiple language modeling and machine
translation benchmarks and show that it is able to achieve significant
improvements across all the settings. Concretely, TRIME reduces the perplexity
from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory
set from the training corpus. Compared to standard LM training, TRIME adds
negligible computational overhead and is compatible with different neural
architectures, making it a versatile solution for training memory-augmented
LMs.",https://github.com/princeton-nlp/TRIME,49836
Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks,0.619979,"The wide adoption and application of Masked language models~(MLMs) on
sensitive data (from legal to medical) necessitates a thorough quantitative
investigation into their privacy vulnerabilities -- to what extent do MLMs leak
information about their training data? Prior attempts at measuring leakage of
MLMs via membership inference attacks have been inconclusive, implying the
potential robustness of MLMs to privacy attacks. In this work, we posit that
prior attempts were inconclusive because they based their attack solely on the
MLM's model score. We devise a stronger membership inference attack based on
likelihood ratio hypothesis testing that involves an additional reference MLM
to more accurately quantify the privacy risks of memorization in MLMs. We show
that masked language models are extremely susceptible to likelihood ratio
membership inference attacks: Our empirical results, on models trained on
medical notes, show that our attack improves the AUC of prior membership
inference attacks from 0.66 to an alarmingly high 0.90 level, with a
significant improvement in the low-error region: at 1% false positive rate, our
attack is 51X more powerful than prior work.",None,-1
Shadow-Background-Noise 3D Spatial Decomposition Using Sparse Low-Rank Gaussian Properties for Video-SAR Moving Target Shadow Enhancement,0.483617,"Moving target shadows among video synthetic aperture radar (Video-SAR) images
are always interfered by low scattering backgrounds and cluttered noises,
causing poor detec-tion-tracking accuracy. Thus, a shadow-background-noise 3D
spatial decomposition (SBN-3D-SD) model is proposed to enhance shadows for
higher detection-tracking accuracy. It leverages the sparse property of
shadows, the low-rank property of back-grounds, and the Gaussian property of
noises to perform 3D spatial three-decomposition. It separates shadows from
back-grounds and noises by the alternating direction method of multi-pliers
(ADMM). Results on the Sandia National Laboratories (SNL) data verify its
effectiveness. It boosts the shadow saliency from the qualitative and
quantitative evaluation. It boosts the shadow detection accuracy of Faster
R-CNN, RetinaNet and YOLOv3. It also boosts the shadow tracking accuracy of
TransTrack, FairMOT and ByteTrack.",None,-1
LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding,0.78243,"Structured document understanding has attracted considerable attention and
made significant progress recently, owing to its crucial role in intelligent
document processing. However, most existing related models can only deal with
the document data of specific language(s) (typically English) included in the
pre-training collection, which is extremely limited. To address this issue, we
propose a simple yet effective Language-independent Layout Transformer (LiLT)
for structured document understanding. LiLT can be pre-trained on the
structured documents of a single language and then directly fine-tuned on other
languages with the corresponding off-the-shelf monolingual/multilingual
pre-trained textual models. Experimental results on eight languages have shown
that LiLT can achieve competitive or even superior performance on diverse
widely-used downstream benchmarks, which enables language-independent benefit
from the pre-training of document layout structure. Code and model are publicly
available at https://github.com/jpWang/LiLT.",https://github.com/jpWang/LiLT,-1
"Towards Automated Document Revision: Grammatical Error Correction, Fluency Edits, and Beyond",0.133078,"Natural language processing technology has rapidly improved automated
grammatical error correction tasks, and the community begins to explore
document-level revision as one of the next challenges. To go beyond
sentence-level automated grammatical error correction to NLP-based
document-level revision assistant, there are two major obstacles: (1) there are
few public corpora with document-level revisions being annotated by
professional editors, and (2) it is not feasible to elicit all possible
references and evaluate the quality of revision with such references because
there are infinite possibilities of revision. This paper tackles these
challenges. First, we introduce a new document-revision corpus, TETRA, where
professional editors revised academic papers sampled from the ACL anthology
which contain few trivial grammatical errors that enable us to focus more on
document- and paragraph-level edits such as coherence and consistency. Second,
we explore reference-less and interpretable methods for meta-evaluation that
can detect quality improvements by document revision. We show the uniqueness of
TETRA compared with existing document revision corpora and demonstrate that a
fine-tuned pre-trained language model can discriminate the quality of documents
after revision even when the difference is subtle. This promising result will
encourage the community to further explore automated document revision models
and metrics in future.",https://github.com/chemicaltree/tetra,-1
Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation,0.205546,"In recent years, machine learning models have rapidly become better at
generating clinical consultation notes; yet, there is little work on how to
properly evaluate the generated consultation notes to understand the impact
they may have on both the clinician using them and the patient's clinical
safety. To address this we present an extensive human evaluation study of
consultation notes where 5 clinicians (i) listen to 57 mock consultations, (ii)
write their own notes, (iii) post-edit a number of automatically generated
notes, and (iv) extract all the errors, both quantitative and qualitative. We
then carry out a correlation study with 18 automatic quality metrics and the
human judgements. We find that a simple, character-based Levenshtein distance
metric performs on par if not better than common model-based metrics like
BertScore. All our findings and annotations are open-sourced.",https://github.com/babylonhealth/primock57,-1
Scalable Planning and Learning Framework Development for Swarm-to-Swarm Engagement Problems,0.129981,"Development of guidance, navigation and control frameworks/algorithms for
swarms attracted significant attention in recent years. That being said,
algorithms for planning swarm allocations/trajectories for engaging with enemy
swarms is largely an understudied problem. Although small-scale scenarios can
be addressed with tools from differential game theory, existing approaches fail
to scale for large-scale multi-agent pursuit evasion (PE) scenarios. In this
work, we propose a reinforcement learning (RL) based framework to decompose to
large-scale swarm engagement problems into a number of independent multi-agent
pursuit-evasion games. We simulate a variety of multi-agent PE scenarios, where
finite time capture is guaranteed under certain conditions. The calculated PE
statistics are provided as a reward signal to the high level allocation layer,
which uses an RL algorithm to allocate controlled swarm units to eliminate
enemy swarm units with maximum efficiency. We verify our approach in
large-scale swarm-to-swarm engagement simulations.",None,-1
Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning,0.474729,"Conversational recommender systems (CRS) aim to proactively elicit user
preference and recommend high-quality items through natural language
conversations. Typically, a CRS consists of a recommendation module to predict
preferred items for users and a conversation module to generate appropriate
responses. To develop an effective CRS, it is essential to seamlessly integrate
the two modules. Existing works either design semantic alignment strategies, or
share knowledge resources and representations between the two modules. However,
these approaches still rely on different architectures or techniques to develop
the two modules, making it difficult for effective module integration.
  To address this problem, we propose a unified CRS model named UniCRS based on
knowledge-enhanced prompt learning. Our approach unifies the recommendation and
conversation subtasks into the prompt learning paradigm, and utilizes
knowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to
fulfill both subtasks in a unified approach. In the prompt design, we include
fused knowledge representations, task-specific soft tokens, and the dialogue
context, which can provide sufficient contextual information to adapt the PLM
for the CRS task. Besides, for the recommendation subtask, we also incorporate
the generated response template as an important part of the prompt, to enhance
the information interaction between the two subtasks. Extensive experiments on
two public CRS datasets have demonstrated the effectiveness of our approach.",https://github.com/RUCAIBox/UniCRS,-1
Event knowledge in large language models: the gap between the impossible and the unlikely,0.1312,"Word co-occurrence patterns in language corpora contain a surprising amount
of conceptual knowledge. Large language models (LLMs), trained to predict words
in context, leverage these patterns to achieve impressive performance on
diverse semantic tasks requiring world knowledge. An important but understudied
question about LLMs' semantic abilities is whether they acquire generalized
knowledge of common events. Here, we test whether five pre-trained LLMs (from
2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions
of agent-patient interactions than to minimally different implausible versions
of the same event. Using three curated sets of minimal sentence pairs (total
n=1,215), we found that pre-trained LLMs possess substantial event knowledge,
outperforming other distributional language models. In particular, they almost
always assign higher likelihood to possible vs. impossible events (The teacher
bought the laptop vs. The laptop bought the teacher). However, LLMs show less
consistent preferences for likely vs. unlikely events (The nanny tutored the
boy vs. The boy tutored the nanny). In follow-up analyses, we show that (i) LLM
scores are driven by both plausibility and surface-level sentence features,
(ii) LLM scores generalize well across syntactic variants (active vs. passive
constructions) but less well across semantic variants (synonymous sentences),
(iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence
plausibility serves as an organizing dimension in internal LLM representations.
Overall, our results show that important aspects of event knowledge naturally
emerge from distributional linguistic patterns, but also highlight a gap
between representations of possible/impossible and likely/unlikely events.",https://github.com/carina-kauf/lm-event-knowledge,-1
Repairing Bugs in Python Assignments Using Large Language Models,0.642346,"Students often make mistakes on their introductory programming assignments as
part of their learning process. Unfortunately, providing custom repairs for
these mistakes can require a substantial amount of time and effort from class
instructors. Automated program repair (APR) techniques can be used to
synthesize such fixes. Prior work has explored the use of symbolic and neural
techniques for APR in the education domain. Both types of approaches require
either substantial engineering efforts or large amounts of data and training.
We propose to use a large language model trained on code, such as Codex, to
build an APR system -- MMAPR -- for introductory Python programming
assignments. Our system can fix both syntactic and semantic mistakes by
combining multi-modal prompts, iterative querying, test-case-based selection of
few-shots, and program chunking. We evaluate MMAPR on 286 real student programs
and compare to a baseline built by combining a state-of-the-art Python syntax
repair engine, BIFI, and state-of-the-art Python semantic repair engine for
student assignments, Refactory. We find that MMAPR can fix more programs and
produce smaller patches on average.",None,-1
Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation,0.739853,"Transformers have revolutionized vision and natural language processing with
their ability to scale with large datasets. But in robotic manipulation, data
is both limited and expensive. Can manipulation still benefit from Transformers
with the right problem formulation? We investigate this question with PerAct, a
language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation.
PerAct encodes language goals and RGB-D voxel observations with a Perceiver
Transformer, and outputs discretized actions by ``detecting the next best voxel
action''. Unlike frameworks that operate on 2D images, the voxelized 3D
observation and action space provides a strong structural prior for efficiently
learning 6-DoF actions. With this formulation, we train a single multi-task
Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks
(with 18 variations) from just a few demonstrations per task. Our results show
that PerAct significantly outperforms unstructured image-to-action agents and
3D ConvNet baselines for a wide range of tabletop tasks.",https://github.com/lucidrains/perceiver-pytorch,-1
MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation,0.480304,"Applying existing methods to emotional support conversation -- which provides
valuable assistance to people who are in need -- has two major limitations: (a)
they generally employ a conversation-level emotion label, which is too
coarse-grained to capture user's instant mental state; (b) most of them focus
on expressing empathy in the response(s) rather than gradually reducing user's
distress. To address the problems, we propose a novel model \textbf{MISC},
which firstly infers the user's fine-grained emotional status, and then
responds skillfully using a mixture of strategy. Experimental results on the
benchmark dataset demonstrate the effectiveness of our method and reveal the
benefits of fine-grained emotion understanding as well as mixed-up strategy
modeling. Our code and data could be found in
\url{https://github.com/morecry/MISC}.",https://github.com/morecry/MISC,-1
Transformer-based SAR Image Despeckling,0.424057,"Synthetic Aperture Radar (SAR) images are usually degraded by a
multiplicative noise known as speckle which makes processing and interpretation
of SAR images difficult. In this paper, we introduce a transformer-based
network for SAR image despeckling. The proposed despeckling network comprises
of a transformer-based encoder which allows the network to learn global
dependencies between different image regions - aiding in better despeckling.
The network is trained end-to-end with synthetically generated speckled images
using a composite loss function. Experiments show that the proposed method
achieves significant improvements over traditional and convolutional neural
network-based despeckling methods on both synthetic and real SAR images.",https://github.com/malshaV/sar_transformer,-1
Grammar-Based Grounded Lexicon Learning,0.156944,"We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist
approach toward learning a compositional and grounded meaning representation of
language from grounded data, such as paired images and texts. At the core of
G2L2 is a collection of lexicon entries, which map each word to a tuple of a
syntactic type and a neuro-symbolic semantic program. For example, the word
shiny has a syntactic type of adjective; its neuro-symbolic semantic program
has the symbolic form {\lambda}x. filter(x, SHINY), where the concept SHINY is
associated with a neural network embedding, which will be used to classify
shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries
associated with each token. It then derives the meaning of the sentence as an
executable neuro-symbolic program by composing lexical meanings based on
syntax. The recovered meaning programs can be executed on grounded inputs. To
facilitate learning in an exponentially-growing compositional space, we
introduce a joint parsing and expected execution algorithm, which does local
marginalization over derivations to reduce the training time. We evaluate G2L2
on two domains: visual reasoning and language-driven navigation. Results show
that G2L2 can generalize from small amounts of data to novel compositions of
words.",None,-1
SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer,0.346647,"Point cloud completion has become increasingly popular among generation tasks
of 3D point clouds, as it is a challenging yet indispensable problem to recover
the complete shape of a 3D object from its partial observation. In this paper,
we propose a novel SeedFormer to improve the ability of detail preservation and
recovery in point cloud completion. Unlike previous methods based on a global
feature vector, we introduce a new shape representation, namely Patch Seeds,
which not only captures general structures from partial inputs but also
preserves regional information of local patterns. Then, by integrating seed
features into the generation process, we can recover faithful details for
complete point clouds in a coarse-to-fine manner. Moreover, we devise an
Upsample Transformer by extending the transformer structure into basic
operations of point generators, which effectively incorporates spatial and
semantic relationships between neighboring points. Qualitative and quantitative
evaluations demonstrate that our method outperforms state-of-the-art completion
networks on several benchmark datasets. Our code is available at
https://github.com/hrzhou2/seedformer.",https://github.com/hrzhou2/seedformer,-1
PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition,0.0424101,"The widely studied task of Natural Language Inference (NLI) requires a system
to recognize whether one piece of text is textually entailed by another, i.e.
whether the entirety of its meaning can be inferred from the other. In current
NLI datasets and models, textual entailment relations are typically defined on
the sentence- or paragraph-level. However, even a simple sentence often
contains multiple propositions, i.e. distinct units of meaning conveyed by the
sentence. As these propositions can carry different truth values in the context
of a given premise, we argue for the need to recognize the textual entailment
relation of each proposition in a sentence individually.
  We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert
human raters. Our dataset structure resembles the tasks of (1) segmenting
sentences within a document to the set of propositions, and (2) classifying the
entailment relation of each proposition with respect to a different yet
topically-aligned document, i.e. documents describing the same event or entity.
We establish strong baselines for the segmentation and entailment tasks.
Through case studies on summary hallucination detection and document-level NLI,
we demonstrate that our conceptual framework is potentially useful for
understanding and explaining the compositionality of NLI labels.",https://github.com/google-research-datasets/propsegment,-1
SDF-StyleGAN: Implicit SDF-Based StyleGAN for 3D Shape Generation,0.970239,"We present a StyleGAN2-based deep learning approach for 3D shape generation,
called SDF-StyleGAN, with the aim of reducing visual and geometric
dissimilarity between generated shapes and a shape collection. We extend
StyleGAN2 to 3D generation and utilize the implicit signed distance function
(SDF) as the 3D shape representation, and introduce two novel global and local
shape discriminators that distinguish real and fake SDF values and gradients to
significantly improve shape geometry and visual quality. We further complement
the evaluation metrics of 3D generative models with the shading-image-based
Fr\'echet inception distance (FID) scores to better assess visual quality and
shape distribution of the generated shapes. Experiments on shape generation
demonstrate the superior performance of SDF-StyleGAN over the state-of-the-art.
We further demonstrate the efficacy of SDF-StyleGAN in various tasks based on
GAN inversion, including shape reconstruction, shape completion from partial
point clouds, single-view image-based shape generation, and shape style
editing. Extensive ablation studies justify the efficacy of our framework
design. Our code and trained models are available at
https://github.com/Zhengxinyang/SDF-StyleGAN.",https://github.com/Zhengxinyang/SDF-StyleGAN,-1
PP-YOLOE: An evolved version of YOLO,0.62568,"In this report, we present PP-YOLOE, an industrial state-of-the-art object
detector with high performance and friendly deployment. We optimize on the
basis of the previous PP-YOLOv2, using anchor-free paradigm, more powerful
backbone and neck equipped with CSPRepResStage, ET-head and dynamic label
assignment algorithm TAL. We provide s/m/l/x models for different practice
scenarios. As a result, PP-YOLOE-l achieves 51.4 mAP on COCO test-dev and 78.1
FPS on Tesla V100, yielding a remarkable improvement of (+1.9 AP, +13.35% speed
up) and (+1.3 AP, +24.96% speed up), compared to the previous state-of-the-art
industrial models PP-YOLOv2 and YOLOX respectively. Further, PP-YOLOE inference
speed achieves 149.2 FPS with TensorRT and FP16-precision. We also conduct
extensive experiments to verify the effectiveness of our designs. Source code
and pre-trained models are available at
https://github.com/PaddlePaddle/PaddleDetection.",https://github.com/PaddlePaddle/PaddleDetection,-1
VALHALLA: Visual Hallucination for Machine Translation,0.251167,"Designing better machine translation systems by considering auxiliary inputs
such as images has attracted much attention in recent years. While existing
methods show promising performance over the conventional text-only translation
systems, they typically require paired text and image as input during
inference, which limits their applicability to real-world scenarios. In this
paper, we introduce a visual hallucination framework, called VALHALLA, which
requires only source sentences at inference time and instead uses hallucinated
visual representations for multimodal machine translation. In particular, given
a source sentence an autoregressive hallucination transformer is used to
predict a discrete visual representation from the input text, and the combined
text and hallucinated representations are utilized to obtain the target
translation. We train the hallucination transformer jointly with the
translation transformer using standard backpropagation with cross-entropy
losses while being guided by an additional loss that encourages consistency
between predictions using either ground-truth or hallucinated visual
representations. Extensive experiments on three standard translation datasets
with a diverse set of language pairs demonstrate the effectiveness of our
approach over both text-only baselines and state-of-the-art methods. Project
page: http://www.svcl.ucsd.edu/projects/valhalla.",http://www.svcl.ucsd.edu/projects/valhalla,-1
CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer Learning,0.107396,"Generating artistic portraits is a challenging problem in computer vision.
Existing portrait stylization models that generate good quality results are
based on Image-to-Image Translation and require abundant data from both source
and target domains. However, without enough data, these methods would result in
overfitting. In this work, we propose CtlGAN, a new few-shot artistic portraits
generation model with a novel contrastive transfer learning strategy. We adapt
a pretrained StyleGAN in the source domain to a target artistic domain with no
more than 10 artistic faces. To reduce overfitting to the few training
examples, we introduce a novel Cross-Domain Triplet loss which explicitly
encourages the target instances generated from different latent codes to be
distinguishable. We propose a new encoder which embeds real faces into Z+ space
and proposes a dual-path training strategy to better cope with the adapted
decoder and eliminate the artifacts. Extensive qualitative, quantitative
comparisons and a user study show our method significantly outperforms
state-of-the-arts under 10-shot and 1-shot settings and generates high quality
artistic portraits. The code will be made publicly available.",None,-1
A Span-level Bidirectional Network for Aspect Sentiment Triplet Extraction,0.499575,"Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment
analysis task that aims to extract triplets of aspect terms, sentiments, and
opinion terms from review sentences. Recently, span-level models achieve
gratifying results on ASTE task by taking advantage of the predictions of all
possible spans. Since all possible spans significantly increases the number of
potential aspect and opinion candidates, it is crucial and challenging to
efficiently extract the triplet elements among them. In this paper, we present
a span-level bidirectional network which utilizes all possible spans as input
and extracts triplets from spans bidirectionally. Specifically, we devise both
the aspect decoder and opinion decoder to decode the span representations and
extract triples from aspect-to-opinion and opinion-to-aspect directions. With
these two decoders complementing with each other, the whole network can extract
triplets from spans more comprehensively. Moreover, considering that mutual
exclusion cannot be guaranteed between the spans, we design a similar span
separation loss to facilitate the downstream task of distinguishing the correct
span by expanding the KL divergence of similar spans during the training
process; in the inference process, we adopt an inference strategy to remove
conflicting triplets from the results base on their confidence scores.
Experimental results show that our framework not only significantly outperforms
state-of-the-art methods, but achieves better performance in predicting
triplets with multi-token entities and extracting triplets in sentences contain
multi-triplets.",https://github.com/chen1310054465/SBN,-1
Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind,0.0561353,"When reading a story, humans can quickly understand new fictional characters
with a few observations, mainly by drawing analogies to fictional and real
people they already know. This reflects the few-shot and meta-learning essence
of humans' inference of characters' mental states, i.e., theory-of-mind (ToM),
which is largely ignored in existing research. We fill this gap with a novel
NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM
in a realistic narrative understanding scenario. Our dataset consists of ~1,000
parsed movie scripts, each corresponding to a few-shot character understanding
task that requires models to mimic humans' ability of fast digesting characters
with a few starting scenes in a new movie.
  We propose a novel ToM prompting approach designed to explicitly assess the
influence of multiple ToM dimensions. It surpasses existing baseline models,
underscoring the significance of modeling multiple ToM dimensions for our task.
Our extensive human study verifies that humans are capable of solving our
problem by inferring characters' mental states based on their previously seen
movies. In comparison, our systems based on either state-of-the-art large
language models (GPT-4) or meta-learning algorithms lags >20% behind,
highlighting a notable limitation in existing approaches' ToM capabilities.",None,-1
ProsocialDialog: A Prosocial Backbone for Conversational Agents,0.521271,"Most existing dialogue systems fail to respond properly to potentially unsafe
user utterances by either ignoring or passively agreeing with them. To address
this issue, we introduce ProsocialDialog, the first large-scale multi-turn
dialogue dataset to teach conversational agents to respond to problematic
content following social norms. Covering diverse unethical, problematic,
biased, and toxic situations, ProsocialDialog contains responses that encourage
prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb,
RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists
of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue
safety labels accompanied by free-form rationales.
  With this dataset, we introduce a dialogue safety detection module, Canary,
capable of generating RoTs given conversational context, and a
socially-informed dialogue agent, Prost. Empirical results show that Prost
generates more socially acceptable dialogues compared to other state-of-the-art
language and dialogue models in both in-domain and out-of-domain settings.
Additionally, Canary effectively guides conversational agents and off-the-shelf
language models to generate significantly more prosocial responses. Our work
highlights the promise and importance of creating and steering conversational
AI to be socially responsible.",https://hyunw.kim/prosocial-dialog,-1
Variable Bitrate Neural Fields,0.68203,"Neural approximations of scalar and vector fields, such as signed distance
functions and radiance fields, have emerged as accurate, high-quality
representations. State-of-the-art results are obtained by conditioning a neural
approximation with a lookup from trainable feature grids that take on part of
the learning task and allow for smaller, more efficient neural networks.
Unfortunately, these feature grids usually come at the cost of significantly
increased memory consumption compared to stand-alone neural network models. We
present a dictionary method for compressing such feature grids, reducing their
memory consumption by up to 100x and permitting a multiresolution
representation which can be useful for out-of-core streaming. We formulate the
dictionary optimization as a vector-quantized auto-decoder problem which lets
us learn end-to-end discrete neural representations in a space where no direct
supervision is available and with dynamic topology and structure. Our source
code will be available at https://github.com/nv-tlabs/vqad.",https://github.com/nv-tlabs/nglod,-1
Can language models handle recursively nested grammatical structures? A case study on comparing models and humans,0.258604,"How should we compare the capabilities of language models (LMs) and humans? I
draw inspiration from comparative psychology to highlight some challenges. In
particular, I consider a case study: processing of recursively nested
grammatical structures. Prior work suggests that LMs cannot handle these
structures as reliably as humans can. However, the humans were provided with
instructions and training, while the LMs were evaluated zero-shot. I therefore
match the evaluation more closely. Providing large LMs with a simple prompt --
substantially less content than the human training -- allows the LMs to
consistently outperform the human results, and even to extrapolate to more
deeply nested conditions than were tested with humans. Further, reanalyzing the
prior human data suggests that the humans may not perform above chance at the
difficult structures initially. Thus, large LMs may indeed process recursively
nested grammatical structures as reliably as humans. This case study highlights
how discrepancies in the evaluation can confound comparisons of language models
and humans. I therefore reflect on the broader challenge of comparing human and
model capabilities, and highlight an important difference between evaluating
cognitive models and foundation models.",https://github.com/google/BIG-bench/,-1
E-NER -- An Annotated Named Entity Recognition Corpus of Legal Text,0.10811,"Identifying named entities such as a person, location or organization, in
documents can highlight key information to readers. Training Named Entity
Recognition (NER) models requires an annotated data set, which can be a
time-consuming labour-intensive task. Nevertheless, there are publicly
available NER data sets for general English. Recently there has been interest
in developing NER for legal text. However, prior work and experimental results
reported here indicate that there is a significant degradation in performance
when NER methods trained on a general English data set are applied to legal
text. We describe a publicly available legal NER data set, called E-NER, based
on legal company filings available from the US Securities and Exchange
Commission's EDGAR data set. Training a number of different NER algorithms on
the general English CoNLL-2003 corpus but testing on our test collection
confirmed significant degradations in accuracy, as measured by the F1-score, of
between 29.4\% and 60.4\%, compared to training and testing on the E-NER
collection.",https://github.com/terenceau2/E-NER-Dataset,-1
polyBERT: A chemical language model to enable fully machine-driven ultrafast polymer informatics,0.202677,"Polymers are a vital part of everyday life. Their chemical universe is so
large that it presents unprecedented opportunities as well as significant
challenges to identify suitable application-specific candidates. We present a
complete end-to-end machine-driven polymer informatics pipeline that can search
this space for suitable candidates at unprecedented speed and accuracy. This
pipeline includes a polymer chemical fingerprinting capability called polyBERT
(inspired by Natural Language Processing concepts), and a multitask learning
approach that maps the polyBERT fingerprints to a host of properties. polyBERT
is a chemical linguist that treats the chemical structure of polymers as a
chemical language. The present approach outstrips the best presently available
concepts for polymer property prediction based on handcrafted fingerprint
schemes in speed by two orders of magnitude while preserving accuracy, thus
making it a strong candidate for deployment in scalable architectures including
cloud infrastructures.",https://github.com/Ramprasad-Group/polyBERT,-1
Multimodal Token Fusion for Vision Transformers,0.589839,"Many adaptations of transformers have emerged to address the single-modal
vision tasks, where self-attention modules are stacked to handle input sources
like images. Intuitively, feeding multiple modalities of data to vision
transformers could improve the performance, yet the inner-modal attentive
weights may also be diluted, which could thus undermine the final performance.
In this paper, we propose a multimodal token fusion method (TokenFusion),
tailored for transformer-based vision tasks. To effectively fuse multiple
modalities, TokenFusion dynamically detects uninformative tokens and
substitutes these tokens with projected and aggregated inter-modal features.
Residual positional alignment is also adopted to enable explicit utilization of
the inter-modal alignments after fusion. The design of TokenFusion allows the
transformer to learn correlations among multimodal features, while the
single-modal transformer architecture remains largely intact. Extensive
experiments are conducted on a variety of homogeneous and heterogeneous
modalities and demonstrate that TokenFusion surpasses state-of-the-art methods
in three typical vision tasks: multimodal image-to-image translation, RGB-depth
semantic segmentation, and 3D object detection with point cloud and images. Our
code is available at https://github.com/yikaiw/TokenFusion.",https://github.com/yikaiw/TokenFusion,-1
Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure,0.0323173,"Generative models have demonstrated impressive results on Aspect-based
Sentiment Analysis (ABSA) tasks, particularly for the emerging task of
extracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these
models struggle with implicit sentiment expressions, which are commonly
observed in opinionated content such as online reviews. In this work, we
introduce GEN-SCL-NAT, which consists of two techniques for improved structured
generation for ACOS quadruple extraction. First, we propose GEN-SCL, a
supervised contrastive learning objective that aids quadruple prediction by
encouraging the model to produce input representations that are discriminable
across key input attributes, such as sentiment polarity and the existence of
implicit opinions and aspects. Second, we introduce GEN-NAT, a new structured
generation format that better adapts autoregressive encoder-decoder models to
extract quadruples in a generative fashion. Experimental results show that
GEN-SCL-NAT achieves top performance across three ACOS datasets, averaging
1.48% F1 improvement, with a maximum 1.73% increase on the LAPTOP-L1 dataset.
Additionally, we see significant gains on implicit aspect and opinion splits
that have been shown as challenging for existing ACOS approaches.",https://github.com/jpeper/GEN_SCL_NAT,-1
Pointillism: Accurate 3D bounding box estimation with multi-radars,0.50552,"Autonomous perception requires high-quality environment sensing in the form
of 3D bounding boxes of dynamic objects. The primary sensors used in automotive
systems are light-based cameras and LiDARs. However, they are known to fail in
adverse weather conditions. Radars can potentially solve this problem as they
are barely affected by adverse weather conditions. However, specular
reflections of wireless signals cause poor performance of radar point clouds.
We introduce Pointillism, a system that combines data from multiple spatially
separated radars with an optimal separation to mitigate these problems. We
introduce a novel concept of Cross Potential Point Clouds, which uses the
spatial diversity induced by multiple radars and solves the problem of noise
and sparsity in radar point clouds. Furthermore, we present the design of
RP-net, a novel deep learning architecture, designed explicitly for radar's
sparse data distribution, to enable accurate 3D bounding box estimation. The
spatial techniques designed and proposed in this paper are fundamental to
radars point cloud distribution and would benefit other radar sensing
applications.",None,-1
Online Continual Learning for Embedded Devices,0.396486,"Real-time on-device continual learning is needed for new applications such as
home robots, user personalization on smartphones, and augmented/virtual reality
headsets. However, this setting poses unique challenges: embedded devices have
limited memory and compute capacity and conventional machine learning models
suffer from catastrophic forgetting when updated on non-stationary data
streams. While several online continual learning models have been developed,
their effectiveness for embedded applications has not been rigorously studied.
In this paper, we first identify criteria that online continual learners must
meet to effectively perform real-time, on-device learning. We then study the
efficacy of several online continual learning methods when used with mobile
neural networks. We measure their performance, memory usage, compute
requirements, and ability to generalize to out-of-domain inputs.",https://github.com/tyler-hayes/Embedded-CL,-1
Perception Prioritized Training of Diffusion Models,0.616106,"Diffusion models learn to restore noisy data, which is corrupted with
different levels of noise, by optimizing the weighted sum of the corresponding
loss terms, i.e., denoising score matching loss. In this paper, we show that
restoring data corrupted with certain noise levels offers a proper pretext task
for the model to learn rich visual concepts. We propose to prioritize such
noise levels over other levels during training, by redesigning the weighting
scheme of the objective function. We show that our simple redesign of the
weighting scheme significantly improves the performance of diffusion models
regardless of the datasets, architectures, and sampling strategies.",https://github.com/jychoi118/P2-weighting,-1
Decoupling Makes Weakly Supervised Local Feature Better,0.459884,"Weakly supervised learning can help local feature methods to overcome the
obstacle of acquiring a large-scale dataset with densely labeled
correspondences. However, since weak supervision cannot distinguish the losses
caused by the detection and description steps, directly conducting weakly
supervised learning within a joint describe-then-detect pipeline suffers
limited performance. In this paper, we propose a decoupled describe-then-detect
pipeline tailored for weakly supervised local feature learning. Within our
pipeline, the detection step is decoupled from the description step and
postponed until discriminative and robust descriptors are learned. In addition,
we introduce a line-to-window search strategy to explicitly use the camera pose
information for better descriptor learning. Extensive experiments show that our
method, namely PoSFeat (Camera Pose Supervised Feature), outperforms previous
fully and weakly supervised methods and achieves state-of-the-art performance
on a wide range of downstream tasks.",https://github.com/The-Learning-And-Vision-Atelier-LAVA/PoSFeat,-1
Readability Controllable Biomedical Document Summarization,0.2999,"Different from general documents, it is recognised that the ease with which
people can understand a biomedical text is eminently varied, owing to the
highly technical nature of biomedical documents and the variance of readers'
domain knowledge. However, existing biomedical document summarization systems
have paid little attention to readability control, leaving users with summaries
that are incompatible with their levels of expertise. In recognition of this
urgent demand, we introduce a new task of readability controllable
summarization for biomedical documents, which aims to recognise users'
readability demands and generate summaries that better suit their needs:
technical summaries for experts and plain language summaries (PLS) for laymen.
To establish this task, we construct a corpus consisting of biomedical papers
with technical summaries and PLSs written by the authors, and benchmark
multiple advanced controllable abstractive and extractive summarization models
based on pre-trained language models (PLMs) with prevalent controlling and
generation techniques. Moreover, we propose a novel masked language model (MLM)
based metric and its variant to effectively evaluate the readability
discrepancy between lay and technical summaries. Experimental results from
automated and human evaluations show that though current control techniques
allow for a certain degree of readability adjustment during generation, the
performance of existing controllable summarization methods is far from
desirable in this task.",None,-1
PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices,0.551371,"Neural radiance-density field methods have become increasingly popular for
the task of novel-view rendering. Their recent extension to hash-based
positional encoding ensures fast training and inference with visually pleasing
results. However, density-based methods struggle with recovering accurate
surface geometry. Hybrid methods alleviate this issue by optimizing the density
based on an underlying SDF. However, current SDF methods are overly smooth and
miss fine geometric details. In this work, we combine the strengths of these
two lines of work in a novel hash-based implicit surface representation. We
propose improvements to the two areas by replacing the voxel hash encoding with
a permutohedral lattice which optimizes faster, especially for higher
dimensions. We additionally propose a regularization scheme which is crucial
for recovering high-frequency geometric detail. We evaluate our method on
multiple datasets and show that we can recover geometric detail at the level of
pores and wrinkles while using only RGB images for supervision. Furthermore,
using sphere tracing we can render novel views at 30 fps on an RTX 3090. Code
is publicly available at: https://radualexandru.github.io/permuto_sdf",https://radualexandru.github.io/permuto_sdf,-1
Generative Cooperative Learning for Unsupervised Video Anomaly Detection,0.504041,"Video anomaly detection is well investigated in weakly-supervised and
one-class classification (OCC) settings. However, unsupervised video anomaly
detection methods are quite sparse, likely because anomalies are less frequent
in occurrence and usually not well-defined, which when coupled with the absence
of ground truth supervision, could adversely affect the performance of the
learning algorithms. This problem is challenging yet rewarding as it can
completely eradicate the costs of obtaining laborious annotations and enable
such systems to be deployed without human intervention. To this end, we propose
a novel unsupervised Generative Cooperative Learning (GCL) approach for video
anomaly detection that exploits the low frequency of anomalies towards building
a cross-supervision between a generator and a discriminator. In essence, both
networks get trained in a cooperative fashion, thereby allowing unsupervised
learning. We conduct extensive experiments on two large-scale video anomaly
detection datasets, UCF crime, and ShanghaiTech. Consistent improvement over
the existing state-of-the-art unsupervised and OCC methods corroborate the
effectiveness of our approach.",None,-1
Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning for Chinese Spell Checking,0.485456,"Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling
errors. Recent researches start from the pretrained knowledge of language
models and take multimodal information into CSC models to improve the
performance. However, they overlook the rich knowledge in the dictionary, the
reference book where one can learn how one character should be pronounced,
written, and used. In this paper, we propose the LEAD framework, which renders
the CSC model to learn heterogeneous knowledge from the dictionary in terms of
phonetics, vision, and meaning. LEAD first constructs positive and negative
samples according to the knowledge of character phonetics, glyphs, and
definitions in the dictionary. Then a unified contrastive learning-based
training scheme is employed to refine the representations of the CSC models.
Extensive experiments and detailed analyses on the SIGHAN benchmark datasets
demonstrate the effectiveness of our proposed methods.",https://github.com/geekjuruo/LEAD,-1
SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations,0.69777,"We present SpeechMatrix, a large-scale multilingual corpus of
speech-to-speech translations mined from real speech of European Parliament
recordings. It contains speech alignments in 136 language pairs with a total of
418 thousand hours of speech. To evaluate the quality of this parallel speech,
we train bilingual speech-to-speech translation models on mined data only and
establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test
sets. Enabled by the multilinguality of SpeechMatrix, we also explore
multilingual speech-to-speech translation, a topic which was addressed by few
other works. We also demonstrate that model pre-training and sparse scaling
using Mixture-of-Experts bring large gains to translation performance. The
mined data and models are freely available.",https://github.com/facebookresearch/fairseq/tree/ust/examples/speech_matrix,-1
On the Use of BERT for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation,0.787022,"In recent years, pre-trained models have become dominant in most natural
language processing (NLP) tasks. However, in the area of Automated Essay
Scoring (AES), pre-trained models such as BERT have not been properly used to
outperform other deep learning models such as LSTM. In this paper, we introduce
a novel multi-scale essay representation for BERT that can be jointly learned.
We also employ multiple losses and transfer learning from out-of-domain essays
to further improve the performance. Experiment results show that our approach
derives much benefit from joint learning of multi-scale essay representation
and obtains almost the state-of-the-art result among all deep learning models
in the ASAP task. Our multi-scale essay representation also generalizes well to
CommonLit Readability Prize data set, which suggests that the novel text
representation proposed in this paper may be a new and effective choice for
long-text tasks.",https://github.com/lingochamp/Multi-Scale-BERT-AES,-1
DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation,0.0849768,"Dialog response generation in open domain is an important research topic
where the main challenge is to generate relevant and diverse responses. In this
paper, we propose a new dialog pre-training framework called DialogVED, which
introduces continuous latent variables into the enhanced encoder-decoder
pre-training framework to increase the relevance and diversity of responses.
With the help of a large dialog corpus (Reddit), we pre-train the model using
the following 4 tasks adopted in language models (LMs) and variational
autoencoders (VAEs): 1) masked language model; 2) response generation; 3)
bag-of-words prediction; and 4) KL divergence reduction. We also add additional
parameters to model the turn structure in dialogs to improve the performance of
the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and
DSTC7-AVSD benchmarks for response generation. Experimental results show that
our model achieves the new state-of-the-art results on all these datasets.",None,-1
CoMER: Modeling Coverage for Transformer-based Handwritten Mathematical Expression Recognition,0.0,"The Transformer-based encoder-decoder architecture has recently made
significant advances in recognizing handwritten mathematical expressions.
However, the transformer model still suffers from the lack of coverage problem,
making its expression recognition rate (ExpRate) inferior to its RNN
counterpart. Coverage information, which records the alignment information of
the past steps, has proven effective in the RNN models. In this paper, we
propose CoMER, a model that adopts the coverage information in the transformer
decoder. Specifically, we propose a novel Attention Refinement Module (ARM) to
refine the attention weights with past alignment information without hurting
its parallelism. Furthermore, we take coverage information to the extreme by
proposing self-coverage and cross-coverage, which utilize the past alignment
information from the current and previous layers. Experiments show that CoMER
improves the ExpRate by 0.61%/2.09%/1.59% compared to the current
state-of-the-art model, and reaches 59.33%/59.81%/62.97% on the CROHME
2014/2016/2019 test sets.",https://github.com/Green-Wood/CoMER,-1
Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling,0.501251,"Existing research generally treats Chinese character as a minimum unit for
representation. However, such Chinese character representation will suffer two
bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich
internal features (e.g., radicals and strokes); and 2) Parameter bottleneck,
each individual character has to be represented by a unique vector. In this
paper, we introduce a novel representation method for Chinese characters to
break the bottlenecks, namely StrokeNet, which represents a Chinese character
by a Latinized stroke sequence (e.g., ""ao1 (concave)"" to ""ajaie"" and ""tu1
(convex)"" to ""aeaqe""). Specifically, StrokeNet maps each stroke to a specific
Latin character, thus allowing similar Chinese characters to have similar Latin
representations. With the introduction of StrokeNet to neural machine
translation (NMT), many powerful but not applicable techniques to non-Latin
languages (e.g., shared subword vocabulary learning and ciphertext-based data
augmentation) can now be perfectly implemented. Experiments on the widely-used
NIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT
tasks show that StrokeNet can provide a significant performance boost over the
strong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17
Chinese-English task which is better than any previously reported results
without using monolingual data. Code and scripts are freely available at
https://github.com/zjwang21/StrokeNet.",https://github.com/zjwang21/StrokeNet,-1
Towards Textual Out-of-Domain Detection without In-Domain Labels,0.133314,"In many real-world settings, machine learning models need to identify user
inputs that are out-of-domain (OOD) so as to avoid performing wrong actions.
This work focuses on a challenging case of OOD detection, where no labels for
in-domain data are accessible (e.g., no intent labels for the intent
classification task). To this end, we first evaluate different language model
based approaches that predict likelihood for a sequence of tokens. Furthermore,
we propose a novel representation learning based method by combining
unsupervised clustering and contrastive learning so that better data
representations for OOD detection can be learned. Through extensive
experiments, we demonstrate that this method can significantly outperform
likelihood-based methods and can be even competitive to the state-of-the-art
supervised approaches with label information.",None,-1
NAFSSR: Stereo Image Super-Resolution Using NAFNet,0.242941,"Stereo image super-resolution aims at enhancing the quality of
super-resolution results by utilizing the complementary information provided by
binocular systems. To obtain reasonable performance, most methods focus on
finely designing modules, loss functions, and etc. to exploit information from
another viewpoint. This has the side effect of increasing system complexity,
making it difficult for researchers to evaluate new ideas and compare methods.
This paper inherits a strong and simple image restoration model, NAFNet, for
single-view feature extraction and extends it by adding cross attention modules
to fuse features between views to adapt to binocular scenarios. The proposed
baseline for stereo image super-resolution is noted as NAFSSR. Furthermore,
training/testing strategies are proposed to fully exploit the performance of
NAFSSR. Extensive experiments demonstrate the effectiveness of our method. In
particular, NAFSSR outperforms the state-of-the-art methods on the KITTI 2012,
KITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place
in the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models
will be released at https://github.com/megvii-research/NAFNet.",https://github.com/megvii-research/NAFNet,-1
SimVP: Simpler yet Better Video Prediction,0.422232,"From CNN, RNN, to ViT, we have witnessed remarkable advancements in video
prediction, incorporating auxiliary inputs, elaborate neural architectures, and
sophisticated training strategies. We admire these progresses but are confused
about the necessity: is there a simple method that can perform comparably well?
This paper proposes SimVP, a simple video prediction model that is completely
built upon CNN and trained by MSE loss in an end-to-end fashion. Without
introducing any additional tricks and complicated strategies, we can achieve
state-of-the-art performance on five benchmark datasets. Through extended
experiments, we demonstrate that SimVP has strong generalization and
extensibility on real-world datasets. The significant reduction of training
cost makes it easier to scale to complex scenarios. We believe SimVP can serve
as a solid baseline to stimulate the further development of video prediction.
The code is available at
\href{https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction}{Github}.",None,-1
PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models,0.864665,"Generalizable 3D part segmentation is important but challenging in vision and
robotics. Training deep models via conventional supervised methods requires
large-scale 3D datasets with fine-grained part annotations, which are costly to
collect. This paper explores an alternative way for low-shot part segmentation
of 3D point clouds by leveraging a pretrained image-language model, GLIP, which
achieves superior performance on open-vocabulary 2D detection. We transfer the
rich knowledge from 2D to 3D through GLIP-based part detection on point cloud
rendering and a novel 2D-to-3D label lifting algorithm. We also utilize
multi-view 3D priors and few-shot prompt tuning to boost performance
significantly. Extensive evaluation on PartNet and PartNet-Mobility datasets
shows that our method enables excellent zero-shot 3D part segmentation. Our
few-shot version not only outperforms existing few-shot approaches by a large
margin but also achieves highly competitive results compared to the fully
supervised counterpart. Furthermore, we demonstrate that our method can be
directly applied to iPhone-scanned point clouds without significant domain
gaps.",None,-1
Evaluating Human-Language Model Interaction,0.499323,"Many real-world applications of language models (LMs), such as writing
assistance and code autocomplete, involve human-LM interaction. However, most
benchmarks are non-interactive in that a model produces output without human
involvement. To evaluate human-LM interaction, we develop a new framework,
Human-AI Language-based Interaction Evaluation (HALIE), that defines the
components of interactive systems and dimensions to consider when designing
evaluation metrics. Compared to standard, non-interactive evaluation, HALIE
captures (i) the interactive process, not only the final output; (ii) the
first-person subjective experience, not just a third-party assessment; and
(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We
then design five tasks to cover different forms of interaction: social
dialogue, question answering, crossword puzzles, summarization, and metaphor
generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3
and AI21 Labs' Jurassic-1), we find that better non-interactive performance
does not always translate to better human-LM interaction. In particular, we
highlight three cases where the results from non-interactive and interactive
metrics diverge and underscore the importance of human-LM interaction for LM
evaluation.",https://github.com/stanford-crfm/halie,-1
Fast Event-based Optical Flow Estimation by Triplet Matching,0.0934411,"Event cameras are novel bio-inspired sensors that offer advantages over
traditional cameras (low latency, high dynamic range, low power, etc.). Optical
flow estimation methods that work on packets of events trade off speed for
accuracy, while event-by-event (incremental) methods have strong assumptions
and have not been tested on common benchmarks that quantify progress in the
field. Towards applications on resource-constrained devices, it is important to
develop optical flow algorithms that are fast, light-weight and accurate. This
work leverages insights from neuroscience, and proposes a novel optical flow
estimation scheme based on triplet matching. The experiments on publicly
available benchmarks demonstrate its capability to handle complex scenes with
comparable results as prior packet-based algorithms. In addition, the proposed
method achieves the fastest execution time (> 10 kHz) on standard CPUs as it
requires only three events in estimation. We hope that our research opens the
door to real-time, incremental motion estimation methods and applications in
real-world scenarios.",None,-1
"The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink",0.862472,"Machine Learning (ML) workloads have rapidly grown in importance, but raised
concerns about their carbon footprint. Four best practices can reduce ML
training energy by up to 100x and CO2 emissions up to 1000x. By following best
practices, overall ML energy use (across research, development, and production)
held steady at <15% of Google's total energy use for the past three years. If
the whole ML field were to adopt best practices, total carbon emissions from
training would reduce. Hence, we recommend that ML papers include emissions
explicitly to foster competition on more than just model quality. Estimates of
emissions in papers that omitted them have been off 100x-100,000x, so
publishing emissions has the added benefit of ensuring accurate accounting.
Given the importance of climate change, we must get the numbers right to make
certain that we work on its biggest challenges.",None,-1
Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning,0.461305,"We introduce Patch Aligned Contrastive Learning (PACL), a modified
compatibility function for CLIP's contrastive loss, intending to train an
alignment between the patch tokens of the vision encoder and the CLS token of
the text encoder. With such an alignment, a model can identify regions of an
image corresponding to a given text input, and therefore transfer seamlessly to
the task of open vocabulary semantic segmentation without requiring any
segmentation annotations during training. Using pre-trained CLIP encoders with
PACL, we are able to set the state-of-the-art on the task of open vocabulary
zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC,
Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also
applicable to image-level predictions and when used with a CLIP backbone,
provides a general improvement in zero-shot classification accuracy compared to
CLIP, across a suite of 12 image classification datasets.",None,-1
High-resolution Face Swapping via Latent Semantics Disentanglement,0.42671,"We present a novel high-resolution face swapping method using the inherent
prior knowledge of a pre-trained GAN model. Although previous research can
leverage generative priors to produce high-resolution results, their quality
can suffer from the entangled semantics of the latent space. We explicitly
disentangle the latent semantics by utilizing the progressive nature of the
generator, deriving structure attributes from the shallow layers and appearance
attributes from the deeper ones. Identity and pose information within the
structure attributes are further separated by introducing a landmark-driven
structure transfer latent direction. The disentangled latent code produces rich
generative features that incorporate feature blending to produce a plausible
swapping result. We further extend our method to video face swapping by
enforcing two spatio-temporal constraints on the latent space and the image
space. Extensive experiments demonstrate that the proposed method outperforms
state-of-the-art image/video face swapping methods in terms of hallucination
quality and consistency. Code can be found at:
https://github.com/cnnlstm/FSLSD_HiRes.",https://github.com/cnnlstm/FSLSD_HiRes,-1
HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising,0.162413,"The paper presents a novel approach for vector-floorplan generation via a
diffusion model, which denoises 2D coordinates of room/door corners with two
inference objectives: 1) a single-step noise as the continuous quantity to
precisely invert the continuous forward process; and 2) the final 2D coordinate
as the discrete quantity to establish geometric incident relationships such as
parallelism, orthogonality, and corner-sharing. Our task is graph-conditioned
floorplan generation, a common workflow in floorplan design. We represent a
floorplan as 1D polygonal loops, each of which corresponds to a room or a door.
Our diffusion model employs a Transformer architecture at the core, which
controls the attention masks based on the input graph-constraint and directly
generates vector-graphics floorplans via a discrete and continuous denoising
process. We have evaluated our approach on RPLAN dataset. The proposed approach
makes significant improvements in all the metrics against the state-of-the-art
with significant margins, while being capable of generating non-Manhattan
structures and controlling the exact number of corners per room. A project
website with supplementary video and document is here
https://aminshabani.github.io/housediffusion.",https://aminshabani.github.io/housediffusion,-1
Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models,0.343467,"Recent open-domain dialogue models have brought numerous breakthroughs.
However, building a chat system is not scalable since it often requires a
considerable volume of human-human dialogue data, especially when enforcing
features such as persona, style, or safety. In this work, we study the
challenge of imposing roles on open-domain dialogue systems, with the goal of
making the systems maintain consistent roles while conversing naturally with
humans. To accomplish this, the system must satisfy a role specification that
includes certain conditions on the stated features as well as a system policy
on whether or not certain types of utterances are allowed. For this, we propose
an efficient data collection framework leveraging in-context few-shot learning
of large-scale language models for building role-satisfying dialogue dataset
from scratch. We then compare various architectures for open-domain dialogue
systems in terms of meeting role specifications while maintaining
conversational abilities. Automatic and human evaluations show that our models
return few out-of-bounds utterances, keeping competitive performance on general
metrics. We release a Korean dialogue dataset we built for further research.",https://github.com/naver-ai/carecall-corpus,-1
A Graph Neural Network Approach to Automated Model Building in Cryo-EM Maps,0.579403,"Electron cryo-microscopy (cryo-EM) produces three-dimensional (3D) maps of
the electrostatic potential of biological macromolecules, including proteins.
Along with knowledge about the imaged molecules, cryo-EM maps allow de novo
atomic modelling, which is typically done through a laborious manual process.
Taking inspiration from recent advances in machine learning applications to
protein structure prediction, we propose a graph neural network (GNN) approach
for automated model building of proteins in cryo-EM maps. The GNN acts on a
graph with nodes assigned to individual amino acids and edges representing the
protein chain. Combining information from the voxel-based cryo-EM data, the
amino acid sequence data and prior knowledge about protein geometries, the GNN
refines the geometry of the protein chain and classifies the amino acids for
each of its nodes. Application to 28 test cases shows that our approach
outperforms the state-of-the-art and approximates manual building for cryo-EM
maps with resolutions better than 3.5 \r{A}.",https://github.com/3dem/model-angelo,-1
Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,0.663112,"Despite the success, the process of fine-tuning large-scale PLMs brings
prohibitive adaptation costs. In fact, fine-tuning all the parameters of a
colossal model and retaining separate instances for different tasks are
practically infeasible. This necessitates a new branch of research focusing on
the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this
paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes
a small portion of the model parameters while keeping the rest untouched,
largely reducing both the computation and storage costs. Recent studies have
demonstrated that a series of delta tuning methods with distinct tuned
parameter selection could achieve performance on a par with full-parameter
fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In
this paper, we first formally describe the problem of delta tuning and then
comprehensively review recent delta tuning approaches. We also propose a
unified categorization criterion that divide existing delta tuning methods into
three groups: addition-based, specification-based, and reparameterization-based
methods. Though initially proposed as an efficient method to steer large
models, we believe that some of the fascinating evidence discovered along with
delta tuning could help further reveal the mechanisms of PLMs and even deep
neural networks. To this end, we discuss the theoretical principles underlying
the effectiveness of delta tuning and propose frameworks to interpret delta
tuning from the perspective of optimization and optimal control, respectively.
Furthermore, we provide a holistic empirical study of representative methods,
where results on over 100 NLP tasks demonstrate a comprehensive performance
comparison of different approaches. The experimental results also cover the
analysis of combinatorial, scaling and transferable properties of delta tuning.",None,-1
"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",0.841303,"Pretrained general-purpose language models can achieve state-of-the-art
accuracies in various natural language processing domains by adapting to
downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of
their success, the size of these models has increased rapidly, requiring
high-performance hardware, software, and algorithmic techniques to enable
training such large models. As the result of a joint effort between Microsoft
and NVIDIA, we present details on the training of the largest monolithic
transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530
billion parameters. In this paper, we first focus on the infrastructure as well
as the 3D parallelism methodology used to train this model using DeepSpeed and
Megatron. Next, we detail the training process, the design of our training
corpus, and our data curation techniques, which we believe is a key ingredient
to the success of the model. Finally, we discuss various evaluation results, as
well as other interesting observations and new properties exhibited by MT-NLG.
We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning
accuracies on several NLP benchmarks and establishes new state-of-the-art
results. We believe that our contributions will help further the development of
large-scale training infrastructures, large-scale language models, and natural
language generations.",https://github.com/leogao2/commoncrawl_downloader,-1
Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness,0.488539,"A notable challenge in Multi-Document Summarization (MDS) is the
extremely-long length of the input. In this paper, we present an
extract-then-abstract Transformer framework to overcome the problem.
Specifically, we leverage pre-trained language models to construct a
hierarchical extractor for salient sentence selection across documents and an
abstractor for rewriting the selected contents as summaries. However, learning
such a framework is challenging since the optimal contents for the abstractor
are generally unknown. Previous works typically create pseudo extraction oracle
to enable the supervised learning for both the extractor and the abstractor.
Nevertheless, we argue that the performance of such methods could be restricted
due to the insufficient information for prediction and inconsistent objectives
between training and testing. To this end, we propose a loss weighting
mechanism that makes the model aware of the unequal importance for the
sentences not in the pseudo extraction oracle, and leverage the fine-tuned
abstractor to generate summary references as auxiliary signals for learning the
extractor. Moreover, we propose a reinforcement learning method that can
efficiently apply to the extractor for harmonizing the optimization between
training and testing. Experiment results show that our framework substantially
outperforms strong baselines with comparable model sizes and achieves the best
results on the Multi-News, Multi-XScience, and WikiCatSum corpora.",None,-1
PromptBERT: Improving BERT Sentence Embeddings with Prompts,0.624511,"We propose PromptBERT, a novel contrastive learning method for learning
better sentence representation. We firstly analyze the drawback of current
sentence embedding from original BERT and find that it is mainly due to the
static token embedding bias and ineffective BERT layers. Then we propose the
first prompt-based sentence embeddings method and discuss two prompt
representing methods and three prompt searching methods to make BERT achieve
better sentence embeddings. Moreover, we propose a novel unsupervised training
objective by the technology of template denoising, which substantially shortens
the performance gap between the supervised and unsupervised settings. Extensive
experiments show the effectiveness of our method. Compared to SimCSE,
PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and
RoBERTa in the unsupervised setting.",https://github.com/kongds/Prompt-BERT,-1
Submodularity In Machine Learning and Artificial Intelligence,0.239109,"In this manuscript, we offer a gentle review of submodularity and
supermodularity and their properties. We offer a plethora of submodular
definitions; a full description of a number of example submodular functions and
their generalizations; example discrete constraints; a discussion of basic
algorithms for maximization, minimization, and other operations; a brief
overview of continuous submodular extensions; and some historical applications.
We then turn to how submodularity is useful in machine learning and artificial
intelligence. This includes summarization, and we offer a complete account of
the differences between and commonalities amongst sketching, coresets,
extractive and abstractive summarization in NLP, data distillation and
condensation, and data subset selection and feature selection. We discuss a
variety of ways to produce a submodular function useful for machine learning,
including heuristic hand-crafting, learning or approximately learning a
submodular function or aspects thereof, and some advantages of the use of a
submodular function as a coreset producer. We discuss submodular combinatorial
information functions, and how submodularity is useful for clustering, data
partitioning, parallel machine learning, active and semi-supervised learning,
probabilistic modeling, and structured norms and loss functions.",None,-1
REGTR: End-to-end Point Cloud Correspondences with Transformers,0.79298,"Despite recent success in incorporating learning into point cloud
registration, many works focus on learning feature descriptors and continue to
rely on nearest-neighbor feature matching and outlier filtering through RANSAC
to obtain the final set of correspondences for pose estimation. In this work,
we conjecture that attention mechanisms can replace the role of explicit
feature matching and RANSAC, and thus propose an end-to-end framework to
directly predict the final set of correspondences. We use a network
architecture consisting primarily of transformer layers containing self and
cross attentions, and train it to predict the probability each point lies in
the overlapping region and its corresponding position in the other point cloud.
The required rigid transformation can then be estimated directly from the
predicted correspondences without further post-processing. Despite its
simplicity, our approach achieves state-of-the-art performance on 3DMatch and
ModelNet benchmarks. Our source code can be found at
https://github.com/yewzijian/RegTR .",https://github.com/yewzijian/RegTR,-1
On the Effects of Image Quality Degradation on Minutiae- and Ridge-Based Automatic Fingerprint Recognition,0.398929,"The effect of image quality degradation on the verification performance of
automatic fingerprint recognition is investigated. We study the performance of
two fingerprint matchers based on minutiae and ridge information under varying
fingerprint image quality. The ridge-based system is found to be more robust to
image quality degradation than the minutiae-based system for a number of
different image quality criteria.",None,-1
Estimating Soft Labels for Out-of-Domain Intent Detection,0.113667,"Out-of-Domain (OOD) intent detection is important for practical dialog
systems. To alleviate the issue of lacking OOD training samples, some works
propose synthesizing pseudo OOD samples and directly assigning one-hot OOD
labels to these pseudo samples. However, these one-hot labels introduce noises
to the training process because some hard pseudo OOD samples may coincide with
In-Domain (IND) intents. In this paper, we propose an adaptive soft pseudo
labeling (ASoul) method that can estimate soft labels for pseudo OOD samples
when training OOD detectors. Semantic connections between pseudo OOD samples
and IND intents are captured using an embedding graph. A co-training framework
is further introduced to produce resulting soft labels following the smoothness
assumption, i.e., close samples are likely to have similar labels. Extensive
experiments on three benchmark datasets show that ASoul consistently improves
the OOD detection performance and outperforms various competitive baselines.",None,-1
Surgical Fine-Tuning Improves Adaptation to Distribution Shifts,0.243894,"A common approach to transfer learning under distribution shift is to
fine-tune the last few layers of a pre-trained model, preserving learned
features while also adapting to the new task. This paper shows that in such
settings, selectively fine-tuning a subset of layers (which we term surgical
fine-tuning) matches or outperforms commonly used fine-tuning approaches.
Moreover, the type of distribution shift influences which subset is more
effective to tune: for example, for image corruptions, fine-tuning only the
first few layers works best. We validate our findings systematically across
seven real-world data tasks spanning three types of distribution shifts.
Theoretically, we prove that for two-layer neural networks in an idealized
setting, first-layer tuning can outperform fine-tuning all layers. Intuitively,
fine-tuning more parameters on a small target dataset can cause information
learned during pre-training to be forgotten, and the relevant information
depends on the type of shift.",None,-1
Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning,0.497836,"Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks for both high-resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is \textit{language
adaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual
texts of a language using the pre-training objective. However, adapting to a
target language individually takes a large disk space and limits the
cross-lingual transfer abilities of the resulting models because they have been
specialized for a single language. In this paper, we perform
\textit{multilingual adaptive fine-tuning} on 17 most-resourced African
languages and three other high-resource languages widely spoken on the African
continent to encourage cross-lingual transfer learning. To further specialize
the multilingual PLM, we removed vocabulary tokens from the embedding layer
that corresponds to non-African writing scripts before MAFT, thus reducing the
model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa
and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space.
Additionally, we show that our adapted PLM also improves the zero-shot
cross-lingual transfer abilities of parameter efficient fine-tuning methods.",None,-1
PAL: Persona-Augmented Emotional Support Conversation Generation,0.154377,"Due to the lack of human resources for mental health support, there is an
increasing demand for employing conversational agents for support. Recent work
has demonstrated the effectiveness of dialogue models in providing emotional
support. As previous studies have demonstrated that seekers' persona is an
important factor for effective support, we investigate whether there are
benefits to modeling such information in dialogue models for support. In this
paper, our empirical analysis verifies that persona has an important impact on
emotional support. Therefore, we propose a framework for dynamically inferring
and modeling seekers' persona. We first train a model for inferring the
seeker's persona from the conversation history. Accordingly, we propose PAL, a
model that leverages persona information and, in conjunction with our
strategy-based controllable generation method, provides personalized emotional
support. Automatic and manual evaluations demonstrate that PAL achieves
state-of-the-art results, outperforming the baselines on the studied benchmark.
Our code and data are publicly available at https://github.com/chengjl19/PAL.",https://github.com/chengjl19/PAL,-1
Exploring Plain Vision Transformer Backbones for Object Detection,0.994474,"We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone
network for object detection. This design enables the original ViT architecture
to be fine-tuned for object detection without needing to redesign a
hierarchical backbone for pre-training. With minimal adaptations for
fine-tuning, our plain-backbone detector can achieve competitive results.
Surprisingly, we observe: (i) it is sufficient to build a simple feature
pyramid from a single-scale feature map (without the common FPN design) and
(ii) it is sufficient to use window attention (without shifting) aided with
very few cross-window propagation blocks. With plain ViT backbones pre-trained
as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the
previous leading methods that were all based on hierarchical backbones,
reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K
pre-training. We hope our study will draw attention to research on
plain-backbone detectors. Code for ViTDet is available in Detectron2.",https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet,-1
SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views,0.922119,"We introduce SparseNeuS, a novel neural rendering based method for the task
of surface reconstruction from multi-view images. This task becomes more
difficult when only sparse images are provided as input, a scenario where
existing neural reconstruction approaches usually produce incomplete or
distorted results. Moreover, their inability of generalizing to unseen new
scenes impedes their application in practice. Contrarily, SparseNeuS can
generalize to new scenes and work well with sparse images (as few as 2 or 3).
SparseNeuS adopts signed distance function (SDF) as the surface representation,
and learns generalizable priors from image features by introducing geometry
encoding volumes for generic surface prediction. Moreover, several strategies
are introduced to effectively leverage sparse views for high-quality
reconstruction, including 1) a multi-level geometry reasoning framework to
recover the surfaces in a coarse-to-fine manner; 2) a multi-scale color
blending scheme for more reliable color prediction; 3) a consistency-aware
fine-tuning scheme to control the inconsistent regions caused by occlusion and
noise. Extensive experiments demonstrate that our approach not only outperforms
the state-of-the-art methods, but also exhibits good efficiency,
generalizability, and flexibility.",https://www.xxlong.site/SparseNeuSarXiv:2206.05737v2,-1
IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images,0.767944,"We propose a neural inverse rendering pipeline called IRON that operates on
photometric images and outputs high-quality 3D content in the format of
triangle meshes and material textures readily deployable in existing graphics
pipelines. Our method adopts neural representations for geometry as signed
distance fields (SDFs) and materials during optimization to enjoy their
flexibility and compactness, and features a hybrid optimization scheme for
neural SDFs: first, optimize using a volumetric radiance field approach to
recover correct topology, then optimize further using edgeaware physics-based
surface rendering for geometry refinement and disentanglement of materials and
lighting. In the second stage, we also draw inspiration from mesh-based
differentiable rendering, and design a novel edge sampling algorithm for neural
SDFs to further improve performance. We show that our IRON achieves
significantly better inverse rendering quality compared to prior works. Our
project page is here: https://kai-46.github.io/IRON-website/",None,-1
Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models,0.317428,"We present a novel way of conditioning a pretrained denoising diffusion
speech model to produce speech in the voice of a novel person unseen during
training. The method requires a short (~3 seconds) sample from the target
person, and generation is steered at inference time, without any training
steps. At the heart of the method lies a sampling process that combines the
estimation of the denoising model with a low-pass version of the new speaker's
sample. The objective and subjective evaluations show that our sampling method
can generate a voice similar to that of the target speaker in terms of
frequency, with an accuracy comparable to state-of-the-art methods, and without
training.",None,-1
Conditional Generation with a Question-Answering Blueprint,0.143715,"The ability to convey relevant and faithful information is critical for many
tasks in conditional generation and yet remains elusive for neural seq-to-seq
models whose outputs often reveal hallucinations and fail to correctly cover
important details. In this work, we advocate planning as a useful intermediate
representation for rendering conditional generation less opaque and more
grounded. Our work proposes a new conceptualization of text plans as a sequence
of question-answer (QA) pairs. We enhance existing datasets (e.g., for
summarization) with a QA blueprint operating as a proxy for both content
selection (i.e.,~what to say) and planning (i.e.,~in what order). We obtain
blueprints automatically by exploiting state-of-the-art question generation
technology and convert input-output pairs into input-blueprint-output tuples.
We develop Transformer-based models, each varying in how they incorporate the
blueprint in the generated output (e.g., as a global plan or iteratively).
Evaluation across metrics and datasets demonstrates that blueprint models are
more factual than alternatives which do not resort to planning and allow
tighter control of the generation output.",None,-1
Multi-modal Contrastive Representation Learning for Entity Alignment,0.522093,"Multi-modal entity alignment aims to identify equivalent entities between two
different multi-modal knowledge graphs, which consist of structural triples and
images associated with entities. Most previous works focus on how to utilize
and encode information from different modalities, while it is not trivial to
leverage multi-modal knowledge in entity alignment because of the modality
heterogeneity. In this paper, we propose MCLEA, a Multi-modal Contrastive
Learning based Entity Alignment model, to obtain effective joint
representations for multi-modal entity alignment. Different from previous
works, MCLEA considers task-oriented modality and models the inter-modal
relationships for each entity representation. In particular, MCLEA firstly
learns multiple individual representations from multiple modalities, and then
performs contrastive learning to jointly model intra-modal and inter-modal
interactions. Extensive experimental results show that MCLEA outperforms
state-of-the-art baselines on public datasets under both supervised and
unsupervised settings.",https://github.com/lzxlin/MCLEA,-1
Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models,0.661245,"Pre-trained vision-language models (e.g., CLIP) have shown promising
zero-shot generalization in many downstream tasks with properly designed text
prompts. Instead of relying on hand-engineered prompts, recent works learn
prompts using the training data from downstream tasks. While effective,
training on domain-specific data reduces a model's generalization capability to
unseen new domains. In this work, we propose test-time prompt tuning (TPT), a
method that can learn adaptive prompts on the fly with a single test sample.
For image classification, TPT optimizes the prompt by minimizing the entropy
with confidence selection so that the model has consistent predictions across
different augmented views of each test sample. In evaluating generalization to
natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP
by 3.6% on average, surpassing previous prompt tuning approaches that require
additional task-specific training data. In evaluating cross-dataset
generalization with unseen categories, TPT performs on par with the
state-of-the-art approaches that use additional training data. Project page:
https://azshue.github.io/TPT.",https://tinyurl.com/yr3zmhma,-1
Semantic Image Synthesis via Diffusion Models,0.490584,"Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable
success in various image generation tasks compared with Generative Adversarial
Nets (GANs). Recent work on semantic image synthesis mainly follows the
\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality
or diversity of generated images. In this paper, we propose a novel framework
based on DDPM for semantic image synthesis. Unlike previous conditional
diffusion model directly feeds the semantic layout and noisy image as input to
a U-Net structure, which may not fully leverage the information in the input
semantic mask, our framework processes semantic layout and noisy image
differently. It feeds noisy image to the encoder of the U-Net structure while
the semantic layout to the decoder by multi-layer spatially-adaptive
normalization operators. To further improve the generation quality and semantic
interpretability in semantic image synthesis, we introduce the classifier-free
guidance sampling strategy, which acknowledge the scores of an unconditional
model for sampling process. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our proposed method, achieving
state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS).",https://github.com/WeilunWang/semantic-diffusion-model,-1
Unifying Motion Deblurring and Frame Interpolation with Events,0.0547979,"Slow shutter speed and long exposure time of frame-based cameras often cause
visual blur and loss of inter-frame information, degenerating the overall
quality of captured videos. To this end, we present a unified framework of
event-based motion deblurring and frame interpolation for blurry video
enhancement, where the extremely low latency of events is leveraged to
alleviate motion blur and facilitate intermediate frame prediction.
Specifically, the mapping relation between blurry frames and sharp latent
images is first predicted by a learnable double integral network, and a fusion
network is then proposed to refine the coarse results via utilizing the
information from consecutive blurry inputs and the concurrent events. By
exploring the mutual constraints among blurry frames, latent images, and event
streams, we further propose a self-supervised learning framework to enable
network training with real-world blurry videos and events. Extensive
experiments demonstrate that our method compares favorably against the
state-of-the-art approaches and achieves remarkable performance on both
synthetic and real-world datasets.",https://github.com/XiangZ-0/EVDI,-1
Deep Learning in Business Analytics: A Clash of Expectations and Reality,0.182016,"Our fast-paced digital economy shaped by global competition requires
increased data-driven decision-making based on artificial intelligence (AI) and
machine learning (ML). The benefits of deep learning (DL) are manifold, but it
comes with limitations that have - so far - interfered with widespread industry
adoption. This paper explains why DL - despite its popularity - has
difficulties speeding up its adoption within business analytics. It is shown -
by a mixture of content analysis and empirical study - that the adoption of
deep learning is not only affected by computational complexity, lacking big
data architecture, lack of transparency (black-box), and skill shortage, but
also by the fact that DL does not outperform traditional ML models in the case
of structured datasets with fixed-length feature vectors. Deep learning should
be regarded as a powerful addition to the existing body of ML models instead of
a one size fits all solution.",None,-1
Real-time Full-stack Traffic Scene Perception for Autonomous Driving with Roadside Cameras,0.423898,"We propose a novel and pragmatic framework for traffic scene perception with
roadside cameras. The proposed framework covers a full-stack of roadside
perception pipeline for infrastructure-assisted autonomous driving, including
object detection, object localization, object tracking, and multi-camera
information fusion. Unlike previous vision-based perception frameworks rely
upon depth offset or 3D annotation at training, we adopt a modular decoupling
design and introduce a landmark-based 3D localization method, where the
detection and localization can be well decoupled so that the model can be
easily trained based on only 2D annotations. The proposed framework applies to
either optical or thermal cameras with pinhole or fish-eye lenses. Our
framework is deployed at a two-lane roundabout located at Ellsworth Rd. and
State St., Ann Arbor, MI, USA, providing 7x24 real-time traffic flow monitoring
and high-precision vehicle trajectory extraction. The whole system runs
efficiently on a low-power edge computing device with all-component end-to-end
delay of less than 20ms.",None,-1
A sequence-to-sequence approach for document-level relation extraction,0.441497,"Motivated by the fact that many relations cross the sentence boundary, there
has been increasing interest in document-level relation extraction (DocRE).
DocRE requires integrating information within and across sentences, capturing
complex interactions between mentions of entities. Most existing methods are
pipeline-based, requiring entities as input. However, jointly learning to
extract entities and relations can improve performance and be more efficient
due to shared parameters and training steps. In this paper, we develop a
sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE
(entity extraction, coreference resolution and relation extraction) end-to-end,
replacing a pipeline of task-specific components. Using a simple strategy we
call entity hinting, we compare our approach to existing pipeline-based methods
on several popular biomedical datasets, in some cases exceeding their
performance. We also report the first end-to-end results on these datasets for
future comparison. Finally, we demonstrate that, under our model, an end-to-end
approach outperforms a pipeline-based approach. Our code, data and trained
models are available at {\url{https://github.com/johngiorgi/seq2rel}}. An
online demo is available at
{\url{https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py}}.",https://github.com/johngiorgi/seq2rel,-1
Connection-minimal Abduction in EL via Translation to FOL -- Technical Report,0.0818234,"Abduction in description logics finds extensions of a knowledge base to make
it entail an observation. As such, it can be used to explain why the
observation does not follow, to repair incomplete knowledge bases, and to
provide possible explanations for unexpected observations. We consider TBox
abduction in the lightweight description logic EL, where the observation is a
concept inclusion and the background knowledge is a TBox, i.e., a set of
concept inclusions. To avoid useless answers, such problems usually come with
further restrictions on the solution space and/or minimality criteria that help
sort the chaff from the grain. We argue that existing minimality notions are
insufficient, and introduce connection minimality. This criterion follows
Occam's razor by rejecting hypotheses that use concept inclusions unrelated to
the problem at hand. We show how to compute a special class of
connection-minimal hypotheses in a sound and complete way. Our technique is
based on a translation to first-order logic, and constructs hypotheses based on
prime implicates. We evaluate a prototype implementation of our approach on
ontologies from the medical domain.",None,-1
Generative Modelling With Inverse Heat Dissipation,0.713086,"While diffusion models have shown great success in image generation, their
noise-inverting generative process does not explicitly consider the structure
of images, such as their inherent multi-scale nature. Inspired by diffusion
models and the empirical success of coarse-to-fine modelling, we propose a new
diffusion-like model that generates images through stochastically reversing the
heat equation, a PDE that locally erases fine-scale information when run over
the 2D plane of the image. We interpret the solution of the forward heat
equation with constant additive noise as a variational approximation in the
diffusion latent variable model. Our new model shows emergent qualitative
properties not seen in standard diffusion models, such as disentanglement of
overall colour and shape in images. Spectral analysis on natural images
highlights connections to diffusion models and reveals an implicit
coarse-to-fine inductive bias in them.",https://github.com/AaltoML/generative-inverse-heat-dissipation,-1
Dataless Knowledge Fusion by Merging Weights of Language Models,0.528632,"Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios.",https://github.com/bloomberg/dataless-model-merging,-1
RelTR: Relation Transformer for Scene Graph Generation,0.821438,"Different objects in the same scene are more or less related to each other,
but only a limited number of these relationships are noteworthy. Inspired by
DETR, which excels in object detection, we view scene graph generation as a set
prediction problem and propose an end-to-end scene graph generation model RelTR
which has an encoder-decoder architecture. The encoder reasons about the visual
feature context while the decoder infers a fixed-size set of triplets
subject-predicate-object using different types of attention mechanisms with
coupled subject and object queries. We design a set prediction loss performing
the matching between the ground truth and predicted triplets for the end-to-end
training. In contrast to most existing scene graph generation methods, RelTR is
a one-stage method that predicts a set of relationships directly only using
visual appearance without combining entities and labeling all possible
predicates. Extensive experiments on the Visual Genome and Open Images V6
datasets demonstrate the superior performance and fast inference of our model.",https://github.com/yrcong/RelTR,-1
An Initial Investigation for Detecting Vocoder Fingerprints of Fake Audio,0.0492075,"Many effective attempts have been made for fake audio detection. However,
they can only provide detection results but no countermeasures to curb this
harm. For many related practical applications, what model or algorithm
generated the fake audio also is needed. Therefore, We propose a new problem
for detecting vocoder fingerprints of fake audio. Experiments are conducted on
the datasets synthesized by eight state-of-the-art vocoders. We have
preliminarily explored the features and model architectures. The t-SNE
visualization shows that different vocoders generate distinct vocoder
fingerprints.",https://github.com/xiph/LPCNet.git,-1
MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations,0.68078,"Emotion Recognition in Conversations (ERC) has considerable prospects for
developing empathetic machines. For multimodal ERC, it is vital to understand
context and fuse modality information in conversations. Recent graph-based
fusion methods generally aggregate multimodal information by exploring unimodal
and cross-modal interactions in a graph. However, they accumulate redundant
information at each layer, limiting the context understanding between
modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network
(MM-DFN) to recognize emotions by fully understanding multimodal conversational
context. Specifically, we design a new graph-based dynamic fusion module to
fuse multimodal contextual features in a conversation. The module reduces
redundancy and enhances complementarity between modalities by capturing the
dynamics of contextual information in different semantic spaces. Extensive
experiments on two public benchmark datasets demonstrate the effectiveness and
superiority of MM-DFN.",https://github.com/zerohd4869/MM-DFN,-1
Dialog Acts for Task-Driven Embodied Agents,0.109679,"Embodied agents need to be able to interact in natural language understanding
task descriptions and asking appropriate follow up questions to obtain
necessary information to be effective at successfully accomplishing tasks for a
wide range of users. In this work, we propose a set of dialog acts for
modelling such dialogs and annotate the TEACh dataset that includes over 3,000
situated, task oriented conversations (consisting of 39.5k utterances in total)
with dialog acts. TEACh-DA is one of the first large scale dataset of dialog
act annotations for embodied task completion. Furthermore, we demonstrate the
use of this annotated dataset in training models for tagging the dialog acts of
a given utterance, predicting the dialog act of the next response given a
dialog history, and use the dialog acts to guide agent's non-dialog behaviour.
In particular, our experiments on the TEACh Execution from Dialog History task
where the model predicts the sequence of low level actions to be executed in
the environment for embodied task completion, demonstrate that dialog acts can
improve end task success rate by up to 2 points compared to the system without
dialog acts.",None,-1
TCTrack: Temporal Contexts for Aerial Tracking,0.567683,"Temporal contexts among consecutive frames are far from being fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier.",https://github.com/vision4robotics/TCTrack,-1
Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior,0.942709,"Learned locomotion policies can rapidly adapt to diverse environments similar
to those experienced during training but lack a mechanism for fast tuning when
they fail in an out-of-distribution test environment. This necessitates a slow
and iterative cycle of reward and environment redesign to achieve good
performance on a new task. As an alternative, we propose learning a single
policy that encodes a structured family of locomotion strategies that solve
training tasks in different ways, resulting in Multiplicity of Behavior (MoB).
Different strategies generalize differently and can be chosen in real-time for
new tasks or environments, bypassing the need for time-consuming retraining. We
release a fast, robust open-source MoB locomotion controller, Walk These Ways,
that can execute diverse gaits with variable footswing, posture, and speed,
unlocking diverse downstream tasks: crouching, hopping, high-speed running,
stair traversal, bracing against shoves, rhythmic dance, and more. Video and
code release: https://gmargo11.github.io/walk-these-ways/",https://gmargo11.github.io/walk-these-ways/,-1
"Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices",0.325607,"In this paper, we consider the problem of real-time video-based facial
emotion analytics, namely, facial expression recognition, prediction of valence
and arousal and detection of action unit points. We propose the novel
frame-level emotion recognition algorithm by extracting facial features with
the single EfficientNet model pre-trained on AffectNet. As a result, our
approach may be implemented even for video analytics on mobile devices.
Experimental results for the large scale Aff-Wild2 database from the third
Affective Behavior Analysis in-the-wild (ABAW) Competition demonstrate that our
simple model is significantly better when compared to the VggFace baseline. In
particular, our method is characterized by 0.15-0.2 higher performance measures
for validation sets in uni-task Expression Classification, Valence-Arousal
Estimation and Expression Classification. Due to simplicity, our approach may
be considered as a new baseline for all four sub-challenges.",None,-1
SC-wLS: Towards Interpretable Feed-forward Camera Re-localization,0.0723039,"Visual re-localization aims to recover camera poses in a known environment,
which is vital for applications like robotics or augmented reality.
Feed-forward absolute camera pose regression methods directly output poses by a
network, but suffer from low accuracy. Meanwhile, scene coordinate based
methods are accurate, but need iterative RANSAC post-processing, which brings
challenges to efficient end-to-end training and inference. In order to have the
best of both worlds, we propose a feed-forward method termed SC-wLS that
exploits all scene coordinate estimates for weighted least squares pose
regression. This differentiable formulation exploits a weight network imposed
on 2D-3D correspondences, and requires pose supervision only. Qualitative
results demonstrate the interpretability of learned weights. Evaluations on
7Scenes and Cambridge datasets show significantly promoted performance when
compared with former feed-forward counterparts. Moreover, our SC-wLS method
enables a new capability: self-supervised test-time adaptation on the weight
network. Codes and models are publicly available.",https://github.com/XinWu98/SC-wLSAbstract.Visualre-localizationaimstorecovercameraposesinaknownenvironment,-1
EfficientNeRF: Efficient Neural Radiance Fields,0.864665,"Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications.",https://github.com/dvlab-research/EfﬁcientNeRF,-1
Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages,0.449836,"Human languages are full of metaphorical expressions. Metaphors help people
understand the world by connecting new concepts and domains to more familiar
ones. Large pre-trained language models (PLMs) are therefore assumed to encode
metaphorical knowledge useful for NLP systems. In this paper, we investigate
this hypothesis for PLMs, by probing metaphoricity information in their
encodings, and by measuring the cross-lingual and cross-dataset generalization
of this information. We present studies in multiple metaphor detection datasets
and in four languages (i.e., English, Spanish, Russian, and Farsi). Our
extensive experiments suggest that contextual representations in PLMs do encode
metaphorical knowledge, and mostly in their middle layers. The knowledge is
transferable between languages and datasets, especially when the annotation is
consistent across training and testing sets. Our findings give helpful insights
for both cognitive and NLP scientists.",https://github.com/EhsanAghazadeh/Metaphors_in_PLMs,-1
MultiCoNER: A Large-scale Multilingual dataset for Complex Named Entity Recognition,0.999854,"We present MultiCoNER, a large multilingual dataset for Named Entity
Recognition that covers 3 domains (Wiki sentences, questions, and search
queries) across 11 languages, as well as multilingual and code-mixing subsets.
This dataset is designed to represent contemporary challenges in NER, including
low-context scenarios (short and uncased text), syntactically complex entities
like movie titles, and long-tail entity distributions. The 26M token dataset is
compiled from public resources using techniques such as heuristic-based
sentence sampling, template extraction and slotting, and machine translation.
We applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a
state-of-the-art GEMNET model that leverages gazetteers. The baseline achieves
moderate performance (macro-F1=54%), highlighting the difficulty of our data.
GEMNET, which uses gazetteers, improvement significantly (average improvement
of macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained
language models, and we believe that it can help further research in building
robust NER systems. MultiCoNER is publicly available at
https://registry.opendata.aws/multiconer/ and we hope that this resource will
help advance research in various aspects of NER.",None,-1
"Alexa, Let's Work Together: Introducing the First Alexa Prize TaskBot Challenge on Conversational Task Assistance",0.993697,"Since its inception in 2016, the Alexa Prize program has enabled hundreds of
university students to explore and compete to develop conversational agents
through the SocialBot Grand Challenge. The goal of the challenge is to build
agents capable of conversing coherently and engagingly with humans on popular
topics for 20 minutes, while achieving an average rating of at least 4.0/5.0.
However, as conversational agents attempt to assist users with increasingly
complex tasks, new conversational AI techniques and evaluation platforms are
needed. The Alexa Prize TaskBot challenge, established in 2021, builds on the
success of the SocialBot challenge by introducing the requirements of
interactively assisting humans with real-world Cooking and Do-It-Yourself
tasks, while making use of both voice and visual modalities. This challenge
requires the TaskBots to identify and understand the user's need, identify and
integrate task and domain knowledge into the interaction, and develop new ways
of engaging the user without distracting them from the task at hand, among
other challenges. This paper provides an overview of the TaskBot challenge,
describes the infrastructure support provided to the teams with the CoBot
Toolkit, and summarizes the approaches the participating teams took to overcome
the research challenges. Finally, it analyzes the performance of the competing
TaskBots during the first year of the competition.",None,-1
Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer,0.926157,"Videos are created to express emotion, exchange information, and share
experiences. Video synthesis has intrigued researchers for a long time. Despite
the rapid progress driven by advances in visual synthesis, most existing
studies focus on improving the frames' quality and the transitions between
them, while little progress has been made in generating longer videos. In this
paper, we present a method that builds on 3D-VQGAN and transformers to generate
videos with thousands of frames. Our evaluation shows that our model trained on
16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse,
and Taichi-HD datasets can generate diverse, coherent, and high-quality long
videos. We also showcase conditional extensions of our approach for generating
meaningful long videos by incorporating temporal information with text and
audio. Videos and code can be found at
https://songweige.github.io/projects/tats/index.html.",https://songweige.github.io/projects/tats,-1
Bangla hate speech detection on social media using attention-based recurrent neural network,0.75001,"Hate speech has spread more rapidly through the daily use of technology and,
most notably, by sharing your opinions or feelings on social media in a
negative aspect. Although numerous works have been carried out in detecting
hate speeches in English, German, and other languages, very few works have been
carried out in the context of the Bengali language. In contrast, millions of
people communicate on social media in Bengali. The few existing works that have
been carried out need improvements in both accuracy and interpretability. This
article proposed encoder decoder based machine learning model, a popular tool
in NLP, to classify user's Bengali comments on Facebook pages. A dataset of
7,425 Bengali comments, consisting of seven distinct categories of hate
speeches, was used to train and evaluate our model. For extracting and encoding
local features from the comments, 1D convolutional layers were used. Finally,
the attention mechanism, LSTM, and GRU based decoders have been used for
predicting hate speech categories. Among the three encoder decoder algorithms,
the attention-based decoder obtained the best accuracy (77%).",None,-1
SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning,0.19128,"We present the SUPERB challenge at SLT 2022, which aims at learning
self-supervised speech representation for better performance, generalization,
and efficiency. The challenge builds upon the SUPERB benchmark and implements
metrics to measure the computation requirements of self-supervised learning
(SSL) representation and to evaluate its generalizability and performance
across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive
coverage of popular speech processing tasks, from speech and speaker
recognition to audio generation and semantic understanding. As SSL has gained
interest in the speech community and showed promising outcomes, we envision the
challenge to uplevel the impact of SSL techniques by motivating more practical
designs of techniques beyond task performance. We summarize the results of 14
submitted models in this paper. We also discuss the main findings from those
submissions and the future directions of SSL research.",https://github.com/s3prl/s3prl,-1
A method for ethical AI in Defence: A case study on developing trustworthy autonomous systems,0.207215,"What does it mean to be responsible and responsive when developing and
deploying trusted autonomous systems in Defence? In this short reflective
article, we describe a case study of building a trusted autonomous system -
Athena AI - within an industry-led, government-funded project with diverse
collaborators and stakeholders. Using this case study, we draw out lessons on
the value and impact of embedding responsible research and innovation-aligned,
ethics-by-design approaches and principles throughout the development of
technology at high translation readiness levels.",None,-1
Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction,0.419326,"Compared with traditional sentence-level relation extraction, document-level
relation extraction is a more challenging task where an entity in a document
may be mentioned multiple times and associated with multiple relations.
However, most methods of document-level relation extraction do not distinguish
between mention-level features and entity-level features, and just apply simple
pooling operation for aggregating mention-level features into entity-level
features. As a result, the distinct semantics between the different mentions of
an entity are overlooked. To address this problem, we propose RSMAN in this
paper which performs selective attentions over different entity mentions with
respect to candidate relations. In this manner, the flexible and
relation-specific representations of entities are obtained which indeed benefit
relation classification. Our extensive experiments upon two benchmark datasets
show that our RSMAN can bring significant improvements for some backbone models
to achieve state-of-the-art performance, especially when an entity have
multiple mentions in the document.",https://github.com/FDUyjx/RSMAN,-1
A Variational Hierarchical Model for Neural Cross-Lingual Summarization,0.29947,"The goal of the cross-lingual summarization (CLS) is to convert a document in
one language (e.g., English) to a summary in another one (e.g., Chinese).
Essentially, the CLS task is the combination of machine translation (MT) and
monolingual summarization (MS), and thus there exists the hierarchical
relationship between MT\&MS and CLS. Existing studies on CLS mainly focus on
utilizing pipeline methods or jointly training an end-to-end model through an
auxiliary MT or MS objective. However, it is very challenging for the model to
directly conduct CLS as it requires both the abilities to translate and
summarize. To address this issue, we propose a hierarchical model for the CLS
task, based on the conditional variational auto-encoder. The hierarchical model
contains two kinds of latent variables at the local and global levels,
respectively. At the local level, there are two latent variables, one for
translation and the other for summarization. As for the global level, there is
another latent variable for cross-lingual summarization conditioned on the two
local-level variables. Experiments on two language directions (English-Chinese)
verify the effectiveness and superiority of the proposed approach. In addition,
we show that our model is able to generate better cross-lingual summaries than
comparison models in the few-shot setting.",https://github.com/XL2248/VHM,-1
Emergent Communication through Metropolis-Hastings Naming Game with Deep Generative Models,0.326028,"Constructive studies on symbol emergence systems seek to investigate
computational models that can better explain human language evolution, the
creation of symbol systems, and the construction of internal representations.
This study provides a new model for emergent communication, which is based on a
probabilistic generative model (PGM) instead of a discriminative model based on
deep reinforcement learning. We define the Metropolis-Hastings (MH) naming game
by generalizing previously proposed models. It is not a referential game with
explicit feedback, as assumed by many emergent communication studies. Instead,
it is a game based on joint attention without explicit feedback.
Mathematically, the MH naming game is proved to be a type of MH algorithm for
an integrative PGM that combines two agents that play the naming game. From
this viewpoint, symbol emergence is regarded as decentralized Bayesian
inference, and semiotic communication is regarded as inter-personal cross-modal
inference. This notion leads to the collective predictive coding hypothesis}
regarding language evolution and, in general, the emergence of symbols. We also
propose the inter-Gaussian mixture model (GMM)+ variational autoencoder (VAE),
a deep generative model for emergent communication based on the MH naming game.
The model has been validated on MNIST and Fruits 360 datasets. Experimental
findings demonstrate that categories are formed from real images observed by
agents, and signs are correctly shared across agents by successfully utilizing
both of the observations of agents via the MH naming game. Furthermore,
scholars verified that visual images were recalled from signs uttered by
agents. Notably, emergent communication without supervision and reward feedback
improved the performance of the unsupervised representation learning of agents.",https://github.com/is0383kk/SymbolEmergence-VAE-GMM,-1
Seeing Like a Toolkit: How Toolkits Envision the Work of AI Ethics,0.171851,"Numerous toolkits have been developed to support ethical AI development.
However, toolkits, like all tools, encode assumptions in their design about
what work should be done and how. In this paper, we conduct a qualitative
analysis of 27 AI ethics toolkits to critically examine how the work of ethics
is imagined and how it is supported by these toolkits. Specifically, we examine
the discourses toolkits rely on when talking about ethical issues, who they
imagine should do the work of ethics, and how they envision the work practices
involved in addressing ethics. Among the toolkits, we identify a mismatch
between the imagined work of ethics and the support the toolkits provide for
doing that work. In particular, we identify a lack of guidance around how to
navigate labor, organizational, and institutional power dynamics as they relate
to performing ethical work. We use these omissions to chart future work for
researchers and designers of AI ethics toolkits.",None,-1
Imagination-Augmented Natural Language Understanding,0.486221,"Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances.",https://github.com/YujieLu10/IACE-NLU,-1
Delving into Out-of-Distribution Detection with Vision-Language Representations,0.693359,"Recognizing out-of-distribution (OOD) samples is critical for machine
learning systems deployed in the open world. The vast majority of OOD detection
methods are driven by a single modality (e.g., either vision or language),
leaving the rich information in multi-modal representations untapped. Inspired
by the recent success of vision-language pre-training, this paper enriches the
landscape of OOD detection from a single-modal to a multi-modal regime.
Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective
zero-shot OOD detection method based on aligning visual features with textual
concepts. We contribute in-depth analysis and theoretical insights to
understand the effectiveness of MCM. Extensive experiments demonstrate that MCM
achieves superior performance on a wide variety of real-world tasks. MCM with
vision-language features outperforms a common baseline with pure visual
features on a hard OOD task with semantically similar classes by 13.1% (AUROC).
Code is available at https://github.com/deeplearning-wisc/MCM.",https://github.com/deeplearning-wisc/MCM,-1
Unified Pretraining Framework for Document Understanding,0.299123,"Document intelligence automates the extraction of information from documents
and supports many business applications. Recent self-supervised learning
methods on large-scale unlabeled document datasets have opened up promising
directions towards reducing annotation efforts by training models with
self-supervised objectives. However, most of the existing document pretraining
methods are still language-dominated. We present UDoc, a new unified
pretraining framework for document understanding. UDoc is designed to support
most document understanding tasks, extending the Transformer to take multimodal
embeddings as input. Each input element is composed of words and visual
features from a semantic region of the input document image. An important
feature of UDoc is that it learns a generic representation by making use of
three self-supervised losses, encouraging the representation to model
sentences, learn similarities, and align modalities. Extensive empirical
analysis demonstrates that the pretraining procedure learns better joint
representations and leads to improvements in downstream tasks.",https://github.com/facebookresearch/detectron2,-1
Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation,0.517115,"A diffusion model learns to predict a vector field of gradients. We propose
to apply chain rule on the learned gradients, and back-propagate the score of a
diffusion model through the Jacobian of a differentiable renderer, which we
instantiate to be a voxel radiance field. This setup aggregates 2D scores at
multiple camera viewpoints into a 3D score, and repurposes a pretrained 2D
model for 3D data generation. We identify a technical challenge of distribution
mismatch that arises in this application, and propose a novel estimation
mechanism to resolve it. We run our algorithm on several off-the-shelf
diffusion image generative models, including the recently released Stable
Diffusion trained on the large-scale LAION dataset.",https://github.com/ashawkey/stable-dreamfusion,-1
Empowering Graph Representation Learning with Test-Time Graph Transformation,0.283025,"As powerful tools for representation learning on graphs, graph neural
networks (GNNs) have facilitated various applications from drug discovery to
recommender systems. Nevertheless, the effectiveness of GNNs is immensely
challenged by issues related to data quality, such as distribution shift,
abnormal features and adversarial attacks. Recent efforts have been made on
tackling these issues from a modeling perspective which requires additional
cost of changing model architectures or re-training model parameters. In this
work, we provide a data-centric view to tackle these issues and propose a graph
transformation framework named GTrans which adapts and refines graph data at
test time to achieve better performance. We provide theoretical analysis on the
design of the framework and discuss why adapting graph data works better than
adapting the model. Extensive experiments have demonstrated the effectiveness
of GTrans on three distinct scenarios for eight benchmark datasets where
suboptimal data is presented. Remarkably, GTrans performs the best in most
cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on
three experimental settings. Code is released at
https://github.com/ChandlerBang/GTrans.",https://github.com/ChandlerBang/GTrans,-1
Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small,0.76108,"Research in mechanistic interpretability seeks to explain behaviors of
machine learning models in terms of their internal components. However, most
previous work either focuses on simple behaviors in small models, or describes
complicated behaviors in larger models with broad strokes. In this work, we
bridge this gap by presenting an explanation for how GPT-2 small performs a
natural language task called indirect object identification (IOI). Our
explanation encompasses 26 attention heads grouped into 7 main classes, which
we discovered using a combination of interpretability approaches relying on
causal interventions. To our knowledge, this investigation is the largest
end-to-end attempt at reverse-engineering a natural behavior ""in the wild"" in a
language model. We evaluate the reliability of our explanation using three
quantitative criteria--faithfulness, completeness and minimality. Though these
criteria support our explanation, they also point to remaining gaps in our
understanding. Our work provides evidence that a mechanistic understanding of
large ML models is feasible, opening opportunities to scale our understanding
to both larger models and more complex tasks.",https://github.com/redwoodresearch/Easy-Transformer,-1
Legal Case Document Summarization: Extractive and Abstractive Methods and their Evaluation,0.437349,"Summarization of legal case judgement documents is a challenging problem in
Legal NLP. However, not much analyses exist on how different families of
summarization models (e.g., extractive vs. abstractive) perform when applied to
legal case documents. This question is particularly important since many recent
transformer-based abstractive summarization models have restrictions on the
number of input tokens, and legal documents are known to be very long. Also, it
is an open question on how best to evaluate legal case document summarization
systems. In this paper, we carry out extensive experiments with several
extractive and abstractive summarization methods (both supervised and
unsupervised) over three legal summarization datasets that we have developed.
Our analyses, that includes evaluation by law practitioners, lead to several
interesting insights on legal summarization in specific and long document
summarization in general.",https://github.com/Law-AI/summarization,-1
Online Continual Learning with Contrastive Vision Transformer,0.145869,"Online continual learning (online CL) studies the problem of learning
sequential tasks from an online data stream without task boundaries, aiming to
adapt to new data while alleviating catastrophic forgetting on the past tasks.
This paper proposes a framework Contrastive Vision Transformer (CVT), which
designs a focal contrastive learning strategy based on a transformer
architecture, to achieve a better stability-plasticity trade-off for online CL.
Specifically, we design a new external attention mechanism for online CL that
implicitly captures previous tasks' information. Besides, CVT contains
learnable focuses for each class, which could accumulate the knowledge of
previous classes to alleviate forgetting. Based on the learnable focuses, we
design a focal contrastive loss to rebalance contrastive learning between new
and past classes and consolidate previously learned representations. Moreover,
CVT contains a dual-classifier structure for decoupling learning current
classes and balancing all observed classes. The extensive experimental results
show that our approach achieves state-of-the-art performance with even fewer
parameters on online CL benchmarks and effectively alleviates the catastrophic
forgetting.",None,-1
Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words,0.282368,"Cosine similarity of contextual embeddings is used in many NLP tasks (e.g.,
QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in
which word similarities estimated by cosine over BERT embeddings are
understated and trace this effect to training data frequency. We find that
relative to human judgements, cosine similarity underestimates the similarity
of frequent words with other instances of the same word or other words across
contexts, even after controlling for polysemy and other factors. We conjecture
that this underestimation of similarity for high frequency words is due to
differences in the representational geometry of high and low frequency words
and provide a formal argument for the two-dimensional case.",https://github.com/katezhou/cosine_and_frequency,-1
Active Learning by Feature Mixing,0.57656,"The promise of active learning (AL) is to reduce labelling costs by selecting
the most valuable examples to annotate from a pool of unlabelled data.
Identifying these examples is especially challenging with high-dimensional data
(e.g. images, videos) and in low-data regimes. In this paper, we propose a
novel method for batch AL called ALFA-Mix. We identify unlabelled instances
with sufficiently-distinct features by seeking inconsistencies in predictions
resulting from interventions on their representations. We construct
interpolations between representations of labelled and unlabelled instances
then examine the predicted labels. We show that inconsistencies in these
predictions help discovering features that the model is unable to recognise in
the unlabelled instances. We derive an efficient implementation based on a
closed-form solution to the optimal interpolation causing changes in
predictions. Our method outperforms all recent AL approaches in 30 different
settings on 12 benchmarks of images, videos, and non-visual data. The
improvements are especially significant in low-data regimes and on self-trained
vision transformers, where ALFA-Mix outperforms the state-of-the-art in 59% and
43% of the experiments respectively.",https://github.com/aminparvaneh/alpha_mix_active_learning,-1
Dynamic Global Memory for Document-level Argument Extraction,0.650797,"Extracting informative arguments of events from news articles is a
challenging problem in information extraction, which requires a global
contextual understanding of each document. While recent work on document-level
extraction has gone beyond single-sentence and increased the cross-sentence
inference capability of end-to-end models, they are still restricted by certain
input sequence length constraints and usually ignore the global context between
events. To tackle this issue, we introduce a new global neural generation-based
framework for document-level event argument extraction by constructing a
document memory store to record the contextual event information and leveraging
it to implicitly and explicitly help with decoding of arguments for later
events. Empirical results show that our framework outperforms prior methods
substantially and it is more robust to adversarially annotated examples with
our constrained decoding design. (Our code and resources are available at
https://github.com/xinyadu/memory_docie for research purpose.)",https://github.com/xinyadu/memory_docie,-1
"DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis",0.647909,"Discriminative learning, restorative learning, and adversarial learning have
proven beneficial for self-supervised learning schemes in computer vision and
medical imaging. Existing efforts, however, omit their synergistic effects on
each other in a ternary setup, which, we envision, can significantly benefit
deep semantic representation learning. To realize this vision, we have
developed DiRA, the first framework that unites discriminative, restorative,
and adversarial learning in a unified manner to collaboratively glean
complementary visual information from unlabeled medical images for fine-grained
semantic representation learning. Our extensive experiments demonstrate that
DiRA (1) encourages collaborative learning among three learning ingredients,
resulting in more generalizable representation across organs, diseases, and
modalities; (2) outperforms fully supervised ImageNet models and increases
robustness in small data regimes, reducing annotation cost across multiple
medical imaging applications; (3) learns fine-grained semantic representation,
facilitating accurate lesion localization with only image-level annotation; and
(4) enhances state-of-the-art restorative approaches, revealing that DiRA is a
general mechanism for united representation learning. All code and pre-trained
models are available at https: //github.com/JLiangLab/DiRA.",https://github.com/JLiangLab/DiRA,-1
SafeText: A Benchmark for Exploring Physical Safety in Language Models,0.864665,"Understanding what constitutes safe text is an important issue in natural
language processing and can often prevent the deployment of models deemed
harmful and unsafe. One such type of safety that has been scarcely studied is
commonsense physical safety, i.e. text that is not explicitly violent and
requires additional commonsense knowledge to comprehend that it leads to
physical harm. We create the first benchmark dataset, SafeText, comprising
real-life scenarios with paired safe and physically unsafe pieces of advice. We
utilize SafeText to empirically study commonsense physical safety across
various models designed for text generation and commonsense reasoning tasks. We
find that state-of-the-art large language models are susceptible to the
generation of unsafe text and have difficulty rejecting unsafe advice. As a
result, we argue for further studies of safety and the assessment of
commonsense physical safety in models before release.",None,-1
DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,0.549166,"With the ever-growing size of pretrained models (PMs), fine-tuning them has
become more expensive and resource-hungry. As a remedy, low-rank adapters
(LoRA) keep the main pretrained weights of the model frozen and just introduce
some learnable truncated SVD modules (so-called LoRA blocks) to the model.
While LoRA blocks are parameter-efficient, they suffer from two major problems:
first, the size of these blocks is fixed and cannot be modified after training
(for example, if we need to change the rank of LoRA blocks, then we need to
re-train them from scratch); second, optimizing their rank requires an
exhaustive search and effort. In this work, we introduce a dynamic low-rank
adaptation (DyLoRA) technique to address these two problems together. Our
DyLoRA method trains LoRA blocks for a range of ranks instead of a single rank
by sorting the representation learned by the adapter module at different ranks
during training. We evaluate our solution on different natural language
understanding (GLUE benchmark) and language generation tasks (E2E, DART and
WebNLG) using different pretrained models such as RoBERTa and GPT with
different sizes. Our results show that we can train dynamic search-free models
with DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA
without significantly compromising performance. Moreover, our models can
perform consistently well on a much larger range of ranks compared to LoRA.",https://github.com/huawei-noah/KD-NLP/tree/main/DyLoRA,-1
In-Hand 3D Object Scanning from an RGB Sequence,0.597316,"We propose a method for in-hand 3D scanning of an unknown object with a
monocular camera. Our method relies on a neural implicit surface representation
that captures both the geometry and the appearance of the object, however, by
contrast with most NeRF-based methods, we do not assume that the camera-object
relative poses are known. Instead, we simultaneously optimize both the object
shape and the pose trajectory. As direct optimization over all shape and pose
parameters is prone to fail without coarse-level initialization, we propose an
incremental approach that starts by splitting the sequence into carefully
selected overlapping segments within which the optimization is likely to
succeed. We reconstruct the object shape and track its poses independently
within each segment, then merge all the segments before performing a global
optimization. We show that our method is able to reconstruct the shape and
color of both textured and challenging texture-less objects, outperforms
classical methods that rely only on appearance features, and that its
performance is close to recent methods that assume known camera poses.",None,-1
FlowFormer: A Transformer Architecture for Optical Flow,0.998661,"We introduce optical Flow transFormer, dubbed as FlowFormer, a
transformer-based neural network architecture for learning optical flow.
FlowFormer tokenizes the 4D cost volume built from an image pair, encodes the
cost tokens into a cost memory with alternate-group transformer (AGT) layers in
a novel latent space, and decodes the cost memory via a recurrent transformer
decoder with dynamic positional cost queries. On the Sintel benchmark,
FlowFormer achieves 1.159 and 2.088 average end-point-error (AEPE) on the clean
and final pass, a 16.5% and 15.5% error reduction from the best published
result (1.388 and 2.47). Besides, FlowFormer also achieves strong
generalization performance. Without being trained on Sintel, FlowFormer
achieves 1.01 AEPE on the clean pass of Sintel training set, outperforming the
best published result (1.29) by 21.7%.",None,-1
The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts,0.781799,"Classroom discourse is a core medium of instruction - analyzing it can
provide a window into teaching and learning as well as driving the development
of new tools for improving instruction. We introduce the largest dataset of
mathematics classroom transcripts available to researchers, and demonstrate how
this data can help improve instruction. The dataset consists of 1,660 45-60
minute long 4th and 5th grade elementary mathematics observations collected by
the National Center for Teacher Effectiveness (NCTE) between 2010-2013. The
anonymized transcripts represent data from 317 teachers across 4 school
districts that serve largely historically marginalized students. The
transcripts come with rich metadata, including turn-level annotations for
dialogic discourse moves, classroom observation scores, demographic
information, survey responses and student test scores. We demonstrate that our
natural language processing model, trained on our turn-level annotations, can
learn to identify dialogic discourse moves and these moves are correlated with
better classroom observation scores and learning outcomes. This dataset opens
up several possibilities for researchers, educators and policymakers to learn
about and improve K-12 instruction. The dataset can be found at
https://github.com/ddemszky/classroom-transcript-analysis.",https://github.com/ddemszky/classroom-transcript-analysis,-1
Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives,0.320536,"This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a
new member in the family of InfoNCE losses that preserves a ranked ordering of
positive samples. In contrast to the standard InfoNCE loss, which requires a
strict binary separation of the training pairs into similar and dissimilar
samples, RINCE can exploit information about a similarity ranking for learning
a corresponding embedding space. We show that the proposed loss function learns
favorable embeddings compared to the standard InfoNCE whenever at least noisy
ranking information can be obtained or when the definition of positives and
negatives is blurry. We demonstrate this for a supervised classification task
with additional superclass labels and noisy similarity scores. Furthermore, we
show that RINCE can also be applied to unsupervised training with experiments
on unsupervised representation learning from videos. In particular, the
embedding yields higher classification accuracy, retrieval rates and performs
better in out-of-distribution detection than the standard InfoNCE loss.",https://github.com/boschresearch/rince,-1
Interpretability for Language Learners Using Example-Based Grammatical Error Correction,0.451257,"Grammatical Error Correction (GEC) should not focus only on high accuracy of
corrections but also on interpretability for language learning. However,
existing neural-based GEC models mainly aim at improving accuracy, and their
interpretability has not been explored. A promising approach for improving
interpretability is an example-based method, which uses similar retrieved
examples to generate corrections. In addition, examples are beneficial in
language learning, helping learners understand the basis of grammatically
incorrect/correct texts and improve their confidence in writing. Therefore, we
hypothesize that incorporating an example-based method into GEC can improve
interpretability as well as support language learners. In this study, we
introduce an Example-Based GEC (EB-GEC) that presents examples to language
learners as a basis for a correction result. The examples consist of pairs of
correct and incorrect sentences similar to a given input and its predicted
correction. Experiments demonstrate that the examples presented by EB-GEC help
language learners decide to accept or refuse suggestions from the GEC output.
Furthermore, the experiments also show that retrieved examples improve the
accuracy of corrections.",https://github.com/kanekomasahiro/eb-gec,-1
3D Pose Based Feedback for Physical Exercises,0.147035,"Unsupervised self-rehabilitation exercises and physical training can cause
serious injuries if performed incorrectly. We introduce a learning-based
framework that identifies the mistakes made by a user and proposes corrective
measures for easier and safer individual training. Our framework does not rely
on hard-coded, heuristic rules. Instead, it learns them from data, which
facilitates its adaptation to specific user needs. To this end, we use a Graph
Convolutional Network (GCN) architecture acting on the user's pose sequence to
model the relationship between the body joints trajectories. To evaluate our
approach, we introduce a dataset with 3 different physical exercises. Our
approach yields 90.9% mistake identification accuracy and successfully corrects
94.2% of the mistakes.",None,-1
XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence,0.864665,"Recent advances in machine learning have significantly improved the
understanding of source code data and achieved good performance on a number of
downstream tasks. Open source repositories like GitHub enable this process with
rich unlabeled code data. However, the lack of high quality labeled data has
largely hindered the progress of several code related tasks, such as program
translation, summarization, synthesis, and code search. This paper introduces
XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for
cross-lingual code intelligence. Our dataset contains fine-grained parallel
data from 8 languages (7 commonly used programming languages and English), and
supports 10 cross-lingual code tasks. To the best of our knowledge, it is the
largest parallel dataset for source code both in terms of size and the number
of languages. We also provide the performance of several state-of-the-art
baseline models for each task. We believe this new dataset can be a valuable
asset for the research community and facilitate the development and validation
of new methods for cross-lingual code intelligence.",https://github.com/reddy-lab-code-research/XLCoST,-1
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models,0.637793,"This study focuses on using large language models (LLMs) as a planner for
embodied agents that can follow natural language instructions to complete
complex tasks in a visually-perceived environment. The high data cost and poor
sample efficiency of existing methods hinders the development of versatile
agents that are capable of many tasks and can learn new tasks quickly. In this
work, we propose a novel method, LLM-Planner, that harnesses the power of large
language models to do few-shot planning for embodied agents. We further propose
a simple but effective way to enhance LLMs with physical grounding to generate
and update plans that are grounded in the current environment. Experiments on
the ALFRED dataset show that our method can achieve very competitive few-shot
performance: Despite using less than 0.5% of paired training data, LLM-Planner
achieves competitive performance with recent baselines that are trained using
the full training data. Existing methods can barely complete any task
successfully under the same few-shot setting. Our work opens the door for
developing versatile and sample-efficient embodied agents that can quickly
learn many tasks. Website: https://dki-lab.github.io/LLM-Planner",https://github.com/google-research/saycan,-1
"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends",0.408797,"This paper surveys vision-language pre-training (VLP) methods for multimodal
intelligence that have been developed in the last few years. We group these
approaches into three categories: ($i$) VLP for image-text tasks, such as image
captioning, image-text retrieval, visual question answering, and visual
grounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image
classification, object detection, and segmentation; and ($iii$) VLP for
video-text tasks, such as video captioning, video-text retrieval, and video
question answering. For each category, we present a comprehensive review of
state-of-the-art methods, and discuss the progress that has been made and
challenges still being faced, using specific systems and models as case
studies. In addition, for each category, we discuss advanced topics being
actively explored in the research community, such as big foundation models,
unified modeling, in-context few-shot learning, knowledge, robustness, and
computer vision in the wild, to name a few.",None,-1
Recurrent Memory Transformer,0.133018,"Transformer-based models show their effectiveness across multiple domains and
tasks. The self-attention allows to combine information from all sequence
elements into context-aware representations. However, global and local
information has to be stored mostly in the same element-wise representations.
Moreover, the length of an input sequence is limited by quadratic computational
complexity of self-attention.
  In this work, we propose and study a memory-augmented segment-level recurrent
Transformer (RMT). Memory allows to store and process local and global
information as well as to pass information between segments of the long
sequence with the help of recurrence.
  We implement a memory mechanism with no changes to Transformer model by
adding special memory tokens to the input or output sequence. Then the model is
trained to control both memory operations and sequence representations
processing.
  Results of experiments show that RMT performs on par with the Transformer-XL
on language modeling for smaller memory sizes and outperforms it for tasks that
require longer sequence processing. We show that adding memory tokens to Tr-XL
is able to improve its performance. This makes Recurrent Memory Transformer a
promising architecture for applications that require learning of long-term
dependencies and general purpose in memory processing, such as algorithmic
tasks and reasoning.",https://github.com/booydar/LM-RMT,-1
AnyFace: Free-style Text-to-Face Synthesis and Manipulation,0.19608,"Existing text-to-image synthesis methods generally are only applicable to
words in the training dataset. However, human faces are so variable to be
described with limited words. So this paper proposes the first free-style
text-to-face method namely AnyFace enabling much wider open world applications
such as metaverse, social media, cosmetics, forensics, etc. AnyFace has a novel
two-stream framework for face image synthesis and manipulation given arbitrary
descriptions of the human face. Specifically, one stream performs text-to-face
generation and the other conducts face image reconstruction. Facial text and
image features are extracted using the CLIP (Contrastive Language-Image
Pre-training) encoders. And a collaborative Cross Modal Distillation (CMD)
module is designed to align the linguistic and visual features across these two
streams. Furthermore, a Diverse Triplet Loss (DT loss) is developed to model
fine-grained features and improve facial diversity. Extensive experiments on
Multi-modal CelebA-HQ and CelebAText-HQ demonstrate significant advantages of
AnyFace over state-of-the-art methods. AnyFace can achieve high-quality,
high-resolution, and high-diversity face synthesis and manipulation results
without any constraints on the number and content of input captions.",None,-1
Human-to-Robot Imitation in the Wild,0.712292,"We approach the problem of learning by watching humans in the wild. While
traditional approaches in Imitation and Reinforcement Learning are promising
for learning in the real world, they are either sample inefficient or are
constrained to lab settings. Meanwhile, there has been a lot of success in
processing passive, unstructured human data. We propose tackling this problem
via an efficient one-shot robot learning algorithm, centered around learning
from a third-person perspective. We call our method WHIRL: In-the-Wild Human
Imitating Robot Learning. WHIRL extracts a prior over the intent of the human
demonstrator, using it to initialize our agent's policy. We introduce an
efficient real-world policy learning scheme that improves using interactions.
Our key contributions are a simple sampling-based policy optimization approach,
a novel objective function for aligning human and robot videos as well as an
exploration method to boost sample efficiency. We show one-shot generalization
and success in real-world settings, including 20 different manipulation tasks
in the wild. Videos and talk at https://human2robot.github.io",https://github.com/hello-robot,-1
Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization,0.212583,"The problems of unfaithful summaries have been widely discussed under the
context of abstractive summarization. Though extractive summarization is less
prone to the common unfaithfulness issues of abstractive summaries, does that
mean extractive is equal to faithful? Turns out that the answer is no. In this
work, we define a typology with five types of broad unfaithfulness problems
(including and beyond not-entailment) that can appear in extractive summaries,
including incorrect coreference, incomplete coreference, incorrect discourse,
incomplete discourse, as well as other misleading information. We ask humans to
label these problems out of 1600 English summaries produced by 16 diverse
extractive systems. We find that 30% of the summaries have at least one of the
five issues. To automatically detect these problems, we find that 5 existing
faithfulness evaluation metrics for summarization have poor correlations with
human judgment. To remedy this, we propose a new metric, ExtEval, that is
designed for detecting unfaithful extractive summaries and is shown to have the
best performance. We hope our work can increase the awareness of unfaithfulness
problems in extractive summarization and help future work to evaluate and
resolve these issues. Our data and code are publicly available at
https://github.com/ZhangShiyue/extractive_is_not_faithful",https://github.com/ZhangShiyue/extractive_is_not_faithful,-1
Unknown-Aware Domain Adversarial Learning for Open-Set Domain Adaptation,0.159793,"Open-Set Domain Adaptation (OSDA) assumes that a target domain contains
unknown classes, which are not discovered in a source domain. Existing domain
adversarial learning methods are not suitable for OSDA because distribution
matching with $\textit{unknown}$ classes leads to negative transfer. Previous
OSDA methods have focused on matching the source and the target distribution by
only utilizing $\textit{known}$ classes. However, this $\textit{known}$-only
matching may fail to learn the target-$\textit{unknown}$ feature space.
Therefore, we propose Unknown-Aware Domain Adversarial Learning (UADAL), which
$\textit{aligns}$ the source and the target-$\textit{known}$ distribution while
simultaneously $\textit{segregating}$ the target-$\textit{unknown}$
distribution in the feature alignment procedure. We provide theoretical
analyses on the optimized state of the proposed $\textit{unknown-aware}$
feature alignment, so we can guarantee both $\textit{alignment}$ and
$\textit{segregation}$ theoretically. Empirically, we evaluate UADAL on the
benchmark datasets, which shows that UADAL outperforms other methods with
better feature alignments by reporting state-of-the-art performances.",https://github.com/JoonHo-Jang/UADAL,-1
Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds,0.384472,"Discovering latent topics from text corpora has been studied for decades.
Many existing topic models adopt a fully unsupervised setting, and their
discovered topics may not cater to users' particular interests due to their
inability of leveraging user guidance. Although there exist seed-guided topic
discovery approaches that leverage user-provided seeds to discover
topic-representative terms, they are less concerned with two factors: (1) the
existence of out-of-vocabulary seeds and (2) the power of pre-trained language
models (PLMs). In this paper, we generalize the task of seed-guided topic
discovery to allow out-of-vocabulary seeds. We propose a novel framework, named
SeeTopic, wherein the general knowledge of PLMs and the local semantics learned
from the input corpus can mutually benefit each other. Experiments on three
real datasets from different domains demonstrate the effectiveness of SeeTopic
in terms of topic coherence, accuracy, and diversity.",None,-1
Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,0.999184,"We apply preference modeling and reinforcement learning from human feedback
(RLHF) to finetune language models to act as helpful and harmless assistants.
We find this alignment training improves performance on almost all NLP
evaluations, and is fully compatible with training for specialized skills such
as python coding and summarization. We explore an iterated online mode of
training, where preference models and RL policies are updated on a weekly
cadence with fresh human feedback data, efficiently improving our datasets and
models. Finally, we investigate the robustness of RLHF training, and identify a
roughly linear relation between the RL reward and the square root of the KL
divergence between the policy and its initialization. Alongside our main
results, we perform peripheral analyses on calibration, competing objectives,
and the use of OOD detection, compare our models with human writers, and
provide samples from our models using prompts appearing in recent related work.",https://github.com/anthropics/hh-rlhf,-1
Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing,0.701912,"We present PHORHUM, a novel, end-to-end trainable, deep neural network
methodology for photorealistic 3D human reconstruction given just a monocular
RGB image. Our pixel-aligned method estimates detailed 3D geometry and, for the
first time, the unshaded surface color together with the scene illumination.
Observing that 3D supervision alone is not sufficient for high fidelity color
reconstruction, we introduce patch-based rendering losses that enable reliable
color reconstruction on visible parts of the human, and detailed and plausible
color estimation for the non-visible parts. Moreover, our method specifically
addresses methodological and practical limitations of prior work in terms of
representing geometry, albedo, and illumination effects, in an end-to-end model
where factors can be effectively disentangled. In extensive experiments, we
demonstrate the versatility and robustness of our approach. Our
state-of-the-art results validate the method qualitatively and for different
metrics, for both geometric and color reconstruction.",None,-1
Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation,0.687172,"In this paper we present Mask DINO, a unified object detection and
segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising
Anchor Boxes) by adding a mask prediction branch which supports all image
segmentation tasks (instance, panoptic, and semantic). It makes use of the
query embeddings from DINO to dot-product a high-resolution pixel embedding map
to predict a set of binary masks. Some key components in DINO are extended for
segmentation through a shared architecture and training process. Mask DINO is
simple, efficient, and scalable, and it can benefit from joint large-scale
detection and segmentation datasets. Our experiments show that Mask DINO
significantly outperforms all existing specialized segmentation methods, both
on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably,
Mask DINO establishes the best results to date on instance segmentation (54.5
AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation
(60.8 mIoU on ADE20K) among models under one billion parameters. Code is
available at \url{https://github.com/IDEACVR/MaskDINO}.",https://github.com/IDEA-Research/MaskDINO,-1
V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer,0.827607,"In this paper, we investigate the application of Vehicle-to-Everything (V2X)
communication to improve the perception performance of autonomous vehicles. We
present a robust cooperative perception framework with V2X communication using
a novel vision Transformer. Specifically, we build a holistic attention model,
namely V2X-ViT, to effectively fuse information across on-road agents (i.e.,
vehicles and infrastructure). V2X-ViT consists of alternating layers of
heterogeneous multi-agent self-attention and multi-scale window self-attention,
which captures inter-agent interaction and per-agent spatial relationships.
These key modules are designed in a unified Transformer architecture to handle
common V2X challenges, including asynchronous information sharing, pose errors,
and heterogeneity of V2X components. To validate our approach, we create a
large-scale V2X perception dataset using CARLA and OpenCDA. Extensive
experimental results demonstrate that V2X-ViT sets new state-of-the-art
performance for 3D object detection and achieves robust performance even under
harsh, noisy environments. The code is available at
https://github.com/DerrickXuNu/v2x-vit.",https://github.com/DerrickXuNu/v2x-vit,-1
Concrete Score Matching: Generalized Score Matching for Discrete Data,0.516264,"Representing probability distributions by the gradient of their density
functions has proven effective in modeling a wide range of continuous data
modalities. However, this representation is not applicable in discrete domains
where the gradient is undefined. To this end, we propose an analogous score
function called the ""Concrete score"", a generalization of the (Stein) score for
discrete settings. Given a predefined neighborhood structure, the Concrete
score of any input is defined by the rate of change of the probabilities with
respect to local directional changes of the input. This formulation allows us
to recover the (Stein) score in continuous domains when measuring such changes
by the Euclidean distance, while using the Manhattan distance leads to our
novel score function in discrete domains. Finally, we introduce a new framework
to learn such scores from samples called Concrete Score Matching (CSM), and
propose an efficient training objective to scale our approach to high
dimensions. Empirically, we demonstrate the efficacy of CSM on density
estimation tasks on a mixture of synthetic, tabular, and high-dimensional image
datasets, and demonstrate that it performs favorably relative to existing
baselines for modeling discrete data.",None,-1
Dialog Inpainting: Turning Documents into Dialogs,0.267734,"Many important questions (e.g. ""How to eat healthier?"") require conversation
to establish context and explore in depth. However, conversational question
answering (ConvQA) systems have long been stymied by scarce training data that
is expensive to collect. To address this problem, we propose a new technique
for synthetically generating diverse and high-quality dialog data: dialog
inpainting. Our approach takes the text of any document and transforms it into
a two-person dialog between the writer and an imagined reader: we treat
sentences from the article as utterances spoken by the writer, and then use a
dialog inpainter to predict what the imagined reader asked or said in between
each of the writer's utterances. By applying this approach to passages from
Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets
totalling 19 million diverse information-seeking dialogs -- 1,000x larger than
the largest existing ConvQA dataset. Furthermore, human raters judge the answer
adequacy and conversationality of WikiDialog to be as good or better than
existing manually-collected datasets. Using our inpainted data to pre-train
ConvQA retrieval systems, we significantly advance state-of-the-art across
three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains
on standard evaluation metrics.",https://github.com/google-research/dialog-inpainting,-1
Compositional Semantic Parsing with Large Language Models,0.516877,"Humans can reason compositionally when presented with new tasks. Previous
research shows that appropriate prompting techniques enable large language
models (LLMs) to solve artificial compositional generalization tasks such as
SCAN. In this work, we identify additional challenges in more realistic
semantic parsing tasks with larger vocabulary and refine these prompting
techniques to address them. Our best method is based on least-to-most
prompting: it decomposes the problem using prompting-based syntactic parsing,
then uses this decomposition to select appropriate exemplars and to
sequentially generate the semantic parse. This method allows us to set a new
state of the art for CFQ while requiring only 1% of the training data used by
traditional approaches. Due to the general nature of our approach, we expect
similar efforts will lead to new results in other tasks and domains, especially
for knowledge-intensive applications.",None,-1
LEVEN: A Large-Scale Chinese Legal Event Detection Dataset,0.272608,"Recognizing facts is the most fundamental step in making judgments, hence
detecting events in the legal documents is important to legal case analysis
tasks. However, existing Legal Event Detection (LED) datasets only concern
incomprehensive event types and have limited annotated data, which restricts
the development of LED methods and their downstream applications. To alleviate
these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection
dataset, with 8,116 legal documents and 150,977 human-annotated event mentions
in 108 event types. Not only charge-related events, LEVEN also covers general
events, which are critical for legal case understanding but neglected in
existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and
has dozens of times the data scale of others, which shall significantly promote
the training and evaluation of LED methods. The results of extensive
experiments indicate that LED is challenging and needs further effort.
Moreover, we simply utilize legal events as side information to promote
downstream applications. The method achieves improvements of average 2.2 points
precision in low-resource judgment prediction, and 1.5 points mean average
precision in unsupervised case retrieval, which suggests the fundamentality of
LED. The source code and dataset can be obtained from
https://github.com/thunlp/LEVEN.",https://github.com/thunlp/LEVEN,-1
SPACE: Speech-driven Portrait Animation with Controllable Expression,0.175364,"Animating portraits using speech has received growing attention in recent
years, with various creative and practical use cases. An ideal generated video
should have good lip sync with the audio, natural facial expressions and head
motions, and high frame quality. In this work, we present SPACE, which uses
speech and a single image to generate high-resolution, and expressive videos
with realistic head pose, without requiring a driving video. It uses a
multi-stage approach, combining the controllability of facial landmarks with
the high-quality synthesis power of a pretrained face generator. SPACE also
allows for the control of emotions and their intensities. Our method
outperforms prior methods in objective metrics for image quality and facial
motions and is strongly preferred by users in pair-wise comparisons. The
project website is available at https://deepimagination.cc/SPACE/",None,-1
The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations,0.114216,"Machine learning models in safety-critical settings like healthcare are often
blackboxes: they contain a large number of parameters which are not transparent
to users. Post-hoc explainability methods where a simple, human-interpretable
model imitates the behavior of these blackbox models are often proposed to help
users trust model predictions. In this work, we audit the quality of such
explanations for different protected subgroups using real data from four
settings in finance, healthcare, college admissions, and the US justice system.
Across two different blackbox model architectures and four popular
explainability methods, we find that the approximation quality of explanation
models, also known as the fidelity, differs significantly between subgroups. We
also demonstrate that pairing explainability methods with recent advances in
robust machine learning can improve explanation fairness in some settings.
However, we highlight the importance of communicating details of non-zero
fidelity gaps to users, since a single solution might not exist across all
settings. Finally, we discuss the implications of unfair explanation models as
a challenging and understudied problem facing the machine learning community.",https://github.com/MLforHealth/ExplanationsSubpopulations,-1
ComFact: A Benchmark for Linking Contextual Commonsense Knowledge,0.981684,"Understanding rich narratives, such as dialogues and stories, often requires
natural language processing systems to access relevant knowledge from
commonsense knowledge graphs. However, these systems typically retrieve facts
from KGs using simple heuristics that disregard the complex challenges of
identifying situationally-relevant commonsense knowledge (e.g.,
contextualization, implicitness, ambiguity).
  In this work, we propose the new task of commonsense fact linking, where
models are given contexts and trained to identify situationally-relevant
commonsense knowledge from KGs. Our novel benchmark, ComFact, contains ~293k
in-context relevance annotations for commonsense triplets across four
stylistically diverse dialogue and storytelling datasets. Experimental results
confirm that heuristic fact linking approaches are imprecise knowledge
extractors. Learned fact linking models demonstrate across-the-board
performance improvements (~34.6% F1) over these heuristics. Furthermore,
improved knowledge retrieval yielded average downstream improvements of 9.8%
for a dialogue response generation task. However, fact linking models still
significantly underperform humans, suggesting our benchmark is a promising
testbed for research in commonsense augmentation of NLP systems.",https://github.com/Silin159/ComFact,-1
Goal-Conditioned Reinforcement Learning: Problems and Solutions,0.457539,"Goal-conditioned reinforcement learning (GCRL), related to a set of complex
RL problems, trains an agent to achieve different goals under particular
scenarios. Compared to the standard RL solutions that learn a policy solely
depending on the states or observations, GCRL additionally requires the agent
to make decisions according to different goals. In this survey, we provide a
comprehensive overview of the challenges and algorithms for GCRL. Firstly, we
answer what the basic problems are studied in this field. Then, we explain how
goals are represented and present how existing solutions are designed from
different points of view. Finally, we make the conclusion and discuss potential
future prospects that recent researches focus on.",https://github.com/apexrl/,-1
Combining Attention Module and Pixel Shuffle for License Plate Super-Resolution,0.495009,"The License Plate Recognition (LPR) field has made impressive advances in the
last decade due to novel deep learning approaches combined with the increased
availability of training data. However, it still has some open issues,
especially when the data come from low-resolution (LR) and low-quality
images/videos, as in surveillance systems. This work focuses on license plate
(LP) reconstruction in LR and low-quality images. We present a Single-Image
Super-Resolution (SISR) approach that extends the attention/transformer module
concept by exploiting the capabilities of PixelShuffle layers and that has an
improved loss function based on LPR predictions. For training the proposed
architecture, we use synthetic images generated by applying heavy Gaussian
noise in terms of Structural Similarity Index Measure (SSIM) to the original
high-resolution (HR) images. In our experiments, the proposed method
outperformed the baselines both quantitatively and qualitatively. The datasets
we created for this work are publicly available to the research community at
https://github.com/valfride/lpr-rsr/",https://github.com/valfride/lpr-rsr/,-1
Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models,0.817992,"Motivations for methods in explainable artificial intelligence (XAI) often
include detecting, quantifying and mitigating bias, and contributing to making
machine learning models fairer. However, exactly how an XAI method can help in
combating biases is often left unspecified. In this paper, we briefly review
trends in explainability and fairness in NLP research, identify the current
practices in which explainability methods are applied to detect and mitigate
bias, and investigate the barriers preventing XAI methods from being used more
widely in tackling fairness issues.",None,-1
Overview of the HASOC Subtrack at FIRE 2022: Offensive Language Identification in Marathi,0.2687,"The widespread of offensive content online has become a reason for great
concern in recent years, motivating researchers to develop robust systems
capable of identifying such content automatically. With the goal of carrying
out a fair evaluation of these systems, several international competitions have
been organized, providing the community with important benchmark data and
evaluation methods for various languages. Organized since 2019, the HASOC (Hate
Speech and Offensive Content Identification) shared task is one of these
initiatives. In its fourth iteration, HASOC 2022 included three subtracks for
English, Hindi, and Marathi. In this paper, we report the results of the HASOC
2022 Marathi subtrack which provided participants with a dataset containing
data from Twitter manually annotated using the popular OLID taxonomy. The
Marathi track featured three additional subtracks, each corresponding to one
level of the taxonomy: Task A - offensive content identification (offensive vs.
non-offensive); Task B - categorization of offensive types (targeted vs.
untargeted), and Task C - offensive target identification (individual vs. group
vs. others). Overall, 59 runs were submitted by 10 teams. The best systems
obtained an F1 of 0.9745 for Subtrack 3A, an F1 of 0.9207 for Subtrack 3B, and
F1 of 0.9607 for Subtrack 3C. The best performing algorithms were a mixture of
traditional and deep learning approaches.",None,-1
"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",0.782825,"The formalization of existing mathematical proofs is a notoriously difficult
process. Despite decades of research on automation and proof assistants,
writing formal proofs remains arduous and only accessible to a few experts.
While previous studies to automate formalization focused on powerful search
algorithms, no attempts were made to take advantage of available informal
proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method
that maps informal proofs to formal proof sketches, and uses the sketches to
guide an automated prover by directing its search to easier sub-problems. We
investigate two relevant setups where informal proofs are either written by
humans or generated by a language model. Our experiments and ablation studies
show that large language models are able to produce well-structured formal
sketches that follow the same reasoning steps as the informal proofs. Guiding
an automated prover with these sketches enhances its performance from 20.9% to
39.3% on a collection of mathematical competition problems.",None,-1
KSG: Knowledge and Skill Graph,0.231202,"The knowledge graph (KG) is an essential form of knowledge representation
that has grown in prominence in recent years. Because it concentrates on
nominal entities and their relationships, traditional knowledge graphs are
static and encyclopedic in nature. On this basis, event knowledge graph (Event
KG) models the temporal and spatial dynamics by text processing to facilitate
downstream applications, such as question-answering, recommendation and
intelligent search. Existing KG research, on the other hand, mostly focuses on
text processing and static facts, ignoring the vast quantity of dynamic
behavioral information included in photos, movies, and pre-trained neural
networks. In addition, no effort has been done to include behavioral
intelligence information into the knowledge graph for deep reinforcement
learning (DRL) and robot learning. In this paper, we propose a novel dynamic
knowledge and skill graph (KSG), and then we develop a basic and specific KSG
based on CN-DBpedia. The nodes are divided into entity and attribute nodes,
with entity nodes containing the agent, environment, and skill (DRL policy or
policy representation), and attribute nodes containing the entity description,
pre-train network, and offline dataset. KSG can search for different agents'
skills in various environments and provide transferable information for
acquiring new skills. This is the first study that we are aware of that looks
into dynamic KSG for skill retrieval and learning. Extensive experimental
results on new skill learning show that KSG boosts new skill learning
efficiency.",None,-1
Mental Disorders on Online Social Media Through the Lens of Language and Behaviour: Analysis and Visualisation,0.151935,"Due to the worldwide accessibility to the Internet along with the continuous
advances in mobile technologies, physical and digital worlds have become
completely blended, and the proliferation of social media platforms has taken a
leading role over this evolution. In this paper, we undertake a thorough
analysis towards better visualising and understanding the factors that
characterise and differentiate social media users affected by mental disorders.
We perform different experiments studying multiple dimensions of language,
including vocabulary uniqueness, word usage, linguistic style, psychometric
attributes, emotions' co-occurrence patterns, and online behavioural traits,
including social engagement and posting trends. Our findings reveal significant
differences on the use of function words, such as adverbs and verb tense, and
topic-specific vocabulary, such as biological processes. As for emotional
expression, we observe that affected users tend to share emotions more
regularly than control individuals on average. Overall, the monthly posting
variance of the affected groups is higher than the control groups. Moreover, we
found evidence suggesting that language use on micro-blogging platforms is less
distinguishable for users who have a mental disorder than other less
restrictive platforms. In particular, we observe on Twitter less quantifiable
differences between affected and control groups compared to Reddit.",None,-1
Motion Transformer with Global Intention Localization and Local Movement Refinement,0.435482,"Predicting multimodal future behavior of traffic participants is essential
for robotic vehicles to make safe decisions. Existing works explore to directly
predict future trajectories based on latent features or utilize dense goal
candidates to identify agent's destinations, where the former strategy
converges slowly since all motion modes are derived from the same feature while
the latter strategy has efficiency issue since its performance highly relies on
the density of goal candidates. In this paper, we propose Motion TRansformer
(MTR) framework that models motion prediction as the joint optimization of
global intention localization and local movement refinement. Instead of using
goal candidates, MTR incorporates spatial intention priors by adopting a small
set of learnable motion query pairs. Each motion query pair takes charge of
trajectory prediction and refinement for a specific motion mode, which
stabilizes the training process and facilitates better multimodal predictions.
Experiments show that MTR achieves state-of-the-art performance on both the
marginal and joint motion prediction challenges, ranking 1st on the
leaderboards of Waymo Open Motion Dataset. The source code is available at
https://github.com/sshaoshuai/MTR.",https://github.com/sshaoshuai/MTR,-1
Fine-Grained Scene Graph Generation with Data Transfer,0.752083,"Scene graph generation (SGG) is designed to extract (subject, predicate,
object) triplets in images. Recent works have made a steady progress on SGG,
and provide useful tools for high-level vision and language understanding.
However, due to the data distribution problems including long-tail distribution
and semantic ambiguity, the predictions of current SGG models tend to collapse
to several frequent but uninformative predicates (e.g., on, at), which limits
practical application of these models in downstream tasks. To deal with the
problems above, we propose a novel Internal and External Data Transfer
(IETrans) method, which can be applied in a plug-and-play fashion and expanded
to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the
data distribution problem by automatically creating an enhanced dataset that
provides more sufficient and coherent annotations for all predicates. By
training on the enhanced dataset, a Neural Motif model doubles the macro
performance while maintaining competitive micro performance. The code and data
are publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch.",https://github.com/waxnkw/IETrans-SGG.pytorch,-1
Robust Speech Recognition via Large-Scale Weak Supervision,0.999121,"We study the capabilities of speech processing systems trained simply to
predict large amounts of transcripts of audio on the internet. When scaled to
680,000 hours of multilingual and multitask supervision, the resulting models
generalize well to standard benchmarks and are often competitive with prior
fully supervised results but in a zero-shot transfer setting without the need
for any fine-tuning. When compared to humans, the models approach their
accuracy and robustness. We are releasing models and inference code to serve as
a foundation for further work on robust speech processing.",https://github.com/openai/whisper,-1
Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning,0.149254,"Providing Emotional Support (ES) to soothe people in emotional distress is an
essential capability in social interactions. Most existing researches on
building ES conversation systems only considered single-turn interactions with
users, which was over-simplified. In comparison, multi-turn ES conversation
systems can provide ES more effectively, but face several new technical
challenges, including: (1) how to adopt appropriate support strategies to
achieve the long-term dialogue goal of comforting the user's emotion; (2) how
to dynamically model the user's state. In this paper, we propose a novel system
MultiESC to address these issues. For strategy planning, drawing inspiration
from the A* search algorithm, we propose lookahead heuristics to estimate the
future user feedback after using particular strategies, which helps to select
strategies that can lead to the best long-term effects. For user state
modeling, MultiESC focuses on capturing users' subtle emotional expressions and
understanding their emotion causes. Extensive experiments show that MultiESC
significantly outperforms competitive baselines in both dialogue generation and
strategy planning. Our codes are available at
https://github.com/lwgkzl/MultiESC.",https://github.com/lwgkzl/MultiESC,-1
Robust Calibration with Multi-domain Temperature Scaling,0.189629,"Uncertainty quantification is essential for the reliable deployment of
machine learning models to high-stakes application domains. Uncertainty
quantification is all the more challenging when training distribution and test
distribution are different, even the distribution shifts are mild. Despite the
ubiquity of distribution shifts in real-world applications, existing
uncertainty quantification approaches mainly study the in-distribution setting
where the train and test distributions are the same. In this paper, we develop
a systematic calibration model to handle distribution shifts by leveraging data
from multiple domains. Our proposed method -- multi-domain temperature scaling
-- uses the heterogeneity in the domains to improve calibration robustness
under distribution shift. Through experiments on three benchmark data sets, we
find our proposed method outperforms existing methods as measured on both
in-distribution and out-of-distribution test sets.",None,-1
Multimodal Hate Speech Detection from Bengali Memes and Texts,0.367947,"Numerous machine learning (ML) and deep learning (DL)-based approaches have
been proposed to utilize textual data from social media for anti-social
behavior analysis like cyberbullying, fake news detection, and identification
of hate speech mainly for highly-resourced languages such as English. However,
despite having a lot of diversity and millions of native speakers, some
languages like Bengali are under-resourced, which is due to a lack of
computational resources for natural language processing (NLP). Similar to other
languages, Bengali social media contents also include images along with texts
(e.g., multimodal memes are posted by embedding short texts into images on
Facebook). Therefore, only the textual data is not enough to judge them since
images might give extra context to make a proper judgement. This paper is about
hate speech detection from multimodal Bengali memes and texts. We prepared the
only multimodal hate speech dataset for-a-kind of problem for Bengali, which we
use to train state-of-the-art neural architectures (e.g., Bi-LSTM/Conv-LSTM
with word embeddings, ConvNets + pre-trained language models, e.g., monolingual
Bangla BERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) to jointly
analyze textual and visual information for hate speech detection. Conv-LSTM and
XLM-RoBERTa models performed best for texts, yielding F1 scores of 0.78 and
0.82, respectively. As of memes, ResNet-152 and DenseNet-161 models yield F1
scores of 0.78 and 0.79, respectively. As for multimodal fusion, XLM-RoBERTa +
DenseNet-161 performed the best, yielding an F1 score of 0.83. Our study
suggests that text modality is most useful for hate speech detection, while
memes are moderately useful.",https://github.com/rezacsedu/Multimodal-Hate-Speech-Bengali,-1
SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations,0.721929,"Accurate mapping of large-scale environments is an essential building block
of most outdoor autonomous systems. Challenges of traditional mapping methods
include the balance between memory consumption and mapping accuracy. This paper
addresses the problem of achieving large-scale 3D reconstruction using implicit
representations built from 3D LiDAR measurements. We learn and store implicit
features through an octree-based, hierarchical structure, which is sparse and
extensible. The implicit features can be turned into signed distance values
through a shallow neural network. We leverage binary cross entropy loss to
optimize the local features with the 3D measurements as supervision. Based on
our implicit representation, we design an incremental mapping system with
regularization to tackle the issue of forgetting in continual learning. Our
experiments show that our 3D reconstructions are more accurate, complete, and
memory-efficient than current state-of-the-art 3D mapping methods.",https://github.com/PRBonn/SHINE_mapping,-1
On Pathologies in KL-Regularized Reinforcement Learning from Expert Demonstrations,0.0953366,"KL-regularized reinforcement learning from expert demonstrations has proved
successful in improving the sample efficiency of deep reinforcement learning
algorithms, allowing them to be applied to challenging physical real-world
tasks. However, we show that KL-regularized reinforcement learning with
behavioral reference policies derived from expert demonstrations can suffer
from pathological training dynamics that can lead to slow, unstable, and
suboptimal online learning. We show empirically that the pathology occurs for
commonly chosen behavioral policy classes and demonstrate its impact on sample
efficiency and online policy performance. Finally, we show that the pathology
can be remedied by non-parametric behavioral reference policies and that this
allows KL-regularized reinforcement learning to significantly outperform
state-of-the-art approaches on a variety of challenging locomotion and
dexterous hand manipulation tasks.",https://sites.google.com/view/nppac,-1
Indoor Localization for Personalized Ambient Assisted Living of Multiple Users in Multi-Floor Smart Environments,0.146213,"This paper presents a multifunctional interdisciplinary framework that makes
four scientific contributions towards the development of personalized ambient
assisted living, with a specific focus to address the different and dynamic
needs of the diverse aging population in the future of smart living
environments. First, it presents a probabilistic reasoning-based mathematical
approach to model all possible forms of user interactions for any activity
arising from the user diversity of multiple users in such environments. Second,
it presents a system that uses this approach with a machine learning method to
model individual user profiles and user-specific user interactions for
detecting the dynamic indoor location of each specific user. Third, to address
the need to develop highly accurate indoor localization systems for increased
trust, reliance, and seamless user acceptance, the framework introduces a novel
methodology where two boosting approaches Gradient Boosting and the AdaBoost
algorithm are integrated and used on a decision tree-based learning model to
perform indoor localization. Fourth, the framework introduces two novel
functionalities to provide semantic context to indoor localization in terms of
detecting each user's floor-specific location as well as tracking whether a
specific user was located inside or outside a given spatial region in a
multi-floor-based indoor setting. These novel functionalities of the proposed
framework were tested on a dataset of localization-related Big Data collected
from 18 different users who navigated in 3 buildings consisting of 5 floors and
254 indoor spatial regions. The results show that this approach of indoor
localization for personalized AAL that models each specific user always
achieves higher accuracy as compared to the traditional approach of modeling an
average user.",None,-1
Generalized Differentiable RANSAC,0.0890393,"We propose $\nabla$-RANSAC, a generalized differentiable RANSAC that allows
learning the entire randomized robust estimation pipeline. The proposed
approach enables the use of relaxation techniques for estimating the gradients
in the sampling distribution, which are then propagated through a
differentiable solver. The trainable quality function marginalizes over the
scores from all the models estimated within $\nabla$-RANSAC to guide the
network learning accurate and useful inlier probabilities or to train feature
detection and matching networks. Our method directly maximizes the probability
of drawing a good hypothesis, allowing us to learn better sampling
distributions. We test $\nabla$-RANSAC on various real-world scenarios on
fundamental and essential matrix estimation, and 3D point cloud registration,
outdoors and indoors, with handcrafted and learning-based features. It is
superior to the state-of-the-art in terms of accuracy while running at a
similar speed to its less accurate alternatives. The code and trained models
are available at https://github.com/weitong8591/differentiable_ransac.",https://github.com/weitong8591/differentiable_ransac,-1
Interactive Concept Bottleneck Models,0.124159,"Concept bottleneck models (CBMs) are interpretable neural networks that first
predict labels for human-interpretable concepts relevant to the prediction
task, and then predict the final label based on the concept label predictions.
We extend CBMs to interactive prediction settings where the model can query a
human collaborator for the label to some concepts. We develop an interaction
policy that, at prediction time, chooses which concepts to request a label for
so as to maximally improve the final prediction. We demonstrate that a simple
policy combining concept prediction uncertainty and influence of the concept on
the final prediction achieves strong performance and outperforms static
approaches as well as active feature acquisition methods proposed in the
literature. We show that the interactive CBM can achieve accuracy gains of
5-10% with only 5 interactions over competitive baselines on the Caltech-UCSD
Birds, CheXpert and OAI datasets.",https://github.com/google-research/google-research/tree/master/interactive_cbms,-1
Synthetic Disinformation Attacks on Automated Fact Verification Systems,0.369925,"Automated fact-checking is a needed technology to curtail the spread of
online misinformation. One current framework for such solutions proposes to
verify claims by retrieving supporting or refuting evidence from related
textual sources. However, the realistic use cases for fact-checkers will
require verifying claims against evidence sources that could be affected by the
same misinformation. Furthermore, the development of modern NLP tools that can
produce coherent, fabricated content would allow malicious actors to
systematically generate adversarial disinformation for fact-checkers.
  In this work, we explore the sensitivity of automated fact-checkers to
synthetic adversarial evidence in two simulated settings: AdversarialAddition,
where we fabricate documents and add them to the evidence repository available
to the fact-checking system, and AdversarialModification, where existing
evidence source documents in the repository are automatically altered. Our
study across multiple models on three benchmarks demonstrates that these
systems suffer significant performance drops against these attacks. Finally, we
discuss the growing threat of modern NLG systems as generators of
disinformation in the context of the challenges they pose to automated
fact-checkers.",https://github.com/Yibing-Du/adversarial-factcheck,-1
MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model,0.869449,"Human motion modeling is important for many modern graphics applications,
which typically require professional skills. In order to remove the skill
barriers for laymen, recent motion generation methods can directly generate
human motions conditioned on natural languages. However, it remains challenging
to achieve diverse and fine-grained motion generation with various text inputs.
To address this problem, we propose MotionDiffuse, the first diffusion
model-based text-driven motion generation framework, which demonstrates several
desired properties over existing methods. 1) Probabilistic Mapping. Instead of
a deterministic language-motion mapping, MotionDiffuse generates motions
through a series of denoising steps in which variations are injected. 2)
Realistic Synthesis. MotionDiffuse excels at modeling complicated data
distribution and generating vivid motion sequences. 3) Multi-Level
Manipulation. MotionDiffuse responds to fine-grained instructions on body
parts, and arbitrary-length motion synthesis with time-varied text prompts. Our
experiments show MotionDiffuse outperforms existing SoTA methods by convincing
margins on text-driven motion generation and action-conditioned motion
generation. A qualitative analysis further demonstrates MotionDiffuse's
controllability for comprehensive motion generation. Homepage:
https://mingyuan-zhang.github.io/projects/MotionDiffuse.html",https://mingyuan-zhang.github.io/projects/MotionDiffuse.html,-1
RA-Depth: Resolution Adaptive Self-Supervised Monocular Depth Estimation,0.104697,"Existing self-supervised monocular depth estimation methods can get rid of
expensive annotations and achieve promising results. However, these methods
suffer from severe performance degradation when directly adopting a model
trained on a fixed resolution to evaluate at other different resolutions. In
this paper, we propose a resolution adaptive self-supervised monocular depth
estimation method (RA-Depth) by learning the scale invariance of the scene
depth. Specifically, we propose a simple yet efficient data augmentation method
to generate images with arbitrary scales for the same scene. Then, we develop a
dual high-resolution network that uses the multi-path encoder and decoder with
dense interactions to aggregate multi-scale features for accurate depth
inference. Finally, to explicitly learn the scale invariance of the scene
depth, we formulate a cross-scale depth consistency loss on depth predictions
with different scales. Extensive experiments on the KITTI, Make3D and NYU-V2
datasets demonstrate that RA-Depth not only achieves state-of-the-art
performance, but also exhibits a good ability of resolution adaptation.",https://github.com/hmhemu/RA-Depth,-1
On the Privacy Properties of GAN-generated Samples,0.143137,"The privacy implications of generative adversarial networks (GANs) are a
topic of great interest, leading to several recent algorithms for training GANs
with privacy guarantees. By drawing connections to the generalization
properties of GANs, we prove that under some assumptions, GAN-generated samples
inherently satisfy some (weak) privacy guarantees. First, we show that if a GAN
is trained on m samples and used to generate n samples, the generated samples
are (epsilon, delta)-differentially-private for (epsilon, delta) pairs where
delta scales as O(n/m). We show that under some special conditions, this upper
bound is tight. Next, we study the robustness of GAN-generated samples to
membership inference attacks. We model membership inference as a hypothesis
test in which the adversary must determine whether a given sample was drawn
from the training dataset or from the underlying data distribution. We show
that this adversary can achieve an area under the ROC curve that scales no
better than O(m^{-1/4}).",None,-1
Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,0.0841114,"Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create ""latent saliency maps"" that can help explain predictions
in human terms.",None,-1
Adversarial Motion Priors Make Good Substitutes for Complex Reward Functions,0.210278,"Training a high-dimensional simulated agent with an under-specified reward
function often leads the agent to learn physically infeasible strategies that
are ineffective when deployed in the real world. To mitigate these unnatural
behaviors, reinforcement learning practitioners often utilize complex reward
functions that encourage physically plausible behaviors. However, a tedious
labor-intensive tuning process is often required to create hand-designed
rewards which might not easily generalize across platforms and tasks. We
propose substituting complex reward functions with ""style rewards"" learned from
a dataset of motion capture demonstrations. A learned style reward can be
combined with an arbitrary task reward to train policies that perform tasks
using naturalistic strategies. These natural strategies can also facilitate
transfer to the real world. We build upon Adversarial Motion Priors -- an
approach from the computer graphics domain that encodes a style reward from a
dataset of reference motions -- to demonstrate that an adversarial approach to
training policies can produce behaviors that transfer to a real quadrupedal
robot without requiring complex reward functions. We also demonstrate that an
effective style reward can be learned from a few seconds of motion capture data
gathered from a German Shepherd and leads to energy-efficient locomotion
strategies with natural gait transitions.",https://bit.ly/3hpvbD6,-1
TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval,0.336976,"Text-Video retrieval is a task of great practical value and has received
increasing attention, among which learning spatial-temporal video
representation is one of the research hotspots. The video encoders in the
state-of-the-art video retrieval models usually directly adopt the pre-trained
vision backbones with the network structure fixed, they therefore can not be
further improved to produce the fine-grained spatial-temporal video
representation. In this paper, we propose Token Shift and Selection Network
(TS2-Net), a novel token shift and selection transformer architecture, which
dynamically adjusts the token sequence and selects informative tokens in both
temporal and spatial dimensions from input video samples. The token shift
module temporally shifts the whole token features back-and-forth across
adjacent frames, to preserve the complete token representation and capture
subtle movements. Then the token selection module selects tokens that
contribute most to local spatial semantics. Based on thorough experiments, the
proposed TS2-Net achieves state-of-the-art performance on major text-video
retrieval benchmarks, including new records on MSRVTT, VATEX, LSMDC,
ActivityNet, and DiDeMo.",https://github.com/yuqi657/ts2_net,-1
TableFormer: Robust Transformer Modeling for Table-Text Encoding,0.304676,"Understanding tables is an important aspect of natural language
understanding. Existing models for table understanding require linearization of
the table structure, where row or column order is encoded as an unwanted bias.
Such spurious biases make the model vulnerable to row and column order
perturbations. Additionally, prior work has not thoroughly modeled the table
structures or table-text alignments, hindering the table-text understanding
ability. In this work, we propose a robust and structurally aware table-text
encoding architecture TableFormer, where tabular structural biases are
incorporated completely through learnable attention biases. TableFormer is (1)
strictly invariant to row and column orders, and, (2) could understand tables
better due to its tabular inductive biases. Our evaluations showed that
TableFormer outperforms strong baselines in all settings on SQA, WTQ and
TabFact table reasoning datasets, and achieves state-of-the-art performance on
SQA, especially when facing answer-invariant row and column order perturbations
(6% improvement over the best baseline), because previous SOTA models'
performance drops by 4% - 6% when facing such perturbations while TableFormer
is not affected.",https://github.com/google-research/tapas/blob/master/TABLEFORMER.md,-1
"Pruned RNN-T for fast, memory-efficient ASR training",0.856165,"The RNN-Transducer (RNN-T) framework for speech recognition has been growing
in popularity, particularly for deployed real-time ASR systems, because it
combines high accuracy with naturally streaming recognition. One of the
drawbacks of RNN-T is that its loss function is relatively slow to compute, and
can use a lot of memory. Excessive GPU memory usage can make it impractical to
use RNN-T loss in cases where the vocabulary size is large: for example, for
Chinese character-based ASR. We introduce a method for faster and more
memory-efficient RNN-T loss computation. We first obtain pruning bounds for the
RNN-T recursion using a simple joiner network that is linear in the encoder and
decoder embeddings; we can evaluate this without using much memory. We then use
those pruning bounds to evaluate the full, non-linear joiner network.",https://github.com/k2-fsa/k2,-1
Studying Bias in GANs through the Lens of Race,0.0,"In this work, we study how the performance and evaluation of generative image
models are impacted by the racial composition of their training datasets. By
examining and controlling the racial distributions in various training
datasets, we are able to observe the impacts of different training
distributions on generated image quality and the racial distributions of the
generated images. Our results show that the racial compositions of generated
images successfully preserve that of the training data. However, we observe
that truncation, a technique used to generate higher quality images during
inference, exacerbates racial imbalances in the data. Lastly, when examining
the relationship between image quality and race, we find that the highest
perceived visual quality images of a given race come from a distribution where
that race is well-represented, and that annotators consistently prefer
generated images of white people over those of Black people.",None,-1
On Vision Features in Multimodal Machine Translation,0.25307,"Previous work on multimodal machine translation (MMT) has focused on the way
of incorporating vision features into translation but little attention is on
the quality of vision models. In this work, we investigate the impact of vision
models on MMT. Given the fact that Transformer is becoming popular in computer
vision, we experiment with various strong models (such as Vision Transformer)
and enhanced features (such as object-detection and image captioning). We
develop a selective attention model to study the patch-level contribution of an
image in MMT. On detailed probing tasks, we find that stronger vision models
are helpful for learning translation from the visual modality. Our results also
suggest the need of carefully examining MMT models, especially when current
benchmarks are small-scale and biased. Our code could be found at
\url{https://github.com/libeineu/fairseq_mmt}.",https://github.com/libeineu/fairseq_mmt,-1
Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning,0.996733,"We present Bit Diffusion: a simple and generic approach for generating
discrete data with continuous state and continuous time diffusion models. The
main idea behind our approach is to first represent the discrete data as binary
bits, and then train a continuous diffusion model to model these bits as real
numbers which we call analog bits. To generate samples, the model first
generates the analog bits, which are then thresholded to obtain the bits that
represent the discrete variables. We further propose two simple techniques,
namely Self-Conditioning and Asymmetric Time Intervals, which lead to a
significant improvement in sample quality. Despite its simplicity, the proposed
approach can achieve strong performance in both discrete image generation and
image captioning tasks. For discrete image generation, we significantly improve
previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens)
and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the
best autoregressive model in both sample quality (measured by FID) and
efficiency. For image captioning on MS-COCO dataset, our approach achieves
competitive results compared to autoregressive models.",https://github.com/google-research/pix2seq,-1
SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model,0.981684,"Generic image inpainting aims to complete a corrupted image by borrowing
surrounding information, which barely generates novel content. By contrast,
multi-modal inpainting provides more flexible and useful controls on the
inpainted content, \eg, a text prompt can be used to describe an object with
richer attributes, and a mask can be used to constrain the shape of the
inpainted object rather than being only considered as a missing area. We
propose a new diffusion-based model named SmartBrush for completing a missing
region with an object using both text and shape-guidance. While previous work
such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not
support shape guidance and tend to modify background texture surrounding the
generated object. Our model incorporates both text and shape guidance with
precision control. To preserve the background better, we propose a novel
training and sampling strategy by augmenting the diffusion U-net with
object-mask prediction. Lastly, we introduce a multi-task training strategy by
jointly training inpainting with text-to-image generation to leverage more
training data. We conduct extensive experiments showing that our model
outperforms all baselines in terms of visual quality, mask controllability, and
background preservation.",https://github.com/CompVis/stable-diffusion,-1
Reducing the Vision and Language Bias for Temporal Sentence Grounding,0.349208,"Temporal sentence grounding (TSG) is an important yet challenging task in
multimedia information retrieval. Although previous TSG methods have achieved
decent performance, they tend to capture the selection biases of frequently
appeared video-query pairs in the dataset rather than present robust multimodal
reasoning abilities, especially for the rarely appeared pairs. In this paper,
we study the above issue of selection biases and accordingly propose a
Debiasing-TSG (D-TSG) model to filter and remove the negative biases in both
vision and language modalities for enhancing the model generalization ability.
Specifically, we propose to alleviate the issue from two perspectives: 1)
Feature distillation. We built a multi-modal debiasing branch to firstly
capture the vision and language biases, and then apply a bias identification
module to explicitly recognize the true negative biases and remove them from
the benign multi-modal representations. 2) Contrastive sample generation. We
construct two types of negative samples to enforce the model to accurately
learn the aligned multi-modal semantics and make complete semantic reasoning.
We apply the proposed model to both commonly and rarely appeared TSG cases, and
demonstrate its effectiveness by achieving the state-of-the-art performance on
three benchmark datasets (ActivityNet Caption, TACoS, and Charades-STA).",None,-1
COLD: A Benchmark for Chinese Offensive Language Detection,0.896621,"Offensive language detection is increasingly crucial for maintaining a
civilized social media platform and deploying pre-trained language models.
However, this task in Chinese is still under exploration due to the scarcity of
reliable datasets. To this end, we propose a benchmark --COLD for Chinese
offensive language analysis, including a Chinese Offensive Language Dataset
--COLDATASET and a baseline detector --COLDETECTOR which is trained on the
dataset. We show that the COLD benchmark contributes to Chinese offensive
language detection which is challenging for existing resources. We then deploy
the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained
language models. We first analyze the offensiveness of existing generative
models and show that these models inevitably expose varying degrees of
offensive issues. Furthermore, we investigate the factors that influence the
offensive generations, and we find that anti-bias contents and keywords
referring to certain groups or revealing negative attitudes trigger offensive
outputs easier.",https://github.com/thu-coai/COLDataset,-1
Scaling Up Probabilistic Circuits by Latent Variable Distillation,0.0418309,"Probabilistic Circuits (PCs) are a unified framework for tractable
probabilistic models that support efficient computation of various
probabilistic queries (e.g., marginal probabilities). One key challenge is to
scale PCs to model large and high-dimensional real-world datasets: we observe
that as the number of parameters in PCs increases, their performance
immediately plateaus. This phenomenon suggests that the existing optimizers
fail to exploit the full expressive power of large PCs. We propose to overcome
such bottleneck by latent variable distillation: we leverage the less tractable
but more expressive deep generative models to provide extra supervision over
the latent variables of PCs. Specifically, we extract information from
Transformer-based generative models to assign values to latent variables of
PCs, providing guidance to PC optimizers. Experiments on both image and
language modeling benchmarks (e.g., ImageNet and WikiText-2) show that latent
variable distillation substantially boosts the performance of large PCs
compared to their counterparts without latent variable distillation. In
particular, on the image modeling benchmarks, PCs achieve competitive
performance against some of the widely-used deep generative models, including
variational autoencoders and flow-based models, opening up new avenues for
tractable generative modeling.",https://github.com/facebookresearch/mae,-1
Sequential Causal Imitation Learning with Unobserved Confounders,0.250952,"""Monkey see monkey do"" is an age-old adage, referring to na\""ive imitation
without a deep understanding of a system's underlying mechanics. Indeed, if a
demonstrator has access to information unavailable to the imitator (monkey),
such as a different set of sensors, then no matter how perfectly the imitator
models its perceived environment (See), attempting to reproduce the
demonstrator's behavior (Do) can lead to poor outcomes. Imitation learning in
the presence of a mismatch between demonstrator and imitator has been studied
in the literature under the rubric of causal imitation learning (Zhang et al.,
2020), but existing solutions are limited to single-stage decision-making. This
paper investigates the problem of causal imitation learning in sequential
settings, where the imitator must make multiple decisions per episode. We
develop a graphical criterion that is necessary and sufficient for determining
the feasibility of causal imitation, providing conditions when an imitator can
match a demonstrator's performance despite differing capabilities. Finally, we
provide an efficient algorithm for determining imitability and corroborate our
theory with simulations.",None,-1
Improving the Factual Correctness of Radiology Report Generation with Semantic Rewards,0.2822,"Neural image-to-text radiology report generation systems offer the potential
to improve radiology reporting by reducing the repetitive process of report
drafting and identifying possible medical errors. These systems have achieved
promising performance as measured by widely used NLG metrics such as BLEU and
CIDEr. However, the current systems face important limitations. First, they
present an increased complexity in architecture that offers only marginal
improvements on NLG metrics. Secondly, these systems that achieve high
performance on these metrics are not always factually complete or consistent
due to both inadequate training and evaluation. Recent studies have shown the
systems can be substantially improved by using new methods encouraging 1) the
generation of domain entities consistent with the reference and 2) describing
these entities in inferentially consistent ways. So far, these methods rely on
weakly-supervised approaches (rule-based) and named entity recognition systems
that are not specific to the chest X-ray domain. To overcome this limitation,
we propose a new method, the RadGraph reward, to further improve the factual
completeness and correctness of generated radiology reports. More precisely, we
leverage the RadGraph dataset containing annotated chest X-ray reports with
entities and relations between entities. On two open radiology report datasets,
our system substantially improves the scores up to 14.2% and 25.3% on metrics
evaluating the factual correctness and completeness of reports.",https://github.com/jbdel/vilmedic,-1
MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer,0.950213,"Self-supervised monocular depth estimation is an attractive solution that
does not require hard-to-source depth labels for training. Convolutional neural
networks (CNNs) have recently achieved great success in this task. However,
their limited receptive field constrains existing network architectures to
reason only locally, dampening the effectiveness of the self-supervised
paradigm. In the light of the recent successes achieved by Vision Transformers
(ViTs), we propose MonoViT, a brand-new framework combining the global
reasoning enabled by ViT models with the flexibility of self-supervised
monocular depth estimation. By combining plain convolutions with Transformer
blocks, our model can reason locally and globally, yielding depth prediction at
a higher level of detail and accuracy, allowing MonoViT to achieve
state-of-the-art performance on the established KITTI dataset. Moreover,
MonoViT proves its superior generalization capacities on other datasets such as
Make3D and DrivingStereo.",https://github.com/zxcqlf/MonoViT,-1
Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos,0.125714,"Understanding dynamic hand motions and actions from egocentric RGB videos is
a fundamental yet challenging task due to self-occlusion and ambiguity. To
address occlusion and ambiguity, we develop a transformer-based framework to
exploit temporal information for robust estimation. Noticing the different
temporal granularity of and the semantic correlation between hand pose
estimation and action recognition, we build a network hierarchy with two
cascaded transformer encoders, where the first one exploits the short-term
temporal cue for hand pose estimation, and the latter aggregates per-frame pose
and object information over a longer time span to recognize the action. Our
approach achieves competitive results on two first-person hand action
benchmarks, namely FPHA and H2O. Extensive ablation studies verify our design
choices.",https://github.com/fylwen/HTT,-1
Artificial Intelligence and Auction Design,0.213981,"Motivated by online advertising auctions, we study auction design in repeated
auctions played by simple Artificial Intelligence algorithms (Q-learning). We
find that first-price auctions with no additional feedback lead to
tacit-collusive outcomes (bids lower than values), while second-price auctions
do not. We show that the difference is driven by the incentive in first-price
auctions to outbid opponents by just one bid increment. This facilitates
re-coordination on low bids after a phase of experimentation. We also show that
providing information about lowest bid to win, as introduced by Google at the
time of switch to first-price auctions, increases competitiveness of auctions.",None,-1
Hierarchical Normalization for Robust Monocular Depth Estimation,0.0720134,"In this paper, we address monocular depth estimation with deep neural
networks. To enable training of deep monocular estimation models with various
sources of datasets, state-of-the-art methods adopt image-level normalization
strategies to generate affine-invariant depth representations. However,
learning with image-level normalization mainly emphasizes the relations of
pixel representations with the global statistic in the images, such as the
structure of the scene, while the fine-grained depth difference may be
overlooked. In this paper, we propose a novel multi-scale depth normalization
method that hierarchically normalizes the depth representations based on
spatial information and depth distributions. Compared with previous
normalization strategies applied only at the holistic image level, the proposed
hierarchical normalization can effectively preserve the fine-grained details
and improve accuracy. We present two strategies that define the hierarchical
normalization contexts in the depth domain and the spatial domain,
respectively. Our extensive experiments show that the proposed normalization
strategy remarkably outperforms previous normalization methods, and we set new
state-of-the-art on five zero-shot transfer benchmark datasets.",None,-1
DeepInteraction: 3D Object Detection via Modality Interaction,0.487218,"Existing top-performance 3D object detectors typically rely on the
multi-modal fusion strategy. This design is however fundamentally restricted
due to overlooking the modality-specific useful information and finally
hampering the model performance. To address this limitation, in this work we
introduce a novel modality interaction strategy where individual per-modality
representations are learned and maintained throughout for enabling their unique
characteristics to be exploited during object detection. To realize this
proposed strategy, we design a DeepInteraction architecture characterized by a
multi-modal representational interaction encoder and a multi-modal predictive
interaction decoder. Experiments on the large-scale nuScenes dataset show that
our proposed method surpasses all prior arts often by a large margin.
Crucially, our method is ranked at the first position at the highly competitive
nuScenes object detection leaderboard.",https://github.com/fudan-zvg/DeepInteraction,-1
Multi-class Token Transformer for Weakly Supervised Semantic Segmentation,0.959163,"This paper proposes a new transformer-based framework to learn class-specific
object localization maps as pseudo labels for weakly supervised semantic
segmentation (WSSS). Inspired by the fact that the attended regions of the
one-class token in the standard vision transformer can be leveraged to form a
class-agnostic localization map, we investigate if the transformer model can
also effectively capture class-specific attention for more discriminative
object localization by learning multiple class tokens within the transformer.
To this end, we propose a Multi-class Token Transformer, termed as MCTformer,
which uses multiple class tokens to learn interactions between the class tokens
and the patch tokens. The proposed MCTformer can successfully produce
class-discriminative object localization maps from class-to-patch attentions
corresponding to different class tokens. We also propose to use a patch-level
pairwise affinity, which is extracted from the patch-to-patch transformer
attention, to further refine the localization maps. Moreover, the proposed
framework is shown to fully complement the Class Activation Mapping (CAM)
method, leading to remarkably superior WSSS results on the PASCAL VOC and MS
COCO datasets. These results underline the importance of the class token for
WSSS.",https://github.com/xulianuwa/MCTformer,-1
Splicing ViT Features for Semantic Appearance Transfer,0.405992,"We present a method for semantically transferring the visual appearance of
one natural image to another. Specifically, our goal is to generate an image in
which objects in a source structure image are ""painted"" with the visual
appearance of their semantically related objects in a target appearance image.
Our method works by training a generator given only a single
structure/appearance image pair as input. To integrate semantic information
into our framework - a pivotal component in tackling this task - our key idea
is to leverage a pre-trained and fixed Vision Transformer (ViT) model which
serves as an external semantic prior. Specifically, we derive novel
representations of structure and appearance extracted from deep ViT features,
untwisting them from the learned self-attention modules. We then establish an
objective function that splices the desired structure and appearance
representations, interweaving them together in the space of ViT features. Our
framework, which we term ""Splice"", does not involve adversarial training, nor
does it require any additional input information such as semantic segmentation
or correspondences, and can generate high-resolution results, e.g., work in HD.
We demonstrate high quality results on a variety of in-the-wild image pairs,
under significant variations in the number of objects, their pose and
appearance.",https://splice-vit.github.io,-1
Diffusion models for missing value imputation in tabular data,0.0533218,"Missing value imputation in machine learning is the task of estimating the
missing values in the dataset accurately using available information. In this
task, several deep generative modeling methods have been proposed and
demonstrated their usefulness, e.g., generative adversarial imputation
networks. Recently, diffusion models have gained popularity because of their
effectiveness in the generative modeling task in images, texts, audio, etc. To
our knowledge, less attention has been paid to the investigation of the
effectiveness of diffusion models for missing value imputation in tabular data.
Based on recent development of diffusion models for time-series data
imputation, we propose a diffusion model approach called ""Conditional
Score-based Diffusion Models for Tabular data"" (TabCSDI). To effectively handle
categorical variables and numerical variables simultaneously, we investigate
three techniques: one-hot encoding, analog bits encoding, and feature
tokenization. Experimental results on benchmark datasets demonstrated the
effectiveness of TabCSDI compared with well-known existing methods, and also
emphasized the importance of the categorical embedding techniques.",https://github.com/vanderschaarlab/hyperimpute,-1
EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain,0.868408,"Existing summarization datasets come with two main drawbacks: (1) They tend
to focus on overly exposed domains, such as news articles or wiki-like texts,
and (2) are primarily monolingual, with few multilingual datasets. In this
work, we propose a novel dataset, called EUR-Lex-Sum, based on manually curated
document summaries of legal acts from the European Union law platform
(EUR-Lex). Documents and their respective summaries exist as cross-lingual
paragraph-aligned data in several of the 24 official European languages,
enabling access to various cross-lingual and lower-resourced summarization
setups. We obtain up to 1,500 document/summary pairs per language, including a
subset of 375 cross-lingually aligned legal acts with texts available in all 24
languages. In this work, the data acquisition process is detailed and key
characteristics of the resource are compared to existing summarization
resources. In particular, we illustrate challenging sub-problems and open
questions on the dataset that could help the facilitation of future research in
the direction of domain-specific cross-lingual summarization. Limited by the
extreme length and language diversity of samples, we further conduct
experiments with suitable extractive monolingual and cross-lingual baselines
for future work. Code for the extraction as well as access to our data and
baselines is available online at: https://github.com/achouhan93/eur-lex-sum.",https://github.com/achouhan93/eur-lex-sum,-1
3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows,0.880469,"Text-to-image AI are capable of generating novel images for inspiration, but
their applications for 3D design workflows and how designers can build 3D
models using AI-provided inspiration have not yet been explored. To investigate
this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a
plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users
to construct text and image prompts based on what they are modeling. In a study
with 13 designers, we found that designers saw great potential in 3DALL-E
within their workflows and could use text-to-image AI to produce reference
images, prevent design fixation, and inspire design considerations. We
elaborate on prompting patterns observed across 3D modeling tasks and provide
measures of prompt complexity observed across participants. From our findings,
we discuss how 3DALL-E can merge with existing generative design workflows and
propose prompt bibliographies as a form of human-AI design history.",None,-1
Learning Non-target Knowledge for Few-shot Semantic Segmentation,0.663444,"Existing studies in few-shot semantic segmentation only focus on mining the
target object information, however, often are hard to tell ambiguous regions,
especially in non-target regions, which include background (BG) and Distracting
Objects (DOs). To alleviate this problem, we propose a novel framework, namely
Non-Target Region Eliminating (NTRE) network, to explicitly mine and eliminate
BG and DO regions in the query. First, a BG Mining Module (BGMM) is proposed to
extract the BG region via learning a general BG prototype. To this end, we
design a BG loss to supervise the learning of BGMM only using the known target
object segmentation ground truth. Then, a BG Eliminating Module and a DO
Eliminating Module are proposed to successively filter out the BG and DO
information from the query feature, based on which we can obtain a BG and
DO-free target object segmentation result. Furthermore, we propose a
prototypical contrastive learning algorithm to improve the model ability of
distinguishing the target object from DOs. Extensive experiments on both
PASCAL-5i and COCO-20i datasets show that our approach is effective despite its
simplicity.",https://github.com/LIUYUANWEI98/NERTNet,-1
PETR: Position Embedding Transformation for Multi-View 3D Object Detection,0.910751,"In this paper, we develop position embedding transformation (PETR) for
multi-view 3D object detection. PETR encodes the position information of 3D
coordinates into image features, producing the 3D position-aware features.
Object query can perceive the 3D position-aware features and perform end-to-end
object detection. PETR achieves state-of-the-art performance (50.4% NDS and
44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.
It can serve as a simple yet strong baseline for future research. Code is
available at \url{https://github.com/megvii-research/PETR}.",https://github.com/megvii-research/PETR,-1
Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos,0.260887,"Unsupervised object-centric learning aims to represent the modular,
compositional, and causal structure of a scene as a set of object
representations and thereby promises to resolve many critical limitations of
traditional single-vector representations such as poor systematic
generalization. Although there have been many remarkable advances in recent
years, one of the most critical problems in this direction has been that
previous methods work only with simple and synthetic scenes but not with
complex and naturalistic images or videos. In this paper, we propose STEVE, an
unsupervised model for object-centric learning in videos. Our proposed model
makes a significant advancement by demonstrating its effectiveness on various
complex and naturalistic videos unprecedented in this line of research.
Interestingly, this is achieved by neither adding complexity to the model
architecture nor introducing a new objective or weak supervision. Rather, it is
achieved by a surprisingly simple architecture that uses a transformer-based
image decoder conditioned on slots and the learning objective is simply to
reconstruct the observation. Our experiment results on various complex and
naturalistic videos show significant improvements compared to the previous
state-of-the-art.",https://sites.google.com/view/slot-transformer-for-videos,-1
On the Road to Online Adaptation for Semantic Image Segmentation,0.139024,"We propose a new problem formulation and a corresponding evaluation framework
to advance research on unsupervised domain adaptation for semantic image
segmentation. The overall goal is fostering the development of adaptive
learning systems that will continuously learn, without supervision, in
ever-changing environments. Typical protocols that study adaptation algorithms
for segmentation models are limited to few domains, adaptation happens offline,
and human intervention is generally required, at least to annotate data for
hyper-parameter tuning. We argue that such constraints are incompatible with
algorithms that can continuously adapt to different real-world situations. To
address this, we propose a protocol where models need to learn online, from
sequences of temporally correlated images, requiring continuous, frame-by-frame
adaptation. We accompany this new protocol with a variety of baselines to
tackle the proposed formulation, as well as an extensive analysis of their
behaviors, which can serve as a starting point for future research.",https://github.com/naver/oasis,-1
Federated learning for violence incident prediction in a simulated cross-institutional psychiatric setting,0.0659601,"Inpatient violence is a common and severe problem within psychiatry. Knowing
who might become violent can influence staffing levels and mitigate severity.
Predictive machine learning models can assess each patient's likelihood of
becoming violent based on clinical notes. Yet, while machine learning models
benefit from having more data, data availability is limited as hospitals
typically do not share their data for privacy preservation. Federated Learning
(FL) can overcome the problem of data limitation by training models in a
decentralised manner, without disclosing data between collaborators. However,
although several FL approaches exist, none of these train Natural Language
Processing models on clinical notes. In this work, we investigate the
application of Federated Learning to clinical Natural Language Processing,
applied to the task of Violence Risk Assessment by simulating a
cross-institutional psychiatric setting. We train and compare four models: two
local models, a federated model and a data-centralised model. Our results
indicate that the federated model outperforms the local models and has similar
performance as the data-centralised model. These findings suggest that
Federated Learning can be used successfully in a cross-institutional setting
and is a step towards new applications of Federated Learning based on clinical
notes",None,-1
Relational Message Passing for Fully Inductive Knowledge Graph Completion,0.0985918,"In knowledge graph completion (KGC), predicting triples involving emerging
entities and/or relations, which are unseen when the KG embeddings are learned,
has become a critical challenge. Subgraph reasoning with message passing is a
promising and popular solution. Some recent methods have achieved good
performance, but they (i) usually can only predict triples involving unseen
entities alone, failing to address more realistic fully inductive situations
with both unseen entities and unseen relations, and (ii) often conduct message
passing over the entities with the relation patterns not fully utilized. In
this study, we propose a new method named RMPI which uses a novel Relational
Message Passing network for fully Inductive KGC. It passes messages directly
between relations to make full use of the relation patterns for subgraph
reasoning with new techniques on graph transformation, graph pruning,
relation-aware neighborhood attention, addressing empty subgraphs, etc., and
can utilize the relation semantics defined in the ontological schema of KG.
Extensive evaluation on multiple benchmarks has shown the effectiveness of
techniques involved in RMPI and its better performance compared with the
existing methods that support fully inductive KGC. RMPI is also comparable to
the state-of-the-art partially inductive KGC methods with very promising
results achieved. Our codes and data are available at
https://github.com/zjukg/RMPI.",https://github.com/zjukg/RMPI,-1
TabLLM: Few-shot Classification of Tabular Data with Large Language Models,0.980842,"We study the application of large language models to zero-shot and few-shot
classification of tabular data. We prompt the large language model with a
serialization of the tabular data to a natural-language string, together with a
short description of the classification problem. In the few-shot setting, we
fine-tune the large language model using some labeled examples. We evaluate
several serialization methods including templates, table-to-text models, and
large language models. Despite its simplicity, we find that this technique
outperforms prior deep-learning-based tabular classification methods on several
benchmark datasets. In most cases, even zero-shot classification obtains
non-trivial performance, illustrating the method's ability to exploit prior
knowledge encoded in large language models. Unlike many deep learning methods
for tabular datasets, this approach is also competitive with strong traditional
baselines like gradient-boosted trees, especially in the very-few-shot setting.",https://github.com/clinicalml/TabLLM,-1
Probabilities Are Not Enough: Formal Controller Synthesis for Stochastic Dynamical Models with Epistemic Uncertainty,0.0935157,"Capturing uncertainty in models of complex dynamical systems is crucial to
designing safe controllers. Stochastic noise causes aleatoric uncertainty,
whereas imprecise knowledge of model parameters leads to epistemic uncertainty.
Several approaches use formal abstractions to synthesize policies that satisfy
temporal specifications related to safety and reachability. However, the
underlying models exclusively capture aleatoric but not epistemic uncertainty,
and thus require that model parameters are known precisely. Our contribution to
overcoming this restriction is a novel abstraction-based controller synthesis
method for continuous-state models with stochastic noise and uncertain
parameters. By sampling techniques and robust analysis, we capture both
aleatoric and epistemic uncertainty, with a user-specified confidence level, in
the transition probability intervals of a so-called interval Markov decision
process (iMDP). We synthesize an optimal policy on this iMDP, which translates
(with the specified confidence level) to a feedback controller for the
continuous model with the same performance guarantees. Our experimental
benchmarks confirm that accounting for epistemic uncertainty leads to
controllers that are more robust against variations in parameter values.",https://github.com/LAVA-LAB/DynAbs,-1
Spatial Pruned Sparse Convolution for Efficient 3D Object Detection,0.327238,"3D scenes are dominated by a large number of background points, which is
redundant for the detection task that mainly needs to focus on foreground
objects. In this paper, we analyze major components of existing sparse 3D CNNs
and find that 3D CNNs ignore the redundancy of data and further amplify it in
the down-sampling process, which brings a huge amount of extra and unnecessary
computational overhead. Inspired by this, we propose a new convolution operator
named spatial pruned sparse convolution (SPS-Conv), which includes two
variants, spatial pruned submanifold sparse convolution (SPSS-Conv) and spatial
pruned regular sparse convolution (SPRS-Conv), both of which are based on the
idea of dynamically determining crucial areas for redundancy reduction. We
validate that the magnitude can serve as important cues to determine crucial
areas which get rid of the extra computations of learning-based methods. The
proposed modules can easily be incorporated into existing sparse 3D CNNs
without extra architectural modifications. Extensive experiments on the KITTI,
Waymo and nuScenes datasets demonstrate that our method can achieve more than
50% reduction in GFLOPs without compromising the performance.",None,-1
A Reference Model for Common Understanding of Capabilities and Skills in Manufacturing,0.210457,"In manufacturing, many use cases of Industry 4.0 require vendor-neutral and
machine-readable information models to describe, implement and execute resource
functions. Such models have been researched under the terms capabilities and
skills. Standardization of such models is required, but currently not
available. This paper presents a reference model developed jointly by members
of various organizations in a working group of the Plattform Industrie 4.0.
This model covers definitions of most important aspects of capabilities and
skills. It can be seen as a basis for further standardization efforts.",None,-1
"YOLOPv2: Better, Faster, Stronger for Panoptic Driving Perception",0.388144,"Over the last decade, multi-tasking learning approaches have achieved
promising results in solving panoptic driving perception problems, providing
both high-precision and high-efficiency performance. It has become a popular
paradigm when designing networks for real-time practical autonomous driving
system, where computation resources are limited. This paper proposed an
effective and efficient multi-task learning network to simultaneously perform
the task of traffic object detection, drivable road area segmentation and lane
detection. Our model achieved the new state-of-the-art (SOTA) performance in
terms of accuracy and speed on the challenging BDD100K dataset. Especially, the
inference time is reduced by half compared to the previous SOTA model. Code
will be released in the near future.",None,-1
SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation,0.308831,"Visual anomaly detection is commonly used in industrial quality inspection.
In this paper, we present a new dataset as well as a new self-supervised
learning method for ImageNet pre-training to improve anomaly detection and
segmentation in 1-class and 2-class 5/10/high-shot training setups. We release
the Visual Anomaly (VisA) Dataset consisting of 10,821 high-resolution color
images (9,621 normal and 1,200 anomalous samples) covering 12 objects in 3
domains, making it the largest industrial anomaly detection dataset to date.
Both image and pixel-level labels are provided. We also propose a new
self-supervised framework - SPot-the-difference (SPD) - which can regularize
contrastive self-supervised pre-training, such as SimSiam, MoCo and SimCLR, to
be more suitable for anomaly detection tasks. Our experiments on VisA and
MVTec-AD dataset show that SPD consistently improves these contrastive
pre-training baselines and even the supervised pre-training. For example, SPD
improves Area Under the Precision-Recall curve (AU-PR) for anomaly segmentation
by 5.9% and 6.8% over SimSiam and supervised pre-training respectively in the
2-class high-shot regime. We open-source the project at
http://github.com/amazon-research/spot-diff .",http://github.com/amazon-research/spot-diff,-1
Lipschitz-constrained Unsupervised Skill Discovery,0.864665,"We study the problem of unsupervised skill discovery, whose goal is to learn
a set of diverse and useful skills with no external reward. There have been a
number of skill discovery methods based on maximizing the mutual information
(MI) between skills and states. However, we point out that their MI objectives
usually prefer static skills to dynamic ones, which may hinder the application
for downstream tasks. To address this issue, we propose Lipschitz-constrained
Skill Discovery (LSD), which encourages the agent to discover more diverse,
dynamic, and far-reaching skills. Another benefit of LSD is that its learned
representation function can be utilized for solving goal-following downstream
tasks even in a zero-shot manner - i.e., without further training or complex
planning. Through experiments on various MuJoCo robotic locomotion and
manipulation environments, we demonstrate that LSD outperforms previous
approaches in terms of skill diversity, state space coverage, and performance
on seven downstream tasks including the challenging task of following multiple
goals on Humanoid. Our code and videos are available at
https://shpark.me/projects/lsd/.",https://vision.snu.ac.kr/projects/lsd/,-1
A Hazard Analysis Framework for Code Synthesis Large Language Models,0.121312,"Codex, a large language model (LLM) trained on a variety of codebases,
exceeds the previous state of the art in its capacity to synthesize and
generate code. Although Codex provides a plethora of benefits, models that may
generate code on such scale have significant limitations, alignment problems,
the potential to be misused, and the possibility to increase the rate of
progress in technical fields that may themselves have destabilizing impacts or
have misuse potential. Yet such safety impacts are not yet known or remain to
be explored. In this paper, we outline a hazard analysis framework constructed
at OpenAI to uncover hazards or safety risks that the deployment of models like
Codex may impose technically, socially, politically, and economically. The
analysis is informed by a novel evaluation framework that determines the
capacity of advanced code generation techniques against the complexity and
expressivity of specification prompts, and their capability to understand and
execute them relative to human ability.",None,-1
Unsupervised Discovery and Composition of Object Light Fields,0.292613,"Neural scene representations, both continuous and discrete, have recently
emerged as a powerful new paradigm for 3D scene understanding. Recent efforts
have tackled unsupervised discovery of object-centric neural scene
representations. However, the high cost of ray-marching, exacerbated by the
fact that each object representation has to be ray-marched separately, leads to
insufficiently sampled radiance fields and thus, noisy renderings, poor
framerates, and high memory and time complexity during training and rendering.
Here, we propose to represent objects in an object-centric, compositional scene
representation as light fields. We propose a novel light field compositor
module that enables reconstructing the global light field from a set of
object-centric light fields. Dubbed Compositional Object Light Fields (COLF),
our method enables unsupervised learning of object-centric neural scene
representations, state-of-the-art reconstruction and novel view synthesis
performance on standard datasets, and rendering and training speeds at orders
of magnitude faster than existing 3D approaches.",https://github.com/cameronosmith/colf,-1
DeiT III: Revenge of the ViT,0.687369,"A Vision Transformer (ViT) is a simple neural architecture amenable to serve
several computer vision tasks. It has limited built-in architectural priors, in
contrast to more recent architectures that incorporate priors either about the
input data or of specific tasks. Recent works show that ViTs benefit from
self-supervised pre-training, in particular BerT-like pre-training like BeiT.
In this paper, we revisit the supervised training of ViTs. Our procedure builds
upon and simplifies a recipe introduced for training ResNet-50. It includes a
new simple data-augmentation procedure with only 3 augmentations, closer to the
practice in self-supervised learning. Our evaluations on Image classification
(ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning
and semantic segmentation show that our procedure outperforms by a large margin
previous fully supervised training recipes for ViT. It also reveals that the
performance of our ViT trained with supervision is comparable to that of more
recent architectures. Our results could serve as better baselines for recent
self-supervised approaches demonstrated on ViT.",None,-1
"""This is Fake! Shared it by Mistake"": Assessing the Intent of Fake News Spreaders",0.390708,"Individuals can be misled by fake news and spread it unintentionally without
knowing it is false. This phenomenon has been frequently observed but has not
been investigated. Our aim in this work is to assess the intent of fake news
spreaders. To distinguish between intentional versus unintentional spreading,
we study the psychological explanations of unintentional spreading. With this
foundation, we then propose an influence graph, using which we assess the
intent of fake news spreaders. Our extensive experiments show that the assessed
intent can help significantly differentiate between intentional and
unintentional fake news spreaders. Furthermore, the estimated intent can
significantly improve the current techniques that detect fake news. To our best
knowledge, this is the first work to model individuals' intent in fake news
spreading.",https://github.com/flairNLP/flair,-1
Differentially Private Decoding in Large Language Models,0.105653,"Recent large-scale natural language processing (NLP) systems use a
pre-trained Large Language Model (LLM) on massive and diverse corpora as a
headstart. In practice, the pre-trained model is adapted to a wide array of
tasks via fine-tuning on task-specific datasets. LLMs, while effective, have
been shown to memorize instances of training data thereby potentially revealing
private information processed during pre-training. The potential leakage might
further propagate to the downstream tasks for which LLMs are fine-tuned. On the
other hand, privacy-preserving algorithms usually involve retraining from
scratch, which is prohibitively expensive for LLMs. In this work, we propose a
simple, easy to interpret, and computationally lightweight perturbation
mechanism to be applied to an already trained model at the decoding stage. Our
perturbation mechanism is model-agnostic and can be used in conjunction with
any LLM. We provide theoretical analysis showing that the proposed mechanism is
differentially private, and experimental results showing a privacy-utility
trade-off.",https://github.com/kingoflolz/mesh-transformer-jax,-1
Algorithms for Weighted Pushdown Automata,0.069249,"Weighted pushdown automata (WPDAs) are at the core of many natural language
processing tasks, like syntax-based statistical machine translation and
transition-based dependency parsing. As most existing dynamic programming
algorithms are designed for context-free grammars (CFGs), algorithms for PDAs
often resort to a PDA-to-CFG conversion. In this paper, we develop novel
algorithms that operate directly on WPDAs. Our algorithms are inspired by
Lang's algorithm, but use a more general definition of pushdown automaton and
either reduce the space requirements by a factor of $|\Gamma|$ (the size of the
stack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the
number of states). When run on the same class of PDAs as Lang's algorithm, our
algorithm is both more space-efficient by a factor of $|\Gamma|$ and more
time-efficient by a factor of $|Q| \cdot |\Gamma|$.",https://github.com/rycolab/wpda,-1
JRDB-Pose: A Large-scale Dataset for Multi-Person Pose Estimation and Tracking,0.182688,"Autonomous robotic systems operating in human environments must understand
their surroundings to make accurate and safe decisions. In crowded human scenes
with close-up human-robot interaction and robot navigation, a deep
understanding requires reasoning about human motion and body dynamics over time
with human body pose estimation and tracking. However, existing datasets either
do not provide pose annotations or include scene types unrelated to robotic
applications. Many datasets also lack the diversity of poses and occlusions
found in crowded human scenes. To address this limitation we introduce
JRDB-Pose, a large-scale dataset and benchmark for multi-person pose estimation
and tracking using videos captured from a social navigation robot. The dataset
contains challenge scenes with crowded indoor and outdoor locations and a
diverse range of scales and occlusion types. JRDB-Pose provides human pose
annotations with per-keypoint occlusion labels and track IDs consistent across
the scene. A public evaluation server is made available for fair evaluation on
a held-out test set. JRDB-Pose is available at https://jrdb.erc.monash.edu/ .",None,-1
Watermark Vaccine: Adversarial Attacks to Prevent Watermark Removal,0.0921845,"As a common security tool, visible watermarking has been widely applied to
protect copyrights of digital images. However, recent works have shown that
visible watermarks can be removed by DNNs without damaging their host images.
Such watermark-removal techniques pose a great threat to the ownership of
images. Inspired by the vulnerability of DNNs on adversarial perturbations, we
propose a novel defence mechanism by adversarial machine learning for good.
From the perspective of the adversary, blind watermark-removal networks can be
posed as our target models; then we actually optimize an imperceptible
adversarial perturbation on the host images to proactively attack against
watermark-removal networks, dubbed Watermark Vaccine. Specifically, two types
of vaccines are proposed. Disrupting Watermark Vaccine (DWV) induces to ruin
the host image along with watermark after passing through watermark-removal
networks. In contrast, Inerasable Watermark Vaccine (IWV) works in another
fashion of trying to keep the watermark not removed and still noticeable.
Extensive experiments demonstrate the effectiveness of our DWV/IWV in
preventing watermark removal, especially on various watermark removal networks.",https://github.com/thinwayliu/Watermark-Vaccine,-1
LiBeamsNet: AUV Velocity Vector Estimation in Situations of Limited DVL Beam Measurements,0.147341,"Autonomous underwater vehicles (AUVs) are employed for marine applications
and can operate in deep underwater environments beyond human reach. A standard
solution for the autonomous navigation problem can be obtained by fusing the
inertial navigation system and the Doppler velocity log sensor (DVL). The
latter measures four beam velocities to estimate the vehicle's velocity vector.
In real-world scenarios, the DVL may receive less than three beam velocities if
the AUV operates in complex underwater environments. In such conditions, the
vehicle's velocity vector could not be estimated leading to a navigation
solution drift and in some situations the AUV is required to abort the mission
and return to the surface. To circumvent such a situation, in this paper we
propose a deep learning framework, LiBeamsNet, that utilizes the inertial data
and the partial beam velocities to regress the missing beams in two missing
beams scenarios. Once all the beams are obtained, the vehicle's velocity vector
can be estimated. The approach performance was validated by sea experiments in
the Mediterranean Sea. The results show up to 7.2% speed error in the vehicle's
velocity vector estimation in a scenario that otherwise could not provide an
estimate.",https://github.com/ansfl/BeamsNet,-1
Compression of Generative Pre-trained Language Models via Quantization,0.0816387,"The increasing size of generative Pre-trained Language Models (PLMs) has
greatly increased the demand for model compression. Despite various methods to
compress BERT or its variants, there are few attempts to compress generative
PLMs, and the underlying difficulty remains unclear. In this paper, we compress
generative PLMs by quantization. We find that previous quantization methods
fail on generative tasks due to the \textit{homogeneous word embeddings} caused
by reduced capacity, and \textit{varied distribution of weights}.
Correspondingly, we propose a token-level contrastive distillation to learn
distinguishable word embeddings, and a module-wise dynamic scaling to make
quantizers adaptive to different modules. Empirical results on various tasks
show that our proposed method outperforms the state-of-the-art compression
methods on generative PLMs by a clear margin. With comparable performance with
the full-precision models, we achieve 14.4x and 13.4x compression rates on
GPT-2 and BART, respectively.",None,-1
Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters,0.149716,"The growing public concerns on data privacy in face recognition can be
greatly addressed by the federated learning (FL) paradigm. However,
conventional FL methods perform poorly due to the uniqueness of the task:
broadcasting class centers among clients is crucial for recognition
performances but leads to privacy leakage. To resolve the privacy-utility
paradox, this work proposes PrivacyFace, a framework largely improves the
federated learning face recognition via communicating auxiliary and
privacy-agnostic information among clients. PrivacyFace mainly consists of two
components: First, a practical Differentially Private Local Clustering (DPLC)
mechanism is proposed to distill sanitized clusters from local class centers.
Second, a consensus-aware recognition loss subsequently encourages global
consensuses among clients, which ergo results in more discriminative features.
The proposed framework is mathematically proved to be differentially private,
introducing a lightweight overhead as well as yielding prominent performance
boosts (\textit{e.g.}, +9.63\% and +10.26\% for TAR@FAR=1e-4 on IJB-B and IJB-C
respectively). Extensive experiments and ablation studies on a large-scale
dataset have demonstrated the efficacy and practicability of our method.",https://github.com/IrvingMeng/MagFace,-1
REPAIR: REnormalizing Permuted Activations for Interpolation Repair,0.591897,"In this paper we look into the conjecture of Entezari et al. (2021) which
states that if the permutation invariance of neural networks is taken into
account, then there is likely no loss barrier to the linear interpolation
between SGD solutions. First, we observe that neuron alignment methods alone
are insufficient to establish low-barrier linear connectivity between SGD
solutions due to a phenomenon we call variance collapse: interpolated deep
networks suffer a collapse in the variance of their activations, causing poor
performance. Next, we propose REPAIR (REnormalizing Permuted Activations for
Interpolation Repair) which mitigates variance collapse by rescaling the
preactivations of such interpolated networks. We explore the interaction
between our method and the choice of normalization layer, network width, and
depth, and demonstrate that using REPAIR on top of neuron alignment methods
leads to 60%-100% relative barrier reduction across a wide variety of
architecture families and tasks. In particular, we report a 74% barrier
reduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 on
CIFAR10.",https://github.com/KellerJordan/REPAIR,-1
Large Language Models Can Self-Improve,0.751607,"Large Language Models (LLMs) have achieved excellent performances in various
tasks. However, fine-tuning an LLM requires extensive supervision. Human, on
the other hand, may improve their reasoning abilities by self-thinking without
external inputs. In this work, we demonstrate that an LLM is also capable of
self-improving with only unlabeled datasets. We use a pre-trained LLM to
generate ""high-confidence"" rationale-augmented answers for unlabeled questions
using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM
using those self-generated solutions as target outputs. We show that our
approach improves the general reasoning ability of a 540B-parameter LLM
(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and
63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,
without any ground truth label. We conduct ablation studies and show that
fine-tuning on reasoning is critical for self-improvement.",https://github.com/google-research/google-research/tree/master/ul2,-1
Intent Contrastive Learning for Sequential Recommendation,0.591449,"Users' interactions with items are driven by various intents (e.g., preparing
for holiday gifts, shopping for fishing equipment, etc.).However, users'
underlying intents are often unobserved/latent, making it challenging to
leverage such latent intents forSequentialrecommendation(SR). To investigate
the benefits of latent intents and leverage them effectively for
recommendation, we proposeIntentContrastiveLearning(ICL), a general learning
paradigm that leverages a latent intent variable into SR. The core idea is to
learn users' intent distribution functions from unlabeled user behavior
sequences and optimize SR models with contrastive self-supervised learning
(SSL) by considering the learned intents to improve recommendation.
Specifically, we introduce a latent variable to represent users' intents and
learn the distribution function of the latent variable via clustering. We
propose to leverage the learned intents into SR models via contrastive SSL,
which maximizes the agreement between a view of sequence and its corresponding
intent. The training is alternated between intent representation learning and
the SR model optimization steps within the generalized expectation-maximization
(EM) framework. Fusing user intent information into SR also improves model
robustness. Experiments conducted on four real-world datasets demonstrate the
superiority of the proposed learning paradigm, which improves performance, and
robustness against data sparsity and noisy interaction issues.",https://github.com/salesforce/ICLRec,-1
Unified Chinese License Plate Detection and Recognition with High Efficiency,0.638564,"Recently, deep learning-based methods have reached an excellent performance
on License Plate (LP) detection and recognition tasks. However, it is still
challenging to build a robust model for Chinese LPs since there are not enough
large and representative datasets. In this work, we propose a new dataset named
Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP
images as a supplement to the existing public benchmarks. The images are mainly
captured with electronic monitoring systems with detailed annotations. To our
knowledge, CRPD is the largest public multi-objective Chinese LP dataset with
annotations of vertices. With CRPD, a unified detection and recognition network
with high efficiency is presented as the baseline. The network is end-to-end
trainable with totally real-time inference efficiency (30 fps with 640p). The
experiments on several public benchmarks demonstrate that our method has
reached competitive performance. The code and dataset will be publicly
available at https://github.com/yxgong0/CRPD.",https://github.com/yxgong0/CRPD,-1
Knowledge Removal in Sampling-based Bayesian Inference,0.344163,"The right to be forgotten has been legislated in many countries, but its
enforcement in the AI industry would cause unbearable costs. When single data
deletion requests come, companies may need to delete the whole models learned
with massive resources. Existing works propose methods to remove knowledge
learned from data for explicitly parameterized models, which however are not
appliable to the sampling-based Bayesian inference, i.e., Markov chain Monte
Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we
propose the first machine unlearning algorithm for MCMC. We first convert the
MCMC unlearning problem into an explicit optimization problem. Based on this
problem conversion, an {\it MCMC influence function} is designed to provably
characterize the learned knowledge from data, which then delivers the MCMC
unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not
compromise the generalizability of the MCMC models. Experiments on Gaussian
mixture models and Bayesian neural networks confirm the effectiveness of the
proposed algorithm. The code is available at
\url{https://github.com/fshp971/mcmc-unlearning}.",https://github.com/fshp971/mcmc-unlearning,-1
Shift-tolerant Perceptual Similarity Metric,0.330114,"Existing perceptual similarity metrics assume an image and its reference are
well aligned. As a result, these metrics are often sensitive to a small
alignment error that is imperceptible to the human eyes. This paper studies the
effect of small misalignment, specifically a small shift between the input and
reference image, on existing metrics, and accordingly develops a shift-tolerant
similarity metric. This paper builds upon LPIPS, a widely used learned
perceptual similarity metric, and explores architectural design considerations
to make it robust against imperceptible misalignment. Specifically, we study a
wide spectrum of neural network elements, such as anti-aliasing filtering,
pooling, striding, padding, and skip connection, and discuss their roles in
making a robust metric. Based on our studies, we develop a new deep neural
network-based perceptual similarity metric. Our experiments show that our
metric is tolerant to imperceptible shifts while being consistent with the
human similarity judgment.",https://tinyurl.com/5n85r28r,-1
Focal Length and Object Pose Estimation via Render and Compare,0.352617,"We introduce FocalPose, a neural render-and-compare method for jointly
estimating the camera-object 6D pose and camera focal length given a single RGB
input image depicting a known object. The contributions of this work are
twofold. First, we derive a focal length update rule that extends an existing
state-of-the-art render-and-compare 6D pose estimator to address the joint
estimation task. Second, we investigate several different loss functions for
jointly estimating the object pose and focal length. We find that a combination
of direct focal length regression with a reprojection loss disentangling the
contribution of translation, rotation, and focal length leads to improved
results. We show results on three challenging benchmark datasets that depict
known 3D models in uncontrolled settings. We demonstrate that our focal length
and 6D pose estimates have lower error than the existing state-of-the-art
methods.",https://ponimatkin.github.io/focalpose,-1
Empathic Conversations: A Multi-level Dataset of Contextualized Conversations,0.176267,"Empathy is a cognitive and emotional reaction to an observed situation of
others. Empathy has recently attracted interest because it has numerous
applications in psychology and AI, but it is unclear how different forms of
empathy (e.g., self-report vs counterpart other-report, concern vs. distress)
interact with other affective phenomena or demographics like gender and age. To
better understand this, we created the {\it Empathic Conversations} dataset of
annotated negative, empathy-eliciting dialogues in which pairs of participants
converse about news articles. People differ in their perception of the empathy
of others. These differences are associated with certain characteristics such
as personality and demographics. Hence, we collected detailed characterization
of the participants' traits, their self-reported empathetic response to news
articles, their conversational partner other-report, and turn-by-turn
third-party assessments of the level of self-disclosure, emotion, and empathy
expressed. This dataset is the first to present empathy in multiple forms along
with personal distress, emotion, personality characteristics, and person-level
demographic information. We present baseline models for predicting some of
these features from conversations.",https://github.com/wwbp/empathic_reactions,-1
A Second Wave of UD Hebrew Treebanking and Cross-Domain Parsing,0.284874,"Foundational Hebrew NLP tasks such as segmentation, tagging and parsing, have
relied to date on various versions of the Hebrew Treebank (HTB, Sima'an et al.
2001). However, the data in HTB, a single-source newswire corpus, is now over
30 years old, and does not cover many aspects of contemporary Hebrew on the
web. This paper presents a new, freely available UD treebank of Hebrew
stratified from a range of topics selected from Hebrew Wikipedia. In addition
to introducing the corpus and evaluating the quality of its annotations, we
deploy automatic validation tools based on grew (Guillaume, 2021), and conduct
the first cross domain parsing experiments in Hebrew. We obtain new
state-of-the-art (SOTA) results on UD NLP tasks, using a combination of the
latest language modelling and some incremental improvements to existing
transformer based approaches. We also release a new version of the UD HTB
matching annotation scheme updates from our new corpus.",https://github.com/amir-zeldes/HebPipe/,-1
Truth Set Algebra: A New Way to Prove Undefinability,0.19379,"The article proposes a new technique for proving the undefinability of
logical connectives through each other and illustrates the technique with
several examples. Some of the obtained results are new proofs of the existing
theorems, others are original to this work.",None,-1
Is Conditional Generative Modeling all you need for Decision-Making?,0.76689,"Recent improvements in conditional generative modeling have made it possible
to generate high-quality images from language descriptions alone. We
investigate whether these methods can directly address the problem of
sequential decision-making. We view decision-making not through the lens of
reinforcement learning (RL), but rather through conditional generative
modeling. To our surprise, we find that our formulation leads to policies that
can outperform existing offline RL approaches across standard benchmarks. By
modeling a policy as a return-conditional diffusion model, we illustrate how we
may circumvent the need for dynamic programming and subsequently eliminate many
of the complexities that come with traditional offline RL. We further
demonstrate the advantages of modeling policies as conditional diffusion models
by considering two other conditioning variables: constraints and skills.
Conditioning on a single constraint or skill during training leads to behaviors
at test-time that can satisfy several constraints together or demonstrate a
composition of skills. Our results illustrate that conditional generative
modeling is a powerful tool for decision-making.",https://github.com/jannerm/diffuser,-1
Latent Image Animator: Learning to Animate Images via Latent Space Navigation,0.70954,"Due to the remarkable progress of deep generative models, animating images
has become increasingly efficient, whereas associated results have become
increasingly realistic. Current animation-approaches commonly exploit structure
representation extracted from driving videos. Such structure representation is
instrumental in transferring motion from driving videos to still images.
However, such approaches fail in case the source image and driving video
encompass large appearance variation. Moreover, the extraction of structure
information requires additional modules that endow the animation-model with
increased complexity. Deviating from such models, we here introduce the Latent
Image Animator (LIA), a self-supervised autoencoder that evades need for
structure representation. LIA is streamlined to animate images by linear
navigation in the latent space. Specifically, motion in generated video is
constructed by linear displacement of codes in the latent space. Towards this,
we learn a set of orthogonal motion directions simultaneously, and use their
linear combination, in order to represent any displacement in the latent space.
Extensive quantitative and qualitative analysis suggests that our model
systematically and significantly outperforms state-of-art methods on VoxCeleb,
Taichi and TED-talk datasets w.r.t. generated quality.",https://wyhsirius.github.io/LIA-project/,-1
CAGroup3D: Class-Aware Grouping for 3D Object Detection on Point Clouds,0.418867,"We present a novel two-stage fully sparse convolutional 3D object detection
framework, named CAGroup3D. Our proposed method first generates some
high-quality 3D proposals by leveraging the class-aware local group strategy on
the object surface voxels with the same semantic predictions, which considers
semantic consistency and diverse locality abandoned in previous bottom-up
approaches. Then, to recover the features of missed voxels due to incorrect
voxel-wise segmentation, we build a fully sparse convolutional RoI pooling
module to directly aggregate fine-grained spatial information from backbone for
further proposal refinement. It is memory-and-computation efficient and can
better encode the geometry-specific features of each 3D proposal. Our model
achieves state-of-the-art 3D detection performance with remarkable gains of
+\textit{3.6\%} on ScanNet V2 and +\textit{2.6}\% on SUN RGB-D in term of
mAP@0.25. Code will be available at https://github.com/Haiyang-W/CAGroup3D.",https://github.com/Haiyang-W/CAGroup3D,-1
Evaluating Explainability for Graph Neural Networks,0.0557395,"As post hoc explanations are increasingly used to understand the behavior of
graph neural networks (GNNs), it becomes crucial to evaluate the quality and
reliability of GNN explanations. However, assessing the quality of GNN
explanations is challenging as existing graph datasets have no or unreliable
ground-truth explanations for a given task. Here, we introduce a synthetic
graph data generator, ShapeGGen, which can generate a variety of benchmark
datasets (e.g., varying graph sizes, degree distributions, homophilic vs.
heterophilic graphs) accompanied by ground-truth explanations. Further, the
flexibility to generate diverse synthetic datasets and corresponding
ground-truth explanations allows us to mimic the data generated by various
real-world applications. We include ShapeGGen and several real-world graph
datasets into an open-source graph explainability library, GraphXAI. In
addition to synthetic and real-world graph datasets with ground-truth
explanations, GraphXAI provides data loaders, data processing functions,
visualizers, GNN model implementations, and evaluation metrics to benchmark the
performance of GNN explainability methods.",https://github.com/mims-harvard/GraphXAI,-1
Understanding Translationese in Cross-Lingual Summarization,0.476648,"Given a document in a source language, cross-lingual summarization (CLS) aims
at generating a concise summary in a different target language. Unlike
monolingual summarization (MS), naturally occurring source-language documents
paired with target-language summaries are rare. To collect large-scale CLS
data, existing datasets typically involve translation in their creation.
However, the translated text is distinguished from the text originally written
in that language, i.e., translationese. In this paper, we first confirm that
different approaches of constructing CLS datasets will lead to different
degrees of translationese. Then we systematically investigate how
translationese affects CLS model evaluation and performance when it appears in
source documents or target summaries. In detail, we find that (1) the
translationese in documents or summaries of test sets might lead to the
discrepancy between human judgment and automatic evaluation; (2) the
translationese in training sets would harm model performance in real-world
applications; (3) though machine-translated documents involve translationese,
they are very useful for building CLS systems on low-resource languages under
specific training strategies. Lastly, we give suggestions for future CLS
research including dataset and model developments. We hope that our work could
let researchers notice the phenomenon of translationese in CLS and take it into
account in the future.",https://github.com/xcfcode/MSAMSum,-1
Locating and Editing Factual Associations in GPT,0.980348,"We analyze the storage and recall of factual associations in autoregressive
transformer language models, finding evidence that these associations
correspond to localized, directly-editable computations. We first develop a
causal intervention for identifying neuron activations that are decisive in a
model's factual predictions. This reveals a distinct set of steps in
middle-layer feed-forward modules that mediate factual predictions while
processing subject tokens. To test our hypothesis that these computations
correspond to factual association recall, we modify feed-forward weights to
update specific factual associations using Rank-One Model Editing (ROME). We
find that ROME is effective on a standard zero-shot relation extraction (zsRE)
model-editing task, comparable to existing methods. To perform a more sensitive
evaluation, we also evaluate ROME on a new dataset of counterfactual
assertions, on which it simultaneously maintains both specificity and
generalization, whereas other methods sacrifice one or another. Our results
confirm an important role for mid-layer feed-forward modules in storing factual
associations and suggest that direct manipulation of computational mechanisms
may be a feasible approach for model editing. The code, dataset,
visualizations, and an interactive demo notebook are available at
https://rome.baulab.info/",https://rome.baulab.info/,-1
The Third International Verification of Neural Networks Competition (VNN-COMP 2022): Summary and Results,0.0949897,"This report summarizes the 3rd International Verification of Neural Networks
Competition (VNN-COMP 2022), held as a part of the 5th Workshop on Formal
Methods for ML-Enabled Autonomous Systems (FoMLAS), which was collocated with
the 34th International Conference on Computer-Aided Verification (CAV).
VNN-COMP is held annually to facilitate the fair and objective comparison of
state-of-the-art neural network verification tools, encourage the
standardization of tool interfaces, and bring together the neural network
verification community. To this end, standardized formats for networks (ONNX)
and specification (VNN-LIB) were defined, tools were evaluated on equal-cost
hardware (using an automatic evaluation pipeline based on AWS instances), and
tool parameters were chosen by the participants before the final test sets were
made public. In the 2022 iteration, 11 teams participated on a diverse set of
12 scored benchmarks. This report summarizes the rules, benchmarks,
participating tools, results, and lessons learned from this iteration of this
competition.",None,-1
Character-level White-Box Adversarial Attacks against Transformers via Attachable Subwords Substitution,0.244974,"We propose the first character-level white-box adversarial attack method
against transformer models. The intuition of our method comes from the
observation that words are split into subtokens before being fed into the
transformer models and the substitution between two close subtokens has a
similar effect to the character modification. Our method mainly contains three
steps. First, a gradient-based method is adopted to find the most vulnerable
words in the sentence. Then we split the selected words into subtokens to
replace the origin tokenization result from the transformer tokenizer. Finally,
we utilize an adversarial loss to guide the substitution of attachable
subtokens in which the Gumbel-softmax trick is introduced to ensure gradient
propagation. Meanwhile, we introduce the visual and length constraint in the
optimization process to achieve minimum character modifications. Extensive
experiments on both sentence-level and token-level tasks demonstrate that our
method could outperform the previous attack methods in terms of success rate
and edit distance. Furthermore, human evaluation verifies our adversarial
examples could preserve their origin labels.",https://github.com/THU-BPM/CWBA,-1
Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion,0.851609,"Digital art synthesis is receiving increasing attention in the multimedia
community because of engaging the public with art effectively. Current digital
art synthesis methods usually use single-modality inputs as guidance, thereby
limiting the expressiveness of the model and the diversity of generated
results. To solve this problem, we propose the multimodal guided artwork
diffusion (MGAD) model, which is a diffusion-based digital artwork generation
approach that utilizes multimodal prompts as guidance to control the
classifier-free diffusion model. Additionally, the contrastive language-image
pretraining (CLIP) model is used to unify text and image modalities. Extensive
experimental results on the quality and quantity of the generated digital art
paintings confirm the effectiveness of the combination of the diffusion model
and multimodal guidance. Code is available at
https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion.",https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion,-1
Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation,0.541482,"Non-autoregressive translation (NAT) models are typically trained with the
cross-entropy loss, which forces the model outputs to be aligned verbatim with
the target sentence and will highly penalize small shifts in word positions.
Latent alignment models relax the explicit alignment by marginalizing out all
monotonic latent alignments with the CTC loss. However, they cannot handle
non-monotonic alignments, which is non-negligible as there is typically global
word reordering in machine translation. In this work, we explore non-monotonic
latent alignments for NAT. We extend the alignment space to non-monotonic
alignments to allow for the global word reordering and further consider all
alignments that overlap with the target sentence. We non-monotonically match
the alignments to the target sentence and train the latent alignment model to
maximize the F1 score of non-monotonic matching. Extensive experiments on major
WMT benchmarks show that our method substantially improves the translation
performance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14
En-De with only one-iteration decoding, closing the gap between
non-autoregressive and autoregressive models.",None,-1
Deep Generalized Unfolding Networks for Image Restoration,0.314065,"Deep neural networks (DNN) have achieved great success in image restoration.
However, most DNN methods are designed as a black box, lacking transparency and
interpretability. Although some methods are proposed to combine traditional
optimization algorithms with DNN, they usually demand pre-defined degradation
processes or handcrafted assumptions, making it difficult to deal with complex
and real-world applications. In this paper, we propose a Deep Generalized
Unfolding Network (DGUNet) for image restoration. Concretely, without loss of
interpretability, we integrate a gradient estimation strategy into the gradient
descent step of the Proximal Gradient Descent (PGD) algorithm, driving it to
deal with complex and real-world image degradation. In addition, we design
inter-stage information pathways across proximal mapping in different PGD
iterations to rectify the intrinsic information loss in most deep unfolding
networks (DUN) through a multi-scale and spatial-adaptive way. By integrating
the flexible gradient descent and informative proximal mapping, we unfold the
iterative PGD algorithm into a trainable DNN. Extensive experiments on various
image restoration tasks demonstrate the superiority of our method in terms of
state-of-the-art performance, interpretability, and generalizability. The
source code is available at
https://github.com/MC-E/Deep-Generalized-Unfolding-Networks-for-Image-Restoration.",https://github.com/MC-E/DGUNet,-1
Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,0.83482,"Prompting-based large language models (LLMs) are surprisingly powerful at
generating natural language reasoning steps or Chains-of-Thoughts (CoT) for
multi-step question answering (QA). They struggle, however, when the necessary
knowledge is either unavailable to the LLM or not up-to-date within its
parameters. While using the question to retrieve relevant text from an external
knowledge source helps LLMs, we observe that this one-step retrieve-and-read
approach is insufficient for multi-step QA. Here, \textit{what to retrieve}
depends on \textit{what has already been derived}, which in turn may depend on
\textit{what was previously retrieved}. To address this, we propose IRCoT, a
new approach for multi-step QA that interleaves retrieval with steps
(sentences) in a CoT, guiding the retrieval with CoT and in turn using
retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves
retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four
datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar
substantial gains in out-of-distribution (OOD) settings as well as with much
smaller models such as Flan-T5-large without additional training. IRCoT reduces
model hallucination, resulting in factually more accurate CoT reasoning. Code,
data, and prompts are available at \url{https://github.com/stonybrooknlp/ircot}",https://github.com/stonybrooknlp/ircot,-1
Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement,0.63148,"Low-Light Image Enhancement is a computer vision task which intensifies the
dark images to appropriate brightness. It can also be seen as an ill-posed
problem in image restoration domain. With the success of deep neural networks,
the convolutional neural networks surpass the traditional algorithm-based
methods and become the mainstream in the computer vision area. To advance the
performance of enhancement algorithms, we propose an image enhancement network
(HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use
a half wavelet attention block on M-Net+ to enrich the features from wavelet
domain. Furthermore, our HWMNet has competitive performance results on two
image enhancement datasets in terms of quantitative metrics and visual quality.
The source code and pretrained model are available at
https://github.com/FanChiMao/HWMNet.",https://github.com/FanChiMao/HWMNet,-1
Diffusion Posterior Sampling for General Noisy Inverse Problems,0.695162,"Diffusion models have been recently studied as powerful generative inverse
problem solvers, owing to their high quality reconstructions and the ease of
combining existing iterative solvers. However, most works focus on solving
simple linear inverse problems in noiseless settings, which significantly
under-represents the complexity of real-world problems. In this work, we extend
diffusion solvers to efficiently handle general noisy (non)linear inverse
problems via approximation of the posterior sampling. Interestingly, the
resulting posterior sampling scheme is a blended version of diffusion sampling
with the manifold constrained gradient without a strict measurement consistency
projection step, yielding a more desirable generative path in noisy settings
compared to the previous studies. Our method demonstrates that diffusion models
can incorporate various measurement noise statistics such as Gaussian and
Poisson, and also efficiently handle noisy nonlinear inverse problems such as
Fourier phase retrieval and non-uniform deblurring. Code available at
https://github.com/DPS2022/diffusion-posterior-sampling",https://github.com/DPS2022/diffusion-posterior-sampling,-1
Generating Executable Action Plans with Environmentally-Aware Language Models,0.150417,"Large Language Models (LLMs) trained using massive text datasets have
recently shown promise in generating action plans for robotic agents from high
level text queries. However, these models typically do not consider the robot's
environment, resulting in generated plans that may not actually be executable,
due to ambiguities in the planned actions or environmental constraints. In this
paper, we propose an approach to generate environmentally-aware action plans
that agents are better able to execute. Our approach involves integrating
environmental objects and object relations as additional inputs into LLM action
plan generation to provide the system with an awareness of its surroundings,
resulting in plans where each generated action is mapped to objects present in
the scene. We also design a novel scoring function that, along with generating
the action steps and associating them with objects, helps the system
disambiguate among object instances and take into account their states. We
evaluated our approach using the VirtualHome simulator and the ActivityPrograms
knowledge base and found that action plans generated from our system had a 310%
improvement in executability and a 147% improvement in correctness over prior
work. The complete code and a demo of our method is publicly available at
https://github.com/hri-ironlab/scene_aware_language_planner.",https://github.com/hri-ironlab/scene_aware_language_planner,-1
Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective,0.181943,"Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called ""neural texts""), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.",None,-1
Learning Optical Flow with Adaptive Graph Reasoning,0.530401,"Estimating per-pixel motion between video frames, known as optical flow, is a
long-standing problem in video understanding and analysis. Most contemporary
optical flow techniques largely focus on addressing the cross-image matching
with feature similarity, with few methods considering how to explicitly reason
over the given scene for achieving a holistic motion understanding. In this
work, taking a fresh perspective, we introduce a novel graph-based approach,
called adaptive graph reasoning for optical flow (AGFlow), to emphasize the
value of scene/context information in optical flow. Our key idea is to decouple
the context reasoning from the matching procedure, and exploit scene
information to effectively assist motion estimation by learning to reason over
the adaptive graph. The proposed AGFlow can effectively exploit the context
information and incorporate it within the matching procedure, producing more
robust and accurate results. On both Sintel clean and final passes, our AGFlow
achieves the best accuracy with EPE of 1.43 and 2.47 pixels, outperforming
state-of-the-art approaches by 11.2% and 13.6%, respectively.",https://github.com/LA30/AGFlow,-1
Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis,0.180688,"Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis
task which involves four elements from user-generated texts: aspect term,
aspect category, opinion term, and sentiment polarity. Most computational
approaches focus on some of the ABSA sub-tasks such as tuple (aspect term,
sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity)
extraction using either pipeline or joint modeling approaches. Recently,
generative approaches have been proposed to extract all four elements as (one
or more) quadruplets from text as a single task. In this work, we take a step
further and propose a unified framework for solving ABSA, and the associated
sub-tasks to improve the performance in few-shot scenarios. To this end, we
fine-tune a T5 model with instructional prompts in a multi-task learning
fashion covering all the sub-tasks, as well as the entire quadruple prediction
task. In experiments with multiple benchmark datasets, we show that the
proposed multi-task prompting approach brings performance boost (by absolute
8.29 F1) in the few-shot learning setting.",https://github.com/amazon-science/instruction-tuning-for-absa,-1
Memory-Based Model Editing at Scale,0.490758,"Even the largest neural networks make errors, and once-correct predictions
can become invalid as the world changes. Model editors make local updates to
the behavior of base (pre-trained) models to inject updated knowledge or
correct undesirable behaviors. Existing model editors have shown promise, but
also suffer from insufficient expressiveness: they struggle to accurately model
an edit's intended scope (examples affected by the edit), leading to inaccurate
predictions for test inputs loosely related to the edit, and they often fail
altogether after many edits. As a higher-capacity alternative, we propose
Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model
(SERAC), which stores edits in an explicit memory and learns to reason over
them to modulate the base model's predictions as needed. To enable more
rigorous evaluation of model editors, we introduce three challenging language
model editing problems based on question answering, fact-checking, and dialogue
generation. We find that only SERAC achieves high performance on all three
problems, consistently outperforming existing approaches to model editing by a
significant margin. Code, data, and additional project information will be made
available at https://sites.google.com/view/serac-editing.",None,-1
Conformal Risk Control,0.887906,"We extend conformal prediction to control the expected value of any monotone
loss function. The algorithm generalizes split conformal prediction together
with its coverage guarantee. Like conformal prediction, the conformal risk
control procedure is tight up to an $\mathcal{O}(1/n)$ factor. We also
introduce extensions of the idea to distribution shift, quantile risk control,
multiple and adversarial risk control, and expectations of U-statistics. Worked
examples from computer vision and natural language processing demonstrate the
usage of our algorithm to bound the false negative rate, graph distance, and
token-level F1-score.",None,-1
Large Language Models Are Reasoning Teachers,0.974122,"Recent works have shown that chain-of-thought (CoT) prompting can elicit
language models to solve complex reasoning tasks, step-by-step. However,
prompt-based CoT methods are dependent on very large models such as GPT-3 175B
which are prohibitive to deploy at scale. In this paper, we use these large
models as reasoning teachers to enable complex reasoning in smaller models and
reduce model size requirements by several orders of magnitude. We propose
Fine-tune-CoT, a method that generates reasoning samples from very large
teacher models to fine-tune smaller models. We evaluate our method on a wide
range of public models and complex tasks. We find that Fine-tune-CoT enables
substantial reasoning capability in small models, far outperforming
prompt-based baselines and even the teacher model in many tasks. Additionally,
we extend our method by leveraging the teacher model's ability to generate
multiple distinct rationales for each original sample. Enriching the
fine-tuning data with such diverse reasoning results in a substantial
performance boost across datasets, even for very small models. We conduct
ablations and sample studies to understand the emergence of reasoning
capabilities of student models. Our code implementation and data are available
at https://github.com/itsnamgyu/reasoning-teacher.",https://github.com/itsnamgyu/reasoning-teacher,-1
Visual Acoustic Matching,0.228849,"We introduce the visual acoustic matching task, in which an audio clip is
transformed to sound like it was recorded in a target environment. Given an
image of the target environment and a waveform for the source audio, the goal
is to re-synthesize the audio to match the target room acoustics as suggested
by its visible geometry and materials. To address this novel task, we propose a
cross-modal transformer model that uses audio-visual attention to inject visual
properties into the audio and generate realistic audio output. In addition, we
devise a self-supervised training objective that can learn acoustic matching
from in-the-wild Web videos, despite their lack of acoustically mismatched
audio. We demonstrate that our approach successfully translates human speech to
a variety of real-world environments depicted in images, outperforming both
traditional acoustic matching and more heavily supervised baselines.",None,-1
Flexible Diffusion Modeling of Long Videos,0.933456,"We present a framework for video modeling based on denoising diffusion
probabilistic models that produces long-duration video completions in a variety
of realistic environments. We introduce a generative model that can at
test-time sample any arbitrary subset of video frames conditioned on any other
subset and present an architecture adapted for this purpose. Doing so allows us
to efficiently compare and optimize a variety of schedules for the order in
which frames in a long video are sampled and use selective sparse and
long-range conditioning on previously sampled frames. We demonstrate improved
video modeling over prior work on a number of datasets and sample temporally
coherent videos over 25 minutes in length. We additionally release a new video
modeling dataset and semantically meaningful metrics based on videos generated
in the CARLA autonomous driving simulator.",https://github.com/vaibhavsaxena11/cwvae,-1
Boundary Smoothing for Named Entity Recognition,0.234542,"Neural named entity recognition (NER) models may easily encounter the
over-confidence issue, which degrades the performance and calibration. Inspired
by label smoothing and driven by the ambiguity of boundary annotation in NER
engineering, we propose boundary smoothing as a regularization technique for
span-based neural NER models. It re-assigns entity probabilities from annotated
spans to the surrounding ones. Built on a simple but strong baseline, our model
achieves results better than or competitive with previous state-of-the-art
systems on eight well-known NER benchmarks. Further empirical analysis suggests
that boundary smoothing effectively mitigates over-confidence, improves model
calibration, and brings flatter neural minima and more smoothed loss
landscapes.",https://github.com/syuoni/eznlp,-1
Emergent Communication for Understanding Human Language Evolution: What's Missing?,0.188079,"Emergent communication protocols among humans and artificial neural network
agents do not yet share the same properties and show some critical mismatches
in results. We describe three important phenomena with respect to the emergence
and benefits of compositionality: ease-of-learning, generalization, and group
size effects (i.e., larger groups create more systematic languages). The latter
two are not fully replicated with neural agents, which hinders the use of
neural emergent communication for language evolution research. We argue that
one possible reason for these mismatches is that key cognitive and
communicative constraints of humans are not yet integrated. Specifically, in
humans, memory constraints and the alternation between the roles of speaker and
listener underlie the emergence of linguistic structure, yet these constraints
are typically absent in neural simulations. We suggest that introducing such
communicative and cognitive constraints would promote more linguistically
plausible behaviors with neural agents.",None,-1
Continuous Scene Representations for Embodied AI,0.11896,"We propose Continuous Scene Representations (CSR), a scene representation
constructed by an embodied agent navigating within a space, where objects and
their relationships are modeled by continuous valued embeddings. Our method
captures feature relationships between objects, composes them into a graph
structure on-the-fly, and situates an embodied agent within the representation.
Our key insight is to embed pair-wise relationships between objects in a latent
space. This allows for a richer representation compared to discrete relations
(e.g., [support], [next-to]) commonly used for building scene representations.
CSR can track objects as the agent moves in a scene, update the representation
accordingly, and detect changes in room configurations. Using CSR, we
outperform state-of-the-art approaches for the challenging downstream task of
visual room rearrangement, without any task specific training. Moreover, we
show the learned embeddings capture salient spatial details of the scene and
show applicability to real world data. A summery video and code is available at
https://prior.allenai.org/projects/csr.",https://github.com/facebookresearch/detectron2,-1
LiDARCap: Long-range Marker-less 3D Human Motion Capture with LiDAR Point Clouds,0.199134,"Existing motion capture datasets are largely short-range and cannot yet fit
the need of long-range applications. We propose LiDARHuman26M, a new human
motion capture dataset captured by LiDAR at a much longer range to overcome
this limitation. Our dataset also includes the ground truth human motions
acquired by the IMU system and the synchronous RGB images. We further present a
strong baseline method, LiDARCap, for LiDAR point cloud human motion capture.
Specifically, we first utilize PointNet++ to encode features of points and then
employ the inverse kinematics solver and SMPL optimizer to regress the pose
through aggregating the temporally encoded features hierarchically.
Quantitative and qualitative experiments show that our method outperforms the
techniques based only on RGB images. Ablation experiments demonstrate that our
dataset is challenging and worthy of further research. Finally, the experiments
on the KITTI Dataset and the Waymo Open Dataset show that our method can be
generalized to different LiDAR sensor settings.",None,-1
FreGAN: Exploiting Frequency Components for Training GANs under Limited Data,0.202105,"Training GANs under limited data often leads to discriminator overfitting and
memorization issues, causing divergent training. Existing approaches mitigate
the overfitting by employing data augmentations, model regularization, or
attention mechanisms. However, they ignore the frequency bias of GANs and take
poor consideration towards frequency information, especially high-frequency
signals that contain rich details. To fully utilize the frequency information
of limited data, this paper proposes FreGAN, which raises the model's frequency
awareness and draws more attention to producing high-frequency signals,
facilitating high-quality generation. In addition to exploiting both real and
generated images' frequency information, we also involve the frequency signals
of real images as a self-supervised constraint, which alleviates the GAN
disequilibrium and encourages the generator to synthesize adequate rather than
arbitrary frequency signals. Extensive results demonstrate the superiority and
effectiveness of our FreGAN in ameliorating generation quality in the low-data
regime (especially when training data is less than 100). Besides, FreGAN can be
seamlessly applied to existing regularization and attention mechanism models to
further boost the performance.",https://github.com/kobeshegu/FreGAN_NeurIPS2022,-1
Transformation-Equivariant 3D Object Detection for Autonomous Driving,0.228777,"3D object detection received increasing attention in autonomous driving
recently. Objects in 3D scenes are distributed with diverse orientations.
Ordinary detectors do not explicitly model the variations of rotation and
reflection transformations. Consequently, large networks and extensive data
augmentation are required for robust detection. Recent equivariant networks
explicitly model the transformation variations by applying shared networks on
multiple transformed point clouds, showing great potential in object geometry
modeling. However, it is difficult to apply such networks to 3D object
detection in autonomous driving due to its large computation cost and slow
reasoning speed. In this work, we present TED, an efficient
Transformation-Equivariant 3D Detector to overcome the computation cost and
speed issues. TED first applies a sparse convolution backbone to extract
multi-channel transformation-equivariant voxel features; and then aligns and
aggregates these equivariant features into lightweight and compact
representations for high-performance 3D object detection. On the highly
competitive KITTI 3D car detection leaderboard, TED ranked 1st among all
submissions with competitive efficiency.",None,-1
CONCRETE: Improving Cross-lingual Fact-checking with Cross-lingual Retrieval,0.396022,"Fact-checking has gained increasing attention due to the widespread of
falsified information. Most fact-checking approaches focus on claims made in
English only due to the data scarcity issue in other languages. The lack of
fact-checking datasets in low-resource languages calls for an effective
cross-lingual transfer technique for fact-checking. Additionally, trustworthy
information in different languages can be complementary and helpful in
verifying facts. To this end, we present the first fact-checking framework
augmented with cross-lingual retrieval that aggregates evidence retrieved from
multiple languages through a cross-lingual retriever. Given the absence of
cross-lingual information retrieval datasets with claim-like queries, we train
the retriever with our proposed Cross-lingual Inverse Cloze Task (X-ICT), a
self-supervised algorithm that creates training instances by translating the
title of a passage. The goal for X-ICT is to learn cross-lingual retrieval in
which the model learns to identify the passage corresponding to a given
translated title. On the X-Fact dataset, our approach achieves 2.23% absolute
F1 improvement in the zero-shot cross-lingual setup over prior systems. The
source code and data are publicly available at
https://github.com/khuangaf/CONCRETE.",https://github.com/khuangaf/CONCRETE,-1
MatteFormer: Transformer-Based Image Matting via Prior-Tokens,0.359677,"In this paper, we propose a transformer-based image matting model called
MatteFormer, which takes full advantage of trimap information in the
transformer block. Our method first introduces a prior-token which is a global
representation of each trimap region (e.g. foreground, background and unknown).
These prior-tokens are used as global priors and participate in the
self-attention mechanism of each block. Each stage of the encoder is composed
of PAST (Prior-Attentive Swin Transformer) block, which is based on the Swin
Transformer block, but differs in a couple of aspects: 1) It has PA-WSA
(Prior-Attentive Window Self-Attention) layer, performing self-attention not
only with spatial-tokens but also with prior-tokens. 2) It has prior-memory
which saves prior-tokens accumulatively from the previous blocks and transfers
them to the next block. We evaluate our MatteFormer on the commonly used image
matting datasets: Composition-1k and Distinctions-646. Experiment results show
that our proposed method achieves state-of-the-art performance with a large
margin. Our codes are available at https://github.com/webtoon/matteformer.",https://github.com/webtoon/matteformer,-1
"Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2",0.20827,"The field of image synthesis has made great strides in the last couple of
years. Recent models are capable of generating images with astonishing quality.
Fine-grained evaluation of these models on some interesting categories such as
faces is still missing. Here, we conduct a quantitative comparison of three
popular systems including Stable Diffusion, Midjourney, and DALL-E 2 in their
ability to generate photorealistic faces in the wild. We find that Stable
Diffusion generates better faces than the other systems, according to the FID
score. We also introduce a dataset of generated faces in the wild dubbed GFW,
including a total of 15,076 faces. Furthermore, we hope that our study spurs
follow-up research in assessing the generative models and improving them. Data
and code are available at data and code, respectively.",None,-1
WiCV 2021: The Eighth Women In Computer Vision Workshop,0.108673,"In this paper, we present the details of Women in Computer Vision Workshop -
WiCV 2021, organized alongside the virtual CVPR 2021. It provides a voice to a
minority (female) group in the computer vision community and focuses on
increasing the visibility of these researchers, both in academia and industry.
WiCV believes that such an event can play an important role in lowering the
gender imbalance in the field of computer vision. WiCV is organized each year
where it provides a)~opportunity for collaboration between researchers from
minority groups, b)~mentorship to female junior researchers, c)~financial
support to presenters to overcome monetary burden and d)~large and diverse
choice of role models, who can serve as examples to younger researchers at the
beginning of their careers. In this paper, we present a report on the workshop
program, trends over the past years, a summary of statistics regarding
presenters, attendees, and sponsorship for the WiCV 2021 workshop.",None,-1
Treewidth-aware Reductions of Normal ASP to SAT -- Is Normal ASP Harder than SAT after All?,0.295012,"Answer Set Programming (ASP) is a paradigm for modeling and solving problems
for knowledge representation and reasoning. There are plenty of results
dedicated to studying the hardness of (fragments of) ASP. So far, these studies
resulted in characterizations in terms of computational complexity as well as
in fine-grained insights presented in form of dichotomy-style results, lower
bounds when translating to other formalisms like propositional satisfiability
(SAT), and even detailed parameterized complexity landscapes. A generic
parameter in parameterized complexity originating from graph theory is the
so-called treewidth, which in a sense captures structural density of a program.
Recently, there was an increase in the number of treewidth-based solvers
related to SAT. While there are translations from (normal) ASP to SAT, no
reduction that preserves treewidth or at least keeps track of the treewidth
increase is known. In this paper we propose a novel reduction from normal ASP
to SAT that is aware of the treewidth, and guarantees that a slight increase of
treewidth is indeed sufficient. Further, we show a new result establishing
that, when considering treewidth, already the fragment of normal ASP is
slightly harder than SAT (under reasonable assumptions in computational
complexity). This also confirms that our reduction probably cannot be
significantly improved and that the slight increase of treewidth is
unavoidable. Finally, we present an empirical study of our novel reduction from
normal ASP to SAT, where we compare treewidth upper bounds that are obtained
via known decomposition heuristics. Overall, our reduction works better with
these heuristics than existing translations.",https://github.com/hmarkus/asp2sat translator,-1
Black-Box Tuning for Language-Model-as-a-Service,0.950062,"Extremely large pre-trained language models (PTMs) such as GPT-3 are usually
released as a service. It allows users to design task-specific prompts to query
the PTMs through some black-box APIs. In such a scenario, which we call
Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually
unavailable. Can we optimize the task prompts by only accessing the model
inference APIs? This paper proposes the black-box tuning framework to optimize
the continuous prompt prepended to the input text via derivative-free
optimization. Instead of optimizing in the original high-dimensional prompt
space, which is intractable for traditional derivative-free optimization, we
perform optimization in a randomly generated subspace due to the low intrinsic
dimensionality of large PTMs. The experimental results show that the black-box
tuning with RoBERTa on a few labeled samples not only significantly outperforms
manual prompt and GPT-3's in-context learning, but also surpasses the
gradient-based counterparts, i.e., prompt tuning and full model tuning.",https://github.com/txsun1997/Black-Box-Tuning,-1
Non-Uniformly Terminating Chase: Size and Complexity,0.0686408,"The chase procedure, originally introduced for checking implication of
database constraints, and later on used for computing data exchange solutions,
has recently become a central algorithmic tool in rule-based ontological
reasoning. In this context, a key problem is non-uniform chase termination:
does the chase of a database w.r.t. a rule-based ontology terminate? And if
this is the case, what is the size of the result of the chase? We focus on
guarded tuple-generating dependencies (TGDs), which form a robust rule-based
ontology language, and study the above central questions for the semi-oblivious
version of the chase. One of our main findings is that non-uniform
semi-oblivious chase termination for guarded TGDs is feasible in polynomial
time w.r.t. the database, and the size of the result of the chase (whenever is
finite) is linear w.r.t. the database. Towards our results concerning
non-uniform chase termination, we show that basic techniques such as
simplification and linearization, originally introduced in the context of
ontological query answering, can be safely applied to the chase termination
problem.",None,-1
Less is More: Learning to Refine Dialogue History for Personalized Dialogue Generation,0.261485,"Personalized dialogue systems explore the problem of generating responses
that are consistent with the user's personality, which has raised much
attention in recent years. Existing personalized dialogue systems have tried to
extract user profiles from dialogue history to guide personalized response
generation. Since the dialogue history is usually long and noisy, most existing
methods truncate the dialogue history to model the user's personality. Such
methods can generate some personalized responses, but a large part of dialogue
history is wasted, leading to sub-optimal performance of personalized response
generation. In this work, we propose to refine the user dialogue history on a
large scale, based on which we can handle more dialogue history and obtain more
abundant and accurate persona information. Specifically, we design an MSP model
which consists of three personal information refiners and a personalized
response generator. With these multi-level refiners, we can sparsely extract
the most valuable information (tokens) from the dialogue history and leverage
other similar users' data to enhance personalization. Experimental results on
two real-world datasets demonstrate the superiority of our model in generating
more informative and personalized responses.",https://github.com/bangbangbang12315/MSP/tree/release,-1
Privacy-Preserving Image Classification Using Vision Transformer,0.27198,"In this paper, we propose a privacy-preserving image classification method
that is based on the combined use of encrypted images and the vision
transformer (ViT). The proposed method allows us not only to apply images
without visual information to ViT models for both training and testing but to
also maintain a high classification accuracy. ViT utilizes patch embedding and
position embedding for image patches, so this architecture is shown to reduce
the influence of block-wise image transformation. In an experiment, the
proposed method for privacy-preserving image classification is demonstrated to
outperform state-of-the-art methods in terms of classification accuracy and
robustness against various attacks.",None,-1
Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation,0.606611,"The recent large-scale vision-language pre-training (VLP) of dual-stream
architectures (e.g., CLIP) with a tremendous amount of image-text pair data,
has shown its superiority on various multimodal alignment tasks. Despite its
success, the resulting models are not capable of multimodal generative tasks
due to the weak text encoder. To tackle this problem, we propose to augment the
dual-stream VLP model with a textual pre-trained language model (PLM) via
vision-language knowledge distillation (VLKD), enabling the capability for
multimodal generation. VLKD is pretty data- and computation-efficient compared
to the pre-training from scratch. Experimental results show that the resulting
model has strong zero-shot performance on multimodal generation tasks, such as
open-ended visual question answering and image captioning. For example, it
achieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous
state-of-the-art zero-shot model with $7\times$ fewer parameters. Furthermore,
the original textual language understanding and generation ability of the PLM
is maintained after VLKD, which makes our model versatile for both multimodal
and unimodal tasks.",None,-1
Prompt Distribution Learning,0.295546,"We present prompt distribution learning for effectively adapting a
pre-trained vision-language model to address downstream recognition tasks. Our
method not only learns low-bias prompts from a few samples but also captures
the distribution of diverse prompts to handle the varying visual
representations. In this way, we provide high-quality task-related content for
facilitating recognition. This prompt distribution learning is realized by an
efficient approach that learns the output embeddings of prompts instead of the
input embeddings. Thus, we can employ a Gaussian distribution to model them
effectively and derive a surrogate loss for efficient training. Extensive
experiments on 12 datasets demonstrate that our method consistently and
significantly outperforms existing methods. For example, with 1 sample per
category, it relatively improves the average result by 9.1% compared to
human-crafted prompts.",None,-1
KPI-BERT: A Joint Named Entity Recognition and Relation Extraction Model for Financial Reports,0.141706,"We present KPI-BERT, a system which employs novel methods of named entity
recognition (NER) and relation extraction (RE) to extract and link key
performance indicators (KPIs), e.g. ""revenue"" or ""interest expenses"", of
companies from real-world German financial documents. Specifically, we
introduce an end-to-end trainable architecture that is based on Bidirectional
Encoder Representations from Transformers (BERT) combining a recurrent neural
network (RNN) with conditional label masking to sequentially tag entities
before it classifies their relations. Our model also introduces a learnable
RNN-based pooling mechanism and incorporates domain expert knowledge by
explicitly filtering impossible relations. We achieve a substantially higher
prediction performance on a new practical dataset of German financial reports,
outperforming several strong baselines including a competing state-of-the-art
span-based entity tagging approach.",None,-1
Teaching language models to support answers with verified quotes,0.864411,"Recent large language models often answer factual questions correctly. But
users can't trust any given claim a model makes without fact-checking, because
language models can hallucinate convincing nonsense. In this work we use
reinforcement learning from human preferences (RLHP) to train ""open-book"" QA
models that generate answers whilst also citing specific evidence for their
claims, which aids in the appraisal of correctness. Supporting evidence is
drawn from multiple documents found via a search engine, or from a single
user-provided document. Our 280 billion parameter model, GopherCite, is able to
produce answers with high quality supporting evidence and abstain from
answering when unsure. We measure the performance of GopherCite by conducting
human evaluation of answers to questions in a subset of the NaturalQuestions
and ELI5 datasets. The model's response is found to be high-quality 80\% of the
time on this Natural Questions subset, and 67\% of the time on the ELI5 subset.
Abstaining from the third of questions for which it is most unsure improves
performance to 90\% and 80\% respectively, approaching human baselines.
However, analysis on the adversarial TruthfulQA dataset shows why citation is
only one part of an overall strategy for safety and trustworthiness: not all
claims supported by evidence are true.",None,-1
Understanding Masked Image Modeling via Learning Occlusion Invariant Feature,0.176299,"Recently, Masked Image Modeling (MIM) achieves great success in
self-supervised visual recognition. However, as a reconstruction-based
framework, it is still an open question to understand how MIM works, since MIM
appears very different from previous well-studied siamese approaches such as
contrastive learning. In this paper, we propose a new viewpoint: MIM implicitly
learns occlusion-invariant features, which is analogous to other siamese
methods while the latter learns other invariance. By relaxing MIM formulation
into an equivalent siamese form, MIM methods can be interpreted in a unified
framework with conventional methods, among which only a) data transformations,
i.e. what invariance to learn, and b) similarity measurements are different.
Furthermore, taking MAE (He et al.) as a representative example of MIM, we
empirically find the success of MIM models relates a little to the choice of
similarity functions, but the learned occlusion invariant feature introduced by
masked image -- it turns out to be a favored initialization for vision
transformers, even though the learned feature could be less semantic. We hope
our findings could inspire researchers to develop more powerful self-supervised
methods in computer vision community.",None,-1
A Large Scale Search Dataset for Unbiased Learning to Rank,0.40558,"The unbiased learning to rank (ULTR) problem has been greatly advanced by
recent deep learning techniques and well-designed debias algorithms. However,
promising results on the existing benchmark datasets may not be extended to the
practical scenario due to the following disadvantages observed from those
popular benchmark datasets: (1) outdated semantic feature extraction where
state-of-the-art large scale pre-trained language models like BERT cannot be
exploited due to the missing of the original text;(2) incomplete display
features for in-depth study of ULTR, e.g., missing the displayed abstract of
documents for analyzing the click necessary bias; (3) lacking real-world user
feedback, leading to the prevalence of synthetic datasets in the empirical
study. To overcome the above disadvantages, we introduce the Baidu-ULTR
dataset. It involves randomly sampled 1.2 billion searching sessions and 7,008
expert annotated queries, which is orders of magnitude larger than the existing
ones. Baidu-ULTR provides:(1) the original semantic feature and a pre-trained
language model for easy usage; (2) sufficient display information such as
position, displayed height, and displayed abstract, enabling the comprehensive
study of different biases with advanced techniques such as causal discovery and
meta-learning; and (3) rich user feedback on search result pages (SERPs) like
dwelling time, allowing for user engagement optimization and promoting the
exploration of multi-task learning in ULTR. In this paper, we present the
design principle of Baidu-ULTR and the performance of benchmark ULTR algorithms
on this new data resource, favoring the exploration of ranking for long-tail
queries and pre-training tasks for ranking. The Baidu-ULTR dataset and
corresponding baseline implementation are available at
https://github.com/ChuXiaokai/baidu_ultr_dataset.",https://github.com/ChuXiaokai/baidu_ultr_dataset,-1
DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech,0.480815,"The majority of current Text-to-Speech (TTS) datasets, which are collections
of individual utterances, contain few conversational aspects. In this paper, we
introduce DailyTalk, a high-quality conversational speech dataset designed for
conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the
open-domain dialogue dataset DailyDialog inheriting its annotated attributes.
On top of our dataset, we extend prior work as our baseline, where a
non-autoregressive TTS is conditioned on historical information in a dialogue.
From the baseline experiment with both general and our novel metrics, we show
that DailyTalk can be used as a general TTS dataset, and more than that, our
baseline can represent contextual information from DailyTalk. The DailyTalk
dataset and baseline code are freely available for academic use with CC-BY-SA
4.0 license.",https://github.com/keonlee9420/DailyTalk,-1
GreaseLM: Graph REASoning Enhanced Language Models for Question Answering,0.684991,"Answering complex questions about textual narratives requires reasoning over
both stated context and the world knowledge that underlies it. However,
pretrained language models (LM), the foundation of most modern QA systems, do
not robustly represent latent relationships between concepts, which is
necessary for reasoning. While knowledge graphs (KG) are often used to augment
LMs with structured representations of world knowledge, it remains an open
question how to effectively fuse and reason over the KG representations and the
language context, which provides situational constraints and nuances. In this
work, we propose GreaseLM, a new model that fuses encoded representations from
pretrained LMs and graph neural networks over multiple layers of modality
interaction operations. Information from both modalities propagates to the
other, allowing language context representations to be grounded by structured
world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in
the context to inform the graph representations of knowledge. Our results on
three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA)
and medical question answering (i.e., MedQA-USMLE) domains demonstrate that
GreaseLM can more reliably answer questions that require reasoning over both
situational constraints and structured knowledge, even outperforming models 8x
larger.",https://github.com/snap-stanford/GreaseLM,-1
Multi-Behavior Enhanced Recommendation with Cross-Interaction Collaborative Relation Modeling,0.410196,"Many previous studies aim to augment collaborative filtering with deep neural
network techniques, so as to achieve better recommendation performance.
However, most existing deep learning-based recommender systems are designed for
modeling singular type of user-item interaction behavior, which can hardly
distill the heterogeneous relations between user and item. In practical
recommendation scenarios, there exist multityped user behaviors, such as browse
and purchase. Due to the overlook of user's multi-behavioral patterns over
different items, existing recommendation methods are insufficient to capture
heterogeneous collaborative signals from user multi-behavior data. Inspired by
the strength of graph neural networks for structured data modeling, this work
proposes a Graph Neural Multi-Behavior Enhanced Recommendation (GNMR) framework
which explicitly models the dependencies between different types of user-item
interactions under a graph-based message passing architecture. GNMR devises a
relation aggregation network to model interaction heterogeneity, and
recursively performs embedding propagation between neighboring nodes over the
user-item interaction graph. Experiments on real-world recommendation datasets
show that our GNMR consistently outperforms state-of-the-art methods. The
source code is available at https://github.com/akaxlh/GNMR.",https://github.com/akaxlh/GNMR,-1
Rhino: Deep Causal Temporal Relationship Learning With History-dependent Noise,0.165598,"Discovering causal relationships between different variables from time series
data has been a long-standing challenge for many domains such as climate
science, finance, and healthcare. Given the complexity of real-world
relationships and the nature of observations in discrete time, causal discovery
methods need to consider non-linear relations between variables, instantaneous
effects and history-dependent noise (the change of noise distribution due to
past actions). However, previous works do not offer a solution addressing all
these problems together. In this paper, we propose a novel causal relationship
learning framework for time-series data, called Rhino, which combines vector
auto-regression, deep learning and variational inference to model non-linear
relationships with instantaneous effects while allowing the noise distribution
to be modulated by historical observations. Theoretically, we prove the
structural identifiability of Rhino. Our empirical results from extensive
synthetic experiments and two real-world benchmarks demonstrate better
discovery performance compared to relevant baselines, with ablation studies
revealing its robustness under model misspecification.",https://github.com/sakhanna/SRU_for_GCI/tree/master/data,-1
Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection,0.322179,"In some scenarios, classifier requires detecting out-of-distribution samples
far from its training data. With desirable characteristics, reconstruction
autoencoder-based methods deal with this problem by using input reconstruction
error as a metric of novelty vs. normality. We formulate the essence of such
approach as a quadruplet domain translation with an intrinsic bias to only
query for a proxy of conditional data uncertainty. Accordingly, an improvement
direction is formalized as maximumly compressing the autoencoder's latent space
while ensuring its reconstructive power for acting as a described domain
translator. From it, strategies are introduced including semantic
reconstruction, data certainty decomposition and normalized L2 distance to
substantially improve original methods, which together establish
state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of
CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method
works without any additional data, hard-to-implement structure, time-consuming
pipeline, and even harming the classification accuracy of known classes.",https://github.com/xxx,-1
ToKen: Task Decomposition and Knowledge Infusion for Few-Shot Hate Speech Detection,0.273795,"Hate speech detection is complex; it relies on commonsense reasoning,
knowledge of stereotypes, and an understanding of social nuance that differs
from one culture to the next. It is also difficult to collect a large-scale
hate speech annotated dataset. In this work, we frame this problem as a
few-shot learning task, and show significant gains with decomposing the task
into its ""constituent"" parts. In addition, we see that infusing knowledge from
reasoning datasets (e.g. Atomic2020) improves the performance even further.
Moreover, we observe that the trained models generalize to out-of-distribution
datasets, showing the superiority of task decomposition and knowledge infusion
compared to previously used methods. Concretely, our method outperforms the
baseline by 17.83% absolute gain in the 16-shot case.",None,-1
Data Contamination: From Memorization to Exploitation,0.649438,"Pretrained language models are typically trained on massive web-based
datasets, which are often ""contaminated"" with downstream test sets. It is not
clear to what extent models exploit the contaminated data for downstream tasks.
We present a principled method to study this question. We pretrain BERT models
on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune
them on the relevant task. Comparing performance between samples seen and
unseen during pretraining enables us to define and quantify levels of
memorization and exploitation. Experiments with two models and three downstream
tasks show that exploitation exists in some cases, but in others the models
memorize the contaminated data, but do not exploit it. We show that these two
measures are affected by different factors such as the number of duplications
of the contaminated data and the model size. Our results highlight the
importance of analyzing massive web-scale datasets to verify that progress in
NLP is obtained by better language understanding and not better data
exploitation.",https://github.com/schwartz-lab-NLP/data_contamination,-1
Convolutional and Residual Networks Provably Contain Lottery Tickets,0.18532,"The Lottery Ticket Hypothesis continues to have a profound practical impact
on the quest for small scale deep neural networks that solve modern deep
learning tasks at competitive performance. These lottery tickets are identified
by pruning large randomly initialized neural networks with architectures that
are as diverse as their applications. Yet, theoretical insights that attest
their existence have been mostly focused on deep fully-connected feed forward
networks with ReLU activation functions. We prove that also modern
architectures consisting of convolutional and residual layers that can be
equipped with almost arbitrary activation functions can contain lottery tickets
with high probability.",None,-1
An ASP approach for reasoning on neural networks under a finitely many-valued semantics for weighted conditional knowledge bases,0.123466,"Weighted knowledge bases for description logics with typicality have been
recently considered under a ""concept-wise"" multipreference semantics (in both
the two-valued and fuzzy case), as the basis of a logical semantics of
MultiLayer Perceptrons (MLPs). In this paper we consider weighted conditional
ALC knowledge bases with typicality in the finitely many-valued case, through
three different semantic constructions. For the boolean fragment LC of ALC we
exploit ASP and ""asprin"" for reasoning with the concept-wise multipreference
entailment under a phi-coherent semantics, suitable to characterize the
stationary states of MLPs. As a proof of concept, we experiment the proposed
approach for checking properties of trained MLPs.
  The paper is under consideration for acceptance in TPLP.",None,-1
ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding,0.331489,"Recently, learned image compression techniques have achieved remarkable
performance, even surpassing the best manually designed lossy image coders.
They are promising to be large-scale adopted. For the sake of practicality, a
thorough investigation of the architecture design of learned image compression,
regarding both compression performance and running speed, is essential. In this
paper, we first propose uneven channel-conditional adaptive coding, motivated
by the observation of energy compaction in learned image compression. Combining
the proposed uneven grouping model with existing context models, we obtain a
spatial-channel contextual adaptive model to improve the coding performance
without damage to running speed. Then we study the structure of the main
transform and propose an efficient model, ELIC, to achieve state-of-the-art
speed and compression ability. With superior performance, the proposed model
also supports extremely fast preview decoding and progressive decoding, which
makes the coming application of learning-based image compression more
promising.",https://github.com/InterDigitalInc/CompressAI/blob/v1.1.8/results/kodak/vtm.json,-1
The Inverse of Exact Renormalization Group Flows as Statistical Inference,0.352224,"We build on the view of the Exact Renormalization Group (ERG) as an
instantiation of Optimal Transport described by a functional
convection-diffusion equation. We provide a new information theoretic
perspective for understanding the ERG through the intermediary of Bayesian
Statistical Inference. This connection is facilitated by the Dynamical Bayesian
Inference scheme, which encodes Bayesian inference in the form of a one
parameter family of probability distributions solving an integro-differential
equation derived from Bayes' law. In this note, we demonstrate how the
Dynamical Bayesian Inference equation is, itself, equivalent to a diffusion
equation which we dub Bayesian Diffusion. Identifying the features that define
Bayesian Diffusion, and mapping them onto the features that define the ERG, we
obtain a dictionary outlining how renormalization can be understood as the
inverse of statistical inference.",None,-1
"GigaST: A 10,000-hour Pseudo Speech Translation Corpus",0.410083,"This paper introduces GigaST, a large-scale pseudo speech translation (ST)
corpus. We create the corpus by translating the text in GigaSpeech, an English
ASR corpus, into German and Chinese. The training set is translated by a strong
machine translation system and the test set is translated by human. ST models
trained with an addition of our corpus obtain new state-of-the-art results on
the MuST-C English-German benchmark test set. We provide a detailed description
of the translation process and verify its quality. We make the translated text
data public and hope to facilitate research in speech translation.
Additionally, we also release the training scripts on NeurST to make it easy to
replicate our systems. GigaST dataset is available at
https://st-benchmark.github.io/resources/GigaST.",https://github.com/bytedance/neurst/,-1
Learning Smooth Neural Functions via Lipschitz Regularization,0.243965,"Neural implicit fields have recently emerged as a useful representation for
3D shapes. These fields are commonly represented as neural networks which map
latent descriptors and 3D coordinates to implicit function values. The latent
descriptor of a neural field acts as a deformation handle for the 3D shape it
represents. Thus, smoothness with respect to this descriptor is paramount for
performing shape-editing operations. In this work, we introduce a novel
regularization designed to encourage smooth latent spaces in neural fields by
penalizing the upper bound on the field's Lipschitz constant. Compared with
prior Lipschitz regularized networks, ours is computationally fast, can be
implemented in four lines of code, and requires minimal hyperparameter tuning
for geometric applications. We demonstrate the effectiveness of our approach on
shape interpolation and extrapolation as well as partial shape reconstruction
from 3D point clouds, showing both qualitative and quantitative improvements
over existing state-of-the-art and non-regularized baselines.",http://github.com/google/jax,-1
Gendered Mental Health Stigma in Masked Language Models,0.659273,"Mental health stigma prevents many individuals from receiving the appropriate
care, and social psychology studies have shown that mental health tends to be
overlooked in men. In this work, we investigate gendered mental health stigma
in masked language models. In doing so, we operationalize mental health stigma
by developing a framework grounded in psychology research: we use clinical
psychology literature to curate prompts, then evaluate the models' propensity
to generate gendered words. We find that masked language models capture
societal stigma about gender in mental health: models are consistently more
likely to predict female subjects than male in sentences about having a mental
health condition (32% vs. 19%), and this disparity is exacerbated for sentences
that indicate treatment-seeking behavior. Furthermore, we find that different
models capture dimensions of stigma differently for men and women, associating
stereotypes like anger, blame, and pity more with women with mental health
conditions than with men. In showing the complex nuances of models' gendered
mental health stigma, we demonstrate that context and overlapping dimensions of
identity are important considerations when assessing computational models'
social biases.",https://github.com/LucilleN/Gendered-MH-Stigma-in-Masked-LMs,-1
MDFEND: Multi-domain Fake News Detection,0.989939,"Fake news spread widely on social media in various domains, which lead to
real-world threats in many aspects like politics, disasters, and finance. Most
existing approaches focus on single-domain fake news detection (SFND), which
leads to unsatisfying performance when these methods are applied to
multi-domain fake news detection. As an emerging field, multi-domain fake news
detection (MFND) is increasingly attracting attention. However, data
distributions, such as word frequency and propagation patterns, vary from
domain to domain, namely domain shift. Facing the challenge of serious domain
shift, existing fake news detection techniques perform poorly for multi-domain
scenarios. Therefore, it is demanding to design a specialized model for MFND.
In this paper, we first design a benchmark of fake news dataset for MFND with
domain label annotated, namely Weibo21, which consists of 4,488 fake news and
4,640 real news from 9 different domains. We further propose an effective
Multi-domain Fake News Detection Model (MDFEND) by utilizing a domain gate to
aggregate multiple representations extracted by a mixture of experts. The
experiments show that MDFEND can significantly improve the performance of
multi-domain fake news detection. Our dataset and code are available at
https://github.com/kennqiang/MDFEND-Weibo21.",None,-1
Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study,0.180753,"In this work, we present an extensive study on the use of pre-trained
language models for the task of automatic Counter Narrative (CN) generation to
fight online hate speech in English. We first present a comparative study to
determine whether there is a particular Language Model (or class of LMs) and a
particular decoding mechanism that are the most appropriate to generate CNs.
Findings show that autoregressive models combined with stochastic decodings are
the most promising. We then investigate how an LM performs in generating a CN
with regard to an unseen target of hate. We find out that a key element for
successful `out of target' experiments is not an overall similarity with the
training data but the presence of a specific subset of training data, i.e. a
target that shares some commonalities with the test target that can be defined
a-priori. We finally introduce the idea of a pipeline based on the addition of
an automatic post-editing step to refine generated CNs.",None,-1
SegViT: Semantic Segmentation with Plain Vision Transformers,0.112642,"We explore the capability of plain Vision Transformers (ViTs) for semantic
segmentation and propose the SegVit. Previous ViT-based segmentation networks
usually learn a pixel-level representation from the output of the ViT.
Differently, we make use of the fundamental component -- attention mechanism,
to generate masks for semantic segmentation. Specifically, we propose the
Attention-to-Mask (ATM) module, in which the similarity maps between a set of
learnable class tokens and the spatial feature maps are transferred to the
segmentation masks. Experiments show that our proposed SegVit using the ATM
module outperforms its counterparts using the plain ViT backbone on the ADE20K
dataset and achieves new state-of-the-art performance on COCO-Stuff-10K and
PASCAL-Context datasets. Furthermore, to reduce the computational cost of the
ViT backbone, we propose query-based down-sampling (QD) and query-based
up-sampling (QU) to build a Shrunk structure. With the proposed Shrunk
structure, the model can save up to $40\%$ computations while maintaining
competitive performance.",https://github.com/zbwxp/SegVit,-1
PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions,0.832064,"Cross-entropy loss and focal loss are the most common choices when training
deep neural networks for classification problems. Generally speaking, however,
a good loss function can take on much more flexible forms, and should be
tailored for different tasks and datasets. Motivated by how functions can be
approximated via Taylor expansion, we propose a simple framework, named
PolyLoss, to view and design loss functions as a linear combination of
polynomial functions. Our PolyLoss allows the importance of different
polynomial bases to be easily adjusted depending on the targeting tasks and
datasets, while naturally subsuming the aforementioned cross-entropy loss and
focal loss as special cases. Extensive experimental results show that the
optimal choice within the PolyLoss is indeed dependent on the task and dataset.
Simply by introducing one extra hyperparameter and adding one line of code, our
Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D
image classification, instance segmentation, object detection, and 3D object
detection tasks, sometimes by a large margin.",https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/car,-1
Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction,0.630297,"Recently, neural implicit surfaces learning by volume rendering has become
popular for multi-view reconstruction. However, one key challenge remains:
existing approaches lack explicit multi-view geometry constraints, hence
usually fail to generate geometry consistent surface reconstruction. To address
this challenge, we propose geometry-consistent neural implicit surfaces
learning for multi-view reconstruction. We theoretically analyze that there
exists a gap between the volume rendering integral and point-based signed
distance function (SDF) modeling. To bridge this gap, we directly locate the
zero-level set of SDF networks and explicitly perform multi-view geometry
optimization by leveraging the sparse geometry from structure from motion (SFM)
and photometric consistency in multi-view stereo. This makes our SDF
optimization unbiased and allows the multi-view geometry constraints to focus
on the true surface optimization. Extensive experiments show that our proposed
method achieves high-quality surface reconstruction in both complex thin
structures and large smooth regions, thus outperforming the state-of-the-arts
by a large margin.",None,-1
Towards a Unified Multi-Dimensional Evaluator for Text Generation,0.637384,"Multi-dimensional evaluation is the dominant paradigm for human evaluation in
Natural Language Generation (NLG), i.e., evaluating the generated text from
multiple explainable dimensions, such as coherence and fluency. However,
automatic evaluation in NLG is still dominated by similarity-based metrics, and
we lack a reliable framework for a more comprehensive evaluation of advanced
models. In this paper, we propose a unified multi-dimensional evaluator UniEval
for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task,
and by guiding the model with different questions, we can use one evaluator to
evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean
QA format, we are able to introduce an intermediate learning phase that enables
UniEval to incorporate external knowledge from multiple related tasks and gain
further improvement. Experiments on three typical NLG tasks show that UniEval
correlates substantially better with human judgments than existing metrics.
Specifically, compared to the top-performing unified evaluators, UniEval
achieves a 23% higher correlation on text summarization, and over 43% on
dialogue response generation. Also, UniEval demonstrates a strong zero-shot
learning ability for unseen evaluation dimensions and tasks. Source code, data
and all pre-trained evaluators are available on our GitHub repository
(https://github.com/maszhongming/UniEval).",https://github.com/maszhongming/UniEval,-1
Acknowledging the Unknown for Multi-label Learning with Single Positive Labels,0.195754,"Due to the difficulty of collecting exhaustive multi-label annotations,
multi-label datasets often contain partial labels. We consider an extreme of
this weakly supervised learning problem, called single positive multi-label
learning (SPML), where each multi-label training image has only one positive
label. Traditionally, all unannotated labels are assumed as negative labels in
SPML, which introduces false negative labels and causes model training to be
dominated by assumed negative labels. In this work, we choose to treat all
unannotated labels from an alternative perspective, i.e. acknowledging they are
unknown. Hence, we propose entropy-maximization (EM) loss to attain a special
gradient regime for providing proper supervision signals. Moreover, we propose
asymmetric pseudo-labeling (APL), which adopts asymmetric-tolerance strategies
and a self-paced procedure, to cooperate with EM loss and then provide more
precise supervision. Experiments show that our method significantly improves
performance and achieves state-of-the-art results on all four benchmarks. Code
is available at https://github.com/Correr-Zhou/SPML-AckTheUnknown.",https://github.com/Correr-Zhou/SPML-AckTheUnknown,-1
Reinforcement Learning with Prior Policy Guidance for Motion Planning of Dual-Arm Free-Floating Space Robot,0.391433,"Reinforcement learning methods as a promising technique have achieved
superior results in the motion planning of free-floating space robots. However,
due to the increase in planning dimension and the intensification of system
dynamics coupling, the motion planning of dual-arm free-floating space robots
remains an open challenge. In particular, the current study cannot handle the
task of capturing a non-cooperative object due to the lack of the pose
constraint of the end-effectors. To address the problem, we propose a novel
algorithm, EfficientLPT, to facilitate RL-based methods to improve planning
accuracy efficiently. Our core contributions are constructing a mixed policy
with prior knowledge guidance and introducing infinite norm to build a more
reasonable reward function. Furthermore, our method successfully captures a
rotating object with different spinning speeds.",None,-1
Summarization as Indirect Supervision for Relation Extraction,0.264666,"Relation extraction (RE) models have been challenged by their reliance on
training data with expensive annotations. Considering that summarization tasks
aim at acquiring concise expressions of synoptical information from the longer
context, these tasks naturally align with the objective of RE, i.e., extracting
a kind of synoptical information that describes the relation of entity
mentions. We present SuRE, which converts RE into a summarization formulation.
SuRE leads to more precise and resource-efficient RE based on indirect
supervision from summarization tasks. To achieve this goal, we develop sentence
and relation conversion techniques that essentially bridge the formulation of
summarization and RE tasks. We also incorporate constraint decoding techniques
with Trie scoring to further enhance summarization-based RE with robust
inference. Experiments on three RE datasets demonstrate the effectiveness of
SuRE in both full-dataset and low-resource settings, showing that summarization
is a promising source of indirect supervision to improve RE models.",https://github.com/luka-group/SuRE,-1
RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization,0.817124,"6-DoF object pose estimation from a monocular image is challenging, and a
post-refinement procedure is generally needed for high-precision estimation. In
this paper, we propose a framework based on a recurrent neural network (RNN)
for object pose refinement, which is robust to erroneous initial poses and
occlusions. During the recurrent iterations, object pose refinement is
formulated as a non-linear least squares problem based on the estimated
correspondence field (between a rendered image and the observed image). The
problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm
enabling end-to-end training. The correspondence field estimation and pose
refinement are conducted alternatively in each iteration to recover the object
poses. Furthermore, to improve the robustness to occlusion, we introduce a
consistency-check mechanism based on the learned descriptors of the 3D model
and observed 2D images, which downweights the unreliable correspondences during
pose optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and
YCB-Video datasets validate the effectiveness of our method and demonstrate
state-of-the-art performance.",https://github.com/DecaYale/RNNPose,-1
Mixed Differential Privacy in Computer Vision,0.215297,"We introduce AdaMix, an adaptive differentially private algorithm for
training deep neural network classifiers using both private and public image
data. While pre-training language models on large public datasets has enabled
strong differential privacy (DP) guarantees with minor loss of accuracy, a
similar practice yields punishing trade-offs in vision tasks. A few-shot or
even zero-shot learning baseline that ignores private data can outperform
fine-tuning on a large private dataset. AdaMix incorporates few-shot training,
or cross-modal zero-shot learning, on public data prior to private fine-tuning,
to improve the trade-off. AdaMix reduces the error increase from the
non-private upper bound from the 167-311\% of the baseline, on average across 6
datasets, to 68-92\% depending on the desired privacy level selected by the
user. AdaMix tackles the trade-off arising in visual classification, whereby
the most privacy sensitive data, corresponding to isolated points in
representation space, are also critical for high classification accuracy. In
addition, AdaMix comes with strong theoretical privacy guarantees and
convergence analysis.",https://github.com/yuxiangw/autodp,-1
Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images,0.0291096,"We present Neural Strands, a novel learning framework for modeling accurate
hair geometry and appearance from multi-view image inputs. The learned hair
model can be rendered in real-time from any viewpoint with high-fidelity
view-dependent effects. Our model achieves intuitive shape and style control
unlike volumetric counterparts. To enable these properties, we propose a novel
hair representation based on a neural scalp texture that encodes the geometry
and appearance of individual strands at each texel location. Furthermore, we
introduce a novel neural rendering framework based on rasterization of the
learned hair strands. Our neural rendering is strand-accurate and anti-aliased,
making the rendering view-consistent and photorealistic. Combining appearance
with a multi-view geometric prior, we enable, for the first time, the joint
learning of appearance and explicit hair geometry from a multi-view setup. We
demonstrate the efficacy of our approach in terms of fidelity and efficiency
for various hairstyles.",None,-1
Label Semantics for Few Shot Named Entity Recognition,0.372126,"We study the problem of few shot learning for named entity recognition.
Specifically, we leverage the semantic information in the names of the labels
as a way of giving the model additional signal and enriched priors. We propose
a neural architecture that consists of two BERT encoders, one to encode the
document and its tokens and another one to encode each of the labels in natural
language format. Our model learns to match the representations of named
entities computed by the first encoder with label representations computed by
the second encoder. The label semantics signal is shown to support improved
state-of-the-art results in multiple few shot NER benchmarks and on-par
performance in standard benchmarks. Our model is especially effective in low
resource settings.",None,-1
Bitext Mining Using Distilled Sentence Representations for Low-Resource Languages,0.118242,"Scaling multilingual representation learning beyond the hundred most frequent
languages is challenging, in particular to cover the long tail of low-resource
languages. A promising approach has been to train one-for-all multilingual
models capable of cross-lingual transfer, but these models often suffer from
insufficient capacity and interference between unrelated languages. Instead, we
move away from this approach and focus on training multiple language (family)
specific representations, but most prominently enable all languages to still be
encoded in the same representational space. To achieve this, we focus on
teacher-student training, allowing all encoders to be mutually compatible for
bitext mining, and enabling fast learning of new languages. We introduce a new
teacher-student training scheme which combines supervised and self-supervised
training, allowing encoders to take advantage of monolingual training data,
which is valuable in the low-resource setting.
  Our approach significantly outperforms the original LASER encoder. We study
very low-resource languages and handle 50 African languages, many of which are
not covered by any other model. For these languages, we train sentence
encoders, mine bitexts, and validate the bitexts by training NMT systems.",https://github.com/facebookresearch/LASER,-1
Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting,0.205273,"In document classification for, e.g., legal and biomedical text, we often
deal with hundreds of classes, including very infrequent ones, as well as
temporal concept drift caused by the influence of real world events, e.g.,
policy changes, conflicts, or pandemics. Class imbalance and drift can
sometimes be mitigated by resampling the training data to simulate (or
compensate for) a known target distribution, but what if the target
distribution is determined by unknown future events? Instead of simply
resampling uniformly to hedge our bets, we focus on the underlying optimization
algorithms used to train such document classifiers and evaluate several
group-robust optimization algorithms, initially proposed to mitigate
group-level disparities. Reframing group-robust algorithms as adaptation
algorithms under concept drift, we find that Invariant Risk Minimization and
Spectral Decoupling outperform sampling-based approaches to class imbalance and
concept drift, and lead to much better performance on minority classes. The
effect is more pronounced the larger the label set.",https://github.com/coastalcph/lw-robust,-1
InternVideo-Ego4D: A Pack of Champion Solutions to Ego4D Challenges,0.884418,"In this report, we present our champion solutions to five tracks at Ego4D
challenge. We leverage our developed InternVideo, a video foundation model, for
five Ego4D tasks, including Moment Queries, Natural Language Queries, Future
Hand Prediction, State Change Object Detection, and Short-term Object
Interaction Anticipation. InternVideo-Ego4D is an effective paradigm to adapt
the strong foundation model to the downstream ego-centric video understanding
tasks with simple head designs. In these five tasks, the performance of
InternVideo-Ego4D comprehensively surpasses the baseline methods and the
champions of CVPR2022, demonstrating the powerful representation ability of
InternVideo as a video foundation model. Our code will be released at
https://github.com/OpenGVLab/ego4d-eccv2022-solutions",https://github.com/OpenGVLab/ego4d-eccv2022-solutions,-1
Cardinality-Regularized Hawkes-Granger Model,0.192066,"We propose a new sparse Granger-causal learning framework for temporal event
data. We focus on a specific class of point processes called the Hawkes
process. We begin by pointing out that most of the existing sparse causal
learning algorithms for the Hawkes process suffer from a singularity in maximum
likelihood estimation. As a result, their sparse solutions can appear only as
numerical artifacts. In this paper, we propose a mathematically well-defined
sparse causal learning framework based on a cardinality-regularized Hawkes
process, which remedies the pathological issues of existing approaches. We
leverage the proposed algorithm for the task of instance-wise causal event
analysis, where sparsity plays a critical role. We validate the proposed
framework with two real use-cases, one from the power grid and the other from
the cloud data center management domain.",https://github.com/iancovert/Neural-GC,-1
SocialVAE: Human Trajectory Prediction using Timewise Latents,0.0,"Predicting pedestrian movement is critical for human behavior analysis and
also for safe and efficient human-agent interactions. However, despite
significant advancements, it is still challenging for existing approaches to
capture the uncertainty and multimodality of human navigation decision making.
In this paper, we propose SocialVAE, a novel approach for human trajectory
prediction. The core of SocialVAE is a timewise variational autoencoder
architecture that exploits stochastic recurrent neural networks to perform
prediction, combined with a social attention mechanism and a backward posterior
approximation to allow for better extraction of pedestrian navigation
strategies. We show that SocialVAE improves current state-of-the-art
performance on several pedestrian trajectory prediction benchmarks, including
the ETH/UCY benchmark, Stanford Drone Dataset, and SportVU NBA movement
dataset. Code is available at: https://github.com/xupei0610/SocialVAE.",None,-1
CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022,0.473537,"In this paper, we describe our submission to the Simultaneous Speech
Translation at IWSLT 2022. We explore strategies to utilize an offline model in
a simultaneous setting without the need to modify the original model. In our
experiments, we show that our onlinization algorithm is almost on par with the
offline setting while being $3\times$ faster than offline in terms of latency
on the test set. We also show that the onlinized offline model outperforms the
best IWSLT2021 simultaneous system in medium and high latency regimes and is
almost on par in the low latency regime. We make our system publicly available.",https://hub.docker.com/repository/docker/polape7/cuni-kit-simultaneous,-1
Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather,0.223763,"Most object detection methods for autonomous driving usually assume a
consistent feature distribution between training and testing data, which is not
always the case when weathers differ significantly. The object detection model
trained under clear weather might not be effective enough in foggy weather
because of the domain gap. This paper proposes a novel domain adaptive object
detection framework for autonomous driving under foggy weather. Our method
leverages both image-level and object-level adaptation to diminish the domain
discrepancy in image style and object appearance. To further enhance the
model's capabilities under challenging samples, we also come up with a new
adversarial gradient reversal layer to perform adversarial mining for the hard
examples together with domain adaptation. Moreover, we propose to generate an
auxiliary domain by data augmentation to enforce a new domain-level metric
regularization. Experimental results on public benchmarks show the
effectiveness and accuracy of the proposed method. The code is available at
https://github.com/jinlong17/DA-Detect.",https://github.com/jinlong17/DA-Detect,-1
Uncertainty Inspired Underwater Image Enhancement,0.728507,"A main challenge faced in the deep learning-based Underwater Image
Enhancement (UIE) is that the ground truth high-quality image is unavailable.
Most of the existing methods first generate approximate reference maps and then
train an enhancement network with certainty. This kind of method fails to
handle the ambiguity of the reference map. In this paper, we resolve UIE into
distribution estimation and consensus process. We present a novel probabilistic
network to learn the enhancement distribution of degraded underwater images.
Specifically, we combine conditional variational autoencoder with adaptive
instance normalization to construct the enhancement distribution. After that,
we adopt a consensus process to predict a deterministic result based on a set
of samples from the distribution. By learning the enhancement distribution, our
method can cope with the bias introduced in the reference map labeling to some
extent. Additionally, the consensus process is useful to capture a robust and
stable result. We examined the proposed method on two widely used real-world
underwater image enhancement datasets. Experimental results demonstrate that
our approach enables sampling possible enhancement predictions. Meanwhile, the
consensus estimate yields competitive performance compared with
state-of-the-art UIE methods. Code available at
https://github.com/zhenqifu/PUIE-Net.",https://github.com/zhenqifu/PUIE-Net,-1
Coupled Iterative Refinement for 6D Multi-Object Pose Estimation,0.376839,"We address the task of 6D multi-object pose: given a set of known 3D objects
and an RGB or RGB-D input image, we detect and estimate the 6D pose of each
object. We propose a new approach to 6D object pose estimation which consists
of an end-to-end differentiable architecture that makes use of geometric
knowledge. Our approach iteratively refines both pose and correspondence in a
tightly coupled manner, allowing us to dynamically remove outliers to improve
accuracy. We use a novel differentiable layer to perform pose refinement by
solving an optimization problem we refer to as Bidirectional Depth-Augmented
Perspective-N-Point (BD-PnP). Our method achieves state-of-the-art accuracy on
standard 6D Object Pose benchmarks. Code is available at
https://github.com/princeton-vl/Coupled-Iterative-Refinement.",None,-1
Online Decision Transformer,0.770206,"Recent work has shown that offline reinforcement learning (RL) can be
formulated as a sequence modeling problem (Chen et al., 2021; Janner et al.,
2021) and solved via approaches similar to large-scale language modeling.
However, any practical instantiation of RL also involves an online component,
where policies pretrained on passive offline datasets are finetuned via
taskspecific interactions with the environment. We propose Online Decision
Transformers (ODT), an RL algorithm based on sequence modeling that blends
offline pretraining with online finetuning in a unified framework. Our
framework uses sequence-level entropy regularizers in conjunction with
autoregressive modeling objectives for sample-efficient exploration and
finetuning. Empirically, we show that ODT is competitive with the
state-of-the-art in absolute performance on the D4RL benchmark but shows much
more significant gains during the finetuning procedure.",https://github.com/kzl/decision-transformer,-1
Dual-Scale Single Image Dehazing Via Neural Augmentation,0.155479,"Model-based single image dehazing algorithms restore haze-free images with
sharp edges and rich details for real-world hazy images at the expense of low
PSNR and SSIM values for synthetic hazy images. Data-driven ones restore
haze-free images with high PSNR and SSIM values for synthetic hazy images but
with low contrast, and even some remaining haze for real world hazy images. In
this paper, a novel single image dehazing algorithm is introduced by combining
model-based and data-driven approaches. Both transmission map and atmospheric
light are first estimated by the model-based methods, and then refined by
dual-scale generative adversarial networks (GANs) based approaches. The
resultant algorithm forms a neural augmentation which converges very fast while
the corresponding data-driven approach might not converge. Haze-free images are
restored by using the estimated transmission map and atmospheric light as well
as the Koschmiederlaw. Experimental results indicate that the proposed
algorithm can remove haze well from real-world and synthetic hazy images.",None,-1
Towards Improved Room Impulse Response Estimation for Speech Recognition,0.282992,"We propose a novel approach for blind room impulse response (RIR) estimation
systems in the context of a downstream application scenario, far-field
automatic speech recognition (ASR). We first draw the connection between
improved RIR estimation and improved ASR performance, as a means of evaluating
neural RIR estimators. We then propose a generative adversarial network (GAN)
based architecture that encodes RIR features from reverberant speech and
constructs an RIR from the encoded features, and uses a novel energy decay
relief loss to optimize for capturing energy-based properties of the input
reverberant speech. We show that our model outperforms the state-of-the-art
baselines on acoustic benchmarks (by 17\% on the energy decay relief and 22\%
on an early-reflection energy metric), as well as in an ASR evaluation task (by
6.9\% in word error rate).",https://github.com/RoyJames/kaldi-reverb/,-1
Red Teaming Language Models with Language Models,0.61459,"Language Models (LMs) often cannot be deployed because of their potential to
harm users in hard-to-predict ways. Prior work identifies harmful behaviors
before deployment by using human annotators to hand-write test cases. However,
human annotation is expensive, limiting the number and diversity of test cases.
In this work, we automatically find cases where a target LM behaves in a
harmful way, by generating test cases (""red teaming"") using another LM. We
evaluate the target LM's replies to generated test questions using a classifier
trained to detect offensive content, uncovering tens of thousands of offensive
replies in a 280B parameter LM chatbot. We explore several methods, from
zero-shot generation to reinforcement learning, for generating test cases with
varying levels of diversity and difficulty. Furthermore, we use prompt
engineering to control LM-generated test cases to uncover a variety of other
harms, automatically finding groups of people that the chatbot discusses in
offensive ways, personal and hospital phone numbers generated as the chatbot's
own contact info, leakage of private training data in generated text, and harms
that occur over the course of a conversation. Overall, LM-based red teaming is
one promising tool (among many needed) for finding and fixing diverse,
undesirable LM behaviors before impacting users.",None,-1
Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer,0.66673,"Text spotting end-to-end methods have recently gained attention in the
literature due to the benefits of jointly optimizing the text detection and
recognition components. Existing methods usually have a distinct separation
between the detection and recognition branches, requiring exact annotations for
the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach
for text spotting and the first text spotting framework which may be trained
with both fully- and weakly-supervised settings. By learning a single latent
representation per word detection, and using a novel loss function based on the
Hungarian loss, our method alleviates the need for expensive localization
annotations. Trained with only text transcription annotations on real data, our
weakly-supervised method achieves competitive performance with previous
state-of-the-art fully-supervised methods. When trained in a fully-supervised
manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks.",None,-1
Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation,0.918898,"Capturing emotions within a conversation plays an essential role in modern
dialogue systems. However, the weak correlation between emotions and semantics
brings many challenges to emotion recognition in conversation (ERC). Even
semantically similar utterances, the emotion may vary drastically depending on
contexts or speakers. In this paper, we propose a Supervised Prototypical
Contrastive Learning (SPCL) loss for the ERC task. Leveraging the Prototypical
Network, the SPCL targets at solving the imbalanced classification problem
through contrastive learning and does not require a large batch size.
Meanwhile, we design a difficulty measure function based on the distance
between classes and introduce curriculum learning to alleviate the impact of
extreme samples. We achieve state-of-the-art results on three widely used
benchmarks. Further, we conduct analytical experiments to demonstrate the
effectiveness of our proposed SPCL and curriculum learning strategy. We release
the code at https://github.com/caskcsg/SPCL.",https://github.com/caskcsg/SPCL,-1
Class-Incremental Learning for Action Recognition in Videos,0.278574,"We tackle catastrophic forgetting problem in the context of class-incremental
learning for video recognition, which has not been explored actively despite
the popularity of continual learning. Our framework addresses this challenging
task by introducing time-channel importance maps and exploiting the importance
maps for learning the representations of incoming examples via knowledge
distillation. We also incorporate a regularization scheme in our objective
function, which encourages individual features obtained from different time
steps in a video to be uncorrelated and eventually improves accuracy by
alleviating catastrophic forgetting. We evaluate the proposed approach on
brand-new splits of class-incremental action recognition benchmarks constructed
upon the UCF101, HMDB51, and Something-Something V2 datasets, and demonstrate
the effectiveness of our algorithm in comparison to the existing continual
learning methods that are originally designed for image data.",https://github.com/mit-han-lab/temporal-shift-module,-1
Thin-Plate Spline Motion Model for Image Animation,0.840597,"Image animation brings life to the static object in the source image
according to the driving video. Recent works attempt to perform motion transfer
on arbitrary objects through unsupervised methods without using a priori
knowledge. However, it remains a significant challenge for current unsupervised
methods when there is a large pose gap between the objects in the source and
driving images. In this paper, a new end-to-end unsupervised motion transfer
framework is proposed to overcome such issue. Firstly, we propose thin-plate
spline motion estimation to produce a more flexible optical flow, which warps
the feature maps of the source image to the feature domain of the driving
image. Secondly, in order to restore the missing regions more realistically, we
leverage multi-resolution occlusion masks to achieve more effective feature
fusion. Finally, additional auxiliary loss functions are designed to ensure
that there is a clear division of labor in the network modules, encouraging the
network to generate high-quality images. Our method can animate a variety of
objects, including talking faces, human bodies, and pixel animations.
Experiments demonstrate that our method performs better on most benchmarks than
the state of the art with visible improvements in pose-related metrics.",https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model,-1
Solving Quantitative Reasoning Problems with Language Models,0.996592,"Language models have achieved remarkable performance on a wide range of tasks
that require natural language understanding. Nevertheless, state-of-the-art
models have generally struggled with tasks that require quantitative reasoning,
such as solving mathematics, science, and engineering problems at the college
level. To help close this gap, we introduce Minerva, a large language model
pretrained on general natural language data and further trained on technical
content. The model achieves state-of-the-art performance on technical
benchmarks without the use of external tools. We also evaluate our model on
over two hundred undergraduate-level problems in physics, biology, chemistry,
economics, and other sciences that require quantitative reasoning, and find
that the model can correctly answer nearly a third of them.",None,-1
BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach,0.576162,"Bilevel optimization (BO) is useful for solving a variety of important
machine learning problems including but not limited to hyperparameter
optimization, meta-learning, continual learning, and reinforcement learning.
Conventional BO methods need to differentiate through the low-level
optimization process with implicit differentiation, which requires expensive
calculations related to the Hessian matrix. There has been a recent quest for
first-order methods for BO, but the methods proposed to date tend to be
complicated and impractical for large-scale deep learning applications. In this
work, we propose a simple first-order BO algorithm that depends only on
first-order gradient information, requires no implicit differentiation, and is
practical and efficient for large-scale non-convex functions in deep learning.
We provide non-asymptotic convergence analysis of the proposed method to
stationary points for non-convex objectives and present empirical results that
show its superior practical performance.",https://github.com/JunjieYang97/stocBiO,-1
Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds,0.209027,"Dense captioning in 3D point clouds is an emerging vision-and-language task
involving object-level 3D scene understanding. Apart from coarse semantic class
prediction and bounding box regression as in traditional 3D object detection,
3D dense captioning aims at producing a further and finer instance-level label
of natural language description on visual appearance and spatial relations for
each scene object of interest. To detect and describe objects in a scene,
following the spirit of neural machine translation, we propose a
transformer-based encoder-decoder architecture, namely SpaCap3D, to transform
objects into descriptions, where we especially investigate the relative
spatiality of objects in 3D scenes and design a spatiality-guided encoder via a
token-to-token spatial relation learning objective and an object-centric
decoder for precise and spatiality-enhanced object caption generation.
Evaluated on two benchmark datasets, ScanRefer and ReferIt3D, our proposed
SpaCap3D outperforms the baseline method Scan2Cap by 4.94% and 9.61% in
CIDEr@0.5IoU, respectively. Our project page with source code and supplementary
files is available at https://SpaCap3D.github.io/ .",https://github.com/heng-hw/SpaCap3D,-1
Benchmarking Self-Supervised Learning on Diverse Pathology Datasets,0.269813,"Computational pathology can lead to saving human lives, but models are
annotation hungry and pathology images are notoriously expensive to annotate.
Self-supervised learning has shown to be an effective method for utilizing
unlabeled data, and its application to pathology could greatly benefit its
downstream tasks. Yet, there are no principled studies that compare SSL methods
and discuss how to adapt them for pathology. To address this need, we execute
the largest-scale study of SSL pre-training on pathology image data, to date.
Our study is conducted using 4 representative SSL methods on diverse downstream
tasks. We establish that large-scale domain-aligned pre-training in pathology
consistently out-performs ImageNet pre-training in standard SSL settings such
as linear and fine-tuning evaluations, as well as in low-label regimes.
Moreover, we propose a set of domain-specific techniques that we experimentally
show leads to a performance boost. Lastly, for the first time, we apply SSL to
the challenging task of nuclei instance segmentation and show large and
consistent performance improvements under diverse settings.",https://lunit-io.github.io/research/publications/pathology_ssl,-1
Knowledge Distillation with the Reused Teacher Classifier,0.612966,"Knowledge distillation aims to compress a powerful yet cumbersome teacher
model into a lightweight student model without much sacrifice of performance.
For this purpose, various approaches have been proposed over the past few
years, generally with elaborately designed knowledge representations, which in
turn increase the difficulty of model development and interpretation. In
contrast, we empirically show that a simple knowledge distillation technique is
enough to significantly narrow down the teacher-student performance gap. We
directly reuse the discriminative classifier from the pre-trained teacher model
for student inference and train a student encoder through feature alignment
with a single $\ell_2$ loss. In this way, the student model is able to achieve
exactly the same performance as the teacher model provided that their extracted
features are perfectly aligned. An additional projector is developed to help
the student encoder match with the teacher classifier, which renders our
technique applicable to various teacher and student architectures. Extensive
experiments demonstrate that our technique achieves state-of-the-art results at
the modest cost of compression ratio due to the added projector.",https://github.com/Rorozhl/CA-MKD,-1
MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning,0.496342,"Instruction tuning, a new learning paradigm that fine-tunes pre-trained
language models on tasks specified through instructions, has shown promising
zero-shot performance on various natural language processing tasks. However, it
has yet to be explored for vision and multimodal tasks. In this work, we
introduce MUL-TIINSTRUCT, the first multimodal instruction tuning benchmark
dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq
format covering 10 broad categories. The tasks are derived from 21 existing
open-source datasets and each task is equipped with 5 expert-written
instructions. We take OFA as the base pre-trained model for multimodal
instruction tuning, and to further improve its zero-shot performance, we
explore multiple transfer learning strategies to leverage the large-scale
NATURAL INSTRUCTIONS dataset. Experimental results demonstrate strong zero-shot
performance on various unseen multimodal tasks and the benefit of transfer
learning from a text-only instruction dataset. We also design a new evaluation
metric - Sensitivity, to evaluate how sensitive the model is to the variety of
instructions. Our results indicate that fine-tuning the model on a diverse set
of tasks and instructions leads to a reduced sensitivity to variations in
instructions for each task.",https://github.com/VT-NLP/MultiInstruct,-1
Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis,0.464344,"Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a
specific aspect in the given sentence. While pre-trained language models such
as BERT have achieved great success, incorporating dynamic semantic changes
into ABSA remains challenging. To this end, in this paper, we propose to
address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method
designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we
first take the Stack-BERT layers as a primary encoder to grasp the overall
semantic of the sentence and then fine-tune it by incorporating a lightweight
Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention
to a small region of the sentences at each step and re-weigh the vitally
important words for better aspect-aware sentiment understanding. Finally,
experimental results on three benchmark datasets demonstrate the effectiveness
and the rationality of our proposed model and provide good interpretable
insights for future semantic modeling.",None,-1
BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog,0.431889,"A typical end-to-end task-oriented dialog system transfers context into
dialog state, and upon which generates a response, which usually faces the
problem of error propagation from both previously generated inaccurate dialog
states and responses, especially in low-resource scenarios. To alleviate these
issues, we propose BORT, a back and denoising reconstruction approach for
end-to-end task-oriented dialog system. Squarely, to improve the accuracy of
dialog states, back reconstruction is used to reconstruct the original input
context from the generated dialog states since inaccurate dialog states cannot
recover the corresponding input context. To enhance the denoising capability of
the model to reduce the impact of error propagation, denoising reconstruction
is used to reconstruct the corrupted dialog state and response. Extensive
experiments conducted on MultiWOZ 2.0 and CamRest676 show the effectiveness of
BORT. Furthermore, BORT demonstrates its advanced capabilities in the zero-shot
domain and low-resource scenarios.",https://github.com/,-1
Category-Level 6D Object Pose Estimation in the Wild: A Semi-Supervised Learning Approach and A New Dataset,0.289318,"6D object pose estimation is one of the fundamental problems in computer
vision and robotics research. While a lot of recent efforts have been made on
generalizing pose estimation to novel object instances within the same
category, namely category-level 6D pose estimation, it is still restricted in
constrained environments given the limited number of annotated data. In this
paper, we collect Wild6D, a new unlabeled RGBD object video dataset with
diverse instances and backgrounds. We utilize this data to generalize
category-level 6D object pose estimation in the wild with semi-supervised
learning. We propose a new model, called Rendering for Pose estimation network
RePoNet, that is jointly trained using the free ground-truths with the
synthetic data, and a silhouette matching objective function on the real-world
data. Without using any 3D annotations on real data, our method outperforms
state-of-the-art methods on the previous dataset and our Wild6D test set (with
manual annotations for evaluation) by a large margin. Project page with Wild6D
data: https://oasisyang.github.io/semi-pose .",None,-1
Probing Pre-Trained Language Models for Cross-Cultural Differences in Values,0.861396,"Language embeds information about social, cultural, and political values
people hold. Prior work has explored social and potentially harmful biases
encoded in Pre-Trained Language models (PTLMs). However, there has been no
systematic study investigating how values embedded in these models vary across
cultures. In this paper, we introduce probes to study which values across
cultures are embedded in these models, and whether they align with existing
theories and cross-cultural value surveys. We find that PTLMs capture
differences in values across cultures, but those only weakly align with
established value surveys. We discuss implications of using mis-aligned models
in cross-cultural settings, as well as ways of aligning PTLMs with value
surveys.",None,-1
Robust $Q$-learning Algorithm for Markov Decision Processes under Wasserstein Uncertainty,0.170716,"We present a novel $Q$-learning algorithm to solve distributionally robust
Markov decision problems, where the corresponding ambiguity set of transition
probabilities for the underlying Markov decision process is a Wasserstein ball
around a (possibly estimated) reference measure. We prove convergence of the
presented algorithm and provide several examples also using real data to
illustrate both the tractability of our algorithm as well as the benefits of
considering distributional robustness when solving stochastic optimal control
problems, in particular when the estimated distributions turn out to be
misspecified in practice.",https://github.com/juliansester/Wasserstein-Q-learning,-1
PHEE: A Dataset for Pharmacovigilance Event Extraction from Text,0.207434,"The primary goal of drug safety researchers and regulators is to promptly
identify adverse drug reactions. Doing so may in turn prevent or reduce the
harm to patients and ultimately improve public health. Evaluating and
monitoring drug safety (i.e., pharmacovigilance) involves analyzing an ever
growing collection of spontaneous reports from health professionals,
physicians, and pharmacists, and information voluntarily submitted by patients.
In this scenario, facilitating analysis of such reports via automation has the
potential to rapidly identify safety signals. Unfortunately, public resources
for developing natural language models for this task are scant. We present
PHEE, a novel dataset for pharmacovigilance comprising over 5000 annotated
events from medical case reports and biomedical literature, making it the
largest such public dataset to date. We describe the hierarchical event schema
designed to provide coarse and fine-grained information about patients'
demographics, treatments and (side) effects. Along with the discussion of the
dataset, we present a thorough experimental evaluation of current
state-of-the-art approaches for biomedical event extraction, point out their
limitations, and highlight open challenges to foster future research in this
area.",https://github.com/ZhaoyueSun/PHEE,-1
BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing,0.507285,"Training and evaluating language models increasingly requires the
construction of meta-datasets --diverse collections of curated data with clear
provenance. Natural language prompting has recently lead to improved zero-shot
generalization by transforming existing, supervised datasets into a diversity
of novel pretraining tasks, highlighting the benefits of meta-dataset curation.
While successful in general-domain text, translating these data-centric
approaches to biomedical language modeling remains challenging, as labeled
biomedical datasets are significantly underrepresented in popular data hubs. To
address this challenge, we introduce BigBIO a community library of 126+
biomedical NLP datasets, currently covering 12 task categories and 10+
languages. BigBIO facilitates reproducible meta-dataset curation via
programmatic access to datasets and their metadata, and is compatible with
current platforms for prompt engineering and end-to-end few/zero shot language
model evaluation. We discuss our process for task schema harmonization, data
auditing, contribution guidelines, and outline two illustrative use cases:
zero-shot evaluation of biomedical prompts and large-scale, multi-task
learning. BigBIO is an ongoing community effort and is available at
https://github.com/bigscience-workshop/biomedical",https://github.com/bigscience-workshop/biomedical,-1
Augmentation Matters: A Simple-yet-Effective Approach to Semi-supervised Semantic Segmentation,0.238062,"Recent studies on semi-supervised semantic segmentation (SSS) have seen fast
progress. Despite their promising performance, current state-of-the-art methods
tend to increasingly complex designs at the cost of introducing more network
components and additional training procedures. Differently, in this work, we
follow a standard teacher-student framework and propose AugSeg, a simple and
clean approach that focuses mainly on data perturbations to boost the SSS
performance. We argue that various data augmentations should be adjusted to
better adapt to the semi-supervised scenarios instead of directly applying
these techniques from supervised learning. Specifically, we adopt a simplified
intensity-based augmentation that selects a random number of data
transformations with uniformly sampling distortion strengths from a continuous
space. Based on the estimated confidence of the model on different unlabeled
samples, we also randomly inject labelled information to augment the unlabeled
samples in an adaptive manner. Without bells and whistles, our simple AugSeg
can readily achieve new state-of-the-art performance on SSS benchmarks under
different partition protocols.",https://github.com/zhenzhao/AugSeg,-1
Large Language Models Struggle to Learn Long-Tail Knowledge,0.939386,"The Internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, while certain pieces of information are ubiquitous
on the web, others appear extremely rarely. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in pre-training datasets scraped from the web. In particular, we
show that a language model's ability to answer a fact-based question relates to
how many documents associated with that question were seen during pre-training.
We identify these relevant documents by entity linking pre-training datasets
and counting documents that contain the same entities as a given
question-answer pair. Our results demonstrate strong correlational and causal
relationships between accuracy and relevant document count for numerous
question answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,
ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models
are better at learning long-tail knowledge, we estimate that today's models
must be scaled by many orders of magnitude to reach competitive QA performance
on questions with little support in the pre-training data. Finally, we show
that retrieval-augmentation can reduce the dependence on relevant pre-training
information, presenting a promising approach for capturing the long-tail.",https://github.com/kingoflolz/mesh-transformer-jax,-1
Can language models learn from explanations in context?,0.781143,"Language Models (LMs) can perform new tasks by adapting to a few in-context
examples. For humans, explanations that connect examples to task principles can
improve learning. We therefore investigate whether explanations of few-shot
examples can help LMs. We annotate questions from 40 challenging tasks with
answer explanations, and various matched control explanations. We evaluate how
different types of explanations, instructions, and controls affect zero- and
few-shot performance. We analyze these results using statistical multilevel
modeling techniques that account for the nested dependencies among conditions,
tasks, prompts, and models. We find that explanations can improve performance
-- even without tuning. Furthermore, explanations hand-tuned for performance on
a small validation set offer substantially larger benefits, and building a
prompt by selecting examples and explanations together substantially improves
performance over selecting examples alone. Finally, even untuned explanations
outperform carefully matched controls, suggesting that the benefits are due to
the link between an example and its explanation, rather than lower-level
features. However, only large models benefit. In summary, explanations can
support the in-context learning of large LMs on challenging tasks.",None,-1
Czech Grammar Error Correction with a Large and Diverse Corpus,0.838224,"We introduce a large and diverse Czech corpus annotated for grammatical error
correction (GEC) with the aim to contribute to the still scarce data resources
in this domain for languages other than English. The Grammar Error Correction
Corpus for Czech (GECCC) offers a variety of four domains, covering error
distributions ranging from high error density essays written by non-native
speakers, to website texts, where errors are expected to be much less common.
We compare several Czech GEC systems, including several Transformer-based ones,
setting a strong baseline to future research. Finally, we meta-evaluate common
GEC metrics against human judgements on our data. We make the new Czech GEC
corpus publicly available under the CC BY-SA 4.0 license at
http://hdl.handle.net/11234/1-4639 .",https://github.com/ufal/errant_czech,-1
"Towards Trustworthy AutoGrading of Short, Multi-lingual, Multi-type Answers",0.971035,"Autograding short textual answers has become much more feasible due to the
rise of NLP and the increased availability of question-answer pairs brought
about by a shift to online education. Autograding performance is still inferior
to human grading. The statistical and black-box nature of state-of-the-art
machine learning models makes them untrustworthy, raising ethical concerns and
limiting their practical utility. Furthermore, the evaluation of autograding is
typically confined to small, monolingual datasets for a specific question type.
This study uses a large dataset consisting of about 10 million question-answer
pairs from multiple languages covering diverse fields such as math and
language, and strong variation in question and answer syntax. We demonstrate
the effectiveness of fine-tuning transformer models for autograding for such
complex datasets. Our best hyperparameter-tuned model yields an accuracy of
about 86.5\%, comparable to the state-of-the-art models that are less general
and more tuned to a specific type of question, subject, and language. More
importantly, we address trust and ethical concerns. By involving humans in the
autograding process, we show how to improve the accuracy of automatically
graded answers, achieving accuracy equivalent to that of teaching assistants.
We also show how teachers can effectively control the type of errors made by
the system and how they can validate efficiently that the autograder's
performance on individual exams is close to the expected performance.",None,-1
"Homomorphisms Between Transfer, Multi-Task, and Meta-Learning Systems",0.0272221,"Transfer learning, multi-task learning, and meta-learning are well-studied
topics concerned with the generalization of knowledge across learning tasks and
are closely related to general intelligence. But, the formal, general systems
differences between them are underexplored in the literature. This lack of
systems-level formalism leads to difficulties in coordinating related,
inter-disciplinary engineering efforts. This manuscript formalizes transfer
learning, multi-task learning, and meta-learning as abstract learning systems,
consistent with the formal-minimalist abstract systems theory of Mesarovic and
Takahara. Moreover, it uses the presented formalism to relate the three
concepts of learning in terms of composition, hierarchy, and structural
homomorphism. Findings are readily depicted in terms of input-output systems,
highlighting the ease of delineating formal, general systems differences
between transfer, multi-task, and meta-learning.",None,-1
SCAMPS: Synthetics for Camera Measurement of Physiological Signals,0.818984,"The use of cameras and computational algorithms for noninvasive, low-cost and
scalable measurement of physiological (e.g., cardiac and pulmonary) vital signs
is very attractive. However, diverse data representing a range of environments,
body motions, illumination conditions and physiological states is laborious,
time consuming and expensive to obtain. Synthetic data have proven a valuable
tool in several areas of machine learning, yet are not widely available for
camera measurement of physiological states. Synthetic data offer ""perfect""
labels (e.g., without noise and with precise synchronization), labels that may
not be possible to obtain otherwise (e.g., precise pixel level segmentation
maps) and provide a high degree of control over variation and diversity in the
dataset. We present SCAMPS, a dataset of synthetics containing 2,800 videos
(1.68M frames) with aligned cardiac and respiratory signals and facial action
intensities. The RGB frames are provided alongside segmentation maps. We
provide precise descriptive statistics about the underlying waveforms,
including inter-beat interval, heart rate variability, and pulse arrival time.
Finally, we present baseline results training on these synthetic data and
testing on real-world datasets to illustrate generalizability.",https://github.com/danmcduff/scampsdataset,-1
Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities,0.999482,"Assembly101 is a new procedural activity dataset featuring 4321 videos of
people assembling and disassembling 101 ""take-apart"" toy vehicles. Participants
work without fixed instructions, and the sequences feature rich and natural
variations in action ordering, mistakes, and corrections. Assembly101 is the
first multi-view action dataset, with simultaneous static (8) and egocentric
(4) recordings. Sequences are annotated with more than 100K coarse and 1M
fine-grained action segments, and 18M 3D hand poses. We benchmark on three
action understanding tasks: recognition, anticipation and temporal
segmentation. Additionally, we propose a novel task of detecting mistakes. The
unique recording format and rich set of annotations allow us to investigate
generalization to new toys, cross-view transfer, long-tailed distributions, and
pose vs. appearance. We envision that Assembly101 will serve as a new challenge
to investigate various activity understanding problems.",https://assembly-101.github.io/,-1
BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision,0.901585,"We present a novel bird's-eye-view (BEV) detector with perspective
supervision, which converges faster and better suits modern image backbones.
Existing state-of-the-art BEV detectors are often tied to certain depth
pre-trained backbones like VoVNet, hindering the synergy between booming image
backbones and BEV detectors. To address this limitation, we prioritize easing
the optimization of BEV detectors by introducing perspective space supervision.
To this end, we propose a two-stage BEV detector, where proposals from the
perspective head are fed into the bird's-eye-view head for final predictions.
To evaluate the effectiveness of our model, we conduct extensive ablation
studies focusing on the form of supervision and the generality of the proposed
detector. The proposed method is verified with a wide spectrum of traditional
and modern image backbones and achieves new SoTA results on the large-scale
nuScenes dataset. The code shall be released soon.",None,-1
Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models,0.128827,"World models learn the consequences of actions in vision-based interactive
systems. However, in practical scenarios such as autonomous driving, there
commonly exists noncontrollable dynamics independent of the action signals,
making it difficult to learn effective world models. To tackle this problem, we
present a novel reinforcement learning approach named Iso-Dream, which improves
the Dream-to-Control framework in two aspects. First, by optimizing the inverse
dynamics, we encourage the world model to learn controllable and
noncontrollable sources of spatiotemporal changes on isolated state transition
branches. Second, we optimize the behavior of the agent on the decoupled latent
imaginations of the world model. Specifically, to estimate state values, we
roll-out the noncontrollable states into the future and associate them with the
current controllable state. In this way, the isolation of dynamics sources can
greatly benefit long-horizon decision-making of the agent, such as a
self-driving car that can avoid potential risks by anticipating the movement of
other vehicles. Experiments show that Iso-Dream is effective in decoupling the
mixed dynamics and remarkably outperforms existing approaches in a wide range
of visual control and prediction domains.",https://github.com/panmt/Iso-Dream,-1
MGFN: Magnitude-Contrastive Glance-and-Focus Network for Weakly-Supervised Video Anomaly Detection,0.13997,"Weakly supervised detection of anomalies in surveillance videos is a
challenging task. Going beyond existing works that have deficient capabilities
to localize anomalies in long videos, we propose a novel glance and focus
network to effectively integrate spatial-temporal information for accurate
anomaly detection. In addition, we empirically found that existing approaches
that use feature magnitudes to represent the degree of anomalies typically
ignore the effects of scene variations, and hence result in sub-optimal
performance due to the inconsistency of feature magnitudes across scenes. To
address this issue, we propose the Feature Amplification Mechanism and a
Magnitude Contrastive Loss to enhance the discriminativeness of feature
magnitudes for detecting anomalies. Experimental results on two large-scale
benchmarks UCF-Crime and XD-Violence manifest that our method outperforms
state-of-the-art approaches.",https://github.com/carolchenyx/MGFN.git,-1
RoPGen: Towards Robust Code Authorship Attribution via Automatic Coding Style Transformation,0.402128,"Source code authorship attribution is an important problem often encountered
in applications such as software forensics, bug fixing, and software quality
analysis. Recent studies show that current source code authorship attribution
methods can be compromised by attackers exploiting adversarial examples and
coding style manipulation. This calls for robust solutions to the problem of
code authorship attribution. In this paper, we initiate the study on making
Deep Learning (DL)-based code authorship attribution robust. We propose an
innovative framework called Robust coding style Patterns Generation (RoPGen),
which essentially learns authors' unique coding style patterns that are hard
for attackers to manipulate or imitate. The key idea is to combine data
augmentation and gradient augmentation at the adversarial training phase. This
effectively increases the diversity of training examples, generates meaningful
perturbations to gradients of deep neural networks, and learns diversified
representations of coding styles. We evaluate the effectiveness of RoPGen using
four datasets of programs written in C, C++, and Java. Experimental results
show that RoPGen can significantly improve the robustness of DL-based code
authorship attribution, by respectively reducing 22.8% and 41.0% of the success
rate of targeted and untargeted attacks on average.",https://github.com/RoPGen/RoPGen,-1
"mRI: Multi-modal 3D Human Pose Estimation Dataset using mmWave, RGB-D, and Inertial Sensors",0.18742,"The ability to estimate 3D human body pose and movement, also known as human
pose estimation (HPE), enables many applications for home-based health
monitoring, such as remote rehabilitation training. Several possible solutions
have emerged using sensors ranging from RGB cameras, depth sensors,
millimeter-Wave (mmWave) radars, and wearable inertial sensors. Despite
previous efforts on datasets and benchmarks for HPE, few dataset exploits
multiple modalities and focuses on home-based health monitoring. To bridge the
gap, we present mRI, a multi-modal 3D human pose estimation dataset with
mmWave, RGB-D, and Inertial Sensors. Our dataset consists of over 160k
synchronized frames from 20 subjects performing rehabilitation exercises and
supports the benchmarks of HPE and action detection. We perform extensive
experiments using our dataset and delineate the strength of each modality. We
hope that the release of mRI can catalyze the research in pose estimation,
multi-modal learning, and action understanding, and more importantly facilitate
the applications of home-based health monitoring.",https://github.com/OpenKinect/libfreenect2,-1
CC-3DT: Panoramic 3D Object Tracking via Cross-Camera Fusion,0.173098,"To track the 3D locations and trajectories of the other traffic participants
at any given time, modern autonomous vehicles are equipped with multiple
cameras that cover the vehicle's full surroundings. Yet, camera-based 3D object
tracking methods prioritize optimizing the single-camera setup and resort to
post-hoc fusion in a multi-camera setup. In this paper, we propose a method for
panoramic 3D object tracking, called CC-3DT, that associates and models object
trajectories both temporally and across views, and improves the overall
tracking consistency. In particular, our method fuses 3D detections from
multiple cameras before association, reducing identity switches significantly
and improving motion modeling. Our experiments on large-scale driving datasets
show that fusion before association leads to a large margin of improvement over
post-hoc fusion. We set a new state-of-the-art with 12.6% improvement in
average multi-object tracking accuracy (AMOTA) among all camera-based methods
on the competitive NuScenes 3D tracking benchmark, outperforming previously
published methods by 6.5% in AMOTA with the same 3D detector.",None,-1
Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation,0.60324,"Optimizing behaviors for dexterous manipulation has been a longstanding
challenge in robotics, with a variety of methods from model-based control to
model-free reinforcement learning having been previously explored in
literature. Perhaps one of the most powerful techniques to learn complex
manipulation strategies is imitation learning. However, collecting and learning
from demonstrations in dexterous manipulation is quite challenging. The
complex, high-dimensional action-space involved with multi-finger control often
leads to poor sample efficiency of learning-based methods. In this work, we
propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning
framework for dexterous manipulation. DIME only requires a single RGB camera to
observe a human operator and teleoperate our robotic hand. Once demonstrations
are collected, DIME employs standard imitation learning methods to train
dexterous manipulation policies. On both simulation and real robot benchmarks
we demonstrate that DIME can be used to solve complex, in-hand manipulation
tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro
hand. Our framework along with pre-collected demonstrations is publicly
available at https://nyu-robot-learning.github.io/dime.",https://nyu-robot-learning.github.io/dime,-1
NR-DFERNet: Noise-Robust Network for Dynamic Facial Expression Recognition,0.194197,"Dynamic facial expression recognition (DFER) in the wild is an extremely
challenging task, due to a large number of noisy frames in the video sequences.
Previous works focus on extracting more discriminative features, but ignore
distinguishing the key frames from the noisy frames. To tackle this problem, we
propose a noise-robust dynamic facial expression recognition network
(NR-DFERNet), which can effectively reduce the interference of noisy frames on
the DFER task. Specifically, at the spatial stage, we devise a dynamic-static
fusion module (DSF) that introduces dynamic features to static features for
learning more discriminative spatial features. To suppress the impact of target
irrelevant frames, we introduce a novel dynamic class token (DCT) for the
transformer at the temporal stage. Moreover, we design a snippet-based filter
(SF) at the decision stage to reduce the effect of too many neutral frames on
non-neutral sequence classification. Extensive experimental results demonstrate
that our NR-DFERNet outperforms the state-of-the-art methods on both the DFEW
and AFEW benchmarks.",None,-1
Local Directional Gradient Pattern: A Local Descriptor for Face Recognition,0.159184,"In this paper a local pattern descriptor in high order derivative space is
proposed for face recognition. The proposed local directional gradient pattern
(LDGP) is a 1D local micropattern computed by encoding the relationships
between the higher order derivatives of the reference pixel in four distinct
directions. The proposed descriptor identifies the relationship between the
high order derivatives of the referenced pixel in four different directions to
compute the micropattern which corresponds to the local feature. Proposed
descriptor considerably reduces the length of the micropattern which
consequently reduces the extraction time and matching time while maintaining
the recognition rate. Results of the extensive experiments conducted on
benchmark databases AT&T, Extended Yale B and CMU-PIE show that the proposed
descriptor significantly reduces the extraction as well as matching time while
the recognition rate is almost similar to the existing state of the art
methods.",None,-1
Optimizing Data Collection for Machine Learning,0.0909035,"Modern deep learning systems require huge data sets to achieve impressive
performance, but there is little guidance on how much or what kind of data to
collect. Over-collecting data incurs unnecessary present costs, while
under-collecting may incur future costs and delay workflows. We propose a new
paradigm for modeling the data collection workflow as a formal optimal data
collection problem that allows designers to specify performance targets,
collection costs, a time horizon, and penalties for failing to meet the
targets. Additionally, this formulation generalizes to tasks requiring multiple
data sources, such as labeled and unlabeled data used in semi-supervised
learning. To solve our problem, we develop Learn-Optimize-Collect (LOC), which
minimizes expected future collection costs. Finally, we numerically compare our
framework to the conventional baseline of estimating data requirements by
extrapolating from neural scaling laws. We significantly reduce the risks of
failing to meet desired performance targets on several classification,
segmentation, and detection tasks, while maintaining low total collection
costs.",None,-1
Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance,0.411262,"Human-translated text displays distinct features from naturally written text
in the same language. This phenomena, known as translationese, has been argued
to confound the machine translation (MT) evaluation. Yet, we find that existing
work on translationese neglects some important factors and the conclusions are
mostly correlational but not causal. In this work, we collect CausalMT, a
dataset where the MT training data are also labeled with the human translation
directions. We inspect two critical factors, the train-test direction match
(whether the human translation directions in the training and test sets are
aligned), and data-model direction match (whether the model learns in the same
direction as the human translation direction in the dataset). We show that
these two factors have a large causal effect on the MT performance, in addition
to the test-model direction mismatch highlighted by existing work on the impact
of translationese. In light of our findings, we provide a set of suggestions
for MT training and evaluation. Our code and data are at
https://github.com/EdisonNi-hku/CausalMT",https://github.com/EdisonNi-hku/CausalMT,-1
Deep Learning for Hate Speech Detection: A Comparative Study,0.513164,"Automated hate speech detection is an important tool in combating the spread
of hate speech, particularly in social media. Numerous methods have been
developed for the task, including a recent proliferation of deep-learning based
approaches. A variety of datasets have also been developed, exemplifying
various manifestations of the hate-speech detection problem. We present here a
large-scale empirical comparison of deep and shallow hate-speech detection
methods, mediated through the three most commonly used datasets. Our goal is to
illuminate progress in the area, and identify strengths and weaknesses in the
current state-of-the-art. We particularly focus our analysis on measures of
practical performance, including detection accuracy, computational efficiency,
capability in using pre-trained models, and domain generalization. In doing so
we aim to provide guidance as to the use of hate-speech detection in practice,
quantify the state-of-the-art, and identify future research directions. Code
and dataset are available at
https://github.com/jmjmalik22/Hate-Speech-Detection.",https://github.com/jmjmalik22/Hate-Speech-Detection,-1
Evaluating the Text-to-SQL Capabilities of Large Language Models,0.90452,"We perform an empirical evaluation of Text-to-SQL capabilities of the Codex
language model. We find that, without any finetuning, Codex is a strong
baseline on the Spider benchmark; we also analyze the failure modes of Codex in
this setting. Furthermore, we demonstrate on the GeoQuery and Scholar
benchmarks that a small number of in-domain examples provided in the prompt
enables Codex to perform better than state-of-the-art models finetuned on such
few-shot examples.",https://github.com/nitarshan/codex-text2sql,-1
GRiT: A Generative Region-to-text Transformer for Object Understanding,0.781912,"This paper presents a Generative RegIon-to-Text transformer, GRiT, for object
understanding. The spirit of GRiT is to formulate object understanding as
<region, text> pairs, where region locates objects and text describes objects.
For example, the text in object detection denotes class names while that in
dense captioning refers to descriptive sentences. Specifically, GRiT consists
of a visual encoder to extract image features, a foreground object extractor to
localize objects, and a text decoder to generate open-set object descriptions.
With the same model architecture, GRiT can understand objects via not only
simple nouns, but also rich descriptive sentences including object attributes
or actions. Experimentally, we apply GRiT to object detection and dense
captioning tasks. GRiT achieves 60.4 AP on COCO 2017 test-dev for object
detection and 15.5 mAP on Visual Genome for dense captioning. Code is available
at https://github.com/JialianW/GRiT",https://github.com/JialianW/GRiT,-1
When Is Partially Observable Reinforcement Learning Not Scary?,0.824867,"Applications of Reinforcement Learning (RL), in which agents learn to make a
sequence of decisions despite lacking complete information about the latent
states of the controlled system, that is, they act under partial observability
of the states, are ubiquitous. Partially observable RL can be notoriously
difficult -- well-known information-theoretic results show that learning
partially observable Markov decision processes (POMDPs) requires an exponential
number of samples in the worst case. Yet, this does not rule out the existence
of large subclasses of POMDPs over which learning is tractable.
  In this paper we identify such a subclass, which we call weakly revealing
POMDPs. This family rules out the pathological instances of POMDPs where
observations are uninformative to a degree that makes learning hard. We prove
that for weakly revealing POMDPs, a simple algorithm combining optimism and
Maximum Likelihood Estimation (MLE) is sufficient to guarantee polynomial
sample complexity. To the best of our knowledge, this is the first provably
sample-efficient result for learning from interactions in overcomplete POMDPs,
where the number of latent states can be larger than the number of
observations.",None,-1
TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models,0.478848,"Language Models (LMs) become outdated as the world changes; they often fail
to perform tasks requiring recent factual information which was absent or
different during training, a phenomenon called temporal misalignment. This is
especially a challenging problem because the research community still lacks a
coherent dataset for assessing the adaptability of LMs to frequently-updated
knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a
lifelong benchmark for ever-evolving LMs that utilizes the difference between
consecutive snapshots of English Wikipedia and English Wikidata for training
and evaluation, respectively. The benchmark hence allows researchers to
periodically track an LM's ability to retain previous knowledge and acquire
updated/new knowledge at each point in time. We also find that training an LM
on the diff data through continual learning methods achieves similar or better
perplexity than on the entire snapshot in our benchmark with 12 times less
computational cost, which verifies that factual knowledge in LMs can be safely
updated with minimal training data via continual learning. The dataset and the
code are available at https://github.com/joeljang/temporalwiki.",None,-1
B-SMART: A Reference Architecture for Artificially Intelligent Autonomic Smart Buildings,0.0751709,"The pervasive application of artificial intelligence and machine learning
algorithms is transforming many industries and aspects of the human experience.
One very important industry trend is the move to convert existing human
dwellings to smart buildings, and to create new smart buildings. Smart
buildings aim to mitigate climate change by reducing energy consumption and
associated carbon emissions. To accomplish this, they leverage artificial
intelligence, big data, and machine learning algorithms to learn and optimize
system performance. These fields of research are currently very rapidly
evolving and advancing, but there has been very little guidance to help
engineers and architects working on smart buildings apply artificial
intelligence algorithms and technologies in a systematic and effective manner.
In this paper we present B-SMART: the first reference architecture for
autonomic smart buildings. B-SMART facilitates the application of artificial
intelligence techniques and technologies to smart buildings by decoupling
conceptually distinct layers of functionality and organizing them into an
autonomic control loop. We also present a case study illustrating how B-SMART
can be applied to accelerate the introduction of artificial intelligence into
an existing smart building.",None,-1
Handwritten Arabic Character Recognition for Children Writ-ing Using Convolutional Neural Network and Stroke Identification,0.158686,"Automatic Arabic handwritten recognition is one of the recently studied
problems in the field of Machine Learning. Unlike Latin languages, Arabic is a
Semitic language that forms a harder challenge, especially with variability of
patterns caused by factors such as writer age. Most of the studies focused on
adults, with only one recent study on children. Moreover, much of the recent
Machine Learning methods focused on using Convolutional Neural Networks, a
powerful class of neural networks that can extract complex features from
images. In this paper we propose a convolutional neural network (CNN) model
that recognizes children handwriting with an accuracy of 91% on the Hijja
dataset, a recent dataset built by collecting images of the Arabic characters
written by children, and 97% on Arabic Handwritten Character Dataset. The
results showed a good improvement over the proposed model from the Hijja
dataset authors, yet it reveals a bigger challenge to solve for children Arabic
handwritten character recognition. Moreover, we proposed a new approach using
multi models instead of single model based on the number of strokes in a
character, and merged Hijja with AHCD which reached an averaged prediction
accuracy of 96%.",None,-1
VIDM: Video Implicit Diffusion Models,0.430515,"Diffusion models have emerged as a powerful generative method for
synthesizing high-quality and diverse set of images. In this paper, we propose
a video generation method based on diffusion models, where the effects of
motion are modeled in an implicit condition manner, i.e. one can sample
plausible video motions according to the latent feature of frames. We improve
the quality of the generated videos by proposing multiple strategies such as
sampling space truncation, robustness penalty, and positional group
normalization. Various experiments are conducted on datasets consisting of
videos with different resolutions and different number of frames. Results show
that the proposed method outperforms the state-of-the-art generative
adversarial network-based methods by a significant margin in terms of FVD
scores as well as perceptible visual quality.",None,-1
"Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report",0.998624,"In September 2021, the ""One Hundred Year Study on Artificial Intelligence""
project (AI100) issued the second report of its planned long-term periodic
assessment of artificial intelligence (AI) and its impact on society. It was
written by a panel of 17 study authors, each of whom is deeply rooted in AI
research, chaired by Michael Littman of Brown University. The report, entitled
""Gathering Strength, Gathering Storms,"" answers a set of 14 questions probing
critical areas of AI development addressing the major risks and dangers of AI,
its effects on society, its public perception and the future of the field. The
report concludes that AI has made a major leap from the lab to people's lives
in recent years, which increases the urgency to understand its potential
negative effects. The questions were developed by the AI100 Standing Committee,
chaired by Peter Stone of the University of Texas at Austin, consisting of a
group of AI leaders with expertise in computer science, sociology, ethics,
economics, and other disciplines.",None,-1
Language Models of Code are Few-Shot Commonsense Learners,0.268118,"We address the general task of structured commonsense reasoning: given a
natural language input, the goal is to generate a graph such as an event -- or
a reasoning-graph. To employ large language models (LMs) for this task,
existing approaches ``serialize'' the output graph as a flat list of nodes and
edges. Although feasible, these serialized graphs strongly deviate from the
natural language corpora that LMs were pre-trained on, hindering LMs from
generating them correctly. In this paper, we show that when we instead frame
structured commonsense reasoning tasks as code generation tasks, pre-trained
LMs of code are better structured commonsense reasoners than LMs of natural
language, even when the downstream task does not involve source code at all. We
demonstrate our approach across three diverse structured commonsense reasoning
tasks. In all these natural language tasks, we show that using our approach, a
code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the
target task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot
setting.",https://github.com/madaan/CoCoGen,-1
Data-driven Feature Tracking for Event Cameras,0.0436769,"Because of their high temporal resolution, increased resilience to motion
blur, and very sparse output, event cameras have been shown to be ideal for
low-latency and low-bandwidth feature tracking, even in challenging scenarios.
Existing feature tracking methods for event cameras are either handcrafted or
derived from first principles but require extensive parameter tuning, are
sensitive to noise, and do not generalize to different scenarios due to
unmodeled effects. To tackle these deficiencies, we introduce the first
data-driven feature tracker for event cameras, which leverages low-latency
events to track features detected in a grayscale frame. We achieve robust
performance via a novel frame attention module, which shares information across
feature tracks. By directly transferring zero-shot from synthetic to real data,
our data-driven tracker outperforms existing approaches in relative feature age
by up to 120% while also achieving the lowest latency. This performance gap is
further increased to 130% by adapting our tracker to real data with a novel
self-supervision strategy.",https://github.com/uzh-rpg/deep_ev_tracker,-1
FedALA: Adaptive Local Aggregation for Personalized Federated Learning,0.351962,"A key challenge in federated learning (FL) is the statistical heterogeneity
that impairs the generalization of the global model on each client. To address
this, we propose a method Federated learning with Adaptive Local Aggregation
(FedALA) by capturing the desired information in the global model for client
models in personalized FL. The key component of FedALA is an Adaptive Local
Aggregation (ALA) module, which can adaptively aggregate the downloaded global
model and local model towards the local objective on each client to initialize
the local model before training in each iteration. To evaluate the
effectiveness of FedALA, we conduct extensive experiments with five benchmark
datasets in computer vision and natural language processing domains. FedALA
outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy.
Furthermore, we also apply ALA module to other federated learning methods and
achieve up to 24.19% improvement in test accuracy.",https://github.com/TsingZ0/FedALA,-1
Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data,0.755447,"Retrieval-based methods have been shown to be effective in NLP tasks via
introducing external knowledge. However, the indexing and retrieving of
large-scale corpora bring considerable computational cost. Surprisingly, we
found that REtrieving from the traINing datA (REINA) only can lead to
significant gains on multiple NLG and NLU tasks. We retrieve the labeled
training instances most similar to the input text and then concatenate them
with the input to feed into the model to generate the output. Experimental
results show that this simple method can achieve significantly better
performance on a variety of NLU and NLG tasks, including summarization, machine
translation, language modeling, and question answering tasks. For instance, our
proposed method achieved state-of-the-art results on XSum, BigPatent, and
CommonsenseQA. Our code is released, https://github.com/microsoft/REINA .",https://github.com/microsoft/REINA,-1
The Causal News Corpus: Annotating Causal Relations in Event Sentences from News,0.343465,"Despite the importance of understanding causality, corpora addressing causal
relations are limited. There is a discrepancy between existing annotation
guidelines of event causality and conventional causality corpora that focus
more on linguistics. Many guidelines restrict themselves to include only
explicit relations or clause-based arguments. Therefore, we propose an
annotation schema for event causality that addresses these concerns. We
annotated 3,559 event sentences from protest event news with labels on whether
it contains causal relations or not. Our corpus is known as the Causal News
Corpus (CNC). A neural network built upon a state-of-the-art pre-trained
language model performed well with 81.20% F1 score on test set, and 83.46% in
5-folds cross-validation. CNC is transferable across two external corpora:
CausalTimeBank (CTB) and Penn Discourse Treebank (PDTB). Leveraging each of
these external datasets for training, we achieved up to approximately 64% F1 on
the CNC test set without additional fine-tuning. CNC also served as an
effective training and pre-training dataset for the two external corpora.
Lastly, we demonstrate the difficulty of our task to the layman in a
crowd-sourced annotation exercise. Our annotated corpus is publicly available,
providing a valuable resource for causal text mining researchers.",https://github.com/tanfiona/,-1
Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP),0.156666,"Contrastively trained language-image models such as CLIP, ALIGN, and BASIC
have demonstrated unprecedented robustness to multiple challenging natural
distribution shifts. Since these language-image models differ from previous
training approaches in several ways, an important question is what causes the
large robustness gains. We answer this question via a systematic experimental
investigation. Concretely, we study five different possible causes for the
robustness gains: (i) the training set size, (ii) the training distribution,
(iii) language supervision at training time, (iv) language supervision at test
time, and (v) the contrastive loss function. Our experiments show that the more
diverse training distribution is the main cause for the robustness gains, with
the other factors contributing little to no robustness. Beyond our experimental
results, we also introduce ImageNet-Captions, a version of ImageNet with
original text annotations from Flickr, to enable further controlled experiments
of language-image training.",None,-1
InstaFormer: Instance-Aware Image-to-Image Translation with Transformer,0.445781,"We present a novel Transformer-based network architecture for instance-aware
image-to-image translation, dubbed InstaFormer, to effectively integrate
global- and instance-level information. By considering extracted content
features from an image as tokens, our networks discover global consensus of
content features by considering context information through a self-attention
module in Transformers. By augmenting such tokens with an instance-level
feature extracted from the content feature with respect to bounding box
information, our framework is capable of learning an interaction between object
instances and the global image, thus boosting the instance-awareness. We
replace layer normalization (LayerNorm) in standard Transformers with adaptive
instance normalization (AdaIN) to enable a multi-modal translation with style
codes. In addition, to improve the instance-awareness and translation quality
at object regions, we present an instance-level content contrastive loss
defined between input and translated image. We conduct experiments to
demonstrate the effectiveness of our InstaFormer over the latest methods and
provide extensive ablation studies.",None,-1
Vision Transformers for Single Image Dehazing,0.998446,"Image dehazing is a representative low-level vision task that estimates
latent haze-free images from hazy images. In recent years, convolutional neural
network-based methods have dominated image dehazing. However, vision
Transformers, which has recently made a breakthrough in high-level vision
tasks, has not brought new dimensions to image dehazing. We start with the
popular Swin Transformer and find that several of its key designs are
unsuitable for image dehazing. To this end, we propose DehazeFormer, which
consists of various improvements, such as the modified normalization layer,
activation function, and spatial information aggregation scheme. We train
multiple variants of DehazeFormer on various datasets to demonstrate its
effectiveness. Specifically, on the most frequently used SOTS indoor set, our
small model outperforms FFA-Net with only 25% #Param and 5% computational cost.
To the best of our knowledge, our large model is the first method with the PSNR
over 40 dB on the SOTS indoor set, dramatically outperforming the previous
state-of-the-art methods. We also collect a large-scale realistic remote
sensing dehazing dataset for evaluating the method's capability to remove
highly non-homogeneous haze.",https://github.com/IDKiro/DehazeFormer,-1
Learnable Graph Convolutional Network and Feature Fusion for Multi-view Learning,0.125959,"In practical applications, multi-view data depicting objectives from assorted
perspectives can facilitate the accuracy increase of learning algorithms.
However, given multi-view data, there is limited work for learning
discriminative node relationships and graph information simultaneously via
graph convolutional network that has drawn the attention from considerable
researchers in recent years. Most of existing methods only consider the
weighted sum of adjacency matrices, yet a joint neural network of both feature
and graph fusion is still under-explored. To cope with these issues, this paper
proposes a joint deep learning framework called Learnable Graph Convolutional
Network and Feature Fusion (LGCN-FF), consisting of two stages: feature fusion
network and learnable graph convolutional network. The former aims to learn an
underlying feature representation from heterogeneous views, while the latter
explores a more discriminative graph fusion via learnable weights and a
parametric activation function dubbed Differentiable Shrinkage Activation (DSA)
function. The proposed LGCN-FF is validated to be superior to various
state-of-the-art methods in multi-view semi-supervised classification.",None,-1
LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning,0.981343,"Fine-tuning large pre-trained models on downstream tasks has been adopted in
a variety of domains recently. However, it is costly to update the entire
parameter set of large pre-trained models. Although recently proposed
parameter-efficient transfer learning (PETL) techniques allow updating a small
subset of parameters (e.g. only using 2% of parameters) inside a pre-trained
backbone network for a new task, they only reduce the training memory
requirement by up to 30%. This is because the gradient computation for the
trainable parameters still requires backpropagation through the large
pre-trained backbone model. To address this, we propose Ladder Side-Tuning
(LST), a new PETL technique that can reduce training memory requirements by
more substantial amounts. Unlike existing parameter-efficient methods that
insert additional parameters inside backbone networks, we train a ladder side
network, a small and separate network that takes intermediate activations as
input via shortcut connections (called ladders) from backbone networks and
makes predictions. LST has significantly lower memory requirements than
previous methods, because it does not require backpropagation through the
backbone network, but instead only through the side network and ladder
connections. We evaluate our method with various models (T5 and CLIP-T5) on
both NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST
saves 69% of the memory costs to fine-tune the whole network, while other
methods only save 26% of that in similar parameter usages (hence, 2.7x more
memory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA
in a low-memory regime. To further show the advantage of this better memory
efficiency, we also apply LST to larger T5 models, attaining better GLUE
performance than full fine-tuning and other PETL methods. The
accuracy-efficiency trade-off also holds on VL tasks.",None,-1
OneRel:Joint Entity and Relation Extraction with One Module in One Step,0.94397,"Joint entity and relation extraction is an essential task in natural language
processing and knowledge graph construction. Existing approaches usually
decompose the joint extraction task into several basic modules or processing
steps to make it easy to conduct. However, such a paradigm ignores the fact
that the three elements of a triple are interdependent and indivisible.
Therefore, previous joint methods suffer from the problems of cascading errors
and redundant information. To address these issues, in this paper, we propose a
novel joint entity and relation extraction model, named OneRel, which casts
joint extraction as a fine-grained triple classification problem. Specifically,
our model consists of a scoring-based classifier and a relation-specific horns
tagging strategy. The former evaluates whether a token pair and a relation
belong to a factual triple. The latter ensures a simple but effective decoding
process. Extensive experimental results on two widely used datasets demonstrate
that the proposed method performs better than the state-of-the-art baselines,
and delivers consistent performance gain on complex scenarios of various
overlapping patterns and multiple triples.",None,-1
Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity,0.229589,"Large Language Models (LLMs) have demonstrated impressive capabilities in
generating fluent text, as well as tendencies to reproduce undesirable social
biases. This study investigates whether LLMs reproduce the moral biases
associated with political groups in the United States, an instance of a broader
capability herein termed moral mimicry. This hypothesis is explored in the
GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral
Foundations Theory, it is shown that these LLMs are indeed moral mimics. When
prompted with a liberal or conservative political identity, the models generate
text reflecting corresponding moral biases. This study also explores the
relationship between moral mimicry and model size, and similarity between human
and LLM moral word use.",https://github.com/mbforbes/social-chemistry-101,-1
"An Exploratory Study of Tweets about the SARS-CoV-2 Omicron Variant: Insights from Sentiment Analysis, Language Interpretation, Source Tracking, Type Classification, and Embedded URL Detection",0.945253,"This paper presents the findings of an exploratory study on the continuously
generating Big Data on Twitter related to the sharing of information, news,
views, opinions, ideas, feedback, and experiences about the COVID-19 pandemic,
with a specific focus on the Omicron variant, which is the globally dominant
variant of SARS-CoV-2 at this time. A total of 12028 tweets about the Omicron
variant were studied, and the specific characteristics of tweets that were
analyzed include - sentiment, language, source, type, and embedded URLs. The
findings of this study are manifold. First, from sentiment analysis, it was
observed that 50.5% of tweets had a neutral emotion. The other emotions - bad,
good, terrible, and great were found in 15.6%, 14.0%, 12.5%, and 7.5% of the
tweets, respectively. Second, the findings of language interpretation showed
that 65.9% of the tweets were posted in English. It was followed by Spanish,
French, Italian, and other languages. Third, the findings from source tracking
showed that Twitter for Android was associated with 35.2% of tweets. It was
followed by Twitter Web App, Twitter for iPhone, Twitter for iPad, and other
sources. Fourth, studying the type of tweets revealed that retweets accounted
for 60.8% of the tweets, it was followed by original tweets and replies that
accounted for 19.8% and 19.4% of the tweets, respectively. Fifth, in terms of
embedded URL analysis, the most common domain embedded in the tweets was found
to be twitter.com, which was followed by biorxiv.org, nature.com, and other
domains. Finally, to support similar research in this field, we have developed
a Twitter dataset that comprises more than 500,000 tweets about the SARS-CoV-2
omicron variant since the first detected case of this variant on November 24,
2021.",None,-1
Understanding DDPM Latent Codes Through Optimal Transport,0.253067,"Diffusion models have recently outperformed alternative approaches to model
the distribution of natural images, such as GANs. Such diffusion models allow
for deterministic sampling via the probability flow ODE, giving rise to a
latent space and an encoder map. While having important practical applications,
such as estimation of the likelihood, the theoretical properties of this map
are not yet fully understood. In the present work, we partially address this
question for the popular case of the VP SDE (DDPM) approach. We show that,
perhaps surprisingly, the DDPM encoder map coincides with the optimal transport
map for common distributions; we support this claim theoretically and by
extensive numerical experiments.",https://github.com/openai/guided-diffusion,-1
Focal Sparse Convolutional Networks for 3D Object Detection,0.485785,"Non-uniformed 3D sparse data, e.g., point clouds or voxels in different
spatial positions, make contribution to the task of 3D object detection in
different ways. Existing basic components in sparse convolutional networks
(Sparse CNNs) process all sparse data, regardless of regular or submanifold
sparse convolution. In this paper, we introduce two new modules to enhance the
capability of Sparse CNNs, both are based on making feature sparsity learnable
with position-wise importance prediction. They are focal sparse convolution
(Focals Conv) and its multi-modal variant of focal sparse convolution with
fusion, or Focals Conv-F for short. The new modules can readily substitute
their plain counterparts in existing Sparse CNNs and be jointly trained in an
end-to-end fashion. For the first time, we show that spatially learnable
sparsity in sparse convolution is essential for sophisticated 3D object
detection. Extensive experiments on the KITTI, nuScenes and Waymo benchmarks
validate the effectiveness of our approach. Without bells and whistles, our
results outperform all existing single-model entries on the nuScenes test
benchmark at the paper submission time. Code and models are at
https://github.com/dvlab-research/FocalsConv.",https://github.com/dvlab-research/FocalsConv,-1
Comparison of biomedical relationship extraction methods and models for knowledge graph creation,0.361659,"Biomedical research is growing at such an exponential pace that scientists,
researchers, and practitioners are no more able to cope with the amount of
published literature in the domain. The knowledge presented in the literature
needs to be systematized in such a way that claims and hypotheses can be easily
found, accessed, and validated. Knowledge graphs can provide such a framework
for semantic knowledge representation from literature. However, in order to
build a knowledge graph, it is necessary to extract knowledge as relationships
between biomedical entities and normalize both entities and relationship types.
In this paper, we present and compare few rule-based and machine learning-based
(Naive Bayes, Random Forests as examples of traditional machine learning
methods and DistilBERT, PubMedBERT, T5 and SciFive-based models as examples of
modern deep learning transformers) methods for scalable relationship extraction
from biomedical literature, and for the integration into the knowledge graphs.
We examine how resilient are these various methods to unbalanced and fairly
small datasets. Our experiments show that transformer-based models handle well
both small (due to pre-training on a large dataset) and unbalanced datasets.
The best performing model was the PubMedBERT-based model fine-tuned on balanced
data, with a reported F1-score of 0.92. DistilBERT-based model followed with
F1-score of 0.89, performing faster and with lower resource requirements.
BERT-based models performed better then T5-based generative models.",None,-1
FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing,0.952565,"We present a benchmark suite of four datasets for evaluating the fairness of
pre-trained language models and the techniques used to fine-tune them for
downstream tasks. Our benchmarks cover four jurisdictions (European Council,
USA, Switzerland, and China), five languages (English, German, French, Italian
and Chinese) and fairness across five attributes (gender, age, region,
language, and legal area). In our experiments, we evaluate pre-trained language
models using several group-robust fine-tuning techniques and show that
performance group disparities are vibrant in many cases, while none of these
techniques guarantee fairness, nor consistently mitigate group disparities.
Furthermore, we provide a quantitative and qualitative analysis of our results,
highlighting open challenges in the development of robustness methods in legal
NLP.",https://github.com/coastalcph/fairlex,-1
On the Statistical Efficiency of Reward-Free Exploration in Non-Linear RL,0.175498,"We study reward-free reinforcement learning (RL) under general non-linear
function approximation, and establish sample efficiency and hardness results
under various standard structural assumptions. On the positive side, we propose
the RFOLIVE (Reward-Free OLIVE) algorithm for sample-efficient reward-free
exploration under minimal structural assumptions, which covers the previously
studied settings of linear MDPs (Jin et al., 2020b), linear completeness
(Zanette et al., 2020b) and low-rank MDPs with unknown representation (Modi et
al., 2021). Our analyses indicate that the explorability or reachability
assumptions, previously made for the latter two settings, are not necessary
statistically for reward-free exploration. On the negative side, we provide a
statistical hardness result for both reward-free and reward-aware exploration
under linear completeness assumptions when the underlying features are unknown,
showing an exponential separation between low-rank and linear completeness
settings.",None,-1
RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction,0.0355049,"In document-level event extraction (DEE) task, event arguments always scatter
across sentences (across-sentence issue) and multiple events may lie in one
document (multi-event issue). In this paper, we argue that the relation
information of event arguments is of great significance for addressing the
above two issues, and propose a new DEE framework which can model the relation
dependencies, called Relation-augmented Document-level Event Extraction
(ReDEE). More specifically, this framework features a novel and tailored
transformer, named as Relation-augmented Attention Transformer (RAAT). RAAT is
scalable to capture multi-scale and multi-amount argument relations. To further
leverage relation information, we introduce a separate event relation
prediction task and adopt multi-task learning method to explicitly enhance
event extraction performance. Extensive experiments demonstrate the
effectiveness of the proposed method, which can achieve state-of-the-art
performance on two public datasets. Our code is available at https://github.
com/TencentYoutuResearch/RAAT.",https://github.com/TencentYoutuResearch/RAAT,-1
