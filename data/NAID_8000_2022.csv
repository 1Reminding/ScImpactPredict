title,TNCSI,abstract,OA,authors_title
Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control,0.125605,"Pretrained language models have demonstrated extraordinary capabilities in
language generation. However, real-world tasks often require controlling the
distribution of generated text in order to mitigate bias, promote fairness, and
achieve personalization. Existing techniques for controlling the distribution
of generated text only work with quantified distributions, which require
pre-defined categories, proportions of the distribution, or an existing corpus
following the desired distributions. However, many important distributions,
such as personal preferences, are unquantified. In this work, we tackle the
problem of generating text following arbitrary distributions (quantified and
unquantified) by proposing Nano, a few-shot human-in-the-loop training
algorithm that continuously learns from human feedback. Nano achieves
state-of-the-art results on single topic/attribute as well as quantified
distribution control compared to previous works. We also show that Nano is able
to learn unquantified distributions, achieves personalization, and captures
differences between different individuals' personal preferences with high
sample efficiency.",https://github.com/sfanxiang/Nano,-1
Image-based Automatic Dial Meter Reading in Unconstrained Scenarios,0.312052,"The replacement of analog meters with smart meters is costly, laborious, and
far from complete in developing countries. The Energy Company of Parana (Copel)
(Brazil) performs more than 4 million meter readings (almost entirely of
non-smart devices) per month, and we estimate that 850 thousand of them are
from dial meters. Therefore, an image-based automatic reading system can reduce
human errors, create a proof of reading, and enable the customers to perform
the reading themselves through a mobile application. We propose novel
approaches for Automatic Dial Meter Reading (ADMR) and introduce a new dataset
for ADMR in unconstrained scenarios, called UFPR-ADMR-v2. Our best-performing
method combines YOLOv4 with a novel regression approach (AngReg), and explores
several postprocessing techniques. Compared to previous works, it decreased the
Mean Absolute Error (MAE) from 1,343 to 129 and achieved a meter recognition
rate (MRR) of 98.90% -- with an error tolerance of 1 Kilowatt-hour (kWh).",None,6178
Phantom -- A RL-driven multi-agent framework to model complex systems,0.300345,"Agent based modelling (ABM) is a computational approach to modelling complex
systems by specifying the behaviour of autonomous decision-making components or
agents in the system and allowing the system dynamics to emerge from their
interactions. Recent advances in the field of Multi-agent reinforcement
learning (MARL) have made it feasible to study the equilibrium of complex
environments where multiple agents learn simultaneously. However, most ABM
frameworks are not RL-native, in that they do not offer concepts and interfaces
that are compatible with the use of MARL to learn agent behaviours. In this
paper, we introduce a new open-source framework, Phantom, to bridge the gap
between ABM and MARL. Phantom is an RL-driven framework for agent-based
modelling of complex multi-agent systems including, but not limited to economic
systems and markets. The framework aims to provide the tools to simplify the
ABM specification in a MARL-compatible way - including features to encode
dynamic partial observability, agent utility functions, heterogeneity in agent
preferences or types, and constraints on the order in which agents can act
(e.g. Stackelberg games, or more complex turn-taking environments). In this
paper, we present these features, their design rationale and present two new
environments leveraging the framework.",None,-1
Towards Self-Supervised Category-Level Object Pose and Size Estimation,0.532729,"In this work, we tackle the challenging problem of category-level object pose
and size estimation from a single depth image. Although previous
fully-supervised works have demonstrated promising performance, collecting
ground-truth pose labels is generally time-consuming and labor-intensive.
Instead, we propose a label-free method that learns to enforce the geometric
consistency between category template mesh and observed object point cloud
under a self-supervision manner. Specifically, our method consists of three key
components: differentiable shape deformation, registration, and rendering. In
particular, shape deformation and registration are applied to the template mesh
to eliminate the differences in shape, pose and scale. A differentiable
renderer is then deployed to enforce geometric consistency between point clouds
lifted from the rendered depth and the observed scene for self-supervision. We
evaluate our approach on real-world datasets and find that our approach
outperforms the simple traditional baseline by large margins while being
competitive with some fully-supervised approaches.",https://github.com/mentian/object-deformnet,-1
SpaceE: Knowledge Graph Embedding by Relational Linear Transformation in the Entity Space,0.18386,"Translation distance based knowledge graph embedding (KGE) methods, such as
TransE and RotatE, model the relation in knowledge graphs as translation or
rotation in the vector space. Both translation and rotation are injective; that
is, the translation or rotation of different vectors results in different
results. In knowledge graphs, different entities may have a relation with the
same entity; for example, many actors starred in one movie. Such a
non-injective relation pattern cannot be well modeled by the translation or
rotation operations in existing translation distance based KGE methods. To
tackle the challenge, we propose a translation distance-based KGE method called
SpaceE to model relations as linear transformations. The proposed SpaceE embeds
both entities and relations in knowledge graphs as matrices and SpaceE
naturally models non-injective relations with singular linear transformations.
We theoretically demonstrate that SpaceE is a fully expressive model with the
ability to infer multiple desired relation patterns, including symmetry,
skew-symmetry, inversion, Abelian composition, and non-Abelian composition.
Experimental results on link prediction datasets illustrate that SpaceE
substantially outperforms many previous translation distance based knowledge
graph embedding methods, especially on datasets with many non-injective
relations. The code is available based on the PaddlePaddle deep learning
platform https://www.paddlepaddle.org.cn.",https://www.paddlepaddle.org.cn/,-1
DeiT III: Revenge of the ViT,0.981459,"A Vision Transformer (ViT) is a simple neural architecture amenable to serve
several computer vision tasks. It has limited built-in architectural priors, in
contrast to more recent architectures that incorporate priors either about the
input data or of specific tasks. Recent works show that ViTs benefit from
self-supervised pre-training, in particular BerT-like pre-training like BeiT.
In this paper, we revisit the supervised training of ViTs. Our procedure builds
upon and simplifies a recipe introduced for training ResNet-50. It includes a
new simple data-augmentation procedure with only 3 augmentations, closer to the
practice in self-supervised learning. Our evaluations on Image classification
(ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning
and semantic segmentation show that our procedure outperforms by a large margin
previous fully supervised training recipes for ViT. It also reveals that the
performance of our ViT trained with supervision is comparable to that of more
recent architectures. Our results could serve as better baselines for recent
self-supervised approaches demonstrated on ViT.",None,-1
Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,0.317675,"Recent research has revealed that neural language models at scale suffer from
poor temporal generalization capability, i.e., the language model pre-trained
on static data from past years performs worse over time on emerging data.
Existing methods mainly perform continual training to mitigate such a
misalignment. While effective to some extent but is far from being addressed on
both the language modeling and downstream tasks. In this paper, we empirically
observe that temporal generalization is closely affiliated with lexical
semantic change, which is one of the essential phenomena of natural languages.
Based on this observation, we propose a simple yet effective lexical-level
masking strategy to post-train a converged language model. Experiments on two
pre-trained language models, two different classification tasks, and four
benchmark datasets demonstrate the effectiveness of our proposed method over
existing temporal adaptation methods, i.e., continual training with new data.
Our code is available at \url{https://github.com/zhaochen0110/LMLM}.",https://github.com/zhaochen0110/LMLM,-1
FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric,0.183343,"Syntax is a fundamental component of language, yet few metrics have been
employed to capture syntactic similarity or coherence at the utterance- and
document-level. The existing standard document-level syntactic similarity
metric is computationally expensive and performs inconsistently when faced with
syntactically dissimilar documents. To address these challenges, we present
FastKASSIM, a metric for utterance- and document-level syntactic similarity
which pairs and averages the most similar constituency parse trees between a
pair of documents based on tree kernels. FastKASSIM is more robust to syntactic
dissimilarities and runs up to to 5.32 times faster than its predecessor over
documents in the r/ChangeMyView corpus. FastKASSIM's improvements allow us to
examine hypotheses in two settings with large documents. We find that
syntactically similar arguments on r/ChangeMyView tend to be more persuasive,
and that syntax is predictive of authorship attribution in the Australian High
Court Judgment corpus.",https://github.com/jasonyux/FastKASSIM,-1
Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion,0.375688,"Deep 3D point cloud models are sensitive to adversarial attacks, which poses
threats to safety-critical applications such as autonomous driving. Robust
training and defend-by-denoising are typical strategies for defending
adversarial perturbations. However, they either induce massive computational
overhead or rely heavily upon specified priors, limiting generalized robustness
against attacks of all kinds. To remedy it, this paper introduces a novel
distortion-aware defense framework that can rebuild the pristine data
distribution with a tailored intensity estimator and a diffusion model. To
perform distortion-aware forward diffusion, we design a distortion estimation
algorithm that is obtained by summing the distance of each point to the
best-fitting plane of its local neighboring points, which is based on the
observation of the local spatial properties of the adversarial point cloud. By
iterative diffusion and reverse denoising, the perturbed point cloud under
various distortions can be restored back to a clean distribution. This approach
enables effective defense against adaptive attacks with varying noise budgets,
enhancing the robustness of existing 3D deep recognition models.",None,-1
Self-Aware Personalized Federated Learning,0.348753,"In the context of personalized federated learning (FL), the critical
challenge is to balance local model improvement and global model tuning when
the personal and global objectives may not be exactly aligned. Inspired by
Bayesian hierarchical models, we develop a self-aware personalized FL method
where each client can automatically balance the training of its local personal
model and the global model that implicitly contributes to other clients'
training. Such a balance is derived from the inter-client and intra-client
uncertainty quantification. A larger inter-client variation implies more
personalization is needed. Correspondingly, our method uses uncertainty-driven
local training steps and aggregation rule instead of conventional local
fine-tuning and sample size-based aggregation. With experimental studies on
synthetic data, Amazon Alexa audio data, and public datasets such as MNIST,
FEMNIST, CIFAR10, and Sent140, we show that our proposed method can achieve
significantly improved personalization performance compared with the existing
counterparts.",https://github.com/CharlieDinh/,-1
Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems,0.401611,"Structure from motion using uncalibrated multi-camera systems is a
challenging task. This paper proposes a bundle adjustment solution that
implements a baseline constraint respecting that these cameras are static to
each other. We assume these cameras are mounted on a mobile platform,
uncalibrated, and coarsely synchronized. To this end, we propose the baseline
constraint that is formulated for the scenario in which the cameras have
overlapping views. The constraint is incorporated in the bundle adjustment
solution to keep the relative motion of different cameras static. Experiments
were conducted using video frames of two collocated GoPro cameras mounted on a
vehicle with no system calibration. These two cameras were placed capturing
overlapping contents. We performed our bundle adjustment using the proposed
constraint and then produced 3D dense point clouds. Evaluations were performed
by comparing these dense point clouds against LiDAR reference data. We showed
that, as compared to traditional bundle adjustment, our proposed method
achieved an improvement of 29.38%.",None,12062
SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues,0.783454,"Dialogue systems are usually categorized into two types, open-domain and
task-oriented. The first one focuses on chatting with users and making them
engage in the conversations, where selecting a proper topic to fit the dialogue
context is essential for a successful dialogue. The other one focuses on a
specific task instead of casual talks, e.g., finding a movie on Friday night,
or playing a song. These two directions have been studied separately due to
their different purposes. However, how smoothly transitioning from social
chatting to task-oriented dialogues is important for triggering business
opportunities, and there is no public data focusing on such scenarios. Hence,
this paper focuses on investigating the conversations starting from open-domain
social chatting and then gradually transitioning to task-oriented purposes, and
releases a large-scale dataset with detailed annotations for encouraging this
research direction. To achieve this goal, this paper proposes a framework to
automatically generate many dialogues without human involvement, in which any
powerful open-domain dialogue generation model can be easily leveraged. The
human evaluation shows that our generated dialogue data has a natural flow at a
reasonable quality, showing that our released data has a great potential of
guiding future research directions and commercial activities. Furthermore, the
released models allow researchers to automatically generate unlimited dialogues
in the target scenarios, which can greatly benefit semi-supervised and
unsupervised approaches.",https://github.com/MiuLab/SalesBot,-1
Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization,0.415939,"Current 3D scene stylization methods transfer textures and colors as styles
using arbitrary style references, lacking meaningful semantic correspondences.
We introduce Reference-Based Non-Photorealistic Radiance Fields (Ref-NPR) to
address this limitation. This controllable method stylizes a 3D scene using
radiance fields with a single stylized 2D view as a reference. We propose a ray
registration process based on the stylized reference view to obtain pseudo-ray
supervision in novel views. Then we exploit semantic correspondences in content
images to fill occluded regions with perceptually similar styles, resulting in
non-photorealistic and continuous novel view sequences. Our experimental
results demonstrate that Ref-NPR outperforms existing scene and video
stylization methods regarding visual quality and semantic correspondence. The
code and data are publicly available on the project page at
https://ref-npr.github.io.",https://ref-npr.github.io,-1
Emotion Analysis using Multi-Layered Networks for Graphical Representation of Tweets,0.527191,"Anticipating audience reaction towards a certain piece of text is integral to
several facets of society ranging from politics, research, and commercial
industries. Sentiment analysis (SA) is a useful natural language processing
(NLP) technique that utilizes both lexical/statistical and deep learning
methods to determine whether different sized texts exhibit a positive,
negative, or neutral emotion. However, there is currently a lack of tools that
can be used to analyse groups of independent texts and extract the primary
emotion from the whole set. Therefore, the current paper proposes a novel
algorithm referred to as the Multi-Layered Tweet Analyzer (MLTA) that
graphically models social media text using multi-layered networks (MLNs) in
order to better encode relationships across independent sets of tweets. Graph
structures are capable of capturing meaningful relationships in complex
ecosystems compared to other representation methods. State of the art Graph
Neural Networks (GNNs) are used to extract information from the Tweet-MLN and
make predictions based on the extracted graph features. Results show that not
only does the MLTA predict from a larger set of possible emotions, delivering a
more accurate sentiment compared to the standard positive, negative or neutral,
it also allows for accurate group-level predictions of Twitter data.",None,-1
Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,0.795867,"End-to-end speech-to-speech translation (S2ST) without relying on
intermediate text representations is a rapidly emerging frontier of research.
Recent works have demonstrated that the performance of such direct S2ST systems
is approaching that of conventional cascade S2ST when trained on comparable
datasets. However, in practice, the performance of direct S2ST is bounded by
the availability of paired S2ST training data. In this work, we explore
multiple approaches for leveraging much more widely available unsupervised and
weakly-supervised speech and text data to improve the performance of direct
S2ST based on Translatotron 2. With our most effective approaches, the average
translation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is
improved by +13.6 BLEU (or +113% relatively), as compared to the previous
state-of-the-art trained without additional data. The improvements on
low-resource language are even more significant (+398% relatively on average).
Our comparative studies suggest future research directions for S2ST and speech
representation learning.",None,-1
FORCE: A Framework of Rule-Based Conversational Recommender System,0.0559572,"The conversational recommender systems (CRSs) have received extensive
attention in recent years. However, most of the existing works focus on various
deep learning models, which are largely limited by the requirement of
large-scale human-annotated datasets. Such methods are not able to deal with
the cold-start scenarios in industrial products. To alleviate the problem, we
propose FORCE, a Framework Of Rule-based Conversational Recommender system that
helps developers to quickly build CRS bots by simple configuration. We conduct
experiments on two datasets in different languages and domains to verify its
effectiveness and usability.",None,122012
An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models,0.320632,"We propose a framework for training non-autoregressive sequence-to-sequence
models for editing tasks, where the original input sequence is iteratively
edited to produce the output. We show that the imitation learning algorithms
designed to train such models for machine translation introduces mismatches
between training and inference that lead to undertraining and poor
generalization in editing scenarios. We address this issue with two
complementary strategies: 1) a roll-in policy that exposes the model to
intermediate training sequences that it is more likely to encounter during
inference, 2) a curriculum that presents easy-to-learn edit operations first,
gradually increasing the difficulty of training samples as the model becomes
competent. We show the efficacy of these strategies on two challenging English
editing tasks: controllable text simplification and abstractive summarization.
Our approach significantly improves output quality on both tasks and controls
output complexity better on the simplification task.",https://github.com/cocoxu/,-1
A Hierarchical Deep Neural Network for Detecting Lines of Codes with Vulnerabilities,0.0387262,"Software vulnerabilities, caused by unintentional flaws in source codes, are
the main root cause of cyberattacks. Source code static analysis has been used
extensively to detect the unintentional defects, i.e. vulnerabilities,
introduced into the source codes by software developers. In this paper, we
propose a deep learning approach to detect vulnerabilities from their LLVM IR
representations based on the techniques that have been used in natural language
processing. The proposed approach uses a hierarchical process to first identify
source codes with vulnerabilities, and then it identifies the lines of codes
that contribute to the vulnerability within the detected source codes. This
proposed two-step approach reduces the false alarm of detecting vulnerable
lines. Our extensive experiment on real-world and synthetic codes collected in
NVD and SARD shows high accuracy (about 98\%) in detecting source code
vulnerabilities.",https://github.com/arashmahyari/PLP,-1
TransBoost: Improving the Best ImageNet Performance using Deep Transduction,0.0220703,"This paper deals with deep transductive learning, and proposes TransBoost as
a procedure for fine-tuning any deep neural model to improve its performance on
any (unlabeled) test set provided at training time. TransBoost is inspired by a
large margin principle and is efficient and simple to use. Our method
significantly improves the ImageNet classification performance on a wide range
of architectures, such as ResNets, MobileNetV3-L, EfficientNetB0, ViT-S, and
ConvNext-T, leading to state-of-the-art transductive performance. Additionally
we show that TransBoost is effective on a wide variety of image classification
datasets. The implementation of TransBoost is provided at:
https://github.com/omerb01/TransBoost .",https://github.com/omerb01/TransBoost,-1
DirectTracker: 3D Multi-Object Tracking Using Direct Image Alignment and Photometric Bundle Adjustment,0.262746,"Direct methods have shown excellent performance in the applications of visual
odometry and SLAM. In this work we propose to leverage their effectiveness for
the task of 3D multi-object tracking. To this end, we propose DirectTracker, a
framework that effectively combines direct image alignment for the short-term
tracking and sliding-window photometric bundle adjustment for 3D object
detection. Object proposals are estimated based on the sparse sliding-window
pointcloud and further refined using an optimization-based cost function that
carefully combines 3D and 2D cues to ensure consistency in image and world
space. We propose to evaluate 3D tracking using the recently introduced
higher-order tracking accuracy (HOTA) metric and the generalized intersection
over union similarity measure to mitigate the limitations of the conventional
use of intersection over union for the evaluation of vision-based trackers. We
perform evaluation on the KITTI Tracking benchmark for the Car class and show
competitive performance in tracking objects both in 2D and 3D.",https://github.com/JonathonLuiten/TrackEval,-1
PseudoClick: Interactive Image Segmentation with Click Imitation,0.77027,"The goal of click-based interactive image segmentation is to obtain precise
object segmentation masks with limited user interaction, i.e., by a minimal
number of user clicks. Existing methods require users to provide all the
clicks: by first inspecting the segmentation mask and then providing points on
mislabeled regions, iteratively. We ask the question: can our model directly
predict where to click, so as to further reduce the user interaction cost? To
this end, we propose {\PseudoClick}, a generic framework that enables existing
segmentation networks to propose candidate next clicks. These automatically
generated clicks, termed pseudo clicks in this work, serve as an imitation of
human clicks to refine the segmentation mask.",None,10913
Data Selection Curriculum for Neural Machine Translation,0.221039,"Neural Machine Translation (NMT) models are typically trained on
heterogeneous data that are concatenated and randomly shuffled. However, not
all of the training data are equally useful to the model. Curriculum training
aims to present the data to the NMT models in a meaningful order. In this work,
we introduce a two-stage curriculum training framework for NMT where we
fine-tune a base NMT model on subsets of data, selected by both deterministic
scoring using pre-trained methods and online scoring that considers prediction
scores of the emerging NMT model. Through comprehensive experiments on six
language pairs comprising low- and high-resource languages from WMT'21, we have
shown that our curriculum strategies consistently demonstrate better quality
(up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer
updates).",https://github.com/google/sentencepiece,-1
Memory-Based Model Editing at Scale,0.999571,"Even the largest neural networks make errors, and once-correct predictions
can become invalid as the world changes. Model editors make local updates to
the behavior of base (pre-trained) models to inject updated knowledge or
correct undesirable behaviors. Existing model editors have shown promise, but
also suffer from insufficient expressiveness: they struggle to accurately model
an edit's intended scope (examples affected by the edit), leading to inaccurate
predictions for test inputs loosely related to the edit, and they often fail
altogether after many edits. As a higher-capacity alternative, we propose
Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model
(SERAC), which stores edits in an explicit memory and learns to reason over
them to modulate the base model's predictions as needed. To enable more
rigorous evaluation of model editors, we introduce three challenging language
model editing problems based on question answering, fact-checking, and dialogue
generation. We find that only SERAC achieves high performance on all three
problems, consistently outperforming existing approaches to model editing by a
significant margin. Code, data, and additional project information will be made
available at https://sites.google.com/view/serac-editing.",None,-1
Efficient Training of Language Models to Fill in the Middle,0.719577,"We show that autoregressive language models can learn to infill text after we
apply a straightforward transformation to the dataset, which simply moves a
span of text from the middle of a document to its end. While this data
augmentation has garnered much interest in recent years, we provide extensive
evidence that training models with a large fraction of data transformed in this
way does not harm the original left-to-right generative capability, as measured
by perplexity and sampling evaluations across a wide range of scales. Given the
usefulness, simplicity, and efficiency of training models to fill-in-the-middle
(FIM), we suggest that future autoregressive language models be trained with
FIM by default. To this end, we run a series of ablations on key
hyperparameters, such as the data transformation frequency, the structure of
the transformation, and the method of selecting the infill span. We use these
ablations to prescribe strong default settings and best practices to train FIM
models. We have released our best infilling model trained with best practices
in our API, and release our infilling benchmarks to aid future research.",None,-1
Pronunciation Modeling of Foreign Words for Mandarin ASR by Considering the Effect of Language Transfer,0.49142,"One of the challenges in automatic speech recognition is foreign words
recognition. It is observed that a speaker's pronunciation of a foreign word is
influenced by his native language knowledge, and such phenomenon is known as
the effect of language transfer. This paper focuses on examining the phonetic
effect of language transfer in automatic speech recognition. A set of lexical
rules is proposed to convert an English word into Mandarin phonetic
representation. In this way, a Mandarin lexicon can be augmented by including
English words. Hence, the Mandarin ASR system becomes capable to recognize
English words without retraining or re-estimation of the acoustic model
parameters. Using the lexicon that derived from the proposed rules, the ASR
performance of Mandarin English mixed speech is improved without harming the
accuracy of Mandarin only speech. The proposed lexical rules are generalized
and they can be directly applied to unseen English words.",None,-1
Depth-aware Neural Style Transfer using Instance Normalization,0.454876,"Neural Style Transfer (NST) is concerned with the artistic stylization of
visual media. It can be described as the process of transferring the style of
an artistic image onto an ordinary photograph. Recently, a number of studies
have considered the enhancement of the depth-preserving capabilities of the NST
algorithms to address the undesired effects that occur when the input content
images include numerous objects at various depths. Our approach uses a deep
residual convolutional network with instance normalization layers that utilizes
an advanced depth prediction network to integrate depth preservation as an
additional loss function to content and style. We demonstrate results that are
effective in retaining the depth and global structure of content images. Three
different evaluation processes show that our system is capable of preserving
the structure of the stylized results while exhibiting style-capture
capabilities and aesthetic qualities comparable or superior to state-of-the-art
methods. Project page:
https://ioannoue.github.io/depth-aware-nst-using-in.html.",https://ioannoue.github.io/depth-aware-nst-using-in.html,97
SPECTRE: Spectral Conditioning Helps to Overcome the Expressivity Limits of One-shot Graph Generators,0.601672,"We approach the graph generation problem from a spectral perspective by first
generating the dominant parts of the graph Laplacian spectrum and then building
a graph matching these eigenvalues and eigenvectors. Spectral conditioning
allows for direct modeling of the global and local graph structure and helps to
overcome the expressivity and mode collapse issues of one-shot graph
generators. Our novel GAN, called SPECTRE, enables the one-shot generation of
much larger graphs than previously possible with one-shot models. SPECTRE
outperforms state-of-the-art deep autoregressive generators in terms of
modeling fidelity, while also avoiding expensive sequential generation and
dependence on node ordering. A case in point, in sizable synthetic and
real-world graphs SPECTRE achieves a 4-to-170 fold improvement over the best
competitor that does not overfit and is 23-to-30 times faster than
autoregressive generators.",https://github.com/KarolisMart/SPECTRE,-1
Handling sign language transcription system with the computer-friendly numerical multilabels,0.158244,"This paper presents our recent developments in the automatic processing of
sign language corpora using the Hamburg Sign Language Annotation System
(HamNoSys). We designed an automated tool to convert HamNoSys annotations into
numerical labels for defined initial features of body and hand positions. Our
proposed numerical multilabels greatly simplify annotations' structure without
significant loss of gloss meaning. These numerical multilabels can potentially
be used to feed the machine learning models, which would accelerate the
development of vision-based sign language recognition. In addition, this tool
can assist experts in the annotation process and help identify semantic errors.
The code and sample annotations are publicly available at
\url{https://github.com/hearai/parse-hamnosys}.",https://github.com/hearai/parse-hamnosys,-1
Densely Constrained Depth Estimator for Monocular 3D Object Detection,0.687387,"Estimating accurate 3D locations of objects from monocular images is a
challenging problem because of lacking depth. Previous work shows that
utilizing the object's keypoint projection constraints to estimate multiple
depth candidates boosts the detection performance. However, the existing
methods can only utilize vertical edges as projection constraints for depth
estimation. So these methods only use a small number of projection constraints
and produce insufficient depth candidates, leading to inaccurate depth
estimation. In this paper, we propose a method that utilizes dense projection
constraints from edges of any direction. In this way, we employ much more
projection constraints and produce considerable depth candidates. Besides, we
present a graph matching weighting module to merge the depth candidates. The
proposed method DCD (Densely Constrained Detector) achieves state-of-the-art
performance on the KITTI and WOD benchmarks. Code is released at
https://github.com/BraveGroup/DCD.",https://github.com/BraveGroup/DCD,-1
NAN: Noise-Aware NeRFs for Burst-Denoising,0.653369,"Burst denoising is now more relevant than ever, as computational photography
helps overcome sensitivity issues inherent in mobile phones and small cameras.
A major challenge in burst-denoising is in coping with pixel misalignment,
which was so far handled with rather simplistic assumptions of simple motion,
or the ability to align in pre-processing. Such assumptions are not realistic
in the presence of large motion and high levels of noise. We show that Neural
Radiance Fields (NeRFs), originally suggested for physics-based novel-view
rendering, can serve as a powerful framework for burst denoising. NeRFs have an
inherent capability of handling noise as they integrate information from
multiple images, but they are limited in doing so, mainly since they build on
pixel-wise operations which are suitable to ideal imaging conditions. Our
approach, termed NAN, leverages inter-view and spatial information in NeRFs to
better deal with noise. It achieves state-of-the-art results in burst denoising
and is especially successful in coping with large movement and occlusions,
under very high levels of noise. With the rapid advances in accelerating NeRFs,
it could provide a powerful platform for denoising in challenging environments.",None,6308
A Unified Continuous Learning Framework for Multi-modal Knowledge Discovery and Pre-training,0.340155,"Multi-modal pre-training and knowledge discovery are two important research
topics in multi-modal machine learning. Nevertheless, none of existing works
make attempts to link knowledge discovery with knowledge guided multi-modal
pre-training. In this paper, we propose to unify them into a continuous
learning framework for mutual improvement. Taking the open-domain uni-modal
datasets of images and texts as input, we maintain a knowledge graph as the
foundation to support these two tasks. For knowledge discovery, a pre-trained
model is used to identify cross-modal links on the graph. For model
pre-training, the knowledge graph is used as the external knowledge to guide
the model updating. These two steps are iteratively performed in our framework
for continuous learning. The experimental results on MS-COCO and Flickr30K with
respect to both knowledge discovery and the pre-trained model validate the
effectiveness of our framework.",None,-1
Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation,0.99495,"Although the problem of hallucinations in neural machine translation (NMT)
has received some attention, research on this highly pathological phenomenon
lacks solid ground. Previous work has been limited in several ways: it often
resorts to artificial settings where the problem is amplified, it disregards
some (common) types of hallucinations, and it does not validate adequacy of
detection heuristics. In this paper, we set foundations for the study of NMT
hallucinations. First, we work in a natural setting, i.e., in-domain data
without artificial noise neither in training nor in inference. Next, we
annotate a dataset of over 3.4k sentences indicating different kinds of
critical errors and hallucinations. Then, we turn to detection methods and both
revisit methods used previously and propose using glass-box uncertainty-based
detectors. Overall, we show that for preventive settings, (i) previously used
methods are largely inadequate, (ii) sequence log-probability works best and
performs on par with reference-based methods. Finally, we propose
DeHallucinator, a simple method for alleviating hallucinations at test time
that significantly reduces the hallucinatory rate. To ease future research, we
release our annotated dataset for WMT18 German-English data, along with the
model, training data, and code.",https://github.com/deep-spin/hallucinations-in-nmt,-1
Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation,0.205987,"The past several years have witnessed Variational Auto-Encoder's superiority
in various text generation tasks. However, due to the sequential nature of the
text, auto-regressive decoders tend to ignore latent variables and then reduce
to simple language models, known as the KL vanishing problem, which would
further deteriorate when VAE is combined with Transformer-based structures. To
ameliorate this problem, we propose DELLA, a novel variational Transformer
framework. DELLA learns a series of layer-wise latent variables with each
inferred from those of lower layers and tightly coupled with the hidden states
by low-rank tensor product. In this way, DELLA forces these posterior latent
variables to be fused deeply with the whole computation path and hence
incorporate more information. We theoretically demonstrate that our method can
be regarded as entangling latent variables to avoid posterior information
decrease through layers, enabling DELLA to get higher non-zero KL values even
without any annealing or thresholding tricks. Experiments on four unconditional
and three conditional generation tasks show that DELLA could better alleviate
KL vanishing and improve both quality and diversity compared to several strong
baselines.",https://github.com/OpenVLG/DELLA.git,-1
Deep Leaning-Based Ultra-Fast Stair Detection,0.124783,"Staircases are some of the most common building structures in urban
environments. Stair detection is an important task for various applications,
including the environmental perception of exoskeleton robots, humanoid robots,
and rescue robots and the navigation of visually impaired people. Most existing
stair detection algorithms have difficulty dealing with the diversity of stair
structure materials, extreme light and serious occlusion. Inspired by human
perception, we propose an end-to-end method based on deep learning.
Specifically, we treat the process of stair line detection as a multitask
involving coarse-grained semantic segmentation and object detection. The input
images are divided into cells, and a simple neural network is used to judge
whether each cell contains stair lines. For cells containing stair lines, the
locations of the stair lines relative to each cell are regressed. Extensive
experiments on our dataset show that our method can achieve high performance in
terms of both speed and accuracy. A lightweight version can even achieve 300+
frames per second with the same resolution. Our code and dataset will be soon
available at GitHub.",None,3720
Understanding Influence Functions and Datamodels via Harmonic Analysis,0.799181,"Influence functions estimate effect of individual data points on predictions
of the model on test data and were adapted to deep learning in Koh and Liang
[2017]. They have been used for detecting data poisoning, detecting helpful and
harmful examples, influence of groups of datapoints, etc. Recently, Ilyas et
al. [2022] introduced a linear regression method they termed datamodels to
predict the effect of training points on outputs on test data. The current
paper seeks to provide a better theoretical understanding of such interesting
empirical phenomena. The primary tool is harmonic analysis and the idea of
noise stability. Contributions include: (a) Exact characterization of the
learnt datamodel in terms of Fourier coefficients. (b) An efficient method to
estimate the residual error and quality of the optimum linear datamodel without
having to train the datamodel. (c) New insights into when influences of groups
of datapoints may or may not add up linearly.",https://github.com/libffcv/ffcv/,-1
PoseGU: 3D Human Pose Estimation with Novel Human Pose Generator and Unbiased Learning,0.265801,"3D pose estimation has recently gained substantial interests in computer
vision domain. Existing 3D pose estimation methods have a strong reliance on
large size well-annotated 3D pose datasets, and they suffer poor model
generalization on unseen poses due to limited diversity of 3D poses in training
sets. In this work, we propose PoseGU, a novel human pose generator that
generates diverse poses with access only to a small size of seed samples, while
equipping the Counterfactual Risk Minimization to pursue an unbiased evaluation
objective. Extensive experiments demonstrate PoseGU outforms almost all the
state-of-the-art 3D human pose methods under consideration over three popular
benchmark datasets. Empirical analysis also proves PoseGU generates 3D poses
with improved data diversity and better generalization ability.",None,-1
UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes,0.635325,"We introduce UViM, a unified approach capable of modeling a wide range of
computer vision tasks. In contrast to previous models, UViM has the same
functional form for all tasks; it requires no task-specific modifications which
require extensive human expertise. The approach involves two components: (I) a
base model (feed-forward) which is trained to directly predict raw vision
outputs, guided by a learned discrete code and (II) a language model
(autoregressive) that is trained to generate the guiding code. These components
complement each other: the language model is well-suited to modeling structured
interdependent data, while the base model is efficient at dealing with
high-dimensional outputs. We demonstrate the effectiveness of UViM on three
diverse and challenging vision tasks: panoptic segmentation, depth prediction
and image colorization, where we achieve competitive and near state-of-the-art
results. Our experimental results suggest that UViM is a promising candidate
for a unified modeling approach in computer vision.",https://github.com/google-research/big_vision,-1
Generalised Implicit Neural Representations,0.350687,"We consider the problem of learning implicit neural representations (INRs)
for signals on non-Euclidean domains. In the Euclidean case, INRs are trained
on a discrete sampling of a signal over a regular lattice. Here, we assume that
the continuous signal exists on some unknown topological space from which we
sample a discrete graph. In the absence of a coordinate system to identify the
sampled nodes, we propose approximating their location with a spectral
embedding of the graph. This allows us to train INRs without knowing the
underlying continuous domain, which is the case for most graph signals in
nature, while also making the INRs independent of any choice of coordinate
system. We show experiments with our method on various real-world signals on
non-Euclidean domains.",https://github.com/danielegrattarola/GINR,-1
Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs,0.529341,"Question answering over temporal knowledge graphs (KGs) efficiently uses
facts contained in a temporal KG, which records entity relations and when they
occur in time, to answer natural language questions (e.g., ""Who was the
president of the US before Obama?""). These questions often involve three
time-related challenges that previous work fail to adequately address: 1)
questions often do not specify exact timestamps of interest (e.g., ""Obama""
instead of 2000); 2) subtle lexical differences in time relations (e.g.,
""before"" vs ""after""); 3) off-the-shelf temporal KG embeddings that previous
work builds on ignore the temporal order of timestamps, which is crucial for
answering temporal-order related questions. In this paper, we propose a
time-sensitive question answering (TSQA) framework to tackle these problems.
TSQA features a timestamp estimation module to infer the unwritten timestamp
from the question. We also employ a time-sensitive KG encoder to inject
ordering information into the temporal KG embeddings that TSQA is based on.
With the help of techniques to reduce the search space for potential answers,
TSQA significantly outperforms the previous state of the art on a new benchmark
for question answering over temporal KGs, especially achieving a 32% (absolute)
error reduction on complex questions that require multiple steps of reasoning
over facts in the temporal KG.",https://github.com/apoorvumang/CronKGQA,-1
Seeking Diverse Reasoning Logic: Controlled Equation Expression Generation for Solving Math Word Problems,0.418547,"To solve Math Word Problems, human students leverage diverse reasoning logic
that reaches different possible equation solutions. However, the mainstream
sequence-to-sequence approach of automatic solvers aims to decode a fixed
solution equation supervised by human annotation. In this paper, we propose a
controlled equation generation solver by leveraging a set of control codes to
guide the model to consider certain reasoning logic and decode the
corresponding equations expressions transformed from the human reference. The
empirical results suggest that our method universally improves the performance
on single-unknown (Math23K) and multiple-unknown (DRAW1K, HMWP) benchmarks,
with substantial improvements up to 13.2% accuracy on the challenging
multiple-unknown datasets.",https://github.com/yiyunya/CTRL-MWP,7897
LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning,0.817618,"Recently, deep learning models have made great progress in MWP solving on
answer accuracy. However, they are uninterpretable since they mainly rely on
shallow heuristics to achieve high performance without understanding and
reasoning the grounded math logic. To address this issue and make a step
towards interpretable MWP solving, we first construct a high-quality MWP
dataset named InterMWP which consists of 11,495 MWPs and annotates
interpretable logical formulas based on algebraic knowledge as the grounded
linguistic logic of each solution equation. Different from existing MWP
datasets, our InterMWP benchmark asks for a solver to not only output the
solution expressions but also predict the corresponding logical formulas. We
further propose a novel approach with logical prompt and interpretation
generation, called LogicSolver. For each MWP, our LogicSolver first retrieves
some highly-correlated algebraic knowledge and then passes them to the backbone
model as prompts to improve the semantic representations of MWPs. With these
improved semantic representations, our LogicSolver generates corresponding
solution expressions and interpretable knowledge formulas in accord with the
generated solution expressions, simultaneously. Experimental results show that
our LogicSolver has stronger logical formula-based interpretability than
baselines while achieving higher answer accuracy with the help of logical
prompts, simultaneously. The source code and dataset is available at
https://github.com/yangzhch6/InterMWP.",https://github.com/yangzhch6/InterMWP,-1
Semi-Supervised Formality Style Transfer with Consistency Training,0.750027,"Formality style transfer (FST) is a task that involves paraphrasing an
informal sentence into a formal one without altering its meaning. To address
the data-scarcity problem of existing parallel datasets, previous studies tend
to adopt a cycle-reconstruction scheme to utilize additional unlabeled data,
where the FST model mainly benefits from target-side unlabeled sentences. In
this work, we propose a simple yet effective semi-supervised framework to
better utilize source-side unlabeled sentences based on consistency training.
Specifically, our approach augments pseudo-parallel data obtained from a
source-side informal sentence by enforcing the model to generate similar
outputs for its perturbed version. Moreover, we empirically examined the
effects of various data perturbation methods and propose effective data
filtering strategies to improve our framework. Experimental results on the
GYAFC benchmark demonstrate that our approach can achieve state-of-the-art
results, even with less than 40% of the parallel data.",https://github.com/Aolius/semi-fst,-1
CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion,0.784557,"Knowledge graphs store a large number of factual triples while they are still
incomplete, inevitably. The previous knowledge graph completion (KGC) models
predict missing links between entities merely relying on fact-view data,
ignoring the valuable commonsense knowledge. The previous knowledge graph
embedding (KGE) techniques suffer from invalid negative sampling and the
uncertainty of fact-view link prediction, limiting KGC's performance. To
address the above challenges, we propose a novel and scalable Commonsense-Aware
Knowledge Embedding (CAKE) framework to automatically extract commonsense from
factual triples with entity concepts. The generated commonsense augments
effective self-supervision to facilitate both high-quality negative sampling
(NS) and joint commonsense and fact-view link prediction. Experimental results
on the KGC task demonstrate that assembling our framework could enhance the
performance of the original KGE models, and the proposed commonsense-aware NS
module is superior to other NS techniques. Besides, our proposed framework
could be easily adaptive to various KGE models and explain the predicted
results.",https://github.com/ngl567/CAKE,-1
Language Models of Code are Few-Shot Commonsense Learners,0.983474,"We address the general task of structured commonsense reasoning: given a
natural language input, the goal is to generate a graph such as an event -- or
a reasoning-graph. To employ large language models (LMs) for this task,
existing approaches ``serialize'' the output graph as a flat list of nodes and
edges. Although feasible, these serialized graphs strongly deviate from the
natural language corpora that LMs were pre-trained on, hindering LMs from
generating them correctly. In this paper, we show that when we instead frame
structured commonsense reasoning tasks as code generation tasks, pre-trained
LMs of code are better structured commonsense reasoners than LMs of natural
language, even when the downstream task does not involve source code at all. We
demonstrate our approach across three diverse structured commonsense reasoning
tasks. In all these natural language tasks, we show that using our approach, a
code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the
target task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot
setting.",https://github.com/madaan/CoCoGen,-1
DPMS: An ADD-Based Symbolic Approach for Generalized MaxSAT Solving,0.0837034,"Boolean MaxSAT, as well as generalized formulations such as Min-MaxSAT and
Max-hybrid-SAT, are fundamental optimization problems in Boolean reasoning.
Existing methods for MaxSAT have been successful in solving benchmarks in CNF
format. They lack, however, the ability to handle 1) (non-CNF) hybrid
constraints, such as XORs and 2) generalized MaxSAT problems natively. To
address this issue, we propose a novel dynamic-programming approach for solving
generalized MaxSAT problems with hybrid constraints -- called
\emph{Dynamic-Programming-MaxSAT} or DPMS for short -- based on Algebraic
Decision Diagrams (ADDs). With the power of ADDs and the (graded)
project-join-tree builder, our versatile framework admits many generalizations
of CNF-MaxSAT, such as MaxSAT, Min-MaxSAT, and MinSAT with hybrid constraints.
Moreover, DPMS scales provably well on instances with low width. Empirical
results indicate that DPMS is able to solve certain problems quickly, where
other algorithms based on various techniques all fail. Hence, DPMS is a
promising framework and opens a new line of research that invites more
investigation in the future.",None,-1
Quantitative Method for Security Situation of the Power Information Network Based on the Evolutionary Neural Network,0.108854,"Cybersecurity is the security cornerstone of digital transformation of the
power grid and construction of new power systems. The traditional network
security situation quantification method only analyzes from the perspective of
network performance, ignoring the impact of various power application services
on the security situation, so the quantification results cannot fully reflect
the power information network risk state. This study proposes a method for
quantifying security situation of the power information network based on the
evolutionary neural network. First, the security posture system architecture is
designed by analyzing the business characteristics of power information network
applications. Second, combining the importance of power application business,
the spatial element index system of coupled interconnection is established from
three dimensions of network reliability, threat, and vulnerability. Then, the
BP neural network optimized by the genetic evolutionary algorithm is
incorporated into the element index calculation process, and the quantitative
model of security posture of the power information network based on the
evolutionary neural network is constructed. Finally, a simulation experiment
environment is built according to a power sector network topology, and the
effectiveness and robustness of the method proposed in the study are verified.",None,-1
Score-based Generative Modeling Secretly Minimizes the Wasserstein Distance,0.516922,"Score-based generative models are shown to achieve remarkable empirical
performances in various applications such as image generation and audio
synthesis. However, a theoretical understanding of score-based diffusion models
is still incomplete. Recently, Song et al. showed that the training objective
of score-based generative models is equivalent to minimizing the
Kullback-Leibler divergence of the generated distribution from the data
distribution. In this work, we show that score-based models also minimize the
Wasserstein distance between them under suitable assumptions on the model.
Specifically, we prove that the Wasserstein distance is upper bounded by the
square root of the objective function up to multiplicative constants and a
fixed constant offset. Our proof is based on a novel application of the theory
of optimal transport, which can be of independent interest to the society. Our
numerical experiments support our findings. By analyzing our upper bounds, we
provide a few techniques to obtain tighter upper bounds.",https://github.com/UW-Madison-Lee-Lab/score-wasserstein,-1
Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production,0.606095,"Mixture of Experts (MoE) models with conditional execution of sparsely
activated layers have enabled training models with a much larger number of
parameters. As a result, these models have achieved significantly better
quality on various natural language processing tasks including machine
translation. However, it remains challenging to deploy such models in real-life
scenarios due to the large memory requirements and inefficient inference. In
this work, we introduce a highly efficient inference framework with several
optimization approaches to accelerate the computation of sparse models and cut
down the memory consumption significantly. While we achieve up to 26x speed-up
in terms of throughput, we also reduce the model size almost to one eighth of
the original 32-bit float model by quantizing expert weights into 4-bit
integers. As a result, we are able to deploy 136x larger models with 27% less
cost and significantly better quality compared to the existing solutions. This
enables a paradigm shift in deploying large scale multilingual MoE transformers
models replacing the traditional practice of distilling teacher models into
dozens of smaller models per language or task.",https://github.com/NVIDIA/FasterTransformer,-1
Accelerating Machine Learning via the Weber-Fechner Law,0.528608,"The Weber-Fechner Law observes that human perception scales as the logarithm
of the stimulus. We argue that learning algorithms for human concepts could
benefit from the Weber-Fechner Law. Specifically, we impose Weber-Fechner on
simple neural networks, with or without convolution, via the logarithmic power
series of their sorted output. Our experiments show surprising performance and
accuracy on the MNIST data set within a few training iterations and limited
computational resources, suggesting that Weber-Fechner can accelerate machine
learning of human concepts.",None,-1
Effidit: Your AI Writing Assistant,0.410136,"In this technical report, we introduce Effidit (Efficient and Intelligent
Editing), a digital writing assistant that facilitates users to write
higher-quality text more efficiently by using artificial intelligence (AI)
technologies. Previous writing assistants typically provide the function of
error checking (to detect and correct spelling and grammatical errors) and
limited text-rewriting functionality. With the emergence of large-scale neural
language models, some systems support automatically completing a sentence or a
paragraph. In Effidit, we significantly expand the capacities of a writing
assistant by providing functions in five categories: text completion, error
checking, text polishing, keywords to sentences (K2S), and cloud input methods
(cloud IME). In the text completion category, Effidit supports generation-based
sentence completion, retrieval-based sentence completion, and phrase
completion. In contrast, many other writing assistants so far only provide one
or two of the three functions. For text polishing, we have three functions:
(context-aware) phrase polishing, sentence paraphrasing, and sentence
expansion, whereas many other writing assistants often support one or two
functions in this category. The main contents of this report include major
modules of Effidit, methods for implementing these modules, and evaluation
results of some key methods.",https://github.com/facebookresearch/unlikelihood_training,-1
Impact of Tokenization on Language Models: An Analysis for Turkish,0.561189,"Tokenization is an important text preprocessing step to prepare input tokens
for deep language models. WordPiece and BPE are de facto methods employed by
important models, such as BERT and GPT. However, the impact of tokenization can
be different for morphologically rich languages, such as Turkic languages,
where many words can be generated by adding prefixes and suffixes. We compare
five tokenizers at different granularity levels, i.e. their outputs vary from
smallest pieces of characters to the surface form of words, including a
Morphological-level tokenizer. We train these tokenizers and pretrain
medium-sized language models using RoBERTa pretraining procedure on the Turkish
split of the OSCAR corpus. We then fine-tune our models on six downstream
tasks. Our experiments, supported by statistical tests, reveal that
Morphological-level tokenizer has challenging performance with de facto
tokenizers. Furthermore, we find that increasing the vocabulary size improves
the performance of Morphological and Word-level tokenizers more than that of de
facto tokenizers. The ratio of the number of vocabulary parameters to the total
number of model parameters can be empirically chosen as 20% for de facto
tokenizers and 40% for other tokenizers to obtain a reasonable trade-off
between model size and performance.",None,-1
RobustLoc: Robust Camera Pose Regression in Challenging Driving Environments,0.775592,"Camera relocalization has various applications in autonomous driving.
Previous camera pose regression models consider only ideal scenarios where
there is little environmental perturbation. To deal with challenging driving
environments that may have changing seasons, weather, illumination, and the
presence of unstable objects, we propose RobustLoc, which derives its
robustness against perturbations from neural differential equations. Our model
uses a convolutional neural network to extract feature maps from multi-view
images, a robust neural differential equation diffusion block module to diffuse
information interactively, and a branched pose decoder with multi-layer
training to estimate the vehicle poses. Experiments demonstrate that RobustLoc
surpasses current state-of-the-art camera pose regression models and achieves
robust performance in various environments. Our code is released at:
https://github.com/sijieaaa/RobustLoc",https://github.com/sijieaaa/RobustLoc,-1
MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation,0.657132,"Responding with multi-modal content has been recognized as an essential
capability for an intelligent conversational agent. In this paper, we introduce
the MMDialog dataset to better facilitate multi-modal conversation. MMDialog is
composed of a curated set of 1.08 million real-world dialogues with 1.53
million unique images across 4,184 topics. MMDialog has two main and unique
advantages. First, it is the largest multi-modal conversation dataset by the
number of dialogues by 88x. Second, it contains massive topics to generalize
the open-domain. To build engaging dialogue system with this dataset, we
propose and normalize two response producing tasks based on retrieval and
generative scenarios. In addition, we build two baselines for above tasks with
state-of-the-art techniques and report their experimental performance. We also
propose a novel evaluation metric MM-Relevance to measure the multi-modal
responses. Our dataset and scripts are available in
https://github.com/victorsungo/MMDialog.",https://github.com/victorsungo/MMDialog,-1
Diffeomorphic Counterfactuals with Generative Models,0.510276,"Counterfactuals can explain classification decisions of neural networks in a
human interpretable way. We propose a simple but effective method to generate
such counterfactuals. More specifically, we perform a suitable diffeomorphic
coordinate transformation and then perform gradient ascent in these coordinates
to find counterfactuals which are classified with great confidence as a
specified target class. We propose two methods to leverage generative models to
construct such suitable coordinate systems that are either exactly or
approximately diffeomorphic. We analyze the generation process theoretically
using Riemannian differential geometry and validate the quality of the
generated counterfactuals using various qualitative and quantitative measures.",https://github.com/annahdo/counterfactuals,-1
In-Vehicle Interface Adaptation to Environment-Induced Cognitive Workload,0.521885,"Many car accidents are caused by human distractions, including cognitive
distractions. In-vehicle human-machine interfaces (HMIs) have evolved
throughout the years, providing more and more functions. Interaction with the
HMIs can, however, also lead to further distractions and, as a consequence,
accidents. To tackle this problem, we propose using adaptive HMIs that change
according to the mental workload of the driver. In this work, we present the
current status as well as preliminary results of a user study using
naturalistic secondary tasks while driving (i.e., the primary task) that
attempt to understand the effects of one such interface.",None,67
Convolutional and Residual Networks Provably Contain Lottery Tickets,0.926726,"The Lottery Ticket Hypothesis continues to have a profound practical impact
on the quest for small scale deep neural networks that solve modern deep
learning tasks at competitive performance. These lottery tickets are identified
by pruning large randomly initialized neural networks with architectures that
are as diverse as their applications. Yet, theoretical insights that attest
their existence have been mostly focused on deep fully-connected feed forward
networks with ReLU activation functions. We prove that also modern
architectures consisting of convolutional and residual layers that can be
equipped with almost arbitrary activation functions can contain lottery tickets
with high probability.",None,-1
Read before Generate! Faithful Long Form Question Answering with Machine Reading,0.633893,"Long-form question answering (LFQA) aims to generate a paragraph-length
answer for a given question. While current work on LFQA using large pre-trained
model for generation are effective at producing fluent and somewhat relevant
content, one primary challenge lies in how to generate a faithful answer that
has less hallucinated content. We propose a new end-to-end framework that
jointly models answer generation and machine reading. The key idea is to
augment the generation model with fine-grained, answer-related salient
information which can be viewed as an emphasis on faithful facts.
State-of-the-art results on two LFQA datasets, ELI5 and MS MARCO, demonstrate
the effectiveness of our method, in comparison with strong baselines on
automatic and human evaluation metrics. A detailed analysis further proves the
competency of our methods in generating fluent, relevant, and more faithful
answers.",https://github.com/facebookresearch/faiss,-1
Exact and approximate determination of the Pareto set using minimal correction subsets,0.0746911,"Recently, it has been shown that the enumeration of Minimal Correction
Subsets (MCS) of Boolean formulas allows solving Multi-Objective Boolean
Optimization (MOBO) formulations. However, a major drawback of this approach is
that most MCSs do not correspond to Pareto-optimal solutions. In fact, one can
only know that a given MCS corresponds to a Pareto-optimal solution when all
MCSs are enumerated. Moreover, if it is not possible to enumerate all MCSs,
then there is no guarantee of the quality of the approximation of the Pareto
frontier. This paper extends the state of the art for solving MOBO using MCSs.
First, we show that it is possible to use MCS enumeration to solve MOBO
problems such that each MCS necessarily corresponds to a Pareto-optimal
solution. Additionally, we also propose two new algorithms that can find a (1 +
{\varepsilon})-approximation of the Pareto frontier using MCS enumeration.
Experimental results in several benchmark sets show that the newly proposed
algorithms allow finding better approximations of the Pareto frontier than
state-of-the-art algorithms, and with guaranteed approximation ratios.",None,-1
Tackling Online One-Class Incremental Learning by Removing Negative Contrasts,0.355712,"Recent work studies the supervised online continual learning setting where a
learner receives a stream of data whose class distribution changes over time.
Distinct from other continual learning settings the learner is presented new
samples only once and must distinguish between all seen classes. A number of
successful methods in this setting focus on storing and replaying a subset of
samples alongside incoming data in a computationally efficient manner. One
recent proposal ER-AML achieved strong performance in this setting by applying
an asymmetric loss based on contrastive learning to the incoming data and
replayed data. However, a key ingredient of the proposed method is avoiding
contrasts between incoming data and stored data, which makes it impractical for
the setting where only one new class is introduced in each phase of the stream.
In this work we adapt a recently proposed approach (\textit{BYOL}) from
self-supervised learning to the supervised learning setting, unlocking the
constraint on contrasts. We then show that supplementing this with additional
regularization on class prototypes yields a new method that achieves strong
performance in the one-class incremental learning setting and is competitive
with the top performing methods in the multi-class incremental setting.",None,-1
Cross-Lingual QA as a Stepping Stone for Monolingual Open QA in Icelandic,0.218349,"It can be challenging to build effective open question answering (open QA)
systems for languages other than English, mainly due to a lack of labeled data
for training. We present a data efficient method to bootstrap such a system for
languages other than English. Our approach requires only limited QA resources
in the given language, along with machine-translated data, and at least a
bilingual language model. To evaluate our approach, we build such a system for
the Icelandic language and evaluate performance over trivia style datasets. The
corpora used for training are English in origin but machine translated into
Icelandic. We train a bilingual Icelandic/English language model to embed
English context and Icelandic questions following methodology introduced with
DensePhrases (Lee et al., 2021). The resulting system is an open domain
cross-lingual QA system between Icelandic and English. Finally, the system is
adapted for Icelandic only open QA, demonstrating how it is possible to
efficiently create an open QA system with limited access to curated datasets in
the language of interest.",https://github.com/soskek/bookcorpus/issues/27,-1
Locality-aware Attention Network with Discriminative Dynamics Learning for Weakly Supervised Anomaly Detection,0.45271,"Video anomaly detection is recently formulated as a multiple instance
learning task under weak supervision, in which each video is treated as a bag
of snippets to be determined whether contains anomalies. Previous efforts
mainly focus on the discrimination of the snippet itself without modeling the
temporal dynamics, which refers to the variation of adjacent snippets.
Therefore, we propose a Discriminative Dynamics Learning (DDL) method with two
objective functions, i.e., dynamics ranking loss and dynamics alignment loss.
The former aims to enlarge the score dynamics gap between positive and negative
bags while the latter performs temporal alignment of the feature dynamics and
score dynamics within the bag. Moreover, a Locality-aware Attention Network
(LA-Net) is constructed to capture global correlations and re-calibrate the
location preference across snippets, followed by a multilayer perceptron with
causal convolution to obtain anomaly scores. Experimental results show that our
method achieves significant improvements on two challenging benchmarks, i.e.,
UCF-Crime and XD-Violence.",None,-1
Weight Predictor Network with Feature Selection for Small Sample Tabular Biomedical Data,0.456113,"Tabular biomedical data is often high-dimensional but with a very small
number of samples. Although recent work showed that well-regularised simple
neural networks could outperform more sophisticated architectures on tabular
data, they are still prone to overfitting on tiny datasets with many
potentially irrelevant features. To combat these issues, we propose Weight
Predictor Network with Feature Selection (WPFS) for learning neural networks
from high-dimensional and small sample data by reducing the number of learnable
parameters and simultaneously performing feature selection. In addition to the
classification network, WPFS uses two small auxiliary networks that together
output the weights of the first layer of the classification model. We evaluate
on nine real-world biomedical datasets and demonstrate that WPFS outperforms
other standard as well as more recent methods typically applied to tabular
data. Furthermore, we investigate the proposed feature selection mechanism and
show that it improves performance while providing useful insights into the
learning task.",https://github.com/andreimargeloiu/WPFS,-1
Roadmap for Cybersecurity in Autonomous Vehicles,0.824327,"Autonomous vehicles are on the horizon and will be transforming
transportation safety and comfort. These vehicles will be connected to various
external systems and utilize advanced embedded systems to perceive their
environment and make intelligent decisions. However, this increased
connectivity makes these vehicles vulnerable to various cyber-attacks that can
have catastrophic effects. Attacks on automotive systems are already on the
rise in today's vehicles and are expected to become more commonplace in future
autonomous vehicles. Thus, there is a need to strengthen cybersecurity in
future autonomous vehicles. In this article, we discuss major automotive
cyber-attacks over the past decade and present state-of-the-art solutions that
leverage artificial intelligence (AI). We propose a roadmap towards building
secure autonomous vehicles and highlight key open challenges that need to be
addressed.",None,-1
Understanding the Covariance Structure of Convolutional Filters,0.523239,"Neural network weights are typically initialized at random from univariate
distributions, controlling just the variance of individual weights even in
highly-structured operations like convolutions. Recent ViT-inspired
convolutional networks such as ConvMixer and ConvNeXt use large-kernel
depthwise convolutions whose learned filters have notable structure; this
presents an opportunity to study their empirical covariances. In this work, we
first observe that such learned filters have highly-structured covariance
matrices, and moreover, we find that covariances calculated from small networks
may be used to effectively initialize a variety of larger networks of different
depths, widths, patch sizes, and kernel sizes, indicating a degree of
model-independence to the covariance structure. Motivated by these findings, we
then propose a learning-free multivariate initialization scheme for
convolutional filters using a simple, closed-form construction of their
covariance. Models using our initialization outperform those using traditional
univariate initializations, and typically meet or exceed the performance of
those initialized from the covariances of learned filters; in some cases, this
improvement can be achieved without training the depthwise convolutional
filters at all.",None,-1
Incremental Online Learning Algorithms Comparison for Gesture and Visual Smart Sensors,0.352106,"Tiny machine learning (TinyML) in IoT systems exploits MCUs as edge devices
for data processing. However, traditional TinyML methods can only perform
inference, limited to static environments or classes. Real case scenarios
usually work in dynamic environments, thus drifting the context where the
original neural model is no more suitable. For this reason, pre-trained models
reduce accuracy and reliability during their lifetime because the data recorded
slowly becomes obsolete or new patterns appear. Continual learning strategies
maintain the model up to date, with runtime fine-tuning of the parameters. This
paper compares four state-of-the-art algorithms in two real applications: i)
gesture recognition based on accelerometer data and ii) image classification.
Our results confirm these systems' reliability and the feasibility of deploying
them in tiny-memory MCUs, with a drop in the accuracy of a few percentage
points with respect to the original models for unconstrained computing
platforms.",https://gitlab.com/alba11/,-1
Fine Detailed Texture Learning for 3D Meshes with Generative Models,0.301897,"This paper presents a method to reconstruct high-quality textured 3D models
from both multi-view and single-view images. The reconstruction is posed as an
adaptation problem and is done progressively where in the first stage, we focus
on learning accurate geometry, whereas in the second stage, we focus on
learning the texture with a generative adversarial network. In the generative
learning pipeline, we propose two improvements. First, since the learned
textures should be spatially aligned, we propose an attention mechanism that
relies on the learnable positions of pixels. Secondly, since discriminator
receives aligned texture maps, we augment its input with a learnable embedding
which improves the feedback to the generator. We achieve significant
improvements on multi-view sequences from Tripod dataset as well as on
single-view image datasets, Pascal 3D+ and CUB. We demonstrate that our method
achieves superior 3D textured models compared to the previous works. Please
visit our web-page for 3D visuals.",None,-1
Black-Box Tuning for Language-Model-as-a-Service,0.999964,"Extremely large pre-trained language models (PTMs) such as GPT-3 are usually
released as a service. It allows users to design task-specific prompts to query
the PTMs through some black-box APIs. In such a scenario, which we call
Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually
unavailable. Can we optimize the task prompts by only accessing the model
inference APIs? This paper proposes the black-box tuning framework to optimize
the continuous prompt prepended to the input text via derivative-free
optimization. Instead of optimizing in the original high-dimensional prompt
space, which is intractable for traditional derivative-free optimization, we
perform optimization in a randomly generated subspace due to the low intrinsic
dimensionality of large PTMs. The experimental results show that the black-box
tuning with RoBERTa on a few labeled samples not only significantly outperforms
manual prompt and GPT-3's in-context learning, but also surpasses the
gradient-based counterparts, i.e., prompt tuning and full model tuning.",https://github.com/txsun1997/Black-Box-Tuning,-1
Deep Reinforcement Learning for Stabilization of Large-scale Probabilistic Boolean Networks,0.35901,"The ability to direct a Probabilistic Boolean Network (PBN) to a desired
state is important to applications such as targeted therapeutics in cancer
biology. Reinforcement Learning (RL) has been proposed as a framework that
solves a discrete-time optimal control problem cast as a Markov Decision
Process. We focus on an integrative framework powered by a model-free deep RL
method that can address different flavours of the control problem (e.g., with
or without control inputs; attractor state or a subset of the state space as
the target domain). The method is agnostic to the distribution of probabilities
for the next state, hence it does not use the probability transition matrix.
The time complexity is linear on the time steps, or interactions between the
agent (deep RL) and the environment (PBN), during training. Indeed, we explore
the scalability of the deep RL approach to (set) stabilization of large-scale
PBNs and demonstrate successful control on large networks, including a
metastatic melanoma PBN with 200 nodes.",https://github.com/UoS-PLCCN/pbn-rl/,-1
Reinforcement Learning with Brain-Inspired Modulation can Improve Adaptation to Environmental Changes,0.426493,"Developments in reinforcement learning (RL) have allowed algorithms to
achieve impressive performance in highly complex, but largely static problems.
In contrast, biological learning seems to value efficiency of adaptation to a
constantly-changing world. Here we build on a recently-proposed neuronal
learning rule that assumes each neuron can optimize its energy balance by
predicting its own future activity. That assumption leads to a neuronal
learning rule that uses presynaptic input to modulate prediction error. We
argue that an analogous RL rule would use action probability to modulate reward
prediction error. This modulation makes the agent more sensitive to negative
experiences, and more careful in forming preferences. We embed the proposed
rule in both tabular and deep-Q-network RL algorithms, and find that it
outperforms conventional algorithms in simple, but highly-dynamic tasks. We
suggest that the new rule encapsulates a core principle of biological
intelligence; an important component for allowing algorithms to adapt to change
in a human-like way.",https://github.com/echalmers/modulated_td_error,-1
Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning,0.507345,"Automatically discovering failures in vision models under real-world settings
remains an open challenge. This work demonstrates how off-the-shelf,
large-scale, image-to-text and text-to-image models, trained on vast amounts of
data, can be leveraged to automatically find such failures. In essence, a
conditional text-to-image generative model is used to generate large amounts of
synthetic, yet realistic, inputs given a ground-truth label. Misclassified
inputs are clustered and a captioning model is used to describe each cluster.
Each cluster's description is used in turn to generate more inputs and assess
whether specific clusters induce more failures than expected. We use this
pipeline to demonstrate that we can effectively interrogate classifiers trained
on ImageNet to find specific failure cases and discover spurious correlations.
We also show that we can scale the approach to generate adversarial datasets
targeting specific classifier architectures. This work serves as a
proof-of-concept demonstrating the utility of large-scale generative models to
automatically discover bugs in vision models in an open-ended manner. We also
describe a number of limitations and pitfalls related to this approach.",None,-1
FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations,0.487823,"Recent advances in generative adversarial networks have shown that it is
possible to generate high-resolution and hyperrealistic images. However, the
images produced by GANs are only as fair and representative as the datasets on
which they are trained. In this paper, we propose a method for directly
modifying a pre-trained StyleGAN2 model that can be used to generate a balanced
set of images with respect to one (e.g., eyeglasses) or more attributes (e.g.,
gender and eyeglasses). Our method takes advantage of the style space of the
StyleGAN2 model to perform disentangled control of the target attributes to be
debiased. Our method does not require training additional models and directly
debiases the GAN model, paving the way for its use in various downstream
applications. Our experiments show that our method successfully debiases the
GAN model within a few minutes without compromising the quality of the
generated images. To promote fair generative models, we share the code and
debiased models at http://catlab-team.github.io/fairstyle.",http://catlab-team.github.io/fairstyle,-1
RigoBERTa: A State-of-the-Art Language Model For Spanish,0.050971,"This paper presents RigoBERTa, a State-of-the-Art Language Model for Spanish.
RigoBERTa is trained over a well-curated corpus formed up from different
subcorpora with key features. It follows the DeBERTa architecture, which has
several advantages over other architectures of similar size as BERT or RoBERTa.
RigoBERTa performance is assessed over 13 NLU tasks in comparison with other
available Spanish language models, namely, MarIA, BERTIN and BETO. RigoBERTa
outperformed the three models in 10 out of the 13 tasks, achieving new
""State-of-the-Art"" results.",None,-1
Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment,0.872701,"Training a generative adversarial network (GAN) with limited data has been a
challenging task. A feasible solution is to start with a GAN well-trained on a
large scale source domain and adapt it to the target domain with a few samples,
termed as few shot generative model adaption. However, existing methods are
prone to model overfitting and collapse in extremely few shot setting (less
than 10). To solve this problem, we propose a relaxed spatial structural
alignment method to calibrate the target generative models during the adaption.
We design a cross-domain spatial structural consistency loss comprising the
self-correlation and disturbance correlation consistency loss. It helps align
the spatial structural information between the synthesis image pairs of the
source and target domains. To relax the cross-domain alignment, we compress the
original latent space of generative models to a subspace. Image pairs generated
from the subspace are pulled closer. Qualitative and quantitative experiments
show that our method consistently surpasses the state-of-the-art methods in few
shot setting.",https://github.com/StevenShaw1999/RSSA,18126
A Robust Pedestrian Detection Approach for Autonomous Vehicles,0.0870884,"Nowadays, utilizing Advanced Driver-Assistance Systems (ADAS) has absorbed a
huge interest as a potential solution for reducing road traffic issues. Despite
recent technological advances in such systems, there are still many inquiries
that need to be overcome. For instance, ADAS requires accurate and real-time
detection of pedestrians in various driving scenarios. To solve the mentioned
problem, this paper aims to fine-tune the YOLOv5s framework for handling
pedestrian detection challenges on the real-world instances of Caltech
pedestrian dataset. We also introduce a developed toolbox for preparing
training and test data and annotations of Caltech pedestrian dataset into the
format recognizable by YOLOv5. Experimental results of utilizing our approach
show that the mean Average Precision (mAP) of our fine-tuned model for
pedestrian detection task is more than 91 percent when performing at the
highest rate of 70 FPS. Moreover, the experiments on the Caltech pedestrian
dataset samples have verified that our proposed approach is an effective and
accurate method for pedestrian detection and can outperform other existing
methodologies.",https://github.com/GuilanITS/Caltech-Pedestrian-YOLO,-1
ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval,0.959352,"Neural retrievers based on pre-trained language models (PLMs), such as
dual-encoders, have achieved promising performance on the task of open-domain
question answering (QA). Their effectiveness can further reach new
state-of-the-arts by incorporating cross-architecture knowledge distillation.
However, most of the existing studies just directly apply conventional
distillation methods. They fail to consider the particular situation where the
teacher and student have different structures. In this paper, we propose a
novel distillation method that significantly advances cross-architecture
distillation for dual-encoders. Our method 1) introduces a self on-the-fly
distillation method that can effectively distill late interaction (i.e.,
ColBERT) to vanilla dual-encoder, and 2) incorporates a cascade distillation
process to further improve the performance with a cross-encoder teacher.
Extensive experiments are conducted to validate that our proposed solution
outperforms strong baselines and establish a new state-of-the-art on
open-domain QA benchmarks.",None,15024
Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines,0.615549,"Strong inductive biases give humans the ability to quickly learn to perform a
variety of tasks. Although meta-learning is a method to endow neural networks
with useful inductive biases, agents trained by meta-learning may sometimes
acquire very different strategies from humans. We show that co-training these
agents on predicting representations from natural language task descriptions
and programs induced to generate such tasks guides them toward more human-like
inductive biases. Human-generated language descriptions and program induction
models that add new learned primitives both contain abstract concepts that can
compress description length. Co-training on these representations result in
more human-like behavior in downstream meta-reinforcement learning agents than
less abstract controls (synthetic language descriptions, program induction
without learned primitives), suggesting that the abstraction supported by these
representations is key.",None,-1
Exploring Plain Vision Transformer Backbones for Object Detection,0.999288,"We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone
network for object detection. This design enables the original ViT architecture
to be fine-tuned for object detection without needing to redesign a
hierarchical backbone for pre-training. With minimal adaptations for
fine-tuning, our plain-backbone detector can achieve competitive results.
Surprisingly, we observe: (i) it is sufficient to build a simple feature
pyramid from a single-scale feature map (without the common FPN design) and
(ii) it is sufficient to use window attention (without shifting) aided with
very few cross-window propagation blocks. With plain ViT backbones pre-trained
as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the
previous leading methods that were all based on hierarchical backbones,
reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K
pre-training. We hope our study will draw attention to research on
plain-backbone detectors. Code for ViTDet is available in Detectron2.",https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet,-1
Knowledge Unlearning for Mitigating Privacy Risks in Language Models,0.942763,"Pretrained Language Models (LMs) memorize a vast amount of knowledge during
initial pretraining, including information that may violate the privacy of
personal lives and identities. Previous work addressing privacy issues for
language models has mostly focused on data preprocessing and differential
privacy methods, both requiring re-training the underlying LM. We propose
knowledge unlearning as an alternative method to reduce privacy risks for LMs
post hoc. We show that simply performing gradient ascent on target token
sequences is effective at forgetting them with little to no degradation of
general language modeling performances for larger LMs; it sometimes even
substantially improves the underlying LM with just a few iterations. We also
find that sequential unlearning is better than trying to unlearn all the data
at once and that unlearning is highly dependent on which kind of data (domain)
is forgotten. By showing comparisons with a previous data preprocessing method
and a decoding method known to mitigate privacy risks for LMs, we show that
unlearning can give a stronger empirical privacy guarantee in scenarios where
the data vulnerable to extraction attacks are known a priori while being much
more efficient and robust. We release the code and dataset needed to replicate
our results at https://github.com/joeljang/knowledge-unlearning.",None,-1
CILDA: Contrastive Data Augmentation using Intermediate Layer Knowledge Distillation,0.325531,"Knowledge distillation (KD) is an efficient framework for compressing
large-scale pre-trained language models. Recent years have seen a surge of
research aiming to improve KD by leveraging Contrastive Learning, Intermediate
Layer Distillation, Data Augmentation, and Adversarial Training. In this work,
we propose a learning based data augmentation technique tailored for knowledge
distillation, called CILDA. To the best of our knowledge, this is the first
time that intermediate layer representations of the main task are used in
improving the quality of augmented samples. More precisely, we introduce an
augmentation technique for KD based on intermediate layer matching using
contrastive loss to improve masked adversarial data augmentation. CILDA
outperforms existing state-of-the-art KD approaches on the GLUE benchmark, as
well as in an out-of-domain evaluation.",None,-1
Practical Exposure Correction: Great Truths Are Always Simple,0.672288,"Improving the visual quality of the given degraded observation by correcting
exposure level is a fundamental task in the computer vision community. Existing
works commonly lack adaptability towards unknown scenes because of the
data-driven patterns (deep networks) and limited regularization (traditional
optimization), and they usually need time-consuming inference. These two points
heavily limit their practicability. In this paper, we establish a Practical
Exposure Corrector (PEC) that assembles the characteristics of efficiency and
performance. To be concrete, we rethink the exposure correction to provide a
linear solution with exposure-sensitive compensation. Around generating the
compensation, we introduce an exposure adversarial function as the key engine
to fully extract valuable information from the observation. By applying the
defined function, we construct a segmented shrinkage iterative scheme to
generate the desired compensation. Its shrinkage nature supplies powerful
support for algorithmic stability and robustness. Extensive experimental
evaluations fully reveal the superiority of our proposed PEC. The code is
available at https://rsliu.tech/PEC.",https://rsliu.tech/PEC,11667
Vakyansh: ASR Toolkit for Low Resource Indic languages,0.30804,"We present Vakyansh, an end to end toolkit for Speech Recognition in Indic
languages. India is home to almost 121 languages and around 125 crore speakers.
Yet most of the languages are low resource in terms of data and pretrained
models. Through Vakyansh, we introduce automatic data pipelines for data
creation, model training, model evaluation and deployment. We create 14,000
hours of speech data in 23 Indic languages and train wav2vec 2.0 based
pretrained models. These pretrained models are then finetuned to create state
of the art speech recognition models for 18 Indic languages which are followed
by language models and punctuation restoration models. We open source all these
resources with a mission that this will inspire the speech community to develop
speech first applications using our ASR models in Indic languages.",https://github.com/Open-Speech-EkStep/vakyansh-models,-1
EgoEnv: Human-centric environment representations from egocentric video,0.162704,"First-person video highlights a camera-wearer's activities in the context of
their persistent environment. However, current video understanding approaches
reason over visual features from short video clips that are detached from the
underlying physical space and capture only what is immediately visible. To
facilitate human-centric environment understanding, we present an approach that
links egocentric video and the environment by learning representations that are
predictive of the camera-wearer's (potentially unseen) local surroundings. We
train such models using videos from agents in simulated 3D environments where
the environment is fully observable, and test them on human-captured real-world
videos from unseen environments. On two human-centric video tasks, we show that
models equipped with our environment-aware features consistently outperform
their counterparts with traditional clip features. Moreover, despite being
trained exclusively on simulated videos, our approach successfully handles
real-world videos from HouseTours and Ego4D, and achieves state-of-the-art
results on the Ego4D NLQ challenge. Project page:
https://vision.cs.utexas.edu/projects/ego-env/",https://vision.cs.utexas.edu/projects/ego-env/,-1
A semantic web approach to uplift decentralized household energy data,0.403571,"In a decentralized household energy system comprised of various devices such
as home appliances, electric vehicles, and solar panels, end-users are able to
dig deeper into the system's details and further achieve energy sustainability
if they are presented with data on the electric energy consumption and
production at the granularity of the device. However, many databases in this
field are siloed from other domains, including solely information pertaining to
energy. This may result in the loss of information (e.g. weather) on each
device's energy use. Meanwhile, a large number of these datasets have been
extensively used in computational modeling techniques such as machine learning
models. While such computational approaches achieve great accuracy and
performance by concentrating only on a local view of datasets, model
reliability cannot be guaranteed since such models are very vulnerable to data
input fluctuations when information omission is taken into account. This
article tackles the data isolation issue in the field of smart energy systems
by examining Semantic Web methods on top of a household energy system. We offer
an ontology-based approach for managing decentralized data at the device-level
resolution in a system. As a consequence, the scope of the data associated with
each device may easily be expanded in an interoperable manner throughout the
Web, and additional information, such as weather, can be obtained from the Web,
provided that the data is organized according to W3C standards.",https://github.com/futaoo/semantic-energy,-1
GrASP: Gradient-Based Affordance Selection for Planning,0.0921994,"Planning with a learned model is arguably a key component of intelligence.
There are several challenges in realizing such a component in large-scale
reinforcement learning (RL) problems. One such challenge is dealing effectively
with continuous action spaces when using tree-search planning (e.g., it is not
feasible to consider every action even at just the root node of the tree). In
this paper we present a method for selecting affordances useful for planning --
for learning which small number of actions/options from a continuous space of
actions/options to consider in the tree-expansion process during planning. We
consider affordances that are goal-and-state-conditional mappings to
actions/options as well as unconditional affordances that simply select
actions/options available in all states. Our selection method is gradient
based: we compute gradients through the planning procedure to update the
parameters of the function that represents affordances. Our empirical work
shows that it is feasible to learn to select both primitive-action and option
affordances, and that simultaneously learning to select affordances and
planning with a learned value-equivalent model can outperform model-free RL.",None,-1
Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction,0.999995,"In this paper, we propose an effective yet efficient model PAIE for both
sentence-level and document-level Event Argument Extraction (EAE), which also
generalizes well when there is a lack of training data. On the one hand, PAIE
utilizes prompt tuning for extractive objectives to take the best advantages of
Pre-trained Language Models (PLMs). It introduces two span selectors based on
the prompt to select start/end tokens among input texts for each role. On the
other hand, it captures argument interactions via multi-role prompts and
conducts joint optimization with optimal span assignments via a bipartite
matching loss. Also, with a flexible prompt design, PAIE can extract multiple
arguments with the same role instead of conventional heuristic threshold
tuning. We have conducted extensive experiments on three benchmarks, including
both sentence- and document-level EAE. The results present promising
improvements from PAIE (3.5\% and 2.3\% F1 gains in average on three
benchmarks, for PAIE-base and PAIE-large respectively). Further analysis
demonstrates the efficiency, generalization to few-shot settings, and
effectiveness of different extractive prompt tuning strategies. Our code is
available at https://github.com/mayubo2333/PAIE.",https://github.com/mayubo2333/PAIE,-1
Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection,0.802509,"We study the selection of transfer languages for automatic abusive language
detection. Instead of preparing a dataset for every language, we demonstrate
the effectiveness of cross-lingual transfer learning for zero-shot abusive
language detection. This way we can use existing data from higher-resource
languages to build better detection systems for low-resource languages. Our
datasets are from seven different languages from three language families. We
measure the distance between the languages using several language similarity
measures, especially by quantifying the World Atlas of Language Structures. We
show that there is a correlation between linguistic similarity and classifier
performance. This discovery allows us to choose an optimal transfer language
for zero shot abusive language detection.",None,-1
Towards Involving End-users in Interactive Human-in-the-loop AI Fairness,0.745923,"Ensuring fairness in artificial intelligence (AI) is important to counteract
bias and discrimination in far-reaching applications. Recent work has started
to investigate how humans judge fairness and how to support machine learning
(ML) experts in making their AI models fairer. Drawing inspiration from an
Explainable AI (XAI) approach called \emph{explanatory debugging} used in
interactive machine learning, our work explores designing interpretable and
interactive human-in-the-loop interfaces that allow ordinary end-users without
any technical or domain background to identify potential fairness issues and
possibly fix them in the context of loan decisions. Through workshops with
end-users, we co-designed and implemented a prototype system that allowed
end-users to see why predictions were made, and then to change weights on
features to ""debug"" fairness issues. We evaluated the use of this prototype
system through an online study. To investigate the implications of diverse
human values about fairness around the globe, we also explored how cultural
dimensions might play a role in using this prototype. Our results contribute to
the design of interfaces to allow end-users to be involved in judging and
addressing AI fairness through a human-in-the-loop approach.",None,-1
WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation,0.513385,"Existing few-shot image generation approaches typically employ fusion-based
strategies, either on the image or the feature level, to produce new images.
However, previous approaches struggle to synthesize high-frequency signals with
fine details, deteriorating the synthesis quality. To address this, we propose
WaveGAN, a frequency-aware model for few-shot image generation. Concretely, we
disentangle encoded features into multiple frequency components and perform
low-frequency skip connections to preserve outline and structural information.
Then we alleviate the generator's struggles of synthesizing fine details by
employing high-frequency skip connections, thus providing informative frequency
information to the generator. Moreover, we utilize a frequency L1-loss on the
generated and real images to further impede frequency information loss.
Extensive experiments demonstrate the effectiveness and advancement of our
method on three datasets. Noticeably, we achieve new state-of-the-art with FID
42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822
respectively on Flower, Animal Faces, and VGGFace. GitHub:
https://github.com/kobeshegu/ECCV2022_WaveGAN",https://github.com/kobeshegu/ECCV2022_WaveGAN,2755
A Screen-Shooting Resilient Document Image Watermarking Scheme using Deep Neural Network,0.649139,"With the advent of the screen-reading era, the confidential documents
displayed on the screen can be easily captured by a camera without leaving any
traces. Thus, this paper proposes a novel screen-shooting resilient
watermarking scheme for document image using deep neural network. By applying
this scheme, when the watermarked image is displayed on the screen and captured
by a camera, the watermark can be still extracted from the captured
photographs. Specifically, our scheme is an end-to-end neural network with an
encoder to embed watermark and a decoder to extract watermark. During the
training process, a distortion layer between encoder and decoder is added to
simulate the distortions introduced by screen-shooting process in real scenes,
such as camera distortion, shooting distortion, light source distortion.
Besides, an embedding strength adjustment strategy is designed to improve the
visual quality of the watermarked image with little loss of extraction
accuracy. The experimental results show that the scheme has higher robustness
and visual quality than other three recent state-of-the-arts. Specially, even
if the shooting distances and angles are in extreme, our scheme can also obtain
high extraction accuracy.",https://github.com/gslxr/Screen-Shooting-Resilient-Document-Image-Watermarking,-1
What Do We Maximize in Self-Supervised Learning?,0.313859,"In this paper, we examine self-supervised learning methods, particularly
VICReg, to provide an information-theoretical understanding of their
construction. As a first step, we demonstrate how information-theoretic
quantities can be obtained for a deterministic network, offering a possible
alternative to prior work that relies on stochastic models. This enables us to
demonstrate how VICReg can be (re)discovered from first principles and its
assumptions about data distribution. Furthermore, we empirically demonstrate
the validity of our assumptions, confirming our novel understanding of VICReg.
Finally, we believe that the derivation and insights we obtain can be
generalized to many other SSL methods, opening new avenues for theoretical and
practical understanding of SSL and transfer learning.",None,-1
Pathway to Future Symbiotic Creativity,0.0242748,"This report presents a comprehensive view of our vision on the development
path of the human-machine symbiotic art creation. We propose a classification
of the creative system with a hierarchy of 5 classes, showing the pathway of
creativity evolving from a mimic-human artist (Turing Artists) to a Machine
artist in its own right. We begin with an overview of the limitations of the
Turing Artists then focus on the top two-level systems, Machine Artists,
emphasizing machine-human communication in art creation. In art creation, it is
necessary for machines to understand humans' mental states, including desires,
appreciation, and emotions, humans also need to understand machines' creative
capabilities and limitations. The rapid development of immersive environment
and further evolution into the new concept of metaverse enable symbiotic art
creation through unprecedented flexibility of bi-directional communication
between artists and art manifestation environments. By examining the latest
sensor and XR technologies, we illustrate the novel way for art data collection
to constitute the base of a new form of human-machine bidirectional
communication and understanding in art creation. Based on such communication
and understanding mechanisms, we propose a novel framework for building future
Machine artists, which comes with the philosophy that a human-compatible AI
system should be based on the ""human-in-the-loop"" principle rather than the
traditional ""end-to-end"" dogma. By proposing a new form of inverse
reinforcement learning model, we outline the platform design of machine
artists, demonstrate its functions and showcase some examples of technologies
we have developed. We also provide a systematic exposition of the ecosystem for
AI-based symbiotic art form and community with an economic model built on NFT
technology. Ethical issues for the development of machine artists are also
discussed.",None,-1
Revisiting Neural Scaling Laws in Language and Vision,0.722463,"The remarkable progress in deep learning in recent years is largely driven by
improvements in scale, where bigger models are trained on larger datasets for
longer schedules. To predict the benefit of scale empirically, we argue for a
more rigorous methodology based on the extrapolation loss, instead of reporting
the best-fitting (interpolating) parameters. We then present a recipe for
estimating scaling law parameters reliably from learning curves. We demonstrate
that it extrapolates more accurately than previous methods in a wide range of
architecture families across several domains, including image classification,
neural machine translation (NMT) and language modeling, in addition to tasks
from the BIG-Bench evaluation benchmark. Finally, we release a benchmark
dataset comprising of 90 evaluation tasks to facilitate research in this
domain.",https://github.com/google-research/revisiting_neural_scaling_laws,-1
Pair-Based Joint Encoding with Relational Graph Convolutional Networks for Emotion-Cause Pair Extraction,0.868504,"Emotion-cause pair extraction (ECPE) aims to extract emotion clauses and
corresponding cause clauses, which have recently received growing attention.
Previous methods sequentially encode features with a specified order. They
first encode the emotion and cause features for clause extraction and then
combine them for pair extraction. This lead to an imbalance in inter-task
feature interaction where features extracted later have no direct contact with
the former. To address this issue, we propose a novel Pair-Based Joint Encoding
(PBJE) network, which generates pairs and clauses features simultaneously in a
joint feature encoding manner to model the causal relationship in clauses. PBJE
can balance the information flow among emotion clauses, cause clauses and
pairs. From a multi-relational perspective, we construct a heterogeneous
undirected graph and apply the Relational Graph Convolutional Network (RGCN) to
capture the various relationship between clauses and the relationship between
pairs and clauses. Experimental results show that PBJE achieves
state-of-the-art performance on the Chinese benchmark corpus.",https://github.com/tutuDoki/PBJE-ECPE,-1
"3MASSIV: Multilingual, Multimodal and Multi-Aspect dataset of Social Media Short Videos",0.660262,"We present 3MASSIV, a multilingual, multimodal and multi-aspect,
expertly-annotated dataset of diverse short videos extracted from short-video
social media platform - Moj. 3MASSIV comprises of 50k short videos (20 seconds
average duration) and 100K unlabeled videos in 11 different languages and
captures popular short video trends like pranks, fails, romance, comedy
expressed via unique audio-visual formats like self-shot videos, reaction
videos, lip-synching, self-sung songs, etc. 3MASSIV presents an opportunity for
multimodal and multilingual semantic understanding on these unique videos by
annotating them for concepts, affective states, media types, and audio
language. We present a thorough analysis of 3MASSIV and highlight the variety
and unique aspects of our dataset compared to other contemporary popular
datasets with strong baselines. We also show how the social media content in
3MASSIV is dynamic and temporal in nature, which can be used for semantic
understanding tasks and cross-lingual analysis.",None,-1
MnTTS2: An Open-Source Multi-Speaker Mongolian Text-to-Speech Synthesis Dataset,0.632121,"Text-to-Speech (TTS) synthesis for low-resource languages is an attractive
research issue in academia and industry nowadays. Mongolian is the official
language of the Inner Mongolia Autonomous Region and a representative
low-resource language spoken by over 10 million people worldwide. However,
there is a relative lack of open-source datasets for Mongolian TTS. Therefore,
we make public an open-source multi-speaker Mongolian TTS dataset, named
MnTTS2, for the benefit of related researchers. In this work, we prepare the
transcription from various topics and invite three professional Mongolian
announcers to form a three-speaker TTS dataset, in which each announcer records
10 hours of speeches in Mongolian, resulting 30 hours in total. Furthermore, we
build the baseline system based on the state-of-the-art FastSpeech2 model and
HiFi-GAN vocoder. The experimental results suggest that the constructed MnTTS2
dataset is sufficient to build robust multi-speaker TTS models for real-world
applications. The MnTTS2 dataset, training recipe, and pretrained models are
released at: \url{https://github.com/ssmlkl/MnTTS2}",https://github.com/ssmlkl/MnTTS2,-1
Revisiting Checkpoint Averaging for Neural Machine Translation,0.364203,"Checkpoint averaging is a simple and effective method to boost the
performance of converged neural machine translation models. The calculation is
cheap to perform and the fact that the translation improvement almost comes for
free, makes it widely adopted in neural machine translation research. Despite
the popularity, the method itself simply takes the mean of the model parameters
from several checkpoints, the selection of which is mostly based on empirical
recipes without many justifications. In this work, we revisit the concept of
checkpoint averaging and consider several extensions. Specifically, we
experiment with ideas such as using different checkpoint selection strategies,
calculating weighted average instead of simple mean, making use of gradient
information and fine-tuning the interpolation weights on development data. Our
results confirm the necessity of applying checkpoint averaging for optimal
performance, but also suggest that the landscape between the converged
checkpoints is rather flat and not much further improvement compared to simple
averaging is to be obtained.",https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/get_ende_bleu.sh,-1
Speech Synthesis with Mixed Emotions,0.954974,"Emotional speech synthesis aims to synthesize human voices with various
emotional effects. The current studies are mostly focused on imitating an
averaged style belonging to a specific emotion type. In this paper, we seek to
generate speech with a mixture of emotions at run-time. We propose a novel
formulation that measures the relative difference between the speech samples of
different emotions. We then incorporate our formulation into a
sequence-to-sequence emotional text-to-speech framework. During the training,
the framework does not only explicitly characterize emotion styles, but also
explores the ordinal nature of emotions by quantifying the differences with
other emotions. At run-time, we control the model to produce the desired
emotion mixture by manually defining an emotion attribute vector. The objective
and subjective evaluations have validated the effectiveness of the proposed
framework. To our best knowledge, this research is the first study on
modelling, synthesizing, and evaluating mixed emotions in speech.",https://github.com/chaitanya100100/Relative-Attributes-Zero-Emotions-Demo/,-1
Using Linguistic Typology to Enrich Multilingual Lexicons: the Case of Lexical Gaps in Kinship,0.536722,"This paper describes a method to enrich lexical resources with content
relating to linguistic diversity, based on knowledge from the field of lexical
typology. We capture the phenomenon of diversity through the notions of lexical
gap and language-specific word and use a systematic method to infer gaps
semi-automatically on a large scale. As a first result obtained for the domain
of kinship terminology, known to be very diverse throughout the world, we
publish a lexico-semantic resource consisting of 198 domain concepts, 1,911
words, and 37,370 gaps covering 699 languages. We see potential in the use of
resources such as ours for the improvement of a variety of cross-lingual NLP
tasks, which we demonstrate through a downstream application for the evaluation
of machine translation systems.",https://github.com/kbatsuren/KinDiv,-1
Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval,0.301088,"State-of-the-art neural (re)rankers are notoriously data-hungry which --
given the lack of large-scale training data in languages other than English --
makes them rarely used in multilingual and cross-lingual retrieval settings.
Current approaches therefore commonly transfer rankers trained on English data
to other languages and cross-lingual setups by means of multilingual encoders:
they fine-tune all parameters of pretrained massively multilingual Transformers
(MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy
them in the target language(s). In this work, we show that two
parameter-efficient approaches to cross-lingual transfer, namely Sparse
Fine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more
effective zero-shot transfer to multilingual and cross-lingual retrieval tasks.
We first train language adapters (or SFTMs) via Masked Language Modelling and
then train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping
all other parameters fixed. At inference, this modular design allows us to
compose the ranker by applying the (re)ranking adapter (or SFTM) trained with
source language data together with the language adapter (or SFTM) of a target
language. We carry out a large scale evaluation on the CLEF-2003 and HC4
benchmarks and additionally, as another contribution, extend the former with
queries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed
parameter-efficient methods outperform standard zero-shot transfer with full
MMT fine-tuning, while being more modular and reducing training times. The
gains are particularly pronounced for low-resource languages, where our
approaches also substantially outperform the competitive machine
translation-based rankers.",https://github.com/rlitschk/ModularCLIR,-1
HINT: Hierarchical Neuron Concept Explainer,0.885059,"To interpret deep networks, one main approach is to associate neurons with
human-understandable concepts. However, existing methods often ignore the
inherent relationships of different concepts (e.g., dog and cat both belong to
animals), and thus lose the chance to explain neurons responsible for
higher-level concepts (e.g., animal). In this paper, we study hierarchical
concepts inspired by the hierarchical cognition process of human beings. To
this end, we propose HIerarchical Neuron concepT explainer (HINT) to
effectively build bidirectional associations between neurons and hierarchical
concepts in a low-cost and scalable manner. HINT enables us to systematically
and quantitatively study whether and how the implicit hierarchical
relationships of concepts are embedded into neurons, such as identifying
collaborative neurons responsible to one concept and multimodal neurons for
different concepts, at different semantic levels from concrete concepts (e.g.,
dog) to more abstract ones (e.g., animal). Finally, we verify the faithfulness
of the associations using Weakly Supervised Object Localization, and
demonstrate its applicability in various tasks such as discovering saliency
regions and explaining adversarial attacks. Code is available on
https://github.com/AntonotnaWang/HINT.",https://github.com/AntonotnaWang/HINT,-1
Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task,0.983318,"Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create ""latent saliency maps"" that can help explain predictions
in human terms.",None,-1
Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos,0.998213,"Unsupervised object-centric learning aims to represent the modular,
compositional, and causal structure of a scene as a set of object
representations and thereby promises to resolve many critical limitations of
traditional single-vector representations such as poor systematic
generalization. Although there have been many remarkable advances in recent
years, one of the most critical problems in this direction has been that
previous methods work only with simple and synthetic scenes but not with
complex and naturalistic images or videos. In this paper, we propose STEVE, an
unsupervised model for object-centric learning in videos. Our proposed model
makes a significant advancement by demonstrating its effectiveness on various
complex and naturalistic videos unprecedented in this line of research.
Interestingly, this is achieved by neither adding complexity to the model
architecture nor introducing a new objective or weak supervision. Rather, it is
achieved by a surprisingly simple architecture that uses a transformer-based
image decoder conditioned on slots and the learning objective is simply to
reconstruct the observation. Our experiment results on various complex and
naturalistic videos show significant improvements compared to the previous
state-of-the-art.",https://sites.google.com/view/slot-transformer-for-videos,-1
Generate rather than Retrieve: Large Language Models are Strong Context Generators,0.815401,"Knowledge-intensive tasks, such as open-domain question answering (QA),
require access to a large amount of world or domain knowledge. A common
approach for knowledge-intensive tasks is to employ a retrieve-then-read
pipeline that first retrieves a handful of relevant contextual documents from
an external corpus such as Wikipedia and then predicts an answer conditioned on
the retrieved documents. In this paper, we present a novel perspective for
solving knowledge-intensive tasks by replacing document retrievers with large
language model generators. We call our method generate-then-read (GenRead),
which first prompts a large language model to generate contextutal documents
based on a given question, and then reads the generated documents to produce
the final answer. Furthermore, we propose a novel clustering-based prompting
method that selects distinct prompts, resulting in the generated documents that
cover different perspectives, leading to better recall over acceptable answers.
We conduct extensive experiments on three different knowledge-intensive tasks,
including open-domain QA, fact checking, and dialogue system. Notably, GenRead
achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly
outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0
and +3.9, without retrieving any documents from any external knowledge source.
Lastly, we demonstrate the model performance can be further improved by
combining retrieval and generation. Our code and generated documents can be
found at https://github.com/wyu97/GenRead.",https://github.com/wyu97/GenRead,-1
Fast AdvProp,0.207468,"Adversarial Propagation (AdvProp) is an effective way to improve recognition
models, leveraging adversarial examples. Nonetheless, AdvProp suffers from the
extremely slow training speed, mainly because: a) extra forward and backward
passes are required for generating adversarial examples; b) both original
samples and their adversarial counterparts are used for training (i.e.,
2$\times$ data). In this paper, we introduce Fast AdvProp, which aggressively
revamps AdvProp's costly training components, rendering the method nearly as
cheap as the vanilla training. Specifically, our modifications in Fast AdvProp
are guided by the hypothesis that disentangled learning with adversarial
examples is the key for performance improvements, while other training recipes
(e.g., paired clean and adversarial training samples, multi-step adversarial
attackers) could be largely simplified.
  Our empirical results show that, compared to the vanilla training baseline,
Fast AdvProp is able to further model performance on a spectrum of visual
benchmarks, without incurring extra training cost. Additionally, our ablations
find Fast AdvProp scales better if larger models are used, is compatible with
existing data augmentation methods (i.e., Mixup and CutMix), and can be easily
adapted to other recognition tasks like object detection. The code is available
here: https://github.com/meijieru/fast_advprop.",https://github.com/meijieru/fast_advprop,-1
Deep Learning based Automatic Detection of Dicentric Chromosome,0.547919,"Automatic detection of dicentric chromosomes is an essential step to estimate
radiation exposure and development of end to end emergency bio dosimetry
systems. During accidents, a large amount of data is required to be processed
for extensive testing to formulate a medical treatment plan for the masses,
which requires this process to be automated. Current approaches require human
adjustments according to the data and therefore need a human expert to
calibrate the system. This paper proposes a completely data driven framework
which requires minimum intervention of field experts and can be deployed in
emergency cases with relative ease. Our approach involves YOLOv4 to detect the
chromosomes and remove the debris in each image, followed by a classifier that
differentiates between an analysable chromosome and a non-analysable one.
Images are extracted from YOLOv4 based on the protocols described by
WHO-BIODOSNET. The analysable chromosome is classified as Monocentric or
Dicentric and an image is accepted for consideration of dose estimation based
on the analysable chromosome count. We report an accuracy in dicentric
identification of 94.33% on a 1:1 split of Dicentric and Monocentric
Chromosomes.",None,-1
An End-to-End Transformer Model for Crowd Localization,0.619025,"Crowd localization, predicting head positions, is a more practical and
high-level task than simply counting. Existing methods employ pseudo-bounding
boxes or pre-designed localization maps, relying on complex post-processing to
obtain the head positions. In this paper, we propose an elegant, end-to-end
Crowd Localization Transformer named CLTR that solves the task in the
regression-based paradigm. The proposed method views the crowd localization as
a direct set prediction problem, taking extracted features and trainable
embeddings as input of the transformer-decoder. To reduce the ambiguous points
and generate more reasonable matching results, we introduce a KMO-based
Hungarian matcher, which adopts the nearby context as the auxiliary matching
cost. Extensive experiments conducted on five datasets in various data settings
show the effectiveness of our method. In particular, the proposed method
achieves the best localization performance on the NWPU-Crowd, UCF-QNRF, and
ShanghaiTech Part A datasets.",https://dk-liang.github.io/CLTR/,-1
Are AlphaZero-like Agents Robust to Adversarial Perturbations?,0.519085,"The success of AlphaZero (AZ) has demonstrated that neural-network-based Go
AIs can surpass human performance by a large margin. Given that the state space
of Go is extremely large and a human player can play the game from any legal
state, we ask whether adversarial states exist for Go AIs that may lead them to
play surprisingly wrong actions. In this paper, we first extend the concept of
adversarial examples to the game of Go: we generate perturbed states that are
``semantically'' equivalent to the original state by adding meaningless moves
to the game, and an adversarial state is a perturbed state leading to an
undoubtedly inferior action that is obvious even for Go beginners. However,
searching the adversarial state is challenging due to the large, discrete, and
non-differentiable search space. To tackle this challenge, we develop the first
adversarial attack on Go AIs that can efficiently search for adversarial states
by strategically reducing the search space. This method can also be extended to
other board games such as NoGo. Experimentally, we show that the actions taken
by both Policy-Value neural network (PV-NN) and Monte Carlo tree search (MCTS)
can be misled by adding one or two meaningless stones; for example, on 58\% of
the AlphaGo Zero self-play games, our method can make the widely used KataGo
agent with 50 simulations of MCTS plays a losing action by adding two
meaningless stones. We additionally evaluated the adversarial examples found by
our algorithm with amateur human Go players and 90\% of examples indeed lead
the Go agent to play an obviously inferior action. Our code is available at
\url{https://PaperCode.cc/GoAttack}.",https://PaperCode.cc/GoAttack,-1
BoundaryFace: A mining framework with noise label self-correction for Face Recognition,0.359518,"Face recognition has made tremendous progress in recent years due to the
advances in loss functions and the explosive growth in training sets size. A
properly designed loss is seen as key to extract discriminative features for
classification. Several margin-based losses have been proposed as alternatives
of softmax loss in face recognition. However, two issues remain to consider: 1)
They overlook the importance of hard sample mining for discriminative learning.
2) Label noise ubiquitously exists in large-scale datasets, which can seriously
damage the model's performance. In this paper, starting from the perspective of
decision boundary, we propose a novel mining framework that focuses on the
relationship between a sample's ground truth class center and its nearest
negative class center. Specifically, a closed-set noise label self-correction
module is put forward, making this framework work well on datasets containing a
lot of label noise. The proposed method consistently outperforms SOTA methods
in various face recognition benchmarks. Training code has been released at
https://github.com/SWJTU-3DVision/BoundaryFace.",https://github.com/SWJTU-3DVision/BoundaryFace,2424
Egocentric Human-Object Interaction Detection Exploiting Synthetic Data,0.762107,"We consider the problem of detecting Egocentric HumanObject Interactions
(EHOIs) in industrial contexts. Since collecting and labeling large amounts of
real images is challenging, we propose a pipeline and a tool to generate
photo-realistic synthetic First Person Vision (FPV) images automatically
labeled for EHOI detection in a specific industrial scenario. To tackle the
problem of EHOI detection, we propose a method that detects the hands, the
objects in the scene, and determines which objects are currently involved in an
interaction. We compare the performance of our method with a set of
state-of-the-art baselines. Results show that using a synthetic dataset
improves the performance of an EHOI detection system, especially when few real
data are available. To encourage research on this topic, we publicly release
the proposed dataset at the following url:
https://iplab.dmi.unict.it/EHOI_SYNTH/.",https://github.com/cocodataset/cocoapi,-1
Adversarial Examples for Good: Adversarial Examples Guided Imbalanced Learning,0.345781,"Adversarial examples are inputs for machine learning models that have been
designed by attackers to cause the model to make mistakes. In this paper, we
demonstrate that adversarial examples can also be utilized for good to improve
the performance of imbalanced learning. We provide a new perspective on how to
deal with imbalanced data: adjust the biased decision boundary by training with
Guiding Adversarial Examples (GAEs). Our method can effectively increase the
accuracy of minority classes while sacrificing little accuracy on majority
classes. We empirically show, on several benchmark datasets, our proposed
method is comparable to the state-of-the-art method. To our best knowledge, we
are the first to deal with imbalanced learning with adversarial examples.",None,-1
Hypergraph Convolutional Networks for Weakly-Supervised Semantic Segmentation,0.313599,"Semantic segmentation is a fundamental topic in computer vision. Several deep
learning methods have been proposed for semantic segmentation with outstanding
results. However, these models require a lot of densely annotated images. To
address this problem, we propose a new algorithm that uses HyperGraph
Convolutional Networks for Weakly-supervised Semantic Segmentation
(HyperGCN-WSS). Our algorithm constructs spatial and k-Nearest Neighbor (k-NN)
graphs from the images in the dataset to generate the hypergraphs. Then, we
train a specialized HyperGraph Convolutional Network (HyperGCN) architecture
using some weak signals. The outputs of the HyperGCN are denominated
pseudo-labels, which are later used to train a DeepLab model for semantic
segmentation. HyperGCN-WSS is evaluated on the PASCAL VOC 2012 dataset for
semantic segmentation, using scribbles or clicks as weak signals. Our algorithm
shows competitive performance against previous methods.",None,9488
Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,0.999957,"Prompting-based large language models (LLMs) are surprisingly powerful at
generating natural language reasoning steps or Chains-of-Thoughts (CoT) for
multi-step question answering (QA). They struggle, however, when the necessary
knowledge is either unavailable to the LLM or not up-to-date within its
parameters. While using the question to retrieve relevant text from an external
knowledge source helps LLMs, we observe that this one-step retrieve-and-read
approach is insufficient for multi-step QA. Here, \textit{what to retrieve}
depends on \textit{what has already been derived}, which in turn may depend on
\textit{what was previously retrieved}. To address this, we propose IRCoT, a
new approach for multi-step QA that interleaves retrieval with steps
(sentences) in a CoT, guiding the retrieval with CoT and in turn using
retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves
retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four
datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar
substantial gains in out-of-distribution (OOD) settings as well as with much
smaller models such as Flan-T5-large without additional training. IRCoT reduces
model hallucination, resulting in factually more accurate CoT reasoning. Code,
data, and prompts are available at \url{https://github.com/stonybrooknlp/ircot}",https://github.com/stonybrooknlp/ircot,-1
DDH-QA: A Dynamic Digital Humans Quality Assessment Database,0.89228,"In recent years, large amounts of effort have been put into pushing forward
the real-world application of dynamic digital human (DDH). However, most
current quality assessment research focuses on evaluating static 3D models and
usually ignores motion distortions. Therefore, in this paper, we construct a
large-scale dynamic digital human quality assessment (DDH-QA) database with
diverse motion content as well as multiple distortions to comprehensively study
the perceptual quality of DDHs. Both model-based distortion (noise,
compression) and motion-based distortion (binding error, motion unnaturalness)
are taken into consideration. Ten types of common motion are employed to drive
the DDHs and a total of 800 DDHs are generated in the end. Afterward, we render
the video sequences of the distorted DDHs as the evaluation media and carry out
a well-controlled subjective experiment. Then a benchmark experiment is
conducted with the state-of-the-art video quality assessment (VQA) methods and
the experimental results show that existing VQA methods are limited in
assessing the perceptual loss of DDHs.",https://github.com/zzc-1998/DDH-QA,-1
Huqariq: A Multilingual Speech Corpus of Native Languages of Peru for Speech Recognition,0.203585,"The Huqariq corpus is a multilingual collection of speech from native
Peruvian languages. The transcribed corpus is intended for the research and
development of speech technologies to preserve endangered languages in Peru.
Huqariq is primarily designed for the development of automatic speech
recognition, language identification and text-to-speech tools. In order to
achieve corpus collection sustainably, we employ the crowdsourcing methodology.
Huqariq includes four native languages of Peru, and it is expected that by the
end of the year 2022, it can reach up to 20 native languages out of the 48
native languages in Peru. The corpus has 220 hours of transcribed audio
recorded by more than 500 volunteers, making it the largest speech corpus for
native languages in Peru. In order to verify the quality of the corpus, we
present speech recognition experiments using 220 hours of fully transcribed
audio.",None,-1
READ: Large-Scale Neural Scene Rendering for Autonomous Driving,0.626653,"Synthesizing free-view photo-realistic images is an important task in
multimedia. With the development of advanced driver assistance systems~(ADAS)
and their applications in autonomous vehicles, experimenting with different
scenarios becomes a challenge. Although the photo-realistic street scenes can
be synthesized by image-to-image translation methods, which cannot produce
coherent scenes due to the lack of 3D information. In this paper, a large-scale
neural rendering method is proposed to synthesize the autonomous driving
scene~(READ), which makes it possible to synthesize large-scale driving
scenarios on a PC through a variety of sampling schemes. In order to represent
driving scenarios, we propose an {\omega} rendering network to learn neural
descriptors from sparse point clouds. Our model can not only synthesize
realistic driving scenes but also stitch and edit driving scenes. Experiments
show that our model performs well in large-scale driving scenarios.",https://github.com/JOP-Lee/READ-Large-Scale-Neural-,-1
Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior,0.237442,"Inverse rendering is an ill-posed problem. Previous work has sought to
resolve this by focussing on priors for object or scene shape or appearance. In
this work, we instead focus on a prior for natural illuminations. Current
methods rely on spherical harmonic lighting or other generic representations
and, at best, a simplistic prior on the parameters. We propose a conditional
neural field representation based on a variational auto-decoder with a SIREN
network and, extending Vector Neurons, build equivariance directly into the
network. Using this, we develop a rotation-equivariant, high dynamic range
(HDR) neural illumination model that is compact and able to express complex,
high-frequency features of natural environment maps. Training our model on a
curated dataset of 1.6K HDR environment maps of natural scenes, we compare it
against traditional representations, demonstrate its applicability for an
inverse rendering task and show environment map completion from partial
observations. A PyTorch implementation, our dataset and trained models can be
found at jadgardner.github.io/RENI.",jadgardner.github.io/RENI,-1
Multiclass ASMA vs Targeted PGD Attack in Image Segmentation,0.143085,"Deep learning networks have demonstrated high performance in a large variety
of applications, such as image classification, speech recognition, and natural
language processing. However, there exists a major vulnerability exploited by
the use of adversarial attacks. An adversarial attack imputes images by
altering the input image very slightly, making it nearly undetectable to the
naked eye, but results in a very different classification by the network. This
paper explores the projected gradient descent (PGD) attack and the Adaptive
Mask Segmentation Attack (ASMA) on the image segmentation DeepLabV3 model using
two types of architectures: MobileNetV3 and ResNet50, It was found that PGD was
very consistent in changing the segmentation to be its target while the
generalization of ASMA to a multiclass target was not as effective. The
existence of such attack however puts all of image classification deep learning
networks in danger of exploitation.",None,-1
Depth Perspective-aware Multiple Object Tracking,0.325745,"This paper aims to tackle Multiple Object Tracking (MOT), an important
problem in computer vision but remains challenging due to many practical
issues, especially occlusions. Indeed, we propose a new real-time Depth
Perspective-aware Multiple Object Tracking (DP-MOT) approach to tackle the
occlusion problem in MOT. A simple yet efficient Subject-Ordered Depth
Estimation (SODE) is first proposed to automatically order the depth positions
of detected subjects in a 2D scene in an unsupervised manner. Using the output
from SODE, a new Active pseudo-3D Kalman filter, a simple but effective
extension of Kalman filter with dynamic control variables, is then proposed to
dynamically update the movement of objects. In addition, a new high-order
association approach is presented in the data association step to incorporate
first-order and second-order relationships between the detected objects. The
proposed approach consistently achieves state-of-the-art performance compared
to recent MOT methods on standard MOT benchmarks.",None,-1
A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles,0.868415,"Modern vehicles, including autonomous vehicles and connected vehicles, are
increasingly connected to the external world, which enables various
functionalities and services. However, the improving connectivity also
increases the attack surfaces of the Internet of Vehicles (IoV), causing its
vulnerabilities to cyber-threats. Due to the lack of authentication and
encryption procedures in vehicular networks, Intrusion Detection Systems (IDSs)
are essential approaches to protect modern vehicle systems from network
attacks. In this paper, a transfer learning and ensemble learning-based IDS is
proposed for IoV systems using convolutional neural networks (CNNs) and
hyper-parameter optimization techniques. In the experiments, the proposed IDS
has demonstrated over 99.25% detection rates and F1-scores on two well-known
public benchmark IoV security datasets: the Car-Hacking dataset and the
CICIDS2017 dataset. This shows the effectiveness of the proposed IDS for
cyber-attack detection in both intra-vehicle and external vehicular networks.",https://github.com/Western-OC2-Lab/Intrusion-,-1
Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors,0.786445,"Video game testing requires game-specific knowledge as well as common sense
reasoning about the events in the game. While AI-driven agents can satisfy the
first requirement, it is not yet possible to meet the second requirement
automatically. Therefore, video game testing often still relies on manual
testing, and human testers are required to play the game thoroughly to detect
bugs. As a result, it is challenging to fully automate game testing. In this
study, we explore the possibility of leveraging the zero-shot capabilities of
large language models for video game bug detection. By formulating the bug
detection problem as a question-answering task, we show that large language
models can identify which event is buggy in a sequence of textual descriptions
of events from a game. To this end, we introduce the GameBugDescriptions
benchmark dataset, which consists of 167 buggy gameplay videos and a total of
334 question-answer pairs across 8 games. We extensively evaluate the
performance of six models across the OPT and InstructGPT large language model
families on our benchmark dataset. Our results show promising results for
employing language models to detect video game bugs. With the proper prompting
technique, we could achieve an accuracy of 70.66%, and on some video games, up
to 78.94%. Our code, evaluation data and the benchmark can be found on
https://asgaardlab.github.io/LLMxBugs",https://github.com/facebookresearch/metaseq/,-1
Dataset Condensation with Latent Space Knowledge Factorization and Sharing,0.299334,"In this paper, we introduce a novel approach for systematically solving
dataset condensation problem in an efficient manner by exploiting the
regularity in a given dataset. Instead of condensing the dataset directly in
the original input space, we assume a generative process of the dataset with a
set of learnable codes defined in a compact latent space followed by a set of
tiny decoders which maps them differently to the original input space. By
combining different codes and decoders interchangeably, we can dramatically
increase the number of synthetic examples with essentially the same parameter
count, because the latent space is much lower dimensional and since we can
assume as many decoders as necessary to capture different styles represented in
the dataset with negligible cost. Such knowledge factorization allows efficient
sharing of information between synthetic examples in a systematic way,
providing far better trade-off between compression ratio and quality of the
generated examples. We experimentally show that our method achieves new
state-of-the-art records by significant margins on various benchmark datasets
such as SVHN, CIFAR10, CIFAR100, and TinyImageNet.",None,-1
Learning to segment from object sizes,0.0655667,"Deep learning has proved particularly useful for semantic segmentation, a
fundamental image analysis task. However, the standard deep learning methods
need many training images with ground-truth pixel-wise annotations, which are
usually laborious to obtain and, in some cases (e.g., medical images), require
domain expertise. Therefore, instead of pixel-wise annotations, we focus on
image annotations that are significantly easier to acquire but still
informative, namely the size of foreground objects. We define the object size
as the maximum Chebyshev distance between a foreground and the nearest
background pixel. We propose an algorithm for training a deep segmentation
network from a dataset of a few pixel-wise annotated images and many images
with known object sizes. The algorithm minimizes a discrete
(non-differentiable) loss function defined over the object sizes by sampling
the gradient and then using the standard back-propagation algorithm.
Experiments show that the new approach improves the segmentation performance.",https://github.com/barucden/chdt,-1
UBARv2: Towards Mitigating Exposure Bias in Task-Oriented Dialogs,0.142765,"This paper studies the exposure bias problem in task-oriented dialog systems,
where the model's generated content over multiple turns drives the dialog
context away from the ground-truth distribution at training time, introducing
error propagation and damaging the robustness of the TOD system. To bridge the
gap between training and inference for multi-turn task-oriented dialogs, we
propose session-level sampling which explicitly exposes the model to sampled
generated content of dialog context during training. Additionally, we employ a
dropout-based consistency regularization with the masking strategy R-Mask to
further improve the robustness and performance of the model. The proposed
UBARv2 achieves state-of-the-art performance on the standardized evaluation
benchmark MultiWOZ and extensive experiments show the effectiveness of the
proposed methods.",None,-1
DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing,0.439695,"The meaningful use of electronic health records (EHR) continues to progress
in the digital era with clinical decision support systems augmented by
artificial intelligence. A priority in improving provider experience is to
overcome information overload and reduce the cognitive burden so fewer medical
errors and cognitive biases are introduced during patient care. One major type
of medical error is diagnostic error due to systematic or predictable errors in
judgment that rely on heuristics. The potential for clinical natural language
processing (cNLP) to model diagnostic reasoning in humans with forward
reasoning from data to diagnosis and potentially reduce the cognitive burden
and medical error has not been investigated. Existing tasks to advance the
science in cNLP have largely focused on information extraction and named entity
recognition through classification tasks. We introduce a novel suite of tasks
coined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for
developing and evaluating cNLP models with clinical diagnostic reasoning
ability. The suite includes six tasks from ten publicly available datasets
addressing clinical text understanding, medical knowledge reasoning, and
diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to
be a natural language generation framework to evaluate pre-trained language
models. Experiments with state-of-the-art pre-trained generative language
models using large general domain models and models that were continually
trained on a medical corpus demonstrate opportunities for improvement when
evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab
repository with a systematic approach to load and evaluate models for the cNLP
community.",https://git.doit.wisc.edu/smph-public/dom/uw-icu-data-scienc/drbench,-1
Promises and Pitfalls of Threshold-based Auto-labeling,0.538742,"Creating large-scale high-quality labeled datasets is a major bottleneck in
supervised machine learning workflows. Threshold-based auto-labeling (TBAL),
where validation data obtained from humans is used to find a confidence
threshold above which the data is machine-labeled, reduces reliance on manual
annotation. TBAL is emerging as a widely-used solution in practice. Given the
long shelf-life and diverse usage of the resulting datasets, understanding when
the data obtained by such auto-labeling systems can be relied on is crucial.
This is the first work to analyze TBAL systems and derive sample complexity
bounds on the amount of human-labeled validation data required for guaranteeing
the quality of machine-labeled data. Our results provide two crucial insights.
First, reasonable chunks of unlabeled data can be automatically and accurately
labeled by seemingly bad models. Second, a hidden downside of TBAL systems is
potentially prohibitive validation data usage. Together, these insights
describe the promise and pitfalls of using such systems. We validate our
theoretical guarantees with extensive experiments on synthetic and real
datasets.",https://github.com/harit7/TBAL,-1
Asking the Right Questions in Low Resource Template Extraction,0.41027,"Information Extraction (IE) researchers are mapping tasks to Question
Answering (QA) in order to leverage existing large QA resources, and thereby
improve data efficiency. Especially in template extraction (TE), mapping an
ontology to a set of questions can be more time-efficient than collecting
labeled examples. We ask whether end users of TE systems can design these
questions, and whether it is beneficial to involve an NLP practitioner in the
process. We compare questions to other ways of phrasing natural language
prompts for TE. We propose a novel model to perform TE with prompts, and find
it benefits from questions over other styles of prompts, and that they do not
require an NLP background to author.",None,-1
Privacy-preserving Generative Framework Against Membership Inference Attacks,0.279113,"Artificial intelligence and machine learning have been integrated into all
aspects of our lives and the privacy of personal data has attracted more and
more attention. Since the generation of the model needs to extract the
effective information of the training data, the model has the risk of leaking
the privacy of the training data. Membership inference attacks can measure the
model leakage of source data to a certain degree. In this paper, we design a
privacy-preserving generative framework against membership inference attacks,
through the information extraction and data generation capabilities of the
generative model variational autoencoder (VAE) to generate synthetic data that
meets the needs of differential privacy. Instead of adding noise to the model
output or tampering with the training process of the target model, we directly
process the original data. We first map the source data to the latent space
through the VAE model to get the latent code, then perform noise process
satisfying metric privacy on the latent code, and finally use the VAE model to
reconstruct the synthetic data. Our experimental evaluation demonstrates that
the machine learning model trained with newly generated synthetic data can
effectively resist membership inference attacks and still maintain high
utility.",None,-1
The future is different: Large pre-trained language models fail in prediction tasks,0.0538775,"Large pre-trained language models (LPLM) have shown spectacular success when
fine-tuned on downstream supervised tasks. Yet, it is known that their
performance can drastically drop when there is a distribution shift between the
data used during training and that used at inference time. In this paper we
focus on data distributions that naturally change over time and introduce four
new REDDIT datasets, namely the WALLSTREETBETS, ASKSCIENCE, THE DONALD, and
POLITICS sub-reddits. First, we empirically demonstrate that LPLM can display
average performance drops of about 88% (in the best case!) when predicting the
popularity of future posts from sub-reddits whose topic distribution changes
with time. We then introduce a simple methodology that leverages neural
variational dynamic topic models and attention mechanisms to infer temporal
language model representations for regression tasks. Our models display
performance drops of only about 40% in the worst cases (2% in the best ones)
when predicting the popularity of future posts, while using only about 7% of
the total number of parameters of LPLM and providing interpretable
representations that offer insight into real-world events, like the GameStop
short squeeze of 2021",None,-1
Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models,0.580953,"We investigate what kind of structural knowledge learned in neural network
encoders is transferable to processing natural language. We design artificial
languages with structural properties that mimic natural language, pretrain
encoders on the data, and see how much performance the encoder exhibits on
downstream tasks in natural language. Our experimental results show that
pretraining with an artificial language with a nesting dependency structure
provides some knowledge transferable to natural language. A follow-up probing
analysis indicates that its success in the transfer is related to the amount of
encoded contextual information and what is transferred is the knowledge of
position-aware context dependence of language. Our results provide insights
into how neural network encoders process human languages and the source of
cross-lingual transferability of recent multilingual language models.",https://github.com/moses-smt/mosesdecoder,-1
Towards Neural Numeric-To-Text Generation From Temporal Personal Health Data,0.0606951,"With an increased interest in the production of personal health technologies
designed to track user data (e.g., nutrient intake, step counts), there is now
more opportunity than ever to surface meaningful behavioral insights to
everyday users in the form of natural language. This knowledge can increase
their behavioral awareness and allow them to take action to meet their health
goals. It can also bridge the gap between the vast collection of personal
health data and the summary generation required to describe an individual's
behavioral tendencies. Previous work has focused on rule-based time-series data
summarization methods designed to generate natural language summaries of
interesting patterns found within temporal personal health data. We examine
recurrent, convolutional, and Transformer-based encoder-decoder models to
automatically generate natural language summaries from numeric temporal
personal health data. We showcase the effectiveness of our models on real user
health data logged in MyFitnessPal and show that we can automatically generate
high-quality natural language summaries. Our work serves as a first step
towards the ambitious goal of automatically generating novel and meaningful
temporal summaries from personal health data.",https://github.com/neato47/Neural-Numeric-To-Text-Generation,-1
Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training,0.193468,"Keyphrase generation is the task of automatically predicting keyphrases given
a piece of long text. Despite its recent flourishing, keyphrase generation on
non-English languages haven't been vastly investigated. In this paper, we call
attention to a new setting named multilingual keyphrase generation and we
contribute two new datasets, EcommerceMKP and AcademicMKP, covering six
languages. Technically, we propose a retrieval-augmented method for
multilingual keyphrase generation to mitigate the data shortage problem in
non-English languages. The retrieval-augmented model leverages keyphrase
annotations in English datasets to facilitate generating keyphrases in
low-resource languages. Given a non-English passage, a cross-lingual dense
passage retrieval module finds relevant English passages. Then the associated
English keyphrases serve as external knowledge for keyphrase generation in the
current language. Moreover, we develop a retriever-generator iterative training
algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual
passage retriever. Comprehensive experiments and ablations show that the
proposed approach outperforms all baselines.",None,-1
DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder,0.269034,"Recently most successful image synthesis models are multi stage process to
combine the advantages of different methods, which always includes a VAE-like
model for faithfully reconstructing embedding to image and a prior model to
generate image embedding. At the same time, diffusion models have shown be
capacity to generate high-quality synthetic images. Our work proposes a VQ-VAE
architecture model with a diffusion decoder (DiVAE) to work as the
reconstructing component in image synthesis. We explore how to input image
embedding into diffusion model for excellent performance and find that simple
modification on diffusion's UNet can achieve it. Training on ImageNet, Our
model achieves state-of-the-art results and generates more photorealistic
images specifically. In addition, we apply the DiVAE with an Auto-regressive
generator on conditional synthesis tasks to perform more human-feeling and
detailed samples.",None,-1
PIZZA: A Powerful Image-only Zero-Shot Zero-CAD Approach to 6 DoF Tracking,0.190899,"Estimating the relative pose of a new object without prior knowledge is a
hard problem, while it is an ability very much needed in robotics and Augmented
Reality. We present a method for tracking the 6D motion of objects in RGB video
sequences when neither the training images nor the 3D geometry of the objects
are available. In contrast to previous works, our method can therefore consider
unknown objects in open world instantly, without requiring any prior
information or a specific training phase. We consider two architectures, one
based on two frames, and the other relying on a Transformer Encoder, which can
exploit an arbitrary number of past frames. We train our architectures using
only synthetic renderings with domain randomization. Our results on challenging
datasets are on par with previous works that require much more information
(training images of the target objects, 3D models, and/or depth data). Our
source code is available at https://github.com/nv-nguyen/pizza",https://github.com/nv-nguyen/pizza,-1
Coarse-to-Fine Sparse Sequential Recommendation,0.44761,"Sequential recommendation aims to model dynamic user behavior from historical
interactions. Self-attentive methods have proven effective at capturing
short-term dynamics and long-term preferences. Despite their success, these
approaches still struggle to model sparse data, on which they struggle to learn
high-quality item representations. We propose to model user dynamics from
shopping intents and interacted items simultaneously. The learned intents are
coarse-grained and work as prior knowledge for item recommendation. To this
end, we present a coarse-to-fine self-attention framework, namely CaFe, which
explicitly learns coarse-grained and fine-grained sequential dynamics.
Specifically, CaFe first learns intents from coarse-grained sequences which are
dense and hence provide high-quality user intent representations. Then, CaFe
fuses intent representations into item encoder outputs to obtain improved item
representations. Finally, we infer recommended items based on representations
of items and corresponding intents. Experiments on sparse datasets show that
CaFe outperforms state-of-the-art self-attentive recommenders by 44.03% NDCG@5
on average.",None,104016
Model-Free Opponent Shaping,0.585135,"In general-sum games, the interaction of self-interested learning agents
commonly leads to collectively worst-case outcomes, such as defect-defect in
the iterated prisoner's dilemma (IPD). To overcome this, some methods, such as
Learning with Opponent-Learning Awareness (LOLA), shape their opponents'
learning process. However, these methods are myopic since only a small number
of steps can be anticipated, are asymmetric since they treat other agents as
naive learners, and require the use of higher-order derivatives, which are
calculated through white-box access to an opponent's differentiable learning
algorithm. To address these issues, we propose Model-Free Opponent Shaping
(M-FOS). M-FOS learns in a meta-game in which each meta-step is an episode of
the underlying inner game. The meta-state consists of the inner policies, and
the meta-policy produces a new inner policy to be used in the next episode.
M-FOS then uses generic model-free optimisation methods to learn meta-policies
that accomplish long-horizon opponent shaping. Empirically, M-FOS
near-optimally exploits naive learners and other, more sophisticated algorithms
from the literature. For example, to the best of our knowledge, it is the first
method to learn the well-known Zero-Determinant (ZD) extortion strategy in the
IPD. In the same settings, M-FOS leads to socially optimal outcomes under
meta-self-play. Finally, we show that M-FOS can be scaled to high-dimensional
settings.",https://github.com/luchris429/Model-Free-Opponent-Shaping,-1
Improving Neural Machine Translation by Denoising Training,0.245912,"We present a simple and effective pretraining strategy {D}en{o}ising
{T}raining DoT for neural machine translation. Specifically, we update the
model parameters with source- and target-side denoising tasks at the early
stage and then tune the model normally. Notably, our approach does not increase
any parameters or training steps, requiring the parallel data merely.
Experiments show that DoT consistently improves the neural machine translation
performance across 12 bilingual and 16 multilingual directions (data size
ranges from 80K to 20M). In addition, we show that DoT can complement existing
data manipulation strategies, i.e. curriculum learning, knowledge distillation,
data diversification, bidirectional training, and back-translation.
Encouragingly, we found that DoT outperforms costly pretrained model mBART in
high-resource settings. Analyses show DoT is a novel in-domain cross-lingual
pretraining strategy and could offer further improvements with task-relevant
self-supervisions.",https://github.com/pytorch/fairseq/,-1
Abstraction not Memory: BERT and the English Article System,0.236813,"Article prediction is a task that has long defied accurate linguistic
description. As such, this task is ideally suited to evaluate models on their
ability to emulate native-speaker intuition. To this end, we compare the
performance of native English speakers and pre-trained models on the task of
article prediction set up as a three way choice (a/an, the, zero). Our
experiments with BERT show that BERT outperforms humans on this task across all
articles. In particular, BERT is far superior to humans at detecting the zero
article, possibly because we insert them using rules that the deep neural model
can easily pick up. More interestingly, we find that BERT tends to agree more
with annotators than with the corpus when inter-annotator agreement is high but
switches to agreeing more with the corpus as inter-annotator agreement drops.
We contend that this alignment with annotators, despite being trained on the
corpus, suggests that BERT is not memorising article use, but captures a high
level generalisation of article use akin to human intuition.",https://github.com/H-TayyarMadabushi/Abstraction-not-Memory-BERT-and-the-English-Article-System-NAACL-2022,-1
Controllable Style Transfer via Test-time Training of Implicit Neural Representation,0.169708,"We propose a controllable style transfer framework based on Implicit Neural
Representation that pixel-wisely controls the stylized output via test-time
training. Unlike traditional image optimization methods that often suffer from
unstable convergence and learning-based methods that require intensive training
and have limited generalization ability, we present a model optimization
framework that optimizes the neural networks during test-time with explicit
loss functions for style transfer. After being test-time trained once, thanks
to the flexibility of the INR-based model, our framework can precisely control
the stylized images in a pixel-wise manner and freely adjust image resolution
without further optimization or training. We demonstrate several applications.",https://KU-CVLAB.github.io/INR-st/,-1
What do we Really Know about State of the Art NER?,0.824037,"Named Entity Recognition (NER) is a well researched NLP task and is widely
used in real world NLP scenarios. NER research typically focuses on the
creation of new ways of training NER, with relatively less emphasis on
resources and evaluation. Further, state of the art (SOTA) NER models, trained
on standard datasets, typically report only a single performance measure
(F-score) and we don't really know how well they do for different entity types
and genres of text, or how robust are they to new, unseen entities. In this
paper, we perform a broad evaluation of NER using a popular dataset, that takes
into consideration various text genres and sources constituting the dataset at
hand. Additionally, we generate six new adversarial test sets through small
perturbations in the original test set, replacing select entities while
retaining the context. We also train and test our models on randomly generated
train/dev/test splits followed by an experiment where the models are trained on
a select set of genres but tested genres not seen in training. These
comprehensive evaluation strategies were performed using three SOTA NER models.
Based on our results, we recommend some useful reporting practices for NER
researchers, that could help in providing a better understanding of a SOTA
model's performance in future.",https://github.com/nishkalavallabhi/SOTANER/,-1
Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents,0.402222,"We argue that disentangling content selection from the budget used to cover
salient content improves the performance and applicability of abstractive
summarizers. Our method, FactorSum, does this disentanglement by factorizing
summarization into two steps through an energy function: (1) generation of
abstractive summary views; (2) combination of these views into a final summary,
following a budget and content guidance. This guidance may come from different
sources, including from an advisor model such as BART or BigBird, or in oracle
mode -- from the reference. This factorization achieves significantly higher
ROUGE scores on multiple benchmarks for long document summarization, namely
PubMed, arXiv, and GovReport. Most notably, our model is effective for domain
adaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1
score on arXiv, which indicates a strong performance due to more flexible
budget adaptation and content selection less dependent on domain-specific
textual structure.",https://github.com/thefonseca/factorsum,-1
Speech Emotion Recognition using Self-Supervised Features,0.997567,"Self-supervised pre-trained features have consistently delivered state-of-art
results in the field of natural language processing (NLP); however, their
merits in the field of speech emotion recognition (SER) still need further
investigation. In this paper we introduce a modular End-to- End (E2E) SER
system based on an Upstream + Downstream architecture paradigm, which allows
easy use/integration of a large variety of self-supervised features. Several
SER experiments for predicting categorical emotion classes from the IEMOCAP
dataset are performed. These experiments investigate interactions among
fine-tuning of self-supervised feature models, aggregation of frame-level
features into utterance-level features and back-end classification networks.
The proposed monomodal speechonly based system not only achieves SOTA results,
but also brings light to the possibility of powerful and well finetuned
self-supervised acoustic features that reach results similar to the results
achieved by SOTA multimodal systems using both Speech and Text modalities.",None,-1
Isotropic Representation Can Improve Dense Retrieval,0.123073,"The recent advancement in language representation modeling has broadly
affected the design of dense retrieval models. In particular, many of the
high-performing dense retrieval models evaluate representations of query and
document using BERT, and subsequently apply a cosine-similarity based scoring
to determine the relevance. BERT representations, however, are known to follow
an anisotropic distribution of a narrow cone shape and such an anisotropic
distribution can be undesirable for the cosine-similarity based scoring. In
this work, we first show that BERT-based DR also follows an anisotropic
distribution. To cope with the problem, we introduce unsupervised
post-processing methods of Normalizing Flow and whitening, and develop
token-wise method in addition to the sequence-wise method for applying the
post-processing methods to the representations of dense retrieval models. We
show that the proposed methods can effectively enhance the representations to
be isotropic, then we perform experiments with ColBERT and RepBERT to show that
the performance (NDCG at 10) of document re-ranking can be improved by
5.17\%$\sim$8.09\% for ColBERT and 6.88\%$\sim$22.81\% for RepBERT. To examine
the potential of isotropic representation for improving the robustness of DR
models, we investigate out-of-distribution tasks where the test dataset differs
from the training dataset. The results show that isotropic representation can
achieve a generally improved performance. For instance, when training dataset
is MS-MARCO and test dataset is Robust04, isotropy post-processing can improve
the baseline performance by up to 24.98\%. Furthermore, we show that an
isotropic model trained with an out-of-distribution dataset can even outperform
a baseline model trained with the in-distribution dataset.",https://github.com/SNU-DRL/IsotropicIR.git,-1
Augmentation Matters: A Simple-yet-Effective Approach to Semi-supervised Semantic Segmentation,0.889463,"Recent studies on semi-supervised semantic segmentation (SSS) have seen fast
progress. Despite their promising performance, current state-of-the-art methods
tend to increasingly complex designs at the cost of introducing more network
components and additional training procedures. Differently, in this work, we
follow a standard teacher-student framework and propose AugSeg, a simple and
clean approach that focuses mainly on data perturbations to boost the SSS
performance. We argue that various data augmentations should be adjusted to
better adapt to the semi-supervised scenarios instead of directly applying
these techniques from supervised learning. Specifically, we adopt a simplified
intensity-based augmentation that selects a random number of data
transformations with uniformly sampling distortion strengths from a continuous
space. Based on the estimated confidence of the model on different unlabeled
samples, we also randomly inject labelled information to augment the unlabeled
samples in an adaptive manner. Without bells and whistles, our simple AugSeg
can readily achieve new state-of-the-art performance on SSS benchmarks under
different partition protocols.",https://github.com/zhenzhao/AugSeg,-1
Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds,0.882995,"Dense captioning in 3D point clouds is an emerging vision-and-language task
involving object-level 3D scene understanding. Apart from coarse semantic class
prediction and bounding box regression as in traditional 3D object detection,
3D dense captioning aims at producing a further and finer instance-level label
of natural language description on visual appearance and spatial relations for
each scene object of interest. To detect and describe objects in a scene,
following the spirit of neural machine translation, we propose a
transformer-based encoder-decoder architecture, namely SpaCap3D, to transform
objects into descriptions, where we especially investigate the relative
spatiality of objects in 3D scenes and design a spatiality-guided encoder via a
token-to-token spatial relation learning objective and an object-centric
decoder for precise and spatiality-enhanced object caption generation.
Evaluated on two benchmark datasets, ScanRefer and ReferIt3D, our proposed
SpaCap3D outperforms the baseline method Scan2Cap by 4.94% and 9.61% in
CIDEr@0.5IoU, respectively. Our project page with source code and supplementary
files is available at https://SpaCap3D.github.io/ .",https://github.com/heng-hw/SpaCap3D,-1
Simple and Effective Synthesis of Indoor 3D Scenes,0.549641,"We study the problem of synthesizing immersive 3D indoor scenes from one or
more images. Our aim is to generate high-resolution images and videos from
novel viewpoints, including viewpoints that extrapolate far beyond the input
images while maintaining 3D consistency. Existing approaches are highly
complex, with many separately trained stages and components. We propose a
simple alternative: an image-to-image GAN that maps directly from reprojections
of incomplete point clouds to full high-resolution RGB-D images. On the
Matterport3D and RealEstate10K datasets, our approach significantly outperforms
prior work when evaluated by humans, as well as on FID scores. Further, we show
that our model is useful for generative data augmentation. A
vision-and-language navigation (VLN) agent trained with trajectories
spatially-perturbed by our model improves success rate by up to 1.5% over a
state of the art baseline on the R2R benchmark. Our code will be made available
to facilitate generative data augmentation and applications to downstream
robotics and embodied AI tasks.",https://github.com/google-research/se3ds,-1
Mere Contrastive Learning for Cross-Domain Sentiment Analysis,0.262643,"Cross-domain sentiment analysis aims to predict the sentiment of texts in the
target domain using the model trained on the source domain to cope with the
scarcity of labeled data. Previous studies are mostly cross-entropy-based
methods for the task, which suffer from instability and poor generalization. In
this paper, we explore contrastive learning on the cross-domain sentiment
analysis task. We propose a modified contrastive objective with in-batch
negative samples so that the sentence representations from the same class will
be pushed close while those from the different classes become further apart in
the latent space. Experiments on two widely used datasets show that our model
can achieve state-of-the-art performance in both cross-domain and multi-domain
sentiment analysis tasks. Meanwhile, visualizations demonstrate the
effectiveness of transferring knowledge learned in the source domain to the
target domain and the adversarial test verifies the robustness of our model.",https://github.com/LuoXiaoHeics/COBE,3063
KSS-ICP: Point Cloud Registration based on Kendall Shape Space,0.407152,"Point cloud registration is a popular topic which has been widely used in 3D
model reconstruction, location, and retrieval. In this paper, we propose a new
registration method, KSS-ICP, to address the rigid registration task in Kendall
shape space (KSS) with Iterative Closest Point (ICP). The KSS is a quotient
space that removes influences of translations, scales, and rotations for shape
feature-based analysis. Such influences can be concluded as the similarity
transformations that do not change the shape feature. The point cloud
representation in KSS is invariant to similarity transformations. We utilize
such property to design the KSS-ICP for point cloud registration. To tackle the
difficulty to achieve the KSS representation in general, the proposed KSS-ICP
formulates a practical solution that does not require complex feature analysis,
data training, and optimization. With a simple implementation, KSS-ICP achieves
more accurate registration from point clouds. It is robust to similarity
transformation, non-uniform density, noise, and defective parts. Experiments
show that KSS-ICP has better performance than the state of the art.",None,-1
MMFN: Multi-Modal-Fusion-Net for End-to-End Driving,0.708153,"Inspired by the fact that humans use diverse sensory organs to perceive the
world, sensors with different modalities are deployed in end-to-end driving to
obtain the global context of the 3D scene. In previous works, camera and LiDAR
inputs are fused through transformers for better driving performance. These
inputs are normally further interpreted as high-level map information to assist
navigation tasks. Nevertheless, extracting useful information from the complex
map input is challenging, for redundant information may mislead the agent and
negatively affect driving performance. We propose a novel approach to
efficiently extract features from vectorized High-Definition (HD) maps and
utilize them in the end-to-end driving tasks. In addition, we design a new
expert to further enhance the model performance by considering multi-road
rules. Experimental results prove that both of the proposed improvements enable
our agent to achieve superior performance compared with other methods.",https://github.com/Kin-Zhang/mmfn,-1
Semantic Segmentation-Assisted Instance Feature Fusion for Multi-Level 3D Part Instance Segmentation,0.210938,"Recognizing 3D part instances from a 3D point cloud is crucial for 3D
structure and scene understanding. Several learning-based approaches use
semantic segmentation and instance center prediction as training tasks and fail
to further exploit the inherent relationship between shape semantics and part
instances. In this paper, we present a new method for 3D part instance
segmentation. Our method exploits semantic segmentation to fuse nonlocal
instance features, such as center prediction, and further enhances the fusion
scheme in a multi- and cross-level way. We also propose a semantic region
center prediction task to train and leverage the prediction results to improve
the clustering of instance points. Our method outperforms existing methods with
a large-margin improvement in the PartNet benchmark. We also demonstrate that
our feature fusion scheme can be applied to other existing methods to improve
their performance in indoor scene instance segmentation tasks.",https://isunchy.github.io/projects/3d_instance_segmentation.html,-1
A Human-Centric Perspective on Fairness and Transparency in Algorithmic Decision-Making,0.0538543,"Automated decision systems (ADS) are increasingly used for consequential
decision-making. These systems often rely on sophisticated yet opaque machine
learning models, which do not allow for understanding how a given decision was
arrived at. This is not only problematic from a legal perspective, but
non-transparent systems are also prone to yield unfair outcomes because their
sanity is challenging to assess and calibrate in the first place -- which is
particularly worrisome for human decision-subjects. Based on this observation
and building upon existing work, I aim to make the following three main
contributions through my doctoral thesis: (a) understand how (potential)
decision-subjects perceive algorithmic decisions (with varying degrees of
transparency of the underlying ADS), as compared to similar decisions made by
humans; (b) evaluate different tools for transparent decision-making with
respect to their effectiveness in enabling people to appropriately assess the
quality and fairness of ADS; and (c) develop human-understandable technical
artifacts for fair automated decision-making. Over the course of the first half
of my PhD program, I have already addressed substantial pieces of (a) and (c),
whereas (b) will be the major focus of the second half.",None,-1
Chunk-based Nearest Neighbor Machine Translation,0.837577,"Semi-parametric models, which augment generation with retrieval, have led to
impressive results in language modeling and machine translation, due to their
ability to retrieve fine-grained information from a datastore of examples. One
of the most prominent approaches, $k$NN-MT, exhibits strong domain adaptation
capabilities by retrieving tokens from domain-specific datastores
\citep{khandelwal2020nearest}. However, $k$NN-MT requires an expensive
retrieval operation for every single generated token, leading to a very low
decoding speed (around 8 times slower than a parametric model). In this paper,
we introduce a \textit{chunk-based} $k$NN-MT model which retrieves chunks of
tokens from the datastore, instead of a single token. We propose several
strategies for incorporating the retrieved chunks into the generation process,
and for selecting the steps at which the model needs to search for neighbors in
the datastore. Experiments on machine translation in two settings, static and
``on-the-fly'' domain adaptation, show that the chunk-based $k$NN-MT model
leads to significant speed-ups (up to 4 times) with only a small drop in
translation quality.",https://github.com/deep-spin/chunk-based_knn-mt,-1
3D Scene Inference from Transient Histograms,0.465629,"Time-resolved image sensors that capture light at pico-to-nanosecond
timescales were once limited to niche applications but are now rapidly becoming
mainstream in consumer devices. We propose low-cost and low-power imaging
modalities that capture scene information from minimal time-resolved image
sensors with as few as one pixel. The key idea is to flood illuminate large
scene patches (or the entire scene) with a pulsed light source and measure the
time-resolved reflected light by integrating over the entire illuminated area.
The one-dimensional measured temporal waveform, called \emph{transient},
encodes both distances and albedoes at all visible scene points and as such is
an aggregate proxy for the scene's 3D geometry. We explore the viability and
limitations of the transient waveforms by themselves for recovering scene
information, and also when combined with traditional RGB cameras. We show that
plane estimation can be performed from a single transient and that using only a
few more it is possible to recover a depth map of the whole scene. We also show
two proof-of-concept hardware prototypes that demonstrate the feasibility of
our approach for compact, mobile, and budget-limited applications.",None,-1
Region Embedding with Intra and Inter-View Contrastive Learning,0.481917,"Unsupervised region representation learning aims to extract dense and
effective features from unlabeled urban data. While some efforts have been made
for solving this problem based on multiple views, existing methods are still
insufficient in extracting representations in a view and/or incorporating
representations from different views. Motivated by the success of contrastive
learning for representation learning, we propose to leverage it for multi-view
region representation learning and design a model called ReMVC (Region
Embedding with Multi-View Contrastive Learning) by following two guidelines: i)
comparing a region with others within each view for effective representation
extraction and ii) comparing a region with itself across different views for
cross-view information sharing. We design the intra-view contrastive learning
module which helps to learn distinguished region embeddings and the inter-view
contrastive learning module which serves as a soft co-regularizer to constrain
the embedding parameters and transfer knowledge across multi-views. We exploit
the learned region embeddings in two downstream tasks named land usage
clustering and region popularity prediction. Extensive experiments demonstrate
that our model achieves impressive improvements compared with seven
state-of-the-art baseline methods, and the margins are over 30% in the land
usage clustering task.",https://github.com/Liang-NTU/ReMVC,2797
Generative Category-Level Shape and Pose Estimation with Semantic Primitives,0.638949,"Empowering autonomous agents with 3D understanding for daily objects is a
grand challenge in robotics applications. When exploring in an unknown
environment, existing methods for object pose estimation are still not
satisfactory due to the diversity of object shapes. In this paper, we propose a
novel framework for category-level object shape and pose estimation from a
single RGB-D image. To handle the intra-category variation, we adopt a semantic
primitive representation that encodes diverse shapes into a unified latent
space, which is the key to establish reliable correspondences between observed
point clouds and estimated shapes. Then, by using a SIM(3)-invariant shape
descriptor, we gracefully decouple the shape and pose of an object, thus
supporting latent shape optimization of target objects in arbitrary poses.
Extensive experiments show that the proposed method achieves SOTA pose
estimation performance and better generalization in the real-world dataset.
Code and video are available at https://zju3dv.github.io/gCasp.",https://zju3dv.github.io/gCasp,4589
Human-Object Interaction Detection via Disentangled Transformer,0.830849,"Human-Object Interaction Detection tackles the problem of joint localization
and classification of human object interactions. Existing HOI transformers
either adopt a single decoder for triplet prediction, or utilize two parallel
decoders to detect individual objects and interactions separately, and compose
triplets by a matching process. In contrast, we decouple the triplet prediction
into human-object pair detection and interaction classification. Our main
motivation is that detecting the human-object instances and classifying
interactions accurately needs to learn representations that focus on different
regions. To this end, we present Disentangled Transformer, where both encoder
and decoder are disentangled to facilitate learning of two sub-tasks. To
associate the predictions of disentangled decoders, we first generate a unified
representation for HOI triplets with a base decoder, and then utilize it as
input feature of each disentangled decoder. Extensive experiments show that our
method outperforms prior work on two public HOI benchmarks by a sizeable
margin. Code will be available.",https://github.com/facebookresearch/detectron2,-1
HiSMatch: Historical Structure Matching based Temporal Knowledge Graph Reasoning,0.825023,"A Temporal Knowledge Graph (TKG) is a sequence of KGs with respective
timestamps, which adopts quadruples in the form of (\emph{subject},
\emph{relation}, \emph{object}, \emph{timestamp}) to describe dynamic facts.
TKG reasoning has facilitated many real-world applications via answering such
queries as (\emph{query entity}, \emph{query relation}, \emph{?}, \emph{future
timestamp}) about future. This is actually a matching task between a query and
candidate entities based on their historical structures, which reflect
behavioral trends of the entities at different timestamps. In addition, recent
KGs provide background knowledge of all the entities, which is also helpful for
the matching. Thus, in this paper, we propose the \textbf{Hi}storical
\textbf{S}tructure \textbf{Match}ing (\textbf{HiSMatch}) model. It applies two
structure encoders to capture the semantic information contained in the
historical structures of the query and candidate entities. Besides, it adopts
another encoder to integrate the background knowledge into the model. TKG
reasoning experiments on six benchmark datasets demonstrate the significant
improvement of the proposed HiSMatch model, with up to 5.6\% performance
improvement in MRR, compared to the state-of-the-art baselines.",None,-1
Multi-Scale Representation Learning on Proteins,0.52746,"Proteins are fundamental biological entities mediating key roles in cellular
function and disease. This paper introduces a multi-scale graph construction of
a protein -- HoloProt -- connecting surface to structure and sequence. The
surface captures coarser details of the protein, while sequence as primary
component and structure -- comprising secondary and tertiary components --
capture finer details. Our graph encoder then learns a multi-scale
representation by allowing each level to integrate the encoding from level(s)
below with the graph at that level. We test the learned representation on
different tasks, (i.) ligand binding affinity (regression), and (ii.) protein
function prediction (classification). On the regression task, contrary to
previous methods, our model performs consistently and reliably across different
dataset splits, outperforming all baselines on most splits. On the
classification task, it achieves a performance close to the top-performing
model while using 10x fewer parameters. To improve the memory efficiency of our
construction, we segment the multiplex protein surface manifold into molecular
superpixels and substitute the surface with these superpixels at little to no
performance loss.",https://github.com/rdkit/rdkit/releases/tag/Release_2016_09_4,925
"Theories of ""Gender"" in NLP Bias Research",0.859482,"The rise of concern around Natural Language Processing (NLP) technologies
containing and perpetuating social biases has led to a rich and rapidly growing
area of research. Gender bias is one of the central biases being analyzed, but
to date there is no comprehensive analysis of how ""gender"" is theorized in the
field. We survey nearly 200 articles concerning gender bias in NLP to discover
how the field conceptualizes gender both explicitly (e.g. through definitions
of terms) and implicitly (e.g. through how gender is operationalized in
practice). In order to get a better idea of emerging trajectories of thought,
we split these articles into two sections by time.
  We find that the majority of the articles do not make their theorization of
gender explicit, even if they clearly define ""bias."" Almost none use a model of
gender that is intersectional or inclusive of nonbinary genders; and many
conflate sex characteristics, social gender, and linguistic gender in ways that
disregard the existence and experience of trans, nonbinary, and intersex
people. There is an increase between the two time-sections in statements
acknowledging that gender is a complicated reality, however, very few articles
manage to put this acknowledgment into practice. In addition to analyzing these
findings, we provide specific recommendations to facilitate interdisciplinary
work, and to incorporate theory and methodology from Gender Studies. Our hope
is that this will produce more inclusive gender bias research in NLP.",None,-1
STL-Based Synthesis of Feedback Controllers Using Reinforcement Learning,0.120154,"Deep Reinforcement Learning (DRL) has the potential to be used for
synthesizing feedback controllers (agents) for various complex systems with
unknown dynamics. These systems are expected to satisfy diverse safety and
liveness properties best captured using temporal logic. In RL, the reward
function plays a crucial role in specifying the desired behaviour of these
agents. However, the problem of designing the reward function for an RL agent
to satisfy complex temporal logic specifications has received limited attention
in the literature. To address this, we provide a systematic way of generating
rewards in real-time by using the quantitative semantics of Signal Temporal
Logic (STL), a widely used temporal logic to specify the behaviour of
cyber-physical systems. We propose a new quantitative semantics for STL having
several desirable properties, making it suitable for reward generation. We
evaluate our STL-based reinforcement learning mechanism on several complex
continuous control benchmarks and compare our STL semantics with those
available in the literature in terms of their efficacy in synthesizing the
controller agent. Experimental results establish our new semantics to be the
most suitable for synthesizing feedback controllers for complex continuous
dynamical systems through reinforcement learning.",https://github.com/iitkcpslab/rlstl,-1
Reachability Verification Based Reliability Assessment for Deep Reinforcement Learning Controlled Robotics and Autonomous Systems,0.493345,"Deep Reinforcement Learning (DRL) has achieved impressive performance in
robotics and autonomous systems (RAS). A key challenge to its deployment in
real-life operations is the presence of spuriously unsafe DRL policies.
Unexplored states may lead the agent to make wrong decisions that could result
in hazards, especially in applications where DRL-trained end-to-end controllers
govern the behaviour of RAS. This paper proposes a novel quantitative
reliability assessment framework for DRL-controlled RAS, leveraging
verification evidence generated from formal reliability analysis of neural
networks. A two-level verification framework is introduced to check the safety
property with respect to inaccurate observations that are due to, e.g.,
environmental noise and state changes. Reachability verification tools are
leveraged locally to generate safety evidence of trajectories. In contrast, at
the global level, we quantify the overall reliability as an aggregated metric
of local safety evidence, corresponding to a set of distinct tasks and their
occurrence probabilities. The effectiveness of the proposed verification
framework is demonstrated and validated via experiments on real RAS.",https://github.com/Solitude-SAMR,-1
DeepCuts: Single-Shot Interpretability based Pruning for BERT,0.0838694,"As language models have grown in parameters and layers, it has become much
harder to train and infer with them on single GPUs. This is severely
restricting the availability of large language models such as GPT-3,
BERT-Large, and many others. A common technique to solve this problem is
pruning the network architecture by removing transformer heads, fully-connected
weights, and other modules. The main challenge is to discern the important
parameters from the less important ones. Our goal is to find strong metrics for
identifying such parameters. We thus propose two strategies: Cam-Cut based on
the GradCAM interpretations, and Smooth-Cut based on the SmoothGrad, for
calculating the importance scores. Through this work, we show that our scoring
functions are able to assign more relevant task-based scores to the network
parameters, and thus both our pruning approaches significantly outperform the
standard weight and gradient-based strategies, especially at higher compression
ratios in BERT-based models. We also analyze our pruning masks and find them to
be significantly different from the ones obtained using standard metrics.",https://github.com/RuskinMan/DeepCuts,-1
A General Purpose Neural Architecture for Geospatial Systems,0.179815,"Geospatial Information Systems are used by researchers and Humanitarian
Assistance and Disaster Response (HADR) practitioners to support a wide variety
of important applications. However, collaboration between these actors is
difficult due to the heterogeneous nature of geospatial data modalities (e.g.,
multi-spectral images of various resolutions, timeseries, weather data) and
diversity of tasks (e.g., regression of human activity indicators or detecting
forest fires). In this work, we present a roadmap towards the construction of a
general-purpose neural architecture (GPNA) with a geospatial inductive bias,
pre-trained on large amounts of unlabelled earth observation data in a
self-supervised manner. We envision how such a model may facilitate cooperation
between members of the community. We show preliminary results on the first step
of the roadmap, where we instantiate an architecture that can process a wide
variety of geospatial data modalities and demonstrate that it can achieve
competitive performance with domain-specific architectures on tasks relating to
the U.N.'s Sustainable Development Goals.",None,-1
Black-Box Attack against GAN-Generated Image Detector with Contrastive Perturbation,0.28664,"Visually realistic GAN-generated facial images raise obvious concerns on
potential misuse. Many effective forensic algorithms have been developed to
detect such synthetic images in recent years. It is significant to assess the
vulnerability of such forensic detectors against adversarial attacks. In this
paper, we propose a new black-box attack method against GAN-generated image
detectors. A novel contrastive learning strategy is adopted to train the
encoder-decoder network based anti-forensic model under a contrastive loss
function. GAN images and their simulated real counterparts are constructed as
positive and negative samples, respectively. Leveraging on the trained attack
model, imperceptible contrastive perturbation could be applied to input
synthetic images for removing GAN fingerprint to some extent. As such, existing
GAN-generated image detectors are expected to be deceived. Extensive
experimental results verify that the proposed attack effectively reduces the
accuracy of three state-of-the-art detectors on six popular GANs. High visual
quality of the attacked images is also achieved. The source code will be
available at https://github.com/ZXMMD/BAttGAND.",https://github.com/ZXMMD/BAttGAND,-1
Overview of the HASOC Subtrack at FIRE 2022: Offensive Language Identification in Marathi,0.897191,"The widespread of offensive content online has become a reason for great
concern in recent years, motivating researchers to develop robust systems
capable of identifying such content automatically. With the goal of carrying
out a fair evaluation of these systems, several international competitions have
been organized, providing the community with important benchmark data and
evaluation methods for various languages. Organized since 2019, the HASOC (Hate
Speech and Offensive Content Identification) shared task is one of these
initiatives. In its fourth iteration, HASOC 2022 included three subtracks for
English, Hindi, and Marathi. In this paper, we report the results of the HASOC
2022 Marathi subtrack which provided participants with a dataset containing
data from Twitter manually annotated using the popular OLID taxonomy. The
Marathi track featured three additional subtracks, each corresponding to one
level of the taxonomy: Task A - offensive content identification (offensive vs.
non-offensive); Task B - categorization of offensive types (targeted vs.
untargeted), and Task C - offensive target identification (individual vs. group
vs. others). Overall, 59 runs were submitted by 10 teams. The best systems
obtained an F1 of 0.9745 for Subtrack 3A, an F1 of 0.9207 for Subtrack 3B, and
F1 of 0.9607 for Subtrack 3C. The best performing algorithms were a mixture of
traditional and deep learning approaches.",None,-1
HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks,0.237342,"The workflow of pretraining and fine-tuning has emerged as a popular paradigm
for solving various NLP and V&L (Vision-and-Language) downstream tasks. With
the capacity of pretrained models growing rapidly, how to perform
parameter-efficient fine-tuning has become fairly important for quick transfer
learning and deployment. In this paper, we design a novel unified
parameter-efficient transfer learning framework that works effectively on both
pure language and V&L tasks. In particular, we use a shared hypernetwork that
takes trainable hyper-embeddings as input, and outputs weights for fine-tuning
different small modules in a pretrained language model, such as tuning the
parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and
feed-forward blocks (i.e., adapter-tuning). We define a set of embeddings
(e.g., layer, block, task and visual embeddings) as the key components to
calculate hyper-embeddings, which thus can support both pure language and V&L
tasks. Our proposed framework adds fewer trainable parameters in multi-task
learning while achieving superior performances and transfer ability compared to
state-of-the-art methods. Empirical results on the GLUE benchmark and multiple
V&L tasks confirm the effectiveness of our framework on both textual and visual
modalities.",None,-1
Iterative Next Boundary Detection for Instance Segmentation of Tree Rings in Microscopy Images of Shrub Cross Sections,0.496928,"We address the problem of detecting tree rings in microscopy images of shrub
cross sections. This can be regarded as a special case of the instance
segmentation task with several unique challenges such as the concentric
circular ring shape of the objects and high precision requirements that result
in inadequate performance of existing methods. We propose a new iterative
method which we term Iterative Next Boundary Detection (INBD). It intuitively
models the natural growth direction, starting from the center of the shrub
cross section and detecting the next ring boundary in each iteration step. In
our experiments, INBD shows superior performance to generic instance
segmentation methods and is the only one with a built-in notion of
chronological order. Our dataset and source code are available at
http://github.com/alexander-g/INBD.",http://github.com/alexander-g/INBD,-1
Model-based Multi-agent Reinforcement Learning: Recent Progress and Prospects,0.519939,"Significant advances have recently been achieved in Multi-Agent Reinforcement
Learning (MARL) which tackles sequential decision-making problems involving
multiple participants. However, MARL requires a tremendous number of samples
for effective training. On the other hand, model-based methods have been shown
to achieve provable advantages of sample efficiency. However, the attempts of
model-based methods to MARL have just started very recently. This paper
presents a review of the existing research on model-based MARL, including
theoretical analyses, algorithms, and applications, and analyzes the advantages
and potential of model-based MARL. Specifically, we provide a detailed taxonomy
of the algorithms and point out the pros and cons for each algorithm according
to the challenges inherent to multi-agent scenarios. We also outline promising
directions for future development of this field.",None,-1
STaR: Bootstrapping Reasoning With Reasoning,0.9917,"Generating step-by-step ""chain-of-thought"" rationales improves language model
performance on complex reasoning tasks like mathematics or commonsense
question-answering. However, inducing language model rationale generation
currently requires either constructing massive rationale datasets or
sacrificing accuracy by using only few-shot inference. We propose a technique
to iteratively leverage a small number of rationale examples and a large
dataset without rationales, to bootstrap the ability to perform successively
more complex reasoning. This technique, the ""Self-Taught Reasoner"" (STaR),
relies on a simple loop: generate rationales to answer many questions, prompted
with a few rationale examples; if the generated answers are wrong, try again to
generate a rationale given the correct answer; fine-tune on all the rationales
that ultimately yielded correct answers; repeat. We show that STaR
significantly improves performance on multiple datasets compared to a model
fine-tuned to directly predict final answers, and performs comparably to
fine-tuning a 30$\times$ larger state-of-the-art language model on
CommensenseQA. Thus, STaR lets a model improve itself by learning from its own
generated reasoning.",https://github.com/kingoflolz/mesh-transformer-jax,-1
SNP2Vec: Scalable Self-Supervised Pre-Training for Genome-Wide Association Study,0.454906,"Self-supervised pre-training methods have brought remarkable breakthroughs in
the understanding of text, image, and speech. Recent developments in genomics
has also adopted these pre-training methods for genome understanding. However,
they focus only on understanding haploid sequences, which hinders their
applicability towards understanding genetic variations, also known as single
nucleotide polymorphisms (SNPs), which is crucial for genome-wide association
study. In this paper, we introduce SNP2Vec, a scalable self-supervised
pre-training approach for understanding SNP. We apply SNP2Vec to perform
long-sequence genomics modeling, and we evaluate the effectiveness of our
approach on predicting Alzheimer's disease risk in a Chinese cohort. Our
approach significantly outperforms existing polygenic risk score methods and
all other baselines, including the model that is trained entirely with haploid
sequences. We release our code and dataset on
https://github.com/HLTCHKUST/snp2vec.",https://github.com/HLTCHKUST/snp2vec,-1
Human-to-Robot Imitation in the Wild,0.877126,"We approach the problem of learning by watching humans in the wild. While
traditional approaches in Imitation and Reinforcement Learning are promising
for learning in the real world, they are either sample inefficient or are
constrained to lab settings. Meanwhile, there has been a lot of success in
processing passive, unstructured human data. We propose tackling this problem
via an efficient one-shot robot learning algorithm, centered around learning
from a third-person perspective. We call our method WHIRL: In-the-Wild Human
Imitating Robot Learning. WHIRL extracts a prior over the intent of the human
demonstrator, using it to initialize our agent's policy. We introduce an
efficient real-world policy learning scheme that improves using interactions.
Our key contributions are a simple sampling-based policy optimization approach,
a novel objective function for aligning human and robot videos as well as an
exploration method to boost sample efficiency. We show one-shot generalization
and success in real-world settings, including 20 different manipulation tasks
in the wild. Videos and talk at https://human2robot.github.io",https://github.com/hello-robot,-1
Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction,0.912085,"Compared with traditional sentence-level relation extraction, document-level
relation extraction is a more challenging task where an entity in a document
may be mentioned multiple times and associated with multiple relations.
However, most methods of document-level relation extraction do not distinguish
between mention-level features and entity-level features, and just apply simple
pooling operation for aggregating mention-level features into entity-level
features. As a result, the distinct semantics between the different mentions of
an entity are overlooked. To address this problem, we propose RSMAN in this
paper which performs selective attentions over different entity mentions with
respect to candidate relations. In this manner, the flexible and
relation-specific representations of entities are obtained which indeed benefit
relation classification. Our extensive experiments upon two benchmark datasets
show that our RSMAN can bring significant improvements for some backbone models
to achieve state-of-the-art performance, especially when an entity have
multiple mentions in the document.",https://github.com/FDUyjx/RSMAN,-1
Towards a general purpose machine translation system for Sranantongo,0.148195,"Machine translation for Sranantongo (Sranan, srn), a low-resource Creole
language spoken predominantly in Surinam, is virgin territory. In this study we
create a general purpose machine translation system for srn. In order to
facilitate this research, we introduce the SRNcorpus, a collection of parallel
Dutch (nl) to srn and monolingual srn data. We experiment with a wide range of
proven machine translation methods. Our results demonstrate a strong baseline
machine translation system for srn.",None,-1
SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval,0.673324,"Sampling proper negatives from a large document pool is vital to effectively
train a dense retrieval model. However, existing negative sampling strategies
suffer from the uninformative or false negative problem. In this work, we
empirically show that according to the measured relevance scores, the negatives
ranked around the positives are generally more informative and less likely to
be false negatives. Intuitively, these negatives are not too hard (\emph{may be
false negatives}) or too easy (\emph{uninformative}). They are the ambiguous
negatives and need more attention during training. Thus, we propose a simple
ambiguous negatives sampling method, SimANS, which incorporates a new sampling
probability distribution to sample more ambiguous negatives. Extensive
experiments on four public and one industry datasets show the effectiveness of
our approach. We made the code and models publicly available in
\url{https://github.com/microsoft/SimXNS}.",https://github.com/microsoft/SimXNS,-1
"""Is your explanation stable?"": A Robustness Evaluation Framework for Feature Attribution",0.216408,"Understanding the decision process of neural networks is hard. One vital
method for explanation is to attribute its decision to pivotal features.
Although many algorithms are proposed, most of them solely improve the
faithfulness to the model. However, the real environment contains many random
noises, which may leads to great fluctuations in the explanations. More
seriously, recent works show that explanation algorithms are vulnerable to
adversarial attacks. All of these make the explanation hard to trust in real
scenarios.
  To bridge this gap, we propose a model-agnostic method \emph{Median Test for
Feature Attribution} (MeTFA) to quantify the uncertainty and increase the
stability of explanation algorithms with theoretical guarantees. MeTFA has the
following two functions: (1) examine whether one feature is significantly
important or unimportant and generate a MeTFA-significant map to visualize the
results; (2) compute the confidence interval of a feature attribution score and
generate a MeTFA-smoothed map to increase the stability of the explanation.
Experiments show that MeTFA improves the visual quality of explanations and
significantly reduces the instability while maintaining the faithfulness. To
quantitatively evaluate the faithfulness of an explanation under different
noise settings, we further propose several robust faithfulness metrics.
Experiment results show that the MeTFA-smoothed explanation can significantly
increase the robust faithfulness. In addition, we use two scenarios to show
MeTFA's potential in the applications. First, when applied to the SOTA
explanation method to locate context bias for semantic segmentation models,
MeTFA-significant explanations use far smaller regions to maintain 99\%+
faithfulness. Second, when tested with different explanation-oriented attacks,
MeTFA can help defend vanilla, as well as adaptive, adversarial attacks against
explanations.",None,-1
Momentum Decoding: Open-ended Text Generation As Graph Exploration,0.0582321,"Open-ended text generation with autoregressive language models (LMs) is one
of the core tasks in natural language processing. However, maximization-based
decoding methods (e.g., greedy/beam search) often lead to the degeneration
problem, i.e., the generated text is unnatural and contains undesirable
repetitions. Existing solutions to this problem either introduce randomness
prone to incoherence or require a look-ahead mechanism that demands extra
computational overhead. In this study, we formulate open-ended text generation
from a new perspective, i.e., we view it as an exploration process within a
directed graph. Thereby, we understand the phenomenon of degeneration as
circular loops within the directed graph. Based on our formulation, we propose
a novel decoding method -- \textit{momentum decoding} -- which encourages the
LM to \textit{greedily} explore new nodes outside the current graph. Meanwhile,
it also allows the LM to return to the existing nodes with a momentum
downgraded by a pre-defined resistance function. We extensively test our
approach on three benchmarks from different domains through automatic and human
evaluations. The results show that momentum decoding performs comparably with
the current state of the art while enjoying notably improved inference speed
and computation FLOPs. Furthermore, we conduct a detailed analysis to reveal
the merits and inner workings of our approach. Our codes and other related
resources are publicly available at
https://github.com/gmftbyGMFTBY/MomentumDecoding.",https://github.com/gmftbyGMFTBY/,-1
The Inverse of Exact Renormalization Group Flows as Statistical Inference,0.877106,"We build on the view of the Exact Renormalization Group (ERG) as an
instantiation of Optimal Transport described by a functional
convection-diffusion equation. We provide a new information theoretic
perspective for understanding the ERG through the intermediary of Bayesian
Statistical Inference. This connection is facilitated by the Dynamical Bayesian
Inference scheme, which encodes Bayesian inference in the form of a one
parameter family of probability distributions solving an integro-differential
equation derived from Bayes' law. In this note, we demonstrate how the
Dynamical Bayesian Inference equation is, itself, equivalent to a diffusion
equation which we dub Bayesian Diffusion. Identifying the features that define
Bayesian Diffusion, and mapping them onto the features that define the ERG, we
obtain a dictionary outlining how renormalization can be understood as the
inverse of statistical inference.",None,-1
A2: Efficient Automated Attacker for Boosting Adversarial Training,0.448265,"Based on the significant improvement of model robustness by AT (Adversarial
Training), various variants have been proposed to further boost the
performance. Well-recognized methods have focused on different components of AT
(e.g., designing loss functions and leveraging additional unlabeled data). It
is generally accepted that stronger perturbations yield more robust models.
However, how to generate stronger perturbations efficiently is still missed. In
this paper, we propose an efficient automated attacker called A2 to boost AT by
generating the optimal perturbations on-the-fly during training. A2 is a
parameterized automated attacker to search in the attacker space for the best
attacker against the defense model and examples. Extensive experiments across
different datasets demonstrate that A2 generates stronger perturbations with
low extra cost and reliably improves the robustness of various AT methods
against different attacks.",https://github.com/huyvnphan/PyTorch_CIFAR10,-1
Hierarchical Attention Network for Explainable Depression Detection on Twitter Aided by Metaphor Concept Mappings,0.589872,"Automatic depression detection on Twitter can help individuals privately and
conveniently understand their mental health status in the early stages before
seeing mental health professionals. Most existing black-box-like deep learning
methods for depression detection largely focused on improving classification
performance. However, explaining model decisions is imperative in health
research because decision-making can often be high-stakes and life-and-death.
Reliable automatic diagnosis of mental health problems including depression
should be supported by credible explanations justifying models' predictions. In
this work, we propose a novel explainable model for depression detection on
Twitter. It comprises a novel encoder combining hierarchical attention
mechanisms and feed-forward neural networks. To support psycholinguistic
studies, our model leverages metaphorical concept mappings as input. Thus, it
not only detects depressed individuals, but also identifies features of such
users' tweets and associated metaphor concept mappings.",None,55352
Learning Invariable Semantical Representation from Language for Extensible Policy Generalization,0.0379849,"Recently, incorporating natural language instructions into reinforcement
learning (RL) to learn semantically meaningful representations and foster
generalization has caught many concerns. However, the semantical information in
language instructions is usually entangled with task-specific state
information, which hampers the learning of semantically invariant and reusable
representations. In this paper, we propose a method to learn such
representations called element randomization, which extracts task-relevant but
environment-agnostic semantics from instructions using a set of environments
with randomized elements, e.g., topological structures or textures, yet the
same language instruction. We theoretically prove the feasibility of learning
semantically invariant representations through randomization. In practice, we
accordingly develop a hierarchy of policies, where a high-level policy is
designed to modulate the behavior of a goal-conditioned low-level policy by
proposing subgoals as semantically invariant representations. Experiments on
challenging long-horizon tasks show that (1) our low-level policy reliably
generalizes to tasks against environment changes; (2) our hierarchical policy
exhibits extensible generalization in unseen new tasks that can be decomposed
into several solvable sub-tasks; and (3) by storing and replaying language
trajectories as succinct policy representations, the agent can complete tasks
in a one-shot fashion, i.e., once one successful trajectory has been attained.",None,-1
Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone Disambiguation,0.507519,"Conversion of Chinese Grapheme-to-Phoneme (G2P) plays an important role in
Mandarin Chinese Text-To-Speech (TTS) systems, where one of the biggest
challenges is the task of polyphone disambiguation. Most of the previous
polyphone disambiguation models are trained on manually annotated datasets, and
publicly available datasets for polyphone disambiguation are scarce. In this
paper we propose a simple back-translation-style data augmentation method for
mandarin Chinese polyphone disambiguation, utilizing a large amount of
unlabeled text data. Inspired by the back-translation technique proposed in the
field of machine translation, we build a Grapheme-to-Phoneme (G2P) model to
predict the pronunciation of polyphonic character, and a Phoneme-to-Grapheme
(P2G) model to predict pronunciation into text. Meanwhile, a window-based
matching strategy and a multi-model scoring strategy are proposed to judge the
correctness of the pseudo-label. We design a data balance strategy to improve
the accuracy of some typical polyphonic characters in the training set with
imbalanced distribution or data scarcity. The experimental result shows the
effectiveness of the proposed back-translation-style data augmentation method.",None,-1
Abstract Flow for Temporal Semantic Segmentation on the Permutohedral Lattice,0.17443,"Semantic segmentation is a core ability required by autonomous agents, as
being able to distinguish which parts of the scene belong to which object class
is crucial for navigation and interaction with the environment. Approaches
which use only one time-step of data cannot distinguish between moving objects
nor can they benefit from temporal integration. In this work, we extend a
backbone LatticeNet to process temporal point cloud data. Additionally, we take
inspiration from optical flow methods and propose a new module called Abstract
Flow which allows the network to match parts of the scene with similar abstract
features and gather the information temporally. We obtain state-of-the-art
results on the SemanticKITTI dataset that contains LiDAR scans from real urban
environments. We share the PyTorch implementation of TemporalLatticeNet at
https://github.com/AIS-Bonn/temporal_latticenet .",None,-1
Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance,0.369535,"We study the challenging problem of recovering detailed motion from a single
motion-blurred image. Existing solutions to this problem estimate a single
image sequence without considering the motion ambiguity for each region.
Therefore, the results tend to converge to the mean of the multi-modal
possibilities. In this paper, we explicitly account for such motion ambiguity,
allowing us to generate multiple plausible solutions all in sharp detail. The
key idea is to introduce a motion guidance representation, which is a compact
quantization of 2D optical flow with only four discrete motion directions.
Conditioned on the motion guidance, the blur decomposition is led to a
specific, unambiguous solution by using a novel two-stage decomposition
network. We propose a unified framework for blur decomposition, which supports
various interfaces for generating our motion guidance, including human input,
motion information from adjacent video frames, and learning from a video
dataset. Extensive experiments on synthesized datasets and real-world data show
that the proposed framework is qualitatively and quantitatively superior to
previous methods, and also offers the merit of producing physically plausible
and diverse solutions. Code is available at
https://github.com/zzh-tech/Animation-from-Blur.",https://github.com/zzh-tech/Animation-from-Blur,-1
Stubborn: A Strong Baseline for Indoor Object Navigation,0.852072,"We present a strong baseline that surpasses the performance of previously
published methods on the Habitat Challenge task of navigating to a target
object in indoor environments. Our method is motivated from primary failure
modes of prior state-of-the-art: poor exploration, inaccurate object
identification, and agent getting trapped due to imprecise map construction. We
make three contributions to mitigate these issues: (i) First, we show that
existing map-based methods fail to effectively use semantic clues for
exploration. We present a semantic-agnostic exploration strategy (called
Stubborn) without any learning that surprisingly outperforms prior work. (ii)
We propose a strategy for integrating temporal information to improve object
identification. (iii) Lastly, due to inaccurate depth observation the agent
often gets trapped in small regions. We develop a multi-scale collision map for
obstacle identification that mitigates this issue.",https://github.com/Improbable-AI/Stubborn,-1
A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots,0.684959,"A slot value might be provided segment by segment over multiple-turn
interactions in a dialog, especially for some important information such as
phone numbers and names. It is a common phenomenon in daily life, but little
attention has been paid to it in previous work. To fill the gap, this paper
defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds
a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset
includes a total of 40K dialogs and 500K utterances from four different
domains: Chinese names, phone numbers, ID numbers and license plate numbers.
The data is well annotated with sub-slot values, slot values, dialog states and
actions. We find some new linguistic phenomena and interactive manners in SSTOD
which raise critical challenges of building dialog agents for the task. We test
three state-of-the-art dialog models on SSTOD and find they cannot handle the
task well on any of the four domains. We also investigate an improved model by
involving slot knowledge in a plug-in manner. More work should be done to meet
the new challenges raised from SSTOD which widely exists in real-life
applications. The dataset and code are publicly available via
https://github.com/shunjiu/SSTOD.",https://github.com/shunjiu/SSTOD,7165
Training Language Models with Language Feedback,0.746939,"Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level summarization
ability.",None,150035
Depth Map Decomposition for Monocular Depth Estimation,0.657514,"We propose a novel algorithm for monocular depth estimation that decomposes a
metric depth map into a normalized depth map and scale features. The proposed
network is composed of a shared encoder and three decoders, called G-Net,
N-Net, and M-Net, which estimate gradient maps, a normalized depth map, and a
metric depth map, respectively. M-Net learns to estimate metric depths more
accurately using relative depth features extracted by G-Net and N-Net. The
proposed algorithm has the advantage that it can use datasets without metric
depth labels to improve the performance of metric depth estimation.
Experimental results on various datasets demonstrate that the proposed
algorithm not only provides competitive performance to state-of-the-art
algorithms but also yields acceptable results even when only a small amount of
metric depth data is available for its training.",None,-1
Relighting4D: Neural Relightable Human from Videos,0.559845,"Human relighting is a highly desirable yet challenging task. Existing works
either require expensive one-light-at-a-time (OLAT) captured data using light
stage or cannot freely change the viewpoints of the rendered body. In this
work, we propose a principled framework, Relighting4D, that enables
free-viewpoints relighting from only human videos under unknown illuminations.
Our key insight is that the space-time varying geometry and reflectance of the
human body can be decomposed as a set of neural fields of normal, occlusion,
diffuse, and specular maps. These neural fields are further integrated into
reflectance-aware physically based rendering, where each vertex in the neural
field absorbs and reflects the light from the environment. The whole framework
can be learned from videos in a self-supervised manner, with physically
informed priors designed for regularization. Extensive experiments on both real
and synthetic datasets demonstrate that our framework is capable of relighting
dynamic human actors with free-viewpoints.",None,-1
Domain-Generalized Textured Surface Anomaly Detection,0.370504,"Anomaly detection aims to identify abnormal data that deviates from the
normal ones, while typically requiring a sufficient amount of normal data to
train the model for performing this task. Despite the success of recent anomaly
detection methods, performing anomaly detection in an unseen domain remain a
challenging task. In this paper, we address the task of domain-generalized
textured surface anomaly detection. By observing normal and abnormal surface
data across multiple source domains, our model is expected to be generalized to
an unseen textured surface of interest, in which only a small number of normal
data can be observed during testing. Although with only image-level labels
observed in the training data, our patch-based meta-learning model exhibits
promising generalization ability: not only can it generalize to unseen image
domains, but it can also localize abnormal regions in the query image. Our
experiments verify that our model performs favorably against state-of-the-art
anomaly detection and domain generalization approaches in various settings.",None,9487
Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training,0.45366,"Large-scale vision-language pre-trained (VLP) models are prone to hallucinate
non-existent visual objects when generating text based on visual information.
In this paper, we systematically study the object hallucination problem from
three aspects. First, we examine recent state-of-the-art VLP models, showing
that they still hallucinate frequently, and models achieving better scores on
standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate
how different types of image encoding in VLP influence hallucination, including
region-based, grid-based, and patch-based. Surprisingly, we find that
patch-based features perform the best and smaller patch resolution yields a
non-trivial reduction in object hallucination. Third, we decouple various VLP
objectives and demonstrate that token-level image-text alignment and controlled
generation are crucial to reducing hallucination. Based on that, we propose a
simple yet effective VLP loss named ObjMLM to further mitigate object
hallucination. Results show that it reduces object hallucination by up to 17.4%
when tested on two benchmarks (COCO Caption for in-domain and NoCaps for
out-of-domain evaluation).",https://github.com/wenliangdai/VLP-Object-Hallucination,-1
InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds,0.884219,"In this paper, we take a significant step towards real-world applicability of
monocular neural avatar reconstruction by contributing InstantAvatar, a system
that can reconstruct human avatars from a monocular video within seconds, and
these avatars can be animated and rendered at an interactive rate. To achieve
this efficiency we propose a carefully designed and engineered system, that
leverages emerging acceleration structures for neural fields, in combination
with an efficient empty space-skipping strategy for dynamic scenes. We also
contribute an efficient implementation that we will make available for research
purposes. Compared to existing methods, InstantAvatar converges 130x faster and
can be trained in minutes instead of hours. It achieves comparable or even
better reconstruction quality and novel pose synthesis results. When given the
same time budget, our method significantly outperforms SoTA methods.
InstantAvatar can yield acceptable visual quality in as little as 10 seconds
training time.",None,-1
Geometric Graph Representation Learning via Maximizing Rate Reduction,0.487427,"Learning discriminative node representations benefits various downstream
tasks in graph analysis such as community detection and node classification.
Existing graph representation learning methods (e.g., based on random walk and
contrastive learning) are limited to maximizing the local similarity of
connected nodes. Such pair-wise learning schemes could fail to capture the
global distribution of representations, since it has no explicit constraints on
the global geometric properties of representation space. To this end, we
propose Geometric Graph Representation Learning (G2R) to learn node
representations in an unsupervised manner via maximizing rate reduction. In
this way, G2R maps nodes in distinct groups (implicitly stored in the adjacency
matrix) into different subspaces, while each subspace is compact and different
subspaces are dispersedly distributed. G2R adopts a graph neural network as the
encoder and maximizes the rate reduction with the adjacency matrix.
Furthermore, we theoretically and empirically demonstrate that rate reduction
maximization is equivalent to maximizing the principal angles between different
subspaces. Experiments on real-world datasets show that G2R outperforms various
baselines on node classification and community detection tasks.",None,12156
Optical tracking in team sports,0.798104,"Sports analysis has gained paramount importance for coaches, scouts, and
fans. Recently, computer vision researchers have taken on the challenge of
collecting the necessary data by proposing several methods of automatic player
and ball tracking. Building on the gathered tracking data, data miners are able
to perform quantitative analysis on the performance of players and teams. With
this survey, our goal is to provide a basic understanding for quantitative data
analysts about the process of creating the input data and the characteristics
thereof. Thus, we summarize the recent methods of optical tracking by providing
a comprehensive taxonomy of conventional and deep learning methods, separately.
Moreover, we discuss the preprocessing steps of tracking, the most common
challenges in this domain, and the application of tracking data to sports
teams. Finally, we compare the methods by their cost and limitations, and
conclude the work by highlighting potential future research directions.",None,-1
Eliminating Meta Optimization Through Self-Referential Meta Learning,0.537352,"Meta Learning automates the search for learning algorithms. At the same time,
it creates a dependency on human engineering on the meta-level, where meta
learning algorithms need to be designed. In this paper, we investigate
self-referential meta learning systems that modify themselves without the need
for explicit meta optimization. We discuss the relationship of such systems to
in-context and memory-based meta learning and show that self-referential neural
networks require functionality to be reused in the form of parameter sharing.
Finally, we propose fitness monotonic execution (FME), a simple approach to
avoid explicit meta optimization. A neural network self-modifies to solve
bandit and classic control tasks, improves its self-modifications, and learns
how to learn, purely by assigning more computational resources to better
performing solutions.",None,-1
SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds,0.818192,"With the recent availability and affordability of commercial depth sensors
and 3D scanners, an increasing number of 3D (i.e., RGBD, point cloud) datasets
have been publicized to facilitate research in 3D computer vision. However,
existing datasets either cover relatively small areas or have limited semantic
annotations. Fine-grained understanding of urban-scale 3D scenes is still in
its infancy. In this paper, we introduce SensatUrban, an urban-scale UAV
photogrammetry point cloud dataset consisting of nearly three billion points
collected from three UK cities, covering 7.6 km^2. Each point in the dataset
has been labelled with fine-grained semantic annotations, resulting in a
dataset that is three times the size of the previous existing largest
photogrammetric point cloud dataset. In addition to the more commonly
encountered categories such as road and vegetation, urban-level categories
including rail, bridge, and river are also included in our dataset. Based on
this dataset, we further build a benchmark to evaluate the performance of
state-of-the-art segmentation algorithms. In particular, we provide a
comprehensive analysis and identify several key challenges limiting urban-scale
point cloud understanding. The dataset is available at
http://point-cloud-analysis.cs.ox.ac.uk.",None,-1
Rethinking and Refining the Distinct Metric,0.864665,"Distinct-$n$ score\cite{Li2016} is a widely used automatic metric for
evaluating diversity in language generation tasks. However, we observed that
the original approach for calculating distinct scores has evident biases that
tend to assign higher penalties to longer sequences. We refine the calculation
of distinct scores by scaling the number of distinct tokens based on their
expectations. We provide both empirical and theoretical evidence to show that
our method effectively removes the biases existing in the original distinct
score. Our experiments show that our proposed metric,
\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human
judgment in evaluating response diversity. To foster future research, we
provide an example implementation at
\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.",https://github.com/lsy641/Expectation-Adjusted-Distinct,-1
STDAN: Deformable Attention Network for Space-Time Video Super-Resolution,0.110346,"The target of space-time video super-resolution (STVSR) is to increase the
spatial-temporal resolution of low-resolution (LR) and low frame rate (LFR)
videos. Recent approaches based on deep learning have made significant
improvements, but most of them only use two adjacent frames, that is,
short-term features, to synthesize the missing frame embedding, which cannot
fully explore the information flow of consecutive input LR frames. In addition,
existing STVSR models hardly exploit the temporal contexts explicitly to assist
high-resolution (HR) frame reconstruction. To address these issues, in this
paper, we propose a deformable attention network called STDAN for STVSR. First,
we devise a long-short term feature interpolation (LSTFI) module, which is
capable of excavating abundant content from more neighboring input frames for
the interpolation process through a bidirectional RNN structure. Second, we put
forward a spatial-temporal deformable feature aggregation (STDFA) module, in
which spatial and temporal contexts in dynamic video frames are adaptively
captured and aggregated to enhance SR reconstruction. Experimental results on
several datasets demonstrate that our approach outperforms state-of-the-art
STVSR methods. The code is available at
https://github.com/littlewhitesea/STDAN.",https://github.com/littlewhitesea/STDAN,-1
TraClets: Harnessing the power of computer vision for trajectory classification,0.0789057,"Due to the advent of new mobile devices and tracking sensors in recent years,
huge amounts of data are being produced every day. Therefore, novel
methodologies need to emerge that dive through this vast sea of information and
generate insights and meaningful information. To this end, researchers have
developed several trajectory classification algorithms over the years that are
able to annotate tracking data. Similarly, in this research, a novel
methodology is presented that exploits image representations of trajectories,
called TraClets, in order to classify trajectories in an intuitive humans way,
through computer vision techniques. Several real-world datasets are used to
evaluate the proposed approach and compare its classification performance to
other state-of-the-art trajectory classification algorithms. Experimental
results demonstrate that TraClets achieves a classification performance that is
comparable to, or in most cases, better than the state-of-the-art, acting as a
universal, high-accuracy approach for trajectory classification.",None,-1
ASSIST: Towards Label Noise-Robust Dialogue State Tracking,0.447257,"The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state
tracking (DST). However, substantial noise has been discovered in its state
annotations. Such noise brings about huge challenges for training DST models
robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have
been published recently, there are still lots of noisy labels, especially in
the training set. Besides, it is costly to rectify all the problematic
annotations. In this paper, instead of improving the annotation quality
further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt
dIalogue State Tracking), to train DST models robustly from noisy labels.
ASSIST first generates pseudo labels for each sample in the training set by
using an auxiliary model trained on a small clean dataset, then puts the
generated pseudo labels and vanilla noisy labels together to train the primary
model. We show the validity of ASSIST theoretically. Experimental results also
demonstrate that ASSIST improves the joint goal accuracy of DST by up to
$28.16\%$ on MultiWOZ 2.0 and $8.41\%$ on MultiWOZ 2.4, compared to using only
the vanilla noisy labels.",https://github.com/smartyfh/DST-ASSIST,-1
Outpainting by Queries,0.393231,"Image outpainting, which is well studied with Convolution Neural Network
(CNN) based framework, has recently drawn more attention in computer vision.
However, CNNs rely on inherent inductive biases to achieve effective sample
learning, which may degrade the performance ceiling. In this paper, motivated
by the flexible self-attention mechanism with minimal inductive biases in
transformer architecture, we reframe the generalised image outpainting problem
as a patch-wise sequence-to-sequence autoregression problem, enabling
query-based image outpainting. Specifically, we propose a novel hybrid
vision-transformer-based encoder-decoder framework, named \textbf{Query}
\textbf{O}utpainting \textbf{TR}ansformer (\textbf{QueryOTR}), for
extrapolating visual context all-side around a given image. Patch-wise mode's
global modeling capacity allows us to extrapolate images from the attention
mechanism's query standpoint. A novel Query Expansion Module (QEM) is designed
to integrate information from the predicted queries based on the encoder's
output, hence accelerating the convergence of the pure transformer even with a
relatively small dataset. To further enhance connectivity between each patch,
the proposed Patch Smoothing Module (PSM) re-allocates and averages the
overlapped regions, thus providing seamless predicted images. We experimentally
show that QueryOTR could generate visually appealing results smoothly and
realistically against the state-of-the-art image outpainting approaches.",https://github.com/Kaiseem/QueryOTR,-1
Learned Digital Back-Propagation for Dual-Polarization Dispersion Managed Systems,0.111187,"Digital back-propagation (DBP) and learned DBP (LDBP) are proposed for
nonlinearity mitigation in WDM dual-polarization dispersion-managed systems.
LDBP achieves Q-factor improvement of 1.8 dB and 1.2 dB, respectively, over
linear equalization and a variant of DBP adapted to DM systems.",None,-1
EnvEdit: Environment Editing for Vision-and-Language Navigation,0.72194,"In Vision-and-Language Navigation (VLN), an agent needs to navigate through
the environment based on natural language instructions. Due to limited
available data for agent training and finite diversity in navigation
environments, it is challenging for the agent to generalize to new, unseen
environments. To address this problem, we propose EnvEdit, a data augmentation
method that creates new environments by editing existing environments, which
are used to train a more generalizable agent. Our augmented environments can
differ from the seen environments in three diverse aspects: style, object
appearance, and object classes. Training on these edit-augmented environments
prevents the agent from overfitting to existing environments and helps
generalize better to new, unseen environments. Empirically, on both the
Room-to-Room and the multi-lingual Room-Across-Room datasets, we show that our
proposed EnvEdit method gets significant improvements in all metrics on both
pre-trained and non-pre-trained VLN agents, and achieves the new
state-of-the-art on the test leaderboard. We further ensemble the VLN agents
augmented on different edited environments and show that these edit methods are
complementary. Code and data are available at
https://github.com/jialuli-luka/EnvEdit",https://github.com/jialuli-luka/EnvEdit,-1
Improving Chinese Story Generation via Awareness of Syntactic Dependencies and Semantics,0.784396,"Story generation aims to generate a long narrative conditioned on a given
input. In spite of the success of prior works with the application of
pre-trained models, current neural models for Chinese stories still struggle to
generate high-quality long text narratives. We hypothesise that this stems from
ambiguity in syntactically parsing the Chinese language, which does not have
explicit delimiters for word segmentation. Consequently, neural models suffer
from the inefficient capturing of features in Chinese narratives. In this
paper, we present a new generation framework that enhances the feature
capturing mechanism by informing the generation model of dependencies between
words and additionally augmenting the semantic representation learning through
synonym denoising training. We conduct a range of experiments, and the results
demonstrate that our framework outperforms the state-of-the-art Chinese
generation models on all evaluation metrics, demonstrating the benefits of
enhanced dependency and semantic representation learning.",https://github.com/hehedaozuiteng/Chinese-Story-Generation,-1
Unsupervised Change Detection Based on Image Reconstruction Loss,0.683269,"To train the change detector, bi-temporal images taken at different times in
the same area are used. However, collecting labeled bi-temporal images is
expensive and time consuming. To solve this problem, various unsupervised
change detection methods have been proposed, but they still require unlabeled
bi-temporal images. In this paper, we propose unsupervised change detection
based on image reconstruction loss using only unlabeled single temporal single
image. The image reconstruction model is trained to reconstruct the original
source image by receiving the source image and the photometrically transformed
source image as a pair. During inference, the model receives bi-temporal images
as the input, and tries to reconstruct one of the inputs. The changed region
between bi-temporal images shows high reconstruction loss. Our change detector
showed significant performance in various change detection benchmark datasets
even though only a single temporal single source image was used. The code and
trained models will be publicly available for reproducibility.",https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix,-1
A Transfer Learning Based Model for Text Readability Assessment in German,0.618049,"Text readability assessment has a wide range of applications for different
target people, from language learners to people with disabilities. The fast
pace of textual content production on the web makes it impossible to measure
text complexity without the benefit of machine learning and natural language
processing techniques. Although various research addressed the readability
assessment of English text in recent years, there is still room for improvement
of the models for other languages. In this paper, we proposed a new model for
text complexity assessment for German text based on transfer learning. Our
results show that the model outperforms more classical solutions based on
linguistic features extraction from input text. The best model is based on the
BERT pre-trained language model achieved the Root Mean Square Error (RMSE) of
0.483.",None,12355
Housekeep: Tidying Virtual Households using Commonsense Reasoning,0.719296,"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the
home for embodied AI. In Housekeep, an embodied agent must tidy a house by
rearranging misplaced objects without explicit instructions specifying which
objects need to be rearranged. Instead, the agent must learn from and is
evaluated against human preferences of which objects belong where in a tidy
house. Specifically, we collect a dataset of where humans typically place
objects in tidy and untidy houses constituting 1799 objects, 268 object
categories, 585 placements, and 105 rooms. Next, we propose a modular baseline
approach for Housekeep that integrates planning, exploration, and navigation.
It leverages a fine-tuned large language model (LLM) trained on an internet
text corpus for effective planning. We show that our baseline agent generalizes
to rearranging unseen objects in unknown environments. See our webpage for more
details: https://yashkant.github.io/housekeep/",None,-1
Cross-TOP: Zero-Shot Cross-Schema Task-Oriented Parsing,0.121533,"Deep learning methods have enabled task-oriented semantic parsing of
increasingly complex utterances. However, a single model is still typically
trained and deployed for each task separately, requiring labeled training data
for each, which makes it challenging to support new tasks, even within a single
business vertical (e.g., food-ordering or travel booking). In this paper we
describe Cross-TOP (Cross-Schema Task-Oriented Parsing), a zero-shot method for
complex semantic parsing in a given vertical. By leveraging the fact that user
requests from the same vertical share lexical and semantic similarities, a
single cross-schema parser is trained to service an arbitrary number of tasks,
seen or unseen, within a vertical. We show that Cross-TOP can achieve high
accuracy on a previously unseen task without requiring any additional training
data, thereby providing a scalable way to bootstrap semantic parsers for new
tasks. As part of this work we release the FoodOrdering dataset, a
task-oriented parsing dataset in the food-ordering vertical, with utterances
and annotations derived from five schemas, each from a different restaurant
menu.",https://github.com/amazon-research/food-ordering-semantic-parsing-dataset,-1
Detection of Distracted Driver using Convolution Neural Network,0.558168,"With over 50 million car sales annually and over 1.3 million deaths every
year due to motor accidents we have chosen this space. India accounts for 11
per cent of global death in road accidents. Drivers are held responsible for
78% of accidents. Road safety problems in developing countries is a major
concern and human behavior is ascribed as one of the main causes and
accelerators of road safety problems. Driver distraction has been identified as
the main reason for accidents. Distractions can be caused due to reasons such
as mobile usage, drinking, operating instruments, facial makeup, social
interaction. For the scope of this project, we will focus on building a highly
efficient ML model to classify different driver distractions at runtime using
computer vision. We would also analyze the overall speed and scalability of the
model in order to be able to set it up on an edge device. We use CNN, VGG-16,
RestNet50 and ensemble of CNN to predict the classes.",None,-1
The Role of ImageNet Classes in Fréchet Inception Distance,0.99988,"Fr\'echet Inception Distance (FID) is the primary metric for ranking models
in data-driven generative modeling. While remarkably successful, the metric is
known to sometimes disagree with human judgement. We investigate a root cause
of these discrepancies, and visualize what FID ""looks at"" in generated images.
We show that the feature space that FID is (typically) computed in is so close
to the ImageNet classifications that aligning the histograms of Top-$N$
classifications between sets of generated and real images can reduce FID
substantially -- without actually improving the quality of results. Thus, we
conclude that FID is prone to intentional or accidental distortions. As a
practical example of an accidental distortion, we discuss a case where an
ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while
being worse in terms of human evaluation.",https://github.com/kynkaat/role-of-imagenet-classes-in-fid,44899
QuantFace: Towards Lightweight Face Recognition by Synthetic Data Low-bit Quantization,0.874404,"Deep learning-based face recognition models follow the common trend in deep
neural networks by utilizing full-precision floating-point networks with high
computational costs. Deploying such networks in use-cases constrained by
computational requirements is often infeasible due to the large memory required
by the full-precision model. Previous compact face recognition approaches
proposed to design special compact architectures and train them from scratch
using real training data, which may not be available in a real-world scenario
due to privacy concerns. We present in this work the QuantFace solution based
on low-bit precision format model quantization. QuantFace reduces the required
computational cost of the existing face recognition models without the need for
designing a particular architecture or accessing real training data. QuantFace
introduces privacy-friendly synthetic face data to the quantization process to
mitigate potential privacy concerns and issues related to the accessibility to
real training data. Through extensive evaluation experiments on seven
benchmarks and four network architectures, we demonstrate that QuantFace can
successfully reduce the model size up to 5x while maintaining, to a large
degree, the verification performance of the full-precision model without
accessing real training datasets.",None,-1
Acknowledging the Unknown for Multi-label Learning with Single Positive Labels,0.90977,"Due to the difficulty of collecting exhaustive multi-label annotations,
multi-label datasets often contain partial labels. We consider an extreme of
this weakly supervised learning problem, called single positive multi-label
learning (SPML), where each multi-label training image has only one positive
label. Traditionally, all unannotated labels are assumed as negative labels in
SPML, which introduces false negative labels and causes model training to be
dominated by assumed negative labels. In this work, we choose to treat all
unannotated labels from an alternative perspective, i.e. acknowledging they are
unknown. Hence, we propose entropy-maximization (EM) loss to attain a special
gradient regime for providing proper supervision signals. Moreover, we propose
asymmetric pseudo-labeling (APL), which adopts asymmetric-tolerance strategies
and a self-paced procedure, to cooperate with EM loss and then provide more
precise supervision. Experiments show that our method significantly improves
performance and achieves state-of-the-art results on all four benchmarks. Code
is available at https://github.com/Correr-Zhou/SPML-AckTheUnknown.",https://github.com/Correr-Zhou/SPML-AckTheUnknown,-1
PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model,0.647594,"In this paper, we introduce PanGu-Bot, a Chinese pre-trained open-domain
dialogue generation model based on a large pre-trained language model (PLM)
PANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue
models trained over a massive amount of dialogue data from scratch, we aim to
build a powerful dialogue model with relatively fewer data and computation
costs by inheriting valuable language capabilities and knowledge from PLMs. To
this end, we train PanGu-Bot from the large PLM PANGU-alpha, which has been
proven well-performed on a variety of Chinese natural language tasks. We
investigate different aspects of responses generated by PanGu-Bot, including
response quality, knowledge, and safety. We show that PanGu-Bot outperforms
state-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA
(Zhou et al., 2021), EVA2.0 (Gu et al., 2022)) w.r.t. the above three aspects.
We also demonstrate that PanGu-Bot can be easily deployed to generate emotional
responses without further training. Throughout our empirical analysis, we also
point out that the PanGu-Bot response quality, knowledge correctness, and
safety are still far from perfect, and further explorations are indispensable
to building reliable and smart dialogue systems. Our model and code will be
available at
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot
soon.",https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot,-1
PLM-ICD: Automatic ICD Coding with Pretrained Language Models,0.523438,"Automatically classifying electronic health records (EHRs) into diagnostic
codes has been challenging to the NLP community. State-of-the-art methods
treated this problem as a multilabel classification problem and proposed
various architectures to model this problem. However, these systems did not
leverage the superb performance of pretrained language models, which achieved
superb performance on natural language understanding tasks. Prior work has
shown that pretrained language models underperformed on this task with the
regular finetuning scheme. Therefore, this paper aims at analyzing the causes
of the underperformance and developing a framework for automatic ICD coding
with pretrained language models. We spotted three main issues through the
experiments: 1) large label space, 2) long input sequences, and 3) domain
mismatch between pretraining and fine-tuning. We propose PLMICD, a framework
that tackles the challenges with various strategies. The experimental results
show that our proposed framework can overcome the challenges and achieves
state-of-the-art performance in terms of multiple metrics on the benchmark
MIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD",https://github.com/MiuLab/PLM-ICD,-1
On the Shift Invariance of Max Pooling Feature Maps in Convolutional Neural Networks,0.0928107,"This paper focuses on improving the mathematical interpretability of
convolutional neural networks (CNNs) in the context of image classification.
Specifically, we tackle the instability issue arising in their first layer,
which tends to learn parameters that closely resemble oriented band-pass
filters when trained on datasets like ImageNet. Subsampled convolutions with
such Gabor-like filters are prone to aliasing, causing sensitivity to small
input shifts. In this context, we establish conditions under which the max
pooling operator approximates a complex modulus, which is nearly shift
invariant. We then derive a measure of shift invariance for subsampled
convolutions followed by max pooling. In particular, we highlight the crucial
role played by the filter's frequency and orientation in achieving stability.
We experimentally validate our theory by considering a deterministic feature
extractor based on the dual-tree complex wavelet packet transform, a particular
case of discrete Gabor-like decomposition.",None,-1
FreGAN: Exploiting Frequency Components for Training GANs under Limited Data,0.862824,"Training GANs under limited data often leads to discriminator overfitting and
memorization issues, causing divergent training. Existing approaches mitigate
the overfitting by employing data augmentations, model regularization, or
attention mechanisms. However, they ignore the frequency bias of GANs and take
poor consideration towards frequency information, especially high-frequency
signals that contain rich details. To fully utilize the frequency information
of limited data, this paper proposes FreGAN, which raises the model's frequency
awareness and draws more attention to producing high-frequency signals,
facilitating high-quality generation. In addition to exploiting both real and
generated images' frequency information, we also involve the frequency signals
of real images as a self-supervised constraint, which alleviates the GAN
disequilibrium and encourages the generator to synthesize adequate rather than
arbitrary frequency signals. Extensive results demonstrate the superiority and
effectiveness of our FreGAN in ameliorating generation quality in the low-data
regime (especially when training data is less than 100). Besides, FreGAN can be
seamlessly applied to existing regularization and attention mechanism models to
further boost the performance.",https://github.com/kobeshegu/FreGAN_NeurIPS2022,-1
Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems,0.518913,"Users interacting with voice assistants today need to phrase their requests
in a very specific manner to elicit an appropriate response. This limits the
user experience, and is partly due to the lack of reasoning capabilities of
dialogue platforms and the hand-crafted rules that require extensive labor. One
possible way to improve user experience and relieve the manual efforts of
designers is to build an end-to-end dialogue system that can do reasoning
itself while perceiving user's utterances. In this work, we propose a novel
method to incorporate the knowledge reasoning capability into dialogue systems
in a more scalable and generalizable manner. Our proposed method allows a
single transformer model to directly walk on a large-scale knowledge graph to
generate responses. To the best of our knowledge, this is the first work to
have transformer models generate responses by reasoning over differentiable
knowledge graphs. We investigate the reasoning abilities of the proposed method
on both task-oriented and domain-specific chit-chat dialogues. Empirical
results show that this method can effectively and efficiently incorporate a
knowledge graph into a dialogue system with fully-interpretable reasoning
paths.",https://github.com/Pascalson/DiffKG-Dialog,-1
Excavating RoI Attention for Underwater Object Detection,0.30269,"Self-attention is one of the most successful designs in deep learning, which
calculates the similarity of different tokens and reconstructs the feature
based on the attention matrix. Originally designed for NLP, self-attention is
also popular in computer vision, and can be categorized into pixel-level
attention and patch-level attention. In object detection, RoI features can be
seen as patches from base feature maps. This paper aims to apply the attention
module to RoI features to improve performance. Instead of employing an original
self-attention module, we choose the external attention module, a modified
self-attention with reduced parameters. With the proposed double head structure
and the Positional Encoding module, our method can achieve promising
performance in object detection. The comprehensive experiments show that it
achieves promising performance, especially in the underwater object detection
dataset. The code will be avaiable in:
https://github.com/zsyasd/Excavating-RoI-Attention-for-Underwater-Object-Detection",https://github.com/zsyasd/Excavating-RoI-Attention-for-Underwater-Object-Detection,-1
Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer,0.595334,"In Neural Machine Translation (NMT), each token prediction is conditioned on
the source sentence and the target prefix (what has been previously translated
at a decoding step). However, previous work on interpretability in NMT has
mainly focused solely on source sentence tokens' attributions. Therefore, we
lack a full understanding of the influences of every input token (source
sentence and target prefix) in the model predictions. In this work, we propose
an interpretability method that tracks input tokens' attributions for both
contexts. Our method, which can be extended to any encoder-decoder
Transformer-based model, allows us to better comprehend the inner workings of
current NMT models. We apply the proposed method to both bilingual and
multilingual Transformers and present insights into their behaviour.",https://github.com/mt-upc/,-1
User-Controllable Latent Transformer for StyleGAN Image Layout Editing,0.568531,"Latent space exploration is a technique that discovers interpretable latent
directions and manipulates latent codes to edit various attributes in images
generated by generative adversarial networks (GANs). However, in previous work,
spatial control is limited to simple transformations (e.g., translation and
rotation), and it is laborious to identify appropriate latent directions and
adjust their parameters. In this paper, we tackle the problem of editing the
StyleGAN image layout by annotating the image directly. To do so, we propose an
interactive framework for manipulating latent codes in accordance with the user
inputs. In our framework, the user annotates a StyleGAN image with locations
they want to move or not and specifies a movement direction by mouse dragging.
From these user inputs and initial latent codes, our latent transformer based
on a transformer encoder-decoder architecture estimates the output latent
codes, which are fed to the StyleGAN generator to obtain a result image. To
train our latent transformer, we utilize synthetic data and pseudo-user inputs
generated by off-the-shelf StyleGAN and optical flow models, without manual
supervision. Quantitative and qualitative evaluations demonstrate the
effectiveness of our method over existing methods.",https://github.com/justinpinkney/awesome-pretrained-stylegan2,-1
"EMMT: A simultaneous eye-tracking, 4-electrode EEG and audio corpus for multi-modal reading and translation scenarios",0.353492,"We present the Eyetracked Multi-Modal Translation (EMMT) corpus, a dataset
containing monocular eye movement recordings, audio and 4-electrode
electroencephalogram (EEG) data of 43 participants. The objective was to
collect cognitive signals as responses of participants engaged in a number of
language intensive tasks involving different text-image stimuli settings when
translating from English to Czech.
  Each participant was exposed to 32 text-image stimuli pairs and asked to (1)
read the English sentence, (2) translate it into Czech, (3) consult the image,
(4) translate again, either updating or repeating the previous translation. The
text stimuli consisted of 200 unique sentences with 616 unique words coupled
with 200 unique images as the visual stimuli.
  The recordings were collected over a two week period and all the participants
included in the study were Czech natives with strong English skills. Due to the
nature of the tasks involved in the study and the relatively large number of
participants involved, the corpus is well suited for research in Translation
Process Studies, Cognitive Sciences among other disciplines.",https://github.com/NEUREM3/recording-code-for-eyetracked-multi-modal-translation,-1
Detecting Deepfake by Creating Spatio-Temporal Regularity Disruption,0.502081,"Despite encouraging progress in deepfake detection, generalization to unseen
forgery types remains a significant challenge due to the limited forgery clues
explored during training. In contrast, we notice a common phenomenon in
deepfake: fake video creation inevitably disrupts the statistical regularity in
original videos. Inspired by this observation, we propose to boost the
generalization of deepfake detection by distinguishing the ""regularity
disruption"" that does not appear in real videos. Specifically, by carefully
examining the spatial and temporal properties, we propose to disrupt a real
video through a Pseudo-fake Generator and create a wide range of pseudo-fake
videos for training. Such practice allows us to achieve deepfake detection
without using fake videos and improves the generalization ability in a simple
and efficient manner. To jointly capture the spatial and temporal disruptions,
we propose a Spatio-Temporal Enhancement block to learn the regularity
disruption across space and time on our self-created videos. Through
comprehensive experiments, our method exhibits excellent performance on several
datasets.",https://github.com/MarekKowalski/FaceSwap,-1
Few-shot Object Detection with Refined Contrastive Learning,0.217059,"Due to the scarcity of sampling data in reality, few-shot object detection
(FSOD) has drawn more and more attention because of its ability to quickly
train new detection concepts with less data. However, there are still failure
identifications due to the difficulty in distinguishing confusable classes. We
also notice that the high standard deviation of average precision reveals the
inconsistent detection performance. To this end, we propose a novel FSOD method
with Refined Contrastive Learning (FSRC). A pre-determination component is
introduced to find out the Resemblance Group from novel classes which contains
confusable classes. Afterwards, Refined Contrastive Learning (RCL) is pointedly
performed on this group of classes in order to increase the inter-class
distances among them. In the meantime, the detection results distribute more
uniformly which further improve the performance. Experimental results based on
PASCAL VOC and COCO datasets demonstrate our proposed method outperforms the
current state-of-the-art research.",None,-1
Interpretation Quality Score for Measuring the Quality of interpretability methods,0.210828,"Machine learning (ML) models have been applied to a wide range of natural
language processing (NLP) tasks in recent years. In addition to making accurate
decisions, the necessity of understanding how models make their decisions has
become apparent in many applications. To that end, many interpretability
methods that help explain the decision processes of ML models have been
developed. Yet, there currently exists no widely-accepted metric to evaluate
the quality of explanations generated by these methods. As a result, there
currently is no standard way of measuring to what degree an interpretability
method achieves an intended objective. Moreover, there is no accepted standard
of performance by which we can compare and rank the current existing
interpretability methods. In this paper, we propose a novel metric for
quantifying the quality of explanations generated by interpretability methods.
We compute the metric on three NLP tasks using six interpretability methods and
present our results.",None,12089
A Semantic Framework for Neural-Symbolic Computing,0.112222,"Two approaches to AI, neural networks and symbolic systems, have been proven
very successful for an array of AI problems. However, neither has been able to
achieve the general reasoning ability required for human-like intelligence. It
has been argued that this is due to inherent weaknesses in each approach.
Luckily, these weaknesses appear to be complementary, with symbolic systems
being adept at the kinds of things neural networks have trouble with and
vice-versa. The field of neural-symbolic AI attempts to exploit this asymmetry
by combining neural networks and symbolic AI into integrated systems. Often
this has been done by encoding symbolic knowledge into neural networks.
Unfortunately, although many different methods for this have been proposed,
there is no common definition of an encoding to compare them. We seek to
rectify this problem by introducing a semantic framework for neural-symbolic
AI, which is then shown to be general enough to account for a large family of
neural-symbolic systems. We provide a number of examples and proofs of the
application of the framework to the neural encoding of various forms of
knowledge representation and neural network. These, at first sight disparate
approaches, are all shown to fall within the framework's formal definition of
what we call semantic encoding for neural-symbolic AI.",None,-1
Code Switched and Code Mixed Speech Recognition for Indic languages,0.162029,"Training multilingual automatic speech recognition (ASR) systems is
challenging because acoustic and lexical information is typically language
specific. Training multilingual system for Indic languages is even more tougher
due to lack of open source datasets and results on different approaches. We
compare the performance of end to end multilingual speech recognition system to
the performance of monolingual models conditioned on language identification
(LID). The decoding information from a multilingual model is used for language
identification and then combined with monolingual models to get an improvement
of 50% WER across languages. We also propose a similar technique to solve the
Code Switched problem and achieve a WER of 21.77 and 28.27 over Hindi-English
and Bengali-English respectively. Our work talks on how transformer based ASR
especially wav2vec 2.0 can be applied in developing multilingual ASR and code
switched ASR for Indic languages.",https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation,-1
Meta-Learning a Cross-lingual Manifold for Semantic Parsing,0.710179,"Localizing a semantic parser to support new languages requires effective
cross-lingual generalization. Recent work has found success with
machine-translation or zero-shot methods although these approaches can struggle
to model how native speakers ask questions. We consider how to effectively
leverage minimal annotated examples in new languages for few-shot cross-lingual
semantic parsing. We introduce a first-order meta-learning algorithm to train a
semantic parser with maximal sample efficiency during cross-lingual transfer.
Our algorithm uses high-resource languages to train the parser and
simultaneously optimizes for cross-lingual generalization for lower-resource
languages. Results across six languages on ATIS demonstrate that our
combination of generalization steps yields accurate semantic parsers sampling
$\le$10% of source training data in each new language. Our approach also trains
a competitive model on Spider using English with generalization to Chinese
similarly sampling $\le$10% of training data.",https://github.com/tomsherborne/xgr,-1
METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals,0.323764,"We present an efficient method of pretraining large-scale autoencoding
language models using training signals generated by an auxiliary model.
Originated in ELECTRA, this training strategy has demonstrated
sample-efficiency to pretrain models at the scale of hundreds of millions of
parameters. In this work, we conduct a comprehensive empirical study, and
propose a recipe, namely ""Model generated dEnoising TRaining Objective""
(METRO), which incorporates some of the best modeling techniques developed
recently to speed up, stabilize, and enhance pretrained language models without
compromising model effectiveness. The resultant models, METRO-LM, consisting of
up to 5.4 billion parameters, achieve new state-of-the-art on the GLUE,
SuperGLUE, and SQuAD benchmarks. More importantly, METRO-LM are efficient in
that they often outperform previous large models with significantly smaller
model sizes and lower pretraining cost.",https://github.com/namisan/mt-dnn/tree/master/experiments/superglue,82331
FaceDancer: Pose- and Occlusion-Aware High Fidelity Face Swapping,0.546002,"In this work, we present a new single-stage method for subject agnostic face
swapping and identity transfer, named FaceDancer. We have two major
contributions: Adaptive Feature Fusion Attention (AFFA) and Interpreted Feature
Similarity Regularization (IFSR). The AFFA module is embedded in the decoder
and adaptively learns to fuse attribute features and features conditioned on
identity information without requiring any additional facial segmentation
process. In IFSR, we leverage the intermediate features in an identity encoder
to preserve important attributes such as head pose, facial expression,
lighting, and occlusion in the target face, while still transferring the
identity of the source face with high fidelity. We conduct extensive
quantitative and qualitative experiments on various datasets and show that the
proposed FaceDancer outperforms other state-of-the-art networks in terms of
identityn transfer, while having significantly better pose preservation than
most of the previous methods.",https://github.com/felixrosberg/FaceDancer,-1
Exhaustivity and anti-exhaustivity in the RSA framework: Testing the effect of prior beliefs,0.318134,"During communication, the interpretation of utterances is sensitive to a
listener's probabilistic prior beliefs, something which is captured by one
currently influential model of pragmatics, the Rational Speech Act (RSA)
framework. In this paper we focus on cases when this sensitivity to priors
leads to counterintuitive predictions of the framework. Our domain of interest
is exhaustivity effects, whereby a sentence such as ""Mary came"" is understood
to mean that only Mary came. We show that in the baseline RSA model, under
certain conditions, anti-exhaustive readings are predicted (e.g., ""Mary came""
would be used to convey that both Mary and Peter came). The specific question
we ask is the following: should exhaustive interpretations be derived as purely
pragmatic inferences (as in the classical Gricean view, endorsed in the
baseline RSA model), or should they rather be generated by an encapsulated
semantic mechanism (as argued in some of the recent formal literature)? To
answer this question, we provide a detailed theoretical analysis of different
RSA models and evaluate them against data obtained in a new study which tested
the effects of prior beliefs on both production and comprehension, improving on
previous empirical work. We found no anti-exhaustivity effects, but observed
that message choice is sensitive to priors, as predicted by the RSA framework
overall. The best models turn out to be those which include an encapsulated
exhaustivity mechanism (as other studies concluded on the basis of very
different data). We conclude that, on the one hand, in the division of labor
between semantics and pragmatics, semantics plays a larger role than is often
thought, but, on the other hand, the tradeoff between informativity and cost
which characterizes all RSA models does play a central role for genuine
pragmatic effects.",https://github.com/Alex-Cremers/RSA-Exh-Priors,-1
Intelligent Painter: Picture Composition With Resampling Diffusion Model,0.364044,"Have you ever thought that you can be an intelligent painter? This means that
you can paint a picture with a few expected objects in mind, or with a
desirable scene. This is different from normal inpainting approaches for which
the location of specific objects cannot be determined. In this paper, we
present an intelligent painter that generate a person's imaginary scene in one
go, given explicit hints. We propose a resampling strategy for Denoising
Diffusion Probabilistic Model (DDPM) to intelligently compose unconditional
harmonized pictures according to the input subjects at specific locations. By
exploiting the diffusion property, we resample efficiently to produce realistic
pictures. Experimental results show that our resampling method favors the
semantic meaning of the generated output efficiently and generates less blurry
output. Quantitative analysis of image quality assessment shows that our method
produces higher perceptual quality images compared with the state-of-the-art
methods.",https://github.com/vinesmsuic/ipainter-diffusion,-1
QAGAN: Adversarial Approach To Learning Domain Invariant Language Features,0.212678,"Training models that are robust to data domain shift has gained an increasing
interest both in academia and industry. Question-Answering language models,
being one of the typical problem in Natural Language Processing (NLP) research,
has received much success with the advent of large transformer models. However,
existing approaches mostly work under the assumption that data is drawn from
same distribution during training and testing which is unrealistic and
non-scalable in the wild.
  In this paper, we explore adversarial training approach towards learning
domain-invariant features so that language models can generalize well to
out-of-domain datasets. We also inspect various other ways to boost our model
performance including data augmentation by paraphrasing sentences, conditioning
end of answer span prediction on the start word, and carefully designed
annealing function. Our initial results show that in combination with these
methods, we are able to achieve $15.2\%$ improvement in EM score and $5.6\%$
boost in F1 score on out-of-domain validation dataset over the baseline. We
also dissect our model outputs and visualize the model hidden-states by
projecting them onto a lower-dimensional space, and discover that our specific
adversarial training approach indeed encourages the model to learn domain
invariant embedding and bring them closer in the multi-dimensional space.",https://github.com/towardsautonomy/QAGAN,-1
Classification Of Fake News Headline Based On Neural Networks,0.0319084,"Over the last few years, Text classification is one of the fundamental tasks
in natural language processing (NLP) in which the objective is to categorize
text documents into one of the predefined classes. The news is full of our
life. Therefore, news headlines classification is a crucial task to connect
users with the right news. The news headline classification is a kind of text
classification, which can be generally divided into three mainly parts: feature
extraction, classifier selection, and evaluations. In this article, we use the
dataset, containing news over a period of eighteen years provided by Kaggle
platform to classify news headlines. We choose TF-IDF to extract features and
neural network as the classifier, while the evaluation metrics is accuracy.
From the experiment result, it is obvious that our NN model has the best
performance among these models in the metrics of accuracy. The higher the
accuracy is, the better performance the model will gain. Our NN model owns the
accuracy 0.8622, which is highest accuracy among these four models. And it is
0.0134, 0.033, 0.080 higher than its of other models.",None,33
Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions,0.63702,"Generalization across different environments with the same tasks is critical
for successful applications of visual reinforcement learning (RL) in real
scenarios. However, visual distractions -- which are common in real scenes --
from high-dimensional observations can be hurtful to the learned
representations in visual RL, thus degrading the performance of generalization.
To tackle this problem, we propose a novel approach, namely Characteristic
Reward Sequence Prediction (CRESP), to extract the task-relevant information by
learning reward sequence distributions (RSDs), as the reward signals are
task-relevant in RL and invariant to visual distractions. Specifically, to
effectively capture the task-relevant information via RSDs, CRESP introduces an
auxiliary task -- that is, predicting the characteristic functions of RSDs --
to learn task-relevant representations, because we can well approximate the
high-dimensional distributions by leveraging the corresponding characteristic
functions. Experiments demonstrate that CRESP significantly improves the
performance of generalization on unseen environments, outperforming several
state-of-the-arts on DeepMind Control tasks with different visual distractions.",None,-1
Aligned Weight Regularizers for Pruning Pretrained Neural Networks,0.143807,"While various avenues of research have been explored for iterative pruning,
little is known what effect pruning has on zero-shot test performance and its
potential implications on the choice of pruning criteria. This pruning setup is
particularly important for cross-lingual models that implicitly learn alignment
between language representations during pretraining, which if distorted via
pruning, not only leads to poorer performance on language data used for
retraining but also on zero-shot languages that are evaluated.
  In this work, we show that there is a clear performance discrepancy in
magnitude-based pruning when comparing standard supervised learning to the
zero-shot setting. From this finding, we propose two weight regularizers that
aim to maximize the alignment between units of pruned and unpruned networks to
mitigate alignment distortion in pruned cross-lingual models and perform well
for both non zero-shot and zero-shot settings.
  We provide experimental results on cross-lingual tasks for the zero-shot
setting using XLM-RoBERTa$_{\mathrm{Base}}$, where we also find that pruning
has varying degrees of representational degradation depending on the language
corresponding to the zero-shot test set. This is also the first study that
focuses on cross-lingual language model compression.",None,-1
Feature-Level Debiased Natural Language Understanding,0.324004,"Natural language understanding (NLU) models often rely on dataset biases
rather than intended task-relevant features to achieve high performance on
specific datasets. As a result, these models perform poorly on datasets outside
the training distribution. Some recent studies address this issue by reducing
the weights of biased samples during the training process. However, these
methods still encode biased latent features in representations and neglect the
dynamic nature of bias, which hinders model prediction. We propose an NLU
debiasing method, named debiasing contrastive learning (DCT), to simultaneously
alleviate the above problems based on contrastive learning. We devise a
debiasing, positive sampling strategy to mitigate biased latent features by
selecting the least similar biased positive samples. We also propose a dynamic
negative sampling strategy to capture the dynamic influence of biases by
employing a bias-only model to dynamically select the most similar biased
negative samples. We conduct experiments on three NLU benchmark datasets.
Experimental results show that DCT outperforms state-of-the-art baselines on
out-of-distribution datasets while maintaining in-distribution performance. We
also verify that DCT can reduce biased latent features from the model's
representation.",https://github.com/youganglyu/DCT,-1
BYEL : Bootstrap Your Emotion Latent,0.153907,"With the improved performance of deep learning, the number of studies trying
to apply deep learning to human emotion analysis is increasing rapidly. But
even with this trend going on, it is still difficult to obtain high-quality
images and annotations. For this reason, the Learning from Synthetic Data (LSD)
Challenge, which learns from synthetic images and infers from real images, is
one of the most interesting areas. In general, Domain Adaptation methods are
widely used to address LSD challenges, but there is a limitation that target
domains (real images) are still needed. Focusing on these limitations, we
propose a framework Bootstrap Your Emotion Latent (BYEL), which uses only
synthetic images in training. BYEL is implemented by adding Emotion Classifiers
and Emotion Vector Subtraction to the BYOL framework that performs well in
Self-Supervised Representation Learning. We train our framework using synthetic
images generated from the Aff-wild2 dataset and evaluate it using real images
from the Aff-wild2 dataset. The result shows that our framework (0.3084)
performs 2.8% higher than the baseline (0.3) on the macro F1 score metric.",None,-1
DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation,0.372294,"Prompt learning with immensely large Casual Language Models (CLMs) has been
shown promising for attribute-controllable text generation (CTG). However,
vanilla prompt tuning tends to imitate training corpus characteristics beyond
the control attributes, resulting in a poor generalization ability. Moreover,
it is less able to capture the relationship between different attributes,
further limiting the control performance. In this paper, we propose a new CTG
approach, namely DisCup, which incorporates the attribute knowledge of
discriminator to optimize the control-prompts, steering a frozen CLM to produce
attribute-specific texts. Specifically, the frozen CLM model, capable of
producing multitudinous texts, is first used to generate the next-token
candidates based on the context, so as to ensure the diversity of tokens to be
predicted. Then, we leverage an attribute-discriminator to select
desired/undesired tokens from those candidates, providing the inter-attribute
knowledge. Finally, we bridge the above two traits by an unlikelihood objective
for prompt-tuning. Extensive experimental results show that DisCup can achieve
a new state-of-the-art control performance while maintaining an efficient and
high-quality text generation, only relying on around 10 virtual tokens.",https://github.com/littlehacker26/disc-cooperative-up-tuning,403
Practical Adversarial Attacks on Spatiotemporal Traffic Forecasting Models,0.897133,"Machine learning based traffic forecasting models leverage sophisticated
spatiotemporal auto-correlations to provide accurate predictions of city-wide
traffic states. However, existing methods assume a reliable and unbiased
forecasting environment, which is not always available in the wild. In this
work, we investigate the vulnerability of spatiotemporal traffic forecasting
models and propose a practical adversarial spatiotemporal attack framework.
Specifically, instead of simultaneously attacking all geo-distributed data
sources, an iterative gradient-guided node saliency method is proposed to
identify the time-dependent set of victim nodes. Furthermore, we devise a
spatiotemporal gradient descent based scheme to generate real-valued
adversarial traffic states under a perturbation constraint. Meanwhile, we
theoretically demonstrate the worst performance bound of adversarial traffic
forecasting attacks. Extensive experiments on two real-world datasets show that
the proposed two-step framework achieves up to $67.8\%$ performance degradation
on various advanced spatiotemporal forecasting models. Remarkably, we also show
that adversarial training with our proposed attacks can significantly improve
the robustness of spatiotemporal traffic forecasting models. Our code is
available in \url{https://github.com/luckyfan-cs/ASTFA}.",https://github.com/kdd-hkust/Adv-ST,-1
Improving Persian Relation Extraction Models by Data Augmentation,0.101971,"Relation extraction that is the task of predicting semantic relation type
between entities in a sentence or document is an important task in natural
language processing. Although there are many researches and datasets for
English, Persian suffers from sufficient researches and comprehensive datasets.
The only available Persian dataset for this task is PERLEX, which is a Persian
expert-translated version of the SemEval-2010-Task-8 dataset. In this paper, we
present our augmented dataset and the results and findings of our system,
participated in the Persian relation Extraction shared task of NSURL 2021
workshop. We use PERLEX as the base dataset and enhance it by applying some
text preprocessing steps and by increasing its size via data augmentation
techniques to improve the generalization and robustness of applied models. We
then employ two different models including ParsBERT and multilingual BERT for
relation extraction on the augmented PERLEX dataset. Our best model obtained
64.67% of Macro-F1 on the test phase of the contest and it achieved 83.68% of
Macro-F1 on the test set of PERLEX.",None,-1
Bayes risk CTC: Controllable CTC alignment in Sequence-to-Sequence tasks,0.793711,"Sequence-to-Sequence (seq2seq) tasks transcribe the input sequence to a
target sequence. The Connectionist Temporal Classification (CTC) criterion is
widely used in multiple seq2seq tasks. Besides predicting the target sequence,
a side product of CTC is to predict the alignment, which is the most probable
input-long sequence that specifies a hard aligning relationship between the
input and target units. As there are multiple potential aligning sequences
(called paths) that are equally considered in CTC formulation, the choice of
which path will be most probable and become the predicted alignment is always
uncertain. In addition, it is usually observed that the alignment predicted by
vanilla CTC will drift compared with its reference and rarely provides
practical functionalities. Thus, the motivation of this work is to make the CTC
alignment prediction controllable and thus equip CTC with extra
functionalities. The Bayes risk CTC (BRCTC) criterion is then proposed in this
work, in which a customizable Bayes risk function is adopted to enforce the
desired characteristics of the predicted alignment. With the risk function, the
BRCTC is a general framework to adopt some customizable preference over the
paths in order to concentrate the posterior into a particular subset of the
paths. In applications, we explore one particular preference which yields
models with the down-sampling ability and reduced inference costs. By using
BRCTC with another preference for early emissions, we obtain an improved
performance-latency trade-off for online models. Experimentally, the proposed
BRCTC reduces the inference cost of offline models by up to 47% without
performance degradation and cuts down the overall latency of online systems to
an unseen level.",https://github.com/k2-fsa/k2,-1
Counterfactual reasoning: Do language models need world knowledge for causal understanding?,0.0283709,"Current pre-trained language models have enabled remarkable improvements in
downstream tasks, but it remains difficult to distinguish effects of
statistical correlation from more systematic logical reasoning grounded on
understanding of the real world. In this paper we tease these factors apart by
leveraging counterfactual conditionals, which force language models to predict
unusual consequences based on hypothetical propositions. We introduce a set of
tests drawn from psycholinguistic experiments, as well as larger-scale
controlled datasets, to probe counterfactual predictions from a variety of
popular pre-trained language models. We find that models are consistently able
to override real-world knowledge in counterfactual scenarios, and that this
effect is more robust in case of stronger baseline world knowledge -- however,
we also find that for most models this effect appears largely to be driven by
simple lexical cues. When we mitigate effects of both world knowledge and
lexical cues to test knowledge of linguistic nuances of counterfactuals, we
find that only GPT-3 shows sensitivity to these nuances, though this
sensitivity is also non-trivially impacted by lexical associative factors.",https://github.com/goldengua/Counterfactual_Inference_LM,-1
Minimising Biasing Word Errors for Contextual ASR with the Tree-Constrained Pointer Generator,0.524051,"Contextual knowledge is essential for reducing speech recognition errors on
high-valued long-tail words. This paper proposes a novel tree-constrained
pointer generator (TCPGen) component that enables end-to-end ASR models to bias
towards a list of long-tail words obtained using external contextual
information. With only a small overhead in memory use and computation cost,
TCPGen can structure thousands of biasing words efficiently into a symbolic
prefix-tree and creates a neural shortcut between the tree and the final ASR
output to facilitate the recognition of the biasing words. To enhance TCPGen,
we further propose a novel minimum biasing word error (MBWE) loss that directly
optimises biasing word errors during training, along with a biasing-word-driven
language model discounting (BLMD) method during the test. All contextual ASR
systems were evaluated on the public Librispeech audiobook corpus and the data
from the dialogue state tracking challenges (DSTC) with the biasing lists
extracted from the dialogue-system ontology. Consistent word error rate (WER)
reductions were achieved with TCPGen, which were particularly significant on
the biasing words with around 40\% relative reductions in the recognition error
rates. MBWE and BLMD further improved the effectiveness of TCPGen and achieved
more significant WER reductions on the biasing words. TCPGen also achieved
zero-shot learning of words not in the audio training set with large WER
reductions on the out-of-vocabulary words in the biasing list.",None,-1
Can Pre-trained Language Models Interpret Similes as Smart as Human?,0.644622,"Simile interpretation is a crucial task in natural language processing.
Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art
performance on many tasks. However, it remains under-explored whether PLMs can
interpret similes or not. In this paper, we investigate the ability of PLMs in
simile interpretation by designing a novel task named Simile Property Probing,
i.e., to let the PLMs infer the shared properties of similes. We construct our
simile property probing datasets from both general textual corpora and
human-designed questions, containing 1,633 examples covering seven main
categories. Our empirical study based on the constructed datasets shows that
PLMs can infer similes' shared properties while still underperforming humans.
To bridge the gap with human performance, we additionally design a
knowledge-enhanced training objective by incorporating the simile knowledge
into PLMs via knowledge embedding methods. Our method results in a gain of
8.58% in the probing task and 1.37% in the downstream task of sentiment
classification. The datasets and code are publicly available at
https://github.com/Abbey4799/PLMs-Interpret-Simile.",https://github.com/Abbey4799/PLMs-Interpret-Simile,-1
On resolving conflicts between arguments,0.0806153,"Argument systems are based on the idea that one can construct arguments for
propositions; i.e., structured reasons justifying the belief in a proposition.
Using defeasible rules, arguments need not be valid in all circumstances,
therefore, it might be possible to construct an argument for a proposition as
well as its negation. When arguments support conflicting propositions, one of
the arguments must be defeated, which raises the question of \emph{which
(sub-)arguments can be subject to defeat}?
  In legal argumentation, meta-rules determine the valid arguments by
considering the last defeasible rule of each argument involved in a conflict.
Since it is easier to evaluate arguments using their last rules, \emph{can a
conflict be resolved by considering only the last defeasible rules of the
arguments involved}?
  We propose a new argument system where, instead of deriving a defeat relation
between arguments, \emph{undercutting-arguments} for the defeat of defeasible
rules are constructed. This system allows us, (\textit{i}) to resolve conflicts
(a generalization of rebutting arguments) using only the last rules of the
arguments for inconsistencies, (\textit{ii}) to determine a set of valid
(undefeated) arguments in linear time using an algorithm based on a JTMS,
(\textit{iii}) to establish a relation with Default Logic, and (\textit{iv}) to
prove closure properties such as \emph{cumulativity}. We also propose an
extension of the argument system that enables \emph{reasoning by cases}.",None,-1
Enhancing Deformable Convolution based Video Frame Interpolation with Coarse-to-fine 3D CNN,0.35693,"This paper presents a new deformable convolution-based video frame
interpolation (VFI) method, using a coarse to fine 3D CNN to enhance the
multi-flow prediction. This model first extracts spatio-temporal features at
multiple scales using a 3D CNN, and estimates multi-flows using these features
in a coarse-to-fine manner. The estimated multi-flows are then used to warp the
original input frames as well as context maps, and the warped results are fused
by a synthesis network to produce the final output. This VFI approach has been
fully evaluated against 12 state-of-the-art VFI methods on three commonly used
test databases. The results evidently show the effectiveness of the proposed
method, which offers superior interpolation performance over other state of the
art algorithms, with PSNR gains up to 0.19dB.",https://danier97.github.io/EDC,5466
DisenHCN: Disentangled Hypergraph Convolutional Networks for Spatiotemporal Activity Prediction,0.518827,"Spatiotemporal activity prediction, aiming to predict user activities at a
specific location and time, is crucial for applications like urban planning and
mobile advertising. Existing solutions based on tensor decomposition or graph
embedding suffer from the following two major limitations: 1) ignoring the
fine-grained similarities of user preferences; 2) user's modeling is entangled.
In this work, we propose a hypergraph neural network model called DisenHCN to
bridge the above gaps. In particular, we first unify the fine-grained user
similarity and the complex matching between user preferences and spatiotemporal
activity into a heterogeneous hypergraph. We then disentangle the user
representations into different aspects (location-aware, time-aware, and
activity-aware) and aggregate corresponding aspect's features on the
constructed hypergraph, capturing high-order relations from different aspects
and disentangles the impact of each aspect for final prediction. Extensive
experiments show that our DisenHCN outperforms the state-of-the-art methods by
14.23% to 18.10% on four real-world datasets. Further studies also convincingly
verify the rationality of each component in our DisenHCN.",None,-1
e-CARE: a New Dataset for Exploring Explainable Causal Reasoning,0.55242,"Understanding causality has vital importance for various Natural Language
Processing (NLP) applications. Beyond the labeled instances, conceptual
explanations of the causality can provide deep understanding of the causal
facts to facilitate the causal reasoning process. However, such explanation
information still remains absent in existing causal reasoning resources. In
this paper, we fill this gap by presenting a human-annotated explainable CAusal
REasoning dataset (e-CARE), which contains over 21K causal reasoning questions,
together with natural language formed explanations of the causal questions.
Experimental results show that generating valid explanations for causal facts
still remains especially challenging for the state-of-the-art models, and the
explanation information can be helpful for promoting the accuracy and stability
of causal reasoning models.",https://github.com/Waste-Wood/e-CARE/,21012
Differentiable Data Augmentation for Contrastive Sentence Representation Learning,0.168884,"Fine-tuning a pre-trained language model via the contrastive learning
framework with a large amount of unlabeled sentences or labeled sentence pairs
is a common way to obtain high-quality sentence representations. Although the
contrastive learning framework has shown its superiority on sentence
representation learning over previous methods, the potential of such a
framework is under-explored so far due to the simple method it used to
construct positive pairs. Motivated by this, we propose a method that makes
hard positives from the original training examples. A pivotal ingredient of our
approach is the use of prefix that is attached to a pre-trained language model,
which allows for differentiable data augmentation during contrastive learning.
Our method can be summarized in two steps: supervised prefix-tuning followed by
joint contrastive fine-tuning with unlabeled or labeled examples. Our
experiments confirm the effectiveness of our data augmentation approach. The
proposed method yields significant improvements over existing methods under
both semi-supervised and supervised settings. Our experiments under a low
labeled data setting also show that our method is more label-efficient than the
state-of-the-art contrastive learning methods.",https://github.com/TianduoWang/DiffAug,-1
MSANet: Multi-Similarity and Attention Guidance for Boosting Few-Shot Segmentation,0.386243,"Few-shot segmentation aims to segment unseen-class objects given only a
handful of densely labeled samples. Prototype learning, where the support
feature yields a singleor several prototypes by averaging global and local
object information, has been widely used in FSS. However, utilizing only
prototype vectors may be insufficient to represent the features for all
training data. To extract abundant features and make more precise predictions,
we propose a Multi-Similarity and Attention Network (MSANet) including two
novel modules, a multi-similarity module and an attention module. The
multi-similarity module exploits multiple feature-maps of support images and
query images to estimate accurate semantic relationships. The attention module
instructs the network to concentrate on class-relevant information. The network
is tested on standard FSS datasets, PASCAL-5i 1-shot, PASCAL-5i 5-shot,
COCO-20i 1-shot, and COCO-20i 5-shot. The MSANet with the backbone of
ResNet-101 achieves the state-of-the-art performance for all 4-benchmark
datasets with mean intersection over union (mIoU) of 69.13%, 73.99%, 51.09%,
56.80%, respectively. Code is available at
https://github.com/AIVResearch/MSANet",https://github.com/AIVResearch/MSANet,-1
Track Targets by Dense Spatio-Temporal Position Encoding,0.693672,"In this work, we propose a novel paradigm to encode the position of targets
for target tracking in videos using transformers. The proposed paradigm, Dense
Spatio-Temporal (DST) position encoding, encodes spatio-temporal position
information in a pixel-wise dense fashion. The provided position encoding
provides location information to associate targets across frames beyond
appearance matching by comparing objects in two bounding boxes. Compared to the
typical transformer positional encoding, our proposed encoding is applied to
the 2D CNN features instead of the projected feature vectors to avoid losing
positional information. Moreover, the designed DST encoding can represent the
location of a single-frame object and the evolution of the location of the
trajectory among frames uniformly. Integrated with the DST encoding, we build a
transformer-based multi-object tracking model. The model takes a video clip as
input and conducts the target association in the clip. It can also perform
online inference by associating existing trajectories with objects from the
new-coming frames. Experiments on video multi-object tracking (MOT) and
multi-object tracking and segmentation (MOTS) datasets demonstrate the
effectiveness of the proposed DST position encoding.",https://github.com/open-mmlab/mmtracking,-1
Few-Shot Diffusion Models,0.456698,"Denoising diffusion probabilistic models (DDPM) are powerful hierarchical
latent variable models with remarkable sample generation quality and training
stability. These properties can be attributed to parameter sharing in the
generative hierarchy, as well as a parameter-free diffusion-based inference
procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a
framework for few-shot generation leveraging conditional DDPMs. FSDMs are
trained to adapt the generative process conditioned on a small set of images
from a given class by aggregating image patch information using a set-based
Vision Transformer (ViT). At test time, the model is able to generate samples
from previously unseen classes conditioned on as few as 5 samples from that
class. We empirically show that FSDM can perform few-shot generation and
transfer to new datasets. We benchmark variants of our method on complex vision
datasets for few-shot learning and compare to unconditional and conditional
DDPM baselines. Additionally, we show how conditioning the model on patch-based
input set information improves training convergence.",None,-1
On the Convergence of Semi-Relaxed Sinkhorn with Marginal Constraint and OT Distance Gaps,0.125807,"This paper presents consideration of the Semi-Relaxed Sinkhorn (SR-Sinkhorn)
algorithm for the semi-relaxed optimal transport (SROT) problem, which relaxes
one marginal constraint of the standard OT problem. For evaluation of how the
constraint relaxation affects the algorithm behavior and solution, it is
vitally necessary to present the theoretical convergence analysis in terms not
only of the functional value gap, but also of the marginal constraint gap as
well as the OT distance gap. However, no existing work has addressed all
analyses simultaneously. To this end, this paper presents a comprehensive
convergence analysis for SR-Sinkhorn. After presenting the
$\epsilon$-approximation of the functional value gap based on a new proof
strategy and exploiting this proof strategy, we give the upper bound of the
marginal constraint gap. We also provide its convergence to the
$\epsilon$-approximation when two distributions are in the probability simplex.
Furthermore, the convergence analysis of the OT distance gap to the
$\epsilon$-approximation is given as assisted by the obtained marginal
constraint gap. The latter two theoretical results are the first results
presented in the literature related to the SROT problem.",https://github.com/lntk/uot,1980
Masked Generative Distillation,0.960241,"Knowledge distillation has been applied to various tasks successfully. The
current distillation algorithm usually improves students' performance by
imitating the output of the teacher. This paper shows that teachers can also
improve students' representation power by guiding students' feature recovery.
From this point of view, we propose Masked Generative Distillation (MGD), which
is simple: we mask random pixels of the student's feature and force it to
generate the teacher's full feature through a simple block. MGD is a truly
general feature-based distillation method, which can be utilized on various
tasks, including image classification, object detection, semantic segmentation
and instance segmentation. We experiment on different models with extensive
datasets and the results show that all the students achieve excellent
improvements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1
accuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP,
SOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on
ResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at
https://github.com/yzd-v/MGD.",https://github.com/yzd-v/MGD,5005
Online Detection Of Supply Chain Network Disruptions Using Sequential Change-Point Detection for Hawkes Processes,0.252617,"In this paper, we attempt to detect an inflection or change-point resulting
from the Covid-19 pandemic on supply chain data received from a large furniture
company. To accomplish this, we utilize a modified CUSUM (Cumulative Sum)
procedure on the company's spatial-temporal order data as well as a GLR
(Generalized Likelihood Ratio) based method. We model the order data using the
Hawkes Process Network, a multi-dimensional self and mutually exciting point
process, by discretizing the spatial data and treating each order as an event
that has a corresponding node and time. We apply the methodologies on the
company's most ordered item on a national scale and perform a deep dive into a
single state. Because the item was ordered infrequently in the state compared
to the nation, this approach allows us to show efficacy upon different degrees
of data sparsity. Furthermore, it showcases use potential across differing
levels of spatial detail.",None,-1
Learning Texture Transformer Network for Light Field Super-Resolution,0.17142,"Hand-held light field cameras suffer from low spatial resolution due to the
inherent spatio-angular tradeoff. In this paper, we propose a method to improve
the spatial resolution of light field images with the aid of the Texture
Transformer Network (TTSR). The proposed method consists of three modules: the
first module produces an all-in focus high-resolution perspective image which
serves as a reference image for the second module, i.e. TTSR, which in turn
produces a high-resolution light field. The last module refines the spatial
resolution by imposing a light field prior. The results demonstrate around 4 dB
to 6 dB PSNR gain over a bicubically resized light field image",None,-1
Qtrade AI at SemEval-2022 Task 11: An Unified Framework for Multilingual NER Task,0.562066,"This paper describes our system, which placed third in the Multilingual Track
(subtask 11), fourth in the Code-Mixed Track (subtask 12), and seventh in the
Chinese Track (subtask 9) in the SemEval 2022 Task 11: MultiCoNER Multilingual
Complex Named Entity Recognition. Our system's key contributions are as
follows: 1) For multilingual NER tasks, we offer an unified framework with
which one can easily execute single-language or multilingual NER tasks, 2) for
low-resource code-mixed NER task, one can easily enhance his or her dataset
through implementing several simple data augmentation methods and 3) for
Chinese tasks, we propose a model that can capture Chinese lexical semantic,
lexical border, and lexical graph structural information. Finally, our system
achieves macro-f1 scores of 77.66, 84.35, and 74.00 on subtasks 11, 12, and 9,
respectively, during the testing phase.",None,42
Optimizing Elimination Templates by Greedy Parameter Search,0.522688,"We propose a new method for constructing elimination templates for efficient
polynomial system solving of minimal problems in structure from motion, image
matching, and camera tracking. We first construct a particular affine
parameterization of the elimination templates for systems with a finite number
of distinct solutions. Then, we use a heuristic greedy optimization strategy
over the space of parameters to get a template with a small size. We test our
method on 34 minimal problems in computer vision. For all of them, we found the
templates either of the same or smaller size compared to the state-of-the-art.
For some difficult examples, our templates are, e.g., 2.1, 2.5, 3.8, 6.6 times
smaller. For the problem of refractive absolute pose estimation with unknown
focal length, we have found a template that is 20 times smaller. Our
experiments on synthetic data also show that the new solvers are fast and
numerically accurate. We also present a fast and numerically accurate solver
for the problem of relative pose estimation with unknown common focal length
and radial distortion.",http://github.com/martyushev/EliminationTemplates,-1
Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection,0.544389,"Counterfactually Augmented Data (CAD) aims to improve out-of-domain
generalizability, an indicator of model robustness. The improvement is credited
with promoting core features of the construct over spurious artifacts that
happen to correlate with it. Yet, over-relying on core features may lead to
unintended model bias. Especially, construct-driven CAD -- perturbations of
core features -- may induce models to ignore the context in which core features
are used. Here, we test models for sexism and hate speech detection on
challenging data: non-hateful and non-sexist usage of identity and gendered
terms. In these hard cases, models trained on CAD, especially construct-driven
CAD, show higher false-positive rates than models trained on the original,
unperturbed data. Using a diverse set of CAD -- construct-driven and
construct-agnostic -- reduces such unintended bias.",https://github.com/gesiscss/Uninteded_Bias_in_CAD,-1
CTM -- A Model for Large-Scale Multi-View Tweet Topic Classification,0.368056,"Automatically associating social media posts with topics is an important
prerequisite for effective search and recommendation on many social media
platforms. However, topic classification of such posts is quite challenging
because of (a) a large topic space (b) short text with weak topical cues, and
(c) multiple topic associations per post. In contrast to most prior work which
only focuses on post classification into a small number of topics ($10$-$20$),
we consider the task of large-scale topic classification in the context of
Twitter where the topic space is $10$ times larger with potentially multiple
topic associations per Tweet. We address the challenges above by proposing a
novel neural model, CTM that (a) supports a large topic space of $300$ topics
and (b) takes a holistic approach to tweet content modeling -- leveraging
multi-modal content, author context, and deeper semantic cues in the Tweet. Our
method offers an effective way to classify Tweets into topics at scale by
yielding superior performance to other approaches (a relative lift of
$\mathbf{20}\%$ in median average precision score) and has been successfully
deployed in production at Twitter.",None,-1
Exploring Visual Prompts for Adapting Large-Scale Models,0.827194,"We investigate the efficacy of visual prompting to adapt large-scale models
in vision. Following the recent approach from prompt tuning and adversarial
reprogramming, we learn a single image perturbation such that a frozen model
prompted with this perturbation performs a new task. Through comprehensive
experiments, we demonstrate that visual prompting is particularly effective for
CLIP and robust to distribution shift, achieving performance competitive with
standard linear probes. We further analyze properties of the downstream
dataset, prompt design, and output transformation in regard to adaptation
performance. The surprising effectiveness of visual prompting provides a new
perspective on adapting pre-trained models in vision. Code is available at
http://hjbahng.github.io/visual_prompting .",https://hjbahng.github.io/visual_prompting/,-1
Sphere-Guided Training of Neural Implicit Surfaces,0.116934,"In recent years, neural distance functions trained via volumetric ray
marching have been widely adopted for multi-view 3D reconstruction. These
methods, however, apply the ray marching procedure for the entire scene volume,
leading to reduced sampling efficiency and, as a result, lower reconstruction
quality in the areas of high-frequency details. In this work, we address this
problem via joint training of the implicit function and our new coarse
sphere-based surface reconstruction. We use the coarse representation to
efficiently exclude the empty volume of the scene from the volumetric ray
marching procedure without additional forward passes of the neural surface
network, which leads to an increased fidelity of the reconstructions compared
to the base systems. We evaluate our approach by incorporating it into the
training procedures of several implicit surface modeling methods and observe
uniform improvements across both synthetic and real-world datasets. Our
codebase can be accessed via the project page:
https://andreeadogaru.github.io/SphereGuided",https://andreeadogaru.github.io/SphereGuided,-1
Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words,0.205463,"Prompt-based fine-tuning for pre-trained models has proven effective for many
natural language processing tasks under few-shot settings in general domain.
However, tuning with prompt in biomedical domain has not been investigated
thoroughly. Biomedical words are often rare in general domain, but quite
ubiquitous in biomedical contexts, which dramatically deteriorates the
performance of pre-trained models on downstream biomedical applications even
after fine-tuning, especially in low-resource scenarios. We propose a simple
yet effective approach to helping models learn rare biomedical words during
tuning with prompt. Experimental results show that our method can achieve up to
6% improvement in biomedical natural language inference task without any extra
parameters or training steps using few-shot vanilla prompt settings.",https://github.com/s65b40/prompt_n_paraphrase,-1
Controllable Augmentations for Video Representation Learning,0.132705,"This paper focuses on self-supervised video representation learning. Most
existing approaches follow the contrastive learning pipeline to construct
positive and negative pairs by sampling different clips. However, this
formulation tends to bias to static background and have difficulty establishing
global temporal structures. The major reason is that the positive pairs, i.e.,
different clips sampled from the same video, have limited temporal receptive
field, and usually share similar background but differ in motions. To address
these problems, we propose a framework to jointly utilize local clips and
global videos to learn from detailed region-level correspondence as well as
general long-term temporal relations. Based on a set of controllable
augmentations, we achieve accurate appearance and motion pattern alignment
through soft spatio-temporal region contrast. Our formulation is able to avoid
the low-level redundancy shortcut by mutual information minimization to improve
the generalization. We also introduce local-global temporal order dependency to
further bridge the gap between clip-level and video-level representations for
robust temporal modeling. Extensive experiments demonstrate that our framework
is superior on three video benchmarks in action recognition and video
retrieval, capturing more accurate temporal dynamics.",None,-1
Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model,0.53828,"Contextual ASR, which takes a list of bias terms as input along with audio,
has drawn recent interest as ASR use becomes more widespread. We are releasing
contextual biasing lists to accompany the Earnings21 dataset, creating a public
benchmark for this task. We present baseline results on this benchmark using a
pretrained end-to-end ASR model from the WeNet toolkit. We show results for
shallow fusion contextual biasing applied to two different decoding algorithms.
Our baseline results confirm observations that end-to-end models struggle in
particular with words that are rarely or never seen during training, and that
existing shallow fusion techniques do not adequately address this problem. We
propose an alternate spelling prediction model that improves recall of rare
words by 34.7% relative and of out-of-vocabulary words by 97.2% relative,
compared to contextual biasing without alternate spellings. This model is
conceptually similar to ones used in prior work, but is simpler to implement as
it does not rely on either a pronunciation dictionary or an existing
text-to-speech system.",https://github.com/revdotcom/speech-datasets/tree/main/earnings21,143
Statistical Foundation Behind Machine Learning and Its Impact on Computer Vision,0.138171,"This paper revisits the principle of uniform convergence in statistical
learning, discusses how it acts as the foundation behind machine learning, and
attempts to gain a better understanding of the essential problem that current
deep learning algorithms are solving. Using computer vision as an example
domain in machine learning, the discussion shows that recent research trends in
leveraging increasingly large-scale data to perform pre-training for
representation learning are largely to reduce the discrepancy between a
practically tractable empirical loss and its ultimately desired but intractable
expected loss. Furthermore, this paper suggests a few future research
directions, predicts the continued increase of data, and argues that more
fundamental research is needed on robustness, interpretability, and reasoning
capabilities of machine learning by incorporating structure and knowledge.",None,-1
Do Bayesian Neural Networks Need To Be Fully Stochastic?,0.811019,"We investigate the benefit of treating all the parameters in a Bayesian
neural network stochastically and find compelling theoretical and empirical
evidence that this standard construction may be unnecessary. To this end, we
prove that expressive predictive distributions require only small amounts of
stochasticity. In particular, partially stochastic networks with only $n$
stochastic biases are universal probabilistic predictors for $n$-dimensional
predictive problems. In empirical investigations, we find no systematic benefit
of full stochasticity across four different inference modalities and eight
datasets; partially stochastic networks can match and sometimes even outperform
fully stochastic networks, despite their reduced memory costs.",https://github.com/IntelLabs/bayesian-torch,2733
Watching the News: Towards VideoQA Models that can Read,0.517116,"Video Question Answering methods focus on commonsense reasoning and visual
cognition of objects or persons and their interactions over time. Current
VideoQA approaches ignore the textual information present in the video.
Instead, we argue that textual information is complementary to the action and
provides essential contextualisation cues to the reasoning process. To this
end, we propose a novel VideoQA task that requires reading and understanding
the text in the video. To explore this direction, we focus on news videos and
require QA systems to comprehend and answer questions about the topics
presented by combining visual and textual cues in the video. We introduce the
``NewsVideoQA'' dataset that comprises more than $8,600$ QA pairs on $3,000+$
news videos obtained from diverse news channels from around the world. We
demonstrate the limitations of current Scene Text VQA and VideoQA methods and
propose ways to incorporate scene text information into VideoQA methods.",https://github.com/facebookresearch/mmf,-1
Controlled Language Generation for Language Learning Items,0.0551641,"This work aims to employ natural language generation (NLG) to rapidly
generate items for English language learning applications: this requires both
language models capable of generating fluent, high-quality English, and to
control the output of the generation to match the requirements of the relevant
items. We experiment with deep pretrained models for this task, developing
novel methods for controlling items for factors relevant in language learning:
diverse sentences for different proficiency levels and argument structure to
test grammar. Human evaluation demonstrates high grammatically scores for all
models (3.4 and above out of 4), and higher length (24%) and complexity (9%)
over the baseline for the advanced proficiency model. Our results show that we
can achieve strong performance while adding additional control to ensure
diverse, tailored content for individual users.",https://github.com/EducationalTestingService/concept-control-gen,-1
A hybrid quantum image edge detector for the NISQ era,0.326619,"Edges are image locations where the gray value intensity changes suddenly.
They are among the most important features to understand and segment an image.
Edge detection is a standard task in digital image processing, solved for
example using filtering techniques. However, the amount of data to be processed
grows rapidly and pushes even supercomputers to their limits. Quantum computing
promises exponentially lower memory usage in terms of the number of qubits
compared to the number of classical bits. In this paper, we propose a hybrid
method for quantum edge detection based on the idea of a quantum artificial
neuron. Our method can be practically implemented on quantum computers,
especially on those of the current noisy intermediate-scale quantum era. We
compare six variants of the method to reduce the number of circuits and thus
the time required for the quantum edge detection. Taking advantage of the
scalability of our method, we can practically detect edges in images
considerably larger than reached before.",None,-1
Formal Mathematics Statement Curriculum Learning,0.999974,"We explore the use of expert iteration in the context of language modeling
applied to formal mathematics. We show that at same compute budget, expert
iteration, by which we mean proof search interleaved with learning,
dramatically outperforms proof search only. We also observe that when applied
to a collection of formal statements of sufficiently varied difficulty, expert
iteration is capable of finding and solving a curriculum of increasingly
difficult problems, without the need for associated ground-truth proofs.
Finally, by applying this expert iteration to a manually curated set of problem
statements, we achieve state-of-the-art on the miniF2F benchmark, automatically
solving multiple challenging problems drawn from high school olympiads.",https://github.com/openai/lean-gym,-1
A Computational Inflection for Scientific Discovery,0.708901,"We stand at the foot of a significant inflection in the trajectory of
scientific discovery. As society continues on its fast-paced digital
transformation, so does humankind's collective scientific knowledge and
discourse. We now read and write papers in digitized form, and a great deal of
the formal and informal processes of science are captured digitally --
including papers, preprints and books, code and datasets, conference
presentations, and interactions in social networks and collaboration and
communication platforms. The transition has led to the creation and growth of a
tremendous amount of information -- much of which is available for public
access -- opening exciting opportunities for computational models and systems
that analyze and harness it. In parallel, exponential growth in data processing
power has fueled remarkable advances in artificial intelligence, including
large neural language models capable of learning powerful representations from
unstructured text. Dramatic changes in scientific communication -- such as the
advent of the first scientific journal in the 17th century -- have historically
catalyzed revolutions in scientific thought. The confluence of societal and
computational trends suggests that computer science is poised to ignite a
revolution in the scientific process itself.",None,101093
Efficient Long Sequence Modeling via State Space Augmented Transformer,0.809395,"Transformer models have achieved superior performance in various natural
language processing tasks. However, the quadratic computational cost of the
attention mechanism limits its practicality for long sequences. There are
existing attention variants that improve the computational efficiency, but they
have limited ability to effectively compute global information. In parallel to
Transformer models, state space models (SSMs) are tailored for long sequences,
but they are not flexible enough to capture complicated local information. We
propose SPADE, short for $\underline{\textbf{S}}$tate
s$\underline{\textbf{P}}$ace
$\underline{\textbf{A}}$ugmente$\underline{\textbf{D}}$
Transform$\underline{\textbf{E}}$r. Specifically, we augment a SSM into the
bottom layer of SPADE, and we employ efficient local attention methods for the
other layers. The SSM augments global information, which complements the lack
of long-range dependency issue in local attention methods. Experimental results
on the Long Range Arena benchmark and language modeling tasks demonstrate the
effectiveness of the proposed method. To further demonstrate the scalability of
SPADE, we pre-train large encoder-decoder models and present fine-tuning
results on natural language understanding and natural language generation
tasks.",https://github.com/microsoft/EfficientLongSequenceModeling,82331
Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation,0.919731,"Continual relation extraction (CRE) aims to continually learn new relations
from a class-incremental data stream. CRE model usually suffers from
catastrophic forgetting problem, i.e., the performance of old relations
seriously degrades when the model learns new relations. Most previous work
attributes catastrophic forgetting to the corruption of the learned
representations as new relations come, with an implicit assumption that the CRE
models have adequately learned the old relations. In this paper, through
empirical studies we argue that this assumption may not hold, and an important
reason for catastrophic forgetting is that the learned representations do not
have good robustness against the appearance of analogous relations in the
subsequent learning process. To address this issue, we encourage the model to
learn more precise and robust representations through a simple yet effective
adversarial class augmentation mechanism (ACA), which is easy to implement and
model-agnostic. Experimental results show that ACA can consistently improve the
performance of state-of-the-art CRE models on two popular benchmarks.",https://github.com/Wangpeiyi9979/ACA,10429
Emergent Communication through Metropolis-Hastings Naming Game with Deep Generative Models,0.880075,"Constructive studies on symbol emergence systems seek to investigate
computational models that can better explain human language evolution, the
creation of symbol systems, and the construction of internal representations.
This study provides a new model for emergent communication, which is based on a
probabilistic generative model (PGM) instead of a discriminative model based on
deep reinforcement learning. We define the Metropolis-Hastings (MH) naming game
by generalizing previously proposed models. It is not a referential game with
explicit feedback, as assumed by many emergent communication studies. Instead,
it is a game based on joint attention without explicit feedback.
Mathematically, the MH naming game is proved to be a type of MH algorithm for
an integrative PGM that combines two agents that play the naming game. From
this viewpoint, symbol emergence is regarded as decentralized Bayesian
inference, and semiotic communication is regarded as inter-personal cross-modal
inference. This notion leads to the collective predictive coding hypothesis}
regarding language evolution and, in general, the emergence of symbols. We also
propose the inter-Gaussian mixture model (GMM)+ variational autoencoder (VAE),
a deep generative model for emergent communication based on the MH naming game.
The model has been validated on MNIST and Fruits 360 datasets. Experimental
findings demonstrate that categories are formed from real images observed by
agents, and signs are correctly shared across agents by successfully utilizing
both of the observations of agents via the MH naming game. Furthermore,
scholars verified that visual images were recalled from signs uttered by
agents. Notably, emergent communication without supervision and reward feedback
improved the performance of the unsupervised representation learning of agents.",https://github.com/is0383kk/SymbolEmergence-VAE-GMM,-1
Motion Transformer with Global Intention Localization and Local Movement Refinement,0.999956,"Predicting multimodal future behavior of traffic participants is essential
for robotic vehicles to make safe decisions. Existing works explore to directly
predict future trajectories based on latent features or utilize dense goal
candidates to identify agent's destinations, where the former strategy
converges slowly since all motion modes are derived from the same feature while
the latter strategy has efficiency issue since its performance highly relies on
the density of goal candidates. In this paper, we propose Motion TRansformer
(MTR) framework that models motion prediction as the joint optimization of
global intention localization and local movement refinement. Instead of using
goal candidates, MTR incorporates spatial intention priors by adopting a small
set of learnable motion query pairs. Each motion query pair takes charge of
trajectory prediction and refinement for a specific motion mode, which
stabilizes the training process and facilitates better multimodal predictions.
Experiments show that MTR achieves state-of-the-art performance on both the
marginal and joint motion prediction challenges, ranking 1st on the
leaderboards of Waymo Open Motion Dataset. The source code is available at
https://github.com/sshaoshuai/MTR.",https://github.com/sshaoshuai/MTR,-1
RNNCTPs: A Neural Symbolic Reasoning Method Using Dynamic Knowledge Partitioning Technology,0.117683,"Although traditional symbolic reasoning methods are highly interpretable,
their application in knowledge graph link prediction is limited due to their
low computational efficiency. In this paper, we propose a new neural symbolic
reasoning method: RNNCTPs, which improves computational efficiency by
re-filtering the knowledge selection of Conditional Theorem Provers (CTPs), and
is less sensitive to the embedding size parameter. RNNCTPs are divided into
relation selectors and predictors. The relation selectors are trained
efficiently and interpretably, so that the whole model can dynamically generate
knowledge for the inference of the predictor. In all four datasets, the method
shows competitive performance against traditional methods on the link
prediction task, and can have higher applicability to the selection of datasets
relative to CTPs.",None,-1
Cognitive Modeling of Semantic Fluency Using Transformers,0.0275761,"Can deep language models be explanatory models of human cognition? If so,
what are their limits? In order to explore this question, we propose an
approach called hyperparameter hypothesization that uses predictive
hyperparameter tuning in order to find individuating descriptors of
cognitive-behavioral profiles. We take the first step in this approach by
predicting human performance in the semantic fluency task (SFT), a well-studied
task in cognitive science that has never before been modeled using
transformer-based language models (TLMs). In our task setup, we compare several
approaches to predicting which word an individual performing SFT will utter
next. We report preliminary evidence suggesting that, despite obvious
implementational differences in how people and TLMs learn and use language,
TLMs can be used to identify individual differences in human fluency task
behaviors better than existing computational models, and may offer insights
into human memory retrieval strategies -- cognitive process not typically
considered to be the kinds of things TLMs can model. Finally, we discuss the
implications of this work for cognitive modeling of knowledge representations.",None,-1
Non-Deterministic Approximation Fixpoint Theory and Its Application in Disjunctive Logic Programming,0.125867,"Approximation fixpoint theory (AFT) is an abstract and general algebraic
framework for studying the semantics of nonmonotonic logics. It provides a
unifying study of the semantics of different formalisms for nonmonotonic
reasoning, such as logic programming, default logic and autoepistemic logic. In
this paper, we extend AFT to dealing with non-deterministic constructs that
allow to handle indefinite information, represented e.g. by disjunctive
formulas. This is done by generalizing the main constructions and corresponding
results of AFT to non-deterministic operators, whose ranges are sets of
elements rather than single elements. The applicability and usefulness of this
generalization is illustrated in the context of disjunctive logic programming.",None,-1
Identifying Ethical Issues in AI Partners in Human-AI Co-Creation,0.65119,"Human-AI co-creativity involves humans and AI collaborating on a shared
creative product as partners. In many existing co-creative systems, users
communicate with the AI using buttons or sliders. However, typically, the AI in
co-creative systems cannot communicate back to humans, limiting their potential
to be perceived as partners. This paper starts with an overview of a
comparative study with 38 participants to explore the impact of AI-to-human
communication on user perception and engagement in co-creative systems and the
results show improved collaborative experience and user engagement with the
system incorporating AI-to-human communication. The results also demonstrate
that users perceive co-creative AI as more reliable, personal and intelligent
when it can communicate with the users. The results indicate a need to identify
potential ethical issues from an engaging communicating co-creative AI. Later
in the paper, we present some potential ethical issues in human-AI co-creation
and propose to use participatory design fiction as the research methodology to
investigate the ethical issues associated with a co-creative AI that
communicates with users.",None,-1
What does Transformer learn about source code?,0.271273,"In the field of source code processing, the transformer-based representation
models have shown great powerfulness and have achieved state-of-the-art (SOTA)
performance in many tasks. Although the transformer models process the
sequential source code, pieces of evidence show that they may capture the
structural information (\eg, in the syntax tree, data flow, control flow, \etc)
as well. We propose the aggregated attention score, a method to investigate the
structural information learned by the transformer. We also put forward the
aggregated attention graph, a new way to extract program graphs from the
pre-trained models automatically. We measure our methods from multiple
perspectives. Furthermore, based on our empirical findings, we use the
automatically extracted graphs to replace those ingenious manual designed
graphs in the Variable Misuse task. Experimental results show that the semantic
graphs we extracted automatically are greatly meaningful and effective, which
provide a new perspective for us to understand and use the information
contained in the model.",None,-1
Sockeye 3: Fast Neural Machine Translation with PyTorch,0.454404,"Sockeye 3 is the latest version of the Sockeye toolkit for Neural Machine
Translation (NMT). Now based on PyTorch, Sockeye 3 provides faster model
implementations and more advanced features with a further streamlined codebase.
This enables broader experimentation with faster iteration, efficient training
of stronger and faster models, and the flexibility to move new ideas quickly
from research to production. When running comparable models, Sockeye 3 is up to
126% faster than other PyTorch implementations on GPUs and up to 292% faster on
CPUs. Sockeye 3 is open source software released under the Apache 2.0 license.",https://github.com/awslabs/sockeye,-1
AI-based Malware and Ransomware Detection Models,0.677428,"Cybercrime is one of the major digital threats of this century. In
particular, ransomware attacks have significantly increased, resulting in
global damage costs of tens of billion dollars. In this paper, we train and
test different Machine Learning and Deep Learning models for malware detection,
malware classification and ransomware detection. We introduce a novel and
flexible solution that combines two optimized models for malware and ransomware
detection. Our results demonstrate some improvements both in terms of detection
performances and flexibility. In particular, our combined models pave the way
for easier future enhancements using specialized and thus interchangeable
detection modules.",None,-1
Towards Summary Candidates Fusion,0.848198,"Sequence-to-sequence deep neural models fine-tuned for abstractive
summarization can achieve great performance on datasets with enough human
annotations. Yet, it has been shown that they have not reached their full
potential, with a wide gap between the top beam search output and the oracle
beam. Recently, re-ranking methods have been proposed, to learn to select a
better summary candidate. However, such methods are limited by the summary
quality aspects captured by the first-stage candidates. To bypass this
limitation, we propose a new paradigm in second-stage abstractive summarization
called SummaFusion that fuses several summary candidates to produce a novel
abstractive second-stage summary. Our method works well on several
summarization datasets, improving both the ROUGE scores and qualitative
properties of fused summaries. It is especially good when the candidates to
fuse are worse, such as in the few-shot setup where we set a new
state-of-the-art. We will make our code and checkpoints available at
https://github.com/ntunlp/SummaFusion/.",https://github.com/ntunlp/SummaFusion/,-1
Better Quality Estimation for Low Resource Corpus Mining,0.392217,"Quality Estimation (QE) models have the potential to change how we evaluate
and maybe even train machine translation models. However, these models still
lack the robustness to achieve general adoption. We show that State-of-the-art
QE models, when tested in a Parallel Corpus Mining (PCM) setting, perform
unexpectedly bad due to a lack of robustness to out-of-domain examples. We
propose a combination of multitask training, data augmentation and contrastive
learning to achieve better and more robust QE performance. We show that our
method improves QE performance significantly in the MLQE challenge and the
robustness of QE models when tested in the Parallel Corpus Mining setup. We
increase the accuracy in PCM by more than 0.80, making it on par with
state-of-the-art PCM methods that use millions of sentence pairs to train their
models. In comparison, we use a thousand times less data, 7K parallel sentences
in total, and propose a novel low resource PCM method.",https://github.com/facebookresearch/LASER,2587
ProspectNet: Weighted Conditional Attention for Future Interaction Modeling in Behavior Prediction,0.256978,"Behavior prediction plays an important role in integrated autonomous driving
software solutions. In behavior prediction research, interactive behavior
prediction is a less-explored area, compared to single-agent behavior
prediction. Predicting the motion of interactive agents requires initiating
novel mechanisms to capture the joint behaviors of the interactive pairs. In
this work, we formulate the end-to-end joint prediction problem as a sequential
learning process of marginal learning and joint learning of vehicle behaviors.
We propose ProspectNet, a joint learning block that adopts the weighted
attention score to model the mutual influence between interactive agent pairs.
The joint learning block first weighs the multi-modal predicted candidate
trajectories, then updates the ego-agent's embedding via cross attention.
Furthermore, we broadcast the individual future predictions for each
interactive agent into a pair-wise scoring module to select the top $K$
prediction pairs. We show that ProspectNet outperforms the Cartesian product of
two marginal predictions, and achieves comparable performance on the Waymo
Interactive Motion Prediction benchmarks.",None,428
Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe,0.45696,"Privacy concerns have attracted increasing attention in data-driven products
due to the tendency of machine learning models to memorize sensitive training
data. Generating synthetic versions of such data with a formal privacy
guarantee, such as differential privacy (DP), provides a promising path to
mitigating these privacy concerns, but previous approaches in this direction
have typically failed to produce synthetic data of high quality. In this work,
we show that a simple and practical recipe in the text domain is effective:
simply fine-tuning a pretrained generative language model with DP enables the
model to generate useful synthetic text with strong privacy protection. Through
extensive empirical analyses on both benchmark and private customer data, we
demonstrate that our method produces synthetic text that is competitive in
terms of utility with its non-private counterpart, meanwhile providing strong
protection against potential privacy leakages.",https://github.com/microsoft/dp-transformers,-1
GriddlyJS: A Web IDE for Reinforcement Learning,0.259688,"Progress in reinforcement learning (RL) research is often driven by the
design of new, challenging environments -- a costly undertaking requiring
skills orthogonal to that of a typical machine learning researcher. The
complexity of environment development has only increased with the rise of
procedural-content generation (PCG) as the prevailing paradigm for producing
varied environments capable of testing the robustness and generalization of RL
agents. Moreover, existing environments often require complex build processes,
making reproducing results difficult. To address these issues, we introduce
GriddlyJS, a web-based Integrated Development Environment (IDE) based on the
Griddly engine. GriddlyJS allows researchers to visually design and debug
arbitrary, complex PCG grid-world environments using a convenient graphical
interface, as well as visualize, evaluate, and record the performance of
trained agent models. By connecting the RL workflow to the advanced
functionality enabled by modern web standards, GriddlyJS allows publishing
interactive agent-environment demos that reproduce experimental results
directly to the web. To demonstrate the versatility of GriddlyJS, we use it to
quickly develop a complex compositional puzzle-solving environment alongside
arbitrary human-designed environment configurations and their solutions for use
in automatic curriculum learning and offline RL. The GriddlyJS IDE is open
source and freely available at https://griddly.ai.",https://github.com/GriddlyAI/escape-rooms,-1
Collaborative Image Understanding,0.0720501,"Automatically understanding the contents of an image is a highly relevant
problem in practice. In e-commerce and social media settings, for example, a
common problem is to automatically categorize user-provided pictures. Nowadays,
a standard approach is to fine-tune pre-trained image models with
application-specific data. Besides images, organizations however often also
collect collaborative signals in the context of their application, in
particular how users interacted with the provided online content, e.g., in
forms of viewing, rating, or tagging. Such signals are commonly used for item
recommendation, typically by deriving latent user and item representations from
the data. In this work, we show that such collaborative information can be
leveraged to improve the classification process of new images. Specifically, we
propose a multitask learning framework, where the auxiliary task is to
reconstruct collaborative latent item representations. A series of experiments
on datasets from e-commerce and social media demonstrates that considering
collaborative signals helps to significantly improve the performance of the
main task of image classification by up to 9.1%.",https://github.com/anonymous1e6/cactus,18602
Continual VQA for Disaster Response Systems,0.0580357,"Visual Question Answering (VQA) is a multi-modal task that involves answering
questions from an input image, semantically understanding the contents of the
image and answering it in natural language. Using VQA for disaster management
is an important line of research due to the scope of problems that are answered
by the VQA system. However, the main challenge is the delay caused by the
generation of labels in the assessment of the affected areas. To tackle this,
we deployed pre-trained CLIP model, which is trained on visual-image pairs.
however, we empirically see that the model has poor zero-shot performance.
Thus, we instead use pre-trained embeddings of text and image from this model
for our supervised training and surpass previous state-of-the-art results on
the FloodNet dataset. We expand this to a continual setting, which is a more
real-life scenario. We tackle the problem of catastrophic forgetting using
various experience replay methods. Our training runs are available at:
https://wandb.ai/compyle/continual_vqa_final. Our code is available at
https://github.com/AdityaKane2001/continual_vqa.",None,-1
Chaining Simultaneous Thoughts for Numerical Reasoning,0.234934,"Given that rich information is hidden behind ubiquitous numbers in text,
numerical reasoning over text should be an essential skill of AI systems. To
derive precise equations to solve numerical reasoning problems, previous work
focused on modeling the structures of equations, and has proposed various
structured decoders. Though structure modeling proves to be effective, these
structured decoders construct a single equation in a pre-defined autoregressive
order, potentially placing an unnecessary restriction on how a model should
grasp the reasoning process. Intuitively, humans may have numerous pieces of
thoughts popping up in no pre-defined order; thoughts are not limited to the
problem at hand, and can even be concerned with other related problems. By
comparing diverse thoughts and chaining relevant pieces, humans are less prone
to errors. In this paper, we take this inspiration and propose CANTOR, a
numerical reasoner that models reasoning steps using a directed acyclic graph
where we produce diverse reasoning steps simultaneously without pre-defined
decoding dependencies, and compare and chain relevant ones to reach a solution.
Extensive experiments demonstrated the effectiveness of CANTOR under both
fully-supervised and weakly-supervised settings.",https://github.com/motrom/fastmurty,-1
Putting AI Ethics into Practice: The Hourglass Model of Organizational AI Governance,0.543743,"The organizational use of artificial intelligence (AI) has rapidly spread
across various sectors. Alongside the awareness of the benefits brought by AI,
there is a growing consensus on the necessity of tackling the risks and
potential harms, such as bias and discrimination, brought about by advanced AI
technologies. A multitude of AI ethics principles have been proposed to tackle
these risks, but the outlines of organizational processes and practices for
ensuring socially responsible AI development are in a nascent state. To address
the paucity of comprehensive governance models, we present an AI governance
framework, the hourglass model of organizational AI governance, which targets
organizations that develop and use AI systems. The framework is designed to
help organizations deploying AI systems translate ethical AI principles into
practice and align their AI systems and processes with the forthcoming European
AI Act. The hourglass framework includes governance requirements at the
environmental, organizational, and AI system levels. At the AI system level, we
connect governance requirements to AI system life cycles to ensure governance
throughout the system's life span. The governance model highlights the systemic
nature of AI governance and opens new research avenues into its practical
implementation, the mechanisms that connect different AI governance layers, and
the dynamics between the AI governance actors. The model also offers a starting
point for organizational decision-makers to consider the governance components
needed to ensure social acceptability, mitigate risks, and realize the
potential of AI.",None,-1
Global Convergence of Localized Policy Iteration in Networked Multi-Agent Reinforcement Learning,0.847323,"We study a multi-agent reinforcement learning (MARL) problem where the agents
interact over a given network. The goal of the agents is to cooperatively
maximize the average of their entropy-regularized long-term rewards. To
overcome the curse of dimensionality and to reduce communication, we propose a
Localized Policy Iteration (LPI) algorithm that provably learns a
near-globally-optimal policy using only local information. In particular, we
show that, despite restricting each agent's attention to only its $\kappa$-hop
neighborhood, the agents are able to learn a policy with an optimality gap that
decays polynomially in $\kappa$. In addition, we show the finite-sample
convergence of LPI to the global optimal policy, which explicitly captures the
trade-off between optimality and computational complexity in choosing $\kappa$.
Numerical simulations demonstrate the effectiveness of LPI.",None,-1
SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer,0.894548,"Point cloud completion has become increasingly popular among generation tasks
of 3D point clouds, as it is a challenging yet indispensable problem to recover
the complete shape of a 3D object from its partial observation. In this paper,
we propose a novel SeedFormer to improve the ability of detail preservation and
recovery in point cloud completion. Unlike previous methods based on a global
feature vector, we introduce a new shape representation, namely Patch Seeds,
which not only captures general structures from partial inputs but also
preserves regional information of local patterns. Then, by integrating seed
features into the generation process, we can recover faithful details for
complete point clouds in a coarse-to-fine manner. Moreover, we devise an
Upsample Transformer by extending the transformer structure into basic
operations of point generators, which effectively incorporates spatial and
semantic relationships between neighboring points. Qualitative and quantitative
evaluations demonstrate that our method outperforms state-of-the-art completion
networks on several benchmark datasets. Our code is available at
https://github.com/hrzhou2/seedformer.",https://github.com/hrzhou2/seedformer,-1
What is Software Quality for AI Engineers? Towards a Thinning of the Fog,0.872147,"It is often overseen that AI-enabled systems are also software systems and
therefore rely on software quality assurance (SQA). Thus, the goal of this
study is to investigate the software quality assurance strategies adopted
during the development, integration, and maintenance of AI/ML components and
code. We conducted semi-structured interviews with representatives of ten
Austrian SMEs that develop AI-enabled systems. A qualitative analysis of the
interview data identified 12 issues in the development of AI/ML components.
Furthermore, we identified when quality issues arise in AI/ML components and
how they are detected. The results of this study should guide future work on
software quality assurance processes and techniques for AI/ML components.",None,-1
Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models,0.691178,"Vision-Language Pre-training (VLP) models have achieved state-of-the-art
performance in numerous cross-modal tasks. Since they are optimized to capture
the statistical properties of intra- and inter-modality, there remains risk to
learn social biases presented in the data as well. In this work, we (1)
introduce a counterfactual-based bias measurement \emph{CounterBias} to
quantify the social bias in VLP models by comparing the [MASK]ed prediction
probabilities of factual and counterfactual samples; (2) construct a novel
VL-Bias dataset including 24K image-text pairs for measuring gender bias in VLP
models, from which we observed that significant gender bias is prevalent in VLP
models; and (3) propose a VLP debiasing method \emph{FairVLP} to minimize the
difference in the [MASK]ed prediction probabilities between factual and
counterfactual image-text pairs for VLP debiasing. Although CounterBias and
FairVLP focus on social bias, they are generalizable to serve as tools and
provide new insights to probe and regularize more knowledge in VLP models.",https://github.com/VL-Bias/VL-Bias,-1
Space-based gravitational wave signal detection and extraction with deep neural network,0.730964,"Space-based gravitational wave (GW) detectors will be able to observe signals
from sources that are otherwise nearly impossible from current ground-based
detection. Consequently, the well established signal detection method, matched
filtering, will require a complex template bank, leading to a computational
cost that is too expensive in practice. Here, we develop a high-accuracy GW
signal detection and extraction method for all space-based GW sources. As a
proof of concept, we show that a science-driven and uniform multi-stage
self-attention-based deep neural network can identify synthetic signals that
are submerged in Gaussian noise. Our method exhibits a detection rate exceeding
99% in identifying signals from various sources, with the signal-to-noise ratio
at 50, at a false alarm rate of 1%. while obtaining at least 95% similarity
compared with target signals. We further demonstrate the interpretability and
strong generalization behavior for several extended scenarios.",https://github.com/AI-HPC-Research-Team/space_signal_detection_1,-1
Multi-Class 3D Object Detection with Single-Class Supervision,0.0345281,"While multi-class 3D detectors are needed in many robotics applications,
training them with fully labeled datasets can be expensive in labeling cost. An
alternative approach is to have targeted single-class labels on disjoint data
samples. In this paper, we are interested in training a multi-class 3D object
detection model, while using these single-class labeled data. We begin by
detailing the unique stance of our ""Single-Class Supervision"" (SCS) setting
with respect to related concepts such as partial supervision and semi
supervision. Then, based on the case study of training the multi-class version
of Range Sparse Net (RSN), we adapt a spectrum of algorithms -- from supervised
learning to pseudo-labeling -- to fully exploit the properties of our SCS
setting, and perform extensive ablation studies to identify the most effective
algorithm and practice. Empirical experiments on the Waymo Open Dataset show
that proper training under SCS can approach or match full supervision training
while saving labeling costs.",None,-1
On Improving Cross-dataset Generalization of Deepfake Detectors,0.478737,"Facial manipulation by deep fake has caused major security risks and raised
severe societal concerns. As a countermeasure, a number of deep fake detection
methods have been proposed recently. Most of them model deep fake detection as
a binary classification problem using a backbone convolutional neural network
(CNN) architecture pretrained for the task. These CNN-based methods have
demonstrated very high efficacy in deep fake detection with the Area under the
Curve (AUC) as high as 0.99. However, the performance of these methods degrades
significantly when evaluated across datasets. In this paper, we formulate deep
fake detection as a hybrid combination of supervised and reinforcement learning
(RL) to improve its cross-dataset generalization performance. The proposed
method chooses the top-k augmentations for each test sample by an RL agent in
an image-specific manner. The classification scores, obtained using CNN, of all
the augmentations of each test image are averaged together for final real or
fake classification. Through extensive experimental validation, we demonstrate
the superiority of our method over existing published research in cross-dataset
generalization of deep fake detectors, thus obtaining state-of-the-art
performance.",None,3040
ExAgt: Expert-guided Augmentation for Representation Learning of Traffic Scenarios,0.0849589,"Representation learning in recent years has been addressed with
self-supervised learning methods. The input data is augmented into two
distorted views and an encoder learns the representations that are invariant to
distortions -- cross-view prediction. Augmentation is one of the key components
in cross-view self-supervised learning frameworks to learn visual
representations. This paper presents ExAgt, a novel method to include expert
knowledge for augmenting traffic scenarios, to improve the learnt
representations without any human annotation. The expert-guided augmentations
are generated in an automated fashion based on the infrastructure, the
interactions between the EGO and the traffic participants and an ideal sensor
model. The ExAgt method is applied in two state-of-the-art cross-view
prediction methods and the representations learnt are tested in downstream
tasks like classification and clustering. Results show that the ExAgt method
improves representation learning compared to using only standard augmentations
and it provides a better representation space stability. The code is available
at https://github.com/lab176344/ExAgt.",https://github.com/lab176344/ExAgt,-1
Reverse Survival Model (RSM): A Pipeline for Explaining Predictions of Deep Survival Models,0.122132,"The aim of survival analysis in healthcare is to estimate the probability of
occurrence of an event, such as a patient's death in an intensive care unit
(ICU). Recent developments in deep neural networks (DNNs) for survival analysis
show the superiority of these models in comparison with other well-known models
in survival analysis applications. Ensuring the reliability and explainability
of deep survival models deployed in healthcare is a necessity. Since DNN models
often behave like a black box, their predictions might not be easily trusted by
clinicians, especially when predictions are contrary to a physician's opinion.
A deep survival model that explains and justifies its decision-making process
could potentially gain the trust of clinicians. In this research, we propose
the reverse survival model (RSM) framework that provides detailed insights into
the decision-making process of survival models. For each patient of interest,
RSM can extract similar patients from a dataset and rank them based on the most
relevant features that deep survival models rely on for their predictions.",None,-1
Simplifying Multilingual News Clustering Through Projection From a Shared Space,0.451759,"The task of organizing and clustering multilingual news articles for media
monitoring is essential to follow news stories in real time. Most approaches to
this task focus on high-resource languages (mostly English), with low-resource
languages being disregarded. With that in mind, we present a much simpler
online system that is able to cluster an incoming stream of documents without
depending on language-specific features. We empirically demonstrate that the
use of multilingual contextual embeddings as the document representation
significantly improves clustering quality. We challenge previous crosslingual
approaches by removing the precondition of building monolingual clusters. We
model the clustering process as a set of linear classifiers to aggregate
similar documents, and correct closely-related multilingual clusters through
merging in an online fashion. Our system achieves state-of-the-art results on a
multilingual news stream clustering dataset, and we introduce a new evaluation
for zero-shot news clustering in multiple languages. We make our code available
as open-source.",https://github.com/Priberam/projected-news-clustering,-1
Are Vision Transformers Robust to Spurious Correlations?,0.460456,"Deep neural networks may be susceptible to learning spurious correlations
that hold on average but not in atypical test samples. As with the recent
emergence of vision transformer (ViT) models, it remains underexplored how
spurious correlations are manifested in such architectures. In this paper, we
systematically investigate the robustness of vision transformers to spurious
correlations on three challenging benchmark datasets and compare their
performance with popular CNNs. Our study reveals that when pre-trained on a
sufficiently large dataset, ViT models are more robust to spurious correlations
than CNNs. Key to their success is the ability to generalize better from the
examples where spurious correlations do not hold. Further, we perform extensive
ablations and experiments to understand the role of the self-attention
mechanism in providing robustness under spuriously correlated environments. We
hope that our work will inspire future research on further understanding the
robustness of ViT models.",https://github.com/deeplearning-wisc/vit-spurious-robustness,-1
SDS-200: A Swiss German Speech to Standard German Text Corpus,0.928154,"We present SDS-200, a corpus of Swiss German dialectal speech with Standard
German text translations, annotated with dialect, age, and gender information
of the speakers. The dataset allows for training speech translation, dialect
recognition, and speech synthesis systems, among others. The data was collected
using a web recording tool that is open to the public. Each participant was
given a text in Standard German and asked to translate it to their Swiss German
dialect before recording it. To increase the corpus quality, recordings were
validated by other participants. The data consists of 200 hours of speech by
around 4000 different speakers and covers a large part of the Swiss-German
dialect landscape. We release SDS-200 alongside a baseline speech translation
model, which achieves a word error rate (WER) of 30.3 and a BLEU score of 53.1
on the SDS-200 test set. Furthermore, we use SDS-200 to fine-tune a pre-trained
XLS-R model, achieving 21.6 WER and 64.0 BLEU.",https://github.com/stt4sg/,-1
Peripheral Vision Transformer,0.394505,"Human vision possesses a special type of visual processing systems called
peripheral vision. Partitioning the entire visual field into multiple contour
regions based on the distance to the center of our gaze, the peripheral vision
provides us the ability to perceive various visual features at different
regions. In this work, we take a biologically inspired approach and explore to
model peripheral vision in deep neural networks for visual recognition. We
propose to incorporate peripheral position encoding to the multi-head
self-attention layers to let the network learn to partition the visual field
into diverse peripheral regions given training data. We evaluate the proposed
network, dubbed PerViT, on ImageNet-1K and systematically investigate the inner
workings of the model for machine perception, showing that the network learns
to perceive visual data similarly to the way that human vision does. The
performance improvements in image classification over the baselines across
different model sizes demonstrate the efficacy of the proposed method.",https://github.com/juhongm999/pervit,-1
CliMedBERT: A Pre-trained Language Model for Climate and Health-related Text,0.77687,"Climate change is threatening human health in unprecedented orders and many
ways. These threats are expected to grow unless effective and evidence-based
policies are developed and acted upon to minimize or eliminate them. Attaining
such a task requires the highest degree of the flow of knowledge from science
into policy. The multidisciplinary, location-specific, and vastness of
published science makes it challenging to keep track of novel work in this
area, as well as making the traditional knowledge synthesis methods inefficient
in infusing science into policy. To this end, we consider developing multiple
domain-specific language models (LMs) with different variations from Climate-
and Health-related information, which can serve as a foundational step toward
capturing available knowledge to enable solving different tasks, such as
detecting similarities between climate- and health-related concepts,
fact-checking, relation extraction, evidence of health effects to policy text
generation, and more. To our knowledge, this is the first work that proposes
developing multiple domain-specific language models for the considered domains.
We will make the developed models, resources, and codebase available for the
researchers.",None,-1
"Alexa, Let's Work Together: Introducing the First Alexa Prize TaskBot Challenge on Conversational Task Assistance",0.926114,"Since its inception in 2016, the Alexa Prize program has enabled hundreds of
university students to explore and compete to develop conversational agents
through the SocialBot Grand Challenge. The goal of the challenge is to build
agents capable of conversing coherently and engagingly with humans on popular
topics for 20 minutes, while achieving an average rating of at least 4.0/5.0.
However, as conversational agents attempt to assist users with increasingly
complex tasks, new conversational AI techniques and evaluation platforms are
needed. The Alexa Prize TaskBot challenge, established in 2021, builds on the
success of the SocialBot challenge by introducing the requirements of
interactively assisting humans with real-world Cooking and Do-It-Yourself
tasks, while making use of both voice and visual modalities. This challenge
requires the TaskBots to identify and understand the user's need, identify and
integrate task and domain knowledge into the interaction, and develop new ways
of engaging the user without distracting them from the task at hand, among
other challenges. This paper provides an overview of the TaskBot challenge,
describes the infrastructure support provided to the teams with the CoBot
Toolkit, and summarizes the approaches the participating teams took to overcome
the research challenges. Finally, it analyzes the performance of the competing
TaskBots during the first year of the competition.",None,-1
Streamable Neural Fields,0.367445,"Neural fields have emerged as a new data representation paradigm and have
shown remarkable success in various signal representations. Since they preserve
signals in their network parameters, the data transfer by sending and receiving
the entire model parameters prevents this emerging technology from being used
in many practical scenarios. We propose streamable neural fields, a single
model that consists of executable sub-networks of various widths. The proposed
architectural and training techniques enable a single network to be streamable
over time and reconstruct different qualities and parts of signals. For
example, a smaller sub-network produces smooth and low-frequency signals, while
a larger sub-network can represent fine details. Experimental results have
shown the effectiveness of our method in various domains, such as 2D images,
videos, and 3D signed distance functions. Finally, we demonstrate that our
proposed method improves training stability, by exploiting parameter sharing.",https://github.com/jwcho5576/streamable_nf,-1
Calibrated Interpretation: Confidence Estimation in Semantic Parsing,0.513746,"Sequence generation models are increasingly being used to translate natural
language into programs, i.e. to perform executable semantic parsing. The fact
that semantic parsing aims to predict programs that can lead to executed
actions in the real world motivates developing safe systems. This in turn makes
measuring calibration -- a central component to safety -- particularly
important. We investigate the calibration of popular generation models across
four popular semantic parsing datasets, finding that it varies across models
and datasets. We then analyze factors associated with calibration error and
release new confidence-based challenge splits of two parsing datasets. To
facilitate the inclusion of calibration in semantic parsing evaluations, we
release a library for computing calibration metrics.",https://github.com/esteng/calibration_metric,-1
Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification,0.639928,"A key component of fact verification is thevevidence retrieval, often from
multiple documents. Recent approaches use dense representations and condition
the retrieval of each document on the previously retrieved ones. The latter
step is performed over all the documents in the collection, requiring storing
their dense representations in an index, thus incurring a high memory
footprint. An alternative paradigm is retrieve-and-rerank, where documents are
retrieved using methods such as BM25, their sentences are reranked, and further
documents are retrieved conditioned on these sentences, reducing the memory
requirements. However, such approaches can be brittle as they rely on
heuristics and assume hyperlinks between documents. We propose a novel
retrieve-and-rerank method for multi-hop retrieval, that consists of a
retriever that jointly scores documents in the knowledge source and sentences
from previously retrieved documents using an autoregressive formulation and is
guided by a proof system based on natural logic that dynamically terminates the
retrieval process if the evidence is deemed sufficient. This method is
competitive with current state-of-the-art methods on FEVER, HoVer and
FEVEROUS-S, while using $5$ to $10$ times less memory than competing systems.
Evaluation on an adversarial dataset indicates improved stability of our
approach compared to commonly deployed threshold-based methods. Finally, the
proof system helps humans predict model decisions correctly more often than
using the evidence alone.",https://github.com/Raldir/AdMIRaL,-1
Improved Beam Search for Hallucination Mitigation in Abstractive Summarization,0.388897,"Advancement in large pretrained language models has significantly improved
their performance for conditional language generation tasks including
summarization albeit with hallucinations. To reduce hallucinations,
conventional methods proposed improving beam search or using a fact checker as
a postprocessing step. In this paper, we investigate the use of the Natural
Language Inference (NLI) entailment metric to detect and prevent hallucinations
in summary generation. We propose an NLI-assisted beam re-ranking mechanism by
computing entailment probability scores between the input context and
summarization model-generated beams during saliency-enhanced greedy decoding.
Moreover, a diversity metric is introduced to compare its effectiveness against
vanilla beam search. Our proposed algorithm significantly outperforms vanilla
beam decoding on XSum and CNN/DM datasets.",None,-1
Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning,0.529104,"Indiscriminate data poisoning attacks are quite effective against supervised
learning. However, not much is known about their impact on unsupervised
contrastive learning (CL). This paper is the first to consider indiscriminate
poisoning attacks of contrastive learning. We propose Contrastive Poisoning
(CP), the first effective such attack on CL. We empirically show that
Contrastive Poisoning, not only drastically reduces the performance of CL
algorithms, but also attacks supervised learning models, making it the most
generalizable indiscriminate poisoning attack. We also show that CL algorithms
with a momentum encoder are more robust to indiscriminate poisoning, and
propose a new countermeasure based on matrix completion. Code is available at:
https://github.com/kaiwenzha/contrastive-poisoning.",https://github.com/kaiwenzha/contrastive-poisoning,-1
SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation,0.425696,"Conventional point cloud semantic segmentation methods usually employ an
encoder-decoder architecture, where mid-level features are locally aggregated
to extract geometric information. However, the over-reliance on these
class-agnostic local geometric representations may raise confusion between
local parts from different categories that are similar in appearance or
spatially adjacent. To address this issue, we argue that mid-level features can
be further enhanced with semantic information, and propose semantic-affine
transformation that transforms features of mid-level points belonging to
different categories with class-specific affine parameters. Based on this
technique, we propose SemAffiNet for point cloud semantic segmentation, which
utilizes the attention mechanism in the Transformer module to implicitly and
explicitly capture global structural knowledge within local parts for overall
comprehension of each category. We conduct extensive experiments on the
ScanNetV2 and NYUv2 datasets, and evaluate semantic-affine transformation on
various 3D point cloud and 2D image segmentation baselines, where both
qualitative and quantitative results demonstrate the superiority and
generalization ability of our proposed approach. Code is available at
https://github.com/wangzy22/SemAffiNet.",https://github.com/wangzy22/SemAffiNet,-1
On the Role of Field of View for Occlusion Removal with Airborne Optical Sectioning,0.312833,"Occlusion caused by vegetation is an essential problem for remote sensing
applications in areas, such as search and rescue, wildfire detection, wildlife
observation, surveillance, border control, and others. Airborne Optical
Sectioning (AOS) is an optical, wavelength-independent synthetic aperture
imaging technique that supports computational occlusion removal in real-time.
It can be applied with manned or unmanned aircrafts, such as drones. In this
article, we demonstrate a relationship between forest density and field of view
(FOV) of applied imaging systems. This finding was made with the help of a
simulated procedural forest model which offers the consideration of more
realistic occlusion properties than our previous statistical model. While AOS
has been explored with automatic and autonomous research prototypes in the
past, we present a free AOS integration for DJI systems. It enables bluelight
organizations and others to use and explore AOS with compatible, manually
operated, off-the-shelf drones. The (digitally cropped) default FOV for this
implementation was chosen based on our new finding.",https://github.com/tensorware/aos-simulation,-1
ASR Error Correction with Constrained Decoding on Operation Prediction,0.510625,"Error correction techniques remain effective to refine outputs from automatic
speech recognition (ASR) models. Existing end-to-end error correction methods
based on an encoder-decoder architecture process all tokens in the decoding
phase, creating undesirable latency. In this paper, we propose an ASR error
correction method utilizing the predictions of correction operations. More
specifically, we construct a predictor between the encoder and the decoder to
learn if a token should be kept (""K""), deleted (""D""), or changed (""C"") to
restrict decoding to only part of the input sequence embeddings (the ""C""
tokens) for fast inference. Experiments on three public datasets demonstrate
the effectiveness of the proposed approach in reducing the latency of the
decoding process in ASR correction. It enhances the inference speed by at least
three times (3.4 and 5.7 times) while maintaining the same level of accuracy
(with WER reductions of 0.53% and 1.69% respectively) for our two proposed
models compared to a solid encoder-decoder baseline. In the meantime, we
produce and release a benchmark dataset contributing to the ASR error
correction community to foster research along this line.",https://github.com/yangjingyuan/ConstDecoder,-1
Real-time Full-stack Traffic Scene Perception for Autonomous Driving with Roadside Cameras,0.906484,"We propose a novel and pragmatic framework for traffic scene perception with
roadside cameras. The proposed framework covers a full-stack of roadside
perception pipeline for infrastructure-assisted autonomous driving, including
object detection, object localization, object tracking, and multi-camera
information fusion. Unlike previous vision-based perception frameworks rely
upon depth offset or 3D annotation at training, we adopt a modular decoupling
design and introduce a landmark-based 3D localization method, where the
detection and localization can be well decoupled so that the model can be
easily trained based on only 2D annotations. The proposed framework applies to
either optical or thermal cameras with pinhole or fish-eye lenses. Our
framework is deployed at a two-lane roundabout located at Ellsworth Rd. and
State St., Ann Arbor, MI, USA, providing 7x24 real-time traffic flow monitoring
and high-precision vehicle trajectory extraction. The whole system runs
efficiently on a low-power edge computing device with all-component end-to-end
delay of less than 20ms.",None,-1
Real-time Gesture Animation Generation from Speech for Virtual Human Interaction,0.308295,"We propose a real-time system for synthesizing gestures directly from speech.
Our data-driven approach is based on Generative Adversarial Neural Networks to
model the speech-gesture relationship. We utilize the large amount of speaker
video data available online to train our 3D gesture model. Our model generates
speaker-specific gestures by taking consecutive audio input chunks of two
seconds in length. We animate the predicted gestures on a virtual avatar. We
achieve a delay below three seconds between the time of audio input and gesture
animation. Code and videos are available at
https://github.com/mrebol/Gestures-From-Speech",https://github.com/mrebol/Gestures-From-Speech,-1
An End-to-End Dialogue Summarization System for Sales Calls,0.337177,"Summarizing sales calls is a routine task performed manually by salespeople.
We present a production system which combines generative models fine-tuned for
customer-agent setting, with a human-in-the-loop user experience for an
interactive summary curation process. We address challenging aspects of
dialogue summarization task in a real-world setting including long input
dialogues, content validation, lack of labeled data and quality evaluation. We
show how GPT-3 can be leveraged as an offline data labeler to handle training
data scarcity and accommodate privacy constraints in an industrial setting.
Experiments show significant improvements by our models in tackling the
summarization and content validation tasks on public datasets.",https://github.com/google-research/google-research/tree/master/rouge,-1
EMA-VIO: Deep Visual-Inertial Odometry with External Memory Attention,0.913878,"Accurate and robust localization is a fundamental need for mobile agents.
Visual-inertial odometry (VIO) algorithms exploit the information from camera
and inertial sensors to estimate position and translation. Recent deep learning
based VIO models attract attentions as they provide pose information in a
data-driven way, without the need of designing hand-crafted algorithms.
Existing learning based VIO models rely on recurrent models to fuse multimodal
data and process sensor signal, which are hard to train and not efficient
enough. We propose a novel learning based VIO framework with external memory
attention that effectively and efficiently combines visual and inertial
features for states estimation. Our proposed model is able to estimate pose
accurately and robustly, even in challenging scenarios, e.g., on overcast days
and water-filled ground , which are difficult for traditional VIO algorithms to
extract visual features. Experiments validate that it outperforms both
traditional and learning based VIO baselines in different scenes.",None,-1
An Application of a Runtime Epistemic Probabilistic Event Calculus to Decision-making in e-Health Systems,0.424044,"We present and discuss a runtime architecture that integrates sensorial data
and classifiers with a logic-based decision-making system in the context of an
e-Health system for the rehabilitation of children with neuromotor disorders.
In this application, children perform a rehabilitation task in the form of
games. The main aim of the system is to derive a set of parameters the child's
current level of cognitive and behavioral performance (e.g., engagement,
attention, task accuracy) from the available sensors and classifiers (e.g., eye
trackers, motion sensors, emotion recognition techniques) and take decisions
accordingly. These decisions are typically aimed at improving the child's
performance by triggering appropriate re-engagement stimuli when their
attention is low, by changing the game or making it more difficult when the
child is losing interest in the task as it is too easy. Alongside
state-of-the-art techniques for emotion recognition and head pose estimation,
we use a runtime variant of a probabilistic and epistemic logic programming
dialect of the Event Calculus, known as the Epistemic Probabilistic Event
Calculus. In particular, the probabilistic component of this symbolic framework
allows for a natural interface with the machine learning techniques. We
overview the architecture and its components, and show some of its
characteristics through a discussion of a running example and experiments.
Under consideration for publication in Theory and Practice of Logic Programming
(TPLP).",https://github.com/dasaro/pec-anglican,-1
Admissible Policy Teaching through Reward Design,0.682902,"We study reward design strategies for incentivizing a reinforcement learning
agent to adopt a policy from a set of admissible policies. The goal of the
reward designer is to modify the underlying reward function cost-efficiently
while ensuring that any approximately optimal deterministic policy under the
new reward function is admissible and performs well under the original reward
function. This problem can be viewed as a dual to the problem of optimal reward
poisoning attacks: instead of forcing an agent to adopt a specific policy, the
reward designer incentivizes an agent to avoid taking actions that are
inadmissible in certain states. Perhaps surprisingly, and in contrast to the
problem of optimal reward poisoning attacks, we first show that the reward
design problem for admissible policy teaching is computationally challenging,
and it is NP-hard to find an approximately optimal reward modification. We then
proceed by formulating a surrogate problem whose optimal solution approximates
the optimal solution to the reward design problem in our setting, but is more
amenable to optimization techniques and analysis. For this surrogate problem,
we present characterization results that provide bounds on the value of the
optimal solution. Finally, we design a local search algorithm to solve the
surrogate problem and showcase its utility using simulation-based experiments.",None,-1
Balancing Multi-Domain Corpora Learning for Open-Domain Response Generation,0.111977,"Open-domain conversational systems are assumed to generate equally good
responses on multiple domains. Previous work achieved good performance on the
single corpus, but training and evaluating on multiple corpora from different
domains are less studied. This paper explores methods of generating relevant
responses for each of multiple multi-domain corpora. We first examine
interleaved learning which intermingles multiple corpora as the baseline. We
then investigate two multi-domain learning methods, labeled learning and
multi-task labeled learning, which encode each corpus through a unique corpus
embedding. Furthermore, we propose Domain-specific Frequency (DF), a novel
word-level importance weight that measures the relative importance of a word
for a specific corpus compared to other corpora. Based on DF, we propose
weighted learning, a method that integrates DF to the loss function. We also
adopt DF as a new evaluation metric. Extensive experiments show that our
methods gain significant improvements on both automatic and human evaluation.
We share our code and data for reproducibility",https://github.com/yujie-xing/Balancing_Multi_Domain_Corpus_Learning_for_Open_Domain_Response_Generation,-1
Fruit Quality Assessment with Densely Connected Convolutional Neural Network,0.649101,"Accurate recognition of food items along with quality assessment is of
paramount importance in the agricultural industry. Such automated systems can
speed up the wheel of the food processing sector and save tons of manual labor.
In this connection, the recent advancement of Deep learning-based architectures
has introduced a wide variety of solutions offering remarkable performance in
several classification tasks. In this work, we have exploited the concept of
Densely Connected Convolutional Neural Networks (DenseNets) for fruit quality
assessment. The feature propagation towards the deeper layers has enabled the
network to tackle the vanishing gradient problems and ensured the reuse of
features to learn meaningful insights. Evaluating on a dataset of 19,526 images
containing six fruits having three quality grades for each, the proposed
pipeline achieved a remarkable accuracy of 99.67%. The robustness of the model
was further tested for fruit classification and quality assessment tasks where
the model produced a similar performance, which makes it suitable for real-life
applications.",None,-1
"""Covid vaccine is against Covid but Oxford vaccine is made at Oxford!"" Semantic Interpretation of Proper Noun Compounds",0.308608,"Proper noun compounds, e.g., ""Covid vaccine"", convey information in a
succinct manner (a ""Covid vaccine"" is a ""vaccine that immunizes against the
Covid disease""). These are commonly used in short-form domains, such as news
headlines, but are largely ignored in information-seeking applications. To
address this limitation, we release a new manually annotated dataset, ProNCI,
consisting of 22.5K proper noun compounds along with their free-form semantic
interpretations. ProNCI is 60 times larger than prior noun compound datasets
and also includes non-compositional examples, which have not been previously
explored. We experiment with various neural models for automatically generating
the semantic interpretations from proper noun compounds, ranging from few-shot
prompting to supervised learning, with varying degrees of knowledge about the
constituent nouns. We find that adding targeted knowledge, particularly about
the common noun, results in performance gains of upto 2.8%. Finally, we
integrate our model generated interpretations with an existing Open IE system
and observe an 7.5% increase in yield at a precision of 85%. The dataset and
code are available at https://github.com/dair-iitd/pronci.",https://github.com/dair-iitd/pronci,3100
Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model,0.0634057,"Pre-trained multilingual language models play an important role in
cross-lingual natural language understanding tasks. However, existing methods
did not focus on learning the semantic structure of representation, and thus
could not optimize their performance. In this paper, we propose Multi-level
Multilingual Knowledge Distillation (MMKD), a novel method for improving
multilingual language models. Specifically, we employ a teacher-student
framework to adopt rich semantic representation knowledge in English BERT. We
propose token-, word-, sentence-, and structure-level alignment objectives to
encourage multiple levels of consistency between source-target pairs and
correlation similarity between teacher and student models. We conduct
experiments on cross-lingual evaluation benchmarks including XNLI, PAWS-X, and
XQuAD. Experimental results show that MMKD outperforms other baseline models of
similar size on XNLI and XQuAD and obtains comparable performance on PAWS-X.
Especially, MMKD obtains significant performance gains on low-resource
languages.",None,8728
One-Class Risk Estimation for One-Class Hyperspectral Image Classification,0.425893,"Hyperspectral imagery (HSI) one-class classification is aimed at identifying
a single target class from the HSI by using only knowing positive data, which
can significantly reduce the requirements for annotation. However, when
one-class classification meets HSI, it is difficult for classifiers to find a
balance between the overfitting and underfitting of positive data due to the
problems of distribution overlap and distribution imbalance. Although deep
learning-based methods are currently the mainstream to overcome distribution
overlap in HSI multiclassification, few studies focus on deep learning-based
HSI one-class classification. In this article, a weakly supervised deep HSI
one-class classifier, namely, HOneCls, is proposed, where a risk estimator,the
one-class risk estimator, is particularly introduced to make the fully
convolutional neural network (FCN) with the ability of one class classification
in the case of distribution imbalance. Extensive experiments (20 tasks in
total) were conducted to demonstrate the superiority of the proposed
classifier.",None,-1
Lyapunov function approach for approximation algorithm design and analysis: with applications in submodular maximization,0.44145,"We propose a two-phase systematical framework for approximation algorithm
design and analysis via Lyapunov function. The first phase consists of using
Lyapunov function as an input and outputs a continuous-time approximation
algorithm with a provable approximation ratio. The second phase then converts
this continuous-time algorithm to a discrete-time algorithm with almost the
same approximation ratio along with provable time complexity. One distinctive
feature of our framework is that we only need to know the parametric form of
the Lyapunov function whose complete specification will not be decided until
the end of the first phase by maximizing the approximation ratio of the
continuous-time algorithm. Some immediate benefits of the Lyapunov function
approach include: (i) unifying many existing algorithms; (ii) providing a
guideline to design and analyze new algorithms; and (iii) offering new
perspectives to potentially improve existing algorithms. We use various
submodular maximization problems as running examples to illustrate our
framework.",None,-1
Distillation-Resistant Watermarking for Model Protection in NLP,0.64474,"How can we protect the intellectual property of trained NLP models? Modern
NLP models are prone to stealing by querying and distilling from their publicly
exposed APIs. However, existing protection methods such as watermarking only
work for images but are not applicable to text. We propose
Distillation-Resistant Watermarking (DRW), a novel technique to protect NLP
models from being stolen via distillation. DRW protects a model by injecting
watermarks into the victim's prediction probability corresponding to a secret
key and is able to detect such a key by probing a suspect model. We prove that
a protected model still retains the original accuracy within a certain bound.
We evaluate DRW on a diverse set of NLP tasks including text classification,
part-of-speech tagging, and named entity recognition. Experiments show that DRW
protects the original model and detects stealing suspects at 100% mean average
precision for all four tasks while the prior method fails on two.",https://github.com/XuandongZhao/DRW,-1
Thin-Plate Spline Motion Model for Image Animation,0.99312,"Image animation brings life to the static object in the source image
according to the driving video. Recent works attempt to perform motion transfer
on arbitrary objects through unsupervised methods without using a priori
knowledge. However, it remains a significant challenge for current unsupervised
methods when there is a large pose gap between the objects in the source and
driving images. In this paper, a new end-to-end unsupervised motion transfer
framework is proposed to overcome such issue. Firstly, we propose thin-plate
spline motion estimation to produce a more flexible optical flow, which warps
the feature maps of the source image to the feature domain of the driving
image. Secondly, in order to restore the missing regions more realistically, we
leverage multi-resolution occlusion masks to achieve more effective feature
fusion. Finally, additional auxiliary loss functions are designed to ensure
that there is a clear division of labor in the network modules, encouraging the
network to generate high-quality images. Our method can animate a variety of
objects, including talking faces, human bodies, and pixel animations.
Experiments demonstrate that our method performs better on most benchmarks than
the state of the art with visible improvements in pose-related metrics.",https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model,-1
Motion Policy Networks,0.754901,"Collision-free motion generation in unknown environments is a core building
block for robot manipulation. Generating such motions is challenging due to
multiple objectives; not only should the solutions be optimal, the motion
generator itself must be fast enough for real-time performance and reliable
enough for practical deployment. A wide variety of methods have been proposed
ranging from local controllers to global planners, often being combined to
offset their shortcomings. We present an end-to-end neural model called Motion
Policy Networks (M$\pi$Nets) to generate collision-free, smooth motion from
just a single depth camera observation. M$\pi$Nets are trained on over 3
million motion planning problems in over 500,000 environments. Our experiments
show that M$\pi$Nets are significantly faster than global planners while
exhibiting the reactivity needed to deal with dynamic scenes. They are 46%
better than prior neural planners and more robust than local control policies.
Despite being only trained in simulation, M$\pi$Nets transfer well to the real
robot with noisy partial point clouds. Code and data are publicly available at
https://mpinets.github.io.",https://mpinets.github.io,-1
Handwritten Arabic Character Recognition for Children Writ-ing Using Convolutional Neural Network and Stroke Identification,0.891498,"Automatic Arabic handwritten recognition is one of the recently studied
problems in the field of Machine Learning. Unlike Latin languages, Arabic is a
Semitic language that forms a harder challenge, especially with variability of
patterns caused by factors such as writer age. Most of the studies focused on
adults, with only one recent study on children. Moreover, much of the recent
Machine Learning methods focused on using Convolutional Neural Networks, a
powerful class of neural networks that can extract complex features from
images. In this paper we propose a convolutional neural network (CNN) model
that recognizes children handwriting with an accuracy of 91% on the Hijja
dataset, a recent dataset built by collecting images of the Arabic characters
written by children, and 97% on Arabic Handwritten Character Dataset. The
results showed a good improvement over the proposed model from the Hijja
dataset authors, yet it reveals a bigger challenge to solve for children Arabic
handwritten character recognition. Moreover, we proposed a new approach using
multi models instead of single model based on the number of strokes in a
character, and merged Hijja with AHCD which reached an averaged prediction
accuracy of 96%.",None,-1
iCaps: Iterative Category-level Object Pose and Shape Estimation,0.831346,"This paper proposes a category-level 6D object pose and shape estimation
approach iCaps, which allows tracking 6D poses of unseen objects in a category
and estimating their 3D shapes. We develop a category-level auto-encoder
network using depth images as input, where feature embeddings from the
auto-encoder encode poses of objects in a category. The auto-encoder can be
used in a particle filter framework to estimate and track 6D poses of objects
in a category. By exploiting an implicit shape representation based on signed
distance functions, we build a LatentNet to estimate a latent representation of
the 3D shape given the estimated pose of an object. Then the estimated pose and
shape can be used to update each other in an iterative way. Our category-level
6D object pose and shape estimation pipeline only requires 2D detection and
segmentation for initialization. We evaluate our approach on a publicly
available dataset and demonstrate its effectiveness. In particular, our method
achieves comparably high accuracy on shape estimation.",http://github.com/aerogjy/iCaps,-1
Label-enhanced Prototypical Network with Contrastive Learning for Multi-label Few-shot Aspect Category Detection,0.826386,"Multi-label aspect category detection allows a given review sentence to
contain multiple aspect categories, which is shown to be more practical in
sentiment analysis and attracting increasing attention. As annotating large
amounts of data is time-consuming and labor-intensive, data scarcity occurs
frequently in real-world scenarios, which motivates multi-label few-shot aspect
category detection. However, research on this problem is still in infancy and
few methods are available. In this paper, we propose a novel label-enhanced
prototypical network (LPN) for multi-label few-shot aspect category detection.
The highlights of LPN can be summarized as follows. First, it leverages label
description as auxiliary knowledge to learn more discriminative prototypes,
which can retain aspect-relevant information while eliminating the harmful
effect caused by irrelevant aspects. Second, it integrates with contrastive
learning, which encourages that the sentences with the same aspect label are
pulled together in embedding space while simultaneously pushing apart the
sentences with different aspect labels. In addition, it introduces an adaptive
multi-label inference module to predict the aspect count in the sentence, which
is simple yet effective. Extensive experimental results on three datasets
demonstrate that our proposed model LPN can consistently achieve
state-of-the-art performance.",None,-1
Spatial-temporal Concept based Explanation of 3D ConvNets,0.0513935,"Recent studies have achieved outstanding success in explaining 2D image
recognition ConvNets. On the other hand, due to the computation cost and
complexity of video data, the explanation of 3D video recognition ConvNets is
relatively less studied. In this paper, we present a 3D ACE (Automatic
Concept-based Explanation) framework for interpreting 3D ConvNets. In our
approach: (1) videos are represented using high-level supervoxels, which is
straightforward for human to understand; and (2) the interpreting framework
estimates a score for each voxel, which reflects its importance in the decision
procedure. Experiments show that our method can discover spatial-temporal
concepts of different importance-levels, and thus can explore the influence of
the concepts on a target task, such as action classification, in-depth. The
codes are publicly available.",https://github.com/OrangeeJi/3D-ACE,-1
Fine-grained Czech News Article Dataset: An Interdisciplinary Approach to Trustworthiness Analysis,0.24042,"We present the Verifee Dataset: a novel dataset of news articles with
fine-grained trustworthiness annotations. We develop a detailed methodology
that assesses the texts based on their parameters encompassing editorial
transparency, journalist conventions, and objective reporting while penalizing
manipulative techniques. We bring aboard a diverse set of researchers from
social, media, and computer sciences to overcome barriers and limited framing
of this interdisciplinary problem. We collect over $10,000$ unique articles
from almost $60$ Czech online news sources. These are categorized into one of
the $4$ classes across the credibility spectrum we propose, raging from
entirely trustworthy articles all the way to the manipulative ones. We produce
detailed statistics and study trends emerging throughout the set. Lastly, we
fine-tune multiple popular sequence-to-sequence language models using our
dataset on the trustworthiness classification task and report the best testing
F-1 score of $0.52$. We open-source the dataset, annotation methodology, and
annotators' instructions in full length at https://verifee.ai/research to
enable easy build-up work. We believe similar methods can help prevent
disinformation and educate in the realm of media literacy.",https://verifee.ai/research,351
DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation,0.987203,"Dialog response generation in open domain is an important research topic
where the main challenge is to generate relevant and diverse responses. In this
paper, we propose a new dialog pre-training framework called DialogVED, which
introduces continuous latent variables into the enhanced encoder-decoder
pre-training framework to increase the relevance and diversity of responses.
With the help of a large dialog corpus (Reddit), we pre-train the model using
the following 4 tasks adopted in language models (LMs) and variational
autoencoders (VAEs): 1) masked language model; 2) response generation; 3)
bag-of-words prediction; and 4) KL divergence reduction. We also add additional
parameters to model the turn structure in dialogs to improve the performance of
the pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and
DSTC7-AVSD benchmarks for response generation. Experimental results show that
our model achieves the new state-of-the-art results on all these datasets.",None,-1
Generalized Strategic Classification and the Case of Aligned Incentives,0.609014,"Strategic classification studies learning in settings where self-interested
users can strategically modify their features to obtain favorable predictive
outcomes. A key working assumption, however, is that ""favorable"" always means
""positive""; this may be appropriate in some applications (e.g., loan approval),
but reduces to a fairly narrow view of what user interests can be. In this work
we argue for a broader perspective on what accounts for strategic user
behavior, and propose and study a flexible model of generalized strategic
classification. Our generalized model subsumes most current models but includes
other novel settings; among these, we identify and target one intriguing
sub-class of problems in which the interests of users and the system are
aligned. This setting reveals a surprising fact: that standard max-margin
losses are ill-suited for strategic inputs. Returning to our fully generalized
model, we propose a novel max-margin framework for strategic learning that is
practical and effective, and which we analyze theoretically. We conclude with a
set of experiments that empirically demonstrate the utility of our approach.",https://github.com/SagiLevanon1/GSC,-1
Dual-former: Hybrid Self-attention Transformer for Efficient Image Restoration,0.370225,"Recently, image restoration transformers have achieved comparable performance
with previous state-of-the-art CNNs. However, how to efficiently leverage such
architectures remains an open problem. In this work, we present Dual-former
whose critical insight is to combine the powerful global modeling ability of
self-attention modules and the local modeling ability of convolutions in an
overall architecture. With convolution-based Local Feature Extraction modules
equipped in the encoder and the decoder, we only adopt a novel Hybrid
Transformer Block in the latent layer to model the long-distance dependence in
spatial dimensions and handle the uneven distribution between channels. Such a
design eliminates the substantial computational complexity in previous image
restoration transformers and achieves superior performance on multiple image
restoration tasks. Experiments demonstrate that Dual-former achieves a 1.91dB
gain over the state-of-the-art MAXIM method on the Indoor dataset for single
image dehazing while consuming only 4.2% GFLOPs as MAXIM. For single image
deraining, it exceeds the SOTA method by 0.1dB PSNR on the average results of
five datasets with only 21.5% GFLOPs. Dual-former also substantially surpasses
the latest desnowing method on various datasets, with fewer parameters.",None,-1
Pre-training to Match for Unified Low-shot Relation Extraction,0.588604,"Low-shot relation extraction~(RE) aims to recognize novel relations with very
few or even no samples, which is critical in real scenario application.
Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem
to be with similar target but require totally different underlying abilities.
In this paper, we propose Multi-Choice Matching Networks to unify low-shot
relation extraction. To fill in the gap between zero-shot and few-shot RE, we
propose the triplet-paraphrase meta-training, which leverages triplet
paraphrase to pre-train zero-shot label matching ability and uses meta-learning
paradigm to learn few-shot instance summarizing ability. Experimental results
on three different low-shot RE tasks show that the proposed method outperforms
strong baselines by a large margin, and achieve the best performance on
few-shot RE leaderboard.",https://github.com/fc-liu/MCMN,-1
Distance Matters in Human-Object Interaction Detection,0.53429,"Human-Object Interaction (HOI) detection has received considerable attention
in the context of scene understanding. Despite the growing progress on
benchmarks, we realize that existing methods often perform unsatisfactorily on
distant interactions, where the leading causes are two-fold: 1) Distant
interactions are by nature more difficult to recognize than close ones. A
natural scene often involves multiple humans and objects with intricate spatial
relations, making the interaction recognition for distant human-object largely
affected by complex visual context. 2) Insufficient number of distant
interactions in benchmark datasets results in under-fitting on these instances.
To address these problems, in this paper, we propose a novel two-stage method
for better handling distant interactions in HOI detection. One essential
component in our method is a novel Far Near Distance Attention module. It
enables information propagation between humans and objects, whereby the spatial
distance is skillfully taken into consideration. Besides, we devise a novel
Distance-Aware loss function which leads the model to focus more on distant yet
rare interactions. We conduct extensive experiments on two challenging datasets
- HICO-DET and V-COCO. The results demonstrate that the proposed method can
surpass existing approaches by a large margin, resulting in new
state-of-the-art performance.",None,-1
Statistical guarantees for sparse deep learning,0.53075,"Neural networks are becoming increasingly popular in applications, but our
mathematical understanding of their potential and limitations is still limited.
In this paper, we further this understanding by developing statistical
guarantees for sparse deep learning. In contrast to previous work, we consider
different types of sparsity, such as few active connections, few active nodes,
and other norm-based types of sparsity. Moreover, our theories cover important
aspects that previous theories have neglected, such as multiple outputs,
regularization, and l2-loss. The guarantees have a mild dependence on network
widths and depths, which means that they support the application of sparse but
wide and deep networks from a statistical perspective. Some of the concepts and
tools that we use in our derivations are uncommon in deep learning and, hence,
might be of additional interest.",None,-1
Anomaly detection optimization using big data and deep learning to reduce false-positive,0.561707,"Anomaly-based Intrusion Detection System (IDS) has been a hot research topic
because of its ability to detect new threats rather than only memorized
signatures threats of signature-based IDS. Especially after the availability of
advanced technologies that increase the number of hacking tools and increase
the risk impact of an attack. The problem of any anomaly-based model is its
high false-positive rate. The high false-positive rate is the reason why
anomaly IDS is not commonly applied in practice. Because anomaly-based models
classify an unseen pattern as a threat where it may be normal but not included
in the training dataset. This type of problem is called overfitting where the
model is not able to generalize. Optimizing Anomaly-based models by having a
big training dataset that includes all possible normal cases may be an optimal
solution but could not be applied in practice. Although we can increase the
number of training samples to include much more normal cases, still we need a
model that has more ability to generalize. In this research paper, we propose
applying deep model instead of traditional models because it has more ability
to generalize. Thus, we will obtain less false-positive by using big data and
deep model. We made a comparison between machine learning and deep learning
algorithms in the optimization of anomaly-based IDS by decreasing the
false-positive rate. We did an experiment on the NSL-KDD benchmark and compared
our results with one of the best used classifiers in traditional learning in
IDS optimization. The experiment shows 10% lower false-positive by using deep
learning instead of traditional learning.",None,-1
Non-Deterministic Face Mask Removal Based On 3D Priors,0.131635,"This paper presents a novel image inpainting framework for face mask removal.
Although current methods have demonstrated their impressive ability in
recovering damaged face images, they suffer from two main problems: the
dependence on manually labeled missing regions and the deterministic result
corresponding to each input. The proposed approach tackles these problems by
integrating a multi-task 3D face reconstruction module with a face inpainting
module. Given a masked face image, the former predicts a 3DMM-based
reconstructed face together with a binary occlusion map, providing dense
geometrical and textural priors that greatly facilitate the inpainting task of
the latter. By gradually controlling the 3D shape parameters, our method
generates high-quality dynamic inpainting results with different expressions
and mouth movements. Qualitative and quantitative experiments verify the
effectiveness of the proposed method.",https://github.com/face3d0725/face_de_mask,11135
BIC: Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency,0.893296,"Twitter bots are automatic programs operated by malicious actors to
manipulate public opinion and spread misinformation. Research efforts have been
made to automatically identify bots based on texts and networks on social
media. Existing methods only leverage texts or networks alone, and while few
works explored the shallow combination of the two modalities, we hypothesize
that the interaction and information exchange between texts and graphs could be
crucial for holistically evaluating bot activities on social media. In
addition, according to a recent survey (Cresci, 2020), Twitter bots are
constantly evolving while advanced bots steal genuine users' tweets and dilute
their malicious content to evade detection. This results in greater
inconsistency across the timeline of novel Twitter bots, which warrants more
attention. In light of these challenges, we propose BIC, a Twitter Bot
detection framework with text-graph Interaction and semantic Consistency.
Specifically, in addition to separately modeling the two modalities on social
media, BIC employs a text-graph interaction module to enable information
exchange across modalities in the learning process. In addition, given the
stealing behavior of novel Twitter bots, BIC proposes to model semantic
consistency in tweets based on attention weights while using it to augment the
decision process. Extensive experiments demonstrate that BIC consistently
outperforms state-of-the-art baselines on two widely adopted datasets. Further
analyses reveal that text-graph interactions and modeling semantic consistency
are essential improvements and help combat bot evolution.",https://github.com/Bjarten/early-stopping-pytorch,-1
On the Role of Bidirectionality in Language Model Pre-Training,0.557968,"Prior work on language model pre-training has explored different
architectures and learning objectives, but differences in data, hyperparameters
and evaluation make a principled comparison difficult. In this work, we focus
on bidirectionality as a key factor that differentiates existing approaches,
and present a comprehensive study of its role in next token prediction, text
infilling, zero-shot priming and fine-tuning. We propose a new framework that
generalizes prior approaches, including fully unidirectional models like GPT,
fully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.
Our framework distinguishes between two notions of bidirectionality
(bidirectional context and bidirectional attention) and allows us to control
each of them separately. We find that the optimal configuration is largely
application-dependent (e.g., bidirectional attention is beneficial for
fine-tuning and infilling, but harmful for next token prediction and zero-shot
priming). We train models with up to 6.7B parameters, and find differences to
remain consistent at scale. While prior work on scaling has focused on
left-to-right autoregressive models, our results suggest that this approach
comes with some trade-offs, and it might be worthwhile to develop very large
bidirectional models.",None,-1
Probabilistic and Non-Deterministic Event Data in Process Mining: Embedding Uncertainty in Process Analysis Techniques,0.230994,"Process mining is a subfield of process science that analyzes event data
collected in databases called event logs. Recently, novel types of event data
have become of interest due to the wide industrial application of process
mining analyses. In this paper, we examine uncertain event data. Such data
contain meta-attributes describing the amount of imprecision tied with
attributes recorded in an event log. We provide examples of uncertain event
data, present the state of the art in regard of uncertainty in process mining,
and illustrate open challenges related to this research direction.",None,472
The SIGMORPHON 2022 Shared Task on Morpheme Segmentation,0.458899,"The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems
to decompose a word into a sequence of morphemes and covered most types of
morphology: compounds, derivations, and inflections. Subtask 1, word-level
morpheme segmentation, covered 5 million words in 9 languages (Czech, English,
Spanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13
system submissions from 7 teams and the best system averaged 97.29% F1 score
across all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2,
sentence-level morpheme segmentation, covered 18,735 sentences in 3 languages
(Czech, English, Mongolian), received 10 system submissions from 3 teams, and
the best systems outperformed all three state-of-the-art subword tokenization
methods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis
and support any type of future studies, we released all system predictions, the
evaluation script, and all gold standard datasets.",https://github.com/sigmorphon/2022SegmentationST,23512
Interactive Multi-Class Tiny-Object Detection,0.804051,"Annotating tens or hundreds of tiny objects in a given image is laborious yet
crucial for a multitude of Computer Vision tasks. Such imagery typically
contains objects from various categories, yet the multi-class interactive
annotation setting for the detection task has thus far been unexplored. To
address these needs, we propose a novel interactive annotation method for
multiple instances of tiny objects from multiple classes, based on a few
point-based user inputs. Our approach, C3Det, relates the full image context
with annotator inputs in a local and global manner via late-fusion and
feature-correlation, respectively. We perform experiments on the Tiny-DOTA and
LCell datasets using both two-stage and one-stage object detection
architectures to verify the efficacy of our approach. Our approach outperforms
existing approaches in interactive annotation, achieving higher mAP with fewer
clicks. Furthermore, we validate the annotation efficiency of our approach in a
user study where it is shown to be 2.85x faster and yield only 0.36x task load
(NASA-TLX, lower is better) compared to manual annotation. The code is
available at
https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection.",https://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection,-1
Mirror modular cloning and fast quantum associative retrieval,0.102354,"We show that a quantum state can be perfectly cloned up to global mirroring
with a unitary transformation that depends on one single parameter. We then
show that this is equivalent to ""perfect"" cloning for quantum associative
memories which, as a consequence efficiently hold exponentially more
information than their classical counterparts. Finally, we present a quantum
associative retrieval algorithm which can correct corrupted inputs and is
exponentially faster than the Grover algorithm.",None,-1
Asynchronous Optimisation for Event-based Visual Odometry,0.794295,"Event cameras open up new possibilities for robotic perception due to their
low latency and high dynamic range. On the other hand, developing effective
event-based vision algorithms that fully exploit the beneficial properties of
event cameras remains work in progress. In this paper, we focus on event-based
visual odometry (VO). While existing event-driven VO pipelines have adopted
continuous-time representations to asynchronously process event data, they
either assume a known map, restrict the camera to planar trajectories, or
integrate other sensors into the system. Towards map-free event-only monocular
VO in SE(3), we propose an asynchronous structure-from-motion optimisation
back-end. Our formulation is underpinned by a principled joint optimisation
problem involving non-parametric Gaussian Process motion modelling and
incremental maximum a posteriori inference. A high-performance incremental
computation engine is employed to reason about the camera trajectory with every
incoming event. We demonstrate the robustness of our asynchronous back-end in
comparison to frame-based methods which depend on accurate temporal
accumulation of measurements.",None,-1
Normalizing Flows for Human Pose Anomaly Detection,0.701968,"Video anomaly detection is an ill-posed problem because it relies on many
parameters such as appearance, pose, camera angle, background, and more. We
distill the problem to anomaly detection of human pose, thus decreasing the
risk of nuisance parameters such as appearance affecting the result. Focusing
on pose alone also has the side benefit of reducing bias against distinct
minority groups. Our model works directly on human pose graph sequences and is
exceptionally lightweight (~1K parameters), capable of running on any machine
able to run the pose estimation with negligible additional resources. We
leverage the highly compact pose representation in a normalizing flows
framework, which we extend to tackle the unique characteristics of
spatio-temporal pose data and show its advantages in this use case. The
algorithm is quite general and can handle training data of only normal examples
as well as a supervised setting that consists of labeled normal and abnormal
examples. We report state-of-the-art results on two anomaly detection
benchmarks - the unsupervised ShanghaiTech dataset and the recent supervised
UBnormal dataset.",https://github.com/orhir/STG-NF,-1
Meta Policy Learning for Cold-Start Conversational Recommendation,0.513264,"Conversational recommender systems (CRS) explicitly solicit users'
preferences for improved recommendations on the fly. Most existing CRS
solutions count on a single policy trained by reinforcement learning for a
population of users. However, for users new to the system, such a global policy
becomes ineffective to satisfy them, i.e., the cold-start challenge. In this
paper, we study CRS policy learning for cold-start users via meta-reinforcement
learning. We propose to learn a meta policy and adapt it to new users with only
a few trials of conversational recommendations. To facilitate fast policy
adaptation, we design three synergetic components. Firstly, we design a
meta-exploration policy dedicated to identifying user preferences via a few
exploratory conversations, which accelerates personalized policy adaptation
from the meta policy. Secondly, we adapt the item recommendation module for
each user to maximize the recommendation quality based on the collected
conversation states during conversations. Thirdly, we propose a
Transformer-based state encoder as the backbone to connect the previous two
components. It provides comprehensive state representations by modeling
complicated relations between positive and negative feedback during the
conversation. Extensive experiments on three datasets demonstrate the advantage
of our solution in serving new users, compared with a rich set of
state-of-the-art CRS solutions.",https://github.com/zdchu/MetaCRS.git,-1
3D Dual-Fusion: Dual-Domain Dual-Query Camera-LiDAR Fusion for 3D Object Detection,0.635653,"Fusing data from cameras and LiDAR sensors is an essential technique to
achieve robust 3D object detection. One key challenge in camera-LiDAR fusion
involves mitigating the large domain gap between the two sensors in terms of
coordinates and data distribution when fusing their features. In this paper, we
propose a novel camera-LiDAR fusion architecture called, 3D Dual-Fusion, which
is designed to mitigate the gap between the feature representations of camera
and LiDAR data. The proposed method fuses the features of the camera-view and
3D voxel-view domain and models their interactions through deformable
attention. We redesign the transformer fusion encoder to aggregate the
information from the two domains. Two major changes include 1) dual query-based
deformable attention to fuse the dual-domain features interactively and 2) 3D
local self-attention to encode the voxel-domain queries prior to dual-query
decoding. The results of an experimental evaluation show that the proposed
camera-LiDAR fusion architecture achieved competitive performance on the KITTI
and nuScenes datasets, with state-of-the-art performances in some 3D object
detection benchmarks categories.",None,3955
Large-Field Contextual Feature Learning for Glass Detection,0.773657,"Glass is very common in our daily life. Existing computer vision systems
neglect it and thus may have severe consequences, e.g., a robot may crash into
a glass wall. However, sensing the presence of glass is not straightforward.
The key challenge is that arbitrary objects/scenes can appear behind the glass.
In this paper, we propose an important problem of detecting glass surfaces from
a single RGB image. To address this problem, we construct the first large-scale
glass detection dataset (GDD) and propose a novel glass detection network,
called GDNet-B, which explores abundant contextual cues in a large
field-of-view via a novel large-field contextual feature integration (LCFI)
module and integrates both high-level and low-level boundary features with a
boundary feature enhancement (BFE) module. Extensive experiments demonstrate
that our GDNet-B achieves satisfying glass detection results on the images
within and beyond the GDD testing set. We further validate the effectiveness
and generalization capability of our proposed GDNet-B by applying it to other
vision tasks, including mirror segmentation and salient object detection.
Finally, we show the potential applications of glass detection and discuss
possible future research directions.",None,-1
Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud Analysis,0.59808,"Point clouds are characterized by irregularity and unstructuredness, which
pose challenges in efficient data exploitation and discriminative feature
extraction. In this paper, we present an unsupervised deep neural architecture
called Flattening-Net to represent irregular 3D point clouds of arbitrary
geometry and topology as a completely regular 2D point geometry image (PGI)
structure, in which coordinates of spatial points are captured in colors of
image pixels. \mr{Intuitively, Flattening-Net implicitly approximates a locally
smooth 3D-to-2D surface flattening process while effectively preserving
neighborhood consistency.} \mr{As a generic representation modality, PGI
inherently encodes the intrinsic property of the underlying manifold structure
and facilitates surface-style point feature aggregation.} To demonstrate its
potential, we construct a unified learning framework directly operating on PGIs
to achieve \mr{diverse types of high-level and low-level} downstream
applications driven by specific task networks, including classification,
segmentation, reconstruction, and upsampling. Extensive experiments demonstrate
that our methods perform favorably against the current state-of-the-art
competitors. We will make the code and data publicly available at
https://github.com/keeganhk/Flattening-Net.",https://github.com/keeganhk/Flattening-Net,9714
CIRCLe: Color Invariant Representation Learning for Unbiased Classification of Skin Lesions,0.952044,"While deep learning based approaches have demonstrated expert-level
performance in dermatological diagnosis tasks, they have also been shown to
exhibit biases toward certain demographic attributes, particularly skin types
(e.g., light versus dark), a fairness concern that must be addressed. We
propose CIRCLe, a skin color invariant deep representation learning method for
improving fairness in skin lesion classification. CIRCLe is trained to classify
images by utilizing a regularization loss that encourages images with the same
diagnosis but different skin types to have similar latent representations.
Through extensive evaluation and ablation studies, we demonstrate CIRCLe's
superior performance over the state-of-the-art when evaluated on 16k+ images
spanning 6 Fitzpatrick skin types and 114 diseases, using classification
accuracy, equal opportunity difference (for light versus dark groups), and
normalized accuracy range, a new measure we propose to assess fairness on
multiple skin type groups.",https://github.com/arezou-pakzad/CIRCLe,13990
What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation,0.280826,"Accurate automatic evaluation metrics for open-domain dialogs are in high
demand. Existing model-based metrics for system response evaluation are trained
on human annotated data, which is cumbersome to collect. In this work, we
propose to use information that can be automatically extracted from the next
user utterance, such as its sentiment or whether the user explicitly ends the
conversation, as a proxy to measure the quality of the previous system
response. This allows us to train on a massive set of dialogs with weak
supervision, without requiring manual system turn quality annotations.
Experiments show that our model is comparable to models trained on human
annotated data. Furthermore, our model generalizes across both spoken and
written open-domain dialog corpora collected from real and paid users.",https://github.com/exe1023/DialEvalMetrics,-1
Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates,0.32746,"Calibration strengthens the trustworthiness of black-box models by producing
better accurate confidence estimates on given examples. However, little is
known about if model explanations can help confidence calibration. Intuitively,
humans look at important features attributions and decide whether the model is
trustworthy. Similarly, the explanations can tell us when the model may or may
not know. Inspired by this, we propose a method named CME that leverages model
explanations to make the model less confident with non-inductive attributions.
The idea is that when the model is not highly confident, it is difficult to
identify strong indications of any class, and the tokens accordingly do not
have high attribution scores for any class and vice versa. We conduct extensive
experiments on six datasets with two popular pre-trained language models in the
in-domain and out-of-domain settings. The results show that CME improves
calibration performance in all settings. The expected calibration errors are
further reduced when combined with temperature scaling. Our findings highlight
that model explanations can help calibrate posterior estimates.",https://github.com/crazyofapple/CME-EMNLP2022/,-1
LongShortNet: Exploring Temporal and Semantic Features Fusion in Streaming Perception,0.319893,"Streaming perception is a critical task in autonomous driving that requires
balancing the latency and accuracy of the autopilot system. However, current
methods for streaming perception are limited as they only rely on the current
and adjacent two frames to learn movement patterns. This restricts their
ability to model complex scenes, often resulting in poor detection results. To
address this limitation, we propose LongShortNet, a novel dual-path network
that captures long-term temporal motion and integrates it with short-term
spatial semantics for real-time perception. LongShortNet is notable as it is
the first work to extend long-term temporal modeling to streaming perception,
enabling spatiotemporal feature fusion. We evaluate LongShortNet on the
challenging Argoverse-HD dataset and demonstrate that it outperforms existing
state-of-the-art methods with almost no additional computational cost.",https://github.com/LiChenyang-Github/LongShortNet,-1
Ensemble Semi-supervised Entity Alignment via Cycle-teaching,0.251698,"Entity alignment is to find identical entities in different knowledge graphs.
Although embedding-based entity alignment has recently achieved remarkable
progress, training data insufficiency remains a critical challenge.
Conventional semi-supervised methods also suffer from the incorrect entity
alignment in newly proposed training data. To resolve these issues, we design
an iterative cycle-teaching framework for semi-supervised entity alignment. The
key idea is to train multiple entity alignment models (called aligners)
simultaneously and let each aligner iteratively teach its successor the
proposed new entity alignment. We propose a diversity-aware alignment selection
method to choose reliable entity alignment for each aligner. We also design a
conflict resolution mechanism to resolve the alignment conflict when combining
the new alignment of an aligner and that from its teacher. Besides, considering
the influence of cycle-teaching order, we elaborately design a strategy to
arrange the optimal order that can maximize the overall performance of multiple
aligners. The cycle-teaching process can break the limitations of each model's
learning capability and reduce the noise in new training data, leading to
improved performance. Extensive experiments on benchmark datasets demonstrate
the effectiveness of the proposed cycle-teaching framework, which significantly
outperforms the state-of-the-art models when the training data is insufficient
and the new entity alignment has much noise.",https://github.com/JadeXIN/CycTEA,-1
Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL,0.474923,"Multi-agent Reinforcement Learning (MARL) is a powerful tool for training
autonomous agents acting independently in a common environment. However, it can
lead to sub-optimal behavior when individual incentives and group incentives
diverge. Humans are remarkably capable at solving these social dilemmas. It is
an open problem in MARL to replicate such cooperative behaviors in selfish
agents. In this work, we draw upon the idea of formal contracting from
economics to overcome diverging incentives between agents in MARL. We propose
an augmentation to a Markov game where agents voluntarily agree to binding
transfers of reward, under pre-specified conditions. Our contributions are
theoretical and empirical. First, we show that this augmentation makes all
subgame-perfect equilibria of all Fully Observable Markov Games exhibit
socially optimal behavior, given a sufficiently rich space of contracts. Next,
we show that for general contract spaces, and even under partial observability,
richer contract spaces lead to higher welfare. Hence, contract space design
solves an exploration-exploitation tradeoff, sidestepping incentive issues. We
complement our theoretical analysis with experiments. Issues of exploration in
the contracting augmentation are mitigated using a training methodology
inspired by multi-objective reinforcement learning: Multi-Objective Contract
Augmentation Learning (MOCA). We test our methodology in static, single-move
games, as well as dynamic domains that simulate traffic, pollution management
and common pool resource management.",https://github.com/Algorithmic-Alignment-Lab/contracts,-1
Contrastive Decoding: Open-ended Text Generation as Optimization,0.980781,"Given a language model (LM), maximum probability is a poor decoding objective
for open-ended generation, because it produces short and repetitive text. On
the other hand, sampling can often produce incoherent text that drifts from the
original topics. We propose contrastive decoding (CD), a reliable decoding
approach that optimizes a contrastive objective subject to a plausibility
constraint. The contrastive objective returns the difference between the
likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM
(called the amateur, e.g. OPT-125M), and the constraint ensures that the
outputs are plausible. CD is inspired by the fact that the failures of larger
LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and
that this difference signals which texts should be preferred. CD requires zero
additional training, and produces higher quality text than decoding from the
larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and
significantly outperforms four strong decoding algorithms (e.g., nucleus,
top-k) in automatic and human evaluations across wikipedia, news and story
domains.",https://github.com/XiangLi1999/ContrastiveDecoding.git,-1
DDIPNet and DDIPNet+: Discriminant Deep Image Prior Networks for Remote Sensing Image Classification,0.176068,"Research on remote sensing image classification significantly impacts
essential human routine tasks such as urban planning and agriculture. Nowadays,
the rapid advance in technology and the availability of many high-quality
remote sensing images create a demand for reliable automation methods. The
current paper proposes two novel deep learning-based architectures for image
classification purposes, i.e., the Discriminant Deep Image Prior Network and
the Discriminant Deep Image Prior Network+, which combine Deep Image Prior and
Triplet Networks learning strategies. Experiments conducted over three
well-known public remote sensing image datasets achieved state-of-the-art
results, evidencing the effectiveness of using deep image priors for remote
sensing image classification.",None,-1
Scribble-Supervised LiDAR Semantic Segmentation,0.707702,"Densely annotating LiDAR point clouds remains too expensive and
time-consuming to keep up with the ever growing volume of data. While current
literature focuses on fully-supervised performance, developing efficient
methods that take advantage of realistic weak supervision have yet to be
explored. In this paper, we propose using scribbles to annotate LiDAR point
clouds and release ScribbleKITTI, the first scribble-annotated dataset for
LiDAR semantic segmentation. Furthermore, we present a pipeline to reduce the
performance gap that arises when using such weak annotations. Our pipeline
comprises of three stand-alone contributions that can be combined with any
LiDAR semantic segmentation model to achieve up to 95.7% of the
fully-supervised performance while using only 8% labeled points. Our scribble
annotations and code are available at github.com/ouenal/scribblekitti.",https://github.com/ouenal/scribblekitti,-1
FNeVR: Neural Volume Rendering for Face Animation,0.563112,"Face animation, one of the hottest topics in computer vision, has achieved a
promising performance with the help of generative models. However, it remains a
critical challenge to generate identity preserving and photo-realistic images
due to the sophisticated motion deformation and complex facial detail modeling.
To address these problems, we propose a Face Neural Volume Rendering (FNeVR)
network to fully explore the potential of 2D motion warping and 3D volume
rendering in a unified framework. In FNeVR, we design a 3D Face Volume
Rendering (FVR) module to enhance the facial details for image rendering.
Specifically, we first extract 3D information with a well-designed
architecture, and then introduce an orthogonal adaptive ray-sampling module for
efficient rendering. We also design a lightweight pose editor, enabling FNeVR
to edit the facial pose in a simple yet effective way. Extensive experiments
show that our FNeVR obtains the best overall quality and performance on widely
used talking-head benchmarks.",None,-1
Measuring Cognitive Workload Using Multimodal Sensors,0.422435,"This study aims to identify a set of indicators to estimate cognitive
workload using a multimodal sensing approach and machine learning. A set of
three cognitive tests were conducted to induce cognitive workload in twelve
participants at two levels of task difficulty (Easy and Hard). Four sensors
were used to measure the participants' physiological change, including,
Electrocardiogram (ECG), electrodermal activity (EDA), respiration (RESP), and
blood oxygen saturation (SpO2). To understand the perceived cognitive workload,
NASA-TLX was used after each test and analysed using Chi-Square test. Three
well-know classifiers (LDA, SVM, and DT) were trained and tested independently
using the physiological data. The statistical analysis showed that
participants' perceived cognitive workload was significantly different
(p<0.001) between the tests, which demonstrated the validity of the
experimental conditions to induce different cognitive levels. Classification
results showed that a fusion of ECG and EDA presented good discriminating power
(acc=0.74) for cognitive workload detection. This study provides preliminary
results in the identification of a possible set of indicators of cognitive
workload. Future work needs to be carried out to validate the indicators using
more realistic scenarios and with a larger population.",None,-1
Active Labeling: Streaming Stochastic Gradients,0.0708611,"The workhorse of machine learning is stochastic gradient descent. To access
stochastic gradients, it is common to consider iteratively input/output pairs
of a training dataset. Interestingly, it appears that one does not need full
supervision to access stochastic gradients, which is the main motivation of
this paper. After formalizing the ""active labeling"" problem, which focuses on
active learning with partial supervision, we provide a streaming technique that
provably minimizes the ratio of generalization error over the number of
samples. We illustrate our technique in depth for robust regression.",https://github.com/VivienCabannes/active-labeling,-1
NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs,0.400171,"Complex query answering (CQA) is an essential task for multi-hop and logical
reasoning on knowledge graphs (KGs). Currently, most approaches are limited to
queries among binary relational facts and pay less attention to n-ary facts
(n>=2) containing more than two entities, which are more prevalent in the real
world. Moreover, previous CQA methods can only make predictions for a few given
types of queries and cannot be flexibly extended to more complex logical
queries, which significantly limits their applications. To overcome these
challenges, in this work, we propose a novel N-ary Query Embedding (NQE) model
for CQA over hyper-relational knowledge graphs (HKGs), which include massive
n-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder and
fuzzy logic theory to satisfy all n-ary FOL queries, including existential
quantifiers, conjunction, disjunction, and negation. We also propose a parallel
processing algorithm that can train or predict arbitrary n-ary FOL queries in a
single batch, regardless of the kind of each query, with good flexibility and
extensibility. In addition, we generate a new CQA dataset WD50K-NFOL, including
diverse n-ary FOL queries over WD50K. Experimental results on WD50K-NFOL and
other standard CQA datasets show that NQE is the state-of-the-art CQA method
over HKGs with good generalization capability. Our code and dataset are
publicly available.",https://github.com/LHRLAB/NQE,2442
Score-Guided Intermediate Layer Optimization: Fast Langevin Mixing for Inverse Problems,0.26817,"We prove fast mixing and characterize the stationary distribution of the
Langevin Algorithm for inverting random weighted DNN generators. This result
extends the work of Hand and Voroninski from efficient inversion to efficient
posterior sampling. In practice, to allow for increased expressivity, we
propose to do posterior sampling in the latent space of a pre-trained
generative model. To achieve that, we train a score-based model in the latent
space of a StyleGAN-2 and we use it to solve inverse problems. Our framework,
Score-Guided Intermediate Layer Optimization (SGILO), extends prior work by
replacing the sparsity regularization with a generative prior in the
intermediate layer. Experimentally, we obtain significant improvements over the
previous state-of-the-art, especially in the low measurement regime.",None,-1
Active Self-Training for Weakly Supervised 3D Scene Semantic Segmentation,0.353525,"Since the preparation of labeled data for training semantic segmentation
networks of point clouds is a time-consuming process, weakly supervised
approaches have been introduced to learn from only a small fraction of data.
These methods are typically based on learning with contrastive losses while
automatically deriving per-point pseudo-labels from a sparse set of
user-annotated labels. In this paper, our key observation is that the selection
of what samples to annotate is as important as how these samples are used for
training. Thus, we introduce a method for weakly supervised segmentation of 3D
scenes that combines self-training with active learning. The active learning
selects points for annotation that likely result in performance improvements to
the trained model, while the self-training makes efficient use of the
user-provided labels for learning the model. We demonstrate that our approach
leads to an effective method that provides improvements in scene segmentation
over previous works and baselines, while requiring only a small number of user
annotations.",None,6439
Comprehension of Subtitles from Re-Translating Simultaneous Speech Translation,0.324209,"In simultaneous speech translation, one can vary the size of the output
window, system latency and sometimes the allowed level of rewriting. The effect
of these properties on readability and comprehensibility has not been tested
with modern neural translation systems. In this work, we propose an evaluation
method and investigate the effects on comprehension and user preferences. It is
a pilot study with 14 users on 2 hours of German documentaries or speeches with
online translations into Czech. We collect continuous feedback and answers on
factual questions. Our results show that the subtitling layout or flicker have
a little effect on comprehension, in contrast to machine translation itself and
individual competence. Other results show that users with a limited knowledge
of the source language have different preferences to stability and latency than
the users with zero knowledge. The results are statistically insignificant,
however, we show that our method works and can be reproduced in larger volume.",None,-1
Combining Attention Module and Pixel Shuffle for License Plate Super-Resolution,0.895923,"The License Plate Recognition (LPR) field has made impressive advances in the
last decade due to novel deep learning approaches combined with the increased
availability of training data. However, it still has some open issues,
especially when the data come from low-resolution (LR) and low-quality
images/videos, as in surveillance systems. This work focuses on license plate
(LP) reconstruction in LR and low-quality images. We present a Single-Image
Super-Resolution (SISR) approach that extends the attention/transformer module
concept by exploiting the capabilities of PixelShuffle layers and that has an
improved loss function based on LPR predictions. For training the proposed
architecture, we use synthetic images generated by applying heavy Gaussian
noise in terms of Structural Similarity Index Measure (SSIM) to the original
high-resolution (HR) images. In our experiments, the proposed method
outperformed the baselines both quantitatively and qualitatively. The datasets
we created for this work are publicly available to the research community at
https://github.com/valfride/lpr-rsr/",https://github.com/valfride/lpr-rsr/,-1
NewsStories: Illustrating articles with visual summaries,0.522404,"Recent self-supervised approaches have used large-scale image-text datasets
to learn powerful representations that transfer to many tasks without
finetuning. These methods often assume that there is one-to-one correspondence
between its images and their (short) captions. However, many tasks require
reasoning about multiple images and long text narratives, such as describing
news articles with visual summaries. Thus, we explore a novel setting where the
goal is to learn a self-supervised visual-language representation that is
robust to varying text length and the number of images. In addition, unlike
prior work which assumed captions have a literal relation to the image, we
assume images only contain loose illustrative correspondence with the text. To
explore this problem, we introduce a large-scale multimodal dataset containing
over 31M articles, 22M images and 1M videos. We show that state-of-the-art
image-text alignment methods are not robust to longer narratives with multiple
images. Finally, we introduce an intuitive baseline that outperforms these
methods on zero-shot image-set retrieval by 10% on the GoodNews dataset.",None,-1
Mitigating Both Covariate and Conditional Shift for Domain Generalization,0.138765,"Domain generalization (DG) aims to learn a model on several source domains,
hoping that the model can generalize well to unseen target domains. The
distribution shift between domains contains the covariate shift and conditional
shift, both of which the model must be able to handle for better
generalizability. In this paper, a novel DG method is proposed to deal with the
distribution shift via Visual Alignment and Uncertainty-guided belief Ensemble
(VAUE). Specifically, for the covariate shift, a visual alignment module is
designed to align the distribution of image style to a common empirical
Gaussian distribution so that the covariate shift can be eliminated in the
visual space. For the conditional shift, we adopt an uncertainty-guided belief
ensemble strategy based on the subjective logic and Dempster-Shafer theory. The
conditional distribution given a test sample is estimated by the dynamic
combination of that of source domains. Comprehensive experiments are conducted
to demonstrate the superior performance of the proposed method on four widely
used datasets, i.e., Office-Home, VLCS, TerraIncognita, and PACS.",None,505
Prediction-based One-shot Dynamic Parking Pricing,0.47095,"Many U.S. metropolitan cities are notorious for their severe shortage of
parking spots. To this end, we present a proactive prediction-driven
optimization framework to dynamically adjust parking prices. We use
state-of-the-art deep learning technologies such as neural ordinary
differential equations (NODEs) to design our future parking occupancy rate
prediction model given historical occupancy rates and price information. Owing
to the continuous and bijective characteristics of NODEs, in addition, we
design a one-shot price optimization method given a pre-trained prediction
model, which requires only one iteration to find the optimal solution. In other
words, we optimize the price input to the pre-trained prediction model to
achieve targeted occupancy rates in the parking blocks. We conduct experiments
with the data collected in San Francisco and Seattle for years. Our prediction
model shows the best accuracy in comparison with various temporal or
spatio-temporal forecasting models. Our one-shot optimization method greatly
outperforms other black-box and white-box search methods in terms of the search
time and always returns the optimal price solution.",None,-1
On Transfer of Adversarial Robustness from Pretraining to Downstream Tasks,0.0642528,"As large-scale training regimes have gained popularity, the use of pretrained
models for downstream tasks has become common practice in machine learning.
While pretraining has been shown to enhance the performance of models in
practice, the transfer of robustness properties from pretraining to downstream
tasks remains poorly understood. In this study, we demonstrate that the
robustness of a linear predictor on downstream tasks can be constrained by the
robustness of its underlying representation, regardless of the protocol used
for pretraining. We prove (i) a bound on the loss that holds independent of any
downstream task, as well as (ii) a criterion for robust classification in
particular. We validate our theoretical results in practical applications, show
how our results can be used for calibrating expectations of downstream
robustness, and when our results are useful for optimal transfer learning.
Taken together, our results offer an initial step towards characterizing the
requirements of the representation function for reliable post-adaptation
performance.",https://github.com/lf-tcho/robustness_transfer,-1
Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning,0.589504,"The spread of rumors along with breaking events seriously hinders the truth
in the era of social media. Previous studies reveal that due to the lack of
annotated resources, rumors presented in minority languages are hard to be
detected. Furthermore, the unforeseen breaking events not involved in
yesterday's news exacerbate the scarcity of data resources. In this work, we
propose a novel zero-shot framework based on prompt learning to detect rumors
falling in different domains or presented in different languages. More
specifically, we firstly represent rumor circulated on social media as diverse
propagation threads, then design a hierarchical prompt encoding mechanism to
learn language-agnostic contextual representations for both prompts and rumor
data. To further enhance domain adaptation, we model the domain-invariant
structural features from the propagation threads, to incorporate structural
position representations of influential community response. In addition, a new
virtual response augmentation method is used to improve model training.
Extensive experiments conducted on three real-world datasets demonstrate that
our proposed model achieves much better performance than state-of-the-art
methods and exhibits a superior capacity for detecting rumors at early stages.",https://github.com/PengyaoYi/zeroRumor,-1
DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,0.999942,"With the ever-growing size of pretrained models (PMs), fine-tuning them has
become more expensive and resource-hungry. As a remedy, low-rank adapters
(LoRA) keep the main pretrained weights of the model frozen and just introduce
some learnable truncated SVD modules (so-called LoRA blocks) to the model.
While LoRA blocks are parameter-efficient, they suffer from two major problems:
first, the size of these blocks is fixed and cannot be modified after training
(for example, if we need to change the rank of LoRA blocks, then we need to
re-train them from scratch); second, optimizing their rank requires an
exhaustive search and effort. In this work, we introduce a dynamic low-rank
adaptation (DyLoRA) technique to address these two problems together. Our
DyLoRA method trains LoRA blocks for a range of ranks instead of a single rank
by sorting the representation learned by the adapter module at different ranks
during training. We evaluate our solution on different natural language
understanding (GLUE benchmark) and language generation tasks (E2E, DART and
WebNLG) using different pretrained models such as RoBERTa and GPT with
different sizes. Our results show that we can train dynamic search-free models
with DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA
without significantly compromising performance. Moreover, our models can
perform consistently well on a much larger range of ranks compared to LoRA.",https://github.com/huawei-noah/KD-NLP/tree/main/DyLoRA,-1
"CGiS-Net: Aggregating Colour, Geometry and Implicit Semantic Features for Indoor Place Recognition",0.550614,"We describe a novel approach to indoor place recognition from RGB point
clouds based on aggregating low-level colour and geometry features with
high-level implicit semantic features. It uses a 2-stage deep learning
framework, in which the first stage is trained for the auxiliary task of
semantic segmentation and the second stage uses features from layers in the
first stage to generate discriminate descriptors for place recognition. The
auxiliary task encourages the features to be semantically meaningful, hence
aggregating the geometry and colour in the RGB point cloud data with implicit
semantic information. We use an indoor place recognition dataset derived from
the ScanNet dataset for training and evaluation, with a test set comprising
3,608 point clouds generated from 100 different rooms. Comparison with a
traditional feature-based method and four state-of-the-art deep learning
methods demonstrate that our approach significantly outperforms all five
methods, achieving, for example, a top-3 average recall rate of 75% compared
with 41% for the closest rival method. Our code is available at:
https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition",https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition,-1
Temporal Attention for Language Models,0.680145,"Pretrained language models based on the transformer architecture have shown
great success in NLP. Textual training data often comes from the web and is
thus tagged with time-specific information, but most language models ignore
this information. They are trained on the textual data alone, limiting their
ability to generalize temporally. In this work, we extend the key component of
the transformer architecture, i.e., the self-attention mechanism, and propose
temporal attention - a time-aware self-attention mechanism. Temporal attention
can be applied to any transformer model and requires the input texts to be
accompanied with their relevant time points. It allows the transformer to
capture this temporal information and create time-specific contextualized word
representations. We leverage these representations for the task of semantic
change detection; we apply our proposed mechanism to BERT and experiment on
three datasets in different languages (English, German, and Latin) that also
vary in time, size, and genre. Our proposed model achieves state-of-the-art
results on all the datasets.",https://github.com/guyrosin/temporal_attention,-1
Seeing Like a Toolkit: How Toolkits Envision the Work of AI Ethics,0.946931,"Numerous toolkits have been developed to support ethical AI development.
However, toolkits, like all tools, encode assumptions in their design about
what work should be done and how. In this paper, we conduct a qualitative
analysis of 27 AI ethics toolkits to critically examine how the work of ethics
is imagined and how it is supported by these toolkits. Specifically, we examine
the discourses toolkits rely on when talking about ethical issues, who they
imagine should do the work of ethics, and how they envision the work practices
involved in addressing ethics. Among the toolkits, we identify a mismatch
between the imagined work of ethics and the support the toolkits provide for
doing that work. In particular, we identify a lack of guidance around how to
navigate labor, organizational, and institutional power dynamics as they relate
to performing ethical work. We use these omissions to chart future work for
researchers and designers of AI ethics toolkits.",None,-1
SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization,0.650821,"Matching-based methods, especially those based on space-time memory, are
significantly ahead of other solutions in semi-supervised video object
segmentation (VOS). However, continuously growing and redundant template
features lead to an inefficient inference. To alleviate this, we propose a
novel Sequential Weighted Expectation-Maximization (SWEM) network to greatly
reduce the redundancy of memory features. Different from the previous methods
which only detect feature redundancy between frames, SWEM merges both
intra-frame and inter-frame similar features by leveraging the sequential
weighted EM algorithm. Further, adaptive weights for frame features endow SWEM
with the flexibility to represent hard samples, improving the discrimination of
templates. Besides, the proposed method maintains a fixed number of template
features in memory, which ensures the stable inference complexity of the VOS
system. Extensive experiments on commonly used DAVIS and YouTube-VOS datasets
verify the high efficiency (36 FPS) and high performance (84.3\%
$\mathcal{J}\&\mathcal{F}$ on DAVIS 2017 validation dataset) of SWEM. Code is
available at: https://github.com/lmm077/SWEM.",https://github.com/lmm077/SWEM,-1
On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,0.810268,"Many recent studies on large-scale language models have reported successful
in-context zero- and few-shot learning ability. However, the in-depth analysis
of when in-context learning occurs is still lacking. For example, it is unknown
how in-context learning performance changes as the training corpus varies.
Here, we investigate the effects of the source and size of the pretraining
corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From
our in-depth investigation, we introduce the following observations: (1)
in-context learning performance heavily depends on the corpus domain source,
and the size of the pretraining corpus does not necessarily determine the
emergence of in-context learning, (2) in-context learning ability can emerge
when a language model is trained on a combination of multiple corpora, even
when each corpus does not result in in-context learning on its own, (3)
pretraining with a corpus related to a downstream task does not always
guarantee the competitive in-context learning performance of the downstream
task, especially in the few-shot setting, and (4) the relationship between
language modeling (measured in perplexity) and in-context learning does not
always correlate: e.g., low perplexity does not always imply high in-context
few-shot learning performance.",None,150035
Data Splits and Metrics for Method Benchmarking on Surgical Action Triplet Datasets,0.846434,"In addition to generating data and annotations, devising sensible data
splitting strategies and evaluation metrics is essential for the creation of a
benchmark dataset. This practice ensures consensus on the usage of the data,
homogeneous assessment, and uniform comparison of research methods on the
dataset. This study focuses on CholecT50, which is a 50 video surgical dataset
that formalizes surgical activities as triplets of <instrument, verb, target>.
In this paper, we introduce the standard splits for the CholecT50 and CholecT45
datasets and show how they compare with existing use of the dataset. CholecT45
is the first public release of 45 videos of CholecT50 dataset. We also develop
a metrics library, ivtmetrics, for model evaluation on surgical triplets.
Furthermore, we conduct a benchmark study by reproducing baseline methods in
the most predominantly used deep learning frameworks (PyTorch and TensorFlow)
to evaluate them using the proposed data splits and metrics and release them
publicly to support future research. The proposed data splits and evaluation
metrics will enable global tracking of research progress on the dataset and
facilitate optimal model selection for further deployment.",https://github.com/CAMMA-public,-1
Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation,0.942198,"Sparsely annotated semantic segmentation (SASS) aims to train a segmentation
network with coarse-grained (i.e., point-, scribble-, and block-wise)
supervisions, where only a small proportion of pixels are labeled in each
image. In this paper, we propose a novel tree energy loss for SASS by providing
semantic guidance for unlabeled pixels. The tree energy loss represents images
as minimum spanning trees to model both low-level and high-level pair-wise
affinities. By sequentially applying these affinities to the network
prediction, soft pseudo labels for unlabeled pixels are generated in a
coarse-to-fine manner, achieving dynamic online self-training. The tree energy
loss is effective and easy to be incorporated into existing frameworks by
combining it with a traditional segmentation loss. Compared with previous SASS
methods, our method requires no multistage training strategies, alternating
optimization procedures, additional supervised data, or time-consuming
post-processing while outperforming them in all SASS settings. Code is
available at https://github.com/megvii-research/TreeEnergyLoss.",https://github.com/megvii-research/TreeEnergyLoss,-1
Counterfactual Data Augmentation improves Factuality of Abstractive Summarization,0.294218,"Abstractive summarization systems based on pretrained language models often
generate coherent but factually inconsistent sentences. In this paper, we
present a counterfactual data augmentation approach where we augment data with
perturbed summaries that increase the training data diversity. Specifically, we
present three augmentation approaches based on replacing (i) entities from
other and the same category and (ii) nouns with their corresponding WordNet
hypernyms. We show that augmenting the training data with our approach improves
the factual correctness of summaries without significantly affecting the ROUGE
score. We show that in two commonly used summarization datasets (CNN/Dailymail
and XSum), we improve the factual correctness by about 2.5 points on average",None,61938
Predicting non-native speech perception using the Perceptual Assimilation Model and state-of-the-art acoustic models,0.438925,"Our native language influences the way we perceive speech sounds, affecting
our ability to discriminate non-native sounds. We compare two ideas about the
influence of the native language on speech perception: the Perceptual
Assimilation Model, which appeals to a mental classification of sounds into
native phoneme categories, versus the idea that rich, fine-grained phonetic
representations tuned to the statistics of the native language, are sufficient.
We operationalize this idea using representations from two state-of-the-art
speech models, a Dirichlet process Gaussian mixture model and the more recent
wav2vec 2.0 model. We present a new, open dataset of French- and
English-speaking participants' speech perception behaviour for 61 vowel sounds
from six languages. We show that phoneme assimilation is a better predictor
than fine-grained phonetic modelling, both for the discrimination behaviour as
a whole, and for predicting differences in discriminability associated with
differences in native language background. We also show that wav2vec 2.0, while
not good at capturing the effects of native language on speech perception, is
complementary to information about native phoneme assimilation, and provides a
good model of low-level phonetic representations, supporting the idea that both
categorical and fine-grained perception are used during speech perception.",https://github.com/JAMJU/CONLL_2021_nonnative_speech_perception,-1
COIN: Co-Cluster Infomax for Bipartite Graphs,0.832239,"Bipartite graphs are powerful data structures to model interactions between
two types of nodes, which have been used in a variety of applications, such as
recommender systems, information retrieval, and drug discovery. A fundamental
challenge for bipartite graphs is how to learn informative node embeddings.
Despite the success of recent self-supervised learning methods on bipartite
graphs, their objectives are discriminating instance-wise positive and negative
node pairs, which could contain cluster-level errors. In this paper, we
introduce a novel co-cluster infomax (COIN) framework, which captures the
cluster-level information by maximizing the mutual information of co-clusters.
Different from previous infomax methods which estimate mutual information by
neural networks, COIN could easily calculate mutual information. Besides, COIN
is an end-to-end coclustering method which can be trained jointly with other
objective functions and optimized via back-propagation. Furthermore, we also
provide theoretical analysis for COIN. We theoretically prove that COIN is able
to effectively increase the mutual information of node embeddings and COIN is
upper-bounded by the prior distributions of nodes. We extensively evaluate the
proposed COIN framework on various benchmark datasets and tasks to demonstrate
the effectiveness of COIN.",https://github.com/clhchtcjj/BiNE/tree/master/data/wiki,-1
"The Conversational Short-phrase Speaker Diarization (CSSD) Task: Dataset, Evaluation Metric and Baselines",0.986236,"The conversation scenario is one of the most important and most challenging
scenarios for speech processing technologies because people in conversation
respond to each other in a casual style. Detecting the speech activities of
each person in a conversation is vital to downstream tasks, like natural
language processing, machine translation, etc. People refer to the detection
technology of ""who speak when"" as speaker diarization (SD). Traditionally,
diarization error rate (DER) has been used as the standard evaluation metric of
SD systems for a long time. However, DER fails to give enough importance to
short conversational phrases, which are short but important on the semantic
level. Also, a carefully and accurately manually-annotated testing dataset
suitable for evaluating the conversational SD technologies is still unavailable
in the speech community. In this paper, we design and describe the
Conversational Short-phrases Speaker Diarization (CSSD) task, which consists of
training and testing datasets, evaluation metric and baselines. In the dataset
aspect, despite the previously open-sourced 180-hour conversational
MagicData-RAMC dataset, we prepare an individual 20-hour conversational speech
test dataset with carefully and artificially verified speakers timestamps
annotations for the CSSD task. In the metric aspect, we design the new
conversational DER (CDER) evaluation metric, which calculates the SD accuracy
at the utterance level. In the baseline aspect, we adopt a commonly used
method: Variational Bayes HMM x-vector system, as the baseline of the CSSD
task. Our evaluation metric is publicly available at
https://github.com/SpeechClub/CDER_Metric.",https://github.com/MagicHub-io/MagicData-RAMC,-1
Smoothing Entailment Graphs with Language Models,0.728046,"The diversity and Zipfian frequency distribution of natural language
predicates in corpora leads to sparsity in Entailment Graphs (EGs) built by
Open Relation Extraction (ORE). EGs are computationally efficient and
explainable models of natural language inference, but as symbolic models, they
fail if a novel premise or hypothesis vertex is missing at test-time. We
present theory and methodology for overcoming such sparsity in symbolic models.
First, we introduce a theory of optimal smoothing of EGs by constructing
transitive chains. We then demonstrate an efficient, open-domain, and
unsupervised smoothing method using an off-the-shelf Language Model to find
approximations of missing premise predicates. This improves recall by 25.1 and
16.3 percentage points on two difficult directional entailment datasets, while
raising average precision and maintaining model explainability. Further, in a
QA task we show that EG smoothing is most useful for answering questions with
lesser supporting text, where missing premise predicates are more costly.
Finally, controlled experiments with WordNet confirm our theory and show that
hypothesis smoothing is difficult, but possible in principle.",https://github.com/nighttime/EntGraph,26355
Performance of different machine learning methods on activity recognition and pose estimation datasets,0.111187,"With advancements in computer vision taking place day by day, recently a lot
of light is being shed on activity recognition. With the range for real-world
applications utilizing this field of study increasing across a multitude of
industries such as security and healthcare, it becomes crucial for businesses
to distinguish which machine learning methods perform better than others in the
area. This paper strives to aid in this predicament i.e. building upon previous
related work, it employs both classical and ensemble approaches on rich pose
estimation (OpenPose) and HAR datasets. Making use of appropriate metrics to
evaluate the performance for each model, the results show that overall, random
forest yields the highest accuracy in classifying ADLs. Relatively all the
models have excellent performance across both datasets, except for logistic
regression and AdaBoost perform poorly in the HAR one. With the limitations of
this paper also discussed in the end, the scope for further research is vast,
which can use this paper as a base in aims of producing better results.",None,-1
Detection of road traffic crashes based on collision estimation,0.374645,"This paper introduces a framework based on computer vision that can detect
road traffic crashes (RCTs) by using the installed surveillance/CCTV camera and
report them to the emergency in real-time with the exact location and time of
occurrence of the accident. The framework is built of five modules. We start
with the detection of vehicles by using YOLO architecture; The second module is
the tracking of vehicles using MOSSE tracker, Then the third module is a new
approach to detect accidents based on collision estimation. Then the fourth
module for each vehicle, we detect if there is a car accident or not based on
the violent flow descriptor (ViF) followed by an SVM classifier for crash
prediction. Finally, in the last stage, if there is a car accident, the system
will send a notification to the emergency by using a GPS module that provides
us with the location, time, and date of the accident to be sent to the
emergency with the help of the GSM module. The main objective is to achieve
higher accuracy with fewer false alarms and to implement a simple system based
on pipelining technique.",None,-1
Grounding in social media: An approach to building a chit-chat dialogue model,0.517282,"Building open-domain dialogue systems capable of rich human-like
conversational ability is one of the fundamental challenges in language
generation. However, even with recent advancements in the field, existing
open-domain generative models fail to capture and utilize external knowledge,
leading to repetitive or generic responses to unseen utterances. Current work
on knowledge-grounded dialogue generation primarily focuses on persona
incorporation or searching a fact-based structured knowledge source such as
Wikipedia. Our method takes a broader and simpler approach, which aims to
improve the raw conversation ability of the system by mimicking the human
response behavior through casual interactions found on social media. Utilizing
a joint retriever-generator setup, the model queries a large set of filtered
comment data from Reddit to act as additional context for the seq2seq
generator. Automatic and human evaluations on open-domain dialogue datasets
demonstrate the effectiveness of our approach.",None,-1
Spatial Transformer Network on Skeleton-based Gait Recognition,0.885097,"Skeleton-based gait recognition models usually suffer from the robustness
problem, as the Rank-1 accuracy varies from 90\% in normal walking cases to
70\% in walking with coats cases. In this work, we propose a state-of-the-art
robust skeleton-based gait recognition model called Gait-TR, which is based on
the combination of spatial transformer frameworks and temporal convolutional
networks. Gait-TR achieves substantial improvements over other skeleton-based
gait models with higher accuracy and better robustness on the well-known gait
dataset CASIA-B. Particularly in walking with coats cases, Gait-TR get a 90\%
Rank-1 gait recognition accuracy rate, which is higher than the best result of
silhouette-based models, which usually have higher accuracy than the
silhouette-based gait recognition models. Moreover, our experiment on CASIA-B
shows that the spatial transformer can extract gait features from the human
skeleton better than the widely used graph convolutional network.",None,17341
Capabilities for Better ML Engineering,0.254747,"In spite of machine learning's rapid growth, its engineering support is
scattered in many forms, and tends to favor certain engineering stages,
stakeholders, and evaluation preferences. We envision a capability-based
framework, which uses fine-grained specifications for ML model behaviors to
unite existing efforts towards better ML engineering. We use concrete scenarios
(model design, debugging, and maintenance) to articulate capabilities' broad
applications across various different dimensions, and their impact on building
safer, more generalizable and more trustworthy models that reflect human needs.
Through preliminary experiments, we show capabilities' potential for reflecting
model generalizability, which can provide guidance for ML engineering process.
We discuss challenges and opportunities for capabilities' integration into ML
engineering.",https://github.com/malusamayo/Capabilities-Experiment-Details,-1
Detecting Methane Plumes using PRISMA: Deep Learning Model and Data Augmentation,0.764123,"The new generation of hyperspectral imagers, such as PRISMA, has improved
significantly our detection capability of methane (CH4) plumes from space at
high spatial resolution (30m). We present here a complete framework to identify
CH4 plumes using images from the PRISMA satellite mission and a deep learning
model able to detect plumes over large areas. To compensate for the relative
scarcity of PRISMA images, we trained our model by transposing high resolution
plumes from Sentinel-2 to PRISMA. Our methodology thus avoids computationally
expensive synthetic plume generation from Large Eddy Simulations by generating
a broad and realistic training database, and paves the way for large-scale
detection of methane plumes using future hyperspectral sensors (EnMAP, EMIT,
CarbonMapper).",None,-1
TODE-Trans: Transparent Object Depth Estimation with Transformer,0.547453,"Transparent objects are widely used in industrial automation and daily life.
However, robust visual recognition and perception of transparent objects have
always been a major challenge. Currently, most commercial-grade depth cameras
are still not good at sensing the surfaces of transparent objects due to the
refraction and reflection of light. In this work, we present a
transformer-based transparent object depth estimation approach from a single
RGB-D input. We observe that the global characteristics of the transformer make
it easier to extract contextual information to perform depth estimation of
transparent areas. In addition, to better enhance the fine-grained features, a
feature fusion module (FFM) is designed to assist coherent prediction. Our
empirical evidence demonstrates that our model delivers significant
improvements in recent popular datasets, e.g., 25% gain on RMSE and 21% gain on
REL compared to previous state-of-the-art convolutional-based counterparts in
ClearGrasp dataset. Extensive results show that our transformer-based model
enables better aggregation of the object's RGB and inaccurate depth information
to obtain a better depth representation. Our code and the pre-trained model
will be available at https://github.com/yuchendoudou/TODE.",None,6668
A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment Analysis,0.408954,"Recently, some span-based methods have achieved encouraging performances for
joint aspect-sentiment analysis, which first extract aspects (aspect
extraction) by detecting aspect boundaries and then classify the span-level
sentiments (sentiment classification). However, most existing approaches either
sequentially extract task-specific features, leading to insufficient feature
interactions, or they encode aspect features and sentiment features in a
parallel manner, implying that feature representation in each task is largely
independent of each other except for input sharing. Both of them ignore the
internal correlations between the aspect extraction and sentiment
classification. To solve this problem, we novelly propose a hierarchical
interactive network (HI-ASA) to model two-way interactions between two tasks
appropriately, where the hierarchical interactions involve two steps:
shallow-level interaction and deep-level interaction. First, we utilize
cross-stitch mechanism to combine the different task-specific features
selectively as the input to ensure proper two-way interactions. Second, the
mutual information technique is applied to mutually constrain learning between
two tasks in the output layer, thus the aspect input and the sentiment input
are capable of encoding features of the other task via backpropagation.
Extensive experiments on three real-world datasets demonstrate HI-ASA's
superiority over baselines.",https://github.com/cwei01/HI-ASA,-1
PalGAN: Image Colorization with Palette Generative Adversarial Networks,0.595831,"Multimodal ambiguity and color bleeding remain challenging in colorization.
To tackle these problems, we propose a new GAN-based colorization approach
PalGAN, integrated with palette estimation and chromatic attention. To
circumvent the multimodality issue, we present a new colorization formulation
that estimates a probabilistic palette from the input gray image first, then
conducts color assignment conditioned on the palette through a generative
model. Further, we handle color bleeding with chromatic attention. It studies
color affinities by considering both semantic and intensity correlation. In
extensive experiments, PalGAN outperforms state-of-the-arts in quantitative
evaluation and visual comparison, delivering notable diverse, contrastive, and
edge-preserving appearances. With the palette design, our method enables color
transfer between images even with irrelevant contexts.",https://github.com/shepnerd/PalGAN,-1
Using Deep Mixture-of-Experts to Detect Word Meaning Shift for TempoWiC,0.461566,"This paper mainly describes the dma submission to the TempoWiC task, which
achieves a macro-F1 score of 77.05% and attains the first place in this task.
We first explore the impact of different pre-trained language models. Then we
adopt data cleaning, data augmentation, and adversarial training strategies to
enhance the model generalization and robustness. For further improvement, we
integrate POS information and word semantic representation using a
Mixture-of-Experts (MoE) approach. The experimental results show that MoE can
overcome the feature overuse issue and combine the context, POS, and word
semantic features well. Additionally, we use a model ensemble method for the
final prediction, which has been proven effective by many research works.",None,-1
Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining,0.782053,"Deep visuomotor policy learning, which aims to map raw visual observation to
action, achieves promising results in control tasks such as robotic
manipulation and autonomous driving. However, it requires a huge number of
online interactions with the training environment, which limits its real-world
application. Compared to the popular unsupervised feature learning for visual
recognition, feature pretraining for visuomotor control tasks is much less
explored. In this work, we aim to pretrain policy representations for driving
tasks by watching hours-long uncurated YouTube videos. Specifically, we train
an inverse dynamic model with a small amount of labeled data and use it to
predict action labels for all the YouTube video frames. A new contrastive
policy pretraining method is then developed to learn action-conditioned
features from the video frames with pseudo action labels. Experiments show that
the resulting action-conditioned features obtain substantial improvements for
the downstream reinforcement learning and imitation learning tasks,
outperforming the weights pretrained from previous unsupervised learning
methods and ImageNet pretrained weight. Code, model weights, and data are
available at: https://metadriverse.github.io/ACO.",https://metadriverse.github.io/ACO,-1
Adversarial Motion Priors Make Good Substitutes for Complex Reward Functions,0.915768,"Training a high-dimensional simulated agent with an under-specified reward
function often leads the agent to learn physically infeasible strategies that
are ineffective when deployed in the real world. To mitigate these unnatural
behaviors, reinforcement learning practitioners often utilize complex reward
functions that encourage physically plausible behaviors. However, a tedious
labor-intensive tuning process is often required to create hand-designed
rewards which might not easily generalize across platforms and tasks. We
propose substituting complex reward functions with ""style rewards"" learned from
a dataset of motion capture demonstrations. A learned style reward can be
combined with an arbitrary task reward to train policies that perform tasks
using naturalistic strategies. These natural strategies can also facilitate
transfer to the real world. We build upon Adversarial Motion Priors -- an
approach from the computer graphics domain that encodes a style reward from a
dataset of reference motions -- to demonstrate that an adversarial approach to
training policies can produce behaviors that transfer to a real quadrupedal
robot without requiring complex reward functions. We also demonstrate that an
effective style reward can be learned from a few seconds of motion capture data
gathered from a German Shepherd and leads to energy-efficient locomotion
strategies with natural gait transitions.",https://bit.ly/3hpvbD6,-1
KappaFace: Adaptive Additive Angular Margin Loss for Deep Face Recognition,0.988297,"Feature learning is a widely used method employed for large-scale face
recognition. Recently, large-margin softmax loss methods have demonstrated
significant enhancements on deep face recognition. These methods propose fixed
positive margins in order to enforce intra-class compactness and inter-class
diversity. However, the majority of the proposed methods do not consider the
class imbalance issue, which is a major challenge in practice for developing
deep face recognition models. We hypothesize that it significantly affects the
generalization ability of the deep face models. Inspired by this observation,
we introduce a novel adaptive strategy, called KappaFace, to modulate the
relative importance based on class difficultness and imbalance. With the
support of the von Mises-Fisher distribution, our proposed KappaFace loss can
intensify the margin's magnitude for hard learning or low concentration classes
while relaxing it for counter classes. Experiments conducted on popular facial
benchmarks demonstrate that our proposed method achieves superior performance
to the state-of-the-art.",https://github.com/chingisooinar/KappaFace,2499
Lagrangian Manifold Monte Carlo on Monge Patches,0.494666,"The efficiency of Markov Chain Monte Carlo (MCMC) depends on how the
underlying geometry of the problem is taken into account. For distributions
with strongly varying curvature, Riemannian metrics help in efficient
exploration of the target distribution. Unfortunately, they have significant
computational overhead due to e.g. repeated inversion of the metric tensor, and
current geometric MCMC methods using the Fisher information matrix to induce
the manifold are in practice slow. We propose a new alternative Riemannian
metric for MCMC, by embedding the target distribution into a higher-dimensional
Euclidean space as a Monge patch and using the induced metric determined by
direct geometric reasoning. Our metric only requires first-order gradient
information and has fast inverse and determinants, and allows reducing the
computational complexity of individual iterations from cubic to quadratic in
the problem dimensionality. We demonstrate how Lagrangian Monte Carlo in this
metric efficiently explores the target distributions.",https://github.com/mahaa2/EmbeddedLMC,-1
How Powerful are Spectral Graph Neural Networks,0.987573,"Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based
on graph signal filters. Some models able to learn arbitrary spectral filters
have emerged recently. However, few works analyze the expressive power of
spectral GNNs. This paper studies spectral GNNs' expressive power
theoretically. We first prove that even spectral GNNs without nonlinearity can
produce arbitrary graph signals and give two conditions for reaching
universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2)
no missing frequency components in node features. We also establish a
connection between the expressive power of spectral GNNs and Graph Isomorphism
(GI) testing, the latter of which is often used to characterize spatial GNNs'
expressive power. Moreover, we study the difference in empirical performance
among different spectral GNNs with the same expressive power from an
optimization perspective, and motivate the use of an orthogonal basis whose
weight function corresponds to the graph signal density in the spectrum.
Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to
its orthogonality and flexibility to adapt to a wide range of weight functions.
JacobiConv deserts nonlinearity while outperforming all baselines on both
synthetic and real-world datasets.",https://github.com/GraphPKU/JacobiConv,-1
"Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability",0.745813,"Large-scale pre-training has been proven to be crucial for various computer
vision tasks. However, with the increase of pre-training data amount, model
architecture amount, and the private/inaccessible data, it is not very
efficient or possible to pre-train all the model architectures on large-scale
datasets. In this work, we investigate an alternative strategy for
pre-training, namely Knowledge Distillation as Efficient Pre-training (KDEP),
aiming to efficiently transfer the learned feature representation from existing
pre-trained models to new student models for future downstream tasks. We
observe that existing Knowledge Distillation (KD) methods are unsuitable
towards pre-training since they normally distill the logits that are going to
be discarded when transferred to downstream tasks. To resolve this problem, we
propose a feature-based KD method with non-parametric feature dimension
aligning. Notably, our method performs comparably with supervised pre-training
counterparts in 3 downstream tasks and 9 downstream datasets requiring 10x less
data and 5x less pre-training time. Code is available at
https://github.com/CVMI-Lab/KDEP.",https://github.com/CVMI-Lab/KDEP,-1
mGPT: Few-Shot Learners Go Multilingual,0.847959,"Recent studies report that autoregressive language models can successfully
solve many NLP tasks via zero- and few-shot learning paradigms, which opens up
new possibilities for using the pre-trained language models. This paper
introduces two autoregressive GPT-like models with 1.3 billion and 13 billion
parameters trained on 60 languages from 25 language families using Wikipedia
and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using
GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron
frameworks allow us to parallelize the training and inference steps
effectively. The resulting models show performance on par with the recently
released XGLM models by Facebook, covering more languages and enhancing NLP
possibilities for low resource languages of CIS countries and Russian small
nations. We detail the motivation for the choices of the architecture design,
thoroughly describe the data preparation pipeline, and train five small
versions of the model to choose the most optimal multilingual tokenization
strategy. We measure the model perplexity in all covered languages and evaluate
it on the wide spectre of multilingual tasks, including classification,
generative, sequence labeling and knowledge probing. The models were evaluated
with the zero-shot and few-shot methods. Furthermore, we compared the
classification tasks with the state-of-the-art multilingual model XGLM. source
code and the mGPT XL model are publicly released.",https://github.com/ai-forever/mgpt,338
Offline RL for Natural Language Generation with Implicit Language Q Learning,0.926961,"Large language models distill broad knowledge from text corpora. However,
they can be inconsistent when it comes to completing user specified tasks. This
issue can be addressed by finetuning such models via supervised learning on
curated datasets, or via reinforcement learning. In this work, we propose a
novel offline RL method, implicit language Q-learning (ILQL), designed for use
on language models, that combines both the flexible utility maximization
framework of RL algorithms with the ability of supervised learning to leverage
previously collected data, as well as its simplicity and stability. Our method
employs a combination of value conservatism alongside an implicit dataset
support constraint in learning value functions, which are then used to guide
language model generations towards maximizing user-specified utility functions.
In addition to empirically validating ILQL, we present a detailed empirical
analysis of situations where offline RL can be useful in natural language
generation settings, demonstrating how it can be a more effective utility
optimizer than prior approaches for end-to-end dialogue, and how it can
effectively optimize high variance reward functions based on subjective
judgement, such as whether to label a comment as toxic or not.",None,136864
CrowdFormer: Weakly-supervised Crowd counting with Improved Generalizability,0.807851,"Convolutional neural networks (CNNs) have dominated the field of computer
vision for nearly a decade due to their strong ability to learn local features.
However, due to their limited receptive field, CNNs fail to model the global
context. On the other hand, transformer, an attention-based architecture can
model the global context easily. Despite this, there are limited studies that
investigate the effectiveness of transformers in crowd counting. In addition,
the majority of the existing crowd counting methods are based on the regression
of density maps which requires point-level annotation of each person present in
the scene. This annotation task is laborious and also error-prone. This has led
to increased focus on weakly-supervised crowd counting methods which require
only the count-level annotations. In this paper, we propose a weakly-supervised
method for crowd counting using a pyramid vision transformer. We have conducted
extensive evaluations to validate the effectiveness of the proposed method. Our
method is comparable to the state-of-the-art on the benchmark crowd datasets.
More importantly, it shows remarkable generalizability.",None,2627
Causal Discovery for Fairness,0.366184,"It is crucial to consider the social and ethical consequences of AI and ML
based decisions for the safe and acceptable use of these emerging technologies.
Fairness, in particular, guarantees that the ML decisions do not result in
discrimination against individuals or minorities. Identifying and measuring
reliably fairness/discrimination is better achieved using causality which
considers the causal relation, beyond mere association, between the sensitive
attribute (e.g. gender, race, religion, etc.) and the decision (e.g. job
hiring, loan granting, etc.). The big impediment to the use of causality to
address fairness, however, is the unavailability of the causal model (typically
represented as a causal graph). Existing causal approaches to fairness in the
literature do not address this problem and assume that the causal model is
available. In this paper, we do not make such assumption and we review the
major algorithms to discover causal relations from observable data. This study
focuses on causal discovery and its impact on fairness. In particular, we show
how different causal discovery approaches may result in different causal models
and, most importantly, how even slight differences between causal models can
have significant impact on fairness/discrimination conclusions. These results
are consolidated by empirical analysis using synthetic and standard fairness
benchmark datasets. The main goal of this study is to highlight the importance
of the causal discovery step to appropriately address fairness using causality.",None,-1
Intent Contrastive Learning for Sequential Recommendation,0.967625,"Users' interactions with items are driven by various intents (e.g., preparing
for holiday gifts, shopping for fishing equipment, etc.).However, users'
underlying intents are often unobserved/latent, making it challenging to
leverage such latent intents forSequentialrecommendation(SR). To investigate
the benefits of latent intents and leverage them effectively for
recommendation, we proposeIntentContrastiveLearning(ICL), a general learning
paradigm that leverages a latent intent variable into SR. The core idea is to
learn users' intent distribution functions from unlabeled user behavior
sequences and optimize SR models with contrastive self-supervised learning
(SSL) by considering the learned intents to improve recommendation.
Specifically, we introduce a latent variable to represent users' intents and
learn the distribution function of the latent variable via clustering. We
propose to leverage the learned intents into SR models via contrastive SSL,
which maximizes the agreement between a view of sequence and its corresponding
intent. The training is alternated between intent representation learning and
the SR model optimization steps within the generalized expectation-maximization
(EM) framework. Fusing user intent information into SR also improves model
robustness. Experiments conducted on four real-world datasets demonstrate the
superiority of the proposed learning paradigm, which improves performance, and
robustness against data sparsity and noisy interaction issues.",https://github.com/salesforce/ICLRec,-1
Unified Line and Paragraph Detection by Graph Convolutional Networks,0.240863,"We formulate the task of detecting lines and paragraphs in a document into a
unified two-level clustering problem. Given a set of text detection boxes that
roughly correspond to words, a text line is a cluster of boxes and a paragraph
is a cluster of lines. These clusters form a two-level tree that represents a
major part of the layout of a document. We use a graph convolutional network to
predict the relations between text detection boxes and then build both levels
of clusters from these predictions. Experimentally, we demonstrate that the
unified approach can be highly efficient while still achieving state-of-the-art
quality for detecting paragraphs in public benchmarks and real-world images.",None,1989
Exploration of the possibility of infusing Social Media Trends into generating NFT Recommendations,0.459618,"Recommendations Systems have been identified to be one of the integral
elements of driving sales in e-commerce sites. The utilization of opinion
mining data extracted from trends has been attempted to improve the
recommendations that can be provided by baseline methods in this research when
user-click data is lacking or is difficult to be collected due to privacy
concerns.
  Utilizing social trends to influence the recommendations generated for a set
of unique items has been explored with the use of a suggested scoring
mechanism. Embracing concepts from decentralized networks that are expected to
change how users interact via the internet over the next couple of decades, the
suggested Recommendations System attempts to make use of multiple sources of
information, applying coherent information retrieval techniques to extract
probable trending items.
  The proposed Recommendations Architecture in the research presents a method
to integrate social trends with recommendations to produce promising outputs.",None,-1
Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation and Instance Generation,0.655283,"We study the problem of few-shot Fine-grained Entity Typing (FET), where only
a few annotated entity mentions with contexts are given for each entity type.
Recently, prompt-based tuning has demonstrated superior performance to standard
fine-tuning in few-shot scenarios by formulating the entity type classification
task as a ''fill-in-the-blank'' problem. This allows effective utilization of
the strong language modeling capability of Pre-trained Language Models (PLMs).
Despite the success of current prompt-based tuning approaches, two major
challenges remain: (1) the verbalizer in prompts is either manually designed or
constructed from external knowledge bases, without considering the target
corpus and label hierarchy information, and (2) current approaches mainly
utilize the representation power of PLMs, but have not explored their
generation power acquired through extensive general-domain pre-training. In
this work, we propose a novel framework for few-shot FET consisting of two
modules: (1) an entity type label interpretation module automatically learns to
relate type labels to the vocabulary by jointly leveraging few-shot instances
and the label hierarchy, and (2) a type-based contextualized instance generator
produces new instances based on given instances to enlarge the training set for
better generalization. On three benchmark datasets, our model outperforms
existing methods by significant margins. Code can be found at
https://github.com/teapot123/Fine-Grained-Entity-Typing.",https://github.com/teapot123/Fine-Grained-Entity-Typing,-1
Generating Explanations from Deep Reinforcement Learning Using Episodic Memory,0.0505323,"Deep Reinforcement Learning (RL) involves the use of Deep Neural Networks
(DNNs) to make sequential decisions in order to maximize reward. For many tasks
the resulting sequence of actions produced by a Deep RL policy can be long and
difficult to understand for humans. A crucial component of human explanations
is selectivity, whereby only key decisions and causes are recounted. Imbuing
Deep RL agents with such an ability would make their resulting policies easier
to understand from a human perspective and generate a concise set of
instructions to aid the learning of future agents. To this end we use a Deep RL
agent with an episodic memory system to identify and recount key decisions
during policy execution. We show that these decisions form a short, human
readable explanation that can also be used to speed up the learning of naive
Deep RL agents in an algorithm-independent manner.",None,8495
Diverse Imagenet Models Transfer Better,0.711708,"A commonly accepted hypothesis is that models with higher accuracy on
Imagenet perform better on other downstream tasks, leading to much research
dedicated to optimizing Imagenet accuracy. Recently this hypothesis has been
challenged by evidence showing that self-supervised models transfer better than
their supervised counterparts, despite their inferior Imagenet accuracy. This
calls for identifying the additional factors, on top of Imagenet accuracy, that
make models transferable. In this work we show that high diversity of the
features learnt by the model promotes transferability jointly with Imagenet
accuracy. Encouraged by the recent transferability results of self-supervised
models, we propose a method that combines self-supervised and supervised
pretraining to generate models with both high diversity and high accuracy, and
as a result high transferability. We demonstrate our results on several
architectures and multiple downstream tasks, including both single-label and
multi-label classification.",None,14108
The Need for a Meta-Architecture for Robot Autonomy,0.0970449,"Long-term autonomy of robotic systems implicitly requires dependable
platforms that are able to naturally handle hardware and software faults,
problems in behaviors, or lack of knowledge. Model-based dependable platforms
additionally require the application of rigorous methodologies during the
system development, including the use of correct-by-construction techniques to
implement robot behaviors. As the level of autonomy in robots increases, so do
the cost of offering guarantees about the dependability of the system.
Certifiable dependability of autonomous robots, we argue, can benefit from
formal models of the integration of several cognitive functions, knowledge
processing, reasoning, and meta-reasoning. Here we put forward the case for a
generative model of cognitive architectures for autonomous robotic agents that
subscribes to the principles of model-based engineering and certifiable
dependability, autonomic computing, and knowledge-enabled robotics.",None,-1
Mitigating shortage of labeled data using clustering-based active learning with diversity exploration,0.0547145,"In this paper, we proposed a new clustering-based active learning framework,
namely Active Learning using a Clustering-based Sampling (ALCS), to address the
shortage of labeled data. ALCS employs a density-based clustering approach to
explore the cluster structure from the data without requiring exhaustive
parameter tuning. A bi-cluster boundary-based sample query procedure is
introduced to improve the learning performance for classifying highly
overlapped classes. Additionally, we developed an effective diversity
exploration strategy to address the redundancy among queried samples. Our
experimental results justified the efficacy of the ALCS approach.",https://github.com/XuyangAbert/ALCS,-1
Disentangling Architecture and Training for Optical Flow,0.844879,"How important are training details and datasets to recent optical flow models
like RAFT? And do they generalize? To explore these questions, rather than
develop a new model, we revisit three prominent models, PWC-Net, IRR-PWC and
RAFT, with a common set of modern training techniques and datasets, and observe
significant performance gains, demonstrating the importance and generality of
these training details. Our newly trained PWC-Net and IRR-PWC models show
surprisingly large improvements, up to 30% versus original published results on
Sintel and KITTI 2015 benchmarks. They outperform the more recent Flow1D on
KITTI 2015 while being 3x faster during inference. Our newly trained RAFT
achieves an Fl-all score of 4.31% on KITTI 2015, more accurate than all
published optical flow methods at the time of writing. Our results demonstrate
the benefits of separating the contributions of models, training techniques and
datasets when analyzing performance gains of optical flow methods. Our source
code will be publicly available.",https://autoflow-google.github.io,-1
Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness,0.830497,"Data-driven predictive solutions predominant in commercial applications tend
to suffer from biases and stereotypes, which raises equity concerns. Prediction
models may discover, use, or amplify spurious correlations based on gender or
other protected personal characteristics, thus discriminating against
marginalized groups. Mitigating gender bias has become an important research
focus in natural language processing (NLP) and is an area where annotated
corpora are available. Data augmentation reduces gender bias by adding
counterfactual examples to the training dataset. In this work, we show that
some of the examples in the augmented dataset can be not important or even
harmful for fairness. We hence propose a general method for pruning both the
factual and counterfactual examples to maximize the model's fairness as
measured by the demographic parity, equality of opportunity, and equality of
odds. The fairness achieved by our method surpasses that of data augmentation
on three text classification datasets, using no more than half of the examples
in the augmented dataset. Our experiments are conducted using models of varying
sizes and pre-training settings.",https://github.com/Garrett-R/gender bender,-1
Better Retrieval May Not Lead to Better Question Answering,0.0844047,"Considerable progress has been made recently in open-domain question
answering (QA) problems, which require Information Retrieval (IR) and Reading
Comprehension (RC). A popular approach to improve the system's performance is
to improve the quality of the retrieved context from the IR stage. In this work
we show that for StrategyQA, a challenging open-domain QA dataset that requires
multi-hop reasoning, this common approach is surprisingly ineffective --
improving the quality of the retrieved context hardly improves the system's
performance. We further analyze the system's behavior to identify potential
reasons.",https://github.com/zhengzhongliang/BetterRetrievalNotBetterQA,-1
Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning,0.440975,"Federated Learning (FL) is a machine learning technique that addresses the
privacy challenges in terms of access rights of local datasets by enabling the
training of a model across nodes holding their data samples locally. To achieve
decentralized federated learning, blockchain-based FL was proposed as a
distributed FL architecture. In decentralized FL, the chief is eliminated from
the learning process as workers collaborate between each other to train the
global model. Decentralized FL applications need to account for the additional
delay incurred by blockchain-based FL deployments. Particularly in this
setting, to detect targeted/untargeted poisoning attacks, we investigate the
end-to-end learning completion latency of a realistic decentralized FL process
protected against poisoning attacks. We propose a technique which consists in
decoupling the monitoring phase from the detection phase in defenses against
poisoning attacks in a decentralized federated learning deployment that aim at
monitoring the behavior of the workers. We demonstrate that our proposed
blockchain-based monitoring improved network scalability, robustness and time
efficiency. The parallelization of operations results in minimized latency over
the end-to-end communication, computation, and consensus delays incurred during
the FL and blockchain operations.",https://github.com/LiTrans/BSMD/tree/master/,-1
Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition,0.486811,"Self-supervised learning (SSL) based models have been shown to generate
powerful representations that can be used to improve the performance of
downstream speech tasks. Several state-of-the-art SSL models are available, and
each of these models optimizes a different loss which gives rise to the
possibility of their features being complementary. This paper proposes using an
ensemble of such SSL representations and models, which exploits the
complementary nature of the features extracted by the various pretrained
models. We hypothesize that this results in a richer feature representation and
shows results for the ASR downstream task. To this end, we use three SSL models
that have shown excellent results on ASR tasks, namely HuBERT, Wav2vec2.0, and
WaveLM. We explore the ensemble of models fine-tuned for the ASR task and the
ensemble of features using the embeddings obtained from the pre-trained models
for a downstream ASR task. We get improved performance over individual models
and pre-trained features using Librispeech(100h) and WSJ dataset for the
downstream tasks.",None,-1
PathologyBERT -- Pre-trained Vs. A New Transformer Language Model for Pathology Domain,0.690825,"Pathology text mining is a challenging task given the reporting variability
and constant new findings in cancer sub-type definitions. However, successful
text mining of a large pathology database can play a critical role to advance
'big data' cancer research like similarity-based treatment selection, case
identification, prognostication, surveillance, clinical trial screening, risk
stratification, and many others. While there is a growing interest in
developing language models for more specific clinical domains, no
pathology-specific language space exist to support the rapid data-mining
development in pathology space. In literature, a few approaches fine-tuned
general transformer models on specialized corpora while maintaining the
original tokenizer, but in fields requiring specialized terminology, these
models often fail to perform adequately. We propose PathologyBERT - a
pre-trained masked language model which was trained on 347,173 histopathology
specimen reports and publicly released in the Huggingface repository. Our
comprehensive experiments demonstrate that pre-training of transformer model on
pathology corpora yields performance improvements on Natural Language
Understanding (NLU) and Breast Cancer Diagnose Classification when compared to
nonspecific language models.",None,-1
"From Perception to Programs: Regularize, Overparameterize, and Amortize",0.558913,"Toward combining inductive reasoning with perception abilities, we develop
techniques for neurosymbolic program synthesis where perceptual input is first
parsed by neural nets into a low-dimensional interpretable representation,
which is then processed by a synthesized program. We explore several techniques
for relaxing the problem and jointly learning all modules end-to-end with
gradient descent: multitask learning; amortized inference;
overparameterization; and a differentiable strategy for penalizing lengthy
programs. Collectedly this toolbox improves the stability of gradient-guided
program search, and suggests ways of learning both how to perceive input as
discrete abstractions, and how to symbolically process those abstractions as
programs.",None,1751
In-BoXBART: Get Instructions into Biomedical Multi-Task Learning,0.402515,"Single-task models have proven pivotal in solving specific tasks; however,
they have limitations in real-world applications where multi-tasking is
necessary and domain shifts are exhibited. Recently, instructional prompts have
shown significant improvement towards multi-task generalization; however, the
effect of instructional prompts and Multi-Task Learning (MTL) has not been
systematically studied in the biomedical domain. Motivated by this, this paper
explores the impact of instructional prompts for biomedical MTL. We introduce
the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X)
various categories. Using this meta-dataset, we propose a unified model termed
In-BoXBART, that can jointly learn all tasks of the BoX without any
task-specific modules. To the best of our knowledge, this is the first attempt
to propose a unified model in the biomedical domain and use instructions to
achieve generalization across several biomedical tasks. Experimental results
indicate that the proposed model: 1) outperforms the single-task baseline by
~3% and multi-task (without instruction) baseline by ~18% on an average, and 2)
shows ~23% improvement compared to the single-task baseline in few-shot
learning (i.e., 32 instances per task) on an average. Our analysis indicates
that there is significant room for improvement across tasks in the BoX,
implying the scope for future research direction.",https://github.com/Mihir3009/In-BoXBART,-1
Machine Learning Methods in Solving the Boolean Satisfiability Problem,0.849792,"This paper reviews the recent literature on solving the Boolean
satisfiability problem (SAT), an archetypal NP-complete problem, with the help
of machine learning techniques. Despite the great success of modern SAT solvers
to solve large industrial instances, the design of handcrafted heuristics is
time-consuming and empirical. Under the circumstances, the flexible and
expressive machine learning methods provide a proper alternative to solve this
long-standing problem. We examine the evolving ML-SAT solvers from naive
classifiers with handcrafted features to the emerging end-to-end SAT solvers
such as NeuroSAT, as well as recent progress on combinations of existing CDCL
and local search solvers with machine learning methods. Overall, solving SAT
with machine learning is a promising yet challenging research topic. We
conclude the limitations of current works and suggest possible future
directions.",None,-1
Deep Reinforcement Learning for Multi-class Imbalanced Training,0.291195,"With the rapid growth of memory and computing power, datasets are becoming
increasingly complex and imbalanced. This is especially severe in the context
of clinical data, where there may be one rare event for many cases in the
majority class. We introduce an imbalanced classification framework, based on
reinforcement learning, for training extremely imbalanced data sets, and extend
it for use in multi-class settings. We combine dueling and double deep
Q-learning architectures, and formulate a custom reward function and
episode-training procedure, specifically with the added capability of handling
multi-class imbalanced training. Using real-world clinical case studies, we
demonstrate that our proposed framework outperforms current state-of-the-art
imbalanced learning methods, achieving more fair and balanced classification,
while also significantly improving the prediction of minority classes.",None,-1
UIT-ViCoV19QA: A Dataset for COVID-19 Community-based Question Answering on Vietnamese Language,0.642567,"For the last two years, from 2020 to 2021, COVID-19 has broken disease
prevention measures in many countries, including Vietnam, and negatively
impacted various aspects of human life and the social community. Besides, the
misleading information in the community and fake news about the pandemic are
also serious situations. Therefore, we present the first Vietnamese
community-based question answering dataset for developing question answering
systems for COVID-19 called UIT-ViCoV19QA. The dataset comprises 4,500
question-answer pairs collected from trusted medical sources, with at least one
answer and at most four unique paraphrased answers per question. Along with the
dataset, we set up various deep learning models as baseline to assess the
quality of our dataset and initiate the benchmark results for further research
through commonly used metrics such as BLEU, METEOR, and ROUGE-L. We also
illustrate the positive effects of having multiple paraphrased answers
experimented on these models, especially on Transformer - a dominant
architecture in the field of study.",None,-1
Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference,0.454463,"It has been shown that NLI models are usually biased with respect to the
word-overlap between premise and hypothesis; they take this feature as a
primary cue for predicting the entailment label. In this paper, we focus on an
overlooked aspect of the overlap bias in NLI models: the reverse word-overlap
bias. Our experimental results demonstrate that current NLI models are highly
biased towards the non-entailment label on instances with low overlap, and the
existing debiasing methods, which are reportedly successful on existing
challenge datasets, are generally ineffective in addressing this category of
bias. We investigate the reasons for the emergence of the overlap bias and the
role of minority examples in its mitigation. For the former, we find that the
word-overlap bias does not stem from pre-training, and for the latter, we
observe that in contrast to the accepted assumption, eliminating minority
examples does not affect the generalizability of debiasing methods with respect
to the overlap bias.",https://github.com/sara-rajaee/reverse_bias,-1
Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather,0.922451,"Most object detection methods for autonomous driving usually assume a
consistent feature distribution between training and testing data, which is not
always the case when weathers differ significantly. The object detection model
trained under clear weather might not be effective enough in foggy weather
because of the domain gap. This paper proposes a novel domain adaptive object
detection framework for autonomous driving under foggy weather. Our method
leverages both image-level and object-level adaptation to diminish the domain
discrepancy in image style and object appearance. To further enhance the
model's capabilities under challenging samples, we also come up with a new
adversarial gradient reversal layer to perform adversarial mining for the hard
examples together with domain adaptation. Moreover, we propose to generate an
auxiliary domain by data augmentation to enforce a new domain-level metric
regularization. Experimental results on public benchmarks show the
effectiveness and accuracy of the proposed method. The code is available at
https://github.com/jinlong17/DA-Detect.",https://github.com/jinlong17/DA-Detect,-1
Threshold Treewidth and Hypertree Width,0.0516171,"Treewidth and hypertree width have proven to be highly successful structural
parameters in the context of the Constraint Satisfaction Problem (CSP). When
either of these parameters is bounded by a constant, then CSP becomes solvable
in polynomial time. However, here the order of the polynomial in the running
time depends on the width, and this is known to be unavoidable; therefore, the
problem is not fixed-parameter tractable parameterized by either of these width
measures. Here we introduce an enhancement of tree and hypertree width through
a novel notion of thresholds, allowing the associated decompositions to take
into account information about the computational costs associated with solving
the given CSP instance. Aside from introducing these notions, we obtain
efficient theoretical as well as empirical algorithms for computing threshold
treewidth and hypertree width and show that these parameters give rise to
fixed-parameter algorithms for CSP as well as other, more general problems. We
complement our theoretical results with experimental evaluations in terms of
heuristics as well as exact methods based on SAT/SMT encodings.",None,-1
"Human Detection of Political Speech Deepfakes across Transcripts, Audio, and Video",0.333658,"Recent advances in technology for hyper-realistic visual and audio effects
provoke the concern that deepfake videos of political speeches will soon be
indistinguishable from authentic video recordings. The conventional wisdom in
communication theory predicts people will fall for fake news more often when
the same version of a story is presented as a video versus text. We conduct 5
pre-registered randomized experiments with 2,215 participants to evaluate how
accurately humans distinguish real political speeches from fabrications across
base rates of misinformation, audio sources, question framings, and media
modalities. We find base rates of misinformation minimally influence
discernment and deepfakes with audio produced by the state-of-the-art
text-to-speech algorithms are harder to discern than the same deepfakes with
voice actor audio. Moreover across all experiments, we find audio and visual
information enables more accurate discernment than text alone: human
discernment relies more on how something is said, the audio-visual cues, than
what is said, the speech content.",https://researchbox.org/1723&PEER_REVIEW_passcode=EGVULE,-1
Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language,0.544912,"To solve difficult tasks, humans ask questions to acquire knowledge from
external sources. In contrast, classical reinforcement learning agents lack
such an ability and often resort to exploratory behavior. This is exacerbated
as few present-day environments support querying for knowledge. In order to
study how agents can be taught to query external knowledge via language, we
first introduce two new environments: the grid-world-based Q-BabyAI and the
text-based Q-TextWorld. In addition to physical interactions, an agent can
query an external knowledge source specialized for these environments to gather
information. Second, we propose the ""Asking for Knowledge"" (AFK) agent, which
learns to generate language commands to query for meaningful knowledge that
helps solve the tasks. AFK leverages a non-parametric memory, a pointer
mechanism and an episodic exploration bonus to tackle (1) irrelevant
information, (2) a large query language space, (3) delayed reward for making
meaningful queries. Extensive experiments demonstrate that the AFK agent
outperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld
environments.",https://ioujenliu.github.io/AFK,-1
PG3: Policy-Guided Planning for Generalized Policy Generation,0.375199,"A longstanding objective in classical planning is to synthesize policies that
generalize across multiple problems from the same domain. In this work, we
study generalized policy search-based methods with a focus on the score
function used to guide the search over policies. We demonstrate limitations of
two score functions and propose a new approach that overcomes these
limitations. The main idea behind our approach, Policy-Guided Planning for
Generalized Policy Generation (PG3), is that a candidate policy should be used
to guide planning on training problems as a mechanism for evaluating that
candidate. Theoretical results in a simplified setting give conditions under
which PG3 is optimal or admissible. We then study a specific instantiation of
policy search where planning problems are PDDL-based and policies are lifted
decision lists. Empirical results in six domains confirm that PG3 learns
generalized policies more efficiently and effectively than several baselines.
Code: https://github.com/ryangpeixu/pg3",https://github.com/ryangpeixu/pg3,-1
On Calibrating Semantic Segmentation Models: Analyses and An Algorithm,0.468647,"We study the problem of semantic segmentation calibration. Lots of solutions
have been proposed to approach model miscalibration of confidence in image
classification. However, to date, confidence calibration research on semantic
segmentation is still limited. We provide a systematic study on the calibration
of semantic segmentation models and propose a simple yet effective approach.
First, we find that model capacity, crop size, multi-scale testing, and
prediction correctness have impact on calibration. Among them, prediction
correctness, especially misprediction, is more important to miscalibration due
to over-confidence. Next, we propose a simple, unifying, and effective
approach, namely selective scaling, by separating correct/incorrect prediction
for scaling and more focusing on misprediction logit smoothing. Then, we study
popular existing calibration methods and compare them with selective scaling on
semantic segmentation calibration. We conduct extensive experiments with a
variety of benchmarks on both in-domain and domain-shift calibration and show
that selective scaling consistently outperforms other methods.",https://github.com/dwang181/selectivecal,-1
WinoDict: Probing language models for in-context word acquisition,0.71724,"We introduce a new in-context learning paradigm to measure Large Language
Models' (LLMs) ability to learn novel words during inference. In particular, we
rewrite Winograd-style co-reference resolution problems by replacing the key
concept word with a synthetic but plausible word that the model must understand
to complete the task. Solving this task requires the model to make use of the
dictionary definition of the new word given in the prompt. This benchmark
addresses word acquisition, one important aspect of the diachronic degradation
known to afflict LLMs. As LLMs are frozen in time at the moment they are
trained, they are normally unable to reflect the way language changes over
time. We show that the accuracy of LLMs compared to the original Winograd tasks
decreases radically in our benchmark, thus identifying a limitation of current
models and providing a benchmark to measure future improvements in LLMs ability
to do in-context learning.",https://github.com/google-research/language/tree/master/language/wino_dict,2175
PoissonMat: Remodeling Matrix Factorization using Poisson Distribution and Solving the Cold Start Problem without Input Data,0.789991,"Matrix Factorization is one of the most successful recommender system
techniques over the past decade. However, the classic probabilistic theory
framework for matrix factorization is modeled using normal distributions. To
find better probabilistic models, algorithms such as RankMat, ZeroMat and
DotMat have been invented in recent years. In this paper, we model the user
rating behavior in recommender system as a Poisson process, and design an
algorithm that relies on no input data to solve the recommendation problem and
the cold start issue at the same time. We prove the superiority of our
algorithm in comparison with matrix factorization, random placement, Zipf
placement, ZeroMat, DotMat, etc.",None,-1
A sequence-to-sequence approach for document-level relation extraction,0.99205,"Motivated by the fact that many relations cross the sentence boundary, there
has been increasing interest in document-level relation extraction (DocRE).
DocRE requires integrating information within and across sentences, capturing
complex interactions between mentions of entities. Most existing methods are
pipeline-based, requiring entities as input. However, jointly learning to
extract entities and relations can improve performance and be more efficient
due to shared parameters and training steps. In this paper, we develop a
sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE
(entity extraction, coreference resolution and relation extraction) end-to-end,
replacing a pipeline of task-specific components. Using a simple strategy we
call entity hinting, we compare our approach to existing pipeline-based methods
on several popular biomedical datasets, in some cases exceeding their
performance. We also report the first end-to-end results on these datasets for
future comparison. Finally, we demonstrate that, under our model, an end-to-end
approach outperforms a pipeline-based approach. Our code, data and trained
models are available at {\url{https://github.com/johngiorgi/seq2rel}}. An
online demo is available at
{\url{https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py}}.",https://github.com/johngiorgi/seq2rel,-1
Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection,0.15529,"Over the past few years there has been major progress in the field of
synthetic data generation using simulation based techniques. These methods use
high-end graphics engines and physics-based ray-tracing rendering in order to
represent the world in 3D and create highly realistic images. Datagen has
specialized in the generation of high-quality 3D humans, realistic 3D
environments and generation of realistic human motion. This technology has been
developed into a data generation platform which we used for these experiments.
This work demonstrates the use of synthetic photo-realistic in-cabin data to
train a Driver Monitoring System that uses a lightweight neural network to
detect whether the driver's hands are on the wheel. We demonstrate that when
only a small amount of real data is available, synthetic data can be a simple
way to boost performance. Moreover, we adopt the data-centric approach and show
how performing error analysis and generating the missing edge-cases in our
platform boosts performance. This showcases the ability of human-centric
synthetic data to generalize well to the real world, and help train algorithms
in computer vision settings where data from the target domain is scarce or hard
to collect.",None,-1
Continual Sequence Generation with Adaptive Compositional Modules,0.704817,"Continual learning is essential for real-world deployment when there is a
need to quickly adapt the model to new tasks without forgetting knowledge of
old tasks. Existing work on continual sequence generation either always reuses
existing parameters to learn new tasks, which is vulnerable to catastrophic
forgetting on dissimilar tasks, or blindly adds new parameters for every new
task, which could prevent knowledge sharing between similar tasks. To get the
best of both worlds, in this work, we propose continual sequence generation
with adaptive compositional modules to adaptively add modules in transformer
architectures and compose both old and new modules for new tasks. We also
incorporate pseudo experience replay to facilitate knowledge transfer in those
shared modules. Experiment results on various sequences of generation tasks
show that our framework can adaptively add modules or reuse modules based on
task similarity, outperforming state-of-the-art baselines in terms of both
performance and parameter efficiency. We make our code public at
https://github.com/GT-SALT/Adaptive-Compositional-Modules.",https://github.com/GT-SALT/Adaptive-Compositional-Modules,-1
Regularizing Neural Network Training via Identity-wise Discriminative Feature Suppression,0.0264514,"It is well-known that a deep neural network has a strong fitting capability
and can easily achieve a low training error even with randomly assigned class
labels. When the number of training samples is small, or the class labels are
noisy, networks tend to memorize patterns specific to individual instances to
minimize the training error. This leads to the issue of overfitting and poor
generalisation performance. This paper explores a remedy by suppressing the
network's tendency to rely on instance-specific patterns for empirical error
minimisation. The proposed method is based on an adversarial training
framework. It suppresses features that can be utilized to identify individual
instances among samples within each class. This leads to classifiers only using
features that are both discriminative across classes and common within each
class. We call our method Adversarial Suppression of Identity Features (ASIF),
and demonstrate the usefulness of this technique in boosting generalisation
accuracy when faced with small datasets or noisy labels. Our source code is
available.",https://github.com/avichapman/identity-feature-suppression,-1
Event-Based Dense Reconstruction Pipeline,0.542762,"Event cameras are a new type of sensors that are different from traditional
cameras. Each pixel is triggered asynchronously by event. The trigger event is
the change of the brightness irradiated on the pixel. If the increment or
decrement of brightness is higher than a certain threshold, an event is output.
Compared with traditional cameras, event cameras have the advantages of high
dynamic range and no motion blur. Since events are caused by the apparent
motion of intensity edges, the majority of 3D reconstructed maps consist only
of scene edges, i.e., semi-dense maps, which is not enough for some
applications. In this paper, we propose a pipeline to realize event-based dense
reconstruction. First, deep learning is used to reconstruct intensity images
from events. And then, structure from motion (SfM) is used to estimate camera
intrinsic, extrinsic and sparse point cloud. Finally, multi-view stereo (MVS)
is used to complete dense reconstruction.",https://github.com/colmap/colmap,-1
Goal Recognition as a Deep Learning Task: the GRNet Approach,0.315034,"In automated planning, recognising the goal of an agent from a trace of
observations is an important task with many applications. The state-of-the-art
approaches to goal recognition rely on the application of planning techniques,
which requires a model of the domain actions and of the initial domain state
(written, e.g., in PDDL). We study an alternative approach where goal
recognition is formulated as a classification task addressed by machine
learning. Our approach, called GRNet, is primarily aimed at making goal
recognition more accurate as well as faster by learning how to solve it in a
given domain. Given a planning domain specified by a set of propositions and a
set of action names, the goal classification instances in the domain are solved
by a Recurrent Neural Network (RNN). A run of the RNN processes a trace of
observed actions to compute how likely it is that each domain proposition is
part of the agent's goal, for the problem instance under considerations. These
predictions are then aggregated to choose one of the candidate goals. The only
information required as input of the trained RNN is a trace of action labels,
each one indicating just the name of an observed action. An experimental
analysis confirms that \our achieves good performance in terms of both goal
classification accuracy and runtime, obtaining better performance w.r.t. a
state-of-the-art goal recognition system over the considered benchmarks.",None,3062
Efficient Fine-Tuning of Compressed Language Models with Learners,0.0473809,"Fine-tuning BERT-based models is resource-intensive in memory, computation,
and time. While many prior works aim to improve inference efficiency via
compression techniques, e.g., pruning, these works do not explicitly address
the computational challenges of training to downstream tasks. We introduce
Learner modules and priming, novel methods for fine-tuning that exploit the
overparameterization of pre-trained language models to gain benefits in
convergence speed and resource utilization. Learner modules navigate the double
bind of 1) training efficiently by fine-tuning a subset of parameters, and 2)
training effectively by ensuring quick convergence and high metric scores. Our
results on DistilBERT demonstrate that learners perform on par with or surpass
the baselines. Learners train 7x fewer parameters than state-of-the-art methods
on GLUE. On CoLA, learners fine-tune 20% faster, and have significantly lower
resource utilization.",None,-1
DPCL: a Language Template for Normative Specifications,0.104644,"Several solutions for specifying normative artefacts (norms, contracts,
policies) in a computational processable way have been presented in the
literature. Legal core ontologies have been proposed to systematize concepts
and relationships relevant to normative reasoning. However, no solution amongst
those has achieved general acceptance, and no common ground (representational,
computational) has been identified enabling us to easily compare them. Yet, all
these efforts share the same motivation of representing normative directives,
therefore it is plausible that there may be a representational model
encompassing all of them. This presentation will introduce DPCL, a
domain-specific language (DSL) for specifying higher-level policies (including
norms, contracts, etc.), centred on Hohfeld's framework of fundamental legal
concepts. DPCL has to be seen primarily as a ""template"", i.e. as an
informational model for architectural reference, rather than a fully-fledged
formal language; it aims to make explicit the general requirements that should
be expected in a language for norm specification. In this respect, it goes
rather in the direction of legal core ontologies, but differently from those,
our proposal aims to keep the character of a DSL, rather than a set of axioms
in a logical framework: it is meant to be cross-compiled to underlying
languages/tools adequate to the type of target application. We provide here an
overview of some of the language features.",None,-1
Collaborative Propagation on Multiple Instance Graphs for 3D Instance Segmentation with Single-point Supervision,0.0461746,"Instance segmentation on 3D point clouds has been attracting increasing
attention due to its wide applications, especially in scene understanding
areas. However, most existing methods operate on fully annotated data while
manually preparing ground-truth labels at point-level is very cumbersome and
labor-intensive. To address this issue, we propose a novel weakly supervised
method RWSeg that only requires labeling one object with one point. With these
sparse weak labels, we introduce a unified framework with two branches to
propagate semantic and instance information respectively to unknown regions
using self-attention and a cross-graph random walk method. Specifically, we
propose a Cross-graph Competing Random Walks (CRW) algorithm that encourages
competition among different instance graphs to resolve ambiguities in closely
placed objects, improving instance assignment accuracy. RWSeg generates
high-quality instance-level pseudo labels. Experimental results on ScanNet-v2
and S3DIS datasets show that our approach achieves comparable performance with
fully-supervised methods and outperforms previous weakly-supervised methods by
a substantial margin.",None,-1
Towards Revenue Maximization with Popular and Profitable Products,0.051129,"Economic-wise, a common goal for companies conducting marketing is to
maximize the return revenue/profit by utilizing the various effective marketing
strategies. Consumer behavior is crucially important in economy and targeted
marketing, in which behavioral economics can provide valuable insights to
identify the biases and profit from customers. Finding credible and reliable
information on products' profitability is, however, quite difficult since most
products tends to peak at certain times w.r.t. seasonal sales cycle in a year.
On-Shelf Availability (OSA) plays a key factor for performance evaluation.
Besides, staying ahead of hot product trends means we can increase marketing
efforts without selling out the inventory. To fulfill this gap, in this paper,
we first propose a general profit-oriented framework to address the problem of
revenue maximization based on economic behavior, and compute the 0n-shelf
Popular and most Profitable Products (OPPPs) for the targeted marketing. To
tackle the revenue maximization problem, we model the k-satisfiable product
concept and propose an algorithmic framework for searching OPPP and its
variants. Extensive experiments are conducted on several real-world datasets to
evaluate the effectiveness and efficiency of the proposed algorithm.",None,-1
SATr: Slice Attention with Transformer for Universal Lesion Detection,0.845477,"Universal Lesion Detection (ULD) in computed tomography plays an essential
role in computer-aided diagnosis. Promising ULD results have been reported by
multi-slice-input detection approaches which model 3D context from multiple
adjacent CT slices, but such methods still experience difficulty in obtaining a
global representation among different slices and within each individual slice
since they only use convolution-based fusion operations. In this paper, we
propose a novel Slice Attention Transformer (SATr) block which can be easily
plugged into convolution-based ULD backbones to form hybrid network structures.
Such newly formed hybrid backbones can better model long-distance feature
dependency via the cascaded self-attention modules in the Transformer block
while still holding a strong power of modeling local features with the
convolutional operations in the original backbone. Experiments with five
state-of-the-art methods show that the proposed SATr block can provide an
almost free boost to lesion detection accuracy without extra hyperparameters or
special network designs.",https://github.com/jacobgil/pytorch-grad-cam,-1
Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer,0.510458,"Learning Bird's Eye View (BEV) representation from surrounding-view cameras
is of great importance for autonomous driving. In this work, we propose a
Geometry-guided Kernel Transformer (GKT), a novel 2D-to-BEV representation
learning mechanism. GKT leverages the geometric priors to guide the transformer
to focus on discriminative regions and unfolds kernel features to generate BEV
representation. For fast inference, we further introduce a look-up table (LUT)
indexing method to get rid of the camera's calibrated parameters at runtime.
GKT can run at $72.3$ FPS on 3090 GPU / $45.6$ FPS on 2080ti GPU and is robust
to the camera deviation and the predefined BEV height. And GKT achieves the
state-of-the-art real-time segmentation results, i.e., 38.0 mIoU
(100m$\times$100m perception range at a 0.5m resolution) on the nuScenes val
set. Given the efficiency, effectiveness, and robustness, GKT has great
practical values in autopilot scenarios, especially for real-time running
systems. Code and models will be available at
\url{https://github.com/hustvl/GKT}.",https://github.com/hustvl/GKT,-1
MaskRange: A Mask-classification Model for Range-view based LiDAR Segmentation,0.234821,"Range-view based LiDAR segmentation methods are attractive for practical
applications due to their direct inheritance from efficient 2D CNN
architectures. In literature, most range-view based methods follow the
per-pixel classification paradigm. Recently, in the image segmentation domain,
another paradigm formulates segmentation as a mask-classification problem and
has achieved remarkable performance. This raises an interesting question: can
the mask-classification paradigm benefit the range-view based LiDAR
segmentation and achieve better performance than the counterpart per-pixel
paradigm? To answer this question, we propose a unified mask-classification
model, MaskRange, for the range-view based LiDAR semantic and panoptic
segmentation. Along with the new paradigm, we also propose a novel data
augmentation method to deal with overfitting, context-reliance, and
class-imbalance problems. Extensive experiments are conducted on the
SemanticKITTI benchmark. Among all published range-view based methods, our
MaskRange achieves state-of-the-art performance with $66.10$ mIoU on semantic
segmentation and promising results with $53.10$ PQ on panoptic segmentation
with high efficiency. Our code will be released.",None,-1
Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches,0.194295,"Surgical captioning plays an important role in surgical instruction
prediction and report generation. However, the majority of captioning models
still rely on the heavy computational object detector or feature extractor to
extract regional features. In addition, the detection model requires additional
bounding box annotation which is costly and needs skilled annotators. These
lead to inference delay and limit the captioning model to deploy in real-time
robotic surgery. For this purpose, we design an end-to-end detector and feature
extractor-free captioning model by utilizing the patch-based shifted window
technique. We propose Shifted Window-Based Multi-Layer Perceptrons Transformer
Captioning model (SwinMLP-TranCAP) with faster inference speed and less
computation. SwinMLP-TranCAP replaces the multi-head attention module with
window-based multi-head MLP. Such deployments primarily focus on image
understanding tasks, but very few works investigate the caption generation
task. SwinMLP-TranCAP is also extended into a video version for video
captioning tasks using 3D patches and windows. Compared with previous
detector-based or feature extractor-based models, our models greatly simplify
the architecture design while maintaining performance on two surgical datasets.
The code is publicly available at
https://github.com/XuMengyaAmy/SwinMLP_TranCAP.",https://github.com/XuMengyaAmy/SwinMLP_TranCAP,-1
Solving Quantitative Reasoning Problems with Language Models,0.999958,"Language models have achieved remarkable performance on a wide range of tasks
that require natural language understanding. Nevertheless, state-of-the-art
models have generally struggled with tasks that require quantitative reasoning,
such as solving mathematics, science, and engineering problems at the college
level. To help close this gap, we introduce Minerva, a large language model
pretrained on general natural language data and further trained on technical
content. The model achieves state-of-the-art performance on technical
benchmarks without the use of external tools. We also evaluate our model on
over two hundred undergraduate-level problems in physics, biology, chemistry,
economics, and other sciences that require quantitative reasoning, and find
that the model can correctly answer nearly a third of them.",None,-1
Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation,0.619518,"We consider the problem of multi-agent navigation and collision avoidance
when observations are limited to the local neighborhood of each agent. We
propose InforMARL, a novel architecture for multi-agent reinforcement learning
(MARL) which uses local information intelligently to compute paths for all the
agents in a decentralized manner. Specifically, InforMARL aggregates
information about the local neighborhood of agents for both the actor and the
critic using a graph neural network and can be used in conjunction with any
standard MARL algorithm. We show that (1) in training, InforMARL has better
sample efficiency and performance than baseline approaches, despite using less
information, and (2) in testing, it scales well to environments with arbitrary
numbers of agents and obstacles. We illustrate these results using four task
environments, including one with predetermined goals for each agent, and one in
which the agents collectively try to cover all goals. Code available at
https://github.com/nsidn98/InforMARL.",None,-1
Gradient-based Uncertainty for Monocular Depth Estimation,0.808156,"In monocular depth estimation, disturbances in the image context, like moving
objects or reflecting materials, can easily lead to erroneous predictions. For
that reason, uncertainty estimates for each pixel are necessary, in particular
for safety-critical applications such as automated driving. We propose a post
hoc uncertainty estimation approach for an already trained and thus fixed depth
estimation model, represented by a deep neural network. The uncertainty is
estimated with the gradients which are extracted with an auxiliary loss
function. To avoid relying on ground-truth information for the loss definition,
we present an auxiliary loss function based on the correspondence of the depth
prediction for an image and its horizontally flipped counterpart. Our approach
achieves state-of-the-art uncertainty estimation results on the KITTI and NYU
Depth V2 benchmarks without the need to retrain the neural network. Models and
code are publicly available at https://github.com/jhornauer/GrUMoDepth.",https://github.com/jhornauer/GrUMoDepth,-1
Class-Incremental Learning via Knowledge Amalgamation,0.338542,"Catastrophic forgetting has been a significant problem hindering the
deployment of deep learning algorithms in the continual learning setting.
Numerous methods have been proposed to address the catastrophic forgetting
problem where an agent loses its generalization power of old tasks while
learning new tasks. We put forward an alternative strategy to handle the
catastrophic forgetting with knowledge amalgamation (CFA), which learns a
student network from multiple heterogeneous teacher models specializing in
previous tasks and can be applied to current offline methods. The knowledge
amalgamation process is carried out in a single-head manner with only a
selected number of memorized samples and no annotations. The teachers and
students do not need to share the same network structure, allowing
heterogeneous tasks to be adapted to a compact or sparse data representation.
We compare our method with competitive baselines from different strategies,
demonstrating our approach's advantages.",https://github.com/Ivsucram/CFA,-1
Improving Dense Contrastive Learning with Dense Negative Pairs,0.0430225,"Many contrastive representation learning methods learn a single global
representation of an entire image. However, dense contrastive representation
learning methods such as DenseCL (Wang et al., 2021) can learn better
representations for tasks requiring stronger spatial localization of features,
such as multi-label classification, detection, and segmentation. In this work,
we study how to improve the quality of the representations learned by DenseCL
by modifying the training scheme and objective function, and propose DenseCL++.
We also conduct several ablation studies to better understand the effects of:
(i) various techniques to form dense negative pairs among augmentations of
different images, (ii) cross-view dense negative and positive pairs, and (iii)
an auxiliary reconstruction task. Our results show 3.5% and 4% mAP improvement
over SimCLR (Chen et al., 2020a) andDenseCL in COCO multi-label classification.
In COCO and VOC segmentation tasks, we achieve 1.8% and 0.7% mIoU improvements
over SimCLR, respectively.",None,-1
MSDT: Masked Language Model Scoring Defense in Text Domain,0.109242,"Pre-trained language models allowed us to process downstream tasks with the
help of fine-tuning, which aids the model to achieve fairly high accuracy in
various Natural Language Processing (NLP) tasks. Such easily-downloaded
language models from various websites empowered the public users as well as
some major institutions to give a momentum to their real-life application.
However, it was recently proven that models become extremely vulnerable when
they are backdoor attacked with trigger-inserted poisoned datasets by malicious
users. The attackers then redistribute the victim models to the public to
attract other users to use them, where the models tend to misclassify when
certain triggers are detected within the training sample. In this paper, we
will introduce a novel improved textual backdoor defense method, named MSDT,
that outperforms the current existing defensive algorithms in specific
datasets. The experimental results illustrate that our method can be effective
and constructive in terms of defending against backdoor attack in text domain.
Code is available at https://github.com/jcroh0508/MSDT.",https://github.com/jcroh0508/MSDT,-1
Mobile Robot Manipulation using Pure Object Detection,0.0499145,"This paper addresses the problem of mobile robot manipulation using object
detection. Our approach uses detection and control as complimentary functions
that learn from real-world interactions. We develop an end-to-end manipulation
method based solely on detection and introduce Task-focused Few-shot Object
Detection (TFOD) to learn new objects and settings. Our robot collects its own
training data and automatically determines when to retrain detection to improve
performance across various subtasks (e.g., grasping). Notably, detection
training is low-cost, and our robot learns to manipulate new objects using as
few as four clicks of annotation. In physical experiments, our robot learns
visual control from a single click of annotation and a novel update
formulation, manipulates new objects in clutter and other mobile settings, and
achieves state-of-the-art results on an existing visual servo control and depth
estimation benchmark. Finally, we develop a TFOD Benchmark to support future
object detection research for robotics: https://github.com/griffbr/tfod.",https://github.com/griffbr/tfod,-1
Constants of motion network,0.444285,"The beauty of physics is that there is usually a conserved quantity in an
always-changing system, known as the constant of motion. Finding the constant
of motion is important in understanding the dynamics of the system, but
typically requires mathematical proficiency and manual analytical work. In this
paper, we present a neural network that can simultaneously learn the dynamics
of the system and the constants of motion from data. By exploiting the
discovered constants of motion, it can produce better predictions on dynamics
and can work on a wider range of systems than Hamiltonian-based neural
networks. In addition, the training progresses of our method can be used as an
indication of the number of constants of motion in a system which could be
useful in studying a novel physical system.",https://github.com/machine-discovery/comet/,-1
Image Super-resolution with An Enhanced Group Convolutional Neural Network,0.913283,"CNNs with strong learning abilities are widely chosen to resolve
super-resolution problem. However, CNNs depend on deeper network architectures
to improve performance of image super-resolution, which may increase
computational cost in general. In this paper, we present an enhanced
super-resolution group CNN (ESRGCNN) with a shallow architecture by fully
fusing deep and wide channel features to extract more accurate low-frequency
information in terms of correlations of different channels in single image
super-resolution (SISR). Also, a signal enhancement operation in the ESRGCNN is
useful to inherit more long-distance contextual information for resolving
long-term dependency. An adaptive up-sampling operation is gathered into a CNN
to obtain an image super-resolution model with low-resolution images of
different sizes. Extensive experiments report that our ESRGCNN surpasses the
state-of-the-arts in terms of SISR performance, complexity, execution speed,
image quality evaluation and visual effect in SISR. Code is found at
https://github.com/hellloxiaotian/ESRGCNN.",https://github.com/hellloxiaotian/ESRGCNN,-1
Recognition of Implicit Geographic Movement in Text,0.0448696,"Analyzing the geographic movement of humans, animals, and other phenomena is
a growing field of research. This research has benefited urban planning,
logistics, animal migration understanding, and much more. Typically, the
movement is captured as precise geographic coordinates and time stamps with
Global Positioning Systems (GPS). Although some research uses computational
techniques to take advantage of implicit movement in descriptions of route
directions, hiking paths, and historical exploration routes, innovation would
accelerate with a large and diverse corpus. We created a corpus of sentences
labeled as describing geographic movement or not and including the type of
entity moving. Creating this corpus proved difficult without any comparable
corpora to start with, high human labeling costs, and since movement can at
times be interpreted differently. To overcome these challenges, we developed an
iterative process employing hand labeling, crowd voting for confirmation, and
machine learning to predict more labels. By merging advances in word embeddings
with traditional machine learning models and model ensembling, prediction
accuracy is at an acceptable level to produce a large silver-standard corpus
despite the small gold-standard corpus training set. Our corpus will likely
benefit computational processing of geography in text and spatial cognition, in
addition to detection of movement.",None,1280
Perceiver-VL: Efficient Vision-and-Language Modeling with Iterative Latent Attention,0.110867,"We present Perceiver-VL, a vision-and-language framework that efficiently
handles high-dimensional multimodal inputs such as long videos and text.
Powered by the iterative latent cross-attention of Perceiver, our framework
scales with linear complexity, in contrast to the quadratic complexity of
self-attention used in many state-of-the-art transformer-based models. To
further improve the efficiency of our framework, we also study applying
LayerDrop on cross-attention layers and introduce a mixed-stream architecture
for cross-modal retrieval. We evaluate Perceiver-VL on diverse video-text and
image-text benchmarks, where Perceiver-VL achieves the lowest GFLOPs and
latency while maintaining competitive performance. In addition, we also provide
comprehensive analyses of various aspects of our framework, including
pretraining data, scalability of latent size and input size, dropping
cross-attention layers at inference to reduce latency, modality aggregation
strategy, positional encoding, and weight initialization strategy. Our code and
checkpoints are available at: https://github.com/zinengtang/Perceiver_VL",https://github.com/zinengtang/Perceiver_VL,-1
Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models,0.991265,"Recent open-domain dialogue models have brought numerous breakthroughs.
However, building a chat system is not scalable since it often requires a
considerable volume of human-human dialogue data, especially when enforcing
features such as persona, style, or safety. In this work, we study the
challenge of imposing roles on open-domain dialogue systems, with the goal of
making the systems maintain consistent roles while conversing naturally with
humans. To accomplish this, the system must satisfy a role specification that
includes certain conditions on the stated features as well as a system policy
on whether or not certain types of utterances are allowed. For this, we propose
an efficient data collection framework leveraging in-context few-shot learning
of large-scale language models for building role-satisfying dialogue dataset
from scratch. We then compare various architectures for open-domain dialogue
systems in terms of meeting role specifications while maintaining
conversational abilities. Automatic and human evaluations show that our models
return few out-of-bounds utterances, keeping competitive performance on general
metrics. We release a Korean dialogue dataset we built for further research.",https://github.com/naver-ai/carecall-corpus,-1
Computational Metacognition,0.237274,"Computational metacognition represents a cognitive systems perspective on
high-order reasoning in integrated artificial systems that seeks to leverage
ideas from human metacognition and from metareasoning approaches in artificial
intelligence. The key characteristic is to declaratively represent and then
monitor traces of cognitive activity in an intelligent system in order to
manage the performance of cognition itself. Improvements in cognition then lead
to improvements in behavior and thus performance. We illustrate these concepts
with an agent implementation in a cognitive architecture called MIDCA and show
the value of metacognition in problem-solving. The results illustrate how
computational metacognition improves performance by changing cognition through
meta-level goal operations and learning.",https://github.com/COLAB2/midca,-1
Atari-5: Distilling the Arcade Learning Environment down to Five Games,0.791756,"The Arcade Learning Environment (ALE) has become an essential benchmark for
assessing the performance of reinforcement learning algorithms. However, the
computational cost of generating results on the entire 57-game dataset limits
ALE's use and makes the reproducibility of many results infeasible. We propose
a novel solution to this problem in the form of a principled methodology for
selecting small but representative subsets of environments within a benchmark
suite. We applied our method to identify a subset of five ALE games, called
Atari-5, which produces 57-game median score estimates within 10% of their true
values. Extending the subset to 10-games recovers 80% of the variance for
log-scores for all games within the 57-game set. We show this level of
compression is possible due to a high degree of correlation between many of the
games in ALE.",https://github.com/maitchison/Atari-5,10193
Taylor Genetic Programming for Symbolic Regression,0.379777,"Genetic programming (GP) is a commonly used approach to solve symbolic
regression (SR) problems. Compared with the machine learning or deep learning
methods that depend on the pre-defined model and the training dataset for
solving SR problems, GP is more focused on finding the solution in a search
space. Although GP has good performance on large-scale benchmarks, it randomly
transforms individuals to search results without taking advantage of the
characteristics of the dataset. So, the search process of GP is usually slow,
and the final results could be unstable.To guide GP by these characteristics,
we propose a new method for SR, called Taylor genetic programming (TaylorGP)
(Code and appendix at https://kgae-cup.github.io/TaylorGP/). TaylorGP leverages
a Taylor polynomial to approximate the symbolic equation that fits the dataset.
It also utilizes the Taylor polynomial to extract the features of the symbolic
equation: low order polynomial discrimination, variable separability, boundary,
monotonic, and parity. GP is enhanced by these Taylor polynomial techniques.
Experiments are conducted on three kinds of benchmarks: classical SR, machine
learning, and physics. The experimental results show that TaylorGP not only has
higher accuracy than the nine baseline methods, but also is faster in finding
stable results.",https://kgae-cup.github.io/TaylorGP/,2208
ROAD: Learning an Implicit Recursive Octree Auto-Decoder to Efficiently Encode 3D Shapes,0.351916,"Compact and accurate representations of 3D shapes are central to many
perception and robotics tasks. State-of-the-art learning-based methods can
reconstruct single objects but scale poorly to large datasets. We present a
novel recursive implicit representation to efficiently and accurately encode
large datasets of complex 3D shapes by recursively traversing an implicit
octree in latent space. Our implicit Recursive Octree Auto-Decoder (ROAD)
learns a hierarchically structured latent space enabling state-of-the-art
reconstruction results at a compression ratio above 99%. We also propose an
efficient curriculum learning scheme that naturally exploits the coarse-to-fine
properties of the underlying octree spatial representation. We explore the
scaling law relating latent space dimension, dataset size, and reconstruction
accuracy, showing that increasing the latent space dimension is enough to scale
to large shape datasets. Finally, we show that our learned latent space encodes
a coarse-to-fine hierarchical structure yielding reusable latents across
different levels of details, and we provide qualitative evidence of
generalization to novel shapes outside the training set.",https://zakharos.github.io/projects/road/,-1
Unpaired Image Translation via Vector Symbolic Architectures,0.702435,"Image-to-image translation has played an important role in enabling synthetic
data for computer vision. However, if the source and target domains have a
large semantic mismatch, existing techniques often suffer from source content
corruption aka semantic flipping. To address this problem, we propose a new
paradigm for image-to-image translation using Vector Symbolic Architectures
(VSA), a theoretical framework which defines algebraic operations in a
high-dimensional vector (hypervector) space. We introduce VSA-based constraints
on adversarial learning for source-to-target translations by learning a
hypervector mapping that inverts the translation to ensure consistency with
source content. We show both qualitatively and quantitatively that our method
improves over other state-of-the-art techniques.",https://github.com/facebookresearch/vsait,146
Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions,0.575186,"Pruning is one of the predominant approaches for compressing deep neural
networks (DNNs). Lately, coresets (provable data summarizations) were leveraged
for pruning DNNs, adding the advantage of theoretical guarantees on the
trade-off between the compression rate and the approximation error. However,
coresets in this domain were either data-dependent or generated under
restrictive assumptions on both the model's weights and inputs. In real-world
scenarios, such assumptions are rarely satisfied, limiting the applicability of
coresets. To this end, we suggest a novel and robust framework for computing
such coresets under mild assumptions on the model's weights and without any
assumption on the training data. The idea is to compute the importance of each
neuron in each layer with respect to the output of the following layer. This is
achieved by a combination of L\""{o}wner ellipsoid and Caratheodory theorem. Our
method is simultaneously data-independent, applicable to various networks and
datasets (due to the simplified assumptions), and theoretically supported.
Experimental results show that our method outperforms existing coreset based
neural pruning approaches across a wide range of networks and datasets. For
example, our method achieved a $62\%$ compression rate on ResNet50 on ImageNet
with $1.09\%$ drop in accuracy.",None,506
VPTR: Efficient Transformers for Video Prediction,0.424505,"In this paper, we propose a new Transformer block for video future frames
prediction based on an efficient local spatial-temporal separation attention
mechanism. Based on this new Transformer block, a fully autoregressive video
future frames prediction Transformer is proposed. In addition, a
non-autoregressive video prediction Transformer is also proposed to increase
the inference speed and reduce the accumulated inference errors of its
autoregressive counterpart. In order to avoid the prediction of very similar
future frames, a contrastive feature loss is applied to maximize the mutual
information between predicted and ground-truth future frame features. This work
is the first that makes a formal comparison of the two types of attention-based
video future frames prediction models over different scenarios. The proposed
models reach a performance competitive with more complex state-of-the-art
models. The source code is available at \emph{https://github.com/XiYe20/VPTR}.",None,-1
Motion Prediction via Joint Dependency Modeling in Phase Space,0.40532,"Motion prediction is a classic problem in computer vision, which aims at
forecasting future motion given the observed pose sequence. Various deep
learning models have been proposed, achieving state-of-the-art performance on
motion prediction. However, existing methods typically focus on modeling
temporal dynamics in the pose space. Unfortunately, the complicated and high
dimensionality nature of human motion brings inherent challenges for dynamic
context capturing. Therefore, we move away from the conventional pose based
representation and present a novel approach employing a phase space trajectory
representation of individual joints. Moreover, current methods tend to only
consider the dependencies between physically connected joints. In this paper,
we introduce a novel convolutional neural model to effectively leverage
explicit prior knowledge of motion anatomy, and simultaneously capture both
spatial and temporal information of joint trajectory dynamics. We then propose
a global optimization module that learns the implicit relationships between
individual joint features.
  Empirically, our method is evaluated on large-scale 3D human motion benchmark
datasets (i.e., Human3.6M, CMU MoCap). These results demonstrate that our
method sets the new state-of-the-art on the benchmark datasets. Our code will
be available at https://github.com/Pose-Group/TEID.",https://github.com/Pose-Group/TEID,3258
Perception Prioritized Training of Diffusion Models,0.903955,"Diffusion models learn to restore noisy data, which is corrupted with
different levels of noise, by optimizing the weighted sum of the corresponding
loss terms, i.e., denoising score matching loss. In this paper, we show that
restoring data corrupted with certain noise levels offers a proper pretext task
for the model to learn rich visual concepts. We propose to prioritize such
noise levels over other levels during training, by redesigning the weighting
scheme of the objective function. We show that our simple redesign of the
weighting scheme significantly improves the performance of diffusion models
regardless of the datasets, architectures, and sampling strategies.",https://github.com/jychoi118/P2-weighting,-1
Multilingual Normalization of Temporal Expressions with Masked Language Models,0.543047,"The detection and normalization of temporal expressions is an important task
and preprocessing step for many applications. However, prior work on
normalization is rule-based, which severely limits the applicability in
real-world multilingual settings, due to the costly creation of new rules. We
propose a novel neural method for normalizing temporal expressions based on
masked language modeling. Our multilingual method outperforms prior rule-based
systems in many languages, and in particular, for low-resource languages with
performance improvements of up to 33 F1 on average compared to the state of the
art.",https://github.com/boschresearch/temporal-tagging-eacl,-1
An Anomaly Detection Method for Satellites Using Monte Carlo Dropout,0.539047,"Recently, there has been a significant amount of interest in satellite
telemetry anomaly detection (AD) using neural networks (NN). For AD purposes,
the current approaches focus on either forecasting or reconstruction of the
time series, and they cannot measure the level of reliability or the
probability of correct detection. Although the Bayesian neural network
(BNN)-based approaches are well known for time series uncertainty estimation,
they are computationally intractable. In this paper, we present a tractable
approximation for BNN based on the Monte Carlo (MC) dropout method for
capturing the uncertainty in the satellite telemetry time series, without
sacrificing accuracy. For time series forecasting, we employ an NN, which
consists of several Long Short-Term Memory (LSTM) layers followed by various
dense layers. We employ the MC dropout inside each LSTM layer and before the
dense layers for uncertainty estimation. With the proposed uncertainty region
and by utilizing a post-processing filter, we can effectively capture the
anomaly points. Numerical results show that our proposed time series AD
approach outperforms the existing methods from both prediction accuracy and AD
perspectives.",None,-1
ManiFlow: Implicitly Representing Manifolds with Normalizing Flows,0.104979,"Normalizing Flows (NFs) are flexible explicit generative models that have
been shown to accurately model complex real-world data distributions. However,
their invertibility constraint imposes limitations on data distributions that
reside on lower dimensional manifolds embedded in higher dimensional space.
Practically, this shortcoming is often bypassed by adding noise to the data
which impacts the quality of the generated samples. In contrast to prior work,
we approach this problem by generating samples from the original data
distribution given full knowledge about the perturbed distribution and the
noise model. To this end, we establish that NFs trained on perturbed data
implicitly represent the manifold in regions of maximum likelihood. Then, we
propose an optimization objective that recovers the most likely point on the
manifold given a sample from the perturbed distribution. Finally, we focus on
3D point clouds for which we utilize the explicit nature of NFs, i.e. surface
normals extracted from the gradient of the log-likelihood and the
log-likelihood itself, to apply Poisson surface reconstruction to refine
generated point sets.",None,-1
Approach to Predicting News -- A Precise Multi-LSTM Network With BERT,0.0582208,"Varieties of Democracy (V-Dem) is a new approach to conceptualizing and
measuring democracy and politics. It has information for 200 countries and is
one of the biggest databases for political science. According to the V-Dem
annual democracy report 2019, Taiwan is one of the two countries that got
disseminated false information from foreign governments the most. It also shows
that the ""made-up news"" has caused a great deal of confusion in Taiwanese
society and has serious impacts on global stability. Although there are several
applications helping distinguish the false information, we found out that the
pre-processing of categorizing the news is still done by human labor. However,
human labor may cause mistakes and cannot work for a long time. The growing
demands for automatic machines in the near decades show that while the machine
can do as good as humans or even better, using machines can reduce humans'
burden and cut down costs. Therefore, in this work, we build a predictive model
to classify the category of news. The corpora we used contains 28358 news and
200 news scraped from the online newspaper Liberty Times Net (LTN) website and
includes 8 categories: Technology, Entertainment, Fashion, Politics, Sports,
International, Finance, and Health. At first, we use Bidirectional Encoder
Representations from Transformers (BERT) for word embeddings which transform
each Chinese character into a (1,768) vector. Then, we use a Long Short-Term
Memory (LSTM) layer to transform word embeddings into sentence embeddings and
add another LSTM layer to transform them into document embeddings. Each
document embedding is an input for the final predicting model, which contains
two Dense layers and one Activation layer. And each document embedding is
transformed into 1 vector with 8 real numbers, then the highest one will
correspond to the 8 news categories with up to 99% accuracy.",https://github.com/LanaChen0/Predict_News,-1
Dialog Acts for Task-Driven Embodied Agents,0.866018,"Embodied agents need to be able to interact in natural language understanding
task descriptions and asking appropriate follow up questions to obtain
necessary information to be effective at successfully accomplishing tasks for a
wide range of users. In this work, we propose a set of dialog acts for
modelling such dialogs and annotate the TEACh dataset that includes over 3,000
situated, task oriented conversations (consisting of 39.5k utterances in total)
with dialog acts. TEACh-DA is one of the first large scale dataset of dialog
act annotations for embodied task completion. Furthermore, we demonstrate the
use of this annotated dataset in training models for tagging the dialog acts of
a given utterance, predicting the dialog act of the next response given a
dialog history, and use the dialog acts to guide agent's non-dialog behaviour.
In particular, our experiments on the TEACh Execution from Dialog History task
where the model predicts the sequence of low level actions to be executed in
the environment for embodied task completion, demonstrate that dialog acts can
improve end task success rate by up to 2 points compared to the system without
dialog acts.",None,-1
"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",0.994691,"We introduce INSTRUCTOR, a new method for computing text embeddings given
task instructions: every text input is embedded together with instructions
explaining the use case (e.g., task and domain descriptions). Unlike encoders
from prior work that are more specialized, INSTRUCTOR is a single embedder that
can generate text embeddings tailored to different downstream tasks and
domains, without any further training. We first annotate instructions for 330
diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive
loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are
unseen during training), ranging from classification and information retrieval
to semantic textual similarity and text generation evaluation. INSTRUCTOR,
while having an order of magnitude fewer parameters than the previous best
model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our
analysis suggests that INSTRUCTOR is robust to changes in instructions, and
that instruction finetuning mitigates the challenge of training a single model
on diverse datasets. Our model, code, and data are available at
https://instructor-embedding.github.io.",https://instructor-embedding.github.io,-1
Training Robots without Robots: Deep Imitation Learning for Master-to-Robot Policy Transfer,0.750612,"Deep imitation learning is promising for robot manipulation because it only
requires demonstration samples. In this study, deep imitation learning is
applied to tasks that require force feedback. However, existing demonstration
methods have deficiencies; bilateral teleoperation requires a complex control
scheme and is expensive, and kinesthetic teaching suffers from visual
distractions from human intervention. This research proposes a new
master-to-robot (M2R) policy transfer system that does not require robots for
teaching force feedback-based manipulation tasks. The human directly
demonstrates a task using a controller. This controller resembles the kinematic
parameters of the robot arm and uses the same end-effector with force/torque
(F/T) sensors to measure the force feedback. Using this controller, the
operator can feel force feedback without a bilateral system. The proposed
method can overcome domain gaps between the master and robot using gaze-based
imitation learning and a simple calibration method. Furthermore, a Transformer
is applied to infer policy from F/T sensory input. The proposed system was
evaluated on a bottle-cap-opening task that requires force feedback.",None,-1
PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch,0.820822,"Adversarial patch attacks mislead neural networks by injecting adversarial
pixels within a local region. Patch attacks can be highly effective in a
variety of tasks and physically realizable via attachment (e.g. a sticker) to
the real-world objects. Despite the diversity in attack patterns, adversarial
patches tend to be highly textured and different in appearance from natural
images. We exploit this property and present PatchZero, a general defense
pipeline against white-box adversarial patches without retraining the
downstream classifier or detector. Specifically, our defense detects
adversaries at the pixel-level and ""zeros out"" the patch region by repainting
with mean pixel values. We further design a two-stage adversarial training
scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA
defense performance on the image classification (ImageNet, RESISC45), object
detection (PASCAL VOC), and video classification (UCF101) tasks with little
degradation in benign performance. In addition, PatchZero transfers to
different patch shapes and attack types.",https://github.com/Trusted-AI/adversarial-robustness-toolbox,36875
Improving Rare Word Recognition with LM-aware MWER Training,0.807174,"Language models (LMs) significantly improve the recognition accuracy of
end-to-end (E2E) models on words rarely seen during training, when used in
either the shallow fusion or the rescoring setups. In this work, we introduce
LMs in the learning of hybrid autoregressive transducer (HAT) models in the
discriminative training framework, to mitigate the training versus inference
gap regarding the use of LMs. For the shallow fusion setup, we use LMs during
both hypotheses generation and loss computation, and the LM-aware MWER-trained
model achieves 10\% relative improvement over the model trained with standard
MWER on voice search test sets containing rare words. For the rescoring setup,
we learn a small neural module to generate per-token fusion weights in a
data-dependent manner. This model achieves the same rescoring WER as regular
MWER-trained model, but without the need for sweeping fusion weights.",None,-1
Benchmarking Long-tail Generalization with Likelihood Splits,0.485634,"In order to reliably process natural language, NLP systems must generalize to
the long tail of rare utterances. We propose a method to create challenging
benchmarks that require generalizing to the tail of the distribution by
re-splitting existing datasets. We create 'Likelihood Splits' where examples
that are assigned lower likelihood by a pre-trained language model (LM) are
placed in the test set, and more likely examples are in the training set. This
simple approach can be customized to construct meaningful train-test splits for
a wide range of tasks. Likelihood Splits surface more challenges than random
splits: relative error rates of state-of-the-art models increase by 59% for
semantic parsing on Spider, 93% for natural language inference on SNLI, and 33%
for yes/no question answering on BoolQ, on our splits compared with the
corresponding random splits. Moreover, Likelihood Splits create fairer
benchmarks than adversarial filtering; when the LM used to create the splits is
also employed as the task model, our splits do not unfairly penalize the LM.",https://github.com/ameyagodbole/long-tail-likelihood-splits,-1
A model-based approach to meta-Reinforcement Learning: Transformers and tree search,0.0939036,"Meta-learning is a line of research that develops the ability to leverage
past experiences to efficiently solve new learning problems. Meta-Reinforcement
Learning (meta-RL) methods demonstrate a capability to learn behaviors that
efficiently acquire and exploit information in several meta-RL problems.
  In this context, the Alchemy benchmark has been proposed by Wang et al.
[2021]. Alchemy features a rich structured latent space that is challenging for
state-of-the-art model-free RL methods. These methods fail to learn to properly
explore then exploit.
  We develop a model-based algorithm. We train a model whose principal block is
a Transformer Encoder to fit the symbolic Alchemy environment dynamics. Then we
define an online planner with the learned model using a tree search method.
This algorithm significantly outperforms previously applied model-free RL
methods on the symbolic Alchemy problem.
  Our results reveal the relevance of model-based approaches with online
planning to perform exploration and exploitation successfully in meta-RL.
Moreover, we show the efficiency of the Transformer architecture to learn
complex dynamics that arise from latent spaces present in meta-RL problems.",None,-1
Some Reflections on Drawing Causal Inference using Textual Data: Parallels Between Human Subjects and Organized Texts,0.151625,"We examine the role of textual data as study units when conducting causal
inference by drawing parallels between human subjects and organized texts. %in
human population research. We elaborate on key causal concepts and principles,
and expose some ambiguity and sometimes fallacies. To facilitate better framing
a causal query, we discuss two strategies: (i) shifting from immutable traits
to perceptions of them, and (ii) shifting from some abstract concept/property
to its constituent parts, i.e., adopting a constructivist perspective of an
abstract concept. We hope this article would raise the awareness of the
importance of articulating and clarifying fundamental concepts before delving
into developing methodologies when drawing causal inference using textual data.",None,-1
MOCHA: A Multi-Task Training Approach for Coherent Text Generation from Cognitive Perspective,0.0567198,"Teaching neural models to generate narrative coherent texts is a critical
problem. Recent pre-trained language models have achieved promising results,
but there is still a gap between human written texts and machine-generated
outputs. In this work, we propose a novel multi-task training strategy for
coherent text generation grounded on the cognitive theory of writing, which
empowers the model to learn essential subskills needed for writing including
planning and reviewing besides end-to-end generation. We extensively evaluate
our model on three open-ended generation tasks including story generation, news
article writing and argument generation. Experiments show that our model
achieves better results on both few-shot and fully-supervised settings than
strong baselines, and human evaluations confirm that our model can generate
more coherent outputs.",https://github.com/Derekkk/Mocha-EMNLP22,-1
SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller,0.698318,"In this paper, we propose a new task of sub-event generation for an unseen
process to evaluate the understanding of the coherence of sub-event actions and
objects. To solve the problem, we design SubeventWriter, a sub-event sequence
generation framework with a coherence controller. Given an unseen process, the
framework can iteratively construct the sub-event sequence by generating one
sub-event at each iteration. We also design a very effective coherence
controller to decode more coherent sub-events. As our extensive experiments and
analysis indicate, SubeventWriter can generate more reliable and meaningful
sub-event sequences for unseen processes.",https://github.com/HKUST-KnowComp/SubeventWriter,-1
Prediction of Seismic Intensity Distributions Using Neural Networks,0.356139,"The ground motion prediction equation is commonly used to predict the seismic
intensity distribution. However, it is not easy to apply this method to seismic
distributions affected by underground plate structures, which are commonly
known as abnormal seismic distributions. This study proposes a hybrid of
regression and classification approaches using neural networks. The proposed
model treats the distributions as 2-dimensional data like an image. Our method
can accurately predict seismic intensity distributions, even abnormal
distributions.",None,-1
Learning crop type mapping from regional label proportions in large-scale SAR and optical imagery,0.223015,"The application of deep learning algorithms to Earth observation (EO) in
recent years has enabled substantial progress in fields that rely on remotely
sensed data. However, given the data scale in EO, creating large datasets with
pixel-level annotations by experts is expensive and highly time-consuming. In
this context, priors are seen as an attractive way to alleviate the burden of
manual labeling when training deep learning methods for EO. For some
applications, those priors are readily available. Motivated by the great
success of contrastive-learning methods for self-supervised feature
representation learning in many computer-vision tasks, this study proposes an
online deep clustering method using crop label proportions as priors to learn a
sample-level classifier based on government crop-proportion data for a whole
agricultural region. We evaluate the method using two large datasets from two
different agricultural regions in Brazil. Extensive experiments demonstrate
that the method is robust to different data types (synthetic-aperture radar and
optical images), reporting higher accuracy values considering the major crop
types in the target regions. Thus, it can alleviate the burden of large-scale
image annotation in EO applications.",None,-1
PaCo: Parameter-Compositional Multi-Task Reinforcement Learning,0.656791,"The purpose of multi-task reinforcement learning (MTRL) is to train a single
policy that can be applied to a set of different tasks. Sharing parameters
allows us to take advantage of the similarities among tasks. However, the gaps
between contents and difficulties of different tasks bring us challenges on
both which tasks should share the parameters and what parameters should be
shared, as well as the optimization challenges due to parameter sharing. In
this work, we introduce a parameter-compositional approach (PaCo) as an attempt
to address these challenges. In this framework, a policy subspace represented
by a set of parameters is learned. Policies for all the single tasks lie in
this subspace and can be composed by interpolating with the learned set. It
allows not only flexible parameter sharing but also a natural way to improve
training. We demonstrate the state-of-the-art performance on Meta-World
benchmarks, verifying the effectiveness of the proposed approach.",https://github.com/facebookresearch/mtrl,30737
Find a Way Forward: a Language-Guided Semantic Map Navigator,0.112933,"In this paper, we introduce the map-language navigation task where an agent
executes natural language instructions and moves to the target position based
only on a given 3D semantic map. To tackle the task, we design the
instruction-aware Path Proposal and Discrimination model (iPPD). Our approach
leverages map information to provide instruction-aware path proposals, i.e., it
selects all potential instruction-aligned candidate paths to reduce the
solution space. Next, to represent the map observations along a path for a
better modality alignment, a novel Path Feature Encoding scheme tailored for
semantic maps is proposed. An attention-based Language Driven Discriminator is
designed to evaluate path candidates and determine the best path as the final
result. Our method can naturally avoid error accumulation compared with
single-step greedy decision methods. Comparing to a single-step imitation
learning approach, iPPD has performance gains above 17% on navigation success
and 0.18 on path matching measurement nDTW in challenging unseen environments.",None,-1
Saga: A Platform for Continuous Construction and Serving of Knowledge At Scale,0.522143,"We introduce Saga, a next-generation knowledge construction and serving
platform for powering knowledge-based applications at industrial scale. Saga
follows a hybrid batch-incremental design to continuously integrate billions of
facts about real-world entities and construct a central knowledge graph that
supports multiple production use cases with diverse requirements around data
freshness, accuracy, and availability. In this paper, we discuss the unique
challenges associated with knowledge graph construction at industrial scale,
and review the main components of Saga and how they address these challenges.
Finally, we share lessons-learned from a wide array of production use cases
powered by Saga.",None,3350
ComMU: Dataset for Combinatorial Music Generation,0.677715,"Commercial adoption of automatic music composition requires the capability of
generating diverse and high-quality music suitable for the desired context
(e.g., music for romantic movies, action games, restaurants, etc.). In this
paper, we introduce combinatorial music generation, a new task to create
varying background music based on given conditions. Combinatorial music
generation creates short samples of music with rich musical metadata, and
combines them to produce a complete music. In addition, we introduce ComMU, the
first symbolic music dataset consisting of short music samples and their
corresponding 12 musical metadata for combinatorial music generation. Notable
properties of ComMU are that (1) dataset is manually constructed by
professional composers with an objective guideline that induces regularity, and
(2) it has 12 musical metadata that embraces composers' intentions. Our results
show that we can generate diverse high-quality music only with metadata, and
that our unique metadata such as track-role and extended chord quality improves
the capacity of the automatic composition. We highly recommend watching our
video before reading the paper (https://pozalabs.github.io/ComMU).",None,-1
Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images,0.580896,"Developing an AI-assisted gland segmentation method from histology images is
critical for automatic cancer diagnosis and prognosis; however, the high cost
of pixel-level annotations hinders its applications to broader diseases.
Existing weakly-supervised semantic segmentation methods in computer vision
achieve degenerative results for gland segmentation, since the characteristics
and problems of glandular datasets are different from general object datasets.
We observe that, unlike natural images, the key problem with histology images
is the confusion of classes owning to morphological homogeneity and low color
contrast among different tissues. To this end, we propose a novel method Online
Easy Example Mining (OEEM) that encourages the network to focus on credible
supervision signals rather than noisy signals, therefore mitigating the
influence of inevitable false predictions in pseudo-masks. According to the
characteristics of glandular datasets, we design a strong framework for gland
segmentation. Our results exceed many fully-supervised methods and
weakly-supervised methods for gland segmentation over 4.4% and 6.04% at mIoU,
respectively. Code is available at https://github.com/xmed-lab/OEEM.",https://github.com/xmed-lab/OEEM,-1
FAPM: Fast Adaptive Patch Memory for Real-time Industrial Anomaly Detection,0.923662,"Feature embedding-based methods have shown exceptional performance in
detecting industrial anomalies by comparing features of target images with
normal images. However, some methods do not meet the speed requirements of
real-time inference, which is crucial for real-world applications. To address
this issue, we propose a new method called Fast Adaptive Patch Memory (FAPM)
for real-time industrial anomaly detection. FAPM utilizes patch-wise and
layer-wise memory banks that store the embedding features of images at the
patch and layer level, respectively, which eliminates unnecessary repetitive
computations. We also propose patch-wise adaptive coreset sampling for faster
and more accurate detection. FAPM performs well in both accuracy and speed
compared to other state-of-the-art methods",None,-1
Self-Supervised Equivariant Learning for Oriented Keypoint Detection,0.811833,"Detecting robust keypoints from an image is an integral part of many computer
vision problems, and the characteristic orientation and scale of keypoints play
an important role for keypoint description and matching. Existing
learning-based methods for keypoint detection rely on standard
translation-equivariant CNNs but often fail to detect reliable keypoints
against geometric variations. To learn to detect robust oriented keypoints, we
introduce a self-supervised learning framework using rotation-equivariant CNNs.
We propose a dense orientation alignment loss by an image pair generated by
synthetic transformations for training a histogram-based orientation map. Our
method outperforms the previous methods on an image matching benchmark and a
camera pose estimation benchmark.",None,-1
Test-Time Training for Graph Neural Networks,0.363957,"Graph Neural Networks (GNNs) have made tremendous progress in the graph
classification task. However, a performance gap between the training set and
the test set has often been noticed. To bridge such gap, in this work we
introduce the first test-time training framework for GNNs to enhance the model
generalization capacity for the graph classification task. In particular, we
design a novel test-time training strategy with self-supervised learning to
adjust the GNN model for each test graph sample. Experiments on the benchmark
datasets have demonstrated the effectiveness of the proposed framework,
especially when there are distribution shifts between training set and test
set. We have also conducted exploratory studies and theoretical analysis to
gain deeper understandings on the rationality of the design of the proposed
graph test time training framework (GT3).",None,60225
Adapted Multimodal BERT with Layer-wise Fusion for Sentiment Analysis,0.156356,"Multimodal learning pipelines have benefited from the success of pretrained
language models. However, this comes at the cost of increased model parameters.
In this work, we propose Adapted Multimodal BERT (AMB), a BERT-based
architecture for multimodal tasks that uses a combination of adapter modules
and intermediate fusion layers. The adapter adjusts the pretrained language
model for the task at hand, while the fusion layers perform task-specific,
layer-wise fusion of audio-visual information with textual BERT
representations. During the adaptation process the pre-trained language model
parameters remain frozen, allowing for fast, parameter-efficient training. In
our ablations we see that this approach leads to efficient models, that can
outperform their fine-tuned counterparts and are robust to input noise. Our
experiments on sentiment analysis with CMU-MOSEI show that AMB outperforms the
current state-of-the-art across metrics, with 3.4% relative reduction in the
resulting error and 2.1% relative improvement in 7-class classification
accuracy.",None,-1
NeRF-RPN: A general framework for object detection in NeRFs,0.81361,"This paper presents the first significant object detection framework,
NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model,
NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting
a novel voxel representation that incorporates multi-scale 3D neural volumetric
features, we demonstrate it is possible to regress the 3D bounding boxes of
objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN
is a general framework and can be applied to detect objects without class
labels. We experimented NeRF-RPN with various backbone architectures, RPN head
designs and loss functions. All of them can be trained in an end-to-end manner
to estimate high quality 3D bounding boxes. To facilitate future research in
object detection for NeRF, we built a new benchmark dataset which consists of
both synthetic and real-world data with careful labeling and clean up. Code and
dataset are available at https://github.com/lyclyc52/NeRF_RPN.",https://github.com/lyclyc52/NeRF_RPN,-1
Representation Learning with Diffusion Models,0.163101,"Diffusion models (DMs) have achieved state-of-the-art results for image
synthesis tasks as well as density estimation. Applied in the latent space of a
powerful pretrained autoencoder (LDM), their immense computational requirements
can be significantly reduced without sacrificing sampling quality. However, DMs
and LDMs lack a semantically meaningful representation space as the diffusion
process gradually destroys information in the latent variables. We introduce a
framework for learning such representations with diffusion models (LRDM). To
that end, a LDM is conditioned on the representation extracted from the clean
image by a separate encoder. In particular, the DM and the representation
encoder are trained jointly in order to learn rich representations specific to
the generative denoising process. By introducing a tractable representation
prior, we can efficiently sample from the representation distribution for
unconditional image synthesis without training of any additional model. We
demonstrate that i) competitive image generation results can be achieved with
image-parameterized LDMs, ii) LRDMs are capable of learning semantically
meaningful representations, allowing for faithful image reconstructions and
semantic interpolations. Our implementation is available at
https://github.com/jeremiastraub/diffusion.",https://github.com/jeremiastraub/diﬀusion,-1
Causal Balancing for Domain Generalization,0.471673,"While machine learning models rapidly advance the state-of-the-art on various
real-world tasks, out-of-domain (OOD) generalization remains a challenging
problem given the vulnerability of these models to spurious correlations. We
propose a balanced mini-batch sampling strategy to transform a biased data
distribution into a spurious-free balanced distribution, based on the
invariance of the underlying causal mechanisms for the data generation process.
We argue that the Bayes optimal classifiers trained on such balanced
distribution are minimax optimal across a diverse enough environment space. We
also provide an identifiability guarantee of the latent variable model of the
proposed data generation process, when utilizing enough train environments.
Experiments are conducted on DomainBed, demonstrating empirically that our
method obtains the best performance across 20 baselines reported on the
benchmark.",https://github.com/WANGXinyiLinda/causal-balancing-for-domain-generalization,17592
MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages,0.424825,"We present the results of the Workshop on Multilingual Information Access
(MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question
answering (QA) systems in 16 typologically diverse languages. In this task, we
adapted two large-scale cross-lingual open-retrieval QA datasets in 14
typologically diverse languages, and newly annotated open-retrieval QA data in
2 underrepresented languages: Tagalog and Tamil. Four teams submitted their
systems. The best system leveraging iteratively mined diverse negative examples
and larger pretrained models achieves 32.2 F1, outperforming our baseline by
4.5 points. The second best system uses entity-aware contextualized
representations for document retrieval, and achieves significant improvements
in Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores.",https://github.com/mia-workshop/MIA-Shared-Task-2022,-1
Semantic Similarity Computing Model Based on Multi Model Fine-Grained Nonlinear Fusion,0.764845,"Natural language processing (NLP) task has achieved excellent performance in
many fields, including semantic understanding, automatic summarization, image
recognition and so on. However, most of the neural network models for NLP
extract the text in a fine-grained way, which is not conducive to grasp the
meaning of the text from a global perspective. To alleviate the problem, the
combination of the traditional statistical method and deep learning model as
well as a novel model based on multi model nonlinear fusion are proposed in
this paper. The model uses the Jaccard coefficient based on part of speech,
Term Frequency-Inverse Document Frequency (TF-IDF) and word2vec-CNN algorithm
to measure the similarity of sentences respectively. According to the
calculation accuracy of each model, the normalized weight coefficient is
obtained and the calculation results are compared. The weighted vector is input
into the fully connected neural network to give the final classification
results. As a result, the statistical sentence similarity evaluation algorithm
reduces the granularity of feature extraction, so it can grasp the sentence
features globally. Experimental results show that the matching of sentence
similarity calculation method based on multi model nonlinear fusion is 84%, and
the F1 value of the model is 75%.",None,18646
WATCH: Wasserstein Change Point Detection for High-Dimensional Time Series Data,0.458342,"Detecting relevant changes in dynamic time series data in a timely manner is
crucially important for many data analysis tasks in real-world settings. Change
point detection methods have the ability to discover changes in an unsupervised
fashion, which represents a desirable property in the analysis of unbounded and
unlabeled data streams. However, one limitation of most of the existing
approaches is represented by their limited ability to handle multivariate and
high-dimensional data, which is frequently observed in modern applications such
as traffic flow prediction, human activity recognition, and smart grids
monitoring. In this paper, we attempt to fill this gap by proposing WATCH, a
novel Wasserstein distance-based change point detection approach that models an
initial distribution and monitors its behavior while processing new data
points, providing accurate and robust detection of change points in dynamic
high-dimensional data. An extensive experimental evaluation involving a large
number of benchmark datasets shows that WATCH is capable of accurately
identifying change points and outperforming state-of-the-art methods.",None,-1
Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks,0.999016,"The wide adoption and application of Masked language models~(MLMs) on
sensitive data (from legal to medical) necessitates a thorough quantitative
investigation into their privacy vulnerabilities -- to what extent do MLMs leak
information about their training data? Prior attempts at measuring leakage of
MLMs via membership inference attacks have been inconclusive, implying the
potential robustness of MLMs to privacy attacks. In this work, we posit that
prior attempts were inconclusive because they based their attack solely on the
MLM's model score. We devise a stronger membership inference attack based on
likelihood ratio hypothesis testing that involves an additional reference MLM
to more accurately quantify the privacy risks of memorization in MLMs. We show
that masked language models are extremely susceptible to likelihood ratio
membership inference attacks: Our empirical results, on models trained on
medical notes, show that our attack improves the AUC of prior membership
inference attacks from 0.66 to an alarmingly high 0.90 level, with a
significant improvement in the low-error region: at 1% false positive rate, our
attack is 51X more powerful than prior work.",None,-1
Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering,0.733156,"Retrieval augmented language models have recently become the standard for
knowledge intensive tasks. Rather than relying purely on latent semantics
within the parameters of large neural models, these methods enlist a
semi-parametric memory to encode an index of knowledge for the model to
retrieve over. Most prior work has employed text passages as the unit of
knowledge, which has high coverage at the cost of interpretability,
controllability, and efficiency. The opposite properties arise in other methods
which have instead relied on knowledge base (KB) facts. At the same time, more
recent work has demonstrated the effectiveness of storing and retrieving from
an index of Q-A pairs derived from text \citep{lewis2021paq}. This approach
yields a high coverage knowledge representation that maintains KB-like
properties due to its representations being more atomic units of information.
In this work we push this line of research further by proposing a
question-answer augmented encoder-decoder model and accompanying pretraining
strategy. This yields an end-to-end system that not only outperforms prior QA
retrieval methods on single-hop QA tasks but also enables compositional
reasoning, as demonstrated by strong performance on two multi-hop QA datasets.
Together, these methods improve the ability to interpret and control the model
while narrowing the performance gap with passage retrieval systems.",https://github.com/google-research/language/,-1
A Graph-Based Method for Soccer Action Spotting Using Unsupervised Player Classification,0.575975,"Action spotting in soccer videos is the task of identifying the specific time
when a certain key action of the game occurs. Lately, it has received a large
amount of attention and powerful methods have been introduced. Action spotting
involves understanding the dynamics of the game, the complexity of events, and
the variation of video sequences. Most approaches have focused on the latter,
given that their models exploit the global visual features of the sequences. In
this work, we focus on the former by (a) identifying and representing the
players, referees, and goalkeepers as nodes in a graph, and by (b) modeling
their temporal interactions as sequences of graphs. For the player
identification, or player classification task, we obtain an accuracy of 97.72%
in our annotated benchmark. For the action spotting task, our method obtains an
overall performance of 57.83% average-mAP by combining it with other
audiovisual modalities. This performance surpasses similar graph-based methods
and has competitive results with heavy computing methods. Code and data are
available at https://github.com/IPCV/soccer_action_spotting.",https://github.com/IPCV/soccer_action_spotting,-1
Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for OOD Intent Discovery,0.248232,"Discovering out-of-domain (OOD) intent is important for developing new skills
in task-oriented dialogue systems. The key challenges lie in how to transfer
prior in-domain (IND) knowledge to OOD clustering, as well as jointly learn OOD
representations and cluster assignments. Previous methods suffer from in-domain
overfitting problem, and there is a natural gap between representation learning
and clustering objectives. In this paper, we propose a unified K-nearest
neighbor contrastive learning framework to discover OOD intents. Specifically,
for IND pre-training stage, we propose a KCL objective to learn inter-class
discriminative features, while maintaining intra-class diversity, which
alleviates the in-domain overfitting problem. For OOD clustering stage, we
propose a KCC method to form compact clusters by mining true hard negative
samples, which bridges the gap between clustering and representation learning.
Extensive experiments on three benchmark datasets show that our method achieves
substantial improvements over the state-of-the-art methods.",https://github.com/myt517/KCOD,-1
Surgical Fine-Tuning Improves Adaptation to Distribution Shifts,0.906729,"A common approach to transfer learning under distribution shift is to
fine-tune the last few layers of a pre-trained model, preserving learned
features while also adapting to the new task. This paper shows that in such
settings, selectively fine-tuning a subset of layers (which we term surgical
fine-tuning) matches or outperforms commonly used fine-tuning approaches.
Moreover, the type of distribution shift influences which subset is more
effective to tune: for example, for image corruptions, fine-tuning only the
first few layers works best. We validate our findings systematically across
seven real-world data tasks spanning three types of distribution shifts.
Theoretically, we prove that for two-layer neural networks in an idealized
setting, first-layer tuning can outperform fine-tuning all layers. Intuitively,
fine-tuning more parameters on a small target dataset can cause information
learned during pre-training to be forgotten, and the relevant information
depends on the type of shift.",None,-1
Eliciting Best Practices for Collaboration with Computational Notebooks,0.67087,"Despite the widespread adoption of computational notebooks, little is known
about best practices for their usage in collaborative contexts. In this paper,
we fill this gap by eliciting a catalog of best practices for collaborative
data science with computational notebooks. With this aim, we first look for
best practices through a multivocal literature review. Then, we conduct
interviews with professional data scientists to assess their awareness of these
best practices. Finally, we assess the adoption of best practices through the
analysis of 1,380 Jupyter notebooks retrieved from the Kaggle platform.
Findings reveal that experts are mostly aware of the best practices and tend to
adopt them in their daily work. Nonetheless, they do not consistently follow
all the recommendations as, depending on specific contexts, some are deemed
unfeasible or counterproductive due to the lack of proper tool support. As
such, we envision the design of notebook solutions that allow data scientists
not to have to prioritize exploration and rapid prototyping over writing code
of quality.",None,-1
TurbuGAN: An Adversarial Learning Approach to Spatially-Varying Multiframe Blind Deconvolution with Applications to Imaging Through Turbulence,0.950213,"We present a self-supervised and self-calibrating multi-shot approach to
imaging through atmospheric turbulence, called TurbuGAN. Our approach requires
no paired training data, adapts itself to the distribution of the turbulence,
leverages domain-specific data priors, and can generalize from tens to
thousands of measurements. We achieve such functionality through an adversarial
sensing framework adapted from CryoGAN, which uses a discriminator network to
match the distributions of captured and simulated measurements. Our framework
builds on CryoGAN by (1) generalizing the forward measurement model to
incorporate physically accurate and computationally efficient models for light
propagation through anisoplanatic turbulence, (2) enabling adaptation to
slightly misspecified forward models, and (3) leveraging domain-specific prior
knowledge using pretrained generative networks, when available. We validate
TurbuGAN on both computationally simulated and experimentally captured images
distorted with anisoplanatic turbulence.",None,-1
Task Phasing: Automated Curriculum Learning from Demonstrations,0.130418,"Applying reinforcement learning (RL) to sparse reward domains is notoriously
challenging due to insufficient guiding signals. Common RL techniques for
addressing such domains include (1) learning from demonstrations and (2)
curriculum learning. While these two approaches have been studied in detail,
they have rarely been considered together. This paper aims to do so by
introducing a principled task phasing approach that uses demonstrations to
automatically generate a curriculum sequence. Using inverse RL from
(suboptimal) demonstrations we define a simple initial task. Our task phasing
approach then provides a framework to gradually increase the complexity of the
task all the way to the target task, while retuning the RL agent in each
phasing iteration. Two approaches for phasing are considered: (1) gradually
increasing the proportion of time steps an RL agent is in control, and (2)
phasing out a guiding informative reward function. We present conditions that
guarantee the convergence of these approaches to an optimal policy.
Experimental results on 3 sparse reward domains demonstrate that our task
phasing approaches outperform state-of-the-art approaches with respect to
asymptotic performance.",https://github.com/ParanoidAndroid96/Task-Phasing.git,-1
Guided Depth Super-Resolution by Deep Anisotropic Diffusion,0.892782,"Performing super-resolution of a depth image using the guidance from an RGB
image is a problem that concerns several fields, such as robotics, medical
imaging, and remote sensing. While deep learning methods have achieved good
results in this problem, recent work highlighted the value of combining modern
methods with more formal frameworks. In this work, we propose a novel approach
which combines guided anisotropic diffusion with a deep convolutional network
and advances the state of the art for guided depth super-resolution. The edge
transferring/enhancing properties of the diffusion are boosted by the
contextual reasoning capabilities of modern networks, and a strict adjustment
step guarantees perfect adherence to the source image. We achieve unprecedented
results in three commonly used benchmarks for guided depth super-resolution.
The performance gain compared to other methods is the largest at larger scales,
such as x32 scaling. Code
(https://github.com/prs-eth/Diffusion-Super-Resolution) for the proposed method
is available to promote reproducibility of our results.",https://github.com/prs-eth/Diffusion-Super-Resolution,-1
Deep Learning-Based Perceptual Stimulus Encoder for Bionic Vision,0.19968,"Retinal implants have the potential to treat incurable blindness, yet the
quality of the artificial vision they produce is still rudimentary. An
outstanding challenge is identifying electrode activation patterns that lead to
intelligible visual percepts (phosphenes). Here we propose a PSE based on CNN
that is trained in an end-to-end fashion to predict the electrode activation
patterns required to produce a desired visual percept. We demonstrate the
effectiveness of the encoder on MNIST using a psychophysically validated
phosphene model tailored to individual retinal implant users. The present work
constitutes an essential first step towards improving the quality of the
artificial vision provided by retinal implants.",None,-1
PVS Embeddings of Propositional and Quantified Modal Logic,0.0424726,"Modal logics allow reasoning about various modes of truth: for example, what
it means for something to be possibly true, or to know that something is true
as opposed to merely believing it. This report describes embeddings of
propositional and quantified modal logic in the PVS verification system. The
resources of PVS allow this to be done in an attractive way that supports much
of the standard syntax of modal logic, while providing effective automation.
  The report introduces and formally specifies and verifies several standard
topics in modal logic such as relationships between the standard modal axioms
and properties of the accessibility relation, and attributes of the Barcan
Formula and its converse in both constant and varying domains.",None,-1
Better Few-Shot Relation Extraction with Label Prompt Dropout,0.667347,"Few-shot relation extraction aims to learn to identify the relation between
two entities based on very limited training examples. Recent efforts found that
textual labels (i.e., relation names and relation descriptions) could be
extremely useful for learning class representations, which will benefit the
few-shot learning task. However, what is the best way to leverage such label
information in the learning process is an important research question. Existing
works largely assume such textual labels are always present during both
learning and prediction. In this work, we argue that such approaches may not
always lead to optimal results. Instead, we present a novel approach called
label prompt dropout, which randomly removes label descriptions in the learning
process. Our experiments show that our approach is able to lead to improved
class representations, yielding significantly better results on the few-shot
relation extraction task.",https://github.com/jzhang38/LPD,-1
Visual Comparison of Language Model Adaptation,0.147126,"Neural language models are widely used; however, their model parameters often
need to be adapted to the specific domains and tasks of an application, which
is time- and resource-consuming. Thus, adapters have recently been introduced
as a lightweight alternative for model adaptation. They consist of a small set
of task-specific parameters with a reduced training time and simple parameter
composition. The simplicity of adapter training and composition comes along
with new challenges, such as maintaining an overview of adapter properties and
effectively comparing their produced embedding spaces. To help developers
overcome these challenges, we provide a twofold contribution. First, in close
collaboration with NLP researchers, we conducted a requirement analysis for an
approach supporting adapter evaluation and detected, among others, the need for
both intrinsic (i.e., embedding similarity-based) and extrinsic (i.e.,
prediction-based) explanation methods. Second, motivated by the gathered
requirements, we designed a flexible visual analytics workspace that enables
the comparison of adapter properties. In this paper, we discuss several design
iterations and alternatives for interactive, comparative visual explanation
methods. Our comparative visualizations show the differences in the adapted
embedding vectors and prediction outcomes for diverse human-interpretable
concepts (e.g., person names, human qualities). We evaluate our workspace
through case studies and show that, for instance, an adapter trained on the
language debiasing task according to context-0 (decontextualized) embeddings
introduces a new type of bias where words (even gender-independent words such
as countries) become more similar to female than male pronouns. We demonstrate
that these are artifacts of context-0 embeddings.",https://github.com/Adapter-Hub/adapter-transformers,-1
Image-free Domain Generalization via CLIP for 3D Hand Pose Estimation,0.275158,"RGB-based 3D hand pose estimation has been successful for decades thanks to
large-scale databases and deep learning. However, the hand pose estimation
network does not operate well for hand pose images whose characteristics are
far different from the training data. This is caused by various factors such as
illuminations, camera angles, diverse backgrounds in the input images, etc.
Many existing methods tried to solve it by supplying additional large-scale
unconstrained/target domain images to augment data space; however collecting
such large-scale images takes a lot of labors. In this paper, we present a
simple image-free domain generalization approach for the hand pose estimation
framework that uses only source domain data. We try to manipulate the image
features of the hand pose estimation network by adding the features from text
descriptions using the CLIP (Contrastive Language-Image Pre-training) model.
The manipulated image features are then exploited to train the hand pose
estimation network via the contrastive learning framework. In experiments with
STB and RHD datasets, our algorithm shows improved performance over the
state-of-the-art domain generalization approaches.",None,-1
Monant Medical Misinformation Dataset: Mapping Articles to Fact-Checked Claims,0.604618,"False information has a significant negative influence on individuals as well
as on the whole society. Especially in the current COVID-19 era, we witness an
unprecedented growth of medical misinformation. To help tackle this problem
with machine learning approaches, we are publishing a feature-rich dataset of
approx. 317k medical news articles/blogs and 3.5k fact-checked claims. It also
contains 573 manually and more than 51k automatically labelled mappings between
claims and articles. Mappings consist of claim presence, i.e., whether a claim
is contained in a given article, and article stance towards the claim. We
provide several baselines for these two tasks and evaluate them on the manually
labelled part of the dataset. The dataset enables a number of additional tasks
related to medical misinformation, such as misinformation characterisation
studies or studies of misinformation diffusion between sources.",None,-1
Adapting to Latent Subgroup Shifts via Concepts and Proxies,0.454264,"We address the problem of unsupervised domain adaptation when the source
domain differs from the target domain because of a shift in the distribution of
a latent subgroup. When this subgroup confounds all observed data, neither
covariate shift nor label shift assumptions apply. We show that the optimal
target predictor can be non-parametrically identified with the help of concept
and proxy variables available only in the source domain, and unlabeled data
from the target. The identification results are constructive, immediately
suggesting an algorithm for estimating the optimal predictor in the target. For
continuous observations, when this algorithm becomes impractical, we propose a
latent variable model specific to the data generation process at hand. We show
how the approach degrades as the size of the shift changes, and verify that it
outperforms both covariate and label shift adjustment.",None,-1
The slurk Interaction Server Framework: Better Data for Better Dialog Models,0.33126,"This paper presents the slurk software, a lightweight interaction server for
setting up dialog data collections and running experiments. Slurk enables a
multitude of settings including text-based, speech and video interaction
between two or more humans or humans and bots, and a multimodal display area
for presenting shared or private interactive context. The software is
implemented in Python with an HTML and JS frontend that can easily be adapted
to individual needs. It also provides a setup for pairing participants on
common crowdworking platforms such as Amazon Mechanical Turk and some example
bot scripts for common interaction scenarios.",https://github.com/clp-research/slurk,-1
Controlling the Focus of Pretrained Language Generation Models,0.146238,"The finetuning of pretrained transformer-based language generation models are
typically conducted in an end-to-end manner, where the model learns to attend
to relevant parts of the input by itself. However, there does not exist a
mechanism to directly control the model's focus. This work aims to develop a
control mechanism by which a user can select spans of context as ""highlights""
for the model to focus on, and generate relevant output. To achieve this goal,
we augment a pretrained model with trainable ""focus vectors"" that are directly
applied to the model's embeddings, while the model itself is kept fixed. These
vectors, trained on automatic annotations derived from attribution methods, act
as indicators for context importance. We test our approach on two core
generation tasks: dialogue response generation and abstractive summarization.
We also collect evaluation data where the highlight-generation pairs are
annotated by humans. Our experiments show that the trained focus vectors are
effective in steering the model to generate outputs that are relevant to
user-selected highlights.",https://github.com/Question406/LearningToFocus,28783
Multilingual Machine Translation with Hyper-Adapters,0.917915,"Multilingual machine translation suffers from negative interference across
languages. A common solution is to relax parameter sharing with
language-specific modules like adapters. However, adapters of related languages
are unable to transfer information, and their total number of parameters
becomes prohibitively expensive as the number of languages grows. In this work,
we overcome these drawbacks using hyper-adapters -- hyper-networks that
generate adapters from language and layer embeddings. While past work had poor
results when scaling hyper-networks, we propose a rescaling fix that
significantly improves convergence and enables training larger hyper-networks.
We find that hyper-adapters are more parameter efficient than regular adapters,
reaching the same performance with up to 12 times less parameters. When using
the same number of parameters and FLOPS, our approach consistently outperforms
regular adapters. Also, hyper-adapters converge faster than alternative
approaches and scale better than regular dense networks. Our analysis shows
that hyper-adapters learn to encode language relatedness, enabling positive
transfer across languages.",https://github.com/cbaziotis/fairseq,9766
Unsupervised Non-transferable Text Classification,0.185904,"Training a good deep learning model requires substantial data and computing
resources, which makes the resulting neural model a valuable intellectual
property. To prevent the neural network from being undesirably exploited,
non-transferable learning has been proposed to reduce the model generalization
ability in specific target domains. However, existing approaches require
labeled data for the target domain which can be difficult to obtain.
Furthermore, they do not have the mechanism to still recover the model's
ability to access the target domain. In this paper, we propose a novel
unsupervised non-transferable learning method for the text classification task
that does not require annotated target domain data. We further introduce a
secret key component in our approach for recovering the access to the target
domain, where we design both an explicit and an implicit method for doing so.
Extensive experiments demonstrate the effectiveness of our approach.",https://github.com/ChaosCodes/UNTL,-1
Spiking Approximations of the MaxPooling Operation in Deep SNNs,0.454638,"Spiking Neural Networks (SNNs) are an emerging domain of biologically
inspired neural networks that have shown promise for low-power AI. A number of
methods exist for building deep SNNs, with Artificial Neural Network
(ANN)-to-SNN conversion being highly successful. MaxPooling layers in
Convolutional Neural Networks (CNNs) are an integral component to downsample
the intermediate feature maps and introduce translational invariance, but the
absence of their hardware-friendly spiking equivalents limits such CNNs'
conversion to deep SNNs. In this paper, we present two hardware-friendly
methods to implement Max-Pooling in deep SNNs, thus facilitating easy
conversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute
SNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware
(with MNIST, FMNIST, & CIFAR10 dataset); thus, showing the feasibility of our
approach.",None,-1
Retrieve-and-Fill for Scenario-based Task-Oriented Semantic Parsing,0.349869,"Task-oriented semantic parsing models have achieved strong results in recent
years, but unfortunately do not strike an appealing balance between model size,
runtime latency, and cross-domain generalizability. We tackle this problem by
introducing scenario-based semantic parsing: a variant of the original task
which first requires disambiguating an utterance's ""scenario"" (an intent-slot
template with variable leaf spans) before generating its frame, complete with
ontology and utterance tokens. This formulation enables us to isolate
coarse-grained and fine-grained aspects of the task, each of which we solve
with off-the-shelf neural modules, also optimizing for the axes outlined above.
Concretely, we create a Retrieve-and-Fill (RAF) architecture comprised of (1) a
retrieval module which ranks the best scenario given an utterance and (2) a
filling module which imputes spans into the scenario to create the frame. Our
model is modular, differentiable, interpretable, and allows us to garner extra
supervision from scenarios. RAF achieves strong results in high-resource,
low-resource, and multilingual settings, outperforming recent approaches by
wide margins despite, using base pre-trained encoders, small sequence lengths,
and parallel decoding.",None,-1
Semi-supervised detection of structural damage using Variational Autoencoder and a One-Class Support Vector Machine,0.393621,"In recent years, Artificial Neural Networks (ANNs) have been introduced in
Structural Health Monitoring (SHM) systems. A semi-supervised method with a
data-driven approach allows the ANN training on data acquired from an undamaged
structural condition to detect structural damages. In standard approaches,
after the training stage, a decision rule is manually defined to detect
anomalous data. However, this process could be made automatic using machine
learning methods, whom performances are maximised using hyperparameter
optimization techniques. The paper proposes a semi-supervised method with a
data-driven approach to detect structural anomalies. The methodology consists
of: (i) a Variational Autoencoder (VAE) to approximate undamaged data
distribution and (ii) a One-Class Support Vector Machine (OC-SVM) to
discriminate different health conditions using damage sensitive features
extracted from VAE's signal reconstruction. The method is applied to a scale
steel structure that was tested in nine damage's scenarios by IASC-ASCE
Structural Health Monitoring Task Group.",None,-1
Globally Optimal Event-Based Divergence Estimation for Ventral Landing,0.661295,"Event sensing is a major component in bio-inspired flight guidance and
control systems. We explore the usage of event cameras for predicting
time-to-contact (TTC) with the surface during ventral landing. This is achieved
by estimating divergence (inverse TTC), which is the rate of radial optic flow,
from the event stream generated during landing. Our core contributions are a
novel contrast maximisation formulation for event-based divergence estimation,
and a branch-and-bound algorithm to exactly maximise contrast and find the
optimal divergence value. GPU acceleration is conducted to speed up the global
algorithm. Another contribution is a new dataset containing real event streams
from ventral landing that was employed to test and benchmark our method. Owing
to global optimisation, our algorithm is much more capable at recovering the
true divergence, compared to other heuristic divergence estimators or
event-based optic flow methods. With GPU acceleration, our method also achieves
competitive runtimes.",https://github.com/s-mcleod/ventral-landing-event-dataset,-1
Efficient Task-Oriented Dialogue Systems with Response Selection as an Auxiliary Task,0.617908,"The adoption of pre-trained language models in task-oriented dialogue systems
has resulted in significant enhancements of their text generation abilities.
However, these architectures are slow to use because of the large number of
trainable parameters and can sometimes fail to generate diverse responses. To
address these limitations, we propose two models with auxiliary tasks for
response selection - (1) distinguishing distractors from ground truth responses
and (2) distinguishing synthetic responses from ground truth labels. They
achieve state-of-the-art results on the MultiWOZ 2.1 dataset with combined
scores of 107.5 and 108.3 and outperform a baseline with three times more
parameters. We publish reproducible code and checkpoints and discuss the
effects of applying auxiliary tasks to T5-based architectures.",https://github.com/,-1
BLIND: Bias Removal With No Demographics,0.661212,"Models trained on real-world data tend to imitate and amplify social biases.
Common methods to mitigate biases require prior information on the types of
biases that should be mitigated (e.g., gender or racial bias) and the social
groups associated with each data sample. In this work, we introduce BLIND, a
method for bias removal with no prior knowledge of the demographics in the
dataset. While training a model on a downstream task, BLIND detects biased
samples using an auxiliary model that predicts the main model's success, and
down-weights those samples during the training process. Experiments with racial
and gender biases in sentiment classification and occupation classification
tasks demonstrate that BLIND mitigates social biases without relying on a
costly demographic annotation process. Our method is competitive with other
methods that require demographic information and sometimes even surpasses them.",https://github.com/technion-cs-nlp/BLIND,-1
A Dual-Contrastive Framework for Low-Resource Cross-Lingual Named Entity Recognition,0.324102,"Cross-lingual Named Entity Recognition (NER) has recently become a research
hotspot because it can alleviate the data-hungry problem for low-resource
languages. However, few researches have focused on the scenario where the
source-language labeled data is also limited in some specific domains. A common
approach for this scenario is to generate more training data through
translation or generation-based data augmentation method. Unfortunately, we
find that simply combining source-language data and the corresponding
translation cannot fully exploit the translated data and the improvements
obtained are somewhat limited. In this paper, we describe our novel
dual-contrastive framework ConCNER for cross-lingual NER under the scenario of
limited source-language labeled data. Specifically, based on the
source-language samples and their translations, we design two contrastive
objectives for cross-language NER at different grammatical levels, namely
Translation Contrastive Learning (TCL) to close sentence representations
between translated sentence pairs and Label Contrastive Learning (LCL) to close
token representations within the same labels. Furthermore, we utilize knowledge
distillation method where the NER model trained above is used as the teacher to
train a student model on unlabeled target-language data to better fit the
target language. We conduct extensive experiments on a wide variety of target
languages, and the results demonstrate that ConCNER tends to outperform
multiple baseline methods. For reproducibility, our code for this paper is
available at https://github.com/GKLMIP/ConCNER.",None,-1
Fairness Increases Adversarial Vulnerability,0.346236,"The remarkable performance of deep learning models and their applications in
consequential domains (e.g., facial recognition) introduces important
challenges at the intersection of equity and security. Fairness and robustness
are two desired notions often required in learning models. Fairness ensures
that models do not disproportionately harm (or benefit) some groups over
others, while robustness measures the models' resilience against small input
perturbations.
  This paper shows the existence of a dichotomy between fairness and
robustness, and analyzes when achieving fairness decreases the model robustness
to adversarial samples. The reported analysis sheds light on the factors
causing such contrasting behavior, suggesting that distance to the decision
boundary across groups as a key explainer for this behavior. Extensive
experiments on non-linear models and different architectures validate the
theoretical findings in multiple vision domains. Finally, the paper proposes a
simple, yet effective, solution to construct models achieving good tradeoffs
between fairness and robustness.",None,-1
Improving Data Driven Inverse Text Normalization using Data Augmentation,0.0245531,"Inverse text normalization (ITN) is used to convert the spoken form output of
an automatic speech recognition (ASR) system to a written form. Traditional
handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile
neural modeling approaches require quality large-scale spoken-written pair
examples in the same or similar domain as the ASR system (in-domain data), to
train. Both these approaches require costly and complex annotations. In this
paper, we present a data augmentation technique that effectively generates rich
spoken-written numeric pairs from out-of-domain textual data with minimal human
annotation. We empirically demonstrate that ITN model trained using our data
augmentation technique consistently outperform ITN model trained using only
in-domain data across all numeric surfaces like cardinal, currency, and
fraction, by an overall accuracy of 14.44%.",None,1187
A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference,0.187978,"Most evaluations of attribution methods focus on the English language. In
this work, we present a multilingual approach for evaluating attribution
methods for the Natural Language Inference (NLI) task in terms of faithfulness
and plausibility. First, we introduce a novel cross-lingual strategy to measure
faithfulness based on word alignments, which eliminates the drawbacks of
erasure-based evaluations.We then perform a comprehensive evaluation of
attribution methods, considering different output mechanisms and aggregation
methods. Finally, we augment the XNLI dataset with highlight-based
explanations, providing a multilingual NLI dataset with highlights, to support
future exNLP studies. Our results show that attribution methods performing best
for plausibility and faithfulness are different.",https://www.keremzaman.com/explaiNLI,-1
Volume Rendering Digest (for NeRF),0.561598,"Neural Radiance Fields employ simple volume rendering as a way to overcome
the challenges of differentiating through ray-triangle intersections by
leveraging a probabilistic notion of visibility. This is achieved by assuming
the scene is composed by a cloud of light-emitting particles whose density
changes in space. This technical report summarizes the derivations for
differentiable volume rendering. It is a condensed version of previous reports,
but rewritten in the context of NeRF, and adopting its commonly used notation.",None,-1
Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations,0.793808,"We propose an unsupervised method for 3D geometry-aware representation
learning of articulated objects, in which no image-pose pairs or foreground
masks are used for training. Though photorealistic images of articulated
objects can be rendered with explicit pose control through existing 3D neural
representations, these methods require ground truth 3D pose and foreground
masks for training, which are expensive to obtain. We obviate this need by
learning the representations with GAN training. The generator is trained to
produce realistic images of articulated objects from random poses and latent
vectors by adversarial training. To avoid a high computational cost for GAN
training, we propose an efficient neural representation for articulated objects
based on tri-planes and then present a GAN-based framework for its unsupervised
training. Experiments demonstrate the efficiency of our method and show that
GAN-based training enables the learning of controllable 3D representations
without paired supervision.",https://github.com/open-mmlab/mmpose,-1
Towards True Detail Restoration for Super-Resolution: A Benchmark and a Quality Metric,0.0809544,"Super-resolution (SR) has become a widely researched topic in recent years.
SR methods can improve overall image and video quality and create new
possibilities for further content analysis. But the SR mainstream focuses
primarily on increasing the naturalness of the resulting image despite
potentially losing context accuracy. Such methods may produce an incorrect
digit, character, face, or other structural object even though they otherwise
yield good visual quality. Incorrect detail restoration can cause errors when
detecting and identifying objects both manually and automatically. To analyze
the detail-restoration capabilities of image and video SR models, we developed
a benchmark based on our own video dataset, which contains complex patterns
that SR models generally fail to correctly restore. We assessed 32 recent SR
models using our benchmark and compared their ability to preserve scene
context. We also conducted a crowd-sourced comparison of restored details and
developed an objective assessment metric that outperforms other quality metrics
by correlation with subjective scores for this task. In conclusion, we provide
a deep analysis of benchmark results that yields insights for future SR-based
work.",None,-1
Leveraging QA Datasets to Improve Generative Data Augmentation,0.368318,"The ability of generative language models (GLMs) to generate text has
improved considerably in the last few years, enabling their use for generative
data augmentation. In this work, we propose CONDA, an approach to further
improve GLMs' ability to generate synthetic data by reformulating data
generation as context generation for a given question-answer (QA) pair and
leveraging QA datasets for training context generators. Then, we cast
downstream tasks into the same question answering format and adapt the
fine-tuned context generators to the target task domain. Finally, we use the
fine-tuned GLM to generate relevant contexts, which are in turn used as
synthetic training data for their corresponding tasks. We perform extensive
experiments on multiple classification datasets and demonstrate substantial
improvements in performance for both few- and zero-shot settings. Our analysis
reveals that QA datasets that require high-level reasoning abilities (e.g.,
abstractive and common-sense QA datasets) tend to give the best boost in
performance in both few-shot and zero-shot settings.",https://github.com/dheeraj7596/CONDA,-1
Pre-Avatar: An Automatic Presentation Generation Framework Leveraging Talking Avatar,0.0459139,"Since the beginning of the COVID-19 pandemic, remote conferencing and
school-teaching have become important tools. The previous applications aim to
save the commuting cost with real-time interactions. However, our application
is going to lower the production and reproduction costs when preparing the
communication materials. This paper proposes a system called Pre-Avatar,
generating a presentation video with a talking face of a target speaker with 1
front-face photo and a 3-minute voice recording. Technically, the system
consists of three main modules, user experience interface (UEI), talking face
module and few-shot text-to-speech (TTS) module. The system firstly clones the
target speaker's voice, and then generates the speech, and finally generate an
avatar with appropriate lip and head movements. Under any scenario, users only
need to replace slides with different notes to generate another new video. The
demo has been released here and will be published as free software for use.",None,11661
Search to Pass Messages for Temporal Knowledge Graph Completion,0.60653,"Completing missing facts is a fundamental task for temporal knowledge graphs
(TKGs). Recently, graph neural network (GNN) based methods, which can
simultaneously explore topological and temporal information, have become the
state-of-the-art (SOTA) to complete TKGs. However, these studies are based on
hand-designed architectures and fail to explore the diverse topological and
temporal properties of TKG. To address this issue, we propose to use neural
architecture search (NAS) to design data-specific message passing architecture
for TKG completion. In particular, we develop a generalized framework to
explore topological and temporal information in TKGs. Based on this framework,
we design an expressive search space to fully capture various properties of
different TKGs. Meanwhile, we adopt a search algorithm, which trains a supernet
structure by sampling single path for efficient search with less cost. We
further conduct extensive experiments on three benchmark datasets. The results
show that the searched architectures by our method achieve the SOTA
performances. Besides, the searched models can also implicitly reveal diverse
properties in different TKGs. Our code is released in
https://github.com/striderdu/SPA.",https://github.com/striderdu/SPA,9577
Efficient Hybrid Network: Inducting Scattering Features,0.0546577,"Recent work showed that hybrid networks, which combine predefined and learnt
filters within a single architecture, are more amenable to theoretical analysis
and less prone to overfitting in data-limited scenarios. However, their
performance has yet to prove competitive against the conventional counterparts
when sufficient amounts of training data are available. In an attempt to
address this core limitation of current hybrid networks, we introduce an
Efficient Hybrid Network (E-HybridNet). We show that it is the first scattering
based approach that consistently outperforms its conventional counterparts on a
diverse range of datasets. It is achieved with a novel inductive architecture
that embeds scattering features into the network flow using Hybrid Fusion
Blocks. We also demonstrate that the proposed design inherits the key property
of prior hybrid networks -- an effective generalisation in data-limited
scenarios. Our approach successfully combines the best of the two worlds:
flexibility and power of learnt features and stability and predictability of
scattering representations.",https://github.com/dminskiy/EHybridNet-icpr2022,-1
CNNs and Transformers Perceive Hybrid Images Similar to Humans,0.088686,"Hybrid images is a technique to generate images with two interpretations that
change as a function of viewing distance. It has been utilized to study
multiscale processing of images by the human visual system. Using 63,000 hybrid
images across 10 fruit categories, here we show that predictions of deep
learning vision models qualitatively matches with the human perception of these
images. Our results provide yet another evidence in support of the hypothesis
that Convolutional Neural Networks (CNNs) and Transformers are good at modeling
the feedforward sweep of information in the ventral stream of visual cortex.
Code and data is available at https://github.com/aliborji/hybrid_images.git.",https://github.com/aliborji/hybrid_images.git,-1
World of Bugs: A Platform for Automated Bug Detection in 3D Video Games,0.412464,"We present World of Bugs (WOB), an open platform that aims to support
Automated Bug Detection (ABD) research in video games. We discuss some open
problems in ABD and how they relate to the platform's design, arguing that
learning-based solutions are required if further progress is to be made. The
platform's key feature is a growing collection of common video game bugs that
may be used for training and evaluating ABD approaches.",None,-1
Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification,0.518091,"Semi-supervised learning (SSL) is a common approach to learning predictive
models using not only labeled examples, but also unlabeled examples. While SSL
for the simple tasks of classification and regression has received a lot of
attention from the research community, this is not properly investigated for
complex prediction tasks with structurally dependent variables. This is the
case of multi-label classification and hierarchical multi-label classification
tasks, which may require additional information, possibly coming from the
underlying distribution in the descriptive space provided by unlabeled
examples, to better face the challenging task of predicting simultaneously
multiple class labels.
  In this paper, we investigate this aspect and propose a (hierarchical)
multi-label classification method based on semi-supervised learning of
predictive clustering trees. We also extend the method towards ensemble
learning and propose a method based on the random forest approach. Extensive
experimental evaluation conducted on 23 datasets shows significant advantages
of the proposed method and its extension with respect to their supervised
counterparts. Moreover, the method preserves interpretability and reduces the
time complexity of classical tree-based models.",https://github.com/knowledge-technologies/clus,-1
MMKGR: Multi-hop Multi-modal Knowledge Graph Reasoning,0.551736,"Multi-modal knowledge graphs (MKGs) include not only the relation triplets,
but also related multi-modal auxiliary data (i.e., texts and images), which
enhance the diversity of knowledge. However, the natural incompleteness has
significantly hindered the applications of MKGs. To tackle the problem,
existing studies employ the embedding-based reasoning models to infer the
missing knowledge after fusing the multi-modal features. However, the reasoning
performance of these methods is limited due to the following problems: (1)
ineffective fusion of multi-modal auxiliary features; (2) lack of complex
reasoning ability as well as inability to conduct the multi-hop reasoning which
is able to infer more missing knowledge. To overcome these problems, we propose
a novel model entitled MMKGR (Multi-hop Multi-modal Knowledge Graph Reasoning).
Specifically, the model contains the following two components: (1) a unified
gate-attention network which is designed to generate effective multi-modal
complementary features through sufficient attention interaction and noise
reduction; (2) a complementary feature-aware reinforcement learning method
which is proposed to predict missing elements by performing the multi-hop
reasoning process, based on the features obtained in component (1). The
experimental results demonstrate that MMKGR outperforms the state-of-the-art
approaches in the MKG reasoning task.",None,-1
3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows,0.986412,"Text-to-image AI are capable of generating novel images for inspiration, but
their applications for 3D design workflows and how designers can build 3D
models using AI-provided inspiration have not yet been explored. To investigate
this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a
plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users
to construct text and image prompts based on what they are modeling. In a study
with 13 designers, we found that designers saw great potential in 3DALL-E
within their workflows and could use text-to-image AI to produce reference
images, prevent design fixation, and inspire design considerations. We
elaborate on prompting patterns observed across 3D modeling tasks and provide
measures of prompt complexity observed across participants. From our findings,
we discuss how 3DALL-E can merge with existing generative design workflows and
propose prompt bibliographies as a form of human-AI design history.",None,-1
Towards Inter-character Relationship-driven Story Generation,0.731222,"In this paper, we introduce the task of modeling interpersonal relationships
for story generation. For addressing this task, we propose Relationships as
Latent Variables for Story Generation, (ReLiSt). ReLiSt generates stories
sentence by sentence and has two major components - a relationship selector and
a story continuer. The relationship selector specifies a latent variable to
pick the relationship to exhibit in the next sentence and the story continuer
generates the next sentence while expressing the selected relationship in a
coherent way. Our automatic and human evaluations demonstrate that ReLiSt is
able to generate stories with relationships that are more faithful to desired
relationships while maintaining the content quality. The relationship
assignments to sentences during inference bring interpretability to ReLiSt.",https://github.com/dbamman/book-nlp,-1
One-Shot Adaptation of GAN in Just One CLIP,0.843755,"There are many recent research efforts to fine-tune a pre-trained generator
with a few target images to generate images of a novel domain. Unfortunately,
these methods often suffer from overfitting or under-fitting when fine-tuned
with a single target image. To address this, here we present a novel
single-shot GAN adaptation method through unified CLIP space manipulations.
Specifically, our model employs a two-step training strategy: reference image
search in the source generator using a CLIP-guided latent optimization,
followed by generator fine-tuning with a novel loss function that imposes CLIP
space consistency between the source and adapted generators. To further improve
the adapted model to produce spatially consistent samples with respect to the
source generator, we also propose contrastive regularization for patchwise
relationships in the CLIP space. Experimental results show that our model
generates diverse outputs with the target texture and outperforms the baseline
models both qualitatively and quantitatively. Furthermore, we show that our
CLIP space manipulation strategy allows more effective attribute editing.",https://github.com/cyclomon/OneshotCLIP,-1
Graph Reasoning Transformer for Image Parsing,0.614242,"Capturing the long-range dependencies has empirically proven to be effective
on a wide range of computer vision tasks. The progressive advances on this
topic have been made through the employment of the transformer framework with
the help of the multi-head attention mechanism. However, the attention-based
image patch interaction potentially suffers from problems of redundant
interactions of intra-class patches and unoriented interactions of inter-class
patches. In this paper, we propose a novel Graph Reasoning Transformer (GReaT)
for image parsing to enable image patches to interact following a relation
reasoning pattern. Specifically, the linearly embedded image patches are first
projected into the graph space, where each node represents the implicit visual
center for a cluster of image patches and each edge reflects the relation
weight between two adjacent nodes. After that, global relation reasoning is
performed on this graph accordingly. Finally, all nodes including the relation
information are mapped back into the original space for subsequent processes.
Compared to the conventional transformer, GReaT has higher interaction
efficiency and a more purposeful interaction pattern. Experiments are carried
out on the challenging Cityscapes and ADE20K datasets. Results show that GReaT
achieves consistent performance gains with slight computational overheads on
the state-of-the-art transformer baselines.",https://github.com/open-mmlab/mmsegmentation,-1
Box Supervised Video Segmentation Proposal Network,0.123671,"Video Object Segmentation (VOS) has been targeted by various fully-supervised
and self-supervised approaches. While fully-supervised methods demonstrate
excellent results, self-supervised ones, which do not use pixel-level ground
truth, attract much attention. However, self-supervised approaches pose a
significant performance gap. Box-level annotations provide a balanced
compromise between labeling effort and result quality for image segmentation
but have not been exploited for the video domain. In this work, we propose a
box-supervised video object segmentation proposal network, which takes
advantage of intrinsic video properties. Our method incorporates object motion
in the following way: first, motion is computed using a bidirectional temporal
difference and a novel bounding box-guided motion compensation. Second, we
introduce a novel motion-aware affinity loss that encourages the network to
predict positive pixel pairs if they share similar motion and color. The
proposed method outperforms the state-of-the-art self-supervised benchmark by
16.4% and 6.9% $\mathcal{J}$ &$\mathcal{F}$ score and the majority of fully
supervised methods on the DAVIS and Youtube-VOS dataset without imposing
network architectural specifications. We provide extensive tests and ablations
on the datasets, demonstrating the robustness of our method.",https://github.com/Tanveer81/BoxVOS.git,4189
FedGBF: An efficient vertical federated learning framework via gradient boosting and bagging,0.214954,"Federated learning, conducive to solving data privacy and security problems,
has attracted increasing attention recently. However, the existing federated
boosting model sequentially builds a decision tree model with the weak base
learner, resulting in redundant boosting steps and high interactive
communication costs. In contrast, the federated bagging model saves time by
building multi-decision trees in parallel, but it suffers from performance
loss. With the aim of obtaining an outstanding performance with less time cost,
we propose a novel model in a vertically federated setting termed as Federated
Gradient Boosting Forest (FedGBF). FedGBF simultaneously integrates the
boosting and bagging's preponderance by building the decision trees in parallel
as a base learner for boosting. Subsequent to FedGBF, the problem of
hyperparameters tuning is rising. Then we propose the Dynamic FedGBF, which
dynamically changes each forest's parameters and thus reduces the complexity.
Finally, the experiments based on the benchmark datasets demonstrate the
superiority of our method.",None,-1
Extractive Question Answering on Queries in Hindi and Tamil,0.0360113,"Indic languages like Hindi and Tamil are underrepresented in the natural
language processing (NLP) field compared to languages like English. Due to this
underrepresentation, performance on NLP tasks (such as search algorithms) in
Indic languages are inferior to their English counterparts. This difference
disproportionately affects those who come from lower socioeconomic statuses
because they consume the most Internet content in local languages. The goal of
this project is to build an NLP model that performs better than pre-existing
models for the task of extractive question-answering (QA) on a public dataset
in Hindi and Tamil. Extractive QA is an NLP task where answers to questions are
extracted from a corresponding body of text. To build the best solution, we
used three different models. The first model is an unmodified cross-lingual
version of the NLP model RoBERTa, known as XLM-RoBERTa, that is pretrained on
100 languages. The second model is based on the pretrained RoBERTa model with
an extra classification head for the question answering, but we used a custom
Indic tokenizer, then optimized hyperparameters and fine tuned on the Indic
dataset. The third model is based on XLM-RoBERTa, but with extra finetuning and
training on the Indic dataset. We hypothesize the third model will perform best
because of the variety of languages the XLM-RoBERTa model has been pretrained
on and the additional finetuning on the Indic dataset. This hypothesis was
proven wrong because the paired RoBERTa models performed the best as the
training data used was most specific to the task performed as opposed to the
XLM-RoBERTa models which had much data that was not in either Hindi or Tamil.",None,-1
Generating natural images with direct Patch Distributions Matching,0.777994,"Many traditional computer vision algorithms generate realistic images by
requiring that each patch in the generated image be similar to a patch in a
training image and vice versa. Recently, this classical approach has been
replaced by adversarial training with a patch discriminator. The adversarial
approach avoids the computational burden of finding nearest neighbors of
patches but often requires very long training times and may fail to match the
distribution of patches. In this paper we leverage the recently developed
Sliced Wasserstein Distance and develop an algorithm that explicitly and
efficiently minimizes the distance between patch distributions in two images.
Our method is conceptually simple, requires no training and can be implemented
in a few lines of codes. On a number of image generation tasks we show that our
results are often superior to single-image-GANs, require no training, and can
generate high quality images in a few seconds. Our implementation is available
at https://github.com/ariel415el/GPDM",https://github.com/ariel415el/GPDM,-1
Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness,0.867076,"A notable challenge in Multi-Document Summarization (MDS) is the
extremely-long length of the input. In this paper, we present an
extract-then-abstract Transformer framework to overcome the problem.
Specifically, we leverage pre-trained language models to construct a
hierarchical extractor for salient sentence selection across documents and an
abstractor for rewriting the selected contents as summaries. However, learning
such a framework is challenging since the optimal contents for the abstractor
are generally unknown. Previous works typically create pseudo extraction oracle
to enable the supervised learning for both the extractor and the abstractor.
Nevertheless, we argue that the performance of such methods could be restricted
due to the insufficient information for prediction and inconsistent objectives
between training and testing. To this end, we propose a loss weighting
mechanism that makes the model aware of the unequal importance for the
sentences not in the pseudo extraction oracle, and leverage the fine-tuned
abstractor to generate summary references as auxiliary signals for learning the
extractor. Moreover, we propose a reinforcement learning method that can
efficiently apply to the extractor for harmonizing the optimization between
training and testing. Experiment results show that our framework substantially
outperforms strong baselines with comparable model sizes and achieves the best
results on the Multi-News, Multi-XScience, and WikiCatSum corpora.",None,-1
Multi-Agent Chance-Constrained Stochastic Shortest Path with Application to Risk-Aware Intelligent Intersection,0.688597,"In transportation networks, where traffic lights have traditionally been used
for vehicle coordination, intersections act as natural bottlenecks. A
formidable challenge for existing automated intersections lies in detecting and
reasoning about uncertainty from the operating environment and human-driven
vehicles. In this paper, we propose a risk-aware intelligent intersection
system for autonomous vehicles (AVs) as well as human-driven vehicles (HVs). We
cast the problem as a novel class of Multi-agent Chance-Constrained Stochastic
Shortest Path (MCC-SSP) problems and devise an exact Integer Linear Programming
(ILP) formulation that is scalable in the number of agents' interaction points
(e.g., potential collision points at the intersection). In particular, when the
number of agents within an interaction point is small, which is often the case
in intersections, the ILP has a polynomial number of variables and constraints.
To further improve the running time performance, we show that the collision
risk computation can be performed offline. Additionally, a trajectory
optimization workflow is provided to generate risk-aware trajectories for any
given intersection. The proposed framework is implemented in CARLA simulator
and evaluated under a fully autonomous intersection with AVs only as well as in
a hybrid setup with a signalized intersection for HVs and an intelligent scheme
for AVs. As verified via simulations, the featured approach improves
intersection's efficiency by up to $200\%$ while also conforming to the
specified tunable risk threshold.",None,-1
Contrastive Language-Image Pre-Training with Knowledge Graphs,0.584548,"Recent years have witnessed the fast development of large-scale pre-training
frameworks that can extract multi-modal representations in a unified form and
achieve promising performances when transferred to downstream tasks.
Nevertheless, existing approaches mainly focus on pre-training with simple
image-text pairs, while neglecting the semantic connections between concepts
from different modalities. In this paper, we propose a knowledge-based
pre-training framework, dubbed Knowledge-CLIP, which injects semantic
information into the widely used CLIP model. Through introducing
knowledge-based objectives in the pre-training process and utilizing different
types of knowledge graphs as training data, our model can semantically align
the representations in vision and language with higher quality, and enhance the
reasoning ability across scenarios and modalities. Extensive experiments on
various vision-language downstream tasks demonstrate the effectiveness of
Knowledge-CLIP compared with the original CLIP and competitive baselines.",None,-1
Algorithms for Weighted Pushdown Automata,0.872058,"Weighted pushdown automata (WPDAs) are at the core of many natural language
processing tasks, like syntax-based statistical machine translation and
transition-based dependency parsing. As most existing dynamic programming
algorithms are designed for context-free grammars (CFGs), algorithms for PDAs
often resort to a PDA-to-CFG conversion. In this paper, we develop novel
algorithms that operate directly on WPDAs. Our algorithms are inspired by
Lang's algorithm, but use a more general definition of pushdown automaton and
either reduce the space requirements by a factor of $|\Gamma|$ (the size of the
stack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the
number of states). When run on the same class of PDAs as Lang's algorithm, our
algorithm is both more space-efficient by a factor of $|\Gamma|$ and more
time-efficient by a factor of $|Q| \cdot |\Gamma|$.",https://github.com/rycolab/wpda,-1
Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues,0.447327,"Movies reflect society and also hold power to transform opinions. Social
biases and stereotypes present in movies can cause extensive damage due to
their reach. These biases are not always found to be the need of storyline but
can creep in as the author's bias. Movie production houses would prefer to
ascertain that the bias present in a script is the story's demand. Today, when
deep learning models can give human-level accuracy in multiple tasks, having an
AI solution to identify the biases present in the script at the writing stage
can help them avoid the inconvenience of stalled release, lawsuits, etc. Since
AI solutions are data intensive and there exists no domain specific data to
address the problem of biases in scripts, we introduce a new dataset of movie
scripts that are annotated for identity bias. The dataset contains dialogue
turns annotated for (i) bias labels for seven categories, viz., gender,
race/ethnicity, religion, age, occupation, LGBTQ, and other, which contains
biases like body shaming, personality bias, etc. (ii) labels for sensitivity,
stereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated
with context awareness, (iv) target groups and reason for bias labels and (v)
expert-driven group-validation process for high quality annotations. We also
report various baseline performances for bias identification and category
detection on our dataset.",https://github.com/sahoonihar/HIBD_LREC_2022,-1
Domain-Augmented Domain Adaptation,0.0859292,"Unsupervised domain adaptation (UDA) enables knowledge transfer from the
labelled source domain to the unlabeled target domain by reducing the
cross-domain discrepancy. However, most of the studies were based on direct
adaptation from the source domain to the target domain and have suffered from
large domain discrepancies. To overcome this challenge, in this paper, we
propose the domain-augmented domain adaptation (DADA) to generate pseudo
domains that have smaller discrepancies with the target domain, to enhance the
knowledge transfer process by minimizing the discrepancy between the target
domain and pseudo domains. Furthermore, we design a pseudo-labeling method for
DADA by projecting representations from the target domain to multiple pseudo
domains and taking the averaged predictions on the classification from the
pseudo domains as the pseudo labels. We conduct extensive experiments with the
state-of-the-art domain adaptation methods on four benchmark datasets: Office
Home, Office-31, VisDA2017, and Digital datasets. The results demonstrate the
superiority of our model.",None,-1
Neural-Symbolic Entangled Framework for Complex Query Answering,0.835968,"Answering complex queries over knowledge graphs (KG) is an important yet
challenging task because of the KG incompleteness issue and cascading errors
during reasoning. Recent query embedding (QE) approaches to embed the entities
and relations in a KG and the first-order logic (FOL) queries into a low
dimensional space, answering queries by dense similarity search. However,
previous works mainly concentrate on the target answers, ignoring intermediate
entities' usefulness, which is essential for relieving the cascading error
problem in logical query answering. In addition, these methods are usually
designed with their own geometric or distributional embeddings to handle
logical operators like union, intersection, and negation, with the sacrifice of
the accuracy of the basic operator - projection, and they could not absorb
other embedding methods to their models. In this work, we propose a Neural and
Symbolic Entangled framework (ENeSy) for complex query answering, which enables
the neural and symbolic reasoning to enhance each other to alleviate the
cascading error and KG incompleteness. The projection operator in ENeSy could
be any embedding method with the capability of link prediction, and the other
FOL operators are handled without parameters. With both neural and symbolic
reasoning results contained, ENeSy answers queries in ensembles. ENeSy achieves
the SOTA performance on several benchmarks, especially in the setting of the
training model only with the link prediction task.",None,-1
Weakly Supervised 3D Point Cloud Segmentation via Multi-Prototype Learning,0.519184,"Addressing the annotation challenge in 3D Point Cloud segmentation has
inspired research into weakly supervised learning. Existing approaches mainly
focus on exploiting manifold and pseudo-labeling to make use of large unlabeled
data points. A fundamental challenge here lies in the large intra-class
variations of local geometric structure, resulting in subclasses within a
semantic class. In this work, we leverage this intuition and opt for
maintaining an individual classifier for each subclass. Technically, we design
a multi-prototype classifier, each prototype serves as the classifier weights
for one subclass. To enable effective updating of multi-prototype classifier
weights, we propose two constraints respectively for updating the prototypes
w.r.t. all point features and for encouraging the learning of diverse
prototypes. Experiments on weakly supervised 3D point cloud segmentation tasks
validate the efficacy of proposed method in particular at low-label regime. Our
hypothesis is also verified given the consistent discovery of semantic
subclasses at no cost of additional annotations.",None,-1
MonoNeuralFusion: Online Monocular Neural 3D Reconstruction with Geometric Priors,0.332917,"High-fidelity 3D scene reconstruction from monocular videos continues to be
challenging, especially for complete and fine-grained geometry reconstruction.
The previous 3D reconstruction approaches with neural implicit representations
have shown a promising ability for complete scene reconstruction, while their
results are often over-smooth and lack enough geometric details. This paper
introduces a novel neural implicit scene representation with volume rendering
for high-fidelity online 3D scene reconstruction from monocular videos. For
fine-grained reconstruction, our key insight is to incorporate geometric priors
into both the neural implicit scene representation and neural volume rendering,
thus leading to an effective geometry learning mechanism based on volume
rendering optimization. Benefiting from this, we present MonoNeuralFusion to
perform the online neural 3D reconstruction from monocular videos, by which the
3D scene geometry is efficiently generated and optimized during the on-the-fly
3D monocular scanning. The extensive comparisons with state-of-the-art
approaches show that our MonoNeuralFusion consistently generates much better
complete and fine-grained reconstruction results, both quantitatively and
qualitatively.",https://github.com/AljazBozic/TransformerFusion,-1
Modeling and Validating Temporal Rules with Semantic Petri-Net for Digital Twins,0.239562,"Semantic rule checking on RDFS/OWL data has been widely used in the
construction industry. At present, semantic rule checking is mainly performed
on static models. There are still challenges in integrating temporal models and
semantic models for combined rule checking. In this paper, Semantic Petri-Net
(SPN) is proposed as a novel temporal modeling and validating method, which
implements the states and transitions of the Colored Petri-Net directly based
on RDFS and SPARQL, and realizes two-way sharing of knowledge between domain
semantic webs and temporal models in the runtime. Several cases are provided to
demonstrate the possible applications in digital twins with concurrent state
changes and dependencies.",None,-1
Can Model Compression Improve NLP Fairness,0.656412,"Model compression techniques are receiving increasing attention; however, the
effect of compression on model fairness is still under explored. This is the
first paper to examine the effect of distillation and pruning on the toxicity
and bias of generative language models. We test Knowledge Distillation and
Pruning methods on the GPT2 model and found a consistent pattern of toxicity
and bias reduction after model distillation; this result can be potentially
interpreted by existing line of research which describes model compression as a
regularization technique; our work not only serves as a reference for safe
deployment of compressed models, but also extends the discussion of
""compression as regularization"" into the setting of neural LMs, and hints at
the possibility of using compression to develop fairer models.",https://github.com/unitaryai/detoxify,-1
Measuring Inconsistency in Declarative Process Specifications,0.342388,"We address the problem of measuring inconsistency in declarative process
specifications, with an emphasis on linear temporal logic on fixed traces
(LTLff). As we will show, existing inconsistency measures for classical logic
cannot provide a meaningful assessment of inconsistency in LTL in general, as
they cannot adequately handle the temporal operators. We therefore propose a
novel paraconsistent semantics as a framework for inconsistency measurement. We
then present two new inconsistency measures based on these semantics and show
that they satisfy important desirable properties. We show how these measures
can be applied to declarative process models and investigate the computational
complexity of the introduced approach.",None,-1
MMGL: Multi-Scale Multi-View Global-Local Contrastive learning for Semi-supervised Cardiac Image Segmentation,0.509484,"With large-scale well-labeled datasets, deep learning has shown significant
success in medical image segmentation. However, it is challenging to acquire
abundant annotations in clinical practice due to extensive expertise
requirements and costly labeling efforts. Recently, contrastive learning has
shown a strong capacity for visual representation learning on unlabeled data,
achieving impressive performance rivaling supervised learning in many domains.
In this work, we propose a novel multi-scale multi-view global-local
contrastive learning (MMGL) framework to thoroughly explore global and local
features from different scales and views for robust contrastive learning
performance, thereby improving segmentation performance with limited
annotations. Extensive experiments on the MM-WHS dataset demonstrate the
effectiveness of MMGL framework on semi-supervised cardiac image segmentation,
outperforming the state-of-the-art contrastive learning methods by a large
margin.",None,-1
Learning to Decompose Visual Features with Latent Textual Prompts,0.783576,"Recent advances in pre-training vision-language models like CLIP have shown
great potential in learning transferable visual representations. Nonetheless,
for downstream inference, CLIP-like models suffer from either 1) degraded
accuracy and robustness in the case of inaccurate text descriptions during
retrieval-based inference (the challenge for zero-shot protocol); or 2)
breaking the well-established vision-language alignment (the challenge for
linear probing). To address them, we propose Decomposed Feature Prompting
(DeFo). DeFo leverages a flexible number of learnable embeddings as textual
input while maintaining the vision-language dual-model architecture, which
enables the model to learn decomposed visual features with the help of
feature-level textual prompts. We further use an additional linear layer to
perform classification, allowing a scalable size of language inputs. Our
empirical study shows DeFo's significance in improving the vision-language
models. For example, DeFo obtains 73.2% test accuracy on ImageNet with a
ResNet-50 backbone without tuning any pretrained weights of both the vision and
language encoder, outperforming zero-shot CLIP by a large margin of 15.0%, and
outperforming state-of-the-art vision-language prompt tuning method by 7.6%.",None,-1
Where did you tweet from? Inferring the origin locations of tweets based on contextual information,0.617902,"Public conversations on Twitter comprise many pertinent topics including
disasters, protests, politics, propaganda, sports, climate change,
epidemics/pandemic outbreaks, etc., that can have both regional and global
aspects. Spatial discourse analysis rely on geographical data. However, today
less than 1% of tweets are geotagged; in both cases--point location or bounding
place information. A major issue with tweets is that Twitter users can be at
location A and exchange conversations specific to location B, which we call the
Location A/B problem. The problem is considered solved if location entities can
be classified as either origin locations (Location As) or non-origin locations
(Location Bs). In this work, we propose a simple yet effective framework--the
True Origin Model--to address the problem that uses machine-level natural
language understanding to identify tweets that conceivably contain their origin
location information. The model achieves promising accuracy at country (80%),
state (67%), city (58%), county (56%) and district (64%) levels with support
from a Location Extraction Model as basic as the CoNLL-2003-based RoBERTa. We
employ a tweet contexualizer (locBERT) which is one of the core components of
the proposed model, to investigate multiple tweets' distributions for
understanding Twitter users' tweeting behavior in terms of mentioning origin
and non-origin locations. We also highlight a major concern with the currently
regarded gold standard test set (ground truth) methodology, introduce a new
data set, and identify further research avenues for advancing the area.",None,-1
Motron: Multimodal Probabilistic Human Motion Forecasting,0.693447,"Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic
maximum-likelihood motions and corresponding confidence values for each mode.
Our model aims to be tightly integrated with the robotic
planning-control-interaction loop; outputting physically feasible human motions
and being computationally efficient. We demonstrate the performance of our
model on several challenging real-world motion forecasting datasets,
outperforming a wide array of generative/variational methods while providing
state-of-the-art single-output motions if required. Both using significantly
less computational power than state-of-the art algorithms.",https://github.com/TUM-AAS/motron-cvpr22,-1
Describing Differences between Text Distributions with Natural Language,0.801211,"How do two distributions of texts differ? Humans are slow at answering this,
since discovering patterns might require tediously reading through hundreds of
samples. We propose to automatically summarize the differences by ""learning a
natural language hypothesis"": given two distributions $D_{0}$ and $D_{1}$, we
search for a description that is more often true for $D_{1}$, e.g., ""is
military-related."" To tackle this problem, we fine-tune GPT-3 to propose
descriptions with the prompt: ""[samples of $D_{0}$] + [samples of $D_{1}$] +
the difference between them is_____."" We then re-rank the descriptions by
checking how often they hold on a larger set of samples with a learned
verifier. On a benchmark of 54 real-world binary classification tasks, while
GPT-3 Curie (13B) only generates a description similar to human annotation 7%
of the time, the performance reaches 61% with fine-tuning and re-ranking, and
our best system using GPT-3 Davinci (175B) reaches 76%. We apply our system to
describe distribution shifts, debug dataset shortcuts, summarize unknown tasks,
and label text clusters, and present analyses based on automatically generated
descriptions.",None,-1
Optimizing Fiducial Marker Placement for Improved Visual Localization,0.308416,"Adding fiducial markers to a scene is a well-known strategy for making visual
localization algorithms more robust. Traditionally, these marker locations are
selected by humans who are familiar with visual localization techniques. This
paper explores the problem of automatic marker placement within a scene.
Specifically, given a predetermined set of markers and a scene model, we
compute optimized marker positions within the scene that can improve accuracy
in visual localization. Our main contribution is a novel framework for modeling
camera localizability that incorporates both natural scene features and
artificial fiducial markers added to the scene. We present optimized marker
placement (OMP), a greedy algorithm that is based on the camera localizability
framework. We have also designed a simulation framework for testing marker
placement algorithms on 3D models and images generated from synthetic scenes.
We have evaluated OMP within this testbed and demonstrate an improvement in the
localization rate by up to 20 percent on four different scenes.",https://github.com/doublestrong/OMP,-1
Bridging the Gap between Artificial Intelligence and Artificial General Intelligence: A Ten Commandment Framework for Human-Like Intelligence,0.141859,"The field of artificial intelligence has seen explosive growth and
exponential success. The last phase of development showcased deep learnings
ability to solve a variety of difficult problems across a multitude of domains.
Many of these networks met and exceeded human benchmarks by becoming experts in
the domains in which they are trained. Though the successes of artificial
intelligence have begun to overshadow its failures, there is still much that
separates current artificial intelligence tools from becoming the exceptional
general learners that humans are. In this paper, we identify the ten
commandments upon which human intelligence is systematically and hierarchically
built. We believe these commandments work collectively to serve as the
essential ingredients that lead to the emergence of higher-order cognition and
intelligence. This paper discusses a computational framework that could house
these ten commandments and suggests new architectural modifications that could
lead to the development of smarter, more explainable, and generalizable
artificial systems inspired by a neuromorphic approach.",None,-1
MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors,0.928169,"In this paper, we propose MOTRv2, a simple yet effective pipeline to
bootstrap end-to-end multi-object tracking with a pretrained object detector.
Existing end-to-end methods, MOTR and TrackFormer are inferior to their
tracking-by-detection counterparts mainly due to their poor detection
performance. We aim to improve MOTR by elegantly incorporating an extra object
detector. We first adopt the anchor formulation of queries and then use an
extra object detector to generate proposals as anchors, providing detection
prior to MOTR. The simple modification greatly eases the conflict between joint
learning detection and association tasks in MOTR. MOTRv2 keeps the query
propogation feature and scales well on large-scale benchmarks. MOTRv2 ranks the
1st place (73.4% HOTA on DanceTrack) in the 1st Multiple People Tracking in
Group Dance Challenge. Moreover, MOTRv2 reaches state-of-the-art performance on
the BDD100K dataset. We hope this simple and effective pipeline can provide
some new insights to the end-to-end MOT community. Code is available at
\url{https://github.com/megvii-research/MOTRv2}.",https://github.com/megvii-research/MOTRv2,2415
Watermarking Pre-trained Language Models with Backdooring,0.493193,"Large pre-trained language models (PLMs) have proven to be a crucial
component of modern natural language processing systems. PLMs typically need to
be fine-tuned on task-specific downstream datasets, which makes it hard to
claim the ownership of PLMs and protect the developer's intellectual property
due to the catastrophic forgetting phenomenon. We show that PLMs can be
watermarked with a multi-task learning framework by embedding backdoors
triggered by specific inputs defined by the owners, and those watermarks are
hard to remove even though the watermarked PLMs are fine-tuned on multiple
downstream tasks. In addition to using some rare words as triggers, we also
show that the combination of common words can be used as backdoor triggers to
avoid them being easily detected. Extensive experiments on multiple datasets
demonstrate that the embedded watermarks can be robustly extracted with a high
success rate and less influenced by the follow-up fine-tuning.",None,-1
Comprehensive Benchmark Datasets for Amharic Scene Text Detection and Recognition,0.0608319,"Ethiopic/Amharic script is one of the oldest African writing systems, which
serves at least 23 languages (e.g., Amharic, Tigrinya) in East Africa for more
than 120 million people. The Amharic writing system, Abugida, has 282
syllables, 15 punctuation marks, and 20 numerals. The Amharic syllabic matrix
is derived from 34 base graphemes/consonants by adding up to 12 appropriate
diacritics or vocalic markers to the characters. The syllables with a common
consonant or vocalic markers are likely to be visually similar and challenge
text recognition tasks. In this work, we presented the first comprehensive
public datasets named HUST-ART, HUST-AST, ABE, and Tana for Amharic script
detection and recognition in the natural scene. We have also conducted
extensive experiments to evaluate the performance of the state of art methods
in detecting and recognizing Amharic scene text on our datasets. The evaluation
results demonstrate the robustness of our datasets for benchmarking and its
potential of promoting the development of robust Amharic script detection and
recognition algorithms. Consequently, the outcome will benefit people in East
Africa, including diplomats from several countries and international
communities.",None,-1
Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering,0.664556,"Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a
two-stage framework that first retrieves external knowledge given the visual
question and then predicts the answer based on the retrieved content. However,
the retrieved knowledge is often inadequate. Retrievals are frequently too
general and fail to cover specific knowledge needed to answer the question.
Also, the naturally available supervision (whether the passage contains the
correct answer) is weak and does not guarantee question relevancy. To address
these issues, we propose an Entity-Focused Retrieval (EnFoRe) model that
provides stronger supervision during training and recognizes question-relevant
entities to help retrieve more specific knowledge. Experiments show that our
EnFoRe model achieves superior retrieval performance on OK-VQA, the currently
largest outside-knowledge VQA dataset. We also combine the retrieved knowledge
with state-of-the-art VQA models, and achieve a new state-of-the-art
performance on OK-VQA.",https://github.com/jialinwu17/EnFoRe.git,-1
Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?,0.327602,"The use of Deep Learning and Computer Vision in the Cultural Heritage domain
is becoming highly relevant in the last few years with lots of applications
about audio smart guides, interactive museums and augmented reality. All these
technologies require lots of data to work effectively and be useful for the
user. In the context of artworks, such data is annotated by experts in an
expensive and time consuming process. In particular, for each artwork, an image
of the artwork and a description sheet have to be collected in order to perform
common tasks like Visual Question Answering. In this paper we propose a method
for Visual Question Answering that allows to generate at runtime a description
sheet that can be used for answering both visual and contextual questions about
the artwork, avoiding completely the image and the annotation process. For this
purpose, we investigate on the use of GPT-3 for generating descriptions for
artworks analyzing the quality of generated descriptions through captioning
metrics. Finally we evaluate the performance for Visual Question Answering and
captioning tasks.",None,656
Sequential Causal Imitation Learning with Unobserved Confounders,0.903047,"""Monkey see monkey do"" is an age-old adage, referring to na\""ive imitation
without a deep understanding of a system's underlying mechanics. Indeed, if a
demonstrator has access to information unavailable to the imitator (monkey),
such as a different set of sensors, then no matter how perfectly the imitator
models its perceived environment (See), attempting to reproduce the
demonstrator's behavior (Do) can lead to poor outcomes. Imitation learning in
the presence of a mismatch between demonstrator and imitator has been studied
in the literature under the rubric of causal imitation learning (Zhang et al.,
2020), but existing solutions are limited to single-stage decision-making. This
paper investigates the problem of causal imitation learning in sequential
settings, where the imitator must make multiple decisions per episode. We
develop a graphical criterion that is necessary and sufficient for determining
the feasibility of causal imitation, providing conditions when an imitator can
match a demonstrator's performance despite differing capabilities. Finally, we
provide an efficient algorithm for determining imitability and corroborate our
theory with simulations.",None,-1
Vector Quantized Semantic Communication System,0.343774,"Although analog semantic communication systems have received considerable
attention in the literature, there is less work on digital semantic
communication systems. In this paper, we develop a deep learning (DL)-enabled
vector quantized (VQ) semantic communication system for image transmission,
named VQ-DeepSC. Specifically, we propose a convolutional neural network
(CNN)-based transceiver to extract multi-scale semantic features of images and
introduce multi-scale semantic embedding spaces to perform semantic feature
quantization, rendering the data compatible with digital communication systems.
Furthermore, we employ adversarial training to improve the quality of received
images by introducing a PatchGAN discriminator. Experimental results
demonstrate that the proposed VQ-DeepSC is more robustness than BPG in digital
communication systems and has comparable MS-SSIM performance to the DeepJSCC
method.",None,10261
Less is More: Task-aware Layer-wise Distillation for Language Model Compression,0.675151,"Layer-wise distillation is a powerful tool to compress large models (i.e.
teacher models) into small ones (i.e., student models). The student distills
knowledge from the teacher by mimicking the hidden representations of the
teacher at every intermediate layer. However, layer-wise distillation is
difficult. Since the student has a smaller model capacity than the teacher, it
is often under-fitted. Furthermore, the hidden representations of the teacher
contain redundant information that the student does not necessarily need for
the target task's learning. To address these challenges, we propose a novel
Task-aware layEr-wise Distillation (TED). TED designs task-aware filters to
align the hidden representations of the student and the teacher at each layer.
The filters select the knowledge that is useful for the target task from the
hidden representations. As such, TED reduces the knowledge gap between the two
models and helps the student to fit better on the target task. We evaluate TED
in two scenarios: continual pre-training and fine-tuning. TED demonstrates
significant and consistent improvements over existing distillation methods in
both scenarios. Code is available at
https://github.com/cliang1453/task-aware-distillation.",https://github.com/cliang1453/task-aware-distillation,-1
AdaPrompt: Adaptive Model Training for Prompt-based NLP,0.598129,"Prompt-based learning, with its capability to tackle zero-shot and few-shot
NLP tasks, has gained much attention in community. The main idea is to bridge
the gap between NLP downstream tasks and language modeling (LM), by mapping
these tasks into natural language prompts, which are then filled by pre-trained
language models (PLMs). However, for prompt learning, there are still two
salient gaps between NLP tasks and pretraining. First, prompt information is
not necessarily sufficiently present during LM pretraining. Second,
task-specific data are not necessarily well represented during pretraining. We
address these two issues by proposing AdaPrompt, adaptively retrieving external
data for continual pretraining of PLMs by making use of both task and prompt
characteristics. In addition, we make use of knowledge in Natural Language
Inference models for deriving adaptive verbalizers. Experimental results on
five NLP benchmarks show that AdaPrompt can improve over standard PLMs in
few-shot settings. In addition, in zero-shot settings, our method outperforms
standard prompt-based methods by up to 26.35\% relative error reduction.",https://github.com/cylnlp/AdaPrompt,-1
Learning Reduced Nonlinear State-Space Models: an Output-Error Based Canonical Approach,0.0524384,"The identification of a nonlinear dynamic model is an open topic in control
theory, especially from sparse input-output measurements. A fundamental
challenge of this problem is that very few to zero prior knowledge is available
on both the state and the nonlinear system model. To cope with this challenge,
we investigate the effectiveness of deep learning in the modeling of dynamic
systems with nonlinear behavior by advocating an approach which relies on three
main ingredients: (i) we show that under some structural conditions on the
to-be-identified model, the state can be expressed in function of a sequence of
the past inputs and outputs; (ii) this relation which we call the state map can
be modelled by resorting to the well-documented approximation power of deep
neural networks; (iii) taking then advantage of existing learning schemes, a
state-space model can be finally identified. After the formulation and analysis
of the approach, we show its ability to identify three different nonlinear
systems. The performances are evaluated in terms of open-loop prediction on
test data generated in simulation as well as a real world data-set of unmanned
aerial vehicle flight measurements.",None,8251
PartAL: Efficient Partial Active Learning in Multi-Task Visual Settings,0.112365,"Multi-task learning is central to many real-world applications.
Unfortunately, obtaining labelled data for all tasks is time-consuming,
challenging, and expensive. Active Learning (AL) can be used to reduce this
burden. Existing techniques typically involve picking images to be annotated
and providing annotations for all tasks.
  In this paper, we show that it is more effective to select not only the
images to be annotated but also a subset of tasks for which to provide
annotations at each AL iteration. Furthermore, the annotations that are
provided can be used to guess pseudo-labels for the tasks that remain
unannotated. We demonstrate the effectiveness of our approach on several
popular multi-task datasets.",None,-1
"For Learning in Symmetric Teams, Local Optima are Global Nash Equilibria",0.418476,"Although it has been known since the 1970s that a globally optimal strategy
profile in a common-payoff game is a Nash equilibrium, global optimality is a
strict requirement that limits the result's applicability. In this work, we
show that any locally optimal symmetric strategy profile is also a (global)
Nash equilibrium. Furthermore, we show that this result is robust to
perturbations to the common payoff and to the local optimum. Applied to machine
learning, our result provides a global guarantee for any gradient method that
finds a local optimum in symmetric strategy space. While this result indicates
stability to unilateral deviation, we nevertheless identify broad classes of
games where mixed local optima are unstable under joint, asymmetric deviations.
We analyze the prevalence of instability by running learning algorithms in a
suite of symmetric games, and we conclude by discussing the applicability of
our results to multi-agent RL, cooperative inverse RL, and decentralized
POMDPs.",https://github.com/scottemmons/coordination,117510
I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning,0.680076,"Knowledge graph (KG) embedding seeks to learn vector representations for
entities and relations. Conventional models reason over graph structures, but
they suffer from the issues of graph incompleteness and long-tail entities.
Recent studies have used pre-trained language models to learn embeddings based
on the textual information of entities and relations, but they cannot take
advantage of graph structures. In the paper, we show empirically that these two
kinds of features are complementary for KG embedding. To this end, we propose
CoLE, a Co-distillation Learning method for KG Embedding that exploits the
complementarity of graph structures and text information. Its graph embedding
model employs Transformer to reconstruct the representation of an entity from
its neighborhood subgraph. Its text embedding model uses a pre-trained language
model to generate entity representations from the soft prompts of their names,
descriptions, and relational neighbors. To let the two model promote each
other, we propose co-distillation learning that allows them to distill
selective knowledge from each other's prediction logits. In our co-distillation
learning, each model serves as both a teacher and a student. Experiments on
benchmark datasets demonstrate that the two models outperform their related
baselines, and the ensemble method CoLE with co-distillation learning advances
the state-of-the-art of KG embedding.",https://github.com/nju-websoft/CoLE,11617
Strong Heuristics for Named Entity Linking,0.153178,"Named entity linking (NEL) in news is a challenging endeavour due to the
frequency of unseen and emerging entities, which necessitates the use of
unsupervised or zero-shot methods. However, such methods tend to come with
caveats, such as no integration of suitable knowledge bases (like Wikidata) for
emerging entities, a lack of scalability, and poor interpretability. Here, we
consider person disambiguation in Quotebank, a massive corpus of
speaker-attributed quotations from the news, and investigate the suitability of
intuitive, lightweight, and scalable heuristics for NEL in web-scale corpora.
Our best performing heuristic disambiguates 94% and 63% of the mentions on
Quotebank and the AIDA-CoNLL benchmark, respectively. Additionally, the
proposed heuristics compare favourably to the state-of-the-art unsupervised and
zero-shot methods, Eigenthemes and mGENRE, respectively, thereby serving as
strong baselines for unsupervised and zero-shot entity linking.",https://github.com/epfl-dlab/nelight,-1
ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation,0.499983,"We study the text generation task under the approach of pre-trained language
models (PLMs). Typically, an auto-regressive (AR) method is adopted for
generating texts in a token-by-token manner. Despite many advantages of AR
generation, it usually suffers from inefficient inference. Therefore,
non-autoregressive (NAR) models are proposed to generate all target tokens
simultaneously. However, NAR models usually generate texts of lower quality due
to the absence of token dependency in the output text. In this paper, we
propose ELMER: an efficient and effective PLM for NAR text generation to
explicitly model the token dependency during NAR generation. By leveraging the
early exit technique, ELMER enables the token generations at different layers,
according to their prediction confidence (a more confident token will exit at a
lower layer). Besides, we propose a novel pre-training objective, Layer
Permutation Language Modeling, to pre-train ELMER by permuting the exit layer
for each token in sequences. Experiments on three text generation tasks show
that ELMER significantly outperforms NAR models and further narrows the
performance gap with AR PLMs (\eg ELMER (29.92) vs BART (30.61) ROUGE-L in
XSUM) while achieving over 10 times inference speedup.",https://github.com/RUCAIBox/ELMER,-1
Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge,0.864665,"Cognitively plausible visual dialogue models should keep a mental scoreboard
of shared established facts in the dialogue context. We propose a theory-based
evaluation method for investigating to what degree models pretrained on the
VisDial dataset incrementally build representations that appropriately do
scorekeeping. Our conclusion is that the ability to make the distinction
between shared and privately known statements along the dialogue is moderately
present in the analysed models, but not always incrementally consistent, which
may partially be due to the limited need for grounding interactions in the
original task.",https://github.com/vmurahari3/visdial-diversity,-1
BayesFormer: Transformer with Uncertainty Estimation,0.426247,"Transformer has become ubiquitous due to its dominant performance in various
NLP and image processing tasks. However, it lacks understanding of how to
generate mathematically grounded uncertainty estimates for transformer
architectures. Models equipped with such uncertainty estimates can typically
improve predictive performance, make networks robust, avoid over-fitting and
used as acquisition function in active learning. In this paper, we introduce
BayesFormer, a Transformer model with dropouts designed by Bayesian theory. We
proposed a new theoretical framework to extend the approximate variational
inference-based dropout to Transformer-based architectures. Through extensive
experiments, we validate the proposed architecture in four paradigms and show
improvements across the board: language modeling and classification,
long-sequence understanding, machine translation and acquisition function for
active learning.",None,-1
Task-Adaptive Feature Transformer with Semantic Enrichment for Few-Shot Segmentation,0.0913727,"Few-shot learning allows machines to classify novel classes using only a few
labeled samples. Recently, few-shot segmentation aiming at semantic
segmentation on low sample data has also seen great interest. In this paper, we
propose a learnable module that can be placed on top of existing segmentation
networks for performing few-shot segmentation. This module, called the
task-adaptive feature transformer (TAFT), linearly transforms task-specific
high-level features to a set of task agnostic features well-suited to
conducting few-shot segmentation. The task-conditioned feature transformation
allows an effective utilization of the semantic information in novel classes to
generate tight segmentation masks. We also propose a semantic enrichment (SE)
module that utilizes a pixel-wise attention module for high-level feature and
an auxiliary loss from an auxiliary segmentation network conducting the
semantic segmentation for all training classes. Experiments on PASCAL-$5^i$ and
COCO-$20^i$ datasets confirm that the added modules successfully extend the
capability of existing segmentators to yield highly competitive few-shot
segmentation performances.",None,5113
Abstraction-Refinement for Hierarchical Probabilistic Models,0.345516,"Markov decision processes are a ubiquitous formalism for modelling systems
with non-deterministic and probabilistic behavior. Verification of these models
is subject to the famous state space explosion problem. We alleviate this
problem by exploiting a hierarchical structure with repetitive parts. This
structure not only occurs naturally in robotics, but also in probabilistic
programs describing, e.g., network protocols. Such programs often repeatedly
call a subroutine with similar behavior. In this paper, we focus on a local
case, in which the subroutines have a limited effect on the overall system
state. The key ideas to accelerate analysis of such programs are (1) to treat
the behavior of the subroutine as uncertain and only remove this uncertainty by
a detailed analysis if needed, and (2) to abstract similar subroutines into a
parametric template, and then analyse this template. These two ideas are
embedded into an abstraction-refinement loop that analyses hierarchical MDPs. A
prototypical implementation shows the efficacy of the approach.",None,-1
HSGNet: Object Re-identification with Hierarchical Similarity Graph Network,0.562526,"Object re-identification method is made up of backbone network, feature
aggregation, and loss function. However, most backbone networks lack a special
mechanism to handle rich scale variations and mine discriminative feature
representations. In this paper, we firstly design a hierarchical similarity
graph module (HSGM) to reduce the conflict of backbone and re-identification
networks. The designed HSGM builds a rich hierarchical graph to mine the
mapping relationships between global-local and local-local. Secondly, we divide
the feature map along with the spatial and channel directions in each
hierarchical graph. The HSGM applies the spatial features and channel features
extracted from different locations as nodes, respectively, and utilizes the
similarity scores between nodes to construct spatial and channel similarity
graphs. During the learning process of HSGM, we utilize a learnable parameter
to re-optimize the importance of each position, as well as evaluate the
correlation between different nodes. Thirdly, we develop a novel hierarchical
similarity graph network (HSGNet) by embedding the HSGM in the backbone
network. Furthermore, HSGM can be easily embedded into backbone networks of any
depth to improve object re-identification ability. Finally, extensive
experiments on three large-scale object datasets demonstrate that the proposed
HSGNet is superior to state-of-the-art object re-identification approaches.",None,-1
A Hazard Analysis Framework for Code Synthesis Large Language Models,0.907124,"Codex, a large language model (LLM) trained on a variety of codebases,
exceeds the previous state of the art in its capacity to synthesize and
generate code. Although Codex provides a plethora of benefits, models that may
generate code on such scale have significant limitations, alignment problems,
the potential to be misused, and the possibility to increase the rate of
progress in technical fields that may themselves have destabilizing impacts or
have misuse potential. Yet such safety impacts are not yet known or remain to
be explored. In this paper, we outline a hazard analysis framework constructed
at OpenAI to uncover hazards or safety risks that the deployment of models like
Codex may impose technically, socially, politically, and economically. The
analysis is informed by a novel evaluation framework that determines the
capacity of advanced code generation techniques against the complexity and
expressivity of specification prompts, and their capability to understand and
execute them relative to human ability.",None,-1
"Homomorphisms Between Transfer, Multi-Task, and Meta-Learning Systems",0.889802,"Transfer learning, multi-task learning, and meta-learning are well-studied
topics concerned with the generalization of knowledge across learning tasks and
are closely related to general intelligence. But, the formal, general systems
differences between them are underexplored in the literature. This lack of
systems-level formalism leads to difficulties in coordinating related,
inter-disciplinary engineering efforts. This manuscript formalizes transfer
learning, multi-task learning, and meta-learning as abstract learning systems,
consistent with the formal-minimalist abstract systems theory of Mesarovic and
Takahara. Moreover, it uses the presented formalism to relate the three
concepts of learning in terms of composition, hierarchy, and structural
homomorphism. Findings are readily depicted in terms of input-output systems,
highlighting the ease of delineating formal, general systems differences
between transfer, multi-task, and meta-learning.",None,-1
Small Batch Sizes Improve Training of Low-Resource Neural MT,0.16434,"We study the role of an essential hyper-parameter that governs the training
of Transformers for neural machine translation in a low-resource setting: the
batch size. Using theoretical insights and experimental evidence, we argue
against the widespread belief that batch size should be set as large as allowed
by the memory of the GPUs. We show that in a low-resource setting, a smaller
batch size leads to higher scores in a shorter training time, and argue that
this is due to better regularization of the gradients during training.",https://github.com/google/sentencepiece,-1
Kinematic-aware Hierarchical Attention Network for Human Pose Estimation in Videos,0.28078,"Previous video-based human pose estimation methods have shown promising
results by leveraging aggregated features of consecutive frames. However, most
approaches compromise accuracy to mitigate jitter or do not sufficiently
comprehend the temporal aspects of human motion. Furthermore, occlusion
increases uncertainty between consecutive frames, which results in unsmooth
results. To address these issues, we design an architecture that exploits the
keypoint kinematic features with the following components. First, we
effectively capture the temporal features by leveraging individual keypoint's
velocity and acceleration. Second, the proposed hierarchical transformer
encoder aggregates spatio-temporal dependencies and refines the 2D or 3D input
pose estimated from existing estimators. Finally, we provide an online
cross-supervision between the refined input pose generated from the encoder and
the final pose from our decoder to enable joint optimization. We demonstrate
comprehensive results and validate the effectiveness of our model in various
tasks: 2D pose estimation, 3D pose estimation, body mesh recovery, and sparsely
annotated multi-human pose estimation. Our code is available at
https://github.com/KyungMinJin/HANet.",https://github.com/KyungMinJin/HANet,-1
Towards Sequence-Level Training for Visual Tracking,0.844956,"Despite the extensive adoption of machine learning on the task of visual
object tracking, recent learning-based approaches have largely overlooked the
fact that visual tracking is a sequence-level task in its nature; they rely
heavily on frame-level training, which inevitably induces inconsistency between
training and testing in terms of both data distributions and task objectives.
This work introduces a sequence-level training strategy for visual tracking
based on reinforcement learning and discusses how a sequence-level design of
data sampling, learning objectives, and data augmentation can improve the
accuracy and robustness of tracking algorithms. Our experiments on standard
benchmarks including LaSOT, TrackingNet, and GOT-10k demonstrate that four
representative tracking models, SiamRPN++, SiamAttn, TransT, and TrDiMP,
consistently improve by incorporating the proposed methods in training without
modifying architectures.",https://github.com/byminji/SLTtrack,-1
Predicting Future Occupancy Grids in Dynamic Environment with Spatio-Temporal Learning,0.685907,"Reliably predicting future occupancy of highly dynamic urban environments is
an important precursor for safe autonomous navigation. Common challenges in the
prediction include forecasting the relative position of other vehicles,
modelling the dynamics of vehicles subjected to different traffic conditions,
and vanishing surrounding objects. To tackle these challenges, we propose a
spatio-temporal prediction network pipeline that takes the past information
from the environment and semantic labels separately for generating future
occupancy predictions. Compared to the current SOTA, our approach predicts
occupancy for a longer horizon of 3 seconds and in a relatively complex
environment from the nuScenes dataset. Our experimental results demonstrate the
ability of spatio-temporal networks to understand scene dynamics without the
need for HD-Maps and explicit modeling dynamic objects. We publicly release our
occupancy grid dataset based on nuScenes to support further research.",https://github.com/ksm26/SpatioTemporal-Predictions,-1
CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation,0.757249,"We propose Clustering Mask Transformer (CMT-DeepLab), a transformer-based
framework for panoptic segmentation designed around clustering. It rethinks the
existing transformer architectures used in segmentation and detection;
CMT-DeepLab considers the object queries as cluster centers, which fill the
role of grouping the pixels when applied to segmentation. The clustering is
computed with an alternating procedure, by first assigning pixels to the
clusters by their feature affinity, and then updating the cluster centers and
pixel features. Together, these operations comprise the Clustering Mask
Transformer (CMT) layer, which produces cross-attention that is denser and more
consistent with the final segmentation task. CMT-DeepLab improves the
performance over prior art significantly by 4.4% PQ, achieving a new
state-of-the-art of 55.7% PQ on the COCO test-dev set.",None,-1
Anticipative Feature Fusion Transformer for Multi-Modal Action Anticipation,0.86545,"Although human action anticipation is a task which is inherently multi-modal,
state-of-the-art methods on well known action anticipation datasets leverage
this data by applying ensemble methods and averaging scores of unimodal
anticipation networks. In this work we introduce transformer based modality
fusion techniques, which unify multi-modal data at an early stage. Our
Anticipative Feature Fusion Transformer (AFFT) proves to be superior to popular
score fusion approaches and presents state-of-the-art results outperforming
previous methods on EpicKitchens-100 and EGTEA Gaze+. Our model is easily
extensible and allows for adding new modalities without architectural changes.
Consequently, we extracted audio features on EpicKitchens-100 which we add to
the set of commonly used features in the community.",https://github.com/zeyun-zhong/AFFT,21427
Deep Surface Reconstruction from Point Clouds with Visibility Information,0.303985,"Most current neural networks for reconstructing surfaces from point clouds
ignore sensor poses and only operate on raw point locations. Sensor visibility,
however, holds meaningful information regarding space occupancy and surface
orientation. In this paper, we present two simple ways to augment raw point
clouds with visibility information, so it can directly be leveraged by surface
reconstruction networks with minimal adaptation. Our proposed modifications
consistently improve the accuracy of generated surfaces as well as the
generalization ability of the networks to unseen shape domains. Our code and
data is available at https://github.com/raphaelsulzer/dsrv-data.",https://github.com/raphaelsulzer/dsrv-data,-1
Audio-driven Neural Gesture Reenactment with Video Motion Graphs,0.575627,"Human speech is often accompanied by body gestures including arm and hand
gestures. We present a method that reenacts a high-quality video with gestures
matching a target speech audio. The key idea of our method is to split and
re-assemble clips from a reference video through a novel video motion graph
encoding valid transitions between clips. To seamlessly connect different clips
in the reenactment, we propose a pose-aware video blending network which
synthesizes video frames around the stitched frames between two clips.
Moreover, we developed an audio-based gesture searching algorithm to find the
optimal order of the reenacted frames. Our system generates reenactments that
are consistent with both the audio rhythms and the speech content. We evaluate
our synthesized video quality quantitatively, qualitatively, and with user
studies, demonstrating that our method produces videos of much higher quality
and consistency with the target audio compared to previous work and baselines.",https://yzhou359.github.io/video_reenact,-1
Explainable Reinforcement Learning via Model Transforms,0.262018,"Understanding emerging behaviors of reinforcement learning (RL) agents may be
difficult since such agents are often trained in complex environments using
highly complex decision making procedures. This has given rise to a variety of
approaches to explainability in RL that aim to reconcile discrepancies that may
arise between the behavior of an agent and the behavior that is anticipated by
an observer. Most recent approaches have relied either on domain knowledge that
may not always be available, on an analysis of the agent's policy, or on an
analysis of specific elements of the underlying environment, typically modeled
as a Markov Decision Process (MDP). Our key claim is that even if the
underlying model is not fully known (e.g., the transition probabilities have
not been accurately learned) or is not maintained by the agent (i.e., when
using model-free methods), the model can nevertheless be exploited to
automatically generate explanations. For this purpose, we suggest using formal
MDP abstractions and transforms, previously used in the literature for
expediting the search for optimal policies, to automatically produce
explanations. Since such transforms are typically based on a symbolic
representation of the environment, they can provide meaningful explanations for
gaps between the anticipated and actual agent behavior. We formally define the
explainability problem, suggest a class of transforms that can be used for
explaining emergent behaviors, and suggest methods that enable efficient search
for an explanation. We demonstrate the approach on a set of standard
benchmarks.",https://github.com/sarah-keren/RLPE.git,-1
Exploring Customer Price Preference and Product Profit Role in Recommender Systems,0.473111,"Most of the research in the recommender systems domain is focused on the
optimization of the metrics based on historical data such as Mean Average
Precision (MAP) or Recall. However, there is a gap between the research and
industry since the leading Key Performance Indicators (KPIs) for businesses are
revenue and profit. In this paper, we explore the impact of manipulating the
profit awareness of a recommender system. An average e-commerce business does
not usually use a complicated recommender algorithm. We propose an adjustment
of a predicted ranking for score-based recommender systems and explore the
effect of the profit and customers' price preferences on two industry datasets
from the fashion domain. In the experiments, we show the ability to improve
both the precision and the generated recommendations' profit. Such an outcome
represents a win-win situation when e-commerce increases the profit and
customers get more valuable recommendations.",None,-1
Revisiting Grammatical Error Correction Evaluation and Beyond,0.120345,"Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore
and BARTScore) have been widely used in several sentence generation tasks
(e.g., machine translation and text summarization) due to their better
correlation with human judgments over traditional overlap-based methods.
Although PT-based methods have become the de facto standard for training
grammatical error correction (GEC) systems, GEC evaluation still does not
benefit from pretrained knowledge. This paper takes the first step towards
understanding and improving GEC evaluation with pretraining. We first find that
arbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory
correlation results because of the excessive attention to inessential systems
outputs (e.g., unchanged parts). To alleviate the limitation, we propose a
novel GEC evaluation metric to achieve the best of both worlds, namely PT-M2
which only uses PT-based metrics to score those corrected parts. Experimental
results on the CoNLL14 evaluation task show that PT-M2 significantly
outperforms existing methods, achieving a new state-of-the-art result of 0.949
Pearson correlation. Further analysis reveals that PT-M2 is robust to evaluate
competitive GEC systems. Source code and scripts are freely available at
https://github.com/pygongnlp/PT-M2.",https://github.com/pygongnlp/PT-M2,803
Policy Compliance Detection via Expression Tree Inference,0.313061,"Policy Compliance Detection (PCD) is a task we encounter when reasoning over
texts, e.g. legal frameworks. Previous work to address PCD relies heavily on
modeling the task as a special case of Recognizing Textual Entailment.
Entailment is applicable to the problem of PCD, however viewing the policy as a
single proposition, as opposed to multiple interlinked propositions, yields
poor performance and lacks explainability. To address this challenge, more
recent proposals for PCD have argued for decomposing policies into expression
trees consisting of questions connected with logic operators. Question
answering is used to obtain answers to these questions with respect to a
scenario. Finally, the expression tree is evaluated in order to arrive at an
overall solution. However, this work assumes expression trees are provided by
experts, thus limiting its applicability to new policies. In this work, we
learn how to infer expression trees automatically from policy texts. We ensure
the validity of the inferred trees by introducing constrained decoding using a
finite state automaton to ensure the generation of valid trees. We determine
through automatic evaluation that 63% of the expression trees generated by our
constrained generation model are logically equivalent to gold trees. Human
evaluation shows that 88% of trees generated by our model are correct.",None,-1
A Structure-guided Effective and Temporal-lag Connectivity Network for Revealing Brain Disorder Mechanisms,0.351935,"Brain network provides important insights for the diagnosis of many brain
disorders, and how to effectively model the brain structure has become one of
the core issues in the domain of brain imaging analysis. Recently, various
computational methods have been proposed to estimate the causal relationship
(i.e., effective connectivity) between brain regions. Compared with traditional
correlation-based methods, effective connectivity can provide the direction of
information flow, which may provide additional information for the diagnosis of
brain diseases. However, existing methods either ignore the fact that there is
a temporal-lag in the information transmission across brain regions, or simply
set the temporal-lag value between all brain regions to a fixed value. To
overcome these issues, we design an effective temporal-lag neural network
(termed ETLN) to simultaneously infer the causal relationships and the
temporal-lag values between brain regions, which can be trained in an
end-to-end manner. In addition, we also introduce three mechanisms to better
guide the modeling of brain networks. The evaluation results on the Alzheimer's
Disease Neuroimaging Initiative (ADNI) database demonstrate the effectiveness
of the proposed method.",None,-1
Specialized Re-Ranking: A Novel Retrieval-Verification Framework for Cloth Changing Person Re-Identification,0.70915,"Cloth changing person re-identification(Re-ID) can work under more
complicated scenarios with higher security than normal Re-ID and biometric
techniques and is therefore extremely valuable in applications. Meanwhile,
higher flexibility in appearance always leads to more similar-looking confusing
images, which is the weakness of the widely used retrieval methods. In this
work, we shed light on how to handle these similar images. Specifically, we
propose a novel retrieval-verification framework. Given an image, the retrieval
module can search for similar images quickly. Our proposed verification network
will then compare the input image and the candidate images by contrasting those
local details and give a similarity score. An innovative ranking strategy is
also introduced to take a good balance between retrieval and verification
results. Comprehensive experiments are conducted to show the effectiveness of
our framework and its capability in improving the state-of-the-art methods
remarkably on both synthetic and realistic datasets.",None,-1
From Examples to Rules: Neural Guided Rule Synthesis for Information Extraction,0.0379499,"While deep learning approaches to information extraction have had many
successes, they can be difficult to augment or maintain as needs shift.
Rule-based methods, on the other hand, can be more easily modified. However,
crafting rules requires expertise in linguistics and the domain of interest,
making it infeasible for most users. Here we attempt to combine the advantages
of these two directions while mitigating their drawbacks. We adapt recent
advances from the adjacent field of program synthesis to information
extraction, synthesizing rules from provided examples. We use a
transformer-based architecture to guide an enumerative search, and show that
this reduces the number of steps that need to be explored before a rule is
found. Further, we show that without training the synthesis algorithm on the
specific domain, our synthesized rules achieve state-of-the-art performance on
the 1-shot scenario of a task that focuses on few-shot learning for relation
classification, and competitive performance in the 5-shot scenario.",https://github.com/clulab/releases/tree/master/lrec2022-odinsynth,-1
TINYCD: A (Not So) Deep Learning Model For Change Detection,0.838808,"In this paper, we present a lightweight and effective change detection model,
called TinyCD. This model has been designed to be faster and smaller than
current state-of-the-art change detection models due to industrial needs.
Despite being from 13 to 140 times smaller than the compared change detection
models, and exposing at least a third of the computational complexity, our
model outperforms the current state-of-the-art models by at least $1\%$ on both
F1 score and IoU on the LEVIR-CD dataset, and more than $8\%$ on the WHU-CD
dataset. To reach these results, TinyCD uses a Siamese U-Net architecture
exploiting low-level features in a globally temporal and locally spatial way.
In addition, it adopts a new strategy to mix features in the space-time domain
both to merge the embeddings obtained from the Siamese backbones, and, coupled
with an MLP block, it forms a novel space-semantic attention mechanism, the Mix
and Attention Mask Block (MAMB). Source code, models and results are available
here: https://github.com/AndreaCodegoni/Tiny_model_4_CD",https://github.com/AndreaCodegoni/Tiny_model_4_CD,82
Parametrically Retargetable Decision-Makers Tend To Seek Power,0.561686,"If capable AI agents are generally incentivized to seek power in service of
the objectives we specify for them, then these systems will pose enormous
risks, in addition to enormous benefits. In fully observable environments, most
reward functions have an optimal policy which seeks power by keeping options
open and staying alive. However, the real world is neither fully observable,
nor must trained agents be even approximately reward-optimal. We consider a
range of models of AI decision-making, from optimal, to random, to choices
informed by learning and interacting with an environment. We discover that many
decision-making functions are retargetable, and that retargetability is
sufficient to cause power-seeking tendencies. Our functional criterion is
simple and broad. We show that a range of qualitatively dissimilar
decision-making procedures incentivize agents to seek power. We demonstrate the
flexibility of our results by reasoning about learned policy incentives in
Montezuma's Revenge. These results suggest a safety risk: Eventually,
retargetable training procedures may train real-world agents which seek power
over humans.",None,-1
DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation,0.771033,"Natural language processing (NLP) algorithms have become very successful, but
they still struggle when applied to out-of-distribution examples. In this paper
we propose a controllable generation approach in order to deal with this domain
adaptation (DA) challenge. Given an input text example, our DoCoGen algorithm
generates a domain-counterfactual textual example (D-con) - that is similar to
the original in all aspects, including the task label, but its domain is
changed to a desired one. Importantly, DoCoGen is trained using only unlabeled
examples from multiple domains - no NLP task labels or parallel pairs of
textual examples and their domain-counterfactuals are required. We show that
DoCoGen can generate coherent counterfactuals consisting of multiple sentences.
We use the D-cons generated by DoCoGen to augment a sentiment classifier and a
multi-label intent classifier in 20 and 78 DA setups, respectively, where
source-domain labeled data is scarce. Our model outperforms strong baselines
and improves the accuracy of a state-of-the-art unsupervised DA algorithm.",https://github.com/nitaytech/DoCoGen,-1
"Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models",0.369857,"The energy requirements of current natural language processing models
continue to grow at a rapid, unsustainable pace. Recent works highlighting this
problem conclude there is an urgent need for methods that reduce the energy
needs of NLP and machine learning more broadly. In this article, we investigate
techniques that can be used to reduce the energy consumption of common NLP
applications. In particular, we focus on techniques to measure energy usage and
different hardware and datacenter-oriented settings that can be tuned to reduce
energy consumption for training and inference for language models. We
characterize the impact of these settings on metrics such as computational
performance and energy consumption through experiments conducted on a high
performance computing system as well as popular cloud computing platforms.
These techniques can lead to significant reduction in energy consumption when
training language models or their use for inference. For example,
power-capping, which limits the maximum power a GPU can consume, can enable a
15\% decrease in energy usage with marginal increase in overall computation
time when training a transformer-based language model.",https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py,-1
Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension,0.213036,"Multilingual pre-trained models are able to zero-shot transfer knowledge from
rich-resource to low-resource languages in machine reading comprehension (MRC).
However, inherent linguistic discrepancies in different languages could make
answer spans predicted by zero-shot transfer violate syntactic constraints of
the target language. In this paper, we propose a novel multilingual MRC
framework equipped with a Siamese Semantic Disentanglement Model (SSDM) to
disassociate semantics from syntax in representations learned by multilingual
pre-trained models. To explicitly transfer only semantic knowledge to the
target language, we propose two groups of losses tailored for semantic and
syntactic encoding and disentanglement. Experimental results on three
multilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the
effectiveness of our proposed approach over models based on mBERT and XLM-100.
Code is available at:https://github.com/wulinjuan/SSDM_MRC.",None,-1
Sufficient Statistic Memory Approximate Message Passing,0.838186,"Approximate message passing (AMP) type algorithms have been widely used in
the signal reconstruction of certain large random linear systems. A key feature
of the AMP-type algorithms is that their dynamics can be correctly described by
state evolution. However, state evolution does not necessarily guarantee the
convergence of iterative algorithms. To solve the convergence problem of
AMP-type algorithms in principle, this paper proposes a memory AMP (MAMP) under
a sufficient statistic condition, named sufficient statistic MAMP (SS-MAMP). We
show that the covariance matrices of SS-MAMP are L-banded and convergent. Given
an arbitrary MAMP, we can construct the SS-MAMP by damping, which not only
ensures the convergence, but also preserves the orthogonality, i.e., its
dynamics can be correctly described by state evolution.",None,-1
Momentum Calibration for Text Generation,0.519748,"The input and output of most text generation tasks can be transformed to two
sequences of tokens and they can be modeled using sequence-to-sequence learning
modeling tools such as Transformers. These models are usually trained by
maximizing the likelihood the output text sequence and assumes the input
sequence and all gold preceding tokens are given during training, while during
inference the model suffers from the exposure bias problem (i.e., it only has
access to its previously predicted tokens rather gold tokens during beam
search). In this paper, we propose MoCa ({\bf Mo}mentum {\bf Ca}libration) for
text generation. MoCa is an online method that dynamically generates slowly
evolving (but consistent) samples using a momentum moving average generator
with beam search and MoCa learns to align its model scores of these samples
with their actual qualities. Experiments on four text generation datasets
(i.e., CNN/DailyMail, XSum, SAMSum and Gigaword) show MoCa consistently
improves strong pre-trained transformers using vanilla fine-tuning and we
achieve the state-of-the-art results on CNN/DailyMail and SAMSum datasets.",None,-1
Hierarchical Optimal Transport for Comparing Histopathology Datasets,0.242987,"Scarcity of labeled histopathology data limits the applicability of deep
learning methods to under-profiled cancer types and labels. Transfer learning
allows researchers to overcome the limitations of small datasets by
pre-training machine learning models on larger datasets similar to the small
target dataset. However, similarity between datasets is often determined
heuristically. In this paper, we propose a principled notion of distance
between histopathology datasets based on a hierarchical generalization of
optimal transport distances. Our method does not require any training, is
agnostic to model type, and preserves much of the hierarchical structure in
histopathology datasets imposed by tiling. We apply our method to H&E stained
slides from The Cancer Genome Atlas from six different cancer types. We show
that our method outperforms a baseline distance in a cancer-type prediction
task. Our results also show that our optimal transport distance predicts
difficulty of transferability in a tumor vs.normal prediction setting.",https://github.com/ayeaton/HHOT,-1
Table-based Fact Verification with Self-adaptive Mixture of Experts,0.876011,"The table-based fact verification task has recently gained widespread
attention and yet remains to be a very challenging problem. It inherently
requires informative reasoning over natural language together with different
numerical and logical reasoning on tables (e.g., count, superlative,
comparative). Considering that, we exploit mixture-of-experts and present in
this paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE).
Specifically, we have developed a mixture-of-experts neural network to
recognize and execute different types of reasoning -- the network is composed
of multiple experts, each handling a specific part of the semantics for
reasoning, whereas a management module is applied to decide the contribution of
each expert network to the verification result. A self-adaptive method is
developed to teach the management module combining results of different experts
more efficiently without external knowledge. The experimental results
illustrate that our framework achieves 85.1% accuracy on the benchmark dataset
TabFact, comparable with the previous state-of-the-art models. We hope our
framework can serve as a new baseline for table-based verification. Our code is
available at https://github.com/THUMLP/SaMoE.",https://github.com/THUMLP/SaMoE,2921
Deep Generalized Unfolding Networks for Image Restoration,0.998185,"Deep neural networks (DNN) have achieved great success in image restoration.
However, most DNN methods are designed as a black box, lacking transparency and
interpretability. Although some methods are proposed to combine traditional
optimization algorithms with DNN, they usually demand pre-defined degradation
processes or handcrafted assumptions, making it difficult to deal with complex
and real-world applications. In this paper, we propose a Deep Generalized
Unfolding Network (DGUNet) for image restoration. Concretely, without loss of
interpretability, we integrate a gradient estimation strategy into the gradient
descent step of the Proximal Gradient Descent (PGD) algorithm, driving it to
deal with complex and real-world image degradation. In addition, we design
inter-stage information pathways across proximal mapping in different PGD
iterations to rectify the intrinsic information loss in most deep unfolding
networks (DUN) through a multi-scale and spatial-adaptive way. By integrating
the flexible gradient descent and informative proximal mapping, we unfold the
iterative PGD algorithm into a trainable DNN. Extensive experiments on various
image restoration tasks demonstrate the superiority of our method in terms of
state-of-the-art performance, interpretability, and generalizability. The
source code is available at
https://github.com/MC-E/Deep-Generalized-Unfolding-Networks-for-Image-Restoration.",https://github.com/MC-E/DGUNet,-1
CLSE: Corpus of Linguistically Significant Entities,0.0723719,"One of the biggest challenges of natural language generation (NLG) is the
proper handling of named entities. Named entities are a common source of
grammar mistakes such as wrong prepositions, wrong article handling, or
incorrect entity inflection. Without factoring linguistic representation, such
errors are often underrepresented when evaluating on a small set of arbitrarily
picked argument values, or when translating a dataset from a linguistically
simpler language, like English, to a linguistically complex language, like
Russian. However, for some applications, broadly precise grammatical
correctness is critical -- native speakers may find entity-related grammar
errors silly, jarring, or even offensive.
  To enable the creation of more linguistically diverse NLG datasets, we
release a Corpus of Linguistically Significant Entities (CLSE) annotated by
linguist experts. The corpus includes 34 languages and covers 74 different
semantic types to support various applications from airline ticketing to video
games. To demonstrate one possible use of CLSE, we produce an augmented version
of the Schema-Guided Dialog Dataset, SGD-CLSE. Using the CLSE's entities and a
small number of human translations, we create a linguistically representative
NLG evaluation benchmark in three languages: French (high-resource), Marathi
(low-resource), and Russian (highly inflected language). We establish quality
baselines for neural, template-based, and hybrid NLG systems and discuss the
strengths and weaknesses of each approach.",https://github.com/google-research-datasets/clse,-1
Probabilities Are Not Enough: Formal Controller Synthesis for Stochastic Dynamical Models with Epistemic Uncertainty,0.983455,"Capturing uncertainty in models of complex dynamical systems is crucial to
designing safe controllers. Stochastic noise causes aleatoric uncertainty,
whereas imprecise knowledge of model parameters leads to epistemic uncertainty.
Several approaches use formal abstractions to synthesize policies that satisfy
temporal specifications related to safety and reachability. However, the
underlying models exclusively capture aleatoric but not epistemic uncertainty,
and thus require that model parameters are known precisely. Our contribution to
overcoming this restriction is a novel abstraction-based controller synthesis
method for continuous-state models with stochastic noise and uncertain
parameters. By sampling techniques and robust analysis, we capture both
aleatoric and epistemic uncertainty, with a user-specified confidence level, in
the transition probability intervals of a so-called interval Markov decision
process (iMDP). We synthesize an optimal policy on this iMDP, which translates
(with the specified confidence level) to a feedback controller for the
continuous model with the same performance guarantees. Our experimental
benchmarks confirm that accounting for epistemic uncertainty leads to
controllers that are more robust against variations in parameter values.",https://github.com/LAVA-LAB/DynAbs,-1
Argus++: Robust Real-time Activity Detection for Unconstrained Video Streams with Overlapping Cube Proposals,0.502194,"Activity detection is one of the attractive computer vision tasks to exploit
the video streams captured by widely installed cameras. Although achieving
impressive performance, conventional activity detection algorithms are usually
designed under certain constraints, such as using trimmed and/or
object-centered video clips as inputs. Therefore, they failed to deal with the
multi-scale multi-instance cases in real-world unconstrained video streams,
which are untrimmed and have large field-of-views. Real-time requirements for
streaming analysis also mark brute force expansion of them unfeasible.
  To overcome these issues, we propose Argus++, a robust real-time activity
detection system for analyzing unconstrained video streams. The design of
Argus++ introduces overlapping spatio-temporal cubes as an intermediate concept
of activity proposals to ensure coverage and completeness of activity detection
through over-sampling. The overall system is optimized for real-time processing
on standalone consumer-level hardware. Extensive experiments on different
surveillance and driving scenarios demonstrated its superior performance in a
series of activity detection benchmarks, including CVPR ActivityNet ActEV 2021,
NIST ActEV SDL UF/KF, TRECVID ActEV 2020/2021, and ICCV ROAD 2021.",None,233
MHMS: Multimodal Hierarchical Multimedia Summarization,0.67204,"Multimedia summarization with multimodal output can play an essential role in
real-world applications, i.e., automatically generating cover images and titles
for news articles or providing introductions to online videos. In this work, we
propose a multimodal hierarchical multimedia summarization (MHMS) framework by
interacting visual and language domains to generate both video and textual
summaries. Our MHMS method contains video and textual segmentation and
summarization module, respectively. It formulates a cross-domain alignment
objective with optimal transport distance which leverages cross-domain
interaction to generate the representative keyframe and textual summary. We
evaluated MHMS on three recent multimodal datasets and demonstrated the
effectiveness of our method in producing high-quality multimodal summaries.",None,-1
Model Attribution of Face-swap Deepfake Videos,0.222591,"AI-created face-swap videos, commonly known as Deepfakes, have attracted wide
attention as powerful impersonation attacks. Existing research on Deepfakes
mostly focuses on binary detection to distinguish between real and fake videos.
However, it is also important to determine the specific generation model for a
fake video, which can help attribute it to the source for forensic
investigation. In this paper, we fill this gap by studying the model
attribution problem of Deepfake videos. We first introduce a new dataset with
DeepFakes from Different Models (DFDM) based on several Autoencoder models.
Specifically, five generation models with variations in encoder, decoder,
intermediate layer, input resolution, and compression ratio have been used to
generate a total of 6,450 Deepfake videos based on the same input. Then we take
Deepfakes model attribution as a multiclass classification task and propose a
spatial and temporal attention based method to explore the differences among
Deepfakes in the new dataset. Experimental evaluation shows that most existing
Deepfakes detection methods failed in Deepfakes model attribution, while the
proposed method achieved over 70% accuracy on the high-quality DFDM dataset.",https://github.com/shanface33/Deepfake_Model_Attribution,-1
Unsupervised Domain Adaptation with Implicit Pseudo Supervision for Semantic Segmentation,0.226125,"Pseudo-labelling is a popular technique in unsuper-vised domain adaptation
for semantic segmentation. However, pseudo labels are noisy and inevitably have
confirmation bias due to the discrepancy between source and target domains and
training process. In this paper, we train the model by the pseudo labels which
are implicitly produced by itself to learn new complementary knowledge about
target domain. Specifically, we propose a tri-learning architecture, where
every two branches produce the pseudo labels to train the third one. And we
align the pseudo labels based on the similarity of the probability
distributions for each two branches. To further implicitly utilize the pseudo
labels, we maximize the distances of features for different classes and
minimize the distances for the same classes by triplet loss. Extensive
experiments on GTA5 to Cityscapes and SYNTHIA to Cityscapes tasks show that the
proposed method has considerable improvements.",None,-1
Defending Black-box Skeleton-based Human Activity Classifiers,0.62166,"Skeletal motions have been heavily replied upon for human activity
recognition (HAR). Recently, a universal vulnerability of skeleton-based HAR
has been identified across a variety of classifiers and data, calling for
mitigation. To this end, we propose the first black-box defense method for
skeleton-based HAR to our best knowledge. Our method is featured by full
Bayesian treatments of the clean data, the adversaries and the classifier,
leading to (1) a new Bayesian Energy-based formulation of robust discriminative
classifiers, (2) a new adversary sampling scheme based on natural motion
manifolds, and (3) a new post-train Bayesian strategy for black-box defense. We
name our framework Bayesian Energy-based Adversarial Training or BEAT. BEAT is
straightforward but elegant, which turns vulnerable black-box classifiers into
robust ones without sacrificing accuracy. It demonstrates surprising and
universal effectiveness across a wide range of skeletal HAR classifiers and
datasets, under various attacks. Code is available at
https://github.com/realcrane/RobustActionRecogniser.",https://github.com/realcrane/Defending-Black-box-Skeleton-based-Human-Activity-Classiﬁers,1271
Generating Executable Action Plans with Environmentally-Aware Language Models,0.905127,"Large Language Models (LLMs) trained using massive text datasets have
recently shown promise in generating action plans for robotic agents from high
level text queries. However, these models typically do not consider the robot's
environment, resulting in generated plans that may not actually be executable,
due to ambiguities in the planned actions or environmental constraints. In this
paper, we propose an approach to generate environmentally-aware action plans
that agents are better able to execute. Our approach involves integrating
environmental objects and object relations as additional inputs into LLM action
plan generation to provide the system with an awareness of its surroundings,
resulting in plans where each generated action is mapped to objects present in
the scene. We also design a novel scoring function that, along with generating
the action steps and associating them with objects, helps the system
disambiguate among object instances and take into account their states. We
evaluated our approach using the VirtualHome simulator and the ActivityPrograms
knowledge base and found that action plans generated from our system had a 310%
improvement in executability and a 147% improvement in correctness over prior
work. The complete code and a demo of our method is publicly available at
https://github.com/hri-ironlab/scene_aware_language_planner.",https://github.com/hri-ironlab/scene_aware_language_planner,-1
EpiGNN: Exploring Spatial Transmission with Graph Neural Network for Regional Epidemic Forecasting,0.497115,"Epidemic forecasting is the key to effective control of epidemic transmission
and helps the world mitigate the crisis that threatens public health. To better
understand the transmission and evolution of epidemics, we propose EpiGNN, a
graph neural network-based model for epidemic forecasting. Specifically, we
design a transmission risk encoding module to characterize local and global
spatial effects of regions in epidemic processes and incorporate them into the
model. Meanwhile, we develop a Region-Aware Graph Learner (RAGL) that takes
transmission risk, geographical dependencies, and temporal information into
account to better explore spatial-temporal dependencies and makes regions aware
of related regions' epidemic situations. The RAGL can also combine with
external resources, such as human mobility, to further improve prediction
performance. Comprehensive experiments on five real-world epidemic-related
datasets (including influenza and COVID-19) demonstrate the effectiveness of
our proposed method and show that EpiGNN outperforms state-of-the-art baselines
by 9.48% in RMSE.",https://github.com/Xiefeng69/EpiGNN,-1
Towards automatic generation of Piping and Instrumentation Diagrams (P&IDs) with Artificial Intelligence,0.736403,"Developing Piping and Instrumentation Diagrams (P&IDs) is a crucial step
during the development of chemical processes. Currently, this is a tedious,
manual, and time-consuming task. We propose a novel, completely data-driven
method for the prediction of control structures. Our methodology is inspired by
end-to-end transformer-based human language translation models. We cast the
control structure prediction as a translation task where Process Flow Diagrams
(PFDs) are translated to P&IDs. To use established transformer-based language
translation models, we represent the P&IDs and PFDs as strings using our
recently proposed SFILES 2.0 notation. Model training is performed in a
transfer learning approach. Firstly, we pre-train our model using generated
P&IDs to learn the grammatical structure of the process diagrams. Thereafter,
the model is fine-tuned leveraging transfer learning on real P&IDs. The model
achieved a top-5 accuracy of 74.8% on 10,000 generated P&IDs and 89.2% on
100,000 generated P&IDs. These promising results show great potential for
AI-assisted process engineering. The tests on a dataset of 312 real P&IDs
indicate the need of a larger P&IDs dataset for industry applications.",None,-1
Semeval-2022 Task 1: CODWOE -- Comparing Dictionaries and Word Embeddings,0.908236,"Word embeddings have advanced the state of the art in NLP across numerous
tasks. Understanding the contents of dense neural representations is of utmost
interest to the computational semantics community. We propose to focus on
relating these opaque word vectors with human-readable definitions, as found in
dictionaries. This problem naturally divides into two subtasks: converting
definitions into embeddings, and converting embeddings into definitions. This
task was conducted in a multilingual setting, using comparable sets of
embeddings trained homogeneously.",https://github.com/pi,-1
MEAD: A Multi-Armed Approach for Evaluation of Adversarial Examples Detectors,0.192685,"Detection of adversarial examples has been a hot topic in the last years due
to its importance for safely deploying machine learning algorithms in critical
applications. However, the detection methods are generally validated by
assuming a single implicitly known attack strategy, which does not necessarily
account for real-life threats. Indeed, this can lead to an overoptimistic
assessment of the detectors' performance and may induce some bias in the
comparison between competing detection schemes. We propose a novel multi-armed
framework, called MEAD, for evaluating detectors based on several attack
strategies to overcome this limitation. Among them, we make use of three new
objectives to generate attacks. The proposed performance metric is based on the
worst-case scenario: detection is successful if and only if all different
attacks are correctly recognized. Empirically, we show the effectiveness of our
approach. Moreover, the poor performance obtained for state-of-the-art
detectors opens a new exciting line of research.",https://github.com/meadsubmission/MEAD,-1
Graph Enhanced Contrastive Learning for Radiology Findings Summarization,0.978534,"The impression section of a radiology report summarizes the most prominent
observation from the findings section and is the most important section for
radiologists to communicate to physicians. Summarizing findings is
time-consuming and can be prone to error for inexperienced radiologists, and
thus automatic impression generation has attracted substantial attention. With
the encoder-decoder framework, most previous studies explore incorporating
extra knowledge (e.g., static pre-defined clinical ontologies or extra
background information). Yet, they encode such knowledge by a separate encoder
to treat it as an extra input to their models, which is limited in leveraging
their relations with the original findings. To address the limitation, we
propose a unified framework for exploiting both extra knowledge and the
original findings in an integrated way so that the critical information (i.e.,
key words and their relations) can be extracted in an appropriate way to
facilitate impression generation. In detail, for each input findings, it is
encoded by a text encoder, and a graph is constructed through its entities and
dependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is
adopted to model relation information in the constructed graph. Finally, to
emphasize the key words in the findings, contrastive learning is introduced to
map positive samples (constructed by masking non-key words) closer and push
apart negative ones (constructed by masking key words). The experimental
results on OpenI and MIMIC-CXR confirm the effectiveness of our proposed
method.",https://github.com/jinpeng01/AIG_CL,-1
Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild,0.960751,"Previous methods for dynamic facial expression in the wild are mainly based
on Convolutional Neural Networks (CNNs), whose local operations ignore the
long-range dependencies in videos. To solve this problem, we propose the
spatio-temporal Transformer (STT) to capture discriminative features within
each frame and model contextual relationships among frames. Spatio-temporal
dependencies are captured and integrated by our unified Transformer.
Specifically, given an image sequence consisting of multiple frames as input,
we utilize the CNN backbone to translate each frame into a visual feature
sequence. Subsequently, the spatial attention and the temporal attention within
each block are jointly applied for learning spatio-temporal representations at
the sequence level. In addition, we propose the compact softmax cross entropy
loss to further encourage the learned features have the minimum intra-class
distance and the maximum inter-class distance. Experiments on two in-the-wild
dynamic facial expression datasets (i.e., DFEW and AFEW) indicate that our
method provides an effective way to make use of the spatial and temporal
dependencies for dynamic facial expression recognition. The source code and the
training logs will be made publicly available.",None,36241
Domain Adaptation meets Individual Fairness. And they get along,0.540242,"Many instances of algorithmic bias are caused by distributional shifts. For
example, machine learning (ML) models often perform worse on demographic groups
that are underrepresented in the training data. In this paper, we leverage this
connection between algorithmic fairness and distribution shifts to show that
algorithmic fairness interventions can help ML models overcome distribution
shifts, and that domain adaptation methods (for overcoming distribution shifts)
can mitigate algorithmic biases. In particular, we show that (i) enforcing
suitable notions of individual fairness (IF) can improve the
out-of-distribution accuracy of ML models under the covariate shift assumption
and that (ii) it is possible to adapt representation alignment methods for
domain adaptation to enforce individual fairness. The former is unexpected
because IF interventions were not developed with distribution shifts in mind.
The latter is also unexpected because representation alignment is not a common
approach in the individual fairness literature.",None,-1
Multiview Stereo with Cascaded Epipolar RAFT,0.877271,"We address multiview stereo (MVS), an important 3D vision task that
reconstructs a 3D model such as a dense point cloud from multiple calibrated
images. We propose CER-MVS (Cascaded Epipolar RAFT Multiview Stereo), a new
approach based on the RAFT (Recurrent All-Pairs Field Transforms) architecture
developed for optical flow. CER-MVS introduces five new changes to RAFT:
epipolar cost volumes, cost volume cascading, multiview fusion of cost volumes,
dynamic supervision, and multiresolution fusion of depth maps. CER-MVS is
significantly different from prior work in multiview stereo. Unlike prior work,
which operates by updating a 3D cost volume, CER-MVS operates by updating a
disparity field. Furthermore, we propose an adaptive thresholding method to
balance the completeness and accuracy of the reconstructed point clouds.
Experiments show that our approach achieves competitive performance on DTU (the
second best among known results) and state-of-the-art performance on the
Tanks-and-Temples benchmark (both the intermediate and advanced set). Code is
available at https://github.com/princeton-vl/CER-MVS",https://github.com/princeton-vl/CER-MVS,-1
CC-Riddle: A Question Answering Dataset of Chinese Character Riddles,0.281946,"The Chinese character riddle is a unique form of cultural entertainment
specific to the Chinese language. It typically comprises two parts: the riddle
description and the solution. The solution to the riddle is a single character,
while the riddle description primarily describes the glyph of the solution,
occasionally supplemented with its explanation and pronunciation. Solving
Chinese character riddles is a challenging task that demands understanding of
character glyph, general knowledge, and a grasp of figurative language. In this
paper, we construct a \textbf{C}hinese \textbf{C}haracter riddle dataset named
CC-Riddle, which covers the majority of common simplified Chinese characters.
The construction process is a combination of web crawling, language model
generation and manual filtering. In generation stage, we input the Chinese
phonetic alphabet, glyph and meaning of the solution character into the
generation model, which then produces multiple riddle descriptions. The
generated riddles are then manually filtered and the final CC-Riddle dataset is
composed of both human-written riddles and these filtered, generated riddles.
In order to assess the performance of language models on the task of solving
character riddles, we use retrieval-based, generative and multiple-choice QA
strategies to test three language models: BERT, ChatGPT and ChatGLM. The test
results reveal that current language models still struggle to solve Chinese
character riddles. CC-Riddle is publicly available at
\url{https://github.com/pku0xff/CC-Riddle}.",https://github.com/pku0xff/CC-Riddle,-1
GERE: Generative Evidence Retrieval for Fact Verification,0.928613,"Fact verification (FV) is a challenging task which aims to verify a claim
using multiple evidential sentences from trustworthy corpora, e.g., Wikipedia.
Most existing approaches follow a three-step pipeline framework, including
document retrieval, sentence retrieval and claim verification. High-quality
evidences provided by the first two steps are the foundation of the effective
reasoning in the last step. Despite being important, high-quality evidences are
rarely studied by existing works for FV, which often adopt the off-the-shelf
models to retrieve relevant documents and sentences in an
""index-retrieve-then-rank"" fashion. This classical approach has clear drawbacks
as follows: i) a large document index as well as a complicated search process
is required, leading to considerable memory and computational overhead; ii)
independent scoring paradigms fail to capture the interactions among documents
and sentences in ranking; iii) a fixed number of sentences are selected to form
the final evidence set. In this work, we propose GERE, the first system that
retrieves evidences in a generative fashion, i.e., generating the document
titles as well as evidence sentence identifiers. This enables us to mitigate
the aforementioned technical issues since: i) the memory and computational cost
is greatly reduced because the document index is eliminated and the heavy
ranking process is replaced by a light generative process; ii) the dependency
between documents and that between sentences could be captured via sequential
generation process; iii) the generative formulation allows us to dynamically
select a precise set of relevant evidences for each claim. The experimental
results on the FEVER dataset show that GERE achieves significant improvements
over the state-of-the-art baselines, with both time-efficiency and
memory-efficiency.",https://github.com/Chriskuei/GERE,-1
Retrieval Based Time Series Forecasting,0.245293,"Time series data appears in a variety of applications such as smart
transportation and environmental monitoring. One of the fundamental problems
for time series analysis is time series forecasting. Despite the success of
recent deep time series forecasting methods, they require sufficient
observation of historical values to make accurate forecasting. In other words,
the ratio of the output length (or forecasting horizon) to the sum of the input
and output lengths should be low enough (e.g., 0.3). As the ratio increases
(e.g., to 0.8), the uncertainty for the forecasting accuracy increases
significantly. In this paper, we show both theoretically and empirically that
the uncertainty could be effectively reduced by retrieving relevant time series
as references. In the theoretical analysis, we first quantify the uncertainty
and show its connections to the Mean Squared Error (MSE). Then we prove that
models with references are easier to learn than models without references since
the retrieved references could reduce the uncertainty. To empirically
demonstrate the effectiveness of the retrieval based time series forecasting
models, we introduce a simple yet effective two-stage method, called ReTime
consisting of a relational retrieval and a content synthesis. We also show that
ReTime can be easily adapted to the spatial-temporal time series and time
series imputation settings. Finally, we evaluate ReTime on real-world datasets
to demonstrate its effectiveness.",None,80967
Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021,0.794019,"Human intelligence has the remarkable ability to quickly adapt to new tasks
and environments. Starting from a very young age, humans acquire new skills and
learn how to solve new tasks either by imitating the behavior of others or by
following provided natural language instructions. To facilitate research in
this direction, we propose \emph{IGLU: Interactive Grounded Language
Understanding in a Collaborative Environment}.
  The primary goal of the competition is to approach the problem of how to
build interactive agents that learn to solve a task while provided with
grounded natural language instructions in a collaborative environment.
Understanding the complexity of the challenge, we split it into sub-tasks to
make it feasible for participants.",None,-1
Few-Shot Segmentation via Rich Prototype Generation and Recurrent Prediction Enhancement,0.0230787,"Prototype learning and decoder construction are the keys for few-shot
segmentation. However, existing methods use only a single prototype generation
mode, which can not cope with the intractable problem of objects with various
scales. Moreover, the one-way forward propagation adopted by previous methods
may cause information dilution from registered features during the decoding
process. In this research, we propose a rich prototype generation module (RPGM)
and a recurrent prediction enhancement module (RPEM) to reinforce the prototype
learning paradigm and build a unified memory-augmented decoder for few-shot
segmentation, respectively. Specifically, the RPGM combines superpixel and
K-means clustering to generate rich prototype features with complementary scale
relationships and adapt the scale gap between support and query images. The
RPEM utilizes the recurrent mechanism to design a round-way propagation
decoder. In this way, registered features can provide object-aware information
continuously. Experiments show that our method consistently outperforms other
competitors on two popular benchmarks PASCAL-${{5}^{i}}$ and COCO-${{20}^{i}}$.",None,-1
Differentially Private Counterfactuals via Functional Mechanism,0.264414,"Counterfactual, serving as one emerging type of model explanation, has
attracted tons of attentions recently from both industry and academia.
Different from the conventional feature-based explanations (e.g.,
attributions), counterfactuals are a series of hypothetical samples which can
flip model decisions with minimal perturbations on queries. Given valid
counterfactuals, humans are capable of reasoning under ``what-if''
circumstances, so as to better understand the model decision boundaries.
However, releasing counterfactuals could be detrimental, since it may
unintentionally leak sensitive information to adversaries, which brings about
higher risks on both model security and data privacy. To bridge the gap, in
this paper, we propose a novel framework to generate differentially private
counterfactual (DPC) without touching the deployed model or explanation set,
where noises are injected for protection while maintaining the explanation
roles of counterfactual. In particular, we train an autoencoder with the
functional mechanism to construct noisy class prototypes, and then derive the
DPC from the latent prototypes based on the post-processing immunity of
differential privacy. Further evaluations demonstrate the effectiveness of the
proposed framework, showing that DPC can successfully relieve the risks on both
extraction and inference attacks.",None,-1
"""Think Before You Speak"": Improving Multi-Action Dialog Policy by Planning Single-Action Dialogs",0.635567,"Multi-action dialog policy (MADP), which generates multiple atomic dialog
actions per turn, has been widely applied in task-oriented dialog systems to
provide expressive and efficient system responses. Existing MADP models usually
imitate action combinations from the labeled multi-action dialog samples. Due
to data limitations, they generalize poorly toward unseen dialog flows. While
interactive learning and reinforcement learning algorithms can be applied to
incorporate external data sources of real users and user simulators, they take
significant manual effort to build and suffer from instability. To address
these issues, we propose Planning Enhanced Dialog Policy (PEDP), a novel
multi-task learning framework that learns single-action dialog dynamics to
enhance multi-action prediction. Our PEDP method employs model-based planning
for conceiving what to express before deciding the current response through
simulating single-action dialogs. Experimental results on the MultiWOZ dataset
demonstrate that our fully supervised learning-based method achieves a solid
task success rate of 90.6%, improving 3% compared to the state-of-the-art
methods.",https://github.com/ShuoZhangXJTU/PEDP,-1
On the non-efficient PAC learnability of conjunctive queries,0.0494041,"This note serves three purposes: (i) we provide a self-contained exposition
of the fact that conjunctive queries are not efficiently learnable in the
Probably-Approximately-Correct (PAC) model, paying clear attention to the
complicating fact that this concept class lacks the polynomial-size fitting
property, a property that is tacitly assumed in much of the computational
learning theory literature; (ii) we establish a strong negative PAC
learnability result that applies to many restricted classes of conjunctive
queries (CQs), including acyclic CQs for a wide range of notions of
""acyclicity""; (iii) we show that CQs (and UCQs) are efficiently PAC learnable
with membership queries.",None,-1
Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models,0.0748553,"While multilingual language models can improve NLP performance on
low-resource languages by leveraging higher-resource languages, they also
reduce average performance on all languages (the 'curse of multilinguality').
Here we show another problem with multilingual models: grammatical structures
in higher-resource languages bleed into lower-resource languages, a phenomenon
we call grammatical structure bias. We show this bias via a novel method for
comparing the fluency of multilingual models to the fluency of monolingual
Spanish and Greek models: testing their preference for two carefully-chosen
variable grammatical structures (optional pronoun-drop in Spanish and optional
Subject-Verb ordering in Greek). We find that multilingual BERT is biased
toward the English-like setting (explicit pronouns and Subject-Verb-Object
ordering) as compared to our monolingual control language model. With our case
studies, we hope to bring to light the fine-grained ways in which multilingual
models can be biased,and encourage more linguistically-aware fluency
evaluation.",None,-1
Boosting 3D Object Detection via Object-Focused Image Fusion,0.642357,"3D object detection has achieved remarkable progress by taking point clouds
as the only input. However, point clouds often suffer from incomplete geometric
structures and the lack of semantic information, which makes detectors hard to
accurately classify detected objects. In this work, we focus on how to
effectively utilize object-level information from images to boost the
performance of point-based 3D detector. We present DeMF, a simple yet effective
method to fuse image information into point features. Given a set of point
features and image feature maps, DeMF adaptively aggregates image features by
taking the projected 2D location of the 3D point as reference. We evaluate our
method on the challenging SUN RGB-D dataset, improving state-of-the-art results
by a large margin (+2.1 mAP@0.25 and +2.3mAP@0.5). Code is available at
https://github.com/haoy945/DeMF.",https://github.com/haoy945/DeMF,-1
STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation,0.989237,"How to learn a better speech representation for end-to-end speech-to-text
translation (ST) with limited labeled data? Existing techniques often attempt
to transfer powerful machine translation (MT) capabilities to ST, but neglect
the representation discrepancy across modalities. In this paper, we propose the
Speech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy.
Specifically, we mix up the representation sequences of different modalities,
and take both unimodal speech sequences and multimodal mixed sequences as input
to the translation model in parallel, and regularize their output predictions
with a self-learning framework. Experiments on MuST-C speech translation
benchmark and further analysis show that our method effectively alleviates the
cross-modal representation discrepancy, and achieves significant improvements
over a strong baseline on eight translation directions.",https://github.com/ictnlp/STEMM,-1
The Free Energy Principle for Perception and Action: A Deep Learning Perspective,0.372725,"The free energy principle, and its corollary active inference, constitute a
bio-inspired theory that assumes biological agents act to remain in a
restricted set of preferred states of the world, i.e., they minimize their free
energy. Under this principle, biological agents learn a generative model of the
world and plan actions in the future that will maintain the agent in an
homeostatic state that satisfies its preferences. This framework lends itself
to being realized in silico, as it comprehends important aspects that make it
computationally affordable, such as variational inference and amortized
planning. In this work, we investigate the tool of deep learning to design and
realize artificial agents based on active inference, presenting a deep-learning
oriented presentation of the free energy principle, surveying works that are
relevant in both machine learning and active inference areas, and discussing
the design choices that are involved in the implementation process. This
manuscript probes newer perspectives for the active inference framework,
grounding its theoretical aspects into more pragmatic affairs, offering a
practical guide to active inference newcomers and a starting point for deep
learning practitioners that would like to investigate implementations of the
free energy principle.",None,-1
Parameter-Efficient Tuning by Manipulating Hidden States of Pretrained Language Models For Classification Tasks,0.0334394,"Parameter-efficient tuning aims to distill knowledge for downstream tasks by
optimizing a few introduced parameters while freezing the pretrained language
models (PLMs). Continuous prompt tuning which prepends a few trainable vectors
to the embeddings of input is one of these methods and has drawn much attention
due to its effectiveness and efficiency. This family of methods can be
illustrated as exerting nonlinear transformations of hidden states inside PLMs.
However, a natural question is ignored: can the hidden states be directly used
for classification without changing them? In this paper, we aim to answer this
question by proposing a simple tuning method which only introduces three
trainable vectors. Firstly, we integrate all layers hidden states using the
introduced vectors. And then, we input the integrated hidden state(s) to a
task-specific linear classifier to predict categories. This scheme is similar
to the way ELMo utilises hidden states except that they feed the hidden states
to LSTM-based models. Although our proposed tuning scheme is simple, it
achieves comparable performance with prompt tuning methods like P-tuning and
P-tuning v2, verifying that original hidden states do contain useful
information for classification tasks. Moreover, our method has an advantage
over prompt tuning in terms of time and the number of parameters.",None,334
InducT-GCN: Inductive Graph Convolutional Networks for Text Classification,0.73218,"Text classification aims to assign labels to textual units by making use of
global information. Recent studies have applied graph neural network (GNN) to
capture the global word co-occurrence in a corpus. Existing approaches require
that all the nodes (training and test) in a graph are present during training,
which are transductive and do not naturally generalise to unseen nodes. To make
those models inductive, they use extra resources, like pretrained word
embedding. However, high-quality resource is not always available and hard to
train. Under the extreme settings with no extra resource and limited amount of
training set, can we still learn an inductive graph-based text classification
model? In this paper, we introduce a novel inductive graph-based text
classification framework, InducT-GCN (InducTive Graph Convolutional Networks
for Text classification). Compared to transductive models that require test
documents in training, we construct a graph based on the statistics of training
documents only and represent document vectors with a weighted sum of word
vectors. We then conduct one-directional GCN propagation during testing. Across
five text classification benchmarks, our InducT-GCN outperformed
state-of-the-art methods that are either transductive in nature or pre-trained
additional resources. We also conducted scalability testing by gradually
increasing the data size and revealed that our InducT-GCN can reduce the time
and space complexity. The code is available on:
https://github.com/usydnlp/InductTGCN.",https://github.com/usydnlp/InductTGCN,3559
TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter,0.999787,"Pre-trained language models (PLMs) are fundamental for natural language
processing applications. Most existing PLMs are not tailored to the noisy
user-generated text on social media, and the pre-training does not factor in
the valuable social engagement logs available in a social network. We present
TwHIN-BERT, a multilingual language model productionized at Twitter, trained on
in-domain data from the popular social network. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages,
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on various multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We open-source TwHIN-BERT and our
curated hashtag prediction and social engagement benchmark datasets to the
research community.",None,255449
Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems,0.80854,"Do all instances need inference through the big models for a correct
prediction? Perhaps not; some instances are easy and can be answered correctly
by even small capacity models. This provides opportunities for improving the
computational efficiency of systems. In this work, we present an explorative
study on 'model cascading', a simple technique that utilizes a collection of
models of varying capacities to accurately yet efficiently output predictions.
Through comprehensive experiments in multiple task settings that differ in the
number of models available for cascading (K value), we show that cascading
improves both the computational efficiency and the prediction accuracy. For
instance, in K=3 setting, cascading saves up to 88.93% computation cost and
consistently achieves superior prediction accuracy with an improvement of up to
2.18%. We also study the impact of introducing additional models in the cascade
and show that it further increases the efficiency improvements. Finally, we
hope that our work will facilitate development of efficient NLP systems making
their widespread adoption in real-world applications possible.",None,-1
Mitigating Negative Style Transfer in Hybrid Dialogue System,0.0745131,"As the functionality of dialogue systems evolves, hybrid dialogue systems
that accomplish user-specific goals and participate in open-topic chitchat with
users are attracting growing attention. Existing research learns both tasks
concurrently utilizing a multi-task fusion technique but ignores the negative
transfer phenomenon induced by the unique textual style differences. Therefore,
contrastive learning based on the latent variable model is used to decouple the
various textual genres in the latent space. We devise supervised and
self-supervised positive and negative sample constructions for diverse
datasets. In addition, to capitalize on the style information contained in the
decoupled latent variables, we employ a style prefix that incorporates latent
variables further to control the generation of responses with varying styles.
We performed extensive experiments on three dialogue datasets, including a
hybrid dialogue dataset and two task-oriented dialogue datasets. The
experimental results demonstrate that our method can mitigate the negative
style transfer issue and achieves state-of-the-art performance on multiple
dialogue datasets.",https://github.com/whatissimondoing/HiS-Dialog,-1
GMF: General Multimodal Fusion Framework for Correspondence Outlier Rejection,0.817084,"Rejecting correspondence outliers enables to boost the correspondence
quality, which is a critical step in achieving high point cloud registration
accuracy. The current state-of-the-art correspondence outlier rejection methods
only utilize the structure features of the correspondences. However, texture
information is critical to reject the correspondence outliers in our human
vision system. In this paper, we propose General Multimodal Fusion (GMF) to
learn to reject the correspondence outliers by leveraging both the structure
and texture information. Specifically, two cross-attention-based fusion layers
are proposed to fuse the texture information from paired images and structure
information from point correspondences. Moreover, we propose a convolutional
position encoding layer to enhance the difference between Tokens and enable the
encoding feature pay attention to neighbor information. Our position encoding
layer will make the cross-attention operation integrate both local and global
information. Experiments on multiple datasets(3DMatch, 3DLoMatch, KITTI) and
recent state-of-the-art models (3DRegNet, DGR, PointDSC) prove that our GMF
achieves wide generalization ability and consistently improves the point cloud
registration accuracy. Furthermore, several ablation studies demonstrate the
robustness of the proposed GMF on different loss functions, lighting conditions
and noises.The code is available at https://github.com/XiaoshuiHuang/GMF.",https://github.com/XiaoshuiHuang/GMF,-1
Embedding Hallucination for Few-Shot Language Fine-tuning,0.0216682,"Few-shot language learners adapt knowledge from a pre-trained model to
recognize novel classes from a few-labeled sentences. In such settings,
fine-tuning a pre-trained language model can cause severe over-fitting. In this
paper, we propose an Embedding Hallucination (EmbedHalluc) method, which
generates auxiliary embedding-label pairs to expand the fine-tuning dataset.
The hallucinator is trained by playing an adversarial game with the
discriminator, such that the hallucinated embedding is indiscriminative to the
real ones in the fine-tuning dataset. By training with the extended dataset,
the language learner effectively learns from the diverse hallucinated
embeddings to overcome the over-fitting issue. Experiments demonstrate that our
proposed method is effective in a wide range of language tasks, outperforming
current fine-tuning methods. Further, we show that EmbedHalluc outperforms
other methods that address this over-fitting problem, such as common data
augmentation, semi-supervised pseudo-labeling, and regularization. The code
will be made available at: https://github.com/yiren-jian/EmbedHalluc.",https://github.com/yiren-jian/EmbedHalluc,-1
Near-Term Advances in Quantum Natural Language Processing,0.148897,"This paper describes experiments showing that some tasks in natural language
processing (NLP) can already be performed using quantum computers, though so
far only with small datasets.
  We demonstrate various approaches to topic classification. The first uses an
explicit word-based approach, in which word-topic scoring weights are
implemented as fractional rotations of individual qubit, and a new phrase is
classified based on the accumulation of these weights in a scoring qubit using
entangling controlled-NOT gates. This is compared with more scalable quantum
encodings of word embedding vectors, which are used in the computation of
kernel values in a quantum support vector machine: this approach achieved an
average of 62% accuracy on classification tasks involving over 10000 words,
which is the largest such quantum computing experiment to date.
  We describe a quantum probability approach to bigram modeling that can be
applied to sequences of words and formal concepts, investigating a generative
approximation to these distributions using a quantum circuit Born machine, and
an approach to ambiguity resolution in verb-noun composition using single-qubit
rotations for simple nouns and 2-qubit controlled-NOT gates for simple verbs.
  The smaller systems described have been run successfully on physical quantum
computers, and the larger ones have been simulated. We show that statistically
meaningful results can be obtained using real datasets, but this is much more
difficult to predict than with easier artificial language examples used
previously in developing quantum NLP systems.
  Other approaches to quantum NLP are compared, partly with respect to
contemporary issues including informal language, fluency, and truthfulness.",None,4087
Integrating Human-in-the-loop into Swarm Learning for Decentralized Fake News Detection,0.77687,"Social media has become an effective platform to generate and spread fake
news that can mislead people and even distort public opinion. Centralized
methods for fake news detection, however, cannot effectively protect user
privacy during the process of centralized data collection for training models.
Moreover, it cannot fully involve user feedback in the loop of learning
detection models for further enhancing fake news detection. To overcome these
challenges, this paper proposed a novel decentralized method, Human-in-the-loop
Based Swarm Learning (HBSL), to integrate user feedback into the loop of
learning and inference for recognizing fake news without violating user privacy
in a decentralized manner. It consists of distributed nodes that are able to
independently learn and detect fake news on local data. Furthermore, detection
models trained on these nodes can be enhanced through decentralized model
merging. Experimental results demonstrate that the proposed method outperforms
the state-of-the-art decentralized method in regard of detecting fake news on a
benchmark dataset.",None,-1
Improving Cross-Modal Retrieval with Set of Diverse Embeddings,0.498425,"Cross-modal retrieval across image and text modalities is a challenging task
due to its inherent ambiguity: An image often exhibits various situations, and
a caption can be coupled with diverse images. Set-based embedding has been
studied as a solution to this problem. It seeks to encode a sample into a set
of different embedding vectors that capture different semantics of the sample.
In this paper, we present a novel set-based embedding method, which is distinct
from previous work in two aspects. First, we present a new similarity function
called smooth-Chamfer similarity, which is designed to alleviate the side
effects of existing similarity functions for set-based embedding. Second, we
propose a novel set prediction module to produce a set of embedding vectors
that effectively captures diverse semantics of input by the slot attention
mechanism. Our method is evaluated on the COCO and Flickr30K datasets across
different visual backbones, where it outperforms existing methods including
ones that demand substantially larger computation at inference.",None,-1
Two-stage architectural fine-tuning with neural architecture search using early-stopping in image classification,0.274253,"In many deep neural network (DNN) applications, the difficulty of gathering
high-quality data in the industry field hinders the practical use of DNN. Thus,
the concept of transfer learning has emerged, which leverages the pretrained
knowledge of DNNs trained on large-scale datasets. Therefore, this paper
suggests two-stage architectural fine-tuning, inspired by neural architecture
search (NAS). One of main ideas is mutation, which reduces the search cost
using given architectural information. Moreover, early-stopping is considered
which cuts NAS costs by terminating the search process in advance. Experimental
results verify our proposed method reduces 32.4% computational and 22.3%
searching costs.",None,-1
User-Centric Gender Rewriting,0.563221,"In this paper, we define the task of gender rewriting in contexts involving
two users (I and/or You) - first and second grammatical persons with
independent grammatical gender preferences. We focus on Arabic, a
gender-marking morphologically rich language. We develop a multi-step system
that combines the positive aspects of both rule-based and neural rewriting
models. Our results successfully demonstrate the viability of this approach on
a recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5
on a blind test set. Our proposed system improves over previous work on the
first-person-only version of this task, by 3.05 absolute increase in M2 F0.5.
We demonstrate a use case of our gender rewriting system by using it to
post-edit the output of a commercial MT system to provide personalized outputs
based on the users' grammatical gender preferences. We make our code, data, and
models publicly available.",https://github.com/CAMeL-Lab/gender-rewriting/,16583
Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks,0.133459,"The last few years have witnessed an exponential rise in the propagation of
offensive text on social media. Identification of this text with high precision
is crucial for the well-being of society. Most of the existing approaches tend
to give high toxicity scores to innocuous statements (e.g., ""I am a gay man"").
These false positives result from over-generalization on the training data
where specific terms in the statement may have been used in a pejorative sense
(e.g., ""gay""). Emphasis on such words alone can lead to discrimination against
the classes these systems are designed to protect. In this paper, we address
the problem of offensive language detection on Twitter, while also detecting
the type and the target of the offence. We propose a novel approach called
SyLSTM, which integrates syntactic features in the form of the dependency parse
tree of a sentence and semantic features in the form of word embeddings into a
deep learning architecture using a Graph Convolutional Network. Results show
that the proposed approach significantly outperforms the state-of-the-art BERT
model with orders of magnitude fewer number of parameters.",https://github.com/dv-fenix/SyLSTM,-1
DICTDIS: Dictionary Constrained Disambiguation for Improved NMT,0.205796,"Domain-specific neural machine translation (NMT) systems (\eg, in educational
applications) are socially significant with the potential to help make
information accessible to a diverse set of users in multilingual societies. It
is desirable that such NMT systems be lexically constrained and draw from
domain-specific dictionaries. Dictionaries could present multiple candidate
translations for a source word/phrase due to the polysemous nature of words.
The onus is then on the NMT model to choose the contextually most appropriate
candidate. Prior work has largely ignored this problem and focused on the
single candidate constraint setting wherein the target word or phrase is
replaced by a single constraint. In this work we present \dictdis, a lexically
constrained NMT system that disambiguates between multiple candidate
translations derived from dictionaries. We achieve this by augmenting training
data with multiple dictionary candidates to actively encourage disambiguation
during training by implicitly aligning multiple candidate constraints. We
demonstrate the utility of \dictdis\ via extensive experiments on English-Hindi
and English-German sentences in a variety of domains including regulatory,
finance, engineering. We also present comparisons on standard benchmark test
datasets. In comparison with existing approaches for lexically constrained and
unconstrained NMT, we demonstrate superior performance with respect to
constraint copy and disambiguation related measures on all domains while also
obtaining improved fluency of up to 2-3 BLEU points on some domains.",https://github.com/facebookresearch/fairseq/,-1
Transductive CLIP with Class-Conditional Contrastive Learning,0.161872,"Inspired by the remarkable zero-shot generalization capacity of
vision-language pre-trained model, we seek to leverage the supervision from
CLIP model to alleviate the burden of data labeling. However, such supervision
inevitably contains the label noise, which significantly degrades the
discriminative power of the classification model. In this work, we propose
Transductive CLIP, a novel framework for learning a classification network with
noisy labels from scratch. Firstly, a class-conditional contrastive learning
mechanism is proposed to mitigate the reliance on pseudo labels and boost the
tolerance to noisy labels. Secondly, ensemble labels is adopted as a pseudo
label updating strategy to stabilize the training of deep neural networks with
noisy labels. This framework can reduce the impact of noisy labels from CLIP
model effectively by combining both techniques. Experiments on multiple
benchmark datasets demonstrate the substantial improvements over other
state-of-the-art methods.",None,-1
Discriminative Models Can Still Outperform Generative Models in Aspect Based Sentiment Analysis,0.0961222,"Aspect-based Sentiment Analysis (ABSA) helps to explain customers' opinions
towards products and services. In the past, ABSA models were discriminative,
but more recently generative models have been used to generate aspects and
polarities directly from text. In contrast, discriminative models commonly
first select aspects from the text, and then classify the aspect's polarity.
Previous results showed that generative models outperform discriminative models
on several English ABSA datasets. Here, we evaluate and contrast two
state-of-the-art discriminative and generative models in several settings:
cross-lingual, cross-domain, and cross-lingual and domain, to understand
generalizability in settings other than English mono-lingual in-domain. Our
more thorough evaluation shows that, contrary to previous studies,
discriminative models can still outperform generative models in almost all
settings.",https://github.com/google-research/bert/blob/master/multilingual.md,-1
3D-CSL: self-supervised 3D context similarity learning for Near-Duplicate Video Retrieval,0.230504,"In this paper, we introduce 3D-CSL, a compact pipeline for Near-Duplicate
Video Retrieval (NDVR), and explore a novel self-supervised learning strategy
for video similarity learning. Most previous methods only extract video spatial
features from frames separately and then design kinds of complex mechanisms to
learn the temporal correlations among frame features. However, parts of
spatiotemporal dependencies have already been lost. To address this, our 3D-CSL
extracts global spatiotemporal dependencies in videos end-to-end with a 3D
transformer and find a good balance between efficiency and effectiveness by
matching on clip-level. Furthermore, we propose a two-stage self-supervised
similarity learning strategy to optimize the entire network. Firstly, we
propose PredMAE to pretrain the 3D transformer with video prediction task;
Secondly, ShotMix, a novel video-specific augmentation, and FCS loss, a novel
triplet loss, are proposed further promote the similarity learning results. The
experiments on FIVR-200K and CC_WEB_VIDEO demonstrate the superiority and
reliability of our method, which achieves the state-of-the-art performance on
clip-level NDVR.",None,171
Leveraging Action Affinity and Continuity for Semi-supervised Temporal Action Segmentation,0.363848,"We present a semi-supervised learning approach to the temporal action
segmentation task. The goal of the task is to temporally detect and segment
actions in long, untrimmed procedural videos, where only a small set of videos
are densely labelled, and a large collection of videos are unlabelled. To this
end, we propose two novel loss functions for the unlabelled data: an action
affinity loss and an action continuity loss. The action affinity loss guides
the unlabelled samples learning by imposing the action priors induced from the
labelled set. Action continuity loss enforces the temporal continuity of
actions, which also provides frame-wise classification supervision. In
addition, we propose an Adaptive Boundary Smoothing (ABS) approach to build
coarser action boundaries for more robust and reliable learning. The proposed
loss functions and ABS were evaluated on three benchmarks. Results show that
they significantly improved action segmentation performance with a low amount
(5% and 10%) of labelled data and achieved comparable results to full
supervision with 50% labelled data. Furthermore, ABS succeeded in boosting
performance when integrated into fully-supervised learning.",None,-1
Non-Autoregressive Machine Translation: It's Not as Fast as it Seems,0.83748,"Efficient machine translation models are commercially important as they can
increase inference speeds, and reduce costs and carbon emissions. Recently,
there has been much interest in non-autoregressive (NAR) models, which promise
faster translation. In parallel to the research on NAR models, there have been
successful attempts to create optimized autoregressive models as part of the
WMT shared task on efficient translation. In this paper, we point out flaws in
the evaluation methodology present in the literature on NAR models and we
provide a fair comparison between a state-of-the-art NAR model and the
autoregressive submissions to the shared task. We make the case for consistent
evaluation of NAR models, and also for the importance of comparing NAR models
with other widely used methods for improving efficiency. We run experiments
with a connectionist-temporal-classification-based (CTC) NAR model implemented
in C++ and compare it with AR models using wall clock times. Our results show
that, although NAR models are faster on GPUs, with small batch sizes, they are
almost always slower under more realistic usage conditions. We call for more
realistic and extensive evaluation of NAR models in future work.",https://github.com/jindrahelcl/marian-dev,23570
SRL-SOA: Self-Representation Learning with Sparse 1D-Operational Autoencoder for Hyperspectral Image Band Selection,0.551468,"The band selection in the hyperspectral image (HSI) data processing is an
important task considering its effect on the computational complexity and
accuracy. In this work, we propose a novel framework for the band selection
problem: Self-Representation Learning (SRL) with Sparse 1D-Operational
Autoencoder (SOA). The proposed SLR-SOA approach introduces a novel autoencoder
model, SOA, that is designed to learn a representation domain where the data
are sparsely represented. Moreover, the network composes of 1D-operational
layers with the non-linear neuron model. Hence, the learning capability of
neurons (filters) is greatly improved with shallow architectures. Using compact
architectures is especially crucial in autoencoders as they tend to overfit
easily because of their identity mapping objective. Overall, we show that the
proposed SRL-SOA band selection approach outperforms the competing methods over
two HSI data including Indian Pines and Salinas-A considering the achieved land
cover classification accuracies. The software implementation of the SRL-SOA
approach is shared publicly at https://github.com/meteahishali/SRL-SOA.",https://github.com/meteahishali/SRL-SOA,-1
SARAS-Net: Scale and Relation Aware Siamese Network for Change Detection,0.563476,"Change detection (CD) aims to find the difference between two images at
different times and outputs a change map to represent whether the region has
changed or not. To achieve a better result in generating the change map, many
State-of-The-Art (SoTA) methods design a deep learning model that has a
powerful discriminative ability. However, these methods still get lower
performance because they ignore spatial information and scaling changes between
objects, giving rise to blurry or wrong boundaries. In addition to these, they
also neglect the interactive information of two different images. To alleviate
these problems, we propose our network, the Scale and Relation-Aware Siamese
Network (SARAS-Net) to deal with this issue. In this paper, three modules are
proposed that include relation-aware, scale-aware, and cross-transformer to
tackle the problem of scene change detection more effectively. To verify our
model, we tested three public datasets, including LEVIR-CD, WHU-CD, and DSFIN,
and obtained SoTA accuracy. Our code is available at
https://github.com/f64051041/SARAS-Net.",https://github.com/f64051041/SARAS-Net,-1
A Systematic Evaluation of Response Selection for Open Domain Dialogue,0.787324,"Recent progress on neural approaches for language processing has triggered a
resurgence of interest on building intelligent open-domain chatbots. However,
even the state-of-the-art neural chatbots cannot produce satisfying responses
for every turn in a dialog. A practical solution is to generate multiple
response candidates for the same context, and then perform response
ranking/selection to determine which candidate is the best. Previous work in
response selection typically trains response rankers using synthetic data that
is formed from existing dialogs by using a ground truth response as the single
appropriate response and constructing inappropriate responses via random
selection or using adversarial methods. In this work, we curated a dataset
where responses from multiple response generators produced for the same dialog
context are manually annotated as appropriate (positive) and inappropriate
(negative). We argue that such training data better matches the actual use case
examples, enabling the models to learn to rank responses effectively. With this
new dataset, we conduct a systematic evaluation of state-of-the-art methods for
response selection, and demonstrate that both strategies of using multiple
positive candidates and using manually verified hard negative candidates can
bring in significant performance improvement in comparison to using the
adversarial training data, e.g., increase of 3% and 13% in Recall@1 score,
respectively.",https://github.com/golsun/DialogRPT/,-1
DAGAM: Data Augmentation with Generation And Modification,0.070601,"Text classification is a representative downstream task of natural language
processing, and has exhibited excellent performance since the advent of
pre-trained language models based on Transformer architecture. However, in
pre-trained language models, under-fitting often occurs due to the size of the
model being very large compared to the amount of available training data. Along
with significant importance of data collection in modern machine learning
paradigm, studies have been actively conducted for natural language data
augmentation. In light of this, we introduce three data augmentation schemes
that help reduce underfitting problems of large-scale language models.
Primarily we use a generation model for data augmentation, which is defined as
Data Augmentation with Generation (DAG). Next, we augment data using text
modification techniques such as corruption and word order change (Data
Augmentation with Modification, DAM). Finally, we propose Data Augmentation
with Generation And Modification (DAGAM), which combines DAG and DAM techniques
for a boosted performance. We conduct data augmentation for six benchmark
datasets of text classification task, and verify the usefulness of DAG, DAM,
and DAGAM through BERT-based fine-tuning and evaluation, deriving better
results compared to the performance with original datasets.",None,-1
Learning context-aware adaptive solvers to accelerate quadratic programming,0.336278,"Convex quadratic programming (QP) is an important sub-field of mathematical
optimization. The alternating direction method of multipliers (ADMM) is a
successful method to solve QP. Even though ADMM shows promising results in
solving various types of QP, its convergence speed is known to be highly
dependent on the step-size parameter $\rho$. Due to the absence of a general
rule for setting $\rho$, it is often tuned manually or heuristically. In this
paper, we propose CA-ADMM (Context-aware Adaptive ADMM)) which learns to
adaptively adjust $\rho$ to accelerate ADMM. CA-ADMM extracts the
spatio-temporal context, which captures the dependency of the primal and dual
variables of QP and their temporal evolution during the ADMM iterations.
CA-ADMM chooses $\rho$ based on the extracted context. Through extensive
numerical experiments, we validated that CA-ADMM effectively generalizes to
unseen QP problems with different sizes and classes (i.e., having different QP
parameter structures). Furthermore, we verified that CA-ADMM could dynamically
adjust $\rho$ considering the stage of the optimization process to accelerate
the convergence speed further.",None,-1
OakInk: A Large-scale Knowledge Repository for Understanding Hand-Object Interaction,0.750208,"Learning how humans manipulate objects requires machines to acquire knowledge
from two perspectives: one for understanding object affordances and the other
for learning human's interactions based on the affordances. Even though these
two knowledge bases are crucial, we find that current databases lack a
comprehensive awareness of them. In this work, we propose a multi-modal and
rich-annotated knowledge repository, OakInk, for visual and cognitive
understanding of hand-object interactions. We start to collect 1,800 common
household objects and annotate their affordances to construct the first
knowledge base: Oak. Given the affordance, we record rich human interactions
with 100 selected objects in Oak. Finally, we transfer the interactions on the
100 recorded objects to their virtual counterparts through a novel method:
Tink. The recorded and transferred hand-object interactions constitute the
second knowledge base: Ink. As a result, OakInk contains 50,000 distinct
affordance-aware and intent-oriented hand-object interactions. We benchmark
OakInk on pose estimation and grasp generation tasks. Moreover, we propose two
practical applications of OakInk: intent-based interaction generation and
handover generation. Our datasets and source code are publicly available at
https://github.com/lixiny/OakInk.",https://github.com/lixiny/OakInk,-1
Learning Instrumental Variable from Data Fusion for Treatment Effect Estimation,0.10693,"The advent of the big data era brought new opportunities and challenges to
draw treatment effect in data fusion, that is, a mixed dataset collected from
multiple sources (each source with an independent treatment assignment
mechanism). Due to possibly omitted source labels and unmeasured confounders,
traditional methods cannot estimate individual treatment assignment probability
and infer treatment effect effectively. Therefore, we propose to reconstruct
the source label and model it as a Group Instrumental Variable (GIV) to
implement IV-based Regression for treatment effect estimation. In this paper,
we conceptualize this line of thought and develop a unified framework (Meta-EM)
to (1) map the raw data into a representation space to construct Linear Mixed
Models for the assigned treatment variable; (2) estimate the distribution
differences and model the GIV for the different treatment assignment
mechanisms; and (3) adopt an alternating training strategy to iteratively
optimize the representations and the joint distribution to model GIV for IV
regression. Empirical results demonstrate the advantages of our Meta-EM
compared with state-of-the-art methods.",https://github.com/anpwu/meta-em,-1
Full Contextual Attention for Multi-resolution Transformers in Semantic Segmentation,0.177902,"Transformers have proved to be very effective for visual recognition tasks.
In particular, vision transformers construct compressed global representations
through self-attention and learnable class tokens. Multi-resolution
transformers have shown recent successes in semantic segmentation but can only
capture local interactions in high-resolution feature maps. This paper extends
the notion of global tokens to build GLobal Attention Multi-resolution (GLAM)
transformers. GLAM is a generic module that can be integrated into most
existing transformer backbones. GLAM includes learnable global tokens, which
unlike previous methods can model interactions between all image regions, and
extracts powerful representations during training. Extensive experiments show
that GLAM-Swin or GLAM-Swin-UNet exhibit substantially better performances than
their vanilla counterparts on ADE20K and Cityscapes. Moreover, GLAM can be used
to segment large 3D medical images, and GLAM-nnFormer achieves new
state-of-the-art performance on the BCV dataset.",https://github.com/open-mmlab/mmsegmentation,-1
Selective Annotation Makes Language Models Better Few-Shot Learners,0.946758,"Many recent approaches to natural language tasks are built on the remarkable
abilities of large language models. Large language models can perform
in-context learning, where they learn a new task from a few task
demonstrations, without any parameter updates. This work examines the
implications of in-context learning for the creation of datasets for new
natural language tasks. Departing from recent in-context learning methods, we
formulate an annotation-efficient, two-step framework: selective annotation
that chooses a pool of examples to annotate from unlabeled data in advance,
followed by prompt retrieval that retrieves task examples from the annotated
pool at test time. Based on this framework, we propose an unsupervised,
graph-based selective annotation method, voke-k, to select diverse,
representative examples to annotate. Extensive experiments on 10 datasets
(covering classification, commonsense reasoning, dialogue, and text/code
generation) demonstrate that our selective annotation method improves the task
performance by a large margin. On average, vote-k achieves a 12.9%/11.4%
relative gain under an annotation budget of 18/100, as compared to randomly
selecting examples to annotate. Compared to state-of-the-art supervised
finetuning approaches, it yields similar performance with 10-100x less
annotation cost across 10 tasks. We further analyze the effectiveness of our
framework in various scenarios: language models with varying sizes, alternative
selective annotation methods, and cases where there is a test data domain
shift. We hope that our studies will serve as a basis for data annotations as
large language models are increasingly applied to new tasks. Our code is
available at https://github.com/HKUNLP/icl-selective-annotation.",None,104111
Multi-hop Evidence Retrieval for Cross-document Relation Extraction,0.711505,"Relation Extraction (RE) has been extended to cross-document scenarios
because many relations are not simply described in a single document. This
inevitably brings the challenge of efficient open-space evidence retrieval to
support the inference of cross-document relations, along with the challenge of
multi-hop reasoning on top of entities and evidence scattered in an open set of
documents. To combat these challenges, we propose MR.COD (Multi-hop evidence
retrieval for Cross-document relation extraction), which is a multi-hop
evidence retrieval method based on evidence path mining and ranking. We explore
multiple variants of retrievers to show evidence retrieval is essential in
cross-document RE. We also propose a contextual dense retriever for this
setting. Experiments on CodRED show that evidence retrieval with MR.COD
effectively acquires crossdocument evidence and boosts end-to-end RE
performance in both closed and open settings.",https://github.com/luka-group/MrCoD,-1
Spiking Neural Networks for Frame-based and Event-based Single Object Localization,0.853411,"Spiking neural networks have shown much promise as an energy-efficient
alternative to artificial neural networks. However, understanding the impacts
of sensor noises and input encodings on the network activity and performance
remains difficult with common neuromorphic vision baselines like
classification. Therefore, we propose a spiking neural network approach for
single object localization trained using surrogate gradient descent, for frame-
and event-based sensors. We compare our method with similar artificial neural
networks and show that our model has competitive/better performance in
accuracy, robustness against various corruptions, and has lower energy
consumption. Moreover, we study the impact of neural coding schemes for static
images in accuracy, robustness, and energy efficiency. Our observations differ
importantly from previous studies on bio-plausible learning rules, which helps
in the design of surrogate gradient trained architectures, and offers insight
to design priorities in future neuromorphic technologies in terms of noise
characteristics and data encoding methods.",https://github.com/hendrycks/robustness,-1
Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes,0.570749,"This work concerns the development of deep networks that are certifiably
robust to adversarial attacks. Joint robust classification-detection was
recently introduced as a certified defense mechanism, where adversarial
examples are either correctly classified or assigned to the ""abstain"" class. In
this work, we show that such a provable framework can benefit by extension to
networks with multiple explicit abstain classes, where the adversarial examples
are adaptively assigned to those. We show that naively adding multiple abstain
classes can lead to ""model degeneracy"", then we propose a regularization
approach and a training method to counter this degeneracy by promoting full use
of the multiple abstain classes. Our experiments demonstrate that the proposed
approach consistently achieves favorable standard vs. robust verified accuracy
tradeoffs, outperforming state-of-the-art algorithms for various choices of
number of abstain classes.",https://github.com/sinaBaharlouei/MultipleAbstainDetection,-1
Gaussian Multi-head Attention for Simultaneous Machine Translation,0.607292,"Simultaneous machine translation (SiMT) outputs translation while receiving
the streaming source inputs, and hence needs a policy to determine where to
start translating. The alignment between target and source words often implies
the most informative source word for each target word, and hence provides the
unified control over translation quality and latency, but unfortunately the
existing SiMT methods do not explicitly model the alignment to perform the
control. In this paper, we propose Gaussian Multi-head Attention (GMA) to
develop a new SiMT policy by modeling alignment and translation in a unified
manner. For SiMT policy, GMA models the aligned source position of each target
word, and accordingly waits until its aligned position to start translating. To
integrate the learning of alignment into the translation model, a Gaussian
distribution centered on predicted aligned position is introduced as an
alignment-related prior, which cooperates with translation-related soft
attention to determine the final attention. Experiments on En-Vi and De-En
tasks show that our method outperforms strong baselines on the trade-off
between translation and latency.",https://github.com/ictnlp/GMA,-1
DuAT: Dual-Aggregation Transformer Network for Medical Image Segmentation,0.802303,"Transformer-based models have been widely demonstrated to be successful in
computer vision tasks by modelling long-range dependencies and capturing global
representations. However, they are often dominated by features of large
patterns leading to the loss of local details (e.g., boundaries and small
objects), which are critical in medical image segmentation. To alleviate this
problem, we propose a Dual-Aggregation Transformer Network called DuAT, which
is characterized by two innovative designs, namely, the Global-to-Local Spatial
Aggregation (GLSA) and Selective Boundary Aggregation (SBA) modules. The GLSA
has the ability to aggregate and represent both global and local spatial
features, which are beneficial for locating large and small objects,
respectively. The SBA module is used to aggregate the boundary characteristic
from low-level features and semantic information from high-level features for
better preserving boundary details and locating the re-calibration objects.
Extensive experiments in six benchmark datasets demonstrate that our proposed
model outperforms state-of-the-art methods in the segmentation of skin lesion
images, and polyps in colonoscopy images. In addition, our approach is more
robust than existing methods in various challenging situations such as small
object segmentation and ambiguous object boundaries.",None,-1
Pyramidal Denoising Diffusion Probabilistic Models,0.251969,"Recently, diffusion model have demonstrated impressive image generation
performances, and have been extensively studied in various computer vision
tasks. Unfortunately, training and evaluating diffusion models consume a lot of
time and computational resources. To address this problem, here we present a
novel pyramidal diffusion model that can generate high resolution images
starting from much coarser resolution images using a {\em single} score
function trained with a positional embedding. This enables a neural network to
be much lighter and also enables time-efficient image generation without
compromising its performances. Furthermore, we show that the proposed approach
can be also efficiently used for multi-scale super-resolution problem using a
single score function.",https://github.com/openai/improved-diffusion,-1
Natural Language Syntax Complies with the Free-Energy Principle,0.225491,"Natural language syntax yields an unbounded array of hierarchically
structured expressions. We claim that these are used in the service of active
inference in accord with the free-energy principle (FEP). While conceptual
advances alongside modelling and simulation work have attempted to connect
speech segmentation and linguistic communication with the FEP, we extend this
program to the underlying computations responsible for generating syntactic
objects. We argue that recently proposed principles of economy in language
design - such as ""minimal search"" criteria from theoretical syntax - adhere to
the FEP. This affords a greater degree of explanatory power to the FEP - with
respect to higher language functions - and offers linguistics a grounding in
first principles with respect to computability. We show how both tree-geometric
depth and a Kolmogorov complexity estimate (recruiting a Lempel-Ziv compression
algorithm) can be used to accurately predict legal operations on syntactic
workspaces, directly in line with formulations of variational free energy
minimization. This is used to motivate a general principle of language design
that we term Turing-Chomsky Compression (TCC). We use TCC to align concerns of
linguists with the normative account of self-organization furnished by the FEP,
by marshalling evidence from theoretical linguistics and psycholinguistics to
ground core principles of efficient syntactic computation within active
inference.",None,-1
An Implicit Parametric Morphable Dental Model,0.93292,"3D Morphable models of the human body capture variations among subjects and
are useful in reconstruction and editing applications. Current dental models
use an explicit mesh scene representation and model only the teeth, ignoring
the gum. In this work, we present the first parametric 3D morphable dental
model for both teeth and gum. Our model uses an implicit scene representation
and is learned from rigidly aligned scans. It is based on a component-wise
representation for each tooth and the gum, together with a learnable latent
code for each of such components. It also learns a template shape thus enabling
several applications such as segmentation, interpolation, and tooth
replacement. Our reconstruction quality is on par with the most advanced global
implicit representations while enabling novel applications. Project page:
https://vcai.mpi-inf.mpg.de/projects/DMM/",None,-1
Sparse Adversarial Attack in Multi-agent Reinforcement Learning,0.420824,"Cooperative multi-agent reinforcement learning (cMARL) has many real
applications, but the policy trained by existing cMARL algorithms is not robust
enough when deployed. There exist also many methods about adversarial attacks
on the RL system, which implies that the RL system can suffer from adversarial
attacks, but most of them focused on single agent RL. In this paper, we propose
a \textit{sparse adversarial attack} on cMARL systems. We use (MA)RL with
regularization to train the attack policy. Our experiments show that the policy
trained by the current cMARL algorithm can obtain poor performance when only
one or a few agents in the team (e.g., 1 of 8 or 5 of 25) were attacked at a
few timesteps (e.g., attack 3 of total 40 timesteps).",None,-1
Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models,0.754409,"Recently the prompt-tuning paradigm has attracted significant attention. By
only tuning continuous prompts with a frozen pre-trained language model (PLM),
prompt-tuning takes a step towards deploying a shared frozen PLM to serve
numerous downstream tasks. Although prompt-tuning shows good performance on
certain natural language understanding (NLU) tasks, its effectiveness on
natural language generation (NLG) tasks is still under-explored. In this paper,
we argue that one of the factors hindering the development of prompt-tuning on
NLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different
from the pretraining corpus). For example, our preliminary exploration reveals
a large performance gap between prompt-tuning and fine-tuning when unfamiliar
inputs occur frequently in NLG tasks. This motivates us to propose
input-tuning, which fine-tunes both the continuous prompts and the input
representations, leading to a more effective way to adapt unfamiliar inputs to
frozen PLMs. Our proposed input-tuning is conceptually simple and empirically
powerful. Experimental results on seven NLG tasks demonstrate that input-tuning
is significantly and consistently better than prompt-tuning. Furthermore, on
three of these tasks, input-tuning can achieve a comparable or even better
performance than fine-tuning.",None,-1
IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images,0.944665,"We propose a neural inverse rendering pipeline called IRON that operates on
photometric images and outputs high-quality 3D content in the format of
triangle meshes and material textures readily deployable in existing graphics
pipelines. Our method adopts neural representations for geometry as signed
distance fields (SDFs) and materials during optimization to enjoy their
flexibility and compactness, and features a hybrid optimization scheme for
neural SDFs: first, optimize using a volumetric radiance field approach to
recover correct topology, then optimize further using edgeaware physics-based
surface rendering for geometry refinement and disentanglement of materials and
lighting. In the second stage, we also draw inspiration from mesh-based
differentiable rendering, and design a novel edge sampling algorithm for neural
SDFs to further improve performance. We show that our IRON achieves
significantly better inverse rendering quality compared to prior works. Our
project page is here: https://kai-46.github.io/IRON-website/",None,-1
SiNeRF: Sinusoidal Neural Radiance Fields for Joint Pose Estimation and Scene Reconstruction,0.618825,"NeRFmm is the Neural Radiance Fields (NeRF) that deal with Joint Optimization
tasks, i.e., reconstructing real-world scenes and registering camera parameters
simultaneously. Despite NeRFmm producing precise scene synthesis and pose
estimations, it still struggles to outperform the full-annotated baseline on
challenging scenes. In this work, we identify that there exists a systematic
sub-optimality in joint optimization and further identify multiple potential
sources for it. To diminish the impacts of potential sources, we propose
Sinusoidal Neural Radiance Fields (SiNeRF) that leverage sinusoidal activations
for radiance mapping and a novel Mixed Region Sampling (MRS) for selecting ray
batch efficiently. Quantitative and qualitative results show that compared to
NeRFmm, SiNeRF achieves comprehensive significant improvements in image
synthesis quality and pose estimation accuracy. Codes are available at
https://github.com/yitongx/sinerf.",https://github.com/yitongx/sinerf,-1
Deep Multi-Task Networks For Occluded Pedestrian Pose Estimation,0.335536,"Most of the existing works on pedestrian pose estimation do not consider
estimating the pose of an occluded pedestrian, as the annotations of the
occluded parts are not available in relevant automotive datasets. For example,
CityPersons, a well-known dataset for pedestrian detection in automotive scenes
does not provide pose annotations, whereas MS-COCO, a non-automotive dataset,
contains human pose estimation. In this work, we propose a multi-task framework
to extract pedestrian features through detection and instance segmentation
tasks performed separately on these two distributions. Thereafter, an encoder
learns pose specific features using an unsupervised instance-level domain
adaptation method for the pedestrian instances from both distributions. The
proposed framework has improved state-of-the-art performances of pose
estimation, pedestrian detection, and instance segmentation.",None,-1
GANzzle: Reframing jigsaw puzzle solving as a retrieval task using a generative mental image,0.0840902,"Puzzle solving is a combinatorial challenge due to the difficulty of matching
adjacent pieces. Instead, we infer a mental image from all pieces, which a
given piece can then be matched against avoiding the combinatorial explosion.
Exploiting advancements in Generative Adversarial methods, we learn how to
reconstruct the image given a set of unordered pieces, allowing the model to
learn a joint embedding space to match an encoding of each piece to the cropped
layer of the generator. Therefore we frame the problem as a R@1 retrieval task,
and then solve the linear assignment using differentiable Hungarian attention,
making the process end-to-end. In doing so our model is puzzle size agnostic,
in contrast to prior deep learning methods which are single size. We evaluate
on two new large-scale datasets, where our model is on par with deep learning
methods, while generalizing to multiple puzzle sizes.",https://github.com/IIT-PAVIS/,-1
SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations,0.894775,"We present SpeechMatrix, a large-scale multilingual corpus of
speech-to-speech translations mined from real speech of European Parliament
recordings. It contains speech alignments in 136 language pairs with a total of
418 thousand hours of speech. To evaluate the quality of this parallel speech,
we train bilingual speech-to-speech translation models on mined data only and
establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test
sets. Enabled by the multilinguality of SpeechMatrix, we also explore
multilingual speech-to-speech translation, a topic which was addressed by few
other works. We also demonstrate that model pre-training and sparse scaling
using Mixture-of-Experts bring large gains to translation performance. The
mined data and models are freely available.",https://github.com/facebookresearch/fairseq/tree/ust/examples/speech_matrix,-1
Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation,0.420072,"The principal task in supervised neural machine translation (NMT) is to learn
to generate target sentences conditioned on the source inputs from a set of
parallel sentence pairs, and thus produce a model capable of generalizing to
unseen instances. However, it is commonly observed that the generalization
performance of the model is highly influenced by the amount of parallel data
used in training. Although data augmentation is widely used to enrich the
training data, conventional methods with discrete manipulations fail to
generate diverse and faithful training samples. In this paper, we present a
novel data augmentation paradigm termed Continuous Semantic Augmentation
(CsaNMT), which augments each training instance with an adjacency semantic
region that could cover adequate variants of literal expression under the same
meaning. We conduct extensive experiments on both rich-resource and
low-resource settings involving various language pairs, including WMT14
English-{German,French}, NIST Chinese-English and multiple low-resource IWSLT
translation tasks. The provided empirical evidences show that CsaNMT sets a new
level of performance among existing augmentation techniques, improving on the
state-of-the-art by a large margin. The core codes are contained in Appendix E.",https://github.com/pemywei/csanmt,-1
Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,0.754037,"Pre-trained language models (LMs) are shown to easily generate toxic
language. In this work, we systematically explore domain-adaptive training to
reduce the toxicity of language models. We conduct this study on three
dimensions: training corpus, model size, and parameter efficiency. For the
training corpus, we propose to leverage the generative power of LMs and
generate nontoxic datasets for domain-adaptive training, which mitigates the
exposure bias and is shown to be more data-efficient than using a curated
pre-training corpus. We demonstrate that the self-generation method
consistently outperforms the existing baselines across various model sizes on
both automatic and human evaluations, even when it uses a 1/3 smaller training
corpus. We then comprehensively study detoxifying LMs with parameter sizes
ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never
been studied before. We find that i) large LMs have similar toxicity levels as
smaller ones given the same pre-training corpus, and ii) large LMs require more
endeavor to detoxify. We also explore parameter-efficient training methods for
detoxification. We demonstrate that adding and training adapter-only layers in
LMs not only saves a lot of parameters but also achieves a better trade-off
between toxicity and perplexity than whole model adaptation for the large-scale
models.",https://github.com/NVIDIA/Megatron-LM/,-1
Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions,0.123725,"When answering natural language questions over knowledge bases, missing
facts, incomplete schema and limited scope naturally lead to many questions
being unanswerable. While answerability has been explored in other QA settings,
it has not been studied for QA over knowledge bases (KBQA). We create
GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first
identifying various forms of KB incompleteness that make questions
unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset
with only answerable questions). Experimenting with three state-of-the-art KBQA
models, we find that all three models suffer a drop in performance even after
suitable adaptation for unanswerable questions. In addition, these often detect
unanswerability for wrong reasons and find specific forms of unanswerability
particularly difficult to handle. This underscores the need for further
research in making KBQA systems robust to unanswerability",https://github.com/dair-iitd/GrailQAbility,-1
MVP: Robust Multi-View Practice for Driving Action Localization,0.235166,"Distracted driving causes thousands of deaths per year, and how to apply
deep-learning methods to prevent these tragedies has become a crucial problem.
In Track3 of the 6th AI City Challenge, researchers provide a high-quality
video dataset with densely action annotations. Due to the small data scale and
unclear action boundary, the dataset presents a unique challenge to precisely
localize all the different actions and classify their categories. In this
paper, we make good use of the multi-view synchronization among videos, and
conduct robust Multi-View Practice (MVP) for driving action localization. To
avoid overfitting, we fine-tune SlowFast with Kinetics-700 pre-training as the
feature extractor. Then the features of different views are passed to
ActionFormer to generate candidate action proposals. For precisely localizing
all the actions, we design elaborate post-processing, including model voting,
threshold filtering and duplication removal. The results show that our MVP is
robust for driving action localization, which achieves 28.49% F1-score in the
Track3 test set.",None,1527
"AI Technical Considerations: Data Storage, Cloud usage and AI Pipeline",0.111827,"Artificial intelligence (AI), especially deep learning, requires vast amounts
of data for training, testing, and validation. Collecting these data and the
corresponding annotations requires the implementation of imaging biobanks that
provide access to these data in a standardized way. This requires careful
design and implementation based on the current standards and guidelines and
complying with the current legal restrictions. However, the realization of
proper imaging data collections is not sufficient to train, validate and deploy
AI as resource demands are high and require a careful hybrid implementation of
AI pipelines both on-premise and in the cloud. This chapter aims to help the
reader when technical considerations have to be made about the AI environment
by providing a technical background of different concepts and implementation
aspects involved in data storage, cloud usage, and AI pipelines.",None,-1
Structure and position-aware graph neural network for airway labeling,0.0727977,"We present a novel graph-based approach for labeling the anatomical branches
of a given airway tree segmentation. The proposed method formulates airway
labeling as a branch classification problem in the airway tree graph, where
branch features are extracted using convolutional neural networks (CNN) and
enriched using graph neural networks. Our graph neural network is
structure-aware by having each node aggregate information from its local
neighbors and position-aware by encoding node positions in the graph.
  We evaluated the proposed method on 220 airway trees from subjects with
various severity stages of Chronic Obstructive Pulmonary Disease (COPD). The
results demonstrate that our approach is computationally efficient and
significantly improves branch classification performance than the baseline
method. The overall average accuracy of our method reaches 91.18\% for labeling
all 18 segmental airway branches, compared to 83.83\% obtained by the standard
CNN method. We published our source code at
https://github.com/DIAGNijmegen/spgnn. The proposed algorithm is also publicly
available at
https://grand-challenge.org/algorithms/airway-anatomical-labeling/.",https://github.com/DIAGNijmegen/spgnn,72744
Who's the Expert? On Multi-source Belief Change,0.546985,"Consider the following belief change/merging scenario. A group of information
sources gives a sequence of reports about the state of the world at various
instances (e.g. different points in time). The true states at these instances
are unknown to us. The sources have varying levels of expertise, also unknown
to us, and may be knowledgeable on some topics but not others. This may cause
sources to report false statements in areas they lack expertise. What should we
believe on the basis of these reports? We provide a framework in which to
explore this problem, based on an extension of propositional logic with
expertise formulas. This extended language allows us to express beliefs about
the state of the world at each instance, as well as beliefs about the expertise
of each source. We propose several postulates, provide a couple of families of
concrete operators, and analyse these operators with respect to the postulates.",None,-1
VoLux-GAN: A Generative Model for 3D Face Synthesis with HDRI Relighting,0.839875,"We propose VoLux-GAN, a generative framework to synthesize 3D-aware faces
with convincing relighting. Our main contribution is a volumetric HDRI
relighting method that can efficiently accumulate albedo, diffuse and specular
lighting contributions along each 3D ray for any desired HDR environmental map.
Additionally, we show the importance of supervising the image decomposition
process using multiple discriminators. In particular, we propose a data
augmentation technique that leverages recent advances in single image portrait
relighting to enforce consistent geometry, albedo, diffuse and specular
components. Multiple experiments and comparisons with other generative
frameworks show how our model is a step forward towards photorealistic
relightable 3D generative models.",None,10370
Distilling a Pretrained Language Model to a Multilingual ASR Model,0.26337,"Multilingual speech data often suffer from long-tailed language distribution,
resulting in performance degradation. However, multilingual text data is much
easier to obtain, yielding a more useful general language model. Hence, we are
motivated to distill the rich knowledge embedded inside a well-trained teacher
text model to the student speech model. We propose a novel method called the
Distilling a Language model to a Speech model (Distill-L2S), which aligns the
latent representations of two different modalities. The subtle differences are
handled by the shrinking mechanism, nearest-neighbor interpolation, and a
learnable linear projection layer. We demonstrate the effectiveness of our
distillation method by applying it to the multilingual automatic speech
recognition (ASR) task. We distill the transformer-based cross-lingual language
model (InfoXLM) while fine-tuning the large-scale multilingual ASR model
(XLSR-wav2vec 2.0) for each language. We show the superiority of our method on
20 low-resource languages of the CommonVoice dataset with less than 100 hours
of speech data.",https://github.com/juice500ml/xlm_to_xlsr,-1
DraftRec: Personalized Draft Recommendation for Winning in Multi-Player Online Battle Arena Games,0.882195,"This paper presents a personalized character recommendation system for
Multiplayer Online Battle Arena (MOBA) games which are considered as one of the
most popular online video game genres around the world. When playing MOBA
games, players go through a draft stage, where they alternately select a
virtual character to play. When drafting, players select characters by not only
considering their character preferences, but also the synergy and competence of
their team's character combination. However, the complexity of drafting induces
difficulties for beginners to choose the appropriate characters based on the
characters of their team while considering their own champion preferences. To
alleviate this problem, we propose DraftRec, a novel hierarchical model which
recommends characters by considering each player's champion preferences and the
interaction between the players. DraftRec consists of two networks: the player
network and the match network. The player network captures the individual
player's champion preference, and the match network integrates the complex
relationship between the players and their respective champions. We train and
evaluate our model from a manually collected 280,000 matches of League of
Legends and a publicly available 50,000 matches of Dota2. Empirically, our
method achieved state-of-the-art performance in character recommendation and
match outcome prediction task. Furthermore, a comprehensive user survey
confirms that DraftRec provides convincing and satisfying recommendations. Our
code and dataset are available at https://github.com/dojeon-ai/DraftRec.",https://github.com/dojeon-ai/DraftRec,12146
Audio-visual speech enhancement with a deep Kalman filter generative model,0.45108,"Deep latent variable generative models based on variational autoencoder (VAE)
have shown promising performance for audiovisual speech enhancement (AVSE). The
underlying idea is to learn a VAEbased audiovisual prior distribution for clean
speech data, and then combine it with a statistical noise model to recover a
speech signal from a noisy audio recording and video (lip images) of the target
speaker. Existing generative models developed for AVSE do not take into account
the sequential nature of speech data, which prevents them from fully
incorporating the power of visual data. In this paper, we present an
audiovisual deep Kalman filter (AV-DKF) generative model which assumes a
first-order Markov chain model for the latent variables and effectively fuses
audiovisual data. Moreover, we develop an efficient inference methodology to
estimate speech signals at test time. We conduct a set of experiments to
compare different variants of generative models for speech enhancement. The
results demonstrate the superiority of the AV-DKF model compared with both its
audio-only version and the non-sequential audio-only and audiovisual VAE-based
models.",https://github.com/XiaoyuBIE1994/DVAE_SE,-1
"AugShuffleNet: Communicate More, Compute Less",0.0848053,"As a remarkable compact model, ShuffleNetV2 offers a good example to design
efficient ConvNets but its limit is rarely noticed. In this paper, we rethink
the design pattern of ShuffleNetV2 and find that the channel-wise redundancy
problem still constrains the efficiency improvement of Shuffle block in the
wider ShuffleNetV2. To resolve this issue, we propose another augmented variant
of shuffle block in the form of bottleneck-like structure and more implicit
short connections. To verify the effectiveness of this building block, we
further build a more powerful and efficient model family, termed as
AugShuffleNets. Evaluated on the CIFAR-10 and CIFAR-100 datasets, AugShuffleNet
consistently outperforms ShuffleNetV2 in terms of accuracy with less
computational cost and fewer parameter count.",None,-1
Controlling Bias Exposure for Fair Interpretable Predictions,0.799448,"Recent work on reducing bias in NLP models usually focuses on protecting or
isolating information related to a sensitive attribute (like gender or race).
However, when sensitive information is semantically entangled with the task
information of the input, e.g., gender information is predictive for a
profession, a fair trade-off between task performance and bias mitigation is
difficult to achieve. Existing approaches perform this trade-off by eliminating
bias information from the latent space, lacking control over how much bias is
necessarily required to be removed. We argue that a favorable debiasing method
should use sensitive information 'fairly', rather than blindly eliminating it
(Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we
provide a novel debiasing algorithm by adjusting the predictive model's belief
to (1) ignore the sensitive information if it is not useful for the task; (2)
use sensitive information minimally as necessary for the prediction (while also
incurring a penalty). Experimental results on two text classification tasks
(influenced by gender) and an open-ended generation task (influenced by race)
indicate that our model achieves a desirable trade-off between debiasing and
task performance along with producing debiased rationales as evidence.",https://github.com/ZexueHe/interpretable_debiasing,30843
OpenTAL: Towards Open Set Temporal Action Localization,0.842421,"Temporal Action Localization (TAL) has experienced remarkable success under
the supervised learning paradigm. However, existing TAL methods are rooted in
the closed set assumption, which cannot handle the inevitable unknown actions
in open-world scenarios. In this paper, we, for the first time, step toward the
Open Set TAL (OSTAL) problem and propose a general framework OpenTAL based on
Evidential Deep Learning (EDL). Specifically, the OpenTAL consists of
uncertainty-aware action classification, actionness prediction, and temporal
location regression. With the proposed importance-balanced EDL method,
classification uncertainty is learned by collecting categorical evidence
majorly from important samples. To distinguish the unknown actions from
background video frames, the actionness is learned by the positive-unlabeled
learning. The classification uncertainty is further calibrated by leveraging
the guidance from the temporal localization quality. The OpenTAL is general to
enable existing TAL models for open set scenarios, and experimental results on
THUMOS14 and ActivityNet1.3 benchmarks show the effectiveness of our method.
The code and pre-trained models are released at
https://www.rit.edu/actionlab/opental.",https://www.rit.edu/actionlab/opental,-1
Context based lemmatizer for Polish language,0.28529,"Lemmatization is the process of grouping together the inflected forms of a
word so they can be analysed as a single item, identified by the word's lemma,
or dictionary form. In computational linguistics, lemmatisation is the
algorithmic process of determining the lemma of a word based on its intended
meaning. Unlike stemming, lemmatisation depends on correctly identifying the
intended part of speech and meaning of a word in a sentence, as well as within
the larger context surrounding that sentence. As a result, developing efficient
lemmatisation algorithm is the complex task. In recent years it can be observed
that deep learning models used for this task outperform other methods including
machine learning algorithms. In this paper the polish lemmatizer based on
Google T5 model is presented. The training was run with different context
lengths. The model achieves the best results for polish language lemmatisation
process.",None,-1
CITRIS: Causal Identifiability from Temporal Intervened Sequences,0.804722,"Understanding the latent causal factors of a dynamical system from visual
observations is considered a crucial step towards agents reasoning in complex
environments. In this paper, we propose CITRIS, a variational autoencoder
framework that learns causal representations from temporal sequences of images
in which underlying causal factors have possibly been intervened upon. In
contrast to the recent literature, CITRIS exploits temporality and observing
intervention targets to identify scalar and multidimensional causal factors,
such as 3D rotation angles. Furthermore, by introducing a normalizing flow,
CITRIS can be easily extended to leverage and disentangle representations
obtained by already pretrained autoencoders. Extending previous results on
scalar causal factors, we prove identifiability in a more general setting, in
which only some components of a causal factor are affected by interventions. In
experiments on 3D rendered image sequences, CITRIS outperforms previous methods
on recovering the underlying causal variables. Moreover, using pretrained
autoencoders, CITRIS can even generalize to unseen instantiations of causal
factors, opening future research areas in sim-to-real generalization for causal
representation learning.",https://github.com/phlippe/CITRIS,-1
Structured access: an emerging paradigm for safe AI deployment,0.7871,"Structured access is an emerging paradigm for the safe deployment of
artificial intelligence (AI). Instead of openly disseminating AI systems,
developers facilitate controlled, arm's length interactions with their AI
systems. The aim is to prevent dangerous AI capabilities from being widely
accessible, whilst preserving access to AI capabilities that can be used
safely. The developer must both restrict how the AI system can be used, and
prevent the user from circumventing these restrictions through modification or
reverse engineering of the AI system. Structured access is most effective when
implemented through cloud-based AI services, rather than disseminating AI
software that runs locally on users' hardware. Cloud-based interfaces provide
the AI developer greater scope for controlling how the AI system is used, and
for protecting against unauthorized modifications to the system's design. This
chapter expands the discussion of ""publication norms"" in the AI community,
which to date has focused on the question of how the informational content of
AI research projects should be disseminated (e.g., code and models). Although
this is an important question, there are limits to what can be achieved through
the control of information flows. Structured access views AI software not only
as information that can be shared but also as a tool with which users can have
arm's length interactions. There are early examples of structured access being
practiced by AI developers, but there is much room for further development,
both in the functionality of cloud-based interfaces and in the wider
institutional framework.",None,-1
Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on Synthetic Images,0.0588285,"Most recent style-transfer techniques based on generative architectures are
able to obtain synthetic multimedia contents, or commonly called deepfakes,
with almost no artifacts. Researchers already demonstrated that synthetic
images contain patterns that can determine not only if it is a deepfake but
also the generative architecture employed to create the image data itself.
These traces can be exploited to study problems that have never been addressed
in the context of deepfakes. To this aim, in this paper a first approach to
investigate the image ballistics on deepfake images subject to style-transfer
manipulations is proposed. Specifically, this paper describes a study on
detecting how many times a digital image has been processed by a generative
architecture for style transfer. Moreover, in order to address and study
accurately forensic ballistics on deepfake images, some mathematical properties
of style-transfer operations were investigated.",https://github.com/deepfakes/faceswap,-1
Targeted Honeyword Generation with Language Models,0.0214113,"Honeywords are fictitious passwords inserted into databases in order to
identify password breaches. The major difficulty is how to produce honeywords
that are difficult to distinguish from real passwords. Although the generation
of honeywords has been widely investigated in the past, the majority of
existing research assumes attackers have no knowledge of the users. These
honeyword generating techniques (HGTs) may utterly fail if attackers exploit
users' personally identifiable information (PII) and the real passwords include
users' PII. In this paper, we propose to build a more secure and trustworthy
authentication system that employs off-the-shelf pre-trained language models
which require no further training on real passwords to produce honeywords while
retaining the PII of the associated real password, therefore significantly
raising the bar for attackers.
  We conducted a pilot experiment in which individuals are asked to distinguish
between authentic passwords and honeywords when the username is provided for
GPT-3 and a tweaking technique. Results show that it is extremely difficult to
distinguish the real passwords from the artifical ones for both techniques. We
speculate that a larger sample size could reveal a significant difference
between the two HGT techniques, favouring our proposed approach.",None,-1
AdaCat: Adaptive Categorical Discretization for Autoregressive Models,0.279581,"Autoregressive generative models can estimate complex continuous data
distributions, like trajectory rollouts in an RL environment, image
intensities, and audio. Most state-of-the-art models discretize continuous data
into several bins and use categorical distributions over the bins to
approximate the continuous data distribution. The advantage is that the
categorical distribution can easily express multiple modes and are
straightforward to optimize. However, such approximation cannot express sharp
changes in density without using significantly more bins, making it parameter
inefficient. We propose an efficient, expressive, multimodal parameterization
called Adaptive Categorical Discretization (AdaCat). AdaCat discretizes each
dimension of an autoregressive model adaptively, which allows the model to
allocate density to fine intervals of interest, improving parameter efficiency.
AdaCat generalizes both categoricals and quantile-based regression. AdaCat is a
simple add-on to any discretization-based distribution estimator. In
experiments, AdaCat improves density estimation for real-world tabular data,
images, audio, and trajectories, and improves planning in model-based offline
RL.",https://github.com/ColinQiyangLi/AdaCat,-1
Beyond the Return: Off-policy Function Estimation under User-specified Error-measuring Distributions,0.749518,"Off-policy evaluation often refers to two related tasks: estimating the
expected return of a policy and estimating its value function (or other
functions of interest, such as density ratios). While recent works on
marginalized importance sampling (MIS) show that the former can enjoy provable
guarantees under realizable function approximation, the latter is only known to
be feasible under much stronger assumptions such as prohibitively expressive
discriminators. In this work, we provide guarantees for off-policy function
estimation under only realizability, by imposing proper regularization on the
MIS objectives. Compared to commonly used regularization in MIS, our
regularizer is much more flexible and can account for an arbitrary
user-specified distribution, under which the learned function will be close to
the groundtruth. We provide exact characterization of the optimal dual solution
that needs to be realized by the discriminator class, which determines the
data-coverage assumption in the case of value-function learning. As another
surprising observation, the regularizer can be altered to relax the
data-coverage requirement, and completely eliminate it in the ideal case with
strong side information.",None,-1
On the Explainability of Natural Language Processing Deep Models,0.737148,"While there has been a recent explosion of work on ExplainableAI ExAI on deep
models that operate on imagery and tabular data, textual datasets present new
challenges to the ExAI community. Such challenges can be attributed to the lack
of input structure in textual data, the use of word embeddings that add to the
opacity of the models and the difficulty of the visualization of the inner
workings of deep models when they are trained on textual data.
  Lately, methods have been developed to address the aforementioned challenges
and present satisfactory explanations on Natural Language Processing (NLP)
models. However, such methods are yet to be studied in a comprehensive
framework where common challenges are properly stated and rigorous evaluation
practices and metrics are proposed. Motivated to democratize ExAI methods in
the NLP field, we present in this work a survey that studies model-agnostic as
well as model-specific explainability methods on NLP models. Such methods can
either develop inherently interpretable NLP models or operate on pre-trained
models in a post-hoc manner. We make this distinction and we further decompose
the methods into three categories according to what they explain: (1) word
embeddings (input-level), (2) inner workings of NLP models (processing-level)
and (3) models' decisions (output-level). We also detail the different
evaluation approaches interpretability methods in the NLP field. Finally, we
present a case-study on the well-known neural machine translation in an
appendix and we propose promising future research directions for ExAI in the
NLP field.",None,-1
Towards Accurate Open-Set Recognition via Background-Class Regularization,0.450777,"In open-set recognition (OSR), classifiers should be able to reject
unknown-class samples while maintaining high closed-set classification
accuracy. To effectively solve the OSR problem, previous studies attempted to
limit latent feature space and reject data located outside the limited space
via offline analyses, e.g., distance-based feature analyses, or complicated
network architectures. To conduct OSR via a simple inference process (without
offline analyses) in standard classifier architectures, we use distance-based
classifiers instead of conventional Softmax classifiers. Afterwards, we design
a background-class regularization strategy, which uses background-class data as
surrogates of unknown-class ones during training phase. Specifically, we
formulate a novel regularization loss suitable for distance-based classifiers,
which reserves sufficiently large class-wise latent feature spaces for known
classes and forces background-class samples to be located far away from the
limited spaces. Through our extensive experiments, we show that the proposed
method provides robust OSR results, while maintaining high closed-set
classification accuracy.",https://github.com/Anjin-Liu/Openset_Learning_AOSR,-1
Bilingual Synchronization: Restoring Translational Relationships with Editing Operations,0.362788,"Machine Translation (MT) is usually viewed as a one-shot process that
generates the target language equivalent of some source text from scratch. We
consider here a more general setting which assumes an initial target sequence,
that must be transformed into a valid translation of the source, thereby
restoring parallelism between source and target. For this bilingual
synchronization task, we consider several architectures (both autoregressive
and non-autoregressive) and training regimes, and experiment with multiple
practical settings such as simulated interactive MT, translating with
Translation Memory (TM) and TM cleaning. Our results suggest that one single
generic edit-based system, once fine-tuned, can compare with, or even
outperform, dedicated systems specifically trained for these tasks.",None,7732
Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis on the Role of Sentiment in Political Communication,0.444694,"Social media has become extremely influential when it comes to policy making
in modern societies, especially in the western world, where platforms such as
Twitter allow users to follow politicians, thus making citizens more involved
in political discussion. In the same vein, politicians use Twitter to express
their opinions, debate among others on current topics and promote their
political agendas aiming to influence voter behaviour. In this paper, we
attempt to analyse tweets of politicians from three European countries and
explore the virality of their tweets. Previous studies have shown that tweets
conveying negative sentiment are likely to be retweeted more frequently. By
utilising state-of-the-art pre-trained language models, we performed sentiment
analysis on hundreds of thousands of tweets collected from members of
parliament in Greece, Spain and the United Kingdom, including devolved
administrations. We achieved this by systematically exploring and analysing the
differences between influential and less popular tweets. Our analysis indicates
that politicians' negatively charged tweets spread more widely, especially in
more recent times, and highlights interesting differences between political
parties as well as between politicians and the general population.",https://github.com/cardiffnlp/politics-and-virality-twitter,-1
Neural Face Identification in a 2D Wireframe Projection of a Manifold Object,0.72608,"In computer-aided design (CAD) systems, 2D line drawings are commonly used to
illustrate 3D object designs. To reconstruct the 3D models depicted by a single
2D line drawing, an important key is finding the edge loops in the line drawing
which correspond to the actual faces of the 3D object. In this paper, we
approach the classical problem of face identification from a novel data-driven
point of view. We cast it as a sequence generation problem: starting from an
arbitrary edge, we adopt a variant of the popular Transformer model to predict
the edges associated with the same face in a natural order. This allows us to
avoid searching the space of all possible edge loops with various hand-crafted
rules and heuristics as most existing methods do, deal with challenging cases
such as curved surfaces and nested edge loops, and leverage additional cues
such as face types. We further discuss how possibly imperfect predictions can
be used for 3D object reconstruction.",https://github.com/tpaviot/pythonocc,-1
Grounding Aleatoric Uncertainty for Unsupervised Environment Design,0.492585,"Adaptive curricula in reinforcement learning (RL) have proven effective for
producing policies robust to discrepancies between the train and test
environment. Recently, the Unsupervised Environment Design (UED) framework
generalized RL curricula to generating sequences of entire environments,
leading to new methods with robust minimax regret properties. Problematically,
in partially-observable or stochastic settings, optimal policies may depend on
the ground-truth distribution over aleatoric parameters of the environment in
the intended deployment setting, while curriculum learning necessarily shifts
the training distribution. We formalize this phenomenon as curriculum-induced
covariate shift (CICS), and describe how its occurrence in aleatoric parameters
can lead to suboptimal policies. Directly sampling these parameters from the
ground-truth distribution avoids the issue, but thwarts curriculum learning. We
propose SAMPLR, a minimax regret UED method that optimizes the ground-truth
utility function, even when the underlying training data is biased due to CICS.
We prove, and validate on challenging domains, that our approach preserves
optimality under the ground-truth distribution, while promoting robustness
across the full range of environment settings.",https://github.com/facebookresearch/nle,18829
"Perceive, Interact, Predict: Learning Dynamic and Static Clues for End-to-End Motion Prediction",0.828319,"Motion prediction is highly relevant to the perception of dynamic objects and
static map elements in the scenarios of autonomous driving. In this work, we
propose PIP, the first end-to-end Transformer-based framework which jointly and
interactively performs online mapping, object detection and motion prediction.
PIP leverages map queries, agent queries and mode queries to encode the
instance-wise information of map elements, agents and motion intentions,
respectively. Based on the unified query representation, a differentiable
multi-task interaction scheme is proposed to exploit the correlation between
perception and prediction. Even without human-annotated HD map or agent's
historical tracking trajectory as guidance information, PIP realizes end-to-end
multi-agent motion prediction and achieves better performance than
tracking-based and HD-map-based methods. PIP provides comprehensive high-level
information of the driving scene (vectorized static map and dynamic objects
with motion information), and contributes to the downstream planning and
control. Code and models will be released for facilitating further research.",None,-1
Liability regimes in the age of AI: a use-case driven analysis of the burden of proof,0.309521,"New emerging technologies powered by Artificial Intelligence (AI) have the
potential to disruptively transform our societies for the better. In
particular, data-driven learning approaches (i.e., Machine Learning (ML)) have
been a true revolution in the advancement of multiple technologies in various
application domains. But at the same time there is growing concern about
certain intrinsic characteristics of these methodologies that carry potential
risks to both safety and fundamental rights. Although there are mechanisms in
the adoption process to minimize these risks (e.g., safety regulations), these
do not exclude the possibility of harm occurring, and if this happens, victims
should be able to seek compensation. Liability regimes will therefore play a
key role in ensuring basic protection for victims using or interacting with
these systems. However, the same characteristics that make AI systems
inherently risky, such as lack of causality, opacity, unpredictability or their
self and continuous learning capabilities, may lead to considerable
difficulties when it comes to proving causation. This paper presents three case
studies, as well as the methodology to reach them, that illustrate these
difficulties. Specifically, we address the cases of cleaning robots, delivery
drones and robots in education. The outcome of the proposed analysis suggests
the need to revise liability regimes to alleviate the burden of proof on
victims in cases involving AI technologies.",None,-1
TTTFlow: Unsupervised Test-Time Training with Normalizing Flow,0.336732,"A major problem of deep neural networks for image classification is their
vulnerability to domain changes at test-time. Recent methods have proposed to
address this problem with test-time training (TTT), where a two-branch model is
trained to learn a main classification task and also a self-supervised task
used to perform test-time adaptation. However, these techniques require
defining a proxy task specific to the target application. To tackle this
limitation, we propose TTTFlow: a Y-shaped architecture using an unsupervised
head based on Normalizing Flows to learn the normal distribution of latent
features and detect domain shifts in test examples. At inference, keeping the
unsupervised head fixed, we adapt the model to domain-shifted examples by
maximizing the log likelihood of the Normalizing Flow. Our results show that
our method can significantly improve the accuracy with respect to previous
works.",https://github.com/GustavoVargasHakim/TTTFlow.git,-1
LEVEN: A Large-Scale Chinese Legal Event Detection Dataset,0.907219,"Recognizing facts is the most fundamental step in making judgments, hence
detecting events in the legal documents is important to legal case analysis
tasks. However, existing Legal Event Detection (LED) datasets only concern
incomprehensive event types and have limited annotated data, which restricts
the development of LED methods and their downstream applications. To alleviate
these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection
dataset, with 8,116 legal documents and 150,977 human-annotated event mentions
in 108 event types. Not only charge-related events, LEVEN also covers general
events, which are critical for legal case understanding but neglected in
existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and
has dozens of times the data scale of others, which shall significantly promote
the training and evaluation of LED methods. The results of extensive
experiments indicate that LED is challenging and needs further effort.
Moreover, we simply utilize legal events as side information to promote
downstream applications. The method achieves improvements of average 2.2 points
precision in low-resource judgment prediction, and 1.5 points mean average
precision in unsupervised case retrieval, which suggests the fundamentality of
LED. The source code and dataset can be obtained from
https://github.com/thunlp/LEVEN.",https://github.com/thunlp/LEVEN,-1
Multi-Modal Experience Inspired AI Creation,0.0796279,"AI creation, such as poem or lyrics generation, has attracted increasing
attention from both industry and academic communities, with many promising
models proposed in the past few years. Existing methods usually estimate the
outputs based on single and independent visual or textual information. However,
in reality, humans usually make creations according to their experiences, which
may involve different modalities and be sequentially correlated. To model such
human capabilities, in this paper, we define and solve a novel AI creation
problem based on human experiences. More specifically, we study how to generate
texts based on sequential multi-modal information. Compared with the previous
works, this task is much more difficult because the designed model has to well
understand and adapt the semantics among different modalities and effectively
convert them into the output in a sequential manner. To alleviate these
difficulties, we firstly design a multi-channel sequence-to-sequence
architecture equipped with a multi-modal attention network. For more effective
optimization, we then propose a curriculum negative sampling strategy tailored
for the sequential inputs. To benchmark this problem and demonstrate the
effectiveness of our model, we manually labeled a new multi-modal experience
dataset. With this dataset, we conduct extensive experiments by comparing our
model with a series of representative baselines, where we can demonstrate
significant improvements in our model based on both automatic and
human-centered metrics. The code and data are available at:
\url{https://github.com/Aman-4-Real/MMTG}.",https://github.com/Aman-4-Real/MMTG,-1
Contrastive Representation Learning for Conversational Question Answering over Knowledge Graphs,0.18072,"This paper addresses the task of conversational question answering (ConvQA)
over knowledge graphs (KGs). The majority of existing ConvQA methods rely on
full supervision signals with a strict assumption of the availability of gold
logical forms of queries to extract answers from the KG. However, creating such
a gold logical form is not viable for each potential question in a real-world
scenario. Hence, in the case of missing gold logical forms, the existing
information retrieval-based approaches use weak supervision via heuristics or
reinforcement learning, formulating ConvQA as a KG path ranking problem.
Despite missing gold logical forms, an abundance of conversational contexts,
such as entire dialog history with fluent responses and domain information, can
be incorporated to effectively reach the correct KG path. This work proposes a
contrastive representation learning-based approach to rank KG paths
effectively. Our approach solves two key challenges. Firstly, it allows weak
supervision-based learning that omits the necessity of gold annotations.
Second, it incorporates the conversational context (entire dialog history and
domain information) to jointly learn its homogeneous representation with KG
paths to improve contrastive representations for effective path ranking. We
evaluate our approach on standard datasets for ConvQA, on which it
significantly outperforms existing baselines on all domains and overall.
Specifically, in some cases, the Mean Reciprocal Rank (MRR) and Hit@5 ranking
metrics improve by absolute 10 and 18 points, respectively, compared to the
state-of-the-art performance.",https://github.com/endrikacupaj/PRALINE,-1
Imperceptible Adversarial Attack via Invertible Neural Networks,0.250418,"Adding perturbations via utilizing auxiliary gradient information or
discarding existing details of the benign images are two common approaches for
generating adversarial examples. Though visual imperceptibility is the desired
property of adversarial examples, conventional adversarial attacks still
generate traceable adversarial perturbations. In this paper, we introduce a
novel Adversarial Attack via Invertible Neural Networks (AdvINN) method to
produce robust and imperceptible adversarial examples. Specifically, AdvINN
fully takes advantage of the information preservation property of Invertible
Neural Networks and thereby generates adversarial examples by simultaneously
adding class-specific semantic information of the target class and dropping
discriminant information of the original class. Extensive experiments on
CIFAR-10, CIFAR-100, and ImageNet-1K demonstrate that the proposed AdvINN
method can produce less imperceptible adversarial images than the
state-of-the-art methods and AdvINN yields more robust adversarial examples
with high confidence compared to other adversarial attacks.",https://github.com/jjhuangcs/AdvINN,-1
ProsocialDialog: A Prosocial Backbone for Conversational Agents,0.995284,"Most existing dialogue systems fail to respond properly to potentially unsafe
user utterances by either ignoring or passively agreeing with them. To address
this issue, we introduce ProsocialDialog, the first large-scale multi-turn
dialogue dataset to teach conversational agents to respond to problematic
content following social norms. Covering diverse unethical, problematic,
biased, and toxic situations, ProsocialDialog contains responses that encourage
prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb,
RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists
of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue
safety labels accompanied by free-form rationales.
  With this dataset, we introduce a dialogue safety detection module, Canary,
capable of generating RoTs given conversational context, and a
socially-informed dialogue agent, Prost. Empirical results show that Prost
generates more socially acceptable dialogues compared to other state-of-the-art
language and dialogue models in both in-domain and out-of-domain settings.
Additionally, Canary effectively guides conversational agents and off-the-shelf
language models to generate significantly more prosocial responses. Our work
highlights the promise and importance of creating and steering conversational
AI to be socially responsible.",https://hyunw.kim/prosocial-dialog,-1
Pixel-level Correspondence for Self-Supervised Learning from Video,0.193913,"While self-supervised learning has enabled effective representation learning
in the absence of labels, for vision, video remains a relatively untapped
source of supervision. To address this, we propose Pixel-level Correspondence
(PiCo), a method for dense contrastive learning from video. By tracking points
with optical flow, we obtain a correspondence map which can be used to match
local features at different points in time. We validate PiCo on standard
benchmarks, outperforming self-supervised baselines on multiple dense
prediction tasks, without compromising performance on image classification.",https://github.com/open-mmlab/mmflow,-1
Code-DKT: A Code-based Knowledge Tracing Model for Programming Tasks,0.66959,"Knowledge tracing (KT) models are a popular approach for predicting students'
future performance at practice problems using their prior attempts. Though many
innovations have been made in KT, most models including the state-of-the-art
Deep KT (DKT) mainly leverage each student's response either as correct or
incorrect, ignoring its content. In this work, we propose Code-based Deep
Knowledge Tracing (Code-DKT), a model that uses an attention mechanism to
automatically extract and select domain-specific code features to extend DKT.
We compared the effectiveness of Code-DKT against Bayesian and Deep Knowledge
Tracing (BKT and DKT) on a dataset from a class of 50 students attempting to
solve 5 introductory programming assignments. Our results show that Code-DKT
consistently outperforms DKT by 3.07-4.00% AUC across the 5 assignments, a
comparable improvement to other state-of-the-art domain-general KT models over
DKT. Finally, we analyze problem-specific performance through a set of case
studies for one assignment to demonstrate when and how code features improve
Code-DKT's predictions.",https://github.com/YangAzure/Code-DKT,-1
An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification,0.493819,"Non-hierarchical sparse attention Transformer-based models, such as
Longformer and Big Bird, are popular approaches to working with long documents.
There are clear benefits to these approaches compared to the original
Transformer in terms of efficiency, but Hierarchical Attention Transformer
(HAT) models are a vastly understudied alternative. We develop and release
fully pre-trained HAT models that use segment-wise followed by cross-segment
encoders and compare them with Longformer models and partially pre-trained
HATs. In several long document downstream classification tasks, our best HAT
model outperforms equally-sized Longformer models while using 10-20% less GPU
memory and processing documents 40-45% faster. In a series of ablation studies,
we find that HATs perform best with cross-segment contextualization throughout
the model than alternative configurations that implement either early or late
cross-segment contextualization. Our code is on GitHub:
https://github.com/coastalcph/hierarchical-transformers.",https://github.com/coastalcph/hierarchical-transformers,-1
A Realism Metric for Generated LiDAR Point Clouds,0.244729,"A considerable amount of research is concerned with the generation of
realistic sensor data. LiDAR point clouds are generated by complex simulations
or learned generative models. The generated data is usually exploited to enable
or improve downstream perception algorithms. Two major questions arise from
these procedures: First, how to evaluate the realism of the generated data?
Second, does more realistic data also lead to better perception performance?
This paper addresses both questions and presents a novel metric to quantify the
realism of LiDAR point clouds. Relevant features are learned from real-world
and synthetic point clouds by training on a proxy classification task. In a
series of experiments, we demonstrate the application of our metric to
determine the realism of generated LiDAR data and compare the realism
estimation of our metric to the performance of a segmentation model. We confirm
that our metric provides an indication for the downstream segmentation
performance.",https://github.com/PRBonn/lidar-bonnetal,-1
QuadTree Attention for Vision Transformers,0.766959,"Transformers have been successful in many vision tasks, thanks to their
capability of capturing long-range dependency. However, their quadratic
computational complexity poses a major obstacle for applying them to vision
tasks requiring dense predictions, such as object detection, feature matching,
stereo, etc. We introduce QuadTree Attention, which reduces the computational
complexity from quadratic to linear. Our quadtree transformer builds token
pyramids and computes attention in a coarse-to-fine manner. At each level, the
top K patches with the highest attention scores are selected, such that at the
next level, attention is only evaluated within the relevant regions
corresponding to these top K patches. We demonstrate that quadtree attention
achieves state-of-the-art performance in various vision tasks, e.g. with 4.0%
improvement in feature matching on ScanNet, about 50% flops reduction in stereo
matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification,
1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on
semantic segmentation over previous state-of-the-art transformers. The codes
are available at https://github.com/Tangshitao/QuadtreeAttention.",https://github.com/Tangshitao/QuadtreeAttention,-1
LingMess: Linguistically Informed Multi Expert Scorers for Coreference Resolution,0.882429,"While coreference resolution typically involves various linguistic
challenges, recent models are based on a single pairwise scorer for all types
of pairs. We present LingMess, a new coreference model that defines different
categories of coreference cases and optimize multiple pairwise scorers, where
each scorer learns a specific set of linguistic challenges. Our model
substantially improves pairwise scores for most categories and outperforms
cluster-level performance on Ontonotes and 5 additional datasets. Our model is
available in https://github.com/shon-otmazgin/lingmess-coref",https://github.com/shon-otmazgin/lingmess-coref,-1
BoxGraph: Semantic Place Recognition and Pose Estimation from 3D LiDAR,0.780225,"This paper is about extremely robust and lightweight localisation using LiDAR
point clouds based on instance segmentation and graph matching. We model 3D
point clouds as fully-connected graphs of semantically identified components
where each vertex corresponds to an object instance and encodes its shape.
Optimal vertex association across graphs allows for full 6-Degree-of-Freedom
(DoF) pose estimation and place recognition by measuring similarity. This
representation is very concise, condensing the size of maps by a factor of 25
against the state-of-the-art, requiring only 3kB to represent a 1.4MB laser
scan. We verify the efficacy of our system on the SemanticKITTI dataset, where
we achieve a new state-of-the-art in place recognition, with an average of
88.4% recall at 100% precision where the next closest competitor follows with
64.9%. We also show accurate metric pose estimation performance - estimating
6-DoF pose with median errors of 10 cm and 0.33 deg.",None,-1
Multiple Attribute Fairness: Application to Fraud Detection,0.0648778,"We propose a fairness measure relaxing the equality conditions in the popular
equal odds fairness regime for classification. We design an iterative,
model-agnostic, grid-based heuristic that calibrates the outcomes per sensitive
attribute value to conform to the measure. The heuristic is designed to handle
high arity attribute values and performs a per attribute sanitization of
outcomes across different protected attribute values. We also extend our
heuristic for multiple attributes. Highlighting our motivating application,
fraud detection, we show that the proposed heuristic is able to achieve
fairness across multiple values of a single protected attribute, multiple
protected attributes. When compared to current fairness techniques, that focus
on two groups, we achieve comparable performance across several public data
sets.",None,-1
SPICEprop: Backpropagating Errors Through Memristive Spiking Neural Networks,0.244261,"We present a fully memristive spiking neural network (MSNN) consisting of
novel memristive neurons trained using the backpropagation through time (BPTT)
learning rule. Gradient descent is applied directly to the memristive
integrated-and-fire (MIF) neuron designed using analog SPICE circuit models,
which generates distinct depolarization, hyperpolarization, and repolarization
voltage waveforms. Synaptic weights are trained by BPTT using the membrane
potential of the MIF neuron model and can be processed on memristive crossbars.
The natural spiking dynamics of the MIF neuron model are fully differentiable,
eliminating the need for gradient approximations that are prevalent in the
spiking neural network literature. Despite the added complexity of training
directly on SPICE circuit models, we achieve 97.58% accuracy on the MNIST
testing dataset and 75.26% on the Fashion-MNIST testing dataset, the highest
accuracies among all fully MSNNs.",None,-1
SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners,0.539665,"Recently, self-supervised Masked Autoencoders (MAE) have attracted
unprecedented attention for their impressive representation learning ability.
However, the pretext task, Masked Image Modeling (MIM), reconstructs the
missing local patches, lacking the global understanding of the image. This
paper extends MAE to a fully supervised setting by adding a supervised
classification branch, thereby enabling MAE to learn global features from
golden labels effectively. The proposed Supervised MAE (SupMAE) only exploits a
visible subset of image patches for classification, unlike the standard
supervised pre-training where all image patches are used. Through experiments,
we demonstrate that SupMAE is not only more training efficient but it also
learns more robust and transferable features. Specifically, SupMAE achieves
comparable performance with MAE using only 30% of compute when evaluated on
ImageNet with the ViT-B/16 model. SupMAE's robustness on ImageNet variants and
transfer learning performance outperforms MAE and standard supervised
pre-training counterparts. Codes are available at
https://github.com/enyac-group/supmae.",https://github.com/enyac-group/supmae,-1
C3KG: A Chinese Commonsense Conversation Knowledge Graph,0.411014,"Existing commonsense knowledge bases often organize tuples in an isolated
manner, which is deficient for commonsense conversational models to plan the
next steps. To fill the gap, we curate a large-scale multi-turn human-written
conversation corpus, and create the first Chinese commonsense conversation
knowledge graph which incorporates both social commonsense knowledge and dialog
flow information. To show the potential of our graph, we develop a
graph-conversation matching approach, and benchmark two graph-grounded
conversational tasks.",https://github.com/XiaoMi/C3KG,-1
SynSciPass: detecting appropriate uses of scientific text generation,0.167,"Approaches to machine generated text detection tend to focus on binary
classification of human versus machine written text. In the scientific domain
where publishers might use these models to examine manuscripts under
submission, misclassification has the potential to cause harm to authors.
Additionally, authors may appropriately use text generation models such as with
the use of assistive technologies like translation tools. In this setting, a
binary classification scheme might be used to flag appropriate uses of
assistive text generation technology as simply machine generated which is a
cause of concern. In our work, we simulate this scenario by presenting a
state-of-the-art detector trained on the DAGPap22 with machine translated
passages from Scielo and find that the model performs at random. Given this
finding, we develop a framework for dataset development that provides a nuanced
approach to detecting machine generated text by having labels for the type of
technology used such as for translation or paraphrase resulting in the
construction of SynSciPass. By training the same model that performed well on
DAGPap22 on SynSciPass, we show that not only is the model more robust to
domain shifts but also is able to uncover the type of technology used for
machine generated text. Despite this, we conclude that current datasets are
neither comprehensive nor realistic enough to understand how these models would
perform in the wild where manuscript submissions can come from many unknown or
novel distributions, how they would perform on scientific full-texts rather
than small passages, and what might happen when there is a mix of appropriate
and inappropriate uses of natural language generation.",https://github.com/domenicrosati/synscipass,-1
SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model,0.677774,"Data-driven speech processing models usually perform well with a large amount
of text supervision, but collecting transcribed speech data is costly.
Therefore, we propose SpeechCLIP, a novel framework bridging speech and text
through images to enhance speech models without transcriptions. We leverage
state-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images
and spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior
state-of-the-art on image-speech retrieval and performs zero-shot speech-text
retrieval without direct supervision from transcriptions. Moreover, SpeechCLIP
can directly retrieve semantically related keywords from speech.",https://github.com/atosystem/SpeechCLIP,-1
Automatic Creativity Measurement in Scratch Programs Across Modalities,0.27607,"Promoting creativity is considered an important goal of education, but
creativity is notoriously hard to measure.In this paper, we make the journey
fromdefining a formal measure of creativity that is efficientlycomputable to
applying the measure in a practical domain. The measure is general and relies
on coretheoretical concepts in creativity theory, namely fluency, flexibility,
and originality, integratingwith prior cognitive science literature. We adapted
the general measure for projects in the popular visual programming language
Scratch.We designed a machine learning model for predicting the creativity of
Scratch projects, trained and evaluated on human expert creativity assessments
in an extensive user study. Our results show that opinions about creativity in
Scratch varied widely across experts. The automatic creativity assessment
aligned with the assessment of the human experts more than the experts agreed
with each other. This is a first step in providing computational models for
measuring creativity that can be applied to educational technologies, and to
scale up the benefit of creativity education in schools.",None,-1
MatteFormer: Transformer-Based Image Matting via Prior-Tokens,0.953863,"In this paper, we propose a transformer-based image matting model called
MatteFormer, which takes full advantage of trimap information in the
transformer block. Our method first introduces a prior-token which is a global
representation of each trimap region (e.g. foreground, background and unknown).
These prior-tokens are used as global priors and participate in the
self-attention mechanism of each block. Each stage of the encoder is composed
of PAST (Prior-Attentive Swin Transformer) block, which is based on the Swin
Transformer block, but differs in a couple of aspects: 1) It has PA-WSA
(Prior-Attentive Window Self-Attention) layer, performing self-attention not
only with spatial-tokens but also with prior-tokens. 2) It has prior-memory
which saves prior-tokens accumulatively from the previous blocks and transfers
them to the next block. We evaluate our MatteFormer on the commonly used image
matting datasets: Composition-1k and Distinctions-646. Experiment results show
that our proposed method achieves state-of-the-art performance with a large
margin. Our codes are available at https://github.com/webtoon/matteformer.",https://github.com/webtoon/matteformer,-1
Dual-Domain Cross-Iteration Squeeze-Excitation Network for Sparse Reconstruction of Brain MRI,0.1452,"Magnetic resonance imaging (MRI) is one of the most commonly applied tests in
neurology and neurosurgery. However, the utility of MRI is largely limited by
its long acquisition time, which might induce many problems including patient
discomfort and motion artifacts. Acquiring fewer k-space sampling is a
potential solution to reducing the total scanning time. However, it can lead to
severe aliasing reconstruction artifacts and thus affect the clinical
diagnosis. Nowadays, deep learning has provided new insights into the sparse
reconstruction of MRI. In this paper, we present a new approach to this problem
that iteratively fuses the information of k-space and MRI images using novel
dual Squeeze-Excitation Networks and Cross-Iteration Residual Connections. This
study included 720 clinical multi-coil brain MRI cases adopted from the
open-source deidentified fastMRI Dataset. 8-folder downsampling rate was
applied to generate the sparse k-space. Results showed that the average
reconstruction error over 120 testing cases by our proposed method was 2.28%,
which outperformed the existing image-domain prediction (6.03%, p<0.001),
k-space synthesis (6.12%, p<0.001), and dual-domain feature fusion (4.05%,
p<0.001).",None,-1
Improving Point Cloud Based Place Recognition with Ranking-based Loss and Large Batch Training,0.62881,"The paper presents a simple and effective learning-based method for computing
a discriminative 3D point cloud descriptor for place recognition purposes.
Recent state-of-the-art methods have relatively complex architectures such as
multi-scale oyramid of point Transformers combined with a pyramid of feature
aggregation modules. Our method uses a simple and efficient 3D convolutional
feature extraction, based on a sparse voxelized representation, enhanced with
channel attention blocks. We employ recent advances in image retrieval and
propose a modified version of a loss function based on a differentiable average
precision approximation. Such loss function requires training with very large
batches for the best results. This is enabled by using multistaged
backpropagation. Experimental evaluation on the popular benchmarks proves the
effectiveness of our approach, with a consistent improvement over the state of
the art",https://github.com/jac99/MinkLoc3Dv2,-1
Towards Improving Faithfulness in Abstractive Summarization,0.666072,"Despite the success achieved in neural abstractive summarization based on
pre-trained language models, one unresolved issue is that the generated
summaries are not always faithful to the input document. There are two possible
causes of the unfaithfulness problem: (1) the summarization model fails to
understand or capture the gist of the input text, and (2) the model over-relies
on the language model to generate fluent but inadequate words. In this work, we
propose a Faithfulness Enhanced Summarization model (FES), which is designed
for addressing these two problems and improving faithfulness in abstractive
summarization. For the first problem, we propose to use question-answering (QA)
to examine whether the encoder fully grasps the input document and can answer
the questions on the key information in the input. The QA attention on the
proper input words can also be used to stipulate how the decoder should attend
to the source. For the second problem, we introduce a max-margin loss defined
on the difference between the language and the summarization model, aiming to
prevent the overconfidence of the language model. Extensive experiments on two
benchmark summarization datasets, CNN/DM and XSum, demonstrate that our model
significantly outperforms strong baselines. The evaluation of factual
consistency also shows that our model generates more faithful summaries than
baselines.",https://github.com/iriscxy/FES,-1
Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,0.23994,"We propose a novel application of prompting Pre-trained Language Models
(PLMs) to generate analogies and study how to design effective prompts for two
task settings: generating a source concept analogous to a given target concept
(aka Analogous Concept Generation or ACG), and generating an explanation of the
similarity between a given pair of target concept and source concept (aka
Analogous Explanation Generation or AEG). We found that it is feasible to
prompt InstructGPT to generate meaningful analogies and the best prompts tend
to be precise imperative statements especially with a low temperature setting.
We also systematically analyzed the sensitivity of the InstructGPT model to
prompt design, temperature, and injected spelling errors, and found that the
model is particularly sensitive to certain variations (e.g., questions vs.
imperative statements). Further, we conducted human evaluation on 1.4k of the
generated analogies and found that the quality of generations varies
substantially by model size. The largest InstructGPT model can achieve
human-level performance at generating meaningful analogies for a given target
while there is still room for improvement on the AEG task.",https://github.com/Bhaavya/InstructGPT-Analogies,-1
Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity,0.817573,"This paper analyzes three formal models of Transformer encoders that differ
in the form of their self-attention mechanism: unique hard attention (UHAT);
generalized unique hard attention (GUHAT), which generalizes UHAT; and
averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers,
viewed as string acceptors, can only recognize formal languages in the
complexity class AC$^0$, the class of languages recognizable by families of
Boolean circuits of constant depth and polynomial size. This upper bound
subsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages
or the PARITY language, since those languages are outside AC$^0$ (Furst et al.,
1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are
recognizable by AHAT networks, implying that AHAT can recognize languages that
UHAT and GUHAT cannot.",None,20886
Sequential Nature of Recommender Systems Disrupts the Evaluation Process,0.0587957,"Datasets are often generated in a sequential manner, where the previous
samples and intermediate decisions or interventions affect subsequent samples.
This is especially prominent in cases where there are significant human-AI
interactions, such as in recommender systems. To characterize the importance of
this relationship across samples, we propose to use adversarial attacks on
popular evaluation processes. We present sequence-aware boosting attacks and
provide a lower bound on the amount of extra information that can be exploited
from a confidential test set solely based on the order of the observed data. We
use real and synthetic data to test our methods and show that the evaluation
process on the MovieLense-100k dataset can be affected by $\sim1\%$ which is
important when considering the close competition. Codes are publicly available.",https://github.com/alishiraliGit/augmented-boosting-attack,-1
TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages,0.719008,"Recently, the structural reading comprehension (SRC) task on web pages has
attracted increasing research interests. Although previous SRC work has
leveraged extra information such as HTML tags or XPaths, the informative
topology of web pages is not effectively exploited. In this work, we propose a
Topological Information Enhanced model (TIE), which transforms the token-level
task into a tag-level task by introducing a two-stage process (i.e. node
locating and answer refining). Based on that, TIE integrates Graph Attention
Network (GAT) and Pre-trained Language Model (PLM) to leverage the topological
information of both logical structures and spatial structures. Experimental
results demonstrate that our model outperforms strong baselines and achieves
state-of-the-art performances on the web-based SRC benchmark WebSRC at the time
of writing. The code of TIE will be publicly available at
https://github.com/X-LANCE/TIE.",https://github.com/X-LANCE/TIE,-1
PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices,0.871218,"Neural radiance-density field methods have become increasingly popular for
the task of novel-view rendering. Their recent extension to hash-based
positional encoding ensures fast training and inference with visually pleasing
results. However, density-based methods struggle with recovering accurate
surface geometry. Hybrid methods alleviate this issue by optimizing the density
based on an underlying SDF. However, current SDF methods are overly smooth and
miss fine geometric details. In this work, we combine the strengths of these
two lines of work in a novel hash-based implicit surface representation. We
propose improvements to the two areas by replacing the voxel hash encoding with
a permutohedral lattice which optimizes faster, especially for higher
dimensions. We additionally propose a regularization scheme which is crucial
for recovering high-frequency geometric detail. We evaluate our method on
multiple datasets and show that we can recover geometric detail at the level of
pores and wrinkles while using only RGB images for supervision. Furthermore,
using sphere tracing we can render novel views at 30 fps on an RTX 3090. Code
is publicly available at: https://radualexandru.github.io/permuto_sdf",https://radualexandru.github.io/permuto_sdf,-1
Red Teaming Language Models with Language Models,1.0,"Language Models (LMs) often cannot be deployed because of their potential to
harm users in hard-to-predict ways. Prior work identifies harmful behaviors
before deployment by using human annotators to hand-write test cases. However,
human annotation is expensive, limiting the number and diversity of test cases.
In this work, we automatically find cases where a target LM behaves in a
harmful way, by generating test cases (""red teaming"") using another LM. We
evaluate the target LM's replies to generated test questions using a classifier
trained to detect offensive content, uncovering tens of thousands of offensive
replies in a 280B parameter LM chatbot. We explore several methods, from
zero-shot generation to reinforcement learning, for generating test cases with
varying levels of diversity and difficulty. Furthermore, we use prompt
engineering to control LM-generated test cases to uncover a variety of other
harms, automatically finding groups of people that the chatbot discusses in
offensive ways, personal and hospital phone numbers generated as the chatbot's
own contact info, leakage of private training data in generated text, and harms
that occur over the course of a conversation. Overall, LM-based red teaming is
one promising tool (among many needed) for finding and fixing diverse,
undesirable LM behaviors before impacting users.",None,-1
Concept Graph Neural Networks for Surgical Video Understanding,0.585931,"We constantly integrate our knowledge and understanding of the world to
enhance our interpretation of what we see.
  This ability is crucial in application domains which entail reasoning about
multiple entities and concepts, such as AI-augmented surgery. In this paper, we
propose a novel way of integrating conceptual knowledge into temporal analysis
tasks via temporal concept graph networks. In the proposed networks, a global
knowledge graph is incorporated into the temporal analysis of surgical
instances, learning the meaning of concepts and relations as they apply to the
data. We demonstrate our results in surgical video data for tasks such as
verification of critical view of safety, as well as estimation of Parkland
grading scale. The results show that our method improves the recognition and
detection of complex benchmarks as well as enables other analytic applications
of interest.",https://github.com/CAMMA-public/ivtmetrics,-1
Low-Resource Multilingual and Zero-Shot Multispeaker TTS,0.304947,"While neural methods for text-to-speech (TTS) have shown great advances in
modeling multiple speakers, even in zero-shot settings, the amount of data
needed for those approaches is generally not feasible for the vast majority of
the world's over 6,000 spoken languages. In this work, we bring together the
tasks of zero-shot voice cloning and multilingual low-resource TTS. Using the
language agnostic meta learning (LAML) procedure and modifications to a TTS
encoder, we show that it is possible for a system to learn speaking a new
language using just 5 minutes of training data while retaining the ability to
infer the voice of even unseen speakers in the newly learned language. We show
the success of our proposed approach in terms of intelligibility, naturalness
and similarity to target speaker using objective metrics as well as human
studies and provide our code and trained models open source.",https://github.com/DigitalPhonetics/,-1
Guess What Moves: Unsupervised Video and Image Segmentation by Anticipating Motion,0.610219,"Motion, measured via optical flow, provides a powerful cue to discover and
learn objects in images and videos. However, compared to using appearance, it
has some blind spots, such as the fact that objects become invisible if they do
not move. In this work, we propose an approach that combines the strengths of
motion-based and appearance-based segmentation. We propose to supervise an
image segmentation network with the pretext task of predicting regions that are
likely to contain simple motion patterns, and thus likely to correspond to
objects. As the model only uses a single image as input, we can apply it in two
settings: unsupervised video segmentation, and unsupervised image segmentation.
We achieve state-of-the-art results for videos, and demonstrate the viability
of our approach on still images containing novel objects. Additionally we
experiment with different motion models and optical flow backbones and find the
method to be robust to these change. Project page and code available at
https://www.robots.ox.ac.uk/~vgg/research/gwm.",https://www.robots.ox.ac.uk/~vgg/research/gwm,-1
FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation,0.361167,"The speaker-follower models have proven to be effective in
vision-and-language navigation, where a speaker model is used to synthesize new
instructions to augment the training data for a follower navigation model.
However, in many of the previous methods, the generated instructions are not
directly trained to optimize the performance of the follower. In this paper, we
present \textsc{foam}, a \textsc{Fo}llower-\textsc{a}ware speaker
\textsc{M}odel that is constantly updated given the follower feedback, so that
the generated instructions can be more suitable to the current learning state
of the follower. Specifically, we optimize the speaker using a bi-level
optimization framework and obtain its training signals by evaluating the
follower on labeled data. Experimental results on the Room-to-Room and
Room-across-Room datasets demonstrate that our methods can outperform strong
baseline models across settings. Analyses also reveal that our generated
instructions are of higher quality than the baselines.",https://github.com/PlusLabNLP/follower_aware_speaker,-1
Relative Pose from SIFT Features,0.264289,"This paper proposes the geometric relationship of epipolar geometry and
orientation- and scale-covariant, e.g., SIFT, features. We derive a new linear
constraint relating the unknown elements of the fundamental matrix and the
orientation and scale. This equation can be used together with the well-known
epipolar constraint to, e.g., estimate the fundamental matrix from four SIFT
correspondences, essential matrix from three, and to solve the semi-calibrated
case from three correspondences. Requiring fewer correspondences than the
well-known point-based approaches (e.g., 5PT, 6PT and 7PT solvers) for epipolar
geometry estimation makes RANSAC-like randomized robust estimation
significantly faster. The proposed constraint is tested on a number of problems
in a synthetic environment and on publicly available real-world datasets on
more than 80000 image pairs. It is superior to the state-of-the-art in terms of
processing time while often leading to more accurate results.",None,-1
Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality,0.903804,"Recent visuolinguistic pre-trained models show promising progress on various
end tasks such as image retrieval and video captioning. Yet, they fail
miserably on the recently proposed Winoground dataset, which challenges models
to match paired images and English captions, with items constructed to overlap
lexically but differ in meaning (e.g., ""there is a mug in some grass"" vs.
""there is some grass in a mug""). By annotating the dataset using new
fine-grained tags, we show that solving the Winoground task requires not just
compositional language understanding, but a host of other abilities like
commonsense reasoning or locating small, out-of-focus objects in low-resolution
images. In this paper, we identify the dataset's main challenges through a
suite of experiments on related tasks (probing task, image retrieval task),
data augmentation, and manual inspection of the dataset. Our analysis suggests
that a main challenge in visuolinguistic models may lie in fusing visual and
textual representations, rather than in compositional language understanding.
We release our annotation and code at
https://github.com/ajd12342/why-winoground-hard .",https://github.com/ajd12342/why-winoground-hard,-1
SQuId: Measuring Speech Naturalness in Many Languages,0.798448,"Much of text-to-speech research relies on human evaluation, which incurs
heavy costs and slows down the development process. The problem is particularly
acute in heavily multilingual applications, where recruiting and polling judges
can take weeks. We introduce SQuId (Speech Quality Identification), a
multilingual naturalness prediction model trained on over a million ratings and
tested in 65 locales-the largest effort of this type to date. The main insight
is that training one model on many locales consistently outperforms mono-locale
baselines. We present our task, the model, and show that it outperforms a
competitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then
demonstrate the effectiveness of cross-locale transfer during fine-tuning and
highlight its effect on zero-shot locales, i.e., locales for which there is no
fine-tuning data. Through a series of analyses, we highlight the role of
non-linguistic effects such as sound artifacts in cross-locale transfer.
Finally, we present the effect of our design decision, e.g., model size,
pre-training diversity, and language rebalancing with several ablation
experiments.",None,10439
WaveY-Net: Physics-augmented deep learning for high-speed electromagnetic simulation and optimization,0.844433,"The calculation of electromagnetic field distributions within structured
media is central to the optimization and validation of photonic devices. We
introduce WaveY-Net, a hybrid data- and physics-augmented convolutional neural
network that can predict electromagnetic field distributions with ultra fast
speeds and high accuracy for entire classes of dielectric photonic structures.
This accuracy is achieved by training the neural network to learn only the
magnetic near-field distributions of a system and to use a discrete formalism
of Maxwell's equations in two ways: as physical constraints in the loss
function and as a means to calculate the electric fields from the magnetic
fields. As a model system, we construct a surrogate simulator for periodic
silicon nanostructure arrays and show that the high speed simulator can be
directly and effectively used in the local and global freeform optimization of
metagratings. We anticipate that physics-augmented networks will serve as a
viable Maxwell simulator replacement for many classes of photonic systems,
transforming the way they are designed.",None,-1
Understanding Iterative Revision from Human-Written Text,0.981334,"Writing is, by nature, a strategic, adaptive, and more importantly, an
iterative process. A crucial part of writing is editing and revising the text.
Previous works on text revision have focused on defining edit intention
taxonomies within a single domain or developing computational models with a
single level of edit granularity, such as sentence-level edits, which differ
from human's revision cycles. This work describes IteraTeR: the first
large-scale, multi-domain, edit-intention annotated corpus of iteratively
revised text. In particular, IteraTeR is collected based on a new framework to
comprehensively model the iterative text revisions that generalize to various
domains of formal writing, edit intentions, revision depths, and granularities.
When we incorporate our annotated edit intentions, both generative and
edit-based text revision models significantly improve automatic evaluations.
Through our work, we better understand the text revision process, making vital
connections between edit intentions and writing quality, enabling the creation
of diverse corpora to support computational modeling of iterative text
revisions.",https://github.com/vipulraheja/IteraTeR,1339
Socially Intelligent Genetic Agents for the Emergence of Explicit Norms,0.320399,"Norms help regulate a society. Norms may be explicit (represented in
structured form) or implicit. We address the emergence of explicit norms by
developing agents who provide and reason about explanations for norm violations
in deciding sanctions and identifying alternative norms. These agents use a
genetic algorithm to produce norms and reinforcement learning to learn the
values of these norms. We find that applying explanations leads to norms that
provide better cohesion and goal satisfaction for the agents. Our results are
stable for societies with differing attitudes of generosity.",https://github.com/niravajmeri/,31895
Inference with System W Satisfies Syntax Splitting,0.0766911,"In this paper, we investigate inductive inference with system W from
conditional belief bases with respect to syntax splitting. The concept of
syntax splitting for inductive inference states that inferences about
independent parts of the signature should not affect each other. This was
captured in work by Kern-Isberner, Beierle, and Brewka in the form of
postulates for inductive inference operators expressing syntax splitting as a
combination of relevance and independence; it was also shown that c-inference
fulfils syntax splitting, while system P inference and system Z both fail to
satisfy it. System W is a recently introduced inference system for nonmonotonic
reasoning that captures and properly extends system Z as well as c-inference.
We show that system W fulfils the syntax splitting postulates for inductive
inference operators by showing that it satisfies the required properties of
relevance and independence. This makes system W another inference operator
besides c-inference that fully complies with syntax splitting, while in
contrast to c-inference, also extending rational closure.",None,-1
Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution,0.599197,"Generating agents that can achieve zero-shot coordination (ZSC) with unseen
partners is a new challenge in cooperative multi-agent reinforcement learning
(MARL). Recently, some studies have made progress in ZSC by exposing the agents
to diverse partners during the training process. They usually involve self-play
when training the partners, implicitly assuming that the tasks are homogeneous.
However, many real-world tasks are heterogeneous, and hence previous methods
may be inefficient. In this paper, we study the heterogeneous ZSC problem for
the first time and propose a general method based on coevolution, which
coevolves two populations of agents and partners through three sub-processes:
pairing, updating and selection. Experimental results on various heterogeneous
tasks highlight the necessity of considering the heterogeneous setting and
demonstrate that our proposed method is a promising solution for heterogeneous
ZSC tasks.",https://github.com/HumanCompatibleAI/human-aware-rl,-1
CrossRE: A Cross-Domain Dataset for Relation Extraction,0.638642,"Relation Extraction (RE) has attracted increasing attention, but current RE
evaluation is limited to in-domain evaluation setups. Little is known on how
well a RE system fares in challenging, but realistic out-of-distribution
evaluation setups. To address this gap, we propose CrossRE, a new,
freely-available cross-domain benchmark for RE, which comprises six distinct
text domains and includes multi-label annotations. An additional innovation is
that we release meta-data collected during annotation, to include explanations
and flags of difficult instances. We provide an empirical evaluation with a
state-of-the-art model for relation classification. As the meta-data enables us
to shed new light on the state-of-the-art model, we provide a comprehensive
analysis on the impact of difficult cases and find correlations between model
and human annotations. Overall, our empirical investigation highlights the
difficulty of cross-domain RE. We release our dataset, to spur more research in
this direction.",https://github.com/mainlp/CrossRE,-1
Stylized Knowledge-Grounded Dialogue Generation via Disentangled Template Rewriting,0.33315,"Current Knowledge-Grounded Dialogue Generation (KDG) models specialize in
producing rational and factual responses. However, to establish long-term
relationships with users, the KDG model needs the capability to generate
responses in a desired style or attribute. Thus, we study a new problem:
Stylized Knowledge-Grounded Dialogue Generation (SKDG). It presents two
challenges: (1) How to train a SKDG model where no <context, knowledge,
stylized response> triples are available. (2) How to cohere with context and
preserve the knowledge when generating a stylized response. In this paper, we
propose a novel disentangled template rewriting (DTR) method which generates
responses via combing disentangled style templates (from monolingual stylized
corpus) and content templates (from KDG corpus). The entire framework is
end-to-end differentiable and learned without supervision. Extensive
experiments on two benchmarks indicate that DTR achieves a significant
improvement on all evaluation metrics compared with previous state-of-the-art
stylized dialogue generation methods. Besides, DTR achieves comparable
performance with the state-of-the-art KDG methods in standard KDG evaluation
setting.",https://github.com/victorsungo/SKDG-DTR,-1
HyperNST: Hyper-Networks for Neural Style Transfer,0.401853,"We present HyperNST; a neural style transfer (NST) technique for the artistic
stylization of images, based on Hyper-networks and the StyleGAN2 architecture.
Our contribution is a novel method for inducing style transfer parameterized by
a metric space, pre-trained for style-based visual search (SBVS). We show for
the first time that such space may be used to drive NST, enabling the
application and interpolation of styles from an SBVS system. The technical
contribution is a hyper-network that predicts weight updates to a StyleGAN2
pre-trained over a diverse gamut of artistic content (portraits), tailoring the
style parameterization on a per-region basis using a semantic map of the facial
regions. We show HyperNST to exceed state of the art in content preservation
for our stylized content while retaining good style transfer performance.",None,33989
Question rewriting? Assessing its importance for conversational question answering,0.311409,"In conversational question answering, systems must correctly interpret the
interconnected interactions and generate knowledgeable answers, which may
require the retrieval of relevant information from a background repository.
Recent approaches to this problem leverage neural language models, although
different alternatives can be considered in terms of modules for (a)
representing user questions in context, (b) retrieving the relevant background
information, and (c) generating the answer. This work presents a conversational
question answering system designed specifically for the Search-Oriented
Conversational AI (SCAI) shared task, and reports on a detailed analysis of its
question rewriting module. In particular, we considered different variations of
the question rewriting module to evaluate the influence on the subsequent
components, and performed a careful analysis of the results obtained with the
best system configuration. Our system achieved the best performance in the
shared task and our analysis emphasizes the importance of the conversation
context representation for the overall system performance.",None,-1
"Effectiveness of Text, Acoustic, and Lattice-based representations in Spoken Language Understanding tasks",0.0627967,"In this paper, we perform an exhaustive evaluation of different
representations to address the intent classification problem in a Spoken
Language Understanding (SLU) setup. We benchmark three types of systems to
perform the SLU intent detection task: 1) text-based, 2) lattice-based, and a
novel 3) multimodal approach. Our work provides a comprehensive analysis of
what could be the achievable performance of different state-of-the-art SLU
systems under different circumstances, e.g., automatically- vs.
manually-generated transcripts. We evaluate the systems on the publicly
available SLURP spoken language resource corpus. Our results indicate that
using richer forms of Automatic Speech Recognition (ASR) outputs, namely
word-consensus-networks, allows the SLU system to improve in comparison to the
1-best setup (5.5% relative improvement). However, crossmodal approaches, i.e.,
learning from acoustic and text embeddings, obtains performance similar to the
oracle setup, a relative improvement of 17.8% over the 1-best configuration,
being a recommended alternative to overcome the limitations of working with
automatically generated transcripts.",https://github.com/idiap/slu_representations,10700
Self-supervised models of audio effectively explain human cortical responses to speech,0.815524,"Self-supervised language models are very effective at predicting high-level
cortical responses during language comprehension. However, the best current
models of lower-level auditory processing in the human brain rely on either
hand-constructed acoustic filters or representations from supervised audio
neural networks. In this work, we capitalize on the progress of self-supervised
speech representation learning (SSL) to create new state-of-the-art models of
the human auditory system. Compared against acoustic baselines, phonemic
features, and supervised models, representations from the middle layers of
self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently
yield the best prediction performance for fMRI recordings within the auditory
cortex (AC). Brain areas involved in low-level auditory processing exhibit a
preference for earlier SSL model layers, whereas higher-level semantic areas
prefer later layers. We show that these trends are due to the models' ability
to encode information at multiple linguistic levels (acoustic, phonetic, and
lexical) along their representation depth. Overall, these results show that
self-supervised models effectively capture the hierarchy of information
relevant to different stages of speech processing in human cortex.",None,589
Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation,1.0,"A diffusion model learns to predict a vector field of gradients. We propose
to apply chain rule on the learned gradients, and back-propagate the score of a
diffusion model through the Jacobian of a differentiable renderer, which we
instantiate to be a voxel radiance field. This setup aggregates 2D scores at
multiple camera viewpoints into a 3D score, and repurposes a pretrained 2D
model for 3D data generation. We identify a technical challenge of distribution
mismatch that arises in this application, and propose a novel estimation
mechanism to resolve it. We run our algorithm on several off-the-shelf
diffusion image generative models, including the recently released Stable
Diffusion trained on the large-scale LAION dataset.",https://github.com/ashawkey/stable-dreamfusion,-1
Tell Your Story: Task-Oriented Dialogs for Interactive Content Creation,0.168002,"People capture photos and videos to relive and share memories of personal
significance. Recently, media montages (stories) have become a popular mode of
sharing these memories due to their intuitive and powerful storytelling
capabilities. However, creating such montages usually involves a lot of manual
searches, clicks, and selections that are time-consuming and cumbersome,
adversely affecting user experiences.
  To alleviate this, we propose task-oriented dialogs for montage creation as a
novel interactive tool to seamlessly search, compile, and edit montages from a
media collection. To the best of our knowledge, our work is the first to
leverage multi-turn conversations for such a challenging application, extending
the previous literature studying simple media retrieval tasks. We collect a new
dataset C3 (Conversational Content Creation), comprising 10k dialogs
conditioned on media montages simulated from a large media collection.
  We take a simulate-and-paraphrase approach to collect these dialogs to be
both cost and time efficient, while drawing from natural language distribution.
Our analysis and benchmarking of state-of-the-art language models showcase the
multimodal challenges present in the dataset. Lastly, we present a real-world
mobile demo application that shows the feasibility of the proposed work in
real-world applications. Our code and data will be made publicly available.",None,-1
Decay No More: A Persistent Twitter Dataset for Learning Social Meaning,0.253146,"With the proliferation of social media, many studies resort to social media
to construct datasets for developing social meaning understanding systems. For
the popular case of Twitter, most researchers distribute tweet IDs without the
actual text contents due to the data distribution policy of the platform. One
issue is that the posts become increasingly inaccessible over time, which leads
to unfair comparisons and a temporal bias in social media research. To
alleviate this challenge of data decay, we leverage a paraphrase model to
propose a new persistent English Twitter dataset for social meaning (PTSM).
PTSM consists of $17$ social meaning datasets in $10$ categories of tasks. We
experiment with two SOTA pre-trained language models and show that our PTSM can
substitute the actual tweets with paraphrases with marginal performance loss.",https://github.com/chiyuzhang94/PTSM,-1
RuCoLA: Russian Corpus of Linguistic Acceptability,0.738787,"Linguistic acceptability (LA) attracts the attention of the research
community due to its many uses, such as testing the grammatical knowledge of
language models and filtering implausible texts with acceptability classifiers.
However, the application scope of LA in languages other than English is limited
due to the lack of high-quality resources. To this end, we introduce the
Russian Corpus of Linguistic Acceptability (RuCoLA), built from the ground up
under the well-established binary LA approach. RuCoLA consists of $9.8$k
in-domain sentences from linguistic publications and $3.6$k out-of-domain
sentences produced by generative models. The out-of-domain set is created to
facilitate the practical use of acceptability for improving language
generation. Our paper describes the data collection protocol and presents a
fine-grained analysis of acceptability classification experiments with a range
of baseline approaches. In particular, we demonstrate that the most widely used
language models still fall behind humans by a large margin, especially when
detecting morphological and semantic errors. We release RuCoLA, the code of
experiments, and a public leaderboard (rucola-benchmark.com) to assess the
linguistic competence of language models for Russian.",https://github.com/RussianNLP/RuCoLA,-1
Data-efficient End-to-end Information Extraction for Statistical Legal Analysis,0.11551,"Legal practitioners often face a vast amount of documents. Lawyers, for
instance, search for appropriate precedents favorable to their clients, while
the number of legal precedents is ever-growing. Although legal search engines
can assist finding individual target documents and narrowing down the number of
candidates, retrieved information is often presented as unstructured text and
users have to examine each document thoroughly which could lead to information
overloading. This also makes their statistical analysis challenging. Here, we
present an end-to-end information extraction (IE) system for legal documents.
By formulating IE as a generation task, our system can be easily applied to
various tasks without domain-specific engineering effort. The experimental
results of four IE tasks on Korean precedents shows that our IE system can
achieve competent scores (-2.3 on average) compared to the rule-based baseline
with as few as 50 training examples per task and higher score (+5.4 on average)
with 200 examples. Finally, our statistical analysis on two case
categories--drunk driving and fraud--with 35k precedents reveals the resulting
structured information from our IE system faithfully reflects the macroscopic
features of Korean legal system.",https://github.com/heartexlabs/label-studio,-1
Improving Imbalanced Text Classification with Dynamic Curriculum Learning,0.129082,"Recent advances in pre-trained language models have improved the performance
for text classification tasks. However, little attention is paid to the
priority scheduling strategy on the samples during training. Humans acquire
knowledge gradually from easy to complex concepts, and the difficulty of the
same material can also vary significantly in different learning stages.
Inspired by this insights, we proposed a novel self-paced dynamic curriculum
learning (SPDCL) method for imbalanced text classification, which evaluates the
sample difficulty by both linguistic character and model capacity. Meanwhile,
rather than using static curriculum learning as in the existing research, our
SPDCL can reorder and resample training data by difficulty criterion with an
adaptive from easy to hard pace. The extensive experiments on several
classification tasks show the effectiveness of SPDCL strategy, especially for
the imbalanced dataset.",None,-1
Learning Probabilities of Causation from Finite Population Data,0.803088,"This paper deals with the problem of learning the probabilities of causation
of subpopulations given finite population data. The tight bounds of three basic
probabilities of causation, the probability of necessity and sufficiency (PNS),
the probability of sufficiency (PS), and the probability of necessity (PN),
were derived by Tian and Pearl. However, obtaining the bounds for each
subpopulation requires experimental and observational distributions of each
subpopulation, which is usually impractical to estimate given finite population
data. We propose a machine learning model that helps to learn the bounds of the
probabilities of causation for subpopulations given finite population data. We
further show by a simulated study that the machine learning model is able to
learn the bounds of PNS for 32768 subpopulations with only knowing roughly 500
of them from the finite population data.",None,-1
Overview of the Shared Task on Fake News Detection in Urdu at FIRE 2021,0.843807,"Automatic detection of fake news is a highly important task in the
contemporary world. This study reports the 2nd shared task called
UrduFake@FIRE2021 on identifying fake news detection in Urdu. The goal of the
shared task is to motivate the community to come up with efficient methods for
solving this vital problem, particularly for the Urdu language. The task is
posed as a binary classification problem to label a given news article as a
real or a fake news article. The organizers provide a dataset comprising news
in five domains: (i) Health, (ii) Sports, (iii) Showbiz, (iv) Technology, and
(v) Business, split into training and testing sets. The training set contains
1300 annotated news articles -- 750 real news, 550 fake news, while the testing
set contains 300 news articles -- 200 real, 100 fake news. 34 teams from 7
different countries (China, Egypt, Israel, India, Mexico, Pakistan, and UAE)
registered to participate in the UrduFake@FIRE2021 shared task. Out of those,
18 teams submitted their experimental results, and 11 of those submitted their
technical reports, which is substantially higher compared to the UrduFake
shared task in 2020 when only 6 teams submitted their technical reports. The
technical reports submitted by the participants demonstrated different data
representation techniques ranging from count-based BoW features to word vector
embeddings as well as the use of numerous machine learning algorithms ranging
from traditional SVM to various neural network architectures including
Transformers such as BERT and RoBERTa. In this year's competition, the best
performing system obtained an F1-macro score of 0.679, which is lower than the
past year's best result of 0.907 F1-macro. Admittedly, while training sets from
the past and the current years overlap to a large extent, the testing set
provided this year is completely different.",https://github.com/MaazAmjad/Urdu-Fake-news-detection-FIRE2021,-1
Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models,0.958189,"NLP models trained on text have been shown to reproduce human stereotypes,
which can magnify harms to marginalized groups when systems are deployed at
scale. We adapt the Agency-Belief-Communion (ABC) stereotype model of Koch et
al. (2016) from social psychology as a framework for the systematic study and
discovery of stereotypic group-trait associations in language models (LMs). We
introduce the sensitivity test (SeT) for measuring stereotypical associations
from language models. To evaluate SeT and other measures using the ABC model,
we collect group-trait judgments from U.S.-based subjects to compare with
English LM stereotypes. Finally, we extend this framework to measure LM
stereotyping of intersectional identities.",https://github.com/TristaCao/U.S_Stereotypes,-1
RELIC: Retrieving Evidence for Literary Claims,0.768384,"Humanities scholars commonly provide evidence for claims that they make about
a work of literature (e.g., a novel) in the form of quotations from the work.
We collect a large-scale dataset (RELiC) of 78K literary quotations and
surrounding critical analysis and use it to formulate the novel task of
literary evidence retrieval, in which models are given an excerpt of literary
analysis surrounding a masked quotation and asked to retrieve the quoted
passage from the set of all passages in the work. Solving this retrieval task
requires a deep understanding of complex literary and linguistic phenomena,
which proves challenging to methods that overwhelmingly rely on lexical and
semantic similarity matching. We implement a RoBERTa-based dense passage
retriever for this task that outperforms existing pretrained information
retrieval baselines; however, experiments and analysis by human domain experts
indicate that there is substantial room for improvement over our dense
retriever.",None,-1
Social Choice Around the Block: On the Computational Social Choice of Blockchain,0.621248,"One of the most innovative aspects of blockchain technology consists in the
introduction of an incentive layer to regulate the behavior of distributed
protocols. The designer of a blockchain system faces therefore issues that are
akin to those relevant for the design of economic mechanisms, and faces them in
a computational setting. From this perspective the present paper argues for the
importance of computational social choice in blockchain research. It identifies
a few challenges at the interface of the two fields that illustrate the strong
potential for cross-fertilization between them.",None,-1
Soft Diffusion: Score Matching for General Corruptions,0.963586,"We define a broader family of corruption processes that generalizes
previously known diffusion models. To reverse these general diffusions, we
propose a new objective called Soft Score Matching that provably learns the
score function for any linear corruption process and yields state of the art
results for CelebA. Soft Score Matching incorporates the degradation process in
the network. Our new loss trains the model to predict a clean image,
\textit{that after corruption}, matches the diffused observation. We show that
our objective learns the gradient of the likelihood under suitable regularity
conditions for a family of corruption processes. We further develop a
principled way to select the corruption levels for general diffusion processes
and a novel sampling method that we call Momentum Sampler. We show
experimentally that our framework works for general linear corruption
processes, such as Gaussian blur and masking. We achieve state-of-the-art FID
score $1.85$ on CelebA-64, outperforming all previous linear diffusion models.
We also show significant computational benefits compared to vanilla denoising
diffusion.",None,28054
FedX: Unsupervised Federated Learning with Cross Knowledge Distillation,0.761076,"This paper presents FedX, an unsupervised federated learning framework. Our
model learns unbiased representation from decentralized and heterogeneous local
data. It employs a two-sided knowledge distillation with contrastive learning
as a core component, allowing the federated system to function without
requiring clients to share any data features. Furthermore, its adaptable
architecture can be used as an add-on module for existing unsupervised
algorithms in federated settings. Experiments show that our model improves
performance significantly (1.58--5.52pp) on five unsupervised algorithms.",https://github.com/Sungwon-Han/FEDX,-1
Penalized Proximal Policy Optimization for Safe Reinforcement Learning,0.774289,"Safe reinforcement learning aims to learn the optimal policy while satisfying
safety constraints, which is essential in real-world applications. However,
current algorithms still struggle for efficient policy updates with hard
constraint satisfaction. In this paper, we propose Penalized Proximal Policy
Optimization (P3O), which solves the cumbersome constrained policy iteration
via a single minimization of an equivalent unconstrained problem. Specifically,
P3O utilizes a simple-yet-effective penalty function to eliminate cost
constraints and removes the trust-region constraint by the clipped surrogate
objective. We theoretically prove the exactness of the proposed method with a
finite penalty factor and provide a worst-case analysis for approximate error
when evaluated on sample trajectories. Moreover, we extend P3O to more
challenging multi-constraint and multi-agent scenarios which are less studied
in previous work. Extensive experiments show that P3O outperforms
state-of-the-art algorithms with respect to both reward improvement and
constraint satisfaction on a set of constrained locomotive tasks.",https://github.com/openai/safety-starter-agents,-1
RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization,0.870943,"6-DoF object pose estimation from a monocular image is challenging, and a
post-refinement procedure is generally needed for high-precision estimation. In
this paper, we propose a framework based on a recurrent neural network (RNN)
for object pose refinement, which is robust to erroneous initial poses and
occlusions. During the recurrent iterations, object pose refinement is
formulated as a non-linear least squares problem based on the estimated
correspondence field (between a rendered image and the observed image). The
problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm
enabling end-to-end training. The correspondence field estimation and pose
refinement are conducted alternatively in each iteration to recover the object
poses. Furthermore, to improve the robustness to occlusion, we introduce a
consistency-check mechanism based on the learned descriptors of the 3D model
and observed 2D images, which downweights the unreliable correspondences during
pose optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and
YCB-Video datasets validate the effectiveness of our method and demonstrate
state-of-the-art performance.",https://github.com/DecaYale/RNNPose,-1
Prompting Is Programming: A Query Language for Large Language Models,0.741358,"Large language models have demonstrated outstanding performance on a wide
range of tasks such as question answering and code generation. On a high level,
given an input, a language model can be used to automatically complete the
sequence in a statistically-likely way. Based on this, users prompt these
models with language instructions or examples, to implement a variety of
downstream tasks. Advanced prompting methods can even imply interaction between
the language model, a user, and external tools such as calculators. However, to
obtain state-of-the-art performance or adapt language models for specific
tasks, complex task- and model-specific programs have to be implemented, which
may still require ad-hoc interaction.
  Based on this, we present the novel idea of Language Model Programming (LMP).
LMP generalizes language model prompting from pure text prompts to an intuitive
combination of text prompting and scripting. Additionally, LMP allows
constraints to be specified over the language model output. This enables easy
adaption to many tasks while abstracting language model internals and providing
high-level semantics.
  To enable LMP, we implement LMQL(short for Language Model Query Language),
which leverages the constraints and control flow from an LMP prompt to generate
an efficient inference procedure that minimizes the number of expensive calls
to the underlying language model.
  We show that LMQL can capture a wide range of state-of-the-art prompting
methods in an intuitive way, especially facilitating interactive flows that are
challenging to implement with existing high-level APIs. Our evaluation shows
that we retain or increase the accuracy on several downstream tasks, while also
significantly reducing the required amount of computation or cost in the case
of pay-to-use APIs (26-85% cost savings).",https://github.com/eth-sri/lmql,-1
Object Level Depth Reconstruction for Category Level 6D Object Pose Estimation From Monocular RGB Image,0.746049,"Recently, RGBD-based category-level 6D object pose estimation has achieved
promising improvement in performance, however, the requirement of depth
information prohibits broader applications. In order to relieve this problem,
this paper proposes a novel approach named Object Level Depth reconstruction
Network (OLD-Net) taking only RGB images as input for category-level 6D object
pose estimation. We propose to directly predict object-level depth from a
monocular RGB image by deforming the category-level shape prior into
object-level depth and the canonical NOCS representation. Two novel modules
named Normalized Global Position Hints (NGPH) and Shape-aware Decoupled Depth
Reconstruction (SDDR) module are introduced to learn high fidelity object-level
depth and delicate shape representations. At last, the 6D object pose is solved
by aligning the predicted canonical representation with the back-projected
object-level depth. Extensive experiments on the challenging CAMERA25 and
REAL275 datasets indicate that our model, though simple, achieves
state-of-the-art performance.",None,-1
Safe Policy Improvement Approaches on Discrete Markov Decision Processes,0.329059,"Safe Policy Improvement (SPI) aims at provable guarantees that a learned
policy is at least approximately as good as a given baseline policy. Building
on SPI with Soft Baseline Bootstrapping (Soft-SPIBB) by Nadjahi et al., we
identify theoretical issues in their approach, provide a corrected theory, and
derive a new algorithm that is provably safe on finite Markov Decision
Processes (MDP). Additionally, we provide a heuristic algorithm that exhibits
the best performance among many state of the art SPI algorithms on two
different benchmarks. Furthermore, we introduce a taxonomy of SPI algorithms
and empirically show an interesting property of two classes of SPI algorithms:
while the mean performance of algorithms that incorporate the uncertainty as a
penalty on the action-value is higher, actively restricting the set of policies
more consistently produces good policies and is, thus, safer.",None,-1
Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence,0.961656,"AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem
to test systems on various language-related capabilities. In this paper, we
frame D&D specifically as a dialogue system challenge, where the tasks are to
both generate the next conversational turn in the game and predict the state of
the game given the dialogue history. We create a gameplay dataset consisting of
nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns,
500,000 dice rolls, and 58 million words. We automatically annotate the data
with partial state information about the game play. We train a large language
model (LM) to generate the next game turn, conditioning it on different
information. The LM can respond as a particular character or as the player who
runs the game--i.e., the Dungeon Master (DM). It is trained to produce dialogue
that is either in-character (roleplaying in the fictional world) or
out-of-character (discussing rules or strategy). We perform a human evaluation
to determine what factors make the generated output plausible and interesting.
We further perform an automatic evaluation to determine how well the model can
predict the game state given the history and examine how well tracking the game
state improves its ability to produce plausible conversational output.",https://www.cis.upenn.edu/~ccb/dnd-data.html,-1
Efficient universal shuffle attack for visual object tracking,0.622087,"Recently, adversarial attacks have been applied in visual object tracking to
deceive deep trackers by injecting imperceptible perturbations into video
frames. However, previous work only generates the video-specific perturbations,
which restricts its application scenarios. In addition, existing attacks are
difficult to implement in reality due to the real-time of tracking and the
re-initialization mechanism. To address these issues, we propose an offline
universal adversarial attack called Efficient Universal Shuffle Attack. It
takes only one perturbation to cause the tracker malfunction on all videos. To
improve the computational efficiency and attack performance, we propose a
greedy gradient strategy and a triple loss to efficiently capture and attack
model-specific feature representations through the gradients. Experimental
results show that EUSA can significantly reduce the performance of
state-of-the-art trackers on OTB2015 and VOT2018.",None,-1
Improving Covariance Conditioning of the SVD Meta-layer by Orthogonality,0.195817,"Inserting an SVD meta-layer into neural networks is prone to make the
covariance ill-conditioned, which could harm the model in the training
stability and generalization abilities. In this paper, we systematically study
how to improve the covariance conditioning by enforcing orthogonality to the
Pre-SVD layer. Existing orthogonal treatments on the weights are first
investigated. However, these techniques can improve the conditioning but would
hurt the performance. To avoid such a side effect, we propose the Nearest
Orthogonal Gradient (NOG) and Optimal Learning Rate (OLR). The effectiveness of
our methods is validated in two applications: decorrelated Batch Normalization
(BN) and Global Covariance Pooling (GCP). Extensive experiments on visual
recognition demonstrate that our methods can simultaneously improve the
covariance conditioning and generalization. Moreover, the combinations with
orthogonal weight can further boost the performances.",https://github.com/KingJamesSong/OrthoImproveCond,-1
Privacy-Preserving Personalized Fitness Recommender System (P3FitRec): A Multi-level Deep Learning Approach,0.305858,"Recommender systems have been successfully used in many domains with the help
of machine learning algorithms. However, such applications tend to use
multi-dimensional user data, which has raised widespread concerns about the
breach of users privacy. Meanwhile, wearable technologies have enabled users to
collect fitness-related data through embedded sensors to monitor their
conditions or achieve personalized fitness goals. In this paper, we propose a
novel privacy-aware personalized fitness recommender system. We introduce a
multi-level deep learning framework that learns important features from a
large-scale real fitness dataset that is collected from wearable IoT devices to
derive intelligent fitness recommendations. Unlike most existing approaches,
our approach achieves personalization by inferring the fitness characteristics
of users from sensory data and thus minimizing the need for explicitly
collecting user identity or biometric information, such as name, age, height,
weight. In particular, our proposed models and algorithms predict (a)
personalized exercise distance recommendations to help users to achieve target
calories, (b) personalized speed sequence recommendations to adjust exercise
speed given the nature of the exercise and the chosen route, and (c)
personalized heart rate sequence to guide the user of the potential health
status for future exercises. Our experimental evaluation on a real-world Fitbit
dataset demonstrated high accuracy in predicting exercise distance, speed
sequence, and heart rate sequence compared to similar studies. Furthermore, our
approach is novel compared to existing studies as it does not require
collecting and using users sensitive information, and thus it preserves the
users privacy.",None,-1
SimSR: Simple Distance-based State Representation for Deep Reinforcement Learning,0.120017,"This work explores how to learn robust and generalizable state representation
from image-based observations with deep reinforcement learning methods.
Addressing the computational complexity, stringent assumptions and
representation collapse challenges in existing work of bisimulation metric, we
devise Simple State Representation (SimSR) operator. SimSR enables us to design
a stochastic approximation method that can practically learn the mapping
functions (encoders) from observations to latent representation space. In
addition to the theoretical analysis and comparison with the existing work, we
experimented and compared our work with recent state-of-the-art solutions in
visual MuJoCo tasks. The results shows that our model generally achieves better
performance and has better robustness and good generalization.",https://github.com/bit1029public/SimSR,9769
Guiding Attention using Partial-Order Relationships for Image Captioning,0.245772,"The use of attention models for automated image captioning has enabled many
systems to produce accurate and meaningful descriptions for images. Over the
years, many novel approaches have been proposed to enhance the attention
process using different feature representations. In this paper, we extend this
approach by creating a guided attention network mechanism, that exploits the
relationship between the visual scene and text-descriptions using spatial
features from the image, high-level information from the topics, and temporal
context from caption generation, which are embedded together in an ordered
embedding space. A pairwise ranking objective is used for training this
embedding space which allows similar images, topics and captions in the shared
semantic space to maintain a partial order in the visual-semantic hierarchy and
hence, helps the model to produce more visually accurate captions. The
experimental results based on MSCOCO dataset shows the competitiveness of our
approach, with many state-of-the-art models on various evaluation metrics.",None,-1
Power Grid Congestion Management via Topology Optimization with AlphaZero,0.902478,"The energy sector is facing rapid changes in the transition towards clean
renewable sources. However, the growing share of volatile, fluctuating
renewable generation such as wind or solar energy has already led to an
increase in power grid congestion and network security concerns. Grid operators
mitigate these by modifying either generation or demand (redispatching,
curtailment, flexible loads). Unfortunately, redispatching of fossil generators
leads to excessive grid operation costs and higher emissions, which is in
direct opposition to the decarbonization of the energy sector. In this paper,
we propose an AlphaZero-based grid topology optimization agent as a non-costly,
carbon-free congestion management alternative. Our experimental evaluation
confirms the potential of topology optimization for power grid operation,
achieves a reduction of the average amount of required redispatching by 60%,
and shows the interoperability with traditional congestion management methods.
Our approach also ranked 1st in the WCCI 2022 Learning to Run a Power Network
(L2RPN) competition. Based on our findings, we identify and discuss open
research problems as well as technical challenges for a productive system on a
real power grid.",https://github.com/enlite-ai/maze-l2rpn-2022-submission,-1
SERE: Exploring Feature Self-relation for Self-supervised Transformer,0.189226,"Learning representations with self-supervision for convolutional networks
(CNN) has been validated to be effective for vision tasks. As an alternative to
CNN, vision transformers (ViT) have strong representation ability with spatial
self-attention and channel-level feedforward networks. Recent works reveal that
self-supervised learning helps unleash the great potential of ViT. Still, most
works follow self-supervised strategies designed for CNN, e.g., instance-level
discrimination of samples, but they ignore the properties of ViT. We observe
that relational modeling on spatial and channel dimensions distinguishes ViT
from other networks. To enforce this property, we explore the feature
SElf-RElation (SERE) for training self-supervised ViT. Specifically, instead of
conducting self-supervised learning solely on feature embeddings from multiple
views, we utilize the feature self-relations, i.e., spatial/channel
self-relations, for self-supervised learning. Self-relation based learning
further enhances the relation modeling ability of ViT, resulting in stronger
representations that stably improve performance on multiple downstream tasks.
Our source code is publicly available at: https://github.com/MCG-NKU/SERE.",https://github.com/MCG-NKU/SERE,-1
Natural Language Processing for Cognitive Analysis of Emotions,0.23399,"Emotion analysis in texts suffers from two major limitations: annotated
gold-standard corpora are mostly small and homogeneous, and emotion
identification is often simplified as a sentence-level classification problem.
To address these issues, we introduce a new annotation scheme for exploring
emotions and their causes, along with a new French dataset composed of
autobiographical accounts of an emotional scene. The texts were collected by
applying the Cognitive Analysis of Emotions developed by A. Finkel to help
people improve on their emotion management. The method requires the manual
analysis of an emotional event by a coach trained in Cognitive Analysis. We
present a rule-based approach to automatically annotate emotions and their
semantic roles (e.g. emotion causes) to facilitate the identification of
relevant aspects by the coach. We investigate future directions for emotion
analysis using graph structures.",https://github.com/pandora-intelligence/crosslingual-coreference,6994
Blur Interpolation Transformer for Real-World Motion from Blur,0.789912,"This paper studies the challenging problem of recovering motion from blur,
also known as joint deblurring and interpolation or blur temporal
super-resolution. The challenges are twofold: 1) the current methods still
leave considerable room for improvement in terms of visual quality even on the
synthetic dataset, and 2) poor generalization to real-world data. To this end,
we propose a blur interpolation transformer (BiT) to effectively unravel the
underlying temporal correlation encoded in blur. Based on multi-scale residual
Swin transformer blocks, we introduce dual-end temporal supervision and
temporally symmetric ensembling strategies to generate effective features for
time-varying motion rendering. In addition, we design a hybrid camera system to
collect the first real-world dataset of one-to-many blur-sharp video pairs.
Experimental results show that BiT has a significant gain over the
state-of-the-art methods on the public dataset Adobe240. Besides, the proposed
real-world dataset effectively helps the model generalize well to real blurry
scenarios. Code and data are available at https://github.com/zzh-tech/BiT.",https://github.com/zzh-tech/BiT,-1
Speciesist Language and Nonhuman Animal Bias in English Masked Language Models,0.189394,"Various existing studies have analyzed what social biases are inherited by
NLP models. These biases may directly or indirectly harm people, therefore
previous studies have focused only on human attributes. However, until recently
no research on social biases in NLP regarding nonhumans existed. In this paper,
we analyze biases to nonhuman animals, i.e. speciesist bias, inherent in
English Masked Language Models such as BERT. We analyzed speciesist bias
against 46 animal names using template-based and corpus-extracted sentences
containing speciesist (or non-speciesist) language. We found that pre-trained
masked language models tend to associate harmful words with nonhuman animals
and have a bias toward using speciesist language for some nonhuman animal
names. Our code for reproducing the experiments will be made available on
GitHub.",None,-1
Mediators: Conversational Agents Explaining NLP Model Behavior,0.651645,"The human-centric explainable artificial intelligence (HCXAI) community has
raised the need for framing the explanation process as a conversation between
human and machine. In this position paper, we establish desiderata for
Mediators, text-based conversational agents which are capable of explaining the
behavior of neural models interactively using natural language. From the
perspective of natural language processing (NLP) research, we engineer a
blueprint of such a Mediator for the task of sentiment analysis and assess how
far along current research is on the path towards dialogue-based explanations.",None,-1
Automatic Severity Classification of Dysarthric speech by using Self-supervised Model with Multi-task Learning,0.349641,"Automatic assessment of dysarthric speech is essential for sustained
treatments and rehabilitation. However, obtaining atypical speech is
challenging, often leading to data scarcity issues. To tackle the problem, we
propose a novel automatic severity assessment method for dysarthric speech,
using the self-supervised model in conjunction with multi-task learning.
Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity
classification and auxiliary automatic speech recognition (ASR). For the
baseline experiments, we employ hand-crafted acoustic features and machine
learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean
dysarthric speech QoLT database, our model outperforms the traditional baseline
methods, with a relative percentage increase of 1.25% for F1-score. In
addition, the proposed model surpasses the model trained without ASR head,
achieving 10.61% relative percentage improvements. Furthermore, we present how
multi-task learning affects the severity classification performance by
analyzing the latent representations and regularization effect.",https://github.com/juice500ml/dysarthria-mtl,654
BIASeD: Bringing Irrationality into Automated System Design,0.278079,"Human perception, memory and decision-making are impacted by tens of
cognitive biases and heuristics that influence our actions and decisions.
Despite the pervasiveness of such biases, they are generally not leveraged by
today's Artificial Intelligence (AI) systems that model human behavior and
interact with humans. In this theoretical paper, we claim that the future of
human-machine collaboration will entail the development of AI systems that
model, understand and possibly replicate human cognitive biases. We propose the
need for a research agenda on the interplay between human cognitive biases and
Artificial Intelligence. We categorize existing cognitive biases from the
perspective of AI systems, identify three broad areas of interest and outline
research directions for the design of AI systems that have a better
understanding of our own biases.",None,-1
End-to-End 3D Hand Pose Estimation from Stereo Cameras,0.692917,"This work proposes an end-to-end approach to estimate full 3D hand pose from
stereo cameras. Most existing methods of estimating hand pose from stereo
cameras apply stereo matching to obtain depth map and use depth-based solution
to estimate hand pose. In contrast, we propose to bypass the stereo matching
and directly estimate the 3D hand pose from the stereo image pairs. The
proposed neural network architecture extends from any keypoint predictor to
estimate the sparse disparity of the hand joints. In order to effectively train
the model, we propose a large scale synthetic dataset that is composed of
stereo image pairs and ground truth 3D hand pose annotations. Experiments show
that the proposed approach outperforms the existing methods based on the stereo
depth.",None,-1
Inkorrect: Online Handwriting Spelling Correction,0.506327,"We introduce Inkorrect, a data- and label-efficient approach for online
handwriting (Digital Ink) spelling correction - DISC. Unlike previous work, the
proposed method does not require multiple samples from the same writer, or
access to character level segmentation. We show that existing automatic
evaluation metrics do not fully capture and are not correlated with the human
perception of the quality of the spelling correction, and propose new ones that
correlate with human perception. We additionally surface an interesting
phenomenon: a trade-off between the similarity and recognizability of the
spell-corrected inks. We further create a family of models corresponding to
different points on the Pareto frontier between those two axes. We show that
Inkorrect's Pareto frontier dominates the points that correspond to prior work.",None,-1
Receding Horizon Inverse Reinforcement Learning,0.486583,"Inverse reinforcement learning (IRL) seeks to infer a cost function that
explains the underlying goals and preferences of expert demonstrations. This
paper presents receding horizon inverse reinforcement learning (RHIRL), a new
IRL algorithm for high-dimensional, noisy, continuous systems with black-box
dynamic models. RHIRL addresses two key challenges of IRL: scalability and
robustness. To handle high-dimensional continuous systems, RHIRL matches the
induced optimal trajectories with expert demonstrations locally in a receding
horizon manner and 'stitches' together the local solutions to learn the cost;
it thereby avoids the 'curse of dimensionality'. This contrasts sharply with
earlier algorithms that match with expert demonstrations globally over the
entire high-dimensional state space. To be robust against imperfect expert
demonstrations and control noise, RHIRL learns a state-dependent cost function
'disentangled' from system dynamics under mild conditions. Experiments on
benchmark tasks show that RHIRL outperforms several leading IRL algorithms in
most instances. We also prove that the cumulative error of RHIRL grows linearly
with the task duration.",None,-1
Neural Neighbor Style Transfer,0.468282,"We propose Neural Neighbor Style Transfer (NNST), a pipeline that offers
state-of-the-art quality, generalization, and competitive efficiency for
artistic style transfer. Our approach is based on explicitly replacing neural
features extracted from the content input (to be stylized) with those from a
style exemplar, then synthesizing the final output based on these rearranged
features. While the spirit of our approach is similar to prior work, we show
that our design decisions dramatically improve the final visual quality.",None,-1
Exploring Diversity-based Active Learning for 3D Object Detection in Autonomous Driving,0.443109,"3D object detection has recently received much attention due to its great
potential in autonomous vehicle (AV). The success of deep learning based object
detectors relies on the availability of large-scale annotated datasets, which
is time-consuming and expensive to compile, especially for 3D bounding box
annotation. In this work, we investigate diversity-based active learning (AL)
as a potential solution to alleviate the annotation burden. Given limited
annotation budget, only the most informative frames and objects are
automatically selected for human to annotate. Technically, we take the
advantage of the multimodal information provided in an AV dataset, and propose
a novel acquisition function that enforces spatial and temporal diversity in
the selected samples. We benchmark the proposed method against other AL
strategies under realistic annotation cost measurement, where the realistic
costs for annotating a frame and a 3D bounding box are both taken into
consideration. We demonstrate the effectiveness of the proposed method on the
nuScenes dataset and show that it outperforms existing AL strategies
significantly.",https://github.com/poodarchu/Det3D,1317
Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics,0.945861,"Modern machine learning research relies on relatively few carefully curated
datasets. Even in these datasets, and typically in `untidy' or raw data,
practitioners are faced with significant issues of data quality and diversity
which can be prohibitively labor intensive to address. Existing methods for
dealing with these challenges tend to make strong assumptions about the
particular issues at play, and often require a priori knowledge or metadata
such as domain labels. Our work is orthogonal to these methods: we instead
focus on providing a unified and efficient framework for Metadata Archaeology
-- uncovering and inferring metadata of examples in a dataset. We curate
different subsets of data that might exist in a dataset (e.g. mislabeled,
atypical, or out-of-distribution examples) using simple transformations, and
leverage differences in learning dynamics between these probe suites to infer
metadata of interest. Our method is on par with far more sophisticated
mitigation methods across different tasks: identifying and correcting
mislabeled examples, classifying minority-group samples, prioritizing points
relevant for training and enabling scalable human auditing of relevant
examples.",None,-1
Target-aware Molecular Graph Generation,0.680885,"Generating molecules with desired biological activities has attracted growing
attention in drug discovery. Previous molecular generation models are designed
as chemocentric methods that hardly consider the drug-target interaction,
limiting their practical applications. In this paper, we aim to generate
molecular drugs in a target-aware manner that bridges biological activity and
molecular design. To solve this problem, we compile a benchmark dataset from
several publicly available datasets and build baselines in a unified framework.
Building on the recent advantages of flow-based molecular generation models, we
propose SiamFlow, which forces the flow to fit the distribution of target
sequence embeddings in latent space. Specifically, we employ an alignment loss
and a uniform loss to bring target sequence embeddings and drug graph
embeddings into agreements while avoiding collapse. Furthermore, we formulate
the alignment into a one-to-many problem by learning spaces of target sequence
embeddings. Experiments quantitatively show that our proposed method learns
meaningful representations in the latent space toward the target-aware
molecular graph generation and provides an alternative approach to bridge
biology and chemistry in drug discovery.",None,-1
CFA: Coupled-hypersphere-based Feature Adaptation for Target-Oriented Anomaly Localization,0.999463,"For a long time, anomaly localization has been widely used in industries.
Previous studies focused on approximating the distribution of normal features
without adaptation to a target dataset. However, since anomaly localization
should precisely discriminate normal and abnormal features, the absence of
adaptation may make the normality of abnormal features overestimated. Thus, we
propose Coupled-hypersphere-based Feature Adaptation (CFA) which accomplishes
sophisticated anomaly localization using features adapted to the target
dataset. CFA consists of (1) a learnable patch descriptor that learns and
embeds target-oriented features and (2) scalable memory bank independent of the
size of the target dataset. And, CFA adopts transfer learning to increase the
normal feature density so that abnormal features can be clearly distinguished
by applying patch descriptor and memory bank to a pre-trained CNN. The proposed
method outperforms the previous methods quantitatively and qualitatively. For
example, it provides an AUROC score of 99.5% in anomaly detection and 98.5% in
anomaly localization of MVTec AD benchmark. In addition, this paper points out
the negative effects of biased features of pre-trained CNNs and emphasizes the
importance of the adaptation to the target dataset. The code is publicly
available at https://github.com/sungwool/CFA_for_anomaly_localization.",https://github.com/sungwool/CFA_for_anomaly_localization,-1
Multi-modal Contrastive Representation Learning for Entity Alignment,0.90175,"Multi-modal entity alignment aims to identify equivalent entities between two
different multi-modal knowledge graphs, which consist of structural triples and
images associated with entities. Most previous works focus on how to utilize
and encode information from different modalities, while it is not trivial to
leverage multi-modal knowledge in entity alignment because of the modality
heterogeneity. In this paper, we propose MCLEA, a Multi-modal Contrastive
Learning based Entity Alignment model, to obtain effective joint
representations for multi-modal entity alignment. Different from previous
works, MCLEA considers task-oriented modality and models the inter-modal
relationships for each entity representation. In particular, MCLEA firstly
learns multiple individual representations from multiple modalities, and then
performs contrastive learning to jointly model intra-modal and inter-modal
interactions. Extensive experimental results show that MCLEA outperforms
state-of-the-art baselines on public datasets under both supervised and
unsupervised settings.",https://github.com/lzxlin/MCLEA,-1
Differentiable Point-Based Radiance Fields for Efficient View Synthesis,0.556855,"We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers.",None,97412
Clip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of Rewards,0.2618,"Derivative-free prompt learning has emerged as a lightweight alternative to
prompt tuning, which only requires model inference to optimize the prompts.
However, existing work did not take full advantage of the over-parameterized
characteristics of large pre-trained language models (PLMs). In this paper, we
propose Clip-Tuning, a simple yet effective method that adopts diverse frozen
""thinned"" networks of PLMs to obtain a mixture of rewards and thus advance the
derivative-free prompt learning. The thinned networks consist of all the hidden
units that survive a stationary dropout strategy, whose inference predictions
reflect an ensemble of partial views over prompted training samples. Our method
outperforms previous gradient-free prompt learning methods and achieves parity
with gradient-based counterparts on seven language understanding benchmarks
under few-shot settings.",None,-1
Prompt-based Generative Approach towards Multi-Hierarchical Medical Dialogue State Tracking,0.167962,"The medical dialogue system is a promising application that can provide great
convenience for patients. The dialogue state tracking (DST) module in the
medical dialogue system which interprets utterances into the machine-readable
structure for downstream tasks is particularly challenging. Firstly, the states
need to be able to represent compound entities such as symptoms with their body
part or diseases with degrees of severity to provide enough information for
decision support. Secondly, these named entities in the utterance might be
discontinuous and scattered across sentences and speakers. These also make it
difficult to annotate a large corpus which is essential for most methods.
Therefore, we first define a multi-hierarchical state structure. We annotate
and publish a medical dialogue dataset in Chinese. To the best of our
knowledge, there are no publicly available ones before. Then we propose a
Prompt-based Generative Approach which can generate slot values with
multi-hierarchies incrementally using a top-down approach. A dialogue style
prompt is also supplemented to utilize the large unlabeled dialogue corpus to
alleviate the data scarcity problem. The experiments show that our approach
outperforms other DST methods and is rather effective in the scenario with
little data.",None,-1
A Marker-based Neural Network System for Extracting Social Determinants of Health,0.228252,"Objective. The impact of social determinants of health (SDoH) on patients'
healthcare quality and the disparity is well-known. Many SDoH items are not
coded in structured forms in electronic health records. These items are often
captured in free-text clinical notes, but there are limited methods for
automatically extracting them. We explore a multi-stage pipeline involving
named entity recognition (NER), relation classification (RC), and text
classification methods to extract SDoH information from clinical notes
automatically.
  Materials and Methods. The study uses the N2C2 Shared Task data, which was
collected from two sources of clinical notes: MIMIC-III and University of
Washington Harborview Medical Centers. It contains 4480 social history sections
with full annotation for twelve SDoHs. In order to handle the issue of
overlapping entities, we developed a novel marker-based NER model. We used it
in a multi-stage pipeline to extract SDoH information from clinical notes.
  Results. Our marker-based system outperformed the state-of-the-art span-based
models at handling overlapping entities based on the overall Micro-F1 score
performance. It also achieved state-of-the-art performance compared to the
shared task methods.
  Conclusion. The major finding of this study is that the multi-stage pipeline
effectively extracts SDoH information from clinical notes. This approach can
potentially improve the understanding and tracking of SDoHs in clinical
settings. However, error propagation may be an issue, and further research is
needed to improve the extraction of entities with complex semantic meanings and
low-resource entities using external knowledge.",https://github.com/flairNLP/flair,-1
Communication Beyond Transmitting Bits: Semantics-Guided Source and Channel Coding,0.810792,"Classical communication paradigms focus on accurately transmitting bits over
a noisy channel, and Shannon theory provides a fundamental theoretical limit on
the rate of reliable communications. In this approach, bits are treated
equally, and the communication system is oblivious to what meaning these bits
convey or how they would be used. Future communications towards intelligence
and conciseness will predictably play a dominant role, and the proliferation of
connected intelligent agents requires a radical rethinking of coded
transmission paradigm to support the new communication morphology on the
horizon. The recent concept of ""semantic communications"" offers a promising
research direction. Injecting semantic guidance into the coded transmission
design to achieve semantics-aware communications shows great potential for
further breakthrough in effectiveness and reliability. This article sheds light
on semantics-guided source and channel coding as a transmission paradigm of
semantic communications, which exploits both data semantics diversity and
wireless channel diversity together to boost the whole system performance. We
present the general system architecture and key techniques, and indicate some
open issues on this topic.",None,-1
Visually-Augmented Language Modeling,0.70736,"Human language is grounded on multimodal knowledge including visual knowledge
like colors, sizes, and shapes. However, current large-scale pre-trained
language models rely on text-only self-supervised training with massive text
data, which precludes them from utilizing relevant visual information when
necessary. To address this, we propose a novel pre-training framework, named
VaLM, to Visually-augment text tokens with retrieved relevant images for
Language Modeling. Specifically, VaLM builds on a novel latent text-image
alignment method via an image retrieval module to fetch corresponding images
given a textual context. With the visually-augmented context, VaLM uses a
visual knowledge fusion layer to enable multimodal grounded language modeling
by attending to both text context and visual knowledge in images. We evaluate
VaLM on various visual knowledge-intensive commonsense reasoning tasks, which
require visual information to excel. The experimental results illustrate that
VaLM outperforms all strong language-only and vision-language baselines with
substantial gains in reasoning object commonsense including color, size, and
shape. Our code is available at https://github.com/Victorwz/VaLM.",https://github.com/Victorwz/VaLM,-1
Zero-Shot On-the-Fly Event Schema Induction,0.810376,"What are the events involved in a pandemic outbreak? What steps should be
taken when planning a wedding? The answers to these questions can be found by
collecting many documents on the complex event of interest, extracting relevant
information, and analyzing it. We present a new approach in which large
language models are utilized to generate source documents that allow
predicting, given a high-level event definition, the specific events,
arguments, and relations between them to construct a schema that describes the
complex event in its entirety. Using our model, complete schemas on any topic
can be generated on-the-fly without any manual data collection, i.e., in a
zero-shot manner. Moreover, we develop efficient methods to extract pertinent
information from texts and demonstrate in a series of experiments that these
schemas are considered to be more complete than human-curated ones in the
majority of examined scenarios. Finally, we show that this framework is
comparable in performance with previous supervised schema induction methods
that rely on collecting real texts while being more general and flexible
without the need for a predefined ontology.",None,-1
Learning Invariant Representation and Risk Minimized for Unsupervised Accent Domain Adaptation,0.0931086,"Unsupervised representation learning for speech audios attained impressive
performances for speech recognition tasks, particularly when annotated speech
is limited. However, the unsupervised paradigm needs to be carefully designed
and little is known about what properties these representations acquire. There
is no guarantee that the model learns meaningful representations for valuable
information for recognition. Moreover, the adaptation ability of the learned
representations to other domains still needs to be estimated. In this work, we
explore learning domain-invariant representations via a direct mapping of
speech representations to their corresponding high-level linguistic
informations. Results prove that the learned latents not only capture the
articulatory feature of each phoneme but also enhance the adaptation ability,
outperforming the baseline largely on accented benchmarks.",None,-1
A Dense Material Segmentation Dataset for Indoor and Outdoor Scene Parsing,0.317593,"A key algorithm for understanding the world is material segmentation, which
assigns a label (metal, glass, etc.) to each pixel. We find that a model
trained on existing data underperforms in some settings and propose to address
this with a large-scale dataset of 3.2 million dense segments on 44,560 indoor
and outdoor images, which is 23x more segments than existing data. Our data
covers a more diverse set of scenes, objects, viewpoints and materials, and
contains a more fair distribution of skin types. We show that a model trained
on our data outperforms a state-of-the-art model across datasets and
viewpoints. We propose a large-scale scene parsing benchmark and baseline of
0.729 per-pixel accuracy, 0.585 mean class accuracy and 0.420 mean IoU across
46 materials.",https://github.com/apple/ml-dms-dataset,-1
Interpretable Proof Generation via Iterative Backward Reasoning,0.128776,"We present IBR, an Iterative Backward Reasoning model to solve the proof
generation tasks on rule-based Question Answering (QA), where models are
required to reason over a series of textual rules and facts to find out the
related proof path and derive the final answer. We handle the limitations of
existed works in two folds: 1) enhance the interpretability of reasoning
procedures with detailed tracking, by predicting nodes and edges in the proof
path iteratively backward from the question; 2) promote the efficiency and
accuracy via reasoning on the elaborate representations of nodes and history
paths, without any intermediate texts that may introduce external noise during
proof generation. There are three main modules in IBR, QA and proof strategy
prediction to obtain the answer and offer guidance for the following procedure;
parent node prediction to determine a node in the existing proof that a new
child node will link to; child node prediction to find out which new node will
be added to the proof. Experiments on both synthetic and paraphrased datasets
demonstrate that IBR has better in-domain performance as well as cross-domain
transferability than several strong baselines. Our code and models are
available at https://github.com/find-knowledge/IBR .",https://github.com/find-knowledge/IBR,-1
Multi-Modal and Multi-Factor Branching Time Active Inference,0.0379965,"Active inference is a state-of-the-art framework for modelling the brain that
explains a wide range of mechanisms such as habit formation, dopaminergic
discharge and curiosity. Recently, two versions of branching time active
inference (BTAI) based on Monte-Carlo tree search have been developed to handle
the exponential (space and time) complexity class that occurs when computing
the prior over all possible policies up to the time horizon. However, those two
versions of BTAI still suffer from an exponential complexity class w.r.t the
number of observed and latent variables being modelled. In the present paper,
we resolve this limitation by first allowing the modelling of several
observations, each of them having its own likelihood mapping. Similarly, we
allow each latent state to have its own transition mapping. The inference
algorithm then exploits the factorisation of the likelihood and transition
mappings to accelerate the computation of the posterior. Those two
optimisations were tested on the dSprites environment in which the metadata of
the dSprites dataset was used as input to the model instead of the dSprites
images. On this task, $BTAI_{VMP}$ (Champion et al., 2022b,a) was able to solve
96.9\% of the task in 5.1 seconds, and $BTAI_{BF}$ (Champion et al., 2021a) was
able to solve 98.6\% of the task in 17.5 seconds. Our new approach
($BTAI_{3MF}$) outperformed both of its predecessors by solving the task
completly (100\%) in only 2.559 seconds. Finally, $BTAI_{3MF}$ has been
implemented in a flexible and easy to use (python) package, and we developed a
graphical user interface to enable the inspection of the model's beliefs,
planning process and behaviour.",https://github.com/ChampiB/Experiments_AI_TS,-1
Unsupervised Lifelong Person Re-identification via Contrastive Rehearsal,0.30211,"Existing unsupervised person re-identification (ReID) methods focus on
adapting a model trained on a source domain to a fixed target domain. However,
an adapted ReID model usually only works well on a certain target domain, but
can hardly memorize the source domain knowledge and generalize to upcoming
unseen data. In this paper, we propose unsupervised lifelong person ReID, which
focuses on continuously conducting unsupervised domain adaptation on new
domains without forgetting the knowledge learnt from old domains. To tackle
unsupervised lifelong ReID, we conduct a contrastive rehearsal on a small
number of stored old samples while sequentially adapting to new domains. We
further set an image-to-image similarity constraint between old and new models
to regularize the model updates in a way that suits old knowledge. We
sequentially train our model on several large-scale datasets in an unsupervised
manner and test it on all seen domains as well as several unseen domains to
validate the generalizability of our method. Our proposed unsupervised lifelong
method achieves strong generalizability, which significantly outperforms
previous lifelong methods on both seen and unseen domains. Code will be made
available at https://github.com/chenhao2345/UCR.",https://github.com/chenhao2345/UCR,12010
Neighboring Backdoor Attacks on Graph Convolutional Network,0.25536,"Backdoor attacks have been widely studied to hide the misclassification rules
in the normal models, which are only activated when the model is aware of the
specific inputs (i.e., the trigger). However, despite their success in the
conventional Euclidean space, there are few studies of backdoor attacks on
graph structured data. In this paper, we propose a new type of backdoor which
is specific to graph data, called neighboring backdoor. Considering the
discreteness of graph data, how to effectively design the triggers while
retaining the model accuracy on the original task is the major challenge. To
address such a challenge, we set the trigger as a single node, and the backdoor
is activated when the trigger node is connected to the target node. To preserve
the model accuracy, the model parameters are not allowed to be modified. Thus,
when the trigger node is not connected, the model performs normally. Under
these settings, in this work, we focus on generating the features of the
trigger node. Two types of backdoors are proposed: (1) Linear Graph Convolution
Backdoor which finds an approximation solution for the feature generation (can
be viewed as an integer programming problem) by looking at the linear part of
GCNs. (2) Variants of existing graph attacks. We extend current gradient-based
attack methods to our backdoor attack scenario. Extensive experiments on two
social networks and two citation networks datasets demonstrate that all
proposed backdoors can achieve an almost 100\% attack success rate while having
no impact on predictive accuracy.",None,-1
MiniViT: Compressing Vision Transformers with Weight Multiplexing,0.777443,"Vision Transformer (ViT) models have recently drawn much attention in
computer vision due to their high model capability. However, ViT models suffer
from huge number of parameters, restricting their applicability on devices with
limited memory. To alleviate this problem, we propose MiniViT, a new
compression framework, which achieves parameter reduction in vision
transformers while retaining the same performance. The central idea of MiniViT
is to multiplex the weights of consecutive transformer blocks. More
specifically, we make the weights shared across layers, while imposing a
transformation on the weights to increase diversity. Weight distillation over
self-attention is also applied to transfer knowledge from large-scale ViT
models to weight-multiplexed compact models. Comprehensive experiments
demonstrate the efficacy of MiniViT, showing that it can reduce the size of the
pre-trained Swin-B transformer by 48\%, while achieving an increase of 1.0\% in
Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters,
MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters,
without seriously compromising the performance. Finally, we verify the
transferability of MiniViT by reporting its performance on downstream
benchmarks. Code and models are available at here.",None,18607
Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models,0.215754,"Prompt learning is a new learning paradigm which reformulates downstream
tasks as similar pretraining tasks on pretrained models by leveraging textual
prompts. Recent works have demonstrated that prompt learning is particularly
useful for few-shot learning, where there is limited training data. Depending
on the granularity of prompts, those methods can be roughly divided into
task-level prompting and instance-level prompting. Task-level prompting methods
learn one universal prompt for all input samples, which is efficient but
ineffective to capture subtle differences among different classes.
Instance-level prompting methods learn a specific prompt for each input, though
effective but inefficient. In this work, we develop a novel prototype-based
prompt learning method to overcome the above limitations. In particular, we
focus on few-shot image recognition tasks on pretrained vision-language models
(PVLMs) and develop a method of prompting through prototype (PTP), where we
define $K$ image prototypes and $K$ prompt prototypes. In PTP, the image
prototype represents a centroid of a certain image cluster in the latent space
and a prompt prototype is defined as a soft prompt in the continuous space. The
similarity between a query image and an image prototype determines how much
this prediction relies on the corresponding prompt prototype. Hence, in PTP,
similar images will utilize similar prompting ways. Through extensive
experiments on seven real-world benchmarks, we show that PTP is an effective
method to leverage the latent knowledge and adaptive to various PVLMs.
Moreover, through detailed analysis, we discuss pros and cons for prompt
learning and parameter-efficient fine-tuning under the context of few-shot
learning.",None,-1
A simple language-agnostic yet very strong baseline system for hate speech and offensive content identification,0.794308,"For automatically identifying hate speech and offensive content in tweets, a
system based on a classical supervised algorithm only fed with character
n-grams, and thus completely language-agnostic, is proposed by the SATLab team.
After its optimization in terms of the feature weighting and the classifier
parameters, it reached, in the multilingual HASOC 2021 challenge, a medium
performance level in English, the language for which it is easy to develop deep
learning approaches relying on many external linguistic resources, but a far
better level for the two less resourced language, Hindi and Marathi. It ends
even first when performances are averaged over the three tasks in these
languages, outperforming many deep learning approaches. These performances
suggest that it is an interesting reference level to evaluate the benefits of
using more complex approaches such as deep learning or taking into account
complementary resources.",None,-1
Lifelong Learning Metrics,0.214647,"The DARPA Lifelong Learning Machines (L2M) program seeks to yield advances in
artificial intelligence (AI) systems so that they are capable of learning (and
improving) continuously, leveraging data on one task to improve performance on
another, and doing so in a computationally sustainable way. Performers on this
program developed systems capable of performing a diverse range of functions,
including autonomous driving, real-time strategy, and drone simulation. These
systems featured a diverse range of characteristics (e.g., task structure,
lifetime duration), and an immediate challenge faced by the program's testing
and evaluation team was measuring system performance across these different
settings. This document, developed in close collaboration with DARPA and the
program performers, outlines a formalism for constructing and characterizing
the performance of agents performing lifelong learning scenarios.",None,-1
Speech Resources in the Tamasheq Language,0.715946,"In this paper we present two datasets for Tamasheq, a developing language
mainly spoken in Mali and Niger. These two datasets were made available for the
IWSLT 2022 low-resource speech translation track, and they consist of
collections of radio recordings from daily broadcast news in Niger (Studio
Kalangou) and Mali (Studio Tamani). We share (i) a massive amount of unlabeled
audio data (671 hours) in five languages: French from Niger, Fulfulde, Hausa,
Tamasheq and Zarma, and (ii) a smaller 17 hours parallel corpus of audio
recordings in Tamasheq, with utterance-level translations in the French
language. All this data is shared under the Creative Commons BY-NC-ND 3.0
license. We hope these resources will inspire the speech community to develop
and benchmark models using the Tamasheq language.",https://github.com/mzboito/IWSLT2022 Tamasheq data.,-1
Resolving Copycat Problems in Visual Imitation Learning via Residual Action Prediction,0.220817,"Imitation learning is a widely used policy learning method that enables
intelligent agents to acquire complex skills from expert demonstrations. The
input to the imitation learning algorithm is usually composed of both the
current observation and historical observations since the most recent
observation might not contain enough information. This is especially the case
with image observations, where a single image only includes one view of the
scene, and it suffers from a lack of motion information and object occlusions.
In theory, providing multiple observations to the imitation learning agent will
lead to better performance. However, surprisingly people find that sometimes
imitation from observation histories performs worse than imitation from the
most recent observation. In this paper, we explain this phenomenon from the
information flow within the neural network perspective. We also propose a novel
imitation learning neural network architecture that does not suffer from this
issue by design. Furthermore, our method scales to high-dimensional image
observations. Finally, we benchmark our approach on two widely used simulators,
CARLA and MuJoCo, and it successfully alleviates the copycat problem and
surpasses the existing solutions.",None,-1
OrthoMAD: Morphing Attack Detection Through Orthogonal Identity Disentanglement,0.388971,"Morphing attacks are one of the many threats that are constantly affecting
deep face recognition systems. It consists of selecting two faces from
different individuals and fusing them into a final image that contains the
identity information of both. In this work, we propose a novel regularisation
term that takes into account the existent identity information in both and
promotes the creation of two orthogonal latent vectors. We evaluate our
proposed method (OrthoMAD) in five different types of morphing in the FRLL
dataset and evaluate the performance of our model when trained on five distinct
datasets. With a small ResNet-18 as the backbone, we achieve state-of-the-art
results in the majority of the experiments, and competitive results in the
others. The code of this paper will be publicly available.",https://github.com/NetoPedro/OrthoMAD,-1
KETOD: Knowledge-Enriched Task-Oriented Dialogue,0.911552,"Existing studies in dialogue system research mostly treat task-oriented
dialogue and chit-chat as separate domains. Towards building a human-like
assistant that can converse naturally and seamlessly with users, it is
important to build a dialogue system that conducts both types of conversations
effectively. In this work, we investigate how task-oriented dialogue and
knowledge-grounded chit-chat can be effectively integrated into a single model.
To this end, we create a new dataset, KETOD (Knowledge-Enriched Task-Oriented
Dialogue), where we naturally enrich task-oriented dialogues with chit-chat
based on relevant entity knowledge. We also propose two new models,
SimpleToDPlus and Combiner, for the proposed task. Experimental results on both
automatic and human evaluations show that the proposed methods can
significantly improve the performance in knowledge-enriched response generation
while maintaining a competitive task-oriented dialog performance. We believe
our new dataset will be a valuable resource for future studies. Our dataset and
code are publicly available at \url{https://github.com/facebookresearch/ketod}.",https://github.com/facebookresearch/ketod,-1
On an Edge-Preserving Variational Model for Optical Flow Estimation,0.102574,"It is well known that classical formulations resembling the Horn and Schunck
model are still largely competitive due to the modern implementation practices.
In most cases, these models outperform many modern flow estimation methods. In
view of this, we propose an effective implementation design for an
edge-preserving $L^1$ regularization approach to optical flow. The mathematical
well-posedness of our proposed model is studied in the space of functions of
bounded variations $BV(\Omega,\mathbb{R}^2)$. The implementation scheme is
designed in multiple steps. The flow field is computed using the robust
Chambolle-Pock primal-dual algorithm. Motivated by the recent studies of Castro
and Donoho we extend the heuristic of iterated median filtering to our flow
estimation. Further, to refine the flow edges we use the weighted median filter
established by Li and Osher as a post-processing step. Our experiments on the
Middlebury dataset show that the proposed method achieves the best average
angular and end-point errors compared to some of the state-of-the-art Horn and
Schunck based variational methods.",None,-1
A General Framework for Modelling Conditional Reasoning -- Preliminary Report,0.0295116,"We introduce and investigate here a formalisation for conditionals that
allows the definition of a broad class of reasoning systems. This framework
covers the most popular kinds of conditional reasoning in logic-based KR: the
semantics we propose is appropriate for a structural analysis of those
conditionals that do not satisfy closure properties associated to classical
logics.",None,-1
Model Generalization in Arrival Runway Occupancy Time Prediction by Feature Equivalences,0.300569,"General real-time runway occupancy time prediction modelling for multiple
airports is a current research gap. An attempt to generalize a real-time
prediction model for Arrival Runway Occupancy Time (AROT) is presented in this
paper by substituting categorical features by their numerical equivalences.
Three days of data, collected from Saab Sensis' Aerobahn system at three US
airports, has been used for this work. Three tree-based machine learning
algorithms: Decision Tree, Random Forest and Gradient Boosting are used to
assess the generalizability of the model using numerical equivalent features.
We have shown that the model trained on numerical equivalent features not only
have performances at least on par with models trained on categorical features
but also can make predictions on unseen data from other airports.",None,-1
A Multibranch Convolutional Neural Network for Hyperspectral Unmixing,0.632268,"Hyperspectral unmixing remains one of the most challenging tasks in the
analysis of such data. Deep learning has been blooming in the field and proved
to outperform other classic unmixing techniques, and can be effectively
deployed onboard Earth observation satellites equipped with hyperspectral
imagers. In this letter, we follow this research pathway and propose a
multi-branch convolutional neural network that benefits from fusing spectral,
spatial, and spectral-spatial features in the unmixing process. The results of
our experiments, backed up with the ablation study, revealed that our
techniques outperform others from the literature and lead to higher-quality
fractional abundance estimation. Also, we investigated the influence of
reducing the training sets on the capabilities of all algorithms and their
robustness against noise, as capturing large and representative ground-truth
sets is time-consuming and costly in practice, especially in emerging Earth
observation scenarios.",https://gitlab.com/jnalepa/mbhuto,-1
On Mitigating Hard Clusters for Face Clustering,0.355252,"Face clustering is a promising way to scale up face recognition systems using
large-scale unlabeled face images. It remains challenging to identify small or
sparse face image clusters that we call hard clusters, which is caused by the
heterogeneity, \ie, high variations in size and sparsity, of the clusters.
Consequently, the conventional way of using a uniform threshold (to identify
clusters) often leads to a terrible misclassification for the samples that
should belong to hard clusters. We tackle this problem by leveraging the
neighborhood information of samples and inferring the cluster memberships (of
samples) in a probabilistic way. We introduce two novel modules,
Neighborhood-Diffusion-based Density (NDDe) and Transition-Probability-based
Distance (TPDi), based on which we can simply apply the standard Density Peak
Clustering algorithm with a uniform threshold. Our experiments on multiple
benchmarks show that each module contributes to the final performance of our
method, and by incorporating them into other advanced face clustering methods,
these two modules can boost the performance of these methods to a new
state-of-the-art. Code is available at:
https://github.com/echoanran/On-Mitigating-Hard-Clusters.",https://github.com/echoanran/On-Mitigating-Hard-Clusters,-1
Automatic Speech Recognition for Speech Assessment of Persian Preschool Children,0.287628,"Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about children's growth and development. The COVID-19
pandemic has highlighted the necessity of online assessment for preschool
children. One of the areas that should be tested is their ability to speak.
Employing an Automatic Speech Recognition (ASR) system would not help since
they are pre-trained on voices that differ from children's in terms of
frequency and amplitude. Because most of these are pre-trained with data in a
specific range of amplitude, their objectives do not make them ready for voices
in different amplitudes. To overcome this issue, we added a new objective to
the masking objective of the Wav2Vec 2.0 model called Random Frequency Pitch
(RFP). In addition, we used our newly introduced dataset to fine-tune our model
for Meaningless Words (MW) and Rapid Automatic Naming (RAN) tests. Using
masking in concatenation with RFP outperforms the masking objective of Wav2Vec
2.0 by reaching a Word Error Rate (WER) of 1.35. Our new approach reaches a WER
of 6.45 on the Persian section of the CommonVoice dataset. Furthermore, our
novel methodology produces positive outcomes in zero- and few-shot scenarios.",https://github.com/AmirAbaskohi/Automatic-Speech-recognition-for-Speech-Assessment-of-Persian-Preschool-Children,-1
Design and Implementation of a Quantum Kernel for Natural Language Processing,0.252406,"Natural language processing (NLP) is the field that attempts to make human
language accessible to computers, and it relies on applying a mathematical
model to express the meaning of symbolic language. One such model, DisCoCat,
defines how to express both the meaning of individual words as well as their
compositional nature. This model can be naturally implemented on quantum
computers, leading to the field quantum NLP (QNLP). Recent experimental work
used quantum machine learning techniques to map from text to class label using
the expectation value of the quantum encoded sentence. Theoretical work has
been done on computing the similarity of sentences but relies on an unrealized
quantum memory store. The main goal of this thesis is to leverage the DisCoCat
model to design a quantum-based kernel function that can be used by a support
vector machine (SVM) for NLP tasks. Two similarity measures were studied: (i)
the transition amplitude approach and (ii) the SWAP test. A simple NLP meaning
classification task from previous work was used to train the word embeddings
and evaluate the performance of both models. The Python module lambeq and its
related software stack was used for implementation. The explicit model from
previous work was used to train word embeddings and achieved a testing accuracy
of $93.09 \pm 0.01$%. It was shown that both the SVM variants achieved a higher
testing accuracy of $95.72 \pm 0.01$% for approach (i) and $97.14 \pm 0.01$%
for (ii). The SWAP test was then simulated under a noise model defined by the
real quantum device, ibmq_guadalupe. The explicit model achieved an accuracy of
$91.94 \pm 0.01$% while the SWAP test SVM achieved 96.7% on the testing
dataset, suggesting that the kernelized classifiers are resilient to noise.
These are encouraging results and motivate further investigations of our
proposed kernelized QNLP paradigm.",https://github.com/mattwright99/thesis,-1
How would Stance Detection Techniques Evolve after the Launch of ChatGPT?,0.784596,"Stance detection refers to the task of extracting the standpoint (Favor,
Against or Neither) towards a target in given texts. Such research gains
increasing attention with the proliferation of social media contents. The
conventional framework of handling stance detection is converting it into text
classification tasks. Deep learning models have already replaced rule-based
models and traditional machine learning models in solving such problems.
Current deep neural networks are facing two main challenges which are
insufficient labeled data and information in social media posts and the
unexplainable nature of deep learning models. A new pre-trained language model
chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our
experiments show that ChatGPT can achieve SOTA or similar performance for
commonly used datasets including SemEval-2016 and P-Stance. At the same time,
ChatGPT can provide explanation for its own prediction, which is beyond the
capability of any existing model. The explanations for the cases it cannot
provide classification results are especially useful. ChatGPT has the potential
to be the best AI model for stance detection tasks in NLP, or at least change
the research paradigm of this field. ChatGPT also opens up the possibility of
building explanatory AI for stance detection.",None,648
Prompt-Based Metric Learning for Few-Shot NER,0.756352,"Few-shot named entity recognition (NER) targets generalizing to unseen labels
and/or domains with few labeled examples. Existing metric learning methods
compute token-level similarities between query and support sets, but are not
able to fully incorporate label semantics into modeling. To address this issue,
we propose a simple method to largely improve metric learning for NER: 1)
multiple prompt schemas are designed to enhance label semantics; 2) we propose
a novel architecture to effectively combine multiple prompt-based
representations. Empirically, our method achieves new state-of-the-art (SOTA)
results under 16 of the 18 considered settings, substantially outperforming the
previous SOTA by an average of 8.84% and a maximum of 34.51% in relative gains
of micro F1. Our code is available at https://github.com/AChen-qaq/ProML.",https://github.com/AChen-qaq/ProML,-1
Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning,0.767272,"Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions
formed from seen state and object during training. Since the same state may be
various in the visual appearance while entangled with different objects, CZSL
is still a challenging task. Some methods recognize state and object with two
trained classifiers, ignoring the impact of the interaction between object and
state; the other methods try to learn the joint representation of the
state-object compositions, leading to the domain gap between seen and unseen
composition sets. In this paper, we propose a novel Siamese Contrastive
Embedding Network (SCEN) (Code: https://github.com/XDUxyLi/SCEN-master) for
unseen composition recognition. Considering the entanglement between state and
object, we embed the visual feature into a Siamese Contrastive Space to capture
prototypes of them separately, alleviating the interaction between state and
object. In addition, we design a State Transition Module (STM) to increase the
diversity of training compositions, improving the robustness of the recognition
model. Extensive experiments indicate that our method significantly outperforms
the state-of-the-art approaches on three challenging benchmark datasets,
including the recent proposed C-QGA dataset.",https://github.com/XDUxyLi/SCEN-master,-1
IMPaSh: A Novel Domain-shift Resistant Representation for Colorectal Cancer Tissue Classification,0.362213,"The appearance of histopathology images depends on tissue type, staining and
digitization procedure. These vary from source to source and are the potential
causes for domain-shift problems. Owing to this problem, despite the great
success of deep learning models in computational pathology, a model trained on
a specific domain may still perform sub-optimally when we apply them to another
domain. To overcome this, we propose a new augmentation called PatchShuffling
and a novel self-supervised contrastive learning framework named IMPaSh for
pre-training deep learning models. Using these, we obtained a ResNet50 encoder
that can extract image representation resistant to domain-shift. We compared
our derived representation against those acquired based on other
domain-generalization techniques by using them for the cross-domain
classification of colorectal tissue images. We show that the proposed method
outperforms other traditional histology domain-adaptation and state-of-the-art
self-supervised learning methods. Code is available at:
https://github.com/trinhvg/IMPash .",https://github.com/trinhvg/IMPash,-1
"Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data",0.77021,"Automatic player identification is an essential and complex task in sports
video analysis. Different strategies have been devised over the years, but
identification based on jersey numbers is one of the most common approaches
given its versatility and relative simplicity. However, automatic detection of
jersey numbers is still challenging due to changing camera angles, low video
resolution, small object size in wide-range shots and transient changes in the
player's posture and movement. In this paper we present a novel approach for
jersey number identification in a small, highly imbalanced dataset from the
Seattle Seahawks practice videos. Our results indicate that simple models can
achieve an acceptable performance on the jersey number detection task and that
synthetic data can improve the performance dramatically (accuracy increase of
~9% overall, ~18% on low frequency numbers) making our approach achieve state
of the art results.",None,-1
Evons: A Dataset for Fake and Real News Virality Analysis and Prediction,0.236548,"We present a novel collection of news articles originating from fake and real
news media sources for the analysis and prediction of news virality. Unlike
existing fake news datasets which either contain claims or news article
headline and body, in this collection each article is supported with a Facebook
engagement count which we consider as an indicator of the article virality. In
addition we also provide the article description and thumbnail image with which
the article was shared on Facebook. These images were automatically annotated
with object tags and color attributes. Using cloud based vision analysis tools,
thumbnail images were also analyzed for faces and detected faces were annotated
with facial attributes. We empirically investigate the use of this collection
on an example task of article virality prediction.",https://github.com/krstovski/evons,-1
TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters,0.361784,"Topic taxonomies, which represent the latent topic (or category) structure of
document collections, provide valuable knowledge of contents in many
applications such as web search and information filtering. Recently, several
unsupervised methods have been developed to automatically construct the topic
taxonomy from a text corpus, but it is challenging to generate the desired
taxonomy without any prior knowledge. In this paper, we study how to leverage
the partial (or incomplete) information about the topic structure as guidance
to find out the complete topic taxonomy. We propose a novel framework for topic
taxonomy completion, named TaxoCom, which recursively expands the topic
taxonomy by discovering novel sub-topic clusters of terms and documents. To
effectively identify novel topics within a hierarchical topic structure,
TaxoCom devises its embedding and clustering techniques to be closely-linked
with each other: (i) locally discriminative embedding optimizes the text
embedding space to be discriminative among known (i.e., given) sub-topics, and
(ii) novelty adaptive clustering assigns terms into either one of the known
sub-topics or novel sub-topics. Our comprehensive experiments on two real-world
datasets demonstrate that TaxoCom not only generates the high-quality topic
taxonomy in terms of term coherency and topic coverage but also outperforms all
other baselines for a downstream task.",https://github.com/joewandy/hlda,-1
Ontology-enhanced Prompt-tuning for Few-shot Learning,0.794797,"Few-shot Learning (FSL) is aimed to make predictions based on a limited
number of samples. Structured data such as knowledge graphs and ontology
libraries has been leveraged to benefit the few-shot setting in various tasks.
However, the priors adopted by the existing methods suffer from challenging
knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder
the performance for few-shot learning. In this study, we explore knowledge
injection for FSL with pre-trained language models and propose
ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the
ontology transformation based on the external knowledge graph to address the
knowledge missing issue, which fulfills and converts structure knowledge to
text. We further introduce span-sensitive knowledge injection via a visible
matrix to select informative knowledge to handle the knowledge noise issue. To
bridge the gap between knowledge and text, we propose a collective training
algorithm to optimize representations jointly. We evaluate our proposed
OntoPrompt in three tasks, including relation extraction, event extraction, and
knowledge graph completion, with eight datasets. Experimental results
demonstrate that our approach can obtain better few-shot performance than
baselines.",None,-1
Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language Models,0.0305711,"With increasing scale, large language models demonstrate both quantitative
improvement and new qualitative capabilities, especially as zero-shot learners,
like GPT-3. However, these results rely heavily on delicate prompt design and
large computation. In this work, we explore whether the strong zero-shot
ability could be achieved at a smaller model scale without any external
supervised data. To achieve this goal, we revisit masked language modeling and
present a geometry-guided self-supervised learning method (Go-tuningfor short)
by taking a small number of task-aware self-supervised data to update language
models further. Experiments show that Go-tuning can enable T5-small (80M)
competitive zero-shot results compared with large language models, such as
T5-XL (3B). We also apply Go-tuning on multi-task settings and develop a
multi-task model, mgo-T5 (250M). It can reach the average performance of OPT
(175B) on 9 datasets.",None,3525
StructNeRF: Neural Radiance Fields for Indoor Scenes with Structural Hints,0.395549,"Neural Radiance Fields (NeRF) achieve photo-realistic view synthesis with
densely captured input images. However, the geometry of NeRF is extremely
under-constrained given sparse views, resulting in significant degradation of
novel view synthesis quality. Inspired by self-supervised depth estimation
methods, we propose StructNeRF, a solution to novel view synthesis for indoor
scenes with sparse inputs. StructNeRF leverages the structural hints naturally
embedded in multi-view inputs to handle the unconstrained geometry issue in
NeRF. Specifically, it tackles the texture and non-texture regions
respectively: a patch-based multi-view consistent photometric loss is proposed
to constrain the geometry of textured regions; for non-textured ones, we
explicitly restrict them to be 3D consistent planes. Through the dense
self-supervised depth constraints, our method improves both the geometry and
the view synthesis performance of NeRF without any additional training on
external data. Extensive experiments on several real-world datasets demonstrate
that StructNeRF surpasses state-of-the-art methods for indoor scenes with
sparse inputs both quantitatively and qualitatively.",https://github.com/barbararoessle/dense-depth-priors-nerf,-1
EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning,0.354405,"Pre-trained vision-language models (VLMs) have achieved impressive results in
a range of vision-language tasks. However, popular VLMs usually consist of
hundreds of millions of parameters which brings challenges for fine-tuning and
deployment in real-world applications due to space, memory, and latency
constraints. In this work, we introduce a distilling then pruning framework to
compress large vision-language models into smaller, faster, and more accurate
ones. We first shrink the size of a pre-trained large VLM and apply knowledge
distillation in the vision-language pre-training stage to obtain a
task-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm
to automatically infer the importance of vision and language modalities for
different downstream tasks and adaptively remove redundant structures and
neurons in different encoders with controllable target sparsity. We apply our
framework to train EfficientVLM, a fast and accurate vision-language model
consisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers,
accounting for only 93 million parameters in total, which is 44.3% of the
teacher model. EfficientVLM retains 98.4% performance of the teacher model and
accelerates its inference speed by 2.2x. EfficientVLM achieves a large absolute
improvement over previous SoTA efficient VLMs of similar sizes by a large
margin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2
(+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation
(CIDEr +6.5), demonstrating a large potential on training lightweight VLMs.",https://github.com/swaggy-TN/EfficientVLM,-1
Scale-free and Task-agnostic Attack: Generating Photo-realistic Adversarial Patterns with Patch Quilting Generator,0.233239,"\noindent Traditional L_p norm-restricted image attack algorithms suffer from
poor transferability to black box scenarios and poor robustness to defense
algorithms. Recent CNN generator-based attack approaches can synthesize
unrestricted and semantically meaningful entities to the image, which is shown
to be transferable and robust. However, such methods attack images by either
synthesizing local adversarial entities, which are only suitable for attacking
specific contents or performing global attacks, which are only applicable to a
specific image scale. In this paper, we propose a novel Patch Quilting
Generative Adversarial Networks (PQ-GAN) to learn the first scale-free CNN
generator that can be applied to attack images with arbitrary scales for
various computer vision tasks. The principal investigation on transferability
of the generated adversarial examples, robustness to defense frameworks, and
visual quality assessment show that the proposed PQG-based attack framework
outperforms the other nine state-of-the-art adversarial attack approaches when
attacking the neural networks trained on two standard evaluation datasets
(i.e., ImageNet and CityScapes).",https://anonymous.4open.science/r/PQAttack-0781,13692
CORE: Consistent Representation Learning for Face Forgery Detection,0.694656,"Face manipulation techniques develop rapidly and arouse widespread public
concerns. Despite that vanilla convolutional neural networks achieve acceptable
performance, they suffer from the overfitting issue. To relieve this issue,
there is a trend to introduce some erasing-based augmentations. We find that
these methods indeed attempt to implicitly induce more consistent
representations for different augmentations via assigning the same label for
different augmented images. However, due to the lack of explicit
regularization, the consistency between different representations is less
satisfactory. Therefore, we constrain the consistency of different
representations explicitly and propose a simple yet effective framework,
COnsistent REpresentation Learning (CORE). Specifically, we first capture the
different representations with different augmentations, then regularize the
cosine distance of the representations to enhance the consistency. Extensive
experiments (in-dataset and cross-dataset) demonstrate that CORE performs
favorably against state-of-the-art face forgery detection methods.",https://github.com/niyunsheng/CORE,-1
HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders,0.217363,"In this paper, HeadPosr is proposed to predict the head poses using a single
RGB image. \textit{HeadPosr} uses a novel architecture which includes a
transformer encoder. In concrete, it consists of: (1) backbone; (2) connector;
(3) transformer encoder; (4) prediction head. The significance of using a
transformer encoder for HPE is studied. An extensive ablation study is
performed on varying the (1) number of encoders; (2) number of heads; (3)
different position embeddings; (4) different activations; (5) input channel
size, in a transformer used in HeadPosr. Further studies on using: (1)
different backbones, (2) using different learning rates are also shown. The
elaborated experiments and ablations studies are conducted using three
different open-source widely used datasets for HPE, i.e., 300W-LP, AFLW2000,
and BIWI datasets. Experiments illustrate that \textit{HeadPosr} outperforms
all the state-of-art methods including both the landmark-free and the others
based on using landmark or depth estimation on the AFLW2000 dataset and BIWI
datasets when trained with 300W-LP. It also outperforms when averaging the
results from the compared datasets, hence setting a benchmark for the problem
of HPE, also demonstrating the effectiveness of using transformers over the
state-of-the-art.",None,-1
Third Time's the Charm? Image and Video Editing with StyleGAN3,0.894966,"StyleGAN is arguably one of the most intriguing and well-studied generative
models, demonstrating impressive performance in image generation, inversion,
and manipulation. In this work, we explore the recent StyleGAN3 architecture,
compare it to its predecessor, and investigate its unique advantages, as well
as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained
on unaligned data, one can still use aligned data for training, without
hindering the ability to generate unaligned imagery. Next, our analysis of the
disentanglement of the different latent spaces of StyleGAN3 indicates that the
commonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts,
underscoring the benefits of using the StyleSpace for fine-grained editing.
Considering image inversion, we observe that existing encoder-based techniques
struggle when trained on unaligned data. We therefore propose an encoding
scheme trained solely on aligned data, yet can still invert unaligned images.
Finally, we introduce a novel video inversion and editing workflow that
leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce
texture sticking and expand the field of view of the edited video.",https://yuval-alaluf.github.io/stylegan3-editing/,62088
Event knowledge in large language models: the gap between the impossible and the unlikely,0.927765,"Word co-occurrence patterns in language corpora contain a surprising amount
of conceptual knowledge. Large language models (LLMs), trained to predict words
in context, leverage these patterns to achieve impressive performance on
diverse semantic tasks requiring world knowledge. An important but understudied
question about LLMs' semantic abilities is whether they acquire generalized
knowledge of common events. Here, we test whether five pre-trained LLMs (from
2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions
of agent-patient interactions than to minimally different implausible versions
of the same event. Using three curated sets of minimal sentence pairs (total
n=1,215), we found that pre-trained LLMs possess substantial event knowledge,
outperforming other distributional language models. In particular, they almost
always assign higher likelihood to possible vs. impossible events (The teacher
bought the laptop vs. The laptop bought the teacher). However, LLMs show less
consistent preferences for likely vs. unlikely events (The nanny tutored the
boy vs. The boy tutored the nanny). In follow-up analyses, we show that (i) LLM
scores are driven by both plausibility and surface-level sentence features,
(ii) LLM scores generalize well across syntactic variants (active vs. passive
constructions) but less well across semantic variants (synonymous sentences),
(iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence
plausibility serves as an organizing dimension in internal LLM representations.
Overall, our results show that important aspects of event knowledge naturally
emerge from distributional linguistic patterns, but also highlight a gap
between representations of possible/impossible and likely/unlikely events.",https://github.com/carina-kauf/lm-event-knowledge,-1
Augmenting Knowledge Graphs for Better Link Prediction,0.374277,"Embedding methods have demonstrated robust performance on the task of link
prediction in knowledge graphs, by mostly encoding entity relationships. Recent
methods propose to enhance the loss function with a literal-aware term. In this
paper, we propose KGA: a knowledge graph augmentation method that incorporates
literals in an embedding model without modifying its loss function. KGA
discretizes quantity and year values into bins, and chains these bins both
horizontally, modeling neighboring values, and vertically, modeling multiple
levels of granularity. KGA is scalable and can be used as a pre-processing step
for any existing knowledge graph embedding model. Experiments on legacy
benchmarks and a new large benchmark, DWD, show that augmenting the knowledge
graph with quantities and years is beneficial for predicting both entities and
numbers, as KGA outperforms the vanilla models and other relevant baselines.
Our ablation studies confirm that both quantities and years contribute to KGA's
performance, and that its performance depends on the discretization and binning
settings. We make the code, models, and the DWD benchmark publicly available to
facilitate reproducibility and future research.",https://github.com/Otamio/KGA/,-1
AI-Aided Integrated Terrestrial and Non-Terrestrial 6G Solutions for Sustainable Maritime Networking,0.816944,"The maritime industry is experiencing a technological revolution that affects
shipbuilding, operation of both seagoing and inland vessels, cargo management,
and working practices in harbors. This ongoing transformation is driven by the
ambition to make the ecosystem more sustainable and cost-efficient.
Digitalization and automation help achieve these goals by transforming shipping
and cruising into a much more cost- and energy-efficient, and decarbonized
industry segment. The key enablers in these processes are always-available
connectivity and content delivery services, which can not only aid shipping
companies in improving their operational efficiency and reducing carbon
emissions but also contribute to enhanced crew welfare and passenger
experience. Due to recent advancements in integrating high-capacity and
ultra-reliable terrestrial and non-terrestrial networking technologies,
ubiquitous maritime connectivity is becoming a reality. To cope with the
increased complexity of managing these integrated systems, this article
advocates the use of artificial intelligence and machine learning-based
approaches to meet the service requirements and energy efficiency targets in
various maritime communications scenarios.",None,-1
Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild,0.540495,"Talking face generation with great practical significance has attracted more
attention in recent audio-visual studies. How to achieve accurate lip
synchronization is a long-standing challenge to be further investigated.
Motivated by xxx, in this paper, an AttnWav2Lip model is proposed by
incorporating spatial attention module and channel attention module into
lip-syncing strategy. Rather than focusing on the unimportant regions of the
face image, the proposed AttnWav2Lip model is able to pay more attention on the
lip region reconstruction. To our limited knowledge, this is the first attempt
to introduce attention mechanism to the scheme of talking face generation. An
extensive experiments have been conducted to evaluate the effectiveness of the
proposed model. Compared to the baseline measured by LSE-D and LSE-C metrics, a
superior performance has been demonstrated on the benchmark lip synthesis
datasets, including LRW, LRS2 and LRS3.",None,-1
Cross-Enhancement Transformer for Action Segmentation,0.329097,"Temporal convolutions have been the paradigm of choice in action
segmentation, which enhances long-term receptive fields by increasing
convolution layers. However, high layers cause the loss of local information
necessary for frame recognition. To solve the above problem, a novel
encoder-decoder structure is proposed in this paper, called Cross-Enhancement
Transformer. Our approach can be effective learning of temporal structure
representation with interactive self-attention mechanism. Concatenated each
layer convolutional feature maps in encoder with a set of features in decoder
produced via self-attention. Therefore, local and global information are used
in a series of frame actions simultaneously. In addition, a new loss function
is proposed to enhance the training process that penalizes over-segmentation
errors. Experiments show that our framework performs state-of-the-art on three
challenging datasets: 50Salads, Georgia Tech Egocentric Activities and the
Breakfast dataset.",None,-1
Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality,0.999972,"We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground.",None,20526
Accelerating Code Search with Deep Hashing and Code Classification,0.47169,"Code search is to search reusable code snippets from source code corpus based
on natural languages queries. Deep learning-based methods of code search have
shown promising results. However, previous methods focus on retrieval accuracy
but lacked attention to the efficiency of the retrieval process. We propose a
novel method CoSHC to accelerate code search with deep hashing and code
classification, aiming to perform an efficient code search without sacrificing
too much accuracy. To evaluate the effectiveness of CoSHC, we apply our method
to five code search models. Extensive experimental results indicate that
compared with previous code search baselines, CoSHC can save more than 90% of
retrieval time meanwhile preserving at least 99% of retrieval accuracy.",None,-1
Learning to Express in Knowledge-Grounded Conversation,0.340851,"Grounding dialogue generation by extra knowledge has shown great potentials
towards building a system capable of replying with knowledgeable and engaging
responses. Existing studies focus on how to synthesize a response with proper
knowledge, yet neglect that the same knowledge could be expressed differently
by speakers even under the same context. In this work, we mainly consider two
aspects of knowledge expression, namely the structure of the response and style
of the content in each part. We therefore introduce two sequential latent
variables to represent the structure and the content style respectively. We
propose a segmentation-based generation model and optimize the model by a
variational approach to discover the underlying pattern of knowledge expression
in a response. Evaluation results on two benchmarks indicate that our model can
learn the structure style defined by a few examples and generate responses in
desired content style.",https://github.com/nlpxucan/ZRKGC,-1
Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas,0.458816,"Vision and language navigation (VLN) is a challenging visually-grounded
language understanding task. Given a natural language navigation instruction, a
visual agent interacts with a graph-based environment equipped with panorama
images and tries to follow the described route. Most prior work has been
conducted in indoor scenarios where best results were obtained for navigation
on routes that are similar to the training routes, with sharp drops in
performance when testing on unseen environments. We focus on VLN in outdoor
scenarios and find that in contrast to indoor VLN, most of the gain in outdoor
VLN on unseen data is due to features like junction type embedding or heading
delta that are specific to the respective environment graph, while image
information plays a very minor role in generalizing VLN to unseen outdoor
areas. These findings show a bias to specifics of graph representations of
urban environments, demanding that VLN tasks grow in scale and diversity of
geographical environments.",https://github.com/raphael-sch/,-1
Label-Driven Denoising Framework for Multi-Label Few-Shot Aspect Category Detection,0.290915,"Multi-Label Few-Shot Aspect Category Detection (FS-ACD) is a new sub-task of
aspect-based sentiment analysis, which aims to detect aspect categories
accurately with limited training instances. Recently, dominant works use the
prototypical network to accomplish this task, and employ the attention
mechanism to extract keywords of aspect category from the sentences to produce
the prototype for each aspect. However, they still suffer from serious noise
problems: (1) due to lack of sufficient supervised data, the previous methods
easily catch noisy words irrelevant to the current aspect category, which
largely affects the quality of the generated prototype; (2) the
semantically-close aspect categories usually generate similar prototypes, which
are mutually noisy and confuse the classifier seriously. In this paper, we
resort to the label information of each aspect to tackle the above problems,
along with proposing a novel Label-Driven Denoising Framework (LDF). Extensive
experimental results show that our framework achieves better performance than
other state-of-the-art methods.",https://github.com/1429904852/LDF,4048
Toward More Effective Human Evaluation for Machine Translation,0.149852,"Improvements in text generation technologies such as machine translation have
necessitated more costly and time-consuming human evaluation procedures to
ensure an accurate signal. We investigate a simple way to reduce cost by
reducing the number of text segments that must be annotated in order to
accurately predict a score for a complete test set. Using a sampling approach,
we demonstrate that information from document membership and automatic metrics
can help improve estimates compared to a pure random sampling baseline. We
achieve gains of up to 20% in average absolute error by leveraging stratified
sampling and control variates. Our techniques can improve estimates made from a
fixed annotation budget, are easy to implement, and can be applied to any
problem with structure similar to the one we study.",None,-1
Consistent Range Approximation for Fair Predictive Modeling,0.319464,"This paper proposes a novel framework for certifying the fairness of
predictive models trained on biased data. It draws from query answering for
incomplete and inconsistent databases to formulate the problem of consistent
range approximation (CRA) of fairness queries for a predictive model on a
target population. The framework employs background knowledge of the data
collection process and biased data, working with or without limited statistics
about the target population, to compute a range of answers for fairness
queries. Using CRA, the framework builds predictive models that are certifiably
fair on the target population, regardless of the availability of external data
during training. The framework's efficacy is demonstrated through evaluations
on real data, showing substantial improvement over existing state-of-the-art
methods.",https://github.com/lodino/Crab,-1
"GMM-IL: Image Classification using Incrementally Learnt, Independent Probabilistic Models for Small Sample Sizes",0.0321443,"Current deep learning classifiers, carry out supervised learning and store
class discriminatory information in a set of shared network weights. These
weights cannot be easily altered to incrementally learn additional classes,
since the classification weights all require retraining to prevent old class
information from being lost and also require the previous training data to be
present. We present a novel two stage architecture which couples visual feature
learning with probabilistic models to represent each class in the form of a
Gaussian Mixture Model. By using these independent class representations within
our classifier, we outperform a benchmark of an equivalent network with a
Softmax head, obtaining increased accuracy for sample sizes smaller than 12 and
increased weighted F1 score for 3 imbalanced class profiles in that sample
range. When learning new classes our classifier exhibits no catastrophic
forgetting issues and only requires the new classes' training images to be
present. This enables a database of growing classes over time which can be
visually indexed and reasoned over.",None,-1
Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances,0.2546,"Dialogue State Tracking (DST) is primarily evaluated using Joint Goal
Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue
state exactly matches the prediction. Generally in DST, the dialogue state or
belief state for a given turn contains all the intents shown by the user till
that turn. Due to this cumulative nature of the belief state, it is difficult
to get a correct prediction once a misprediction has occurred. Thus, although
being a useful metric, it can be harsh at times and underestimate the true
potential of a DST model. Moreover, an improvement in JGA can sometimes
decrease the performance of turn-level or non-cumulative belief state
prediction due to inconsistency in annotations. So, using JGA as the only
metric for model selection may not be ideal for all scenarios. In this work, we
discuss various evaluation metrics used for DST along with their shortcomings.
To address the existing issues, we propose a new evaluation metric named
Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike
JGA, it tries to give penalized rewards to mispredictions that are locally
correct i.e. the root cause of the error is an earlier turn. By doing so, FGA
considers the performance of both cumulative and turn-level prediction flexibly
and provides a better insight than the existing metrics. We also show that FGA
is a better discriminator of DST model performance.",https://github.com/SuvodipDey/FGA,-1
CDialog: A Multi-turn Covid-19 Conversation Dataset for Entity-Aware Dialog Generation,0.433811,"The development of conversational agents to interact with patients and
deliver clinical advice has attracted the interest of many researchers,
particularly in light of the COVID-19 pandemic. The training of an end-to-end
neural based dialog system, on the other hand, is hampered by a lack of
multi-turn medical dialog corpus. We make the very first attempt to release a
high-quality multi-turn Medical Dialog dataset relating to Covid-19 disease
named CDialog, with over 1K conversations collected from the online medical
counselling websites. We annotate each utterance of the conversation with seven
different categories of medical entities, including diseases, symptoms, medical
tests, medical history, remedies, medications and other aspects as additional
labels. Finally, we propose a novel neural medical dialog system based on the
CDialog dataset to advance future research on developing automated medical
dialog systems. We use pre-trained language models for dialogue generation,
incorporating annotated medical entities, to generate a virtual doctor's
response that addresses the patient's query. Experimental results show that the
proposed dialog models perform comparably better when supplemented with entity
information and hence can improve the response quality.",https://github.com/deekshaVarshney/CDialog,-1
Recurrent Memory Transformer,0.940889,"Transformer-based models show their effectiveness across multiple domains and
tasks. The self-attention allows to combine information from all sequence
elements into context-aware representations. However, global and local
information has to be stored mostly in the same element-wise representations.
Moreover, the length of an input sequence is limited by quadratic computational
complexity of self-attention.
  In this work, we propose and study a memory-augmented segment-level recurrent
Transformer (RMT). Memory allows to store and process local and global
information as well as to pass information between segments of the long
sequence with the help of recurrence.
  We implement a memory mechanism with no changes to Transformer model by
adding special memory tokens to the input or output sequence. Then the model is
trained to control both memory operations and sequence representations
processing.
  Results of experiments show that RMT performs on par with the Transformer-XL
on language modeling for smaller memory sizes and outperforms it for tasks that
require longer sequence processing. We show that adding memory tokens to Tr-XL
is able to improve its performance. This makes Recurrent Memory Transformer a
promising architecture for applications that require learning of long-term
dependencies and general purpose in memory processing, such as algorithmic
tasks and reasoning.",https://github.com/booydar/LM-RMT,-1
"Event Causality Identification with Causal News Corpus -- Shared Task 3, CASE 2022",0.797048,"The Event Causality Identification Shared Task of CASE 2022 involved two
subtasks working on the Causal News Corpus. Subtask 1 required participants to
predict if a sentence contains a causal relation or not. This is a supervised
binary classification task. Subtask 2 required participants to identify the
Cause, Effect and Signal spans per causal sentence. This could be seen as a
supervised sequence labeling task. For both subtasks, participants uploaded
their predictions for a held-out test set, and ranking was done based on binary
F1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes
the work of the 17 teams that submitted their results to our competition and 12
system description papers that were received. The best F1 scores achieved for
Subtask 1 and 2 were 86.19% and 54.15%, respectively. All the top-performing
approaches involved pre-trained language models fine-tuned to the targeted
task. We further discuss these approaches and analyze errors across
participants' systems in this paper.",https://github.com/tanfiona/UniCausal,3459
An Ultra-low Power TinyML System for Real-time Visual Processing at Edge,0.430048,"Tiny machine learning (TinyML), executing AI workloads on resource and power
strictly restricted systems, is an important and challenging topic. This brief
firstly presents an extremely tiny backbone to construct high efficiency CNN
models for various visual tasks. Then, a specially designed neural co-processor
(NCP) is interconnected with MCU to build an ultra-low power TinyML system,
which stores all features and weights on chip and completely removes both of
latency and power consumption in off-chip memory access. Furthermore, an
application specific instruction-set is further presented for realizing agile
development and rapid deployment. Extensive experiments demonstrate that the
proposed TinyML system based on our model, NCP and instruction set yields
considerable accuracy and achieves a record ultra-low power of 160mW while
implementing object detection and recognition at 30FPS. The demo video is
available on \url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}.",None,-1
C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References,0.2847,"We consider the problem of pretraining a two-stage open-domain question
answering (QA) system (retriever + reader) with strong transfer capabilities.
The key challenge is how to construct a large amount of high-quality
question-answer-context triplets without task-specific annotations.
Specifically, the triplets should align well with downstream tasks by: (i)
covering a wide range of domains (for open-domain applications), (ii) linking a
question to its semantically relevant context with supporting evidence (for
training the retriever), and (iii) identifying the correct answer in the
context (for training the reader). Previous pretraining approaches generally
fall short of one or more of these requirements. In this work, we automatically
construct a large-scale corpus that meets all three criteria by consulting
millions of references cited within Wikipedia. The well-aligned pretraining
signals benefit both the retriever and the reader significantly. Our pretrained
retriever leads to 2%-10% absolute gains in top-20 accuracy. And with our
pretrained reader, the entire system improves by up to 4% in exact match.",https://github.com/xiangyue9607/C-MORE,-1
University of Cape Town's WMT22 System: Multilingual Machine Translation for Southern African Languages,0.0784818,"The paper describes the University of Cape Town's submission to the
constrained track of the WMT22 Shared Task: Large-Scale Machine Translation
Evaluation for African Languages. Our system is a single multilingual
translation model that translates between English and 8 South / South East
African Languages, as well as between specific pairs of the African languages.
We used several techniques suited for low-resource machine translation (MT),
including overlap BPE, back-translation, synthetic training data generation,
and adding more translation directions during training. Our results show the
value of these techniques, especially for directions where very little or no
bilingual training data is available.",https://github.com/Khalid-Nabigh/UCT-s-WMT22-shared-task,-1
A Variational Hierarchical Model for Neural Cross-Lingual Summarization,0.895683,"The goal of the cross-lingual summarization (CLS) is to convert a document in
one language (e.g., English) to a summary in another one (e.g., Chinese).
Essentially, the CLS task is the combination of machine translation (MT) and
monolingual summarization (MS), and thus there exists the hierarchical
relationship between MT\&MS and CLS. Existing studies on CLS mainly focus on
utilizing pipeline methods or jointly training an end-to-end model through an
auxiliary MT or MS objective. However, it is very challenging for the model to
directly conduct CLS as it requires both the abilities to translate and
summarize. To address this issue, we propose a hierarchical model for the CLS
task, based on the conditional variational auto-encoder. The hierarchical model
contains two kinds of latent variables at the local and global levels,
respectively. At the local level, there are two latent variables, one for
translation and the other for summarization. As for the global level, there is
another latent variable for cross-lingual summarization conditioned on the two
local-level variables. Experiments on two language directions (English-Chinese)
verify the effectiveness and superiority of the proposed approach. In addition,
we show that our model is able to generate better cross-lingual summaries than
comparison models in the few-shot setting.",https://github.com/XL2248/VHM,-1
On Almost-Sure Intention Deception Planning that Exploits Imperfect Observers,0.492248,"Intention deception involves computing a strategy which deceives the opponent
into a wrong belief about the agent's intention or objective. This paper
studies a class of probabilistic planning problems with intention deception and
investigates how a defender's limited sensing modality can be exploited by an
attacker to achieve its attack objective almost surely (with probability one)
while hiding its intention. In particular, we model the attack planning in a
stochastic system modeled as a Markov decision process (MDP). The attacker is
to reach some target states while avoiding unsafe states in the system and
knows that his behavior is monitored by a defender with partial observations.
Given partial state observations for the defender, we develop qualitative
intention deception planning algorithms that construct attack strategies to
play against an action-visible defender and an action-invisible defender,
respectively. The synthesized attack strategy not only ensures the attack
objective is satisfied almost surely but also deceives the defender into
believing that the observed behavior is generated by a normal/legitimate user
and thus failing to detect the presence of an attack. We show the proposed
algorithms are correct and complete and illustrate the deceptive planning
methods with examples.",None,-1
Empowering Graph Representation Learning with Test-Time Graph Transformation,0.967751,"As powerful tools for representation learning on graphs, graph neural
networks (GNNs) have facilitated various applications from drug discovery to
recommender systems. Nevertheless, the effectiveness of GNNs is immensely
challenged by issues related to data quality, such as distribution shift,
abnormal features and adversarial attacks. Recent efforts have been made on
tackling these issues from a modeling perspective which requires additional
cost of changing model architectures or re-training model parameters. In this
work, we provide a data-centric view to tackle these issues and propose a graph
transformation framework named GTrans which adapts and refines graph data at
test time to achieve better performance. We provide theoretical analysis on the
design of the framework and discuss why adapting graph data works better than
adapting the model. Extensive experiments have demonstrated the effectiveness
of GTrans on three distinct scenarios for eight benchmark datasets where
suboptimal data is presented. Remarkably, GTrans performs the best in most
cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on
three experimental settings. Code is released at
https://github.com/ChandlerBang/GTrans.",https://github.com/ChandlerBang/GTrans,-1
TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation,0.597776,"Knowledge graph embedding (KGE) aims to learn continuous vectors of relations
and entities in knowledge graph. Recently, transition-based KGE methods have
achieved promising performance, where the single relation vector learns to
translate head entity to tail entity. However, this scoring pattern is not
suitable for complex scenarios where the same entity pair has different
relations. Previous models usually focus on the improvement of entity
representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single
relation vector. In this paper, we propose a novel transition-based method,
TranS, for knowledge graph embedding. The single relation vector in traditional
scoring patterns is replaced with synthetic relation representation, which can
solve these issues effectively and efficiently. Experiments on a large
knowledge graph dataset, ogbl-wikikg2, show that our model achieves
state-of-the-art results.",None,-1
Vector Quantized Image-to-Image Translation,0.390459,"Current image-to-image translation methods formulate the task with
conditional generation models, leading to learning only the recolorization or
regional changes as being constrained by the rich structural information
provided by the conditional contexts. In this work, we propose introducing the
vector quantization technique into the image-to-image translation framework.
The vector quantized content representation can facilitate not only the
translation, but also the unconditional distribution shared among different
domains. Meanwhile, along with the disentangled style representation, the
proposed method further enables the capability of image extension with
flexibility in both intra- and inter-domains. Qualitative and quantitative
experiments demonstrate that our framework achieves comparable performance to
the state-of-the-art image-to-image translation and image extension methods.
Compared to methods for individual tasks, the proposed method, as a unified
framework, unleashes applications combining image-to-image translation,
unconditional generation, and image extension altogether. For example, it
provides style variability for image generation and extension, and equips
image-to-image translation with further extension capabilities.",https://cyj407.github.io/VQ-I2I/,-1
INFACT: An Online Human Evaluation Framework for Conversational Recommendation,0.0849733,"Conversational recommender systems (CRS) are interactive agents that support
their users in recommendation-related goals through multi-turn conversations.
Generally, a CRS can be evaluated in various dimensions. Today's CRS mainly
rely on offline(computational) measures to assess the performance of their
algorithms in comparison to different baselines. However, offline measures can
have limitations, for example, when the metrics for comparing a newly generated
response with a ground truth do not correlate with human perceptions, because
various alternative generated responses might be suitable too in a given dialog
situation. Current research on machine learning-based CRS models therefore
acknowledges the importance of humans in the evaluation process, knowing that
pure offline measures may not be sufficient in evaluating a highly interactive
system like a CRS.",https://github.com/ahtsham58/INFACT,-1
Disentangling visual and written concepts in CLIP,0.513664,"The CLIP network measures the similarity between natural text and images; in
this work, we investigate the entanglement of the representation of word images
and natural images in its image encoder. First, we find that the image encoder
has an ability to match word images with natural images of scenes described by
those words. This is consistent with previous research that suggests that the
meaning and the spelling of a word might be entangled deep within the network.
On the other hand, we also find that CLIP has a strong ability to match
nonsense words, suggesting that processing of letters is separated from
processing of their meaning. To explicitly determine whether the spelling
capability of CLIP is separable, we devise a procedure for identifying
representation subspaces that selectively isolate or eliminate spelling
capabilities. We benchmark our methods against a range of retrieval tasks, and
we also test them by measuring the appearance of text in CLIP-guided generated
images. We find that our methods are able to cleanly separate spelling
capabilities of CLIP from the visual processing of natural images.",None,-1
Guided Unsupervised Learning by Subaperture Decomposition for Ocean SAR Image Retrieval,0.686632,"Spaceborne synthetic aperture radar (SAR) can provide accurate images of the
ocean surface roughness day-or-night in nearly all weather conditions, being an
unique asset for many geophysical applications. Considering the huge amount of
data daily acquired by satellites, automated techniques for physical features
extraction are needed. Even if supervised deep learning methods attain
state-of-the-art results, they require great amount of labeled data, which are
difficult and excessively expensive to acquire for ocean SAR imagery. To this
end, we use the subaperture decomposition (SD) algorithm to enhance the
unsupervised learning retrieval on the ocean surface, empowering ocean
researchers to search into large ocean databases. We empirically prove that SD
improve the retrieval precision with over 20% for an unsupervised transformer
auto-encoder network. Moreover, we show that SD brings important performance
boost when Doppler centroid images are used as input data, leading the way to
new unsupervised physics guided retrieval algorithms.",None,-1
HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks,0.534684,"Implicit neural representations (INRs) are a rapidly growing research field,
which provides alternative ways to represent multimedia signals. Recent
applications of INRs include image super-resolution, compression of
high-dimensional signals, or 3D rendering. However, these solutions usually
focus on visual data, and adapting them to the audio domain is not trivial.
Moreover, it requires a separately trained model for every data sample. To
address this limitation, we propose HyperSound, a meta-learning method
leveraging hypernetworks to produce INRs for audio signals unseen at training
time. We show that our approach can reconstruct sound waves with quality
comparable to other state-of-the-art models.",None,-1
rPPG-Toolbox: Deep Remote PPG Toolbox,0.746494,"Camera-based physiological measurement is a fast growing field of computer
vision. Remote photoplethysmography (rPPG) utilizes imaging devices (e.g.,
cameras) to measure the peripheral blood volume pulse (BVP) via
photoplethysmography, and enables cardiac measurement via webcams and
smartphones. However, the task is non-trivial with important pre-processing,
modeling, and post-processing steps required to obtain state-of-the-art
results. Replication of results and benchmarking of new models is critical for
scientific progress; however, as with many other applications of deep learning,
reliable codebases are not easy to find or use. We present a comprehensive
toolbox, rPPG-Toolbox, that contains unsupervised and supervised rPPG models
with support for public benchmark datasets, data augmentation, and systematic
evaluation: \url{https://github.com/ubicomplab/rPPG-Toolbox}",https://github.com/ubicomplab/rPPG-Toolbox,-1
DeepGen: Diverse Search Ad Generation and Real-Time Customization,0.314656,"We present DeepGen, a system deployed at web scale for automatically creating
sponsored search advertisements (ads) for BingAds customers. We leverage
state-of-the-art natural language generation (NLG) models to generate fluent
ads from advertiser's web pages in an abstractive fashion and solve practical
issues such as factuality and inference speed. In addition, our system creates
a customized ad in real-time in response to the user's search query, therefore
highlighting different aspects of the same product based on what the user is
looking for. To achieve this, our system generates a diverse choice of smaller
pieces of the ad ahead of time and, at query time, selects the most relevant
ones to be stitched into a complete ad. We improve generation diversity by
training a controllable NLG model to generate multiple ads for the same web
page highlighting different selling points. Our system design further improves
diversity horizontally by first running an ensemble of generation models
trained with different objectives and then using a diversity sampling algorithm
to pick a diverse subset of generation results for online selection.
Experimental results show the effectiveness of our proposed system design. Our
system is currently deployed in production, serving ${\sim}4\%$ of global ads
served in Bing.",None,-1
ELLE: Efficient Lifelong Pre-training for Emerging Data,0.478303,"Current pre-trained language models (PLM) are typically trained with static
data, ignoring that in real-world scenarios, streaming data of various sources
may continuously grow. This requires PLMs to integrate the information from all
the sources in a lifelong manner. Although this goal could be achieved by
exhaustive pre-training on all the existing data, such a process is known to be
computationally expensive. To this end, we propose ELLE, aiming at efficient
lifelong pre-training for emerging data. Specifically, ELLE consists of (1)
function preserved model expansion, which flexibly expands an existing PLM's
width and depth to improve the efficiency of knowledge acquisition; and (2)
pre-trained domain prompts, which disentangle the versatile knowledge learned
during pre-training and stimulate the proper knowledge for downstream tasks. We
experiment ELLE with streaming data from 5 domains on BERT and GPT. The results
show the superiority of ELLE over various lifelong learning baselines in both
pre-training efficiency and downstream performances. The codes are publicly
available at https://github.com/thunlp/ELLE.",https://github.com/thunlp/ELLE,-1
Text Detection Forgot About Document OCR,0.157079,"Detection and recognition of text from scans and other images, commonly
denoted as Optical Character Recognition (OCR), is a widely used form of
automated document processing with a number of methods available. Yet OCR
systems still do not achieve 100% accuracy, requiring human corrections in
applications where correct readout is essential. Advances in machine learning
enabled even more challenging scenarios of text detection and recognition
""in-the-wild"" - such as detecting text on objects from photographs of complex
scenes. While the state-of-the-art methods for in-the-wild text recognition are
typically evaluated on complex scenes, their performance in the domain of
documents is typically not published, and a comprehensive comparison with
methods for document OCR is missing. This paper compares several methods
designed for in-the-wild text recognition and for document text recognition,
and provides their evaluation on the domain of structured documents. The
results suggest that state-of-the-art methods originally proposed for
in-the-wild text detection also achieve competitive results on document text
detection, outperforming available OCR methods. We argue that the application
of document OCR should not be omitted in evaluation of text detection and
recognition methods.",None,-1
Weakly Supervised 3D Multi-person Pose Estimation for Large-scale Scenes based on Monocular Camera and Single LiDAR,0.896511,"Depth estimation is usually ill-posed and ambiguous for monocular
camera-based 3D multi-person pose estimation. Since LiDAR can capture accurate
depth information in long-range scenes, it can benefit both the global
localization of individuals and the 3D pose estimation by providing rich
geometry features. Motivated by this, we propose a monocular camera and single
LiDAR-based method for 3D multi-person pose estimation in large-scale scenes,
which is easy to deploy and insensitive to light. Specifically, we design an
effective fusion strategy to take advantage of multi-modal input data,
including images and point cloud, and make full use of temporal information to
guide the network to learn natural and coherent human motions. Without relying
on any 3D pose annotations, our method exploits the inherent geometry
constraints of point cloud for self-supervision and utilizes 2D keypoints on
images for weak supervision. Extensive experiments on public datasets and our
newly collected dataset demonstrate the superiority and generalization
capability of our proposed method.",https://github.com/4DVLab/FusionPose.git,-1
YATO: Yet Another deep learning based Text analysis Open toolkit,0.0473506,"We introduce YATO, an open-source, easy-to-use toolkit for text analysis with
deep learning. Different from existing heavily engineered toolkits and
platforms, YATO is lightweight and user-friendly for researchers from
cross-disciplinary areas. Designed in a hierarchical structure, YATO supports
free combinations of three types of widely used features including 1)
traditional neural networks (CNN, RNN, etc.); 2) pre-trained language models
(BERT, RoBERTa, ELECTRA, etc.); and 3) user-customized neural features via a
simple configurable file. Benefiting from the advantages of flexibility and
ease of use, YATO can facilitate fast reproduction and refinement of
state-of-the-art NLP models, and promote the cross-disciplinary applications of
NLP techniques. The code, examples, and documentation are publicly available at
https://github.com/jiesutd/YATO. A demo video is also available at
https://www.youtube.com/playlist?list=PLJ0mhzMcRuDUlTkzBfAftOqiJRxYTTjXH.",https://github.com/jiesutd/YATO,-1
Counterfactual Plans under Distributional Ambiguity,0.535598,"Counterfactual explanations are attracting significant attention due to the
flourishing applications of machine learning models in consequential domains. A
counterfactual plan consists of multiple possibilities to modify a given
instance so that the model's prediction will be altered. As the predictive
model can be updated subject to the future arrival of new data, a
counterfactual plan may become ineffective or infeasible with respect to the
future values of the model parameters. In this work, we study the
counterfactual plans under model uncertainty, in which the distribution of the
model parameters is partially prescribed using only the first- and
second-moment information. First, we propose an uncertainty quantification tool
to compute the lower and upper bounds of the probability of validity for any
given counterfactual plan. We then provide corrective methods to adjust the
counterfactual plan to improve the validity measure. The numerical experiments
validate our bounds and demonstrate that our correction increases the
robustness of the counterfactual plans in different real-world datasets.",https://github.com/ngocbh/COPA,-1
On the Typicality of Musical Sequences,0.0257181,"It has been shown in a recent publication that words in human-produced
English language tend to have an information content close to the conditional
entropy. In this paper, we show that the same is true for events in
human-produced monophonic musical sequences. We also show how ""typical
sampling"" influences the distribution of information around the entropy for
single events and sequences.",None,4373
Chord-Conditioned Melody Harmonization with Controllable Harmonicity,0.383318,"Melody harmonization has long been closely associated with chorales composed
by Johann Sebastian Bach. Previous works rarely emphasised chorale generation
conditioned on chord progressions, and there has been a lack of focus on
assistive compositional tools. In this paper, we first designed a music
representation that encoded chord symbols for chord conditioning, and then
proposed DeepChoir, a melody harmonization system that can generate a four-part
chorale for a given melody conditioned on a chord progression. With
controllable harmonicity, users can control the extent of harmonicity for
generated chorales. Experimental results reveal the effectiveness of the music
representation and the controllability of DeepChoir.",https://github.com/sander-wood/deepchoir,-1
Multimodal learning with graphs,0.808843,"Artificial intelligence for graphs has achieved remarkable success in
modeling complex systems, ranging from dynamic networks in biology to
interacting particle systems in physics. However, the increasingly
heterogeneous graph datasets call for multimodal methods that can combine
different inductive biases: the set of assumptions that algorithms use to make
predictions for inputs they have not encountered during training. Learning on
multimodal datasets presents fundamental challenges because the inductive
biases can vary by data modality and graphs might not be explicitly given in
the input. To address these challenges, multimodal graph AI methods combine
different modalities while leveraging cross-modal dependencies using graphs.
Diverse datasets are combined using graphs and fed into sophisticated
multimodal architectures, specified as image-intensive, knowledge-grounded and
language-intensive models. Using this categorization, we introduce a blueprint
for multimodal graph learning, use it to study existing methods and provide
guidelines to design new models.",https://yashaektefaie.github.io/mgl,-1
DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization,0.804437,"Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve
state-of-the-art performance on many generative NLP tasks. However, such models
pose a great challenge in resource-constrained scenarios owing to their large
memory requirements and high latency. To alleviate this issue, we propose to
jointly distill and quantize the model, where knowledge is transferred from the
full-precision teacher model to the quantized and distilled low-precision
student model. Empirical analyses show that, despite the challenging nature of
generative tasks, we were able to achieve a 16.5x model footprint compression
ratio with little performance drop relative to the full-precision counterparts
on multiple summarization and QA datasets. We further pushed the limit of
compression ratio to 27.7x and presented the performance-efficiency trade-off
for generative tasks using pre-trained models. To the best of our knowledge,
this is the first work aiming to effectively distill and quantize
sequence-to-sequence pre-trained models for language generation tasks.",https://www.github.com/amazon-research/dq-bart/,-1
AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning,0.806545,"Biomedical named entity recognition (BioNER) seeks to automatically recognize
biomedical entities in natural language text, serving as a necessary foundation
for downstream text mining tasks and applications such as information
extraction and question answering. Manually labeling training data for the
BioNER task is costly, however, due to the significant domain expertise
required for accurate annotation. The resulting data scarcity causes current
BioNER approaches to be prone to overfitting, to suffer from limited
generalizability, and to address a single entity type at a time (e.g., gene or
disease). We therefore propose a novel all-in-one (AIO) scheme that uses
external data from existing annotated resources to enhance the accuracy and
stability of BioNER models. We further present AIONER, a general-purpose BioNER
tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER
on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and
compares favorably to other state-of-the-art approaches such as multi-task
learning. We further demonstrate the practical utility of AIONER in three
independent tasks to recognize entity types not previously seen in training
data, as well as the advantages of AIONER over existing methods for processing
biomedical text at a large scale (e.g., the entire PubMed data).",None,-1
Cross-Modal ASR Post-Processing System for Error Correction and Utterance Rejection,0.420659,"Although modern automatic speech recognition (ASR) systems can achieve high
performance, they may produce errors that weaken readers' experience and do
harm to downstream tasks. To improve the accuracy and reliability of ASR
hypotheses, we propose a cross-modal post-processing system for speech
recognizers, which 1) fuses acoustic features and textual features from
different modalities, 2) joints a confidence estimator and an error corrector
in multi-task learning fashion and 3) unifies error correction and utterance
rejection modules. Compared with single-modal or single-task models, our
proposed system is proved to be more effective and efficient. Experiment result
shows that our post-processing system leads to more than 10% relative reduction
of character error rate (CER) for both single-speaker and multi-speaker speech
on our industrial ASR system, with about 1.7ms latency for each token, which
ensures that extra latency introduced by post-processing is acceptable in
streaming speech recognition.",None,-1
COLD: A Benchmark for Chinese Offensive Language Detection,0.998362,"Offensive language detection is increasingly crucial for maintaining a
civilized social media platform and deploying pre-trained language models.
However, this task in Chinese is still under exploration due to the scarcity of
reliable datasets. To this end, we propose a benchmark --COLD for Chinese
offensive language analysis, including a Chinese Offensive Language Dataset
--COLDATASET and a baseline detector --COLDETECTOR which is trained on the
dataset. We show that the COLD benchmark contributes to Chinese offensive
language detection which is challenging for existing resources. We then deploy
the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained
language models. We first analyze the offensiveness of existing generative
models and show that these models inevitably expose varying degrees of
offensive issues. Furthermore, we investigate the factors that influence the
offensive generations, and we find that anti-bias contents and keywords
referring to certain groups or revealing negative attitudes trigger offensive
outputs easier.",https://github.com/thu-coai/COLDataset,-1
CD Tools -- Condensed Detachment and Structure Generating Theorem Proving (System Description),0.584615,"CD Tools is a Prolog library for experimenting with condensed detachment in
first-order ATP, which puts a recent formal view centered around proof
structures into practice. From the viewpoint of first-order ATP, condensed
detachment offers a setting that is relatively simple but with essential
features and serious applications, making it attractive as a basis for
developing and evaluating novel techniques. CD Tools includes specialized
provers based on the enumeration of proof structures. We focus here on one of
these, SGCD, which permits to blend goal- and axiom-driven proof search in
particularly flexible ways. In purely goal-driven configurations it acts
similarly to a prover of the clausal tableaux or connection method family. In
blended configurations its performance is much stronger, close to
state-of-the-art provers, while emitting relatively short proofs. Experiments
show characteristics and application possibilities of the structure generating
approach realized by that prover. For a historic problem often studied in ATP
it produced a new proof that is much shorter than any known one.",None,-1
Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning,0.708538,"We analyze the growth of dataset sizes used in machine learning for natural
language processing and computer vision, and extrapolate these using two
methods; using the historical growth rate and estimating the compute-optimal
dataset size for future predicted compute budgets. We investigate the growth in
data usage by estimating the total stock of unlabeled data available on the
internet over the coming decades. Our analysis indicates that the stock of
high-quality language data will be exhausted soon; likely before 2026. By
contrast, the stock of low-quality language data and image data will be
exhausted only much later; between 2030 and 2050 (for low-quality language) and
between 2030 and 2060 (for images). Our work suggests that the current trend of
ever-growing ML models that rely on enormous datasets might slow down if data
efficiency is not drastically improved or new sources of data become available.",https://github.com/epoch-research/data-stock,-1
Revisiting End-to-End Speech-to-Text Translation From Scratch,0.97047,"End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining
its encoder and/or decoder using source transcripts via speech recognition or
text translation tasks, without which translation performance drops
substantially. However, transcripts are not always available, and how
significant such pretraining is for E2E ST has rarely been studied in the
literature. In this paper, we revisit this question and explore the extent to
which the quality of E2E ST trained on speech-translation pairs alone can be
improved. We reexamine several techniques proven beneficial to ST previously,
and offer a set of best practices that biases a Transformer-based E2E ST system
toward training from scratch. Besides, we propose parameterized distance
penalty to facilitate the modeling of locality in the self-attention model for
speech. On four benchmarks covering 23 languages, our experiments show that,
without using any transcripts or pretraining, the proposed system reaches and
even outperforms previous studies adopting pretraining, although the gap
remains in (extremely) low-resource settings. Finally, we discuss neural
acoustic feature modeling, where a neural model is designed to extract acoustic
features from raw speech signals directly, with the goal to simplify inductive
biases and add freedom to the model in describing speech. For the first time,
we demonstrate its feasibility and show encouraging results on ST tasks.",None,21498
Differentiable Logics for Neural Network Training and Verification,0.115972,"The rising popularity of neural networks (NNs) in recent years and their
increasing prevalence in real-world applications have drawn attention to the
importance of their verification. While verification is known to be
computationally difficult theoretically, many techniques have been proposed for
solving it in practice. It has been observed in the literature that by default
neural networks rarely satisfy logical constraints that we want to verify. A
good course of action is to train the given NN to satisfy said constraint prior
to verifying them. This idea is sometimes referred to as continuous
verification, referring to the loop between training and verification. Usually
training with constraints is implemented by specifying a translation for a
given formal logic language into loss functions. These loss functions are then
used to train neural networks. Because for training purposes these functions
need to be differentiable, these translations are called differentiable logics
(DL). This raises several research questions. What kind of differentiable
logics are possible? What difference does a specific choice of DL make in the
context of continuous verification? What are the desirable criteria for a DL
viewed from the point of view of the resulting loss function? In this extended
abstract we will discuss and answer these questions.",None,-1
Tencent AI Lab - Shanghai Jiao Tong University Low-Resource Translation System for the WMT22 Translation Task,0.515415,"This paper describes Tencent AI Lab - Shanghai Jiao Tong University
(TAL-SJTU) Low-Resource Translation systems for the WMT22 shared task. We
participate in the general translation task on
English$\Leftrightarrow$Livonian. Our system is based on M2M100 with novel
techniques that adapt it to the target language pair. (1) Cross-model word
embedding alignment: inspired by cross-lingual word embedding alignment, we
successfully transfer a pre-trained word embedding to M2M100, enabling it to
support Livonian. (2) Gradual adaptation strategy: we exploit Estonian and
Latvian as auxiliary languages for many-to-many translation training and then
adapt to English-Livonian. (3) Data augmentation: to enlarge the parallel data
for English-Livonian, we construct pseudo-parallel data with Estonian and
Latvian as pivot languages. (4) Fine-tuning: to make the most of all available
data, we fine-tune the model with the validation set and online
back-translation, further boosting the performance. In model evaluation: (1) We
find that previous work underestimated the translation performance of Livonian
due to inconsistent Unicode normalization, which may cause a discrepancy of up
to 14.9 BLEU score. (2) In addition to the standard validation set, we also
employ round-trip BLEU to evaluate the models, which we find more appropriate
for this task. Finally, our unconstrained system achieves BLEU scores of 17.0
and 30.4 for English to/from Livonian.",https://github.com/zwhe99/WMT22-En-Liv,-1
PointInst3D: Segmenting 3D Instances by Points,0.691061,"The current state-of-the-art methods in 3D instance segmentation typically
involve a clustering step, despite the tendency towards heuristics, greedy
algorithms, and a lack of robustness to the changes in data statistics. In
contrast, we propose a fully-convolutional 3D point cloud instance segmentation
method that works in a per-point prediction fashion. In doing so it avoids the
challenges that clustering-based methods face: introducing dependencies among
different tasks of the model. We find the key to its success is assigning a
suitable target to each sampled point. Instead of the commonly used static or
distance-based assignment strategies, we propose to use an Optimal Transport
approach to optimally assign target masks to the sampled points according to
the dynamic matching costs. Our approach achieves promising results on both
ScanNet and S3DIS benchmarks. The proposed approach removes intertask
dependencies and thus represents a simpler and more flexible 3D instance
segmentation framework than other competing methods, while achieving improved
segmentation accuracy.",None,-1
On the Relation between Sensitivity and Accuracy in In-context Learning,0.820593,"In-context learning (ICL) suffers from oversensitivity to the prompt, making
it unreliable in real-world scenarios. We study the sensitivity of ICL with
respect to multiple perturbation types. First, we find that label bias obscures
the true sensitivity, and therefore prior work may have significantly
underestimated ICL sensitivity. Second, we observe a strong negative
correlation between ICL sensitivity and accuracy: predictions sensitive to
perturbations are less likely to be correct. Motivated by these findings, we
propose \textsc{SenSel}, a few-shot selective prediction method that abstains
from sensitive predictions. Experiments on ten classification datasets show
that \textsc{SenSel} consistently outperforms two commonly used
confidence-based and entropy-based baselines on abstention decisions.",https://github.com/yandachen/ICLSensitivity,-1
The Topological BERT: Transforming Attention into Topology for Natural Language Processing,0.775584,"In recent years, the introduction of the Transformer models sparked a
revolution in natural language processing (NLP). BERT was one of the first text
encoders using only the attention mechanism without any recurrent parts to
achieve state-of-the-art results on many NLP tasks.
  This paper introduces a text classifier using topological data analysis. We
use BERT's attention maps transformed into attention graphs as the only input
to that classifier. The model can solve tasks such as distinguishing spam from
ham messages, recognizing whether a sentence is grammatically correct, or
evaluating a movie review as negative or positive. It performs comparably to
the BERT baseline and outperforms it on some tasks.
  Additionally, we propose a new method to reduce the number of BERT's
attention heads considered by the topological classifier, which allows us to
prune the number of heads from 144 down to as few as ten with no reduction in
performance. Our work also shows that the topological model displays higher
robustness against adversarial attacks than the original BERT model, which is
maintained during the pruning process. To the best of our knowledge, this work
is the first to confront topological-based models with adversarial attacks in
the context of NLP.",None,-1
Learning from One and Only One Shot,0.0619988,"Humans can generalize from only a few examples and from little pre-training
on similar tasks. Yet, machine learning (ML) typically requires large data to
learn or pre-learn to transfer. Inspired by nativism, we directly model basic
human-innate priors in abstract visual tasks e.g., character/doodle
recognition. This yields a white-box model that learns general-appearance
similarity -- how any two images look in general -- by mimicking how humans
naturally ""distort"" an object at first sight. Using simply the nearest-neighbor
classifier on this similarity space, we achieve human-level character
recognition using only 1--10 examples per class and nothing else (no
pre-training). This differs from few-shot learning (FSL) using significant
pre-training. On standard benchmarks MNIST/EMNIST and the Omniglot challenge,
we outperform both neural-network-based and classical ML in the ""tiny-data""
regime, including FSL pre-trained on large data. Our model enables unsupervised
learning too: by learning the non-Euclidean, general-appearance similarity
space in a k-means style, we can generate human-intuitive archetypes as cluster
``centroids''.",None,-1
Studying Bias in GANs through the Lens of Race,0.978034,"In this work, we study how the performance and evaluation of generative image
models are impacted by the racial composition of their training datasets. By
examining and controlling the racial distributions in various training
datasets, we are able to observe the impacts of different training
distributions on generated image quality and the racial distributions of the
generated images. Our results show that the racial compositions of generated
images successfully preserve that of the training data. However, we observe
that truncation, a technique used to generate higher quality images during
inference, exacerbates racial imbalances in the data. Lastly, when examining
the relationship between image quality and race, we find that the highest
perceived visual quality images of a given race come from a distribution where
that race is well-represented, and that annotators consistently prefer
generated images of white people over those of Black people.",None,-1
Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition,0.389096,"Language model fusion helps smart assistants recognize words which are rare
in acoustic data but abundant in text-only corpora (typed search logs).
However, such corpora have properties that hinder downstream performance,
including being (1) too large, (2) beset with domain-mismatched content, and
(3) heavy-headed rather than heavy-tailed (excessively many duplicate search
queries such as ""weather""). We show that three simple strategies for selecting
language modeling data can dramatically improve rare-word recognition without
harming overall performance. First, to address the heavy-headedness, we
downsample the data according to a soft log function, which tunably reduces
high frequency (head) sentences. Second, to encourage rare-word exposure, we
explicitly filter for words rare in the acoustic data. Finally, we tackle
domain-mismatch via perplexity-based contrastive selection, filtering for
examples matched to the target domain. We down-select a large corpus of web
search queries by a factor of 53x and achieve better LM perplexities than
without down-selection. When shallow-fused with a state-of-the-art, production
speech engine, our LM achieves WER reductions of up to 24% relative on
rare-word sentences (without changing overall WER) compared to a baseline LM
trained on the raw corpus. These gains are further validated through favorable
side-by-side evaluations on live voice search traffic.",None,-1
3rd Place Solution for Google Universal Image Embedding,0.0423574,"This paper presents the 3rd place solution to the Google Universal Image
Embedding Competition on Kaggle. We use ViT-H/14 from OpenCLIP for the backbone
of ArcFace, and trained in 2 stage. 1st stage is done with freezed backbone,
and 2nd stage is whole model training. We achieve 0.692 mean Precision @5 on
private leaderboard. Code available at
https://github.com/YasumasaNamba/google-universal-image-embedding",https://github.com/YasumasaNamba/google-universal-image-embedding,-1
Approaching Reflex Predictions as a Classification Problem Using Extended Phonological Alignments,0.0141751,"This work describes an implementation of the ""extended alignment"" (or
""multitiers"") approach for cognate reflex prediction, submitted to ""Prediction
of Cognate Reflexes"" shared task. Similarly to List2022d, the technique
involves an automatic extension of sequence alignments with multilayered
vectors that encode informational tiers on both site-specific traits, such as
sound classes and distinctive features, as well as contextual and
suprasegmental ones, conveyed by cross-site referrals and replication. The
method allows to generalize the problem of cognate reflex prediction as a
classification problem, with models trained using a parallel corpus of cognate
sets. A model using random forests is trained and evaluated on the shared task
for reflex prediction, and the experimental results are presented and discussed
along with some differences to other implementations.",https://github.com/tresoldi/maniphono,-1
AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling,0.052561,"Neural fields such as implicit surfaces have recently enabled avatar modeling
from raw scans without explicit temporal correspondences. In this work, we
exploit autoregressive modeling to further extend this notion to capture
dynamic effects, such as soft-tissue deformations. Although autoregressive
models are naturally capable of handling dynamics, it is non-trivial to apply
them to implicit representations, as explicit state decoding is infeasible due
to prohibitive memory requirements. In this work, for the first time, we enable
autoregressive modeling of implicit avatars. To reduce the memory bottleneck
and efficiently model dynamic implicit surfaces, we introduce the notion of
articulated observer points, which relate implicit states to the explicit
surface of a parametric human body model. We demonstrate that encoding implicit
surfaces as a set of height fields defined on articulated observer points leads
to significantly better generalization compared to a latent representation. The
experiments show that our approach outperforms the state of the art, achieving
plausible dynamic deformations even for unseen motions.
https://zqbai-jeremy.github.io/autoavatar",None,-1
Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks,0.199169,"Logical approaches to representing language have developed and evaluated
computational models of quantifier words since the 19th century, but today's
NLU models still struggle to capture their semantics. We rely on Generalized
Quantifier Theory for language-independent representations of the semantics of
quantifier words, to quantify their contribution to the errors of NLU models.
We find that quantifiers are pervasive in NLU benchmarks, and their occurrence
at test time is associated with performance drops. Multilingual models also
exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse
for non-English languages. To facilitate directly-targeted probing, we present
an adversarial generalized quantifier NLI task (GQNLI) and show that
pre-trained language models have a clear lack of robustness in generalized
quantifier reasoning.",None,-1
SHREC 2022 Track on Online Detection of Heterogeneous Gestures,0.282976,"This paper presents the outcomes of a contest organized to evaluate methods
for the online recognition of heterogeneous gestures from sequences of 3D hand
poses. The task is the detection of gestures belonging to a dictionary of 16
classes characterized by different pose and motion features. The dataset
features continuous sequences of hand tracking data where the gestures are
interleaved with non-significant motions. The data have been captured using the
Hololens 2 finger tracking system in a realistic use-case of mixed reality
interaction. The evaluation is based not only on the detection performances but
also on the latency and the false positives, making it possible to understand
the feasibility of practical interaction tools based on the algorithms
proposed. The outcomes of the contest's evaluation demonstrate the necessity of
further research to reduce recognition errors, while the computational cost of
the algorithms proposed is sufficiently low.",None,-1
Decomposing Counterfactual Explanations for Consequential Decision Making,0.0200974,"The goal of algorithmic recourse is to reverse unfavorable decisions (e.g.,
from loan denial to approval) under automated decision making by suggesting
actionable feature changes (e.g., reduce the number of credit cards). To
generate low-cost recourse the majority of methods work under the assumption
that the features are independently manipulable (IMF). To address the feature
dependency issue the recourse problem is usually studied through the causal
recourse paradigm. However, it is well known that strong assumptions, as
encoded in causal models and structural equations, hinder the applicability of
these methods in complex domains where causal dependency structures are
ambiguous. In this work, we develop \texttt{DEAR} (DisEntangling Algorithmic
Recourse), a novel and practical recourse framework that bridges the gap
between the IMF and the strong causal assumptions. \texttt{DEAR} generates
recourses by disentangling the latent representation of co-varying features
from a subset of promising recourse features to capture the main practical
recourse desiderata. Our experiments on real-world data corroborate our
theoretically motivated recourse model and highlight our framework's ability to
provide reliable, low-cost recourse in the presence of feature dependencies.",None,-1
CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection,0.568435,"Video anomaly detection (VAD) -- commonly formulated as a multiple-instance
learning problem in a weakly-supervised manner due to its labor-intensive
nature -- is a challenging problem in video surveillance where the frames of
anomaly need to be localized in an untrimmed video. In this paper, we first
propose to utilize the ViT-encoded visual features from CLIP, in contrast with
the conventional C3D or I3D features in the domain, to efficiently extract
discriminative representations in the novel technique. We then model temporal
dependencies and nominate the snippets of interest by leveraging our proposed
Temporal Self-Attention (TSA). The ablation study confirms the effectiveness of
TSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA
outperforms the existing state-of-the-art (SOTA) methods by a large margin on
three commonly-used benchmark datasets in the VAD problem (UCF-Crime,
ShanghaiTech Campus, and XD-Violence). Our source code is available at
https://github.com/joos2010kj/CLIP-TSA.",https://github.com/joos2010kj/CLIP-TSA,-1
When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories,0.999999,"Despite their impressive performance on diverse tasks, large language models
(LMs) still struggle with tasks requiring rich world knowledge, implying the
limitations of relying solely on their parameters to encode a wealth of world
knowledge. This paper aims to understand LMs' strengths and limitations in
memorizing factual knowledge, by conducting large-scale knowledge probing
experiments of 10 models and 4 augmentation methods on PopQA, our new
open-domain QA dataset with 14k questions. We find that LMs struggle with less
popular factual knowledge, and that scaling fails to appreciably improve
memorization of factual knowledge in the long tail. We then show that
retrieval-augmented LMs largely outperform orders of magnitude larger LMs,
while unassisted LMs remain competitive in questions about high-popularity
entities. Based on those findings, we devise a simple, yet effective, method
for powerful and efficient retrieval-augmented LMs, which retrieves
non-parametric memories only when necessary. Experimental results show that
this significantly improves models' performance while reducing the inference
costs.",https://github.com/AlexTMallen/adaptive-retrieval,-1
Polysemanticity and Capacity in Neural Networks,0.664065,"Individual neurons in neural networks often represent a mixture of unrelated
features. This phenomenon, called polysemanticity, can make interpreting neural
networks more difficult and so we aim to understand its causes. We propose
doing so through the lens of feature \emph{capacity}, which is the fractional
dimension each feature consumes in the embedding space. We show that in a toy
model the optimal capacity allocation tends to monosemantically represent the
most important features, polysemantically represent less important features (in
proportion to their impact on the loss), and entirely ignore the least
important features. Polysemanticity is more prevalent when the inputs have
higher kurtosis or sparsity and more prevalent in some architectures than
others. Given an optimal allocation of capacity, we go on to study the geometry
of the embedding space. We find a block-semi-orthogonal structure, with
differing block sizes in different models, highlighting the impact of model
architecture on the interpretability of its neurons.",None,-1
Multilevel Hierarchical Network with Multiscale Sampling for Video Question Answering,0.655592,"Video question answering (VideoQA) is challenging given its multimodal
combination of visual understanding and natural language processing. While most
existing approaches ignore the visual appearance-motion information at
different temporal scales, it is unknown how to incorporate the multilevel
processing capacity of a deep learning model with such multiscale information.
Targeting these issues, this paper proposes a novel Multilevel Hierarchical
Network (MHN) with multiscale sampling for VideoQA. MHN comprises two modules,
namely Recurrent Multimodal Interaction (RMI) and Parallel Visual Reasoning
(PVR). With a multiscale sampling, RMI iterates the interaction of
appearance-motion information at each scale and the question embeddings to
build the multilevel question-guided visual representations. Thereon, with a
shared transformer encoder, PVR infers the visual cues at each level in
parallel to fit with answering different question types that may rely on the
visual information at relevant levels. Through extensive experiments on three
VideoQA datasets, we demonstrate improved performances than previous
state-of-the-arts and justify the effectiveness of each part of our method.",None,-1
PoserNet: Refining Relative Camera Poses Exploiting Object Detections,0.27247,"The estimation of the camera poses associated with a set of images commonly
relies on feature matches between the images. In contrast, we are the first to
address this challenge by using objectness regions to guide the pose estimation
problem rather than explicit semantic object detections. We propose Pose
Refiner Network (PoserNet) a light-weight Graph Neural Network to refine the
approximate pair-wise relative camera poses. PoserNet exploits associations
between the objectness regions - concisely expressed as bounding boxes - across
multiple views to globally refine sparsely connected view graphs. We evaluate
on the 7-Scenes dataset across varied sizes of graphs and show how this process
can be beneficial to optimisation-based Motion Averaging algorithms improving
the median error on the rotation by 62 degrees with respect to the initial
estimates obtained based on bounding boxes. Code and data are available at
https://github.com/IIT-PAVIS/PoserNet.",https://github.com/IIT-PAVIS/PoserNet,-1
ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual Simultaneous Localization and Mapping,0.169812,"In this paper, a novel solution is introduced for visual Simultaneous
Localization and Mapping (vSLAM) that is built up of Deep Learning components.
The proposed architecture is a highly modular framework in which each component
offers state of the art results in their respective fields of vision-based deep
learning solutions. The paper shows that with the synergic integration of these
individual building blocks, a functioning and efficient all-through deep neural
(ATDN) vSLAM system can be created. The Embedding Distance Loss function is
introduced and using it the ATDN architecture is trained. The resulting system
managed to achieve 4.4% translation and 0.0176 deg/m rotational error on a
subset of the KITTI dataset. The proposed architecture can be used for
efficient and low-latency autonomous driving (AD) aiding database creation as
well as a basis for autonomous vehicle (AV) control.",None,-1
REGTR: End-to-end Point Cloud Correspondences with Transformers,0.987995,"Despite recent success in incorporating learning into point cloud
registration, many works focus on learning feature descriptors and continue to
rely on nearest-neighbor feature matching and outlier filtering through RANSAC
to obtain the final set of correspondences for pose estimation. In this work,
we conjecture that attention mechanisms can replace the role of explicit
feature matching and RANSAC, and thus propose an end-to-end framework to
directly predict the final set of correspondences. We use a network
architecture consisting primarily of transformer layers containing self and
cross attentions, and train it to predict the probability each point lies in
the overlapping region and its corresponding position in the other point cloud.
The required rigid transformation can then be estimated directly from the
predicted correspondences without further post-processing. Despite its
simplicity, our approach achieves state-of-the-art performance on 3DMatch and
ModelNet benchmarks. Our source code can be found at
https://github.com/yewzijian/RegTR .",https://github.com/yewzijian/RegTR,-1
From Indoor To Outdoor: Unsupervised Domain Adaptive Gait Recognition,0.540927,"Gait recognition is an important AI task, which has been progressed rapidly
with the development of deep learning. However, existing learning based gait
recognition methods mainly focus on the single domain, especially the
constrained laboratory environment. In this paper, we study a new problem of
unsupervised domain adaptive gait recognition (UDA-GR), that learns a gait
identifier with supervised labels from the indoor scenes (source domain), and
is applied to the outdoor wild scenes (target domain). For this purpose, we
develop an uncertainty estimation and regularization based UDA-GR method.
Specifically, we investigate the characteristic of gaits in the indoor and
outdoor scenes, for estimating the gait sample uncertainty, which is used in
the unsupervised fine-tuning on the target domain to alleviate the noises of
the pseudo labels. We also establish a new benchmark for the proposed problem,
experimental results on which show the effectiveness of the proposed method. We
will release the benchmark and source code in this work to the public.",None,-1
"Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",0.536982,"While the problem of hallucinations in neural machine translation has long
been recognized, so far the progress on its alleviation is very little. Indeed,
recently it turned out that without artificially encouraging models to
hallucinate, previously existing methods fall short and even the standard
sequence log-probability is more informative. It means that characteristics
internal to the model can give much more information than we expect, and before
using external models and measures, we first need to ask: how far can we go if
we use nothing but the translation model itself ? We propose to use a method
that evaluates the percentage of the source contribution to a generated
translation. Intuitively, hallucinations are translations ""detached"" from the
source, hence they can be identified by low source contribution. This method
improves detection accuracy for the most severe hallucinations by a factor of 2
and is able to alleviate hallucinations at test time on par with the previous
best approach that relies on external models. Next, if we move away from
internal model characteristics and allow external tools, we show that using
sentence similarity from cross-lingual embeddings further improves these
results.",https://github.com/facebookresearch/stopes,2821
Robust Watermarking for Video Forgery Detection with Improved Imperceptibility and Robustness,0.113881,"Videos are prone to tampering attacks that alter the meaning and deceive the
audience. Previous video forgery detection schemes find tiny clues to locate
the tampered areas. However, attackers can successfully evade supervision by
destroying such clues using video compression or blurring. This paper proposes
a video watermarking network for tampering localization. We jointly train a
3D-UNet-based watermark embedding network and a decoder that predicts the
tampering mask. The perturbation made by watermark embedding is close to
imperceptible. Considering that there is no off-the-shelf differentiable video
codec simulator, we propose to mimic video compression by ensembling simulation
results of other typical attacks, e.g., JPEG compression and blurring, as an
approximation. Experimental results demonstrate that our method generates
watermarked videos with good imperceptibility and robustly and accurately
locates tampered areas within the attacked version.",None,-1
"S5CL: Unifying Fully-Supervised, Self-Supervised, and Semi-Supervised Learning Through Hierarchical Contrastive Learning",0.150944,"In computational pathology, we often face a scarcity of annotations and a
large amount of unlabeled data. One method for dealing with this is
semi-supervised learning which is commonly split into a self-supervised pretext
task and a subsequent model fine-tuning. Here, we compress this two-stage
training into one by introducing S5CL, a unified framework for
fully-supervised, self-supervised, and semi-supervised learning. With three
contrastive losses defined for labeled, unlabeled, and pseudo-labeled images,
S5CL can learn feature representations that reflect the hierarchy of distance
relationships: similar images and augmentations are embedded the closest,
followed by different looking images of the same class, while images from
separate classes have the largest distance. Moreover, S5CL allows us to
flexibly combine these losses to adapt to different scenarios. Evaluations of
our framework on two public histopathological datasets show strong improvements
in the case of sparse labels: for a H&E-stained colorectal cancer dataset, the
accuracy increases by up to 9% compared to supervised cross-entropy loss; for a
highly imbalanced dataset of single white blood cells from leukemia patient
blood smears, the F1-score increases by up to 6%.",https://github.com/manuel-tran/s5cl,-1
Multi-task Learning for Cross-Lingual Sentiment Analysis,0.764061,"This paper presents a cross-lingual sentiment analysis of news articles using
zero-shot and few-shot learning. The study aims to classify the Croatian news
articles with positive, negative, and neutral sentiments using the Slovene
dataset. The system is based on a trilingual BERT-based model trained in three
languages: English, Slovene, Croatian. The paper analyses different setups
using datasets in two languages and proposes a simple multi-task model to
perform sentiment classification. The evaluation is performed using the
few-shot and zero-shot scenarios in single-task and multi-task experiments for
Croatian and Slovene.",https://github.com/cleopatra-itn/SentimentAnalyserSLHRNews,1490
Online Learning of Reusable Abstract Models for Object Goal Navigation,0.551848,"In this paper, we present a novel approach to incrementally learn an Abstract
Model of an unknown environment, and show how an agent can reuse the learned
model for tackling the Object Goal Navigation task. The Abstract Model is a
finite state machine in which each state is an abstraction of a state of the
environment, as perceived by the agent in a certain position and orientation.
The perceptions are high-dimensional sensory data (e.g., RGB-D images), and the
abstraction is reached by exploiting image segmentation and the Taskonomy model
bank. The learning of the Abstract Model is accomplished by executing actions,
observing the reached state, and updating the Abstract Model with the acquired
information. The learned models are memorized by the agent, and they are reused
whenever it recognizes to be in an environment that corresponds to the stored
model. We investigate the effectiveness of the proposed approach for the Object
Goal Navigation task, relying on public benchmarks. Our results show that the
reuse of learned Abstract Models can boost performance on Object Goal
Navigation.",None,-1
PHEE: A Dataset for Pharmacovigilance Event Extraction from Text,0.951396,"The primary goal of drug safety researchers and regulators is to promptly
identify adverse drug reactions. Doing so may in turn prevent or reduce the
harm to patients and ultimately improve public health. Evaluating and
monitoring drug safety (i.e., pharmacovigilance) involves analyzing an ever
growing collection of spontaneous reports from health professionals,
physicians, and pharmacists, and information voluntarily submitted by patients.
In this scenario, facilitating analysis of such reports via automation has the
potential to rapidly identify safety signals. Unfortunately, public resources
for developing natural language models for this task are scant. We present
PHEE, a novel dataset for pharmacovigilance comprising over 5000 annotated
events from medical case reports and biomedical literature, making it the
largest such public dataset to date. We describe the hierarchical event schema
designed to provide coarse and fine-grained information about patients'
demographics, treatments and (side) effects. Along with the discussion of the
dataset, we present a thorough experimental evaluation of current
state-of-the-art approaches for biomedical event extraction, point out their
limitations, and highlight open challenges to foster future research in this
area.",https://github.com/ZhaoyueSun/PHEE,-1
CZU-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and 10 wearable inertial sensors,0.400692,"Human action recognition has been widely used in many fields of life, and
many human action datasets have been published at the same time. However, most
of the multi-modal databases have some shortcomings in the layout and number of
sensors, which cannot fully represent the action features. Regarding the
problems, this paper proposes a freely available dataset, named CZU-MHAD
(Changzhou University: a comprehensive multi-modal human action dataset). It
consists of 22 actions and three modals temporal synchronized data. These
modals include depth videos and skeleton positions from a kinect v2 camera, and
inertial signals from 10 wearable sensors. Compared with single modal sensors,
multi-modal sensors can collect different modal data, so the use of multi-modal
sensors can describe actions more accurately. Moreover, CZU-MHAD obtains the
3-axis acceleration and 3-axis angular velocity of 10 main motion joints by
binding inertial sensors to them, and these data were captured at the same
time. Experimental results are provided to show that this dataset can be used
to study structural relationships between different parts of the human body
when performing actions and fusion approaches that involve multi-modal sensor
data.",https://github.com/yujmo/czu mhad/,-1
Generalizing to the Future: Mitigating Entity Bias in Fake News Detection,0.722496,"The wide dissemination of fake news is increasingly threatening both
individuals and society. Fake news detection aims to train a model on the past
news and detect fake news of the future. Though great efforts have been made,
existing fake news detection methods overlooked the unintended entity bias in
the real-world data, which seriously influences models' generalization ability
to future data. For example, 97\% of news pieces in 2010-2017 containing the
entity `Donald Trump' are real in our data, but the percentage falls down to
merely 33\% in 2018. This would lead the model trained on the former set to
hardly generalize to the latter, as it tends to predict news pieces about
`Donald Trump' as real for lower training loss. In this paper, we propose an
entity debiasing framework (\textbf{ENDEF}) which generalizes fake news
detection models to the future data by mitigating entity bias from a
cause-effect perspective. Based on the causal graph among entities, news
contents, and news veracity, we separately model the contribution of each cause
(entities and contents) during training. In the inference stage, we remove the
direct effect of the entities to mitigate entity bias. Extensive offline
experiments on the English and Chinese datasets demonstrate that the proposed
framework can largely improve the performance of base fake news detectors, and
online tests verify its superiority in practice. To the best of our knowledge,
this is the first work to explicitly improve the generalization ability of fake
news detection models to the future data. The code has been released at
https://github.com/ICTMCG/ENDEF-SIGIR2022.",https://github.com/ICTMCG/ENDEF-SIGIR2022,-1
Not always about you: Prioritizing community needs when developing endangered language technology,0.757213,"Languages are classified as low-resource when they lack the quantity of data
necessary for training statistical and machine learning tools and models.
Causes of resource scarcity vary but can include poor access to technology for
developing these resources, a relatively small population of speakers, or a
lack of urgency for collecting such resources in bilingual populations where
the second language is high-resource. As a result, the languages described as
low-resource in the literature are as different as Finnish on the one hand,
with millions of speakers using it in every imaginable domain, and Seneca, with
only a small-handful of fluent speakers using the language primarily in a
restricted domain. While issues stemming from the lack of resources necessary
to train models unite this disparate group of languages, many other issues cut
across the divide between widely-spoken low resource languages and endangered
languages. In this position paper, we discuss the unique technological,
cultural, practical, and ethical challenges that researchers and indigenous
speech community members face when working together to develop language
technology to support endangered language documentation and revitalization. We
report the perspectives of language teachers, Master Speakers and elders from
indigenous communities, as well as the point of view of academics. We describe
an ongoing fruitful collaboration and make recommendations for future
partnerships between academic researchers and language community stakeholders.",None,-1
ExtrudeNet: Unsupervised Inverse Sketch-and-Extrude for Shape Parsing,0.570739,"Sketch-and-extrude is a common and intuitive modeling process in computer
aided design. This paper studies the problem of learning the shape given in the
form of point clouds by inverse sketch-and-extrude. We present ExtrudeNet, an
unsupervised end-to-end network for discovering sketch and extrude from point
clouds. Behind ExtrudeNet are two new technical components: 1) an effective
representation for sketch and extrude, which can model extrusion with freeform
sketches and conventional cylinder and box primitives as well; and 2) a
numerical method for computing the signed distance field which is used in the
network learning. This is the first attempt that uses machine learning to
reverse engineer the sketch-and-extrude modeling process of a shape in an
unsupervised fashion. ExtrudeNet not only outputs a compact, editable and
interpretable representation of the shape that can be seamlessly integrated
into modern CAD software, but also aligns with the standard CAD modeling
process facilitating various editing applications, which distinguishes our work
from existing shape parsing research. Code is released at
https://github.com/kimren227/ExtrudeNet.",https://github.com/kimren227/ExtrudeNet,-1
Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations,0.656546,"Although large language models can be prompted for both zero- and few-shot
learning, performance drops significantly when no demonstrations are available.
In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap
by constructing pseudo-demonstrations for a given test input using a raw text
corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the
nearest neighbors to the test input from the corpus and pairing them with
random task labels, and (2) applying a set of techniques to reduce the amount
of direct copying the model does from the resulting demonstrations. Evaluation
on nine classification datasets shows that Z-ICL outperforms previous zero-shot
methods by a significant margin, and is on par with in-context learning with
labeled training data in the few-shot setting. Overall, Z-ICL provides a
significantly higher estimate of the zero-shot performance levels of a model,
and supports future efforts to develop better pseudo-demonstrations that
further improve zero-shot results.",https://github.com/alrope123/z-icl,-1
Evaluating Step-by-Step Reasoning through Symbolic Verification,0.477801,"Pre-trained language models (LMs) have shown remarkable reasoning performance
using explanations or chain-of-thoughts (CoT)) for in-context learning. On the
other hand, these reasoning tasks are usually presumed to be more approachable
for symbolic programming. To understand the mechanism of reasoning of LMs, we
curate synthetic datasets containing equivalent (natural, symbolic) data pairs,
where symbolic examples contain first-order logic rules and predicates from
non-parametric knowledge bases (KBs), supporting automated verification of
intermediate reasoning results. Then we revisit neuro-symbolic approaches and
propose to learn from demonstrations containing logic rules and corresponding
examples to iteratively reason over KBs, recovering Prolog's backward chaining
algorithm and supporting automated verification of LMs' outputs. Comprehensive
experiments are included to systematically compare LMLP with CoT in deductive
reasoning settings, showing that LMLP enjoys more than $25\%$ higher accuracy
than CoT on length generalization benchmarks even with smaller model sizes.",None,-1
End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking,0.197419,"Machine learning systems perform well on pattern matching tasks, but their
ability to perform algorithmic or logical reasoning is not well understood. One
important reasoning capability is algorithmic extrapolation, in which models
trained only on small/simple reasoning problems can synthesize complex
strategies for large/complex problems at test time. Algorithmic extrapolation
can be achieved through recurrent systems, which can be iterated many times to
solve difficult reasoning problems. We observe that this approach fails to
scale to highly complex problems because behavior degenerates when many
iterations are applied -- an issue we refer to as ""overthinking."" We propose a
recall architecture that keeps an explicit copy of the problem instance in
memory so that it cannot be forgotten. We also employ a progressive training
routine that prevents the model from learning behaviors that are specific to
iteration number and instead pushes it to learn behaviors that can be repeated
indefinitely. These innovations prevent the overthinking problem, and enable
recurrent systems to solve extremely hard extrapolation tasks.",https://github.com/aks2203/deep-thinking,-1
Deceptive Planning for Resource Allocation,0.0825479,"We consider a team of autonomous agents that navigate in an adversarial
environment and aim to achieve a task by allocating their resources over a set
of target locations. An adversary in the environment observes the autonomous
team's behavior to infer their objective and responds against the team. In this
setting, we propose strategies for controlling the density of the autonomous
team so that they can deceive the adversary regarding their objective while
achieving the desired final resource allocation. We first develop a prediction
algorithm based on the principle of maximum entropy to express the team's
behavior expected by the adversary. Then, by measuring the deceptiveness via
Kullback-Leibler divergence, we devise convex optimization-based planning
algorithms that deceive the adversary by either exaggerating the behavior
towards a decoy allocation strategy or creating ambiguity regarding the final
allocation strategy. A user study with $320$ participants demonstrates that the
proposed algorithms are effective for deception and reveal the inherent biases
of participants towards proximate goals.",https://github.com/vivianchen98/deception_user_study_data,-1
skrl: Modular and Flexible Library for Reinforcement Learning,0.206473,"skrl is an open-source modular library for reinforcement learning written in
Python and designed with a focus on readability, simplicity, and transparency
of algorithm implementations. In addition to supporting environments that use
the traditional interfaces from OpenAI Gym and DeepMind, it provides the
facility to load, configure, and operate NVIDIA Isaac Gym and NVIDIA Omniverse
Isaac Gym environments. Furthermore, it enables the simultaneous training of
several agents with customizable scopes (subsets of environments among all
available ones), which may or may not share resources, in the same run. The
library's documentation can be found at https://skrl.readthedocs.io and its
source code is available on GitHub at https://github.com/Toni-SM/skrl.",https://github.com/Toni-SM/skrl,-1
SimPer: Simple Self-Supervised Learning of Periodic Targets,0.64874,"From human physiology to environmental evolution, important processes in
nature often exhibit meaningful and strong periodic or quasi-periodic changes.
Due to their inherent label scarcity, learning useful representations for
periodic tasks with limited or no supervision is of great benefit. Yet,
existing self-supervised learning (SSL) methods overlook the intrinsic
periodicity in data, and fail to learn representations that capture periodic or
frequency attributes. In this paper, we present SimPer, a simple contrastive
SSL regime for learning periodic information in data. To exploit the periodic
inductive bias, SimPer introduces customized augmentations, feature similarity
measures, and a generalized contrastive loss for learning efficient and robust
periodic representations. Extensive experiments on common real-world tasks in
human behavior analysis, environmental sensing, and healthcare domains verify
the superior performance of SimPer compared to state-of-the-art SSL methods,
highlighting its intriguing properties including better data efficiency,
robustness to spurious correlations, and generalization to distribution shifts.
Code and data are available at: https://github.com/YyzHarry/SimPer.",https://github.com/YyzHarry/SimPer,-1
Reasoning with fuzzy and uncertain evidence using epistemic random fuzzy sets: general framework and practical models,0.550851,"We introduce a general theory of epistemic random fuzzy sets for reasoning
with fuzzy or crisp evidence. This framework generalizes both the
Dempster-Shafer theory of belief functions, and possibility theory. Independent
epistemic random fuzzy sets are combined by the generalized
product-intersection rule, which extends both Dempster's rule for combining
belief functions, and the product conjunctive combination of possibility
distributions. We introduce Gaussian random fuzzy numbers and their
multi-dimensional extensions, Gaussian random fuzzy vectors, as practical
models for quantifying uncertainty about scalar or vector quantities.
Closed-form expressions for the combination, projection and vacuous extension
of Gaussian random fuzzy numbers and vectors are derived.",None,-1
Enhancing Task Bot Engagement with Synthesized Open-Domain Dialog,0.603242,"Many efforts have been made to construct dialog systems for different types
of conversations, such as task-oriented dialog (TOD) and open-domain dialog
(ODD). To better mimic human-level conversations that usually fuse various
dialog modes, it is essential to build a system that can effectively handle
both TOD and ODD and access different knowledge sources. To address the lack of
available data for the fused task, we propose a framework for automatically
generating dialogues that combine knowledge-grounded ODDs and TODs in various
settings. Additionally, we introduce a unified model PivotBot that is capable
of appropriately adopting TOD and ODD modes and accessing different knowledge
sources in order to effectively tackle the fused task. Evaluation results
demonstrate the superior ability of the proposed model to switch seamlessly
between TOD and ODD tasks.",None,-1
Towards Efficient Dialogue Pre-training with Transferable and Interpretable Latent Structure,0.0640143,"With the availability of massive general-domain dialogue data, pre-trained
dialogue generation appears to be super appealing to transfer knowledge from
the general domain to downstream applications. In most existing work, such
transferable ability is mainly obtained by fitting a large model with hundreds
of millions of parameters on massive data in an exhaustive way, leading to
inefficient running and poor interpretability. This paper proposes a novel
dialogue generation model with a latent structure that is easily transferable
from the general domain to downstream tasks in a lightweight and transparent
way. Experiments on two benchmarks validate the effectiveness of the proposed
model. Thanks to the transferable latent structure, our model is able to yield
better dialogue responses than four strong baselines in terms of both automatic
and human evaluations, and our model with about 22% parameters particularly
delivers a 5x speedup in running time compared with the strongest baseline.
Moreover, the proposed model is explainable by interpreting the discrete latent
variables.",https://github.com/Maluuba/nlg-eval,-1
Who Wrote this? How Smart Replies Impact Language and Agency in the Workplace,0.323676,"AI-mediated communication is designed to help us do our work more quickly and
efficiently. But does it come at a cost? This study uses smart replies (SRs) to
show how AI influences humans without any intent on the part of the developer -
the very use of AI is sufficient. I propose a loss of agency theory as a viable
approach for studying the impact of AI on human agency. This theory focusses on
the transfer of agency that is forced by circumstances (such as time pressure),
human weaknesses (such as complacency), and conceptual priming. Mixed methods
involving a crowdsourced experiment test that theory. The quantitative results
reveal that machine agency affects the content we author and the behavior we
generate. But it is a non-zero-sum game. The transfers between human and
machine agency are fluid; they complement, replace, and reinforce each other at
the same time.",None,-1
Feature Selection Enhancement and Feature Space Visualization for Speech-Based Emotion Recognition,0.368304,"Robust speech emotion recognition relies on the quality of the speech
features. We present speech features enhancement strategy that improves speech
emotion recognition. We used the INTERSPEECH 2010 challenge feature-set. We
identified subsets from the features set and applied Principle Component
Analysis to the subsets. Finally, the features are fused horizontally. The
resulting feature set is analyzed using t-distributed neighbour embeddings
(t-SNE) before the application of features for emotion recognition. The method
is compared with the state-of-the-art methods used in the literature. The
empirical evidence is drawn using two well-known datasets: Emotional Speech
Dataset (EMO-DB) and Ryerson Audio-Visual Database of Emotional Speech and Song
(RAVDESS) for two languages, German and English, respectively. Our method
achieved an average recognition gain of 11.5\% for six out of seven emotions
for the EMO-DB dataset, and 13.8\% for seven out of eight emotions for the
RAVDESS dataset as compared to the baseline study.",None,-1
Sar Ship Detection based on Swin Transformer and Feature Enhancement Feature Pyramid Network,0.784527,"With the booming of Convolutional Neural Networks (CNNs), CNNs such as VGG-16
and ResNet-50 widely serve as backbone in SAR ship detection. However, CNN
based backbone is hard to model long-range dependencies, and causes the lack of
enough high-quality semantic information in feature maps of shallow layers,
which leads to poor detection performance in complicated background and
small-sized ships cases. To address these problems, we propose a SAR ship
detection method based on Swin Transformer and Feature Enhancement Feature
Pyramid Network (FEFPN). Swin Transformer serves as backbone to model
long-range dependencies and generates hierarchical features maps. FEFPN is
proposed to further improve the quality of feature maps by gradually enhancing
the semantic information of feature maps at all levels, especially feature maps
in shallow layers. Experiments conducted on SAR ship detection dataset (SSDD)
reveal the advantage of our proposed methods.",None,-1
Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning,0.102677,"Despite recent advances in natural language understanding and generation, and
decades of research on the development of conversational bots, building
automated agents that can carry on rich open-ended conversations with humans
""in the wild"" remains a formidable challenge. In this work we develop a
real-time, open-ended dialogue system that uses reinforcement learning (RL) to
power a bot's conversational skill at scale. Our work pairs the succinct
embedding of the conversation state generated using SOTA (supervised) language
models with RL techniques that are particularly suited to a dynamic action
space that changes as the conversation progresses. Trained using crowd-sourced
data, our novel system is able to substantially exceeds the (strong) baseline
supervised model with respect to several metrics of interest in a live
experiment with real users of the Google Assistant.",https://github.com/google-research/bert,-1
An Overview of Structural Coverage Metrics for Testing Neural Networks,0.402777,"Deep neural network (DNN) models, including those used in safety-critical
domains, need to be thoroughly tested to ensure that they can reliably perform
well in different scenarios. In this article, we provide an overview of
structural coverage metrics for testing DNN models, including neuron coverage
(NC), k-multisection neuron coverage (kMNC), top-k neuron coverage (TKNC),
neuron boundary coverage (NBC), strong neuron activation coverage (SNAC) and
modified condition/decision coverage (MC/DC). We evaluate the metrics on
realistic DNN models used for perception tasks (including LeNet-1, LeNet-4,
LeNet-5, and ResNet20) as well as on networks used in autonomy (TaxiNet). We
also provide a tool, DNNCov, which can measure the testing coverage for all
these metrics. DNNCov outputs an informative coverage report to enable
researchers and practitioners to assess the adequacy of DNN testing, compare
different coverage measures, and to more conveniently inspect the model's
internals during testing.",https://github.com/DNNCov/DNNCov,-1
Jam or Cream First? Modeling Ambiguity in Neural Machine Translation with SCONES,0.459886,"The softmax layer in neural machine translation is designed to model the
distribution over mutually exclusive tokens. Machine translation, however, is
intrinsically uncertain: the same source sentence can have multiple
semantically equivalent translations. Therefore, we propose to replace the
softmax activation with a multi-label classification layer that can model
ambiguity more effectively. We call our loss function Single-label Contrastive
Objective for Non-Exclusive Sequences (SCONES). We show that the multi-label
output layer can still be trained on single reference training data using the
SCONES loss function. SCONES yields consistent BLEU score gains across six
translation directions, particularly for medium-resource language pairs and
small beam sizes. By using smaller beam sizes we can speed up inference by a
factor of 3.9x and still match or improve the BLEU score obtained using
softmax. Furthermore, we demonstrate that SCONES can be used to train NMT
models that assign the highest probability to adequate translations, thus
mitigating the ""beam search curse"". Additional experiments on synthetic
language pairs with varying levels of uncertainty suggest that the improvements
from SCONES can be attributed to better handling of ambiguity.",None,-1
CRUSH: Contextually Regularized and User anchored Self-supervised Hate speech Detection,0.480087,"The last decade has witnessed a surge in the interaction of people through
social networking platforms. While there are several positive aspects of these
social platforms, the proliferation has led them to become the breeding ground
for cyber-bullying and hate speech. Recent advances in NLP have often been used
to mitigate the spread of such hateful content. Since the task of hate speech
detection is usually applicable in the context of social networks, we introduce
CRUSH, a framework for hate speech detection using user-anchored
self-supervision and contextual regularization. Our proposed approach secures ~
1-12% improvement in test set metrics over best performing previous approaches
on two types of tasks and multiple popular english social media datasets.",https://github.com/parag1604/CRUSH,-1
TALM: Tool Augmented Language Models,0.99359,"Transformer based language models (LMs) demonstrate increasing performance
with scale across a wide variety of tasks. Scale alone however cannot enable
models to solve tasks that require access to ephemeral, changing, or private
data that was unavailable at training time. Many useful tasks may also benefit
from LMs being able to access APIs that read or modify state. In this work, we
present Tool Augmented Language Models (TALM), combining a text-only approach
to augment language models with non-differentiable tools, and an iterative
""self-play"" technique to bootstrap performance starting from few tool
demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA
task and a reasoning oriented math task with simple tools. At a given model
scale, TALM significantly outperforms non-augmented LMs. We further demonstrate
that TALM successfully performs out-of-distribution inferences on both QA and
math tasks, where non-augmented LMs fail. Our results suggest that Tool
Augmented Language Models are a promising direction to enrich LMs'
capabilities, with less dependence on scale.",None,-1
Learning Invariant Rules from Data for Interpretable Anomaly Detection,0.0795614,"In the research area of anomaly detection, novel and promising methods are
frequently developed. However, most existing studies exclusively focus on the
detection task only and ignore the interpretability of the underlying models as
well as their detection results. Nevertheless, anomaly interpretation, which
aims to provide explanation of why specific data instances are identified as
anomalies, is an equally important task in many real-world applications. In
this work, we propose a novel framework which synergizes several machine
learning and data mining techniques to automatically learn invariant rules that
are consistently satisfied in a given dataset. The learned invariant rules can
provide explicit explanation of anomaly detection results in the inference
phase and thus are extremely useful for subsequent decision-making regarding
reported anomalies. Furthermore, our empirical evaluation shows that the
proposed method can also achieve comparable or even better performance in terms
of AUC and partial AUC on public benchmark datasets across various application
domains compared with start-of-the-art anomaly detection models.",https://github.com/NSIBF/InvariantRuleAD,-1
Unifying and Boosting Gradient-Based Training-Free Neural Architecture Search,0.759727,"Neural architecture search (NAS) has gained immense popularity owing to its
ability to automate neural architecture design. A number of training-free
metrics are recently proposed to realize NAS without training, hence making NAS
more scalable. Despite their competitive empirical performances, a unified
theoretical understanding of these training-free metrics is lacking. As a
consequence, (a) the relationships among these metrics are unclear, (b) there
is no theoretical interpretation for their empirical performances, and (c)
there may exist untapped potential in existing training-free NAS, which
probably can be unveiled through a unified theoretical understanding. To this
end, this paper presents a unified theoretical analysis of gradient-based
training-free NAS, which allows us to (a) theoretically study their
relationships, (b) theoretically guarantee their generalization performances,
and (c) exploit our unified theoretical understanding to develop a novel
framework named hybrid NAS (HNAS) which consistently boosts training-free NAS
in a principled way. Remarkably, HNAS can enjoy the advantages of both
training-free (i.e., the superior search efficiency) and training-based (i.e.,
the remarkable search effectiveness) NAS, which we have demonstrated through
extensive experiments.",https://github.com/fmfn/BayesianOptimization,-1
Data-Efficient Backdoor Attacks,0.829504,"Recent studies have proven that deep neural networks are vulnerable to
backdoor attacks. Specifically, by mixing a small number of poisoned samples
into the training set, the behavior of the trained model can be maliciously
controlled. Existing attack methods construct such adversaries by randomly
selecting some clean data from the benign set and then embedding a trigger into
them. However, this selection strategy ignores the fact that each poisoned
sample contributes inequally to the backdoor injection, which reduces the
efficiency of poisoning. In this paper, we formulate improving the poisoned
data efficiency by the selection as an optimization problem and propose a
Filtering-and-Updating Strategy (FUS) to solve it. The experimental results on
CIFAR-10 and ImageNet-10 indicate that the proposed method is effective: the
same attack success rate can be achieved with only 47% to 75% of the poisoned
sample volume compared to the random selection strategy. More importantly, the
adversaries selected according to one setting can generalize well to other
settings, exhibiting strong transferability. The prototype code of our method
is now available at https://github.com/xpf/Data-Efficient-Backdoor-Attacks.",https://github.com/xpf/Data-Efﬁcient-Backdoor-Attacks,175520
The Causal News Corpus: Annotating Causal Relations in Event Sentences from News,0.875968,"Despite the importance of understanding causality, corpora addressing causal
relations are limited. There is a discrepancy between existing annotation
guidelines of event causality and conventional causality corpora that focus
more on linguistics. Many guidelines restrict themselves to include only
explicit relations or clause-based arguments. Therefore, we propose an
annotation schema for event causality that addresses these concerns. We
annotated 3,559 event sentences from protest event news with labels on whether
it contains causal relations or not. Our corpus is known as the Causal News
Corpus (CNC). A neural network built upon a state-of-the-art pre-trained
language model performed well with 81.20% F1 score on test set, and 83.46% in
5-folds cross-validation. CNC is transferable across two external corpora:
CausalTimeBank (CTB) and Penn Discourse Treebank (PDTB). Leveraging each of
these external datasets for training, we achieved up to approximately 64% F1 on
the CNC test set without additional fine-tuning. CNC also served as an
effective training and pre-training dataset for the two external corpora.
Lastly, we demonstrate the difficulty of our task to the layman in a
crowd-sourced annotation exercise. Our annotated corpus is publicly available,
providing a valuable resource for causal text mining researchers.",https://github.com/tanfiona/,-1
Towards Reasoning-Aware Explainable VQA,0.0956489,"The domain of joint vision-language understanding, especially in the context
of reasoning in Visual Question Answering (VQA) models, has garnered
significant attention in the recent past. While most of the existing VQA models
focus on improving the accuracy of VQA, the way models arrive at an answer is
oftentimes a black box. As a step towards making the VQA task more explainable
and interpretable, our method is built upon the SOTA VQA framework by
augmenting it with an end-to-end explanation generation module. In this paper,
we investigate two network architectures, including Long Short-Term Memory
(LSTM) and Transformer decoder, as the explanation generator. Our method
generates human-readable textual explanations while maintaining SOTA VQA
accuracy on the GQA-REX (77.49%) and VQA-E (71.48%) datasets. Approximately
65.16% of the generated explanations are approved by humans as valid. Roughly
60.5% of the generated explanations are valid and lead to the correct answers.",None,-1
ADVISE: ADaptive Feature Relevance and VISual Explanations for Convolutional Neural Networks,0.0565552,"To equip Convolutional Neural Networks (CNNs) with explainability, it is
essential to interpret how opaque models take specific decisions, understand
what causes the errors, improve the architecture design, and identify unethical
biases in the classifiers. This paper introduces ADVISE, a new explainability
method that quantifies and leverages the relevance of each unit of the feature
map to provide better visual explanations. To this end, we propose using
adaptive bandwidth kernel density estimation to assign a relevance score to
each unit of the feature map with respect to the predicted class. We also
propose an evaluation protocol to quantitatively assess the visual
explainability of CNN models. We extensively evaluate our idea in the image
classification task using AlexNet, VGG16, ResNet50, and Xception pretrained on
ImageNet. We compare ADVISE with the state-of-the-art visual explainable
methods and show that the proposed method outperforms competing approaches in
quantifying feature-relevance and visual explainability while maintaining
competitive time complexity. Our experiments further show that ADVISE fulfils
the sensitivity and implementation independence axioms while passing the sanity
checks. The implementation is accessible for reproducibility purposes on
https://github.com/dehshibi/ADVISE.",https://github.com/dehshibi/ADVISE,-1
Apport des ontologies pour le calcul de la similarité sémantique au sein d'un système de recommandation,0.635075,"Measurement of the semantic relatedness or likeness between terms, words, or
text data plays an important role in different applications dealing with
textual data such as knowledge acquisition, recommender system, and natural
language processing. Over the past few years, many ontologies have been
developed and used as a form of structured representation of knowledge bases
for information systems. The calculation of semantic similarity from ontology
has developed and depending on the context is complemented by other similarity
calculation methods. In this paper, we propose and carry on an approach for the
calculation of ontology-based semantic similarity using in the context of a
recommender system.",None,-1
DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling,0.0732905,"Retrieval-based conversational systems learn to rank response candidates for
a given dialogue context by computing the similarity between their vector
representations. However, training on a single textual form of the multi-turn
context limits the ability of a model to learn representations that generalize
to natural perturbations seen during inference. In this paper we propose a
framework that incorporates augmented versions of a dialogue context into the
learning objective. We utilize contrastive learning as an auxiliary objective
to learn robust dialogue context representations that are invariant to
perturbations injected through the augmentation method. We experiment with four
benchmark dialogue datasets and demonstrate that our framework combines well
with existing augmentation methods and can significantly improve over baseline
BERT-based ranking architectures. Furthermore, we propose a novel data
augmentation method, ConMix, that adds token level perturbations through
stochastic mixing of tokens from other contexts in the batch. We show that our
proposed augmentation method outperforms previous data augmentation approaches,
and provides dialogue representations that are more robust to common
perturbations seen during inference.",https://github.com/rkadlec/ubuntu-ranking-dataset-,-1
Transformer Quality in Linear Time,0.999999,"We revisit the design choices in Transformers, and propose methods to address
their weaknesses in handling long sequences. First, we propose a simple layer
named gated attention unit, which allows the use of a weaker single-head
attention with minimal quality loss. We then propose a linear approximation
method complementary to this new layer, which is accelerator-friendly and
highly competitive in quality. The resulting model, named FLASH, matches the
perplexity of improved Transformers over both short (512) and long (8K) context
lengths, achieving training speedups of up to 4.9$\times$ on Wiki-40B and
12.1$\times$ on PG-19 for auto-regressive language modeling, and 4.8$\times$ on
C4 for masked language modeling.",None,22641
CONDA: Continual Unsupervised Domain Adaptation Learning in Visual Perception for Self-Driving Cars,0.350859,"Although unsupervised domain adaptation methods have achieved remarkable
performance in semantic scene segmentation in visual perception for
self-driving cars, these approaches remain impractical in real-world use cases.
In practice, the segmentation models may encounter new data that have not been
seen yet. Also, the previous data training of segmentation models may be
inaccessible due to privacy problems. Therefore, to address these problems, in
this work, we propose a Continual Unsupervised Domain Adaptation (CONDA)
approach that allows the model to continuously learn and adapt with respect to
the presence of the new data. Moreover, our proposed approach is designed
without the requirement of accessing previous training data. To avoid the
catastrophic forgetting problem and maintain the performance of the
segmentation models, we present a novel Bijective Maximum Likelihood loss to
impose the constraint of predicted segmentation distribution shifts. The
experimental results on the benchmark of continual unsupervised domain
adaptation have shown the advanced performance of the proposed CONDA method.",None,4806
Multi hash embeddings in spaCy,0.125457,"The distributed representation of symbols is one of the key technologies in
machine learning systems today, playing a pivotal role in modern natural
language processing. Traditional word embeddings associate a separate vector
with each word. While this approach is simple and leads to good performance, it
requires a lot of memory for representing a large vocabulary. To reduce the
memory footprint, the default embedding layer in spaCy is a hash embeddings
layer. It is a stochastic approximation of traditional embeddings that provides
unique vectors for a large number of words without explicitly storing a
separate vector for each of them. To be able to compute meaningful
representations for both known and unknown words, hash embeddings represent
each word as a summary of the normalized word form, subword information and
word shape. Together, these features produce a multi-embedding of a word. In
this technical report we lay out a bit of history and introduce the embedding
methods in spaCy in detail. Second, we critically evaluate the hash embedding
architecture with multi-embeddings on Named Entity Recognition datasets from a
variety of domains and languages. The experiments validate most key design
choices behind spaCy's embedders, but we also uncover a few surprising results.",https://github.com/explosion/spacy-transformers,-1
"Re2G: Retrieve, Rerank, Generate",0.85879,"As demonstrated by GPT-3 and T5, transformers grow in capability as parameter
spaces become larger and larger. However, for tasks that require a large amount
of knowledge, non-parametric memory allows models to grow dramatically with a
sub-linear increase in computational cost and GPU memory requirements. Recent
models such as RAG and REALM have introduced retrieval into conditional
generation. These models incorporate neural initial retrieval from a corpus of
passages. We build on this line of research, proposing Re2G, which combines
both neural initial retrieval and reranking into a BART-based
sequence-to-sequence generation. Our reranking approach also permits merging
retrieval results from sources with incomparable scores, enabling an ensemble
of BM25 and neural initial retrieval. To train our system end-to-end, we
introduce a novel variation of knowledge distillation to train the initial
retrieval, reranker, and generation using only ground truth on the target
sequence output. We find large gains in four diverse tasks: zero-shot slot
filling, question answering, fact-checking, and dialog, with relative gains of
9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make
our code available as open source at
https://github.com/IBM/kgi-slot-filling/tree/re2g.",https://github.com/IBM/kgi-slot-filling,-1
Towards Explainable Motion Prediction using Heterogeneous Graph Representations,0.250579,"Motion prediction systems aim to capture the future behavior of traffic
scenarios enabling autonomous vehicles to perform safe and efficient planning.
The evolution of these scenarios is highly uncertain and depends on the
interactions of agents with static and dynamic objects in the scene. GNN-based
approaches have recently gained attention as they are well suited to naturally
model these interactions. However, one of the main challenges that remains
unexplored is how to address the complexity and opacity of these models in
order to deal with the transparency requirements for autonomous driving
systems, which includes aspects such as interpretability and explainability. In
this work, we aim to improve the explainability of motion prediction systems by
using different approaches. First, we propose a new Explainable Heterogeneous
Graph-based Policy (XHGP) model based on an heterograph representation of the
traffic scene and lane-graph traversals, which learns interaction behaviors
using object-level and type-level attention. This learned attention provides
information about the most important agents and interactions in the scene.
Second, we explore this same idea with the explanations provided by
GNNExplainer. Third, we apply counterfactual reasoning to provide explanations
of selected individual scenarios by exploring the sensitivity of the trained
model to changes made to the input data, i.e., masking some elements of the
scene, modifying trajectories, and adding or removing dynamic agents. The
explainability analysis provided in this paper is a first step towards more
transparent and reliable motion prediction systems, important from the
perspective of the user, developers and regulatory agencies. The code to
reproduce this work is publicly available at
https://github.com/sancarlim/Explainable-MP/tree/v1.1.",https://github.com/sancarlim/Explainable-MP/tree/v1.1,-1
Invariant Descriptors for Intrinsic Reflectance Optimization,0.23521,"Intrinsic image decomposition aims to factorize an image into albedo
(reflectance) and shading (illumination) sub-components. Being ill-posed and
under-constrained, it is a very challenging computer vision problem. There are
infinite pairs of reflectance and shading images that can reconstruct the same
input. To address the problem, Intrinsic Images in the Wild provides an
optimization framework based on a dense conditional random field (CRF)
formulation that considers long-range material relations. We improve upon their
model by introducing illumination invariant image descriptors: color ratios.
The color ratios and the reflectance intrinsic are both invariant to
illumination and thus are highly correlated. Through detailed experiments, we
provide ways to inject the color ratios into the dense CRF optimization. Our
approach is physics-based, learning-free and leads to more accurate and robust
reflectance decompositions.",https://github.com/seanbell/intrinsic,-1
"Explainable Artificial Intelligence in Construction: The Content, Context, Process, Outcome Evaluation Framework",0.096465,"Explainable artificial intelligence is an emerging and evolving concept. Its
impact on construction, though yet to be realised, will be profound in the
foreseeable future. Still, XAI has received limited attention in construction.
As a result, no evaluation frameworks have been propagated to enable
construction organisations to understand the what, why, how, and when of XAI.
Our paper aims to fill this void by developing a content, context, process, and
outcome evaluation framework that can be used to justify the adoption and
effective management of XAI. After introducing and describing this novel
framework, we discuss its implications for future research. While our novel
framework is conceptual, it provides a frame of reference for construction
organisations to make headway toward realising XAI business value and benefits.",None,-1
Intelligent analysis of EEG signals to assess consumer decisions: A Study on Neuromarketing,0.583814,"Neuromarketing is an emerging field that combines neuroscience and marketing
to understand the factors that influence consumer decisions better. The study
proposes a method to understand consumers' positive and negative reactions to
advertisements (ads) and products by analysing electroencephalogram (EEG)
signals. These signals are recorded using a low-cost single electrode headset
from volunteers belonging to the ages 18-22. A detailed subject dependent (SD)
and subject independent (SI) analysis was performed employing machine learning
methods like Naive Bayes (NB), Support Vector Machine (SVM), k-nearest
neighbour and Decision Tree and the proposed deep learning (DL) model. SVM and
NB yielded an accuracy (Acc.) of 0.63 for the SD analysis. In SI analysis, SVM
performed better for the advertisement, product and gender-based analysis.
Furthermore, the performance of the DL model was on par with that of SVM,
especially, in product and ads-based analysis.",None,-1
SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model,0.873921,"Generic image inpainting aims to complete a corrupted image by borrowing
surrounding information, which barely generates novel content. By contrast,
multi-modal inpainting provides more flexible and useful controls on the
inpainted content, \eg, a text prompt can be used to describe an object with
richer attributes, and a mask can be used to constrain the shape of the
inpainted object rather than being only considered as a missing area. We
propose a new diffusion-based model named SmartBrush for completing a missing
region with an object using both text and shape-guidance. While previous work
such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not
support shape guidance and tend to modify background texture surrounding the
generated object. Our model incorporates both text and shape guidance with
precision control. To preserve the background better, we propose a novel
training and sampling strategy by augmenting the diffusion U-net with
object-mask prediction. Lastly, we introduce a multi-task training strategy by
jointly training inpainting with text-to-image generation to leverage more
training data. We conduct extensive experiments showing that our model
outperforms all baselines in terms of visual quality, mask controllability, and
background preservation.",https://github.com/CompVis/stable-diffusion,-1
Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models,0.280011,"Relations between words are governed by hierarchical structure rather than
linear ordering. Sequence-to-sequence (seq2seq) models, despite their success
in downstream NLP applications, often fail to generalize in a
hierarchy-sensitive manner when performing syntactic transformations - for
example, transforming declarative sentences into questions. However, syntactic
evaluations of seq2seq models have only observed models that were not
pre-trained on natural language data before being trained to perform syntactic
transformations, in spite of the fact that pre-training has been found to
induce hierarchical linguistic generalizations in language models; in other
words, the syntactic capabilities of seq2seq models may have been greatly
understated. We address this gap using the pre-trained seq2seq models T5 and
BART, as well as their multilingual variants mT5 and mBART. We evaluate whether
they generalize hierarchically on two transformations in two languages:
question formation and passivization in English and German. We find that
pre-trained seq2seq models generalize hierarchically when performing syntactic
transformations, whereas models trained from scratch on syntactic
transformations do not. This result presents evidence for the learnability of
hierarchical syntactic information from non-annotated natural language text
while also demonstrating that seq2seq models are capable of syntactic
generalization, though only after exposure to much more language data than
human learners receive.",https://github.com/sebschu/multilingual-transformations,-1
Robust Local Preserving and Global Aligning Network for Adversarial Domain Adaptation,0.605125,"Unsupervised domain adaptation (UDA) requires source domain samples with
clean ground truth labels during training. Accurately labeling a large number
of source domain samples is time-consuming and laborious. An alternative is to
utilize samples with noisy labels for training. However, training with noisy
labels can greatly reduce the performance of UDA. In this paper, we address the
problem that learning UDA models only with access to noisy labels and propose a
novel method called robust local preserving and global aligning network
(RLPGA). RLPGA improves the robustness of the label noise from two aspects. One
is learning a classifier by a robust informative-theoretic-based loss function.
The other is constructing two adjacency weight matrices and two negative weight
matrices by the proposed local preserving module to preserve the local topology
structures of input data. We conduct theoretical analysis on the robustness of
the proposed RLPGA and prove that the robust informative-theoretic-based loss
and the local preserving module are beneficial to reduce the empirical risk of
the target domain. A series of empirical studies show the effectiveness of our
proposed RLPGA.",None,-1
ELQA: A Corpus of Metalinguistic Questions and Answers about English,0.273851,"We present ELQA, a corpus of questions and answers in and about the English
language. Collected from two online forums, the >70k questions (from English
learners and others) cover wide-ranging topics including grammar, meaning,
fluency, and etymology. The answers include descriptions of general properties
of English vocabulary and grammar as well as explanations about specific
(correct and incorrect) usage examples. Unlike most NLP datasets, this corpus
is metalinguistic -- it consists of language about language. As such, it can
facilitate investigations of the metalinguistic capabilities of NLU models, as
well as educational applications in the language learning domain. To study
this, we define a free-form question answering task on our dataset and conduct
evaluations on multiple LLMs (Large Language Models) to analyze their capacity
to generate metalinguistic answers.",https://github.com/google-research/t5x,-1
Spectral Adversarial Training for Robust Graph Neural Network,0.793988,"Recent studies demonstrate that Graph Neural Networks (GNNs) are vulnerable
to slight but adversarially designed perturbations, known as adversarial
examples. To address this issue, robust training methods against adversarial
examples have received considerable attention in the literature.
\emph{Adversarial Training (AT)} is a successful approach to learning a robust
model using adversarially perturbed training samples. Existing AT methods on
GNNs typically construct adversarial perturbations in terms of graph structures
or node features. However, they are less effective and fraught with challenges
on graph data due to the discreteness of graph structure and the relationships
between connected examples. In this work, we seek to address these challenges
and propose Spectral Adversarial Training (SAT), a simple yet effective
adversarial training approach for GNNs. SAT first adopts a low-rank
approximation of the graph structure based on spectral decomposition, and then
constructs adversarial perturbations in the spectral domain rather than
directly manipulating the original graph structure. To investigate its
effectiveness, we employ SAT on three widely used GNNs. Experimental results on
four public graph datasets demonstrate that SAT significantly improves the
robustness of GNNs against adversarial attacks without sacrificing
classification accuracy and training efficiency.",https://github.com/EdisonLeeeee/SAT,-1
Fine-tuning Image Transformers using Learnable Memory,0.413635,"In this paper we propose augmenting Vision Transformer models with learnable
memory tokens. Our approach allows the model to adapt to new tasks, using few
parameters, while optionally preserving its capabilities on previously learned
tasks. At each layer we introduce a set of learnable embedding vectors that
provide contextual information useful for specific datasets. We call these
""memory tokens"". We show that augmenting a model with just a handful of such
tokens per layer significantly improves accuracy when compared to conventional
head-only fine-tuning, and performs only slightly below the significantly more
expensive full fine-tuning. We then propose an attention-masking approach that
enables extension to new downstream tasks, with a computation reuse. In this
setup in addition to being parameters efficient, models can execute both old
and new tasks as a part of single inference at a small incremental cost.",None,-1
On the Effectiveness of Compact Biomedical Transformers,0.664962,"Language models pre-trained on biomedical corpora, such as BioBERT, have
recently shown promising results on downstream biomedical tasks. Many existing
pre-trained models, on the other hand, are resource-intensive and
computationally heavy owing to factors such as embedding size, hidden
dimension, and number of layers. The natural language processing (NLP)
community has developed numerous strategies to compress these models utilising
techniques such as pruning, quantisation, and knowledge distillation, resulting
in models that are considerably faster, smaller, and subsequently easier to use
in practice. By the same token, in this paper we introduce six lightweight
models, namely, BioDistilBERT, BioTinyBERT, BioMobileBERT, DistilBioBERT,
TinyBioBERT, and CompactBioBERT which are obtained either by knowledge
distillation from a biomedical teacher or continual learning on the Pubmed
dataset via the Masked Language Modelling (MLM) objective. We evaluate all of
our models on three biomedical tasks and compare them with BioBERT-v1.1 to
create efficient lightweight models that perform on par with their larger
counterparts. All the models will be publicly available on our Huggingface
profile at https://huggingface.co/nlpie and the codes used to run the
experiments will be available at
https://github.com/nlpie-research/Compact-Biomedical-Transformers.",https://huggingface.co/nlpie,-1
Feature Extraction Framework based on Contrastive Learning with Adaptive Positive and Negative Samples,0.0336759,"In this study, we propose a feature extraction framework based on contrastive
learning with adaptive positive and negative samples (CL-FEFA) that is suitable
for unsupervised, supervised, and semi-supervised single-view feature
extraction. CL-FEFA constructs adaptively the positive and negative samples
from the results of feature extraction, which makes it more appropriate and
accurate. Thereafter, the discriminative features are re extracted to according
to InfoNCE loss based on previous positive and negative samples, which will
make the intra-class samples more compact and the inter-class samples more
dispersed. At the same time, using the potential structure information of
subspace samples to dynamically construct positive and negative samples can
make our framework more robust to noisy data. Furthermore, CL-FEFA considers
the mutual information between positive samples, that is, similar samples in
potential structures, which provides theoretical support for its advantages in
feature extraction. The final numerical experiments prove that the proposed
framework has a strong advantage over the traditional feature extraction
methods and contrastive learning methods.",None,-1
Dynamic Collaborative Multi-Agent Reinforcement Learning Communication for Autonomous Drone Reforestation,0.549418,"We approach autonomous drone-based reforestation with a collaborative
multi-agent reinforcement learning (MARL) setup. Agents can communicate as part
of a dynamically changing network. We explore collaboration and communication
on the back of a high-impact problem. Forests are the main resource to control
rising CO2 conditions. Unfortunately, the global forest volume is decreasing at
an unprecedented rate. Many areas are too large and hard to traverse to plant
new trees. To efficiently cover as much area as possible, here we propose a
Graph Neural Network (GNN) based communication mechanism that enables
collaboration. Agents can share location information on areas needing
reforestation, which increases viewed area and planted tree count. We compare
our proposed communication mechanism with a multi-agent baseline without the
ability to communicate. Results show how communication enables collaboration
and increases collective performance, planting precision and the risk-taking
propensity of individual agents.",None,-1
Rethinking Implicit Neural Representations for Vision Learners,0.252537,"Implicit Neural Representations (INRs) are powerful to parameterize
continuous signals in computer vision. However, almost all INRs methods are
limited to low-level tasks, e.g., image/video compression, super-resolution,
and image generation. The questions on how to explore INRs to high-level tasks
and deep networks are still under-explored. Existing INRs methods suffer from
two problems: 1) narrow theoretical definitions of INRs are inapplicable to
high-level tasks; 2) lack of representation capabilities to deep networks.
Motivated by the above facts, we reformulate the definitions of INRs from a
novel perspective and propose an innovative Implicit Neural Representation
Network (INRN), which is the first study of INRs to tackle both low-level and
high-level tasks. Specifically, we present three key designs for basic blocks
in INRN along with two different stacking ways and corresponding loss
functions. Extensive experiments with analysis on both low-level tasks (image
fitting) and high-level vision tasks (image classification, object detection,
instance segmentation) demonstrate the effectiveness of the proposed method.",None,7989
SC-wLS: Towards Interpretable Feed-forward Camera Re-localization,0.859523,"Visual re-localization aims to recover camera poses in a known environment,
which is vital for applications like robotics or augmented reality.
Feed-forward absolute camera pose regression methods directly output poses by a
network, but suffer from low accuracy. Meanwhile, scene coordinate based
methods are accurate, but need iterative RANSAC post-processing, which brings
challenges to efficient end-to-end training and inference. In order to have the
best of both worlds, we propose a feed-forward method termed SC-wLS that
exploits all scene coordinate estimates for weighted least squares pose
regression. This differentiable formulation exploits a weight network imposed
on 2D-3D correspondences, and requires pose supervision only. Qualitative
results demonstrate the interpretability of learned weights. Evaluations on
7Scenes and Cambridge datasets show significantly promoted performance when
compared with former feed-forward counterparts. Moreover, our SC-wLS method
enables a new capability: self-supervised test-time adaptation on the weight
network. Codes and models are publicly available.",https://github.com/XinWu98/SC-wLSAbstract.Visualre-localizationaimstorecovercameraposesinaknownenvironment,-1
Sparse2Dense: Learning to Densify 3D Features for 3D Object Detection,0.442082,"LiDAR-produced point clouds are the major source for most state-of-the-art 3D
object detectors. Yet, small, distant, and incomplete objects with sparse or
few points are often hard to detect. We present Sparse2Dense, a new framework
to efficiently boost 3D detection performance by learning to densify point
clouds in latent space. Specifically, we first train a dense point 3D detector
(DDet) with a dense point cloud as input and design a sparse point 3D detector
(SDet) with a regular point cloud as input. Importantly, we formulate the
lightweight plug-in S2D module and the point cloud reconstruction module in
SDet to densify 3D features and train SDet to produce 3D features, following
the dense 3D features in DDet. So, in inference, SDet can simulate dense 3D
features from regular (sparse) point cloud inputs without requiring dense
inputs. We evaluate our method on the large-scale Waymo Open Dataset and the
Waymo Domain Adaptation Dataset, showing its high performance and efficiency
over the state of the arts.",None,-1
Emergent Quantized Communication,0.669995,"The field of emergent communication aims to understand the characteristics of
communication as it emerges from artificial agents solving tasks that require
information exchange. Communication with discrete messages is considered a
desired characteristic, for both scientific and applied reasons. However,
training a multi-agent system with discrete communication is not
straightforward, requiring either reinforcement learning algorithms or relaxing
the discreteness requirement via a continuous approximation such as the
Gumbel-softmax. Both these solutions result in poor performance compared to
fully continuous communication. In this work, we propose an alternative
approach to achieve discrete communication -- quantization of communicated
messages. Using message quantization allows us to train the model end-to-end,
achieving superior performance in multiple setups. Moreover, quantization is a
natural framework that runs the gamut from continuous to discrete
communication. Thus, it sets the ground for a broader view of multi-agent
communication in the deep learning era.",None,-1
Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia,0.657772,"While neural networks demonstrate a remarkable ability to model linguistic
content, capturing contextual information related to a speaker's conversational
role is an open area of research. In this work, we analyze the effect of
speaker role on language use through the game of Mafia, in which participants
are assigned either an honest or a deceptive role. In addition to building a
framework to collect a dataset of Mafia game records, we demonstrate that there
are differences in the language produced by players with different roles. We
confirm that classification models are able to rank deceptive players as more
suspicious than honest ones based only on their use of language. Furthermore,
we show that training models on two auxiliary tasks outperforms a standard
BERT-based text classification approach. We also present methods for using our
trained models to identify features that distinguish between player roles,
which could be used to assist players during the Mafia game.",http://github.com/dallinger/Dallinger,-1
Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue,0.761775,"Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question
regarding a given video and dialogue context. Despite the recent success of
multi-modal reasoning to generate answer sentences, existing dialogue systems
still suffer from a text hallucination problem, which denotes indiscriminate
text-copying from input texts without an understanding of the question. This is
due to learning spurious correlations from the fact that answer sentences in
the dataset usually include the words of input texts, thus the VGD system
excessively relies on copying words from input texts by hoping those words to
overlap with ground-truth texts. Hence, we design Text Hallucination Mitigating
(THAM) framework, which incorporates Text Hallucination Regularization (THR)
loss derived from the proposed information-theoretic text hallucination
measurement approach. Applying THAM with current dialogue systems validates the
effectiveness on VGD benchmarks (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows
enhanced interpretability.",https://github.com/dialogtekgeek/DSTC8-AVSD_ofﬁcial,-1
Improving Generalization of Deep Neural Network Acoustic Models with Length Perturbation and N-best Based Label Smoothing,0.190791,"We introduce two techniques, length perturbation and n-best based label
smoothing, to improve generalization of deep neural network (DNN) acoustic
models for automatic speech recognition (ASR). Length perturbation is a data
augmentation algorithm that randomly drops and inserts frames of an utterance
to alter the length of the speech feature sequence. N-best based label
smoothing randomly injects noise to ground truth labels during training in
order to avoid overfitting, where the noisy labels are generated from n-best
hypotheses. We evaluate these two techniques extensively on the 300-hour
Switchboard (SWB300) dataset and an in-house 500-hour Japanese (JPN500) dataset
using recurrent neural network transducer (RNNT) acoustic models for ASR. We
show that both techniques improve the generalization of RNNT models
individually and they can also be complementary. In particular, they yield good
improvements over a strong SWB300 baseline and give state-of-art performance on
SWB300 using RNNT models.",None,-1
Privacy-Friendly Peer-to-Peer Energy Trading: A Game Theoretical Approach,0.362125,"In this paper, we propose a decentralized, privacy-friendly energy trading
platform (PFET) based on game theoretical approach - specifically Stackelberg
competition. Unlike existing trading schemes, PFET provides a competitive
market in which prices and demands are determined based on competition, and
computations are performed in a decentralized manner which does not rely on
trusted third parties. It uses homomorphic encryption cryptosystem to encrypt
sensitive information of buyers and sellers such as sellers$'$ prices and
buyers$'$ demands. Buyers calculate total demand on particular seller using an
encrypted data and sensitive buyer profile data is hidden from sellers. Hence,
privacy of both sellers and buyers is preserved. Through privacy analysis and
performance evaluation, we show that PFET preserves users$'$ privacy in an
efficient manner.",None,-1
Generative Aspect-Based Sentiment Analysis with Contrastive Learning and Expressive Structure,0.86535,"Generative models have demonstrated impressive results on Aspect-based
Sentiment Analysis (ABSA) tasks, particularly for the emerging task of
extracting Aspect-Category-Opinion-Sentiment (ACOS) quadruples. However, these
models struggle with implicit sentiment expressions, which are commonly
observed in opinionated content such as online reviews. In this work, we
introduce GEN-SCL-NAT, which consists of two techniques for improved structured
generation for ACOS quadruple extraction. First, we propose GEN-SCL, a
supervised contrastive learning objective that aids quadruple prediction by
encouraging the model to produce input representations that are discriminable
across key input attributes, such as sentiment polarity and the existence of
implicit opinions and aspects. Second, we introduce GEN-NAT, a new structured
generation format that better adapts autoregressive encoder-decoder models to
extract quadruples in a generative fashion. Experimental results show that
GEN-SCL-NAT achieves top performance across three ACOS datasets, averaging
1.48% F1 improvement, with a maximum 1.73% increase on the LAPTOP-L1 dataset.
Additionally, we see significant gains on implicit aspect and opinion splits
that have been shown as challenging for existing ACOS approaches.",https://github.com/jpeper/GEN_SCL_NAT,-1
"Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices",0.987559,"In this paper, we consider the problem of real-time video-based facial
emotion analytics, namely, facial expression recognition, prediction of valence
and arousal and detection of action unit points. We propose the novel
frame-level emotion recognition algorithm by extracting facial features with
the single EfficientNet model pre-trained on AffectNet. As a result, our
approach may be implemented even for video analytics on mobile devices.
Experimental results for the large scale Aff-Wild2 database from the third
Affective Behavior Analysis in-the-wild (ABAW) Competition demonstrate that our
simple model is significantly better when compared to the VggFace baseline. In
particular, our method is characterized by 0.15-0.2 higher performance measures
for validation sets in uni-task Expression Classification, Valence-Arousal
Estimation and Expression Classification. Due to simplicity, our approach may
be considered as a new baseline for all four sub-challenges.",None,-1
ImPaKT: A Dataset for Open-Schema Knowledge Base Construction,0.338448,"Large language models have ushered in a golden age of semantic parsing. The
seq2seq paradigm allows for open-schema and abstractive attribute and relation
extraction given only small amounts of finetuning data. Language model
pretraining has simultaneously enabled great strides in natural language
inference, reasoning about entailment and implication in free text. These
advances motivate us to construct ImPaKT, a dataset for open-schema information
extraction, consisting of around 2500 text snippets from the C4 corpus, in the
shopping domain (product buying guides), professionally annotated with
extracted attributes, types, attribute summaries (attribute schema discovery
from idiosyncratic text), many-to-one relations between compound and atomic
attributes, and implication relations. We release this data in hope that it
will be useful in fine tuning semantic parsers for information extraction and
knowledge base construction across a variety of domains. We evaluate the power
of this approach by fine-tuning the open source UL2 language model on a subset
of the dataset, extracting a set of implication relations from a corpus of
product buying guides, and conducting human evaluations of the resulting
predictions.",None,-1
Was that so hard? Estimating human classification difficulty,0.0938127,"When doctors are trained to diagnose a specific disease, they learn faster
when presented with cases in order of increasing difficulty. This creates the
need for automatically estimating how difficult it is for doctors to classify a
given case. In this paper, we introduce methods for estimating how hard it is
for a doctor to diagnose a case represented by a medical image, both when
ground truth difficulties are available for training, and when they are not.
Our methods are based on embeddings obtained with deep metric learning.
Additionally, we introduce a practical method for obtaining ground truth human
difficulty for each image case in a dataset using self-assessed certainty. We
apply our methods to two different medical datasets, achieving high Kendall
rank correlation coefficients, showing that we outperform existing methods by a
large margin on our problem and data.",None,-1
Signature Entrenchment and Conceptual Changes in Automated Theory Repair,0.496419,"Human beliefs change, but so do the concepts that underpin them. The recent
Abduction, Belief Revision and Conceptual Change (ABC) repair system combines
several methods from automated theory repair to expand, contract, or reform
logical structures representing conceptual knowledge in artificial agents. In
this paper we focus on conceptual change: repair not only of the membership of
logical concepts, such as what animals can fly, but also concepts themselves,
such that birds may be divided into flightless and flying birds, by changing
the signature of the logical theory used to represent them. We offer a method
for automatically evaluating entrenchment in the signature of a Datalog theory,
in order to constrain automated theory repair to succinct and intuitive
outcomes. Formally, signature entrenchment measures the inferential
contributions of every logical language element used to express conceptual
knowledge, i.e., predicates and the arguments, ranking possible repairs to
retain valuable logical concepts and reject redundant or implausible
alternatives. This quantitative measurement of signature entrenchment offers a
guide to the plausibility of conceptual changes, which we aim to contrast with
human judgements of concept entrenchment in future work.",https://github.com/XuerLi/Publications/tree/main/ACS2021,-1
FL Games: A federated learning framework for distribution shifts,0.358566,"Federated learning aims to train predictive models for data that is
distributed across clients, under the orchestration of a server. However,
participating clients typically each hold data from a different distribution,
whereby predictive models with strong in-distribution generalization can fail
catastrophically on unseen domains. In this work, we argue that in order to
generalize better across non-i.i.d. clients, it is imperative to only learn
correlations that are stable and invariant across domains. We propose FL Games,
a game-theoretic framework for federated learning for learning causal features
that are invariant across clients. While training to achieve the Nash
equilibrium, the traditional best response strategy suffers from high-frequency
oscillations. We demonstrate that FL Games effectively resolves this challenge
and exhibits smooth performance curves. Further, FL Games scales well in the
number of clients, requires significantly fewer communication rounds, and is
agnostic to device heterogeneity. Through empirical evaluation, we demonstrate
that FL Games achieves high out-of-distribution performance on various
benchmarks.",None,-1
Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities,0.639195,"As for other forms of AI, speech recognition has recently been examined with
respect to performance disparities across different user cohorts. One approach
to achieve fairness in speech recognition is to (1) identify speaker cohorts
that suffer from subpar performance and (2) apply fairness mitigation measures
targeting the cohorts discovered. In this paper, we report on initial findings
with both discovery and mitigation of performance disparities using data from a
product-scale AI assistant speech recognition system. We compare cohort
discovery based on geographic and demographic information to a more scalable
method that groups speakers without human labels, using speaker embedding
technology. For fairness mitigation, we find that oversampling of
underrepresented cohorts, as well as modeling speaker cohort membership by
additional input variables, reduces the gap between top- and bottom-performing
cohorts, without deteriorating overall recognition accuracy.",None,31243
Conformance Checking with Uncertainty via SMT (Extended Version),0.513689,"Logs of real-life processes often feature uncertainty pertaining the recorded
timestamps, data values, and/or events. We consider the problem of checking
conformance of uncertain logs against data-aware reference processes.
Specifically, we show how to solve it via SMT encodings, lifting previous work
on data-aware SMT-based conformance checking to this more sophisticated
setting. Our approach is modular, in that it homogeneously accommodates for
different types of uncertainty. Moreover, using appropriate cost functions,
different conformance checking tasks can be addressed. We show the correctness
of our approach and witness feasibility through a proof-of-concept
implementation.",https://github.com/bytekid/cocomot,8569
Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation,0.67135,"After the great success of Vision Transformer variants (ViTs) in computer
vision, it has also demonstrated great potential in domain adaptive semantic
segmentation. Unfortunately, straightforwardly applying local ViTs in domain
adaptive semantic segmentation does not bring in expected improvement. We find
that the pitfall of local ViTs is due to the severe high-frequency components
generated during both the pseudo-label construction and features alignment for
target domains. These high-frequency components make the training of local ViTs
very unsmooth and hurt their transferability. In this paper, we introduce a
low-pass filtering mechanism, momentum network, to smooth the learning dynamics
of target domain features and pseudo labels. Furthermore, we propose a dynamic
of discrepancy measurement to align the distributions in the source and target
domains via dynamic weights to evaluate the importance of the samples. After
tackling the above issues, extensive experiments on sim2real benchmarks show
that the proposed method outperforms the state-of-the-art methods. Our codes
are available at https://github.com/alpc91/TransDA",https://github.com/alpc91/TransDA,24953
Transfer Learning with Pre-trained Conditional Generative Models,0.0740636,"Transfer learning is crucial in training deep neural networks on new target
tasks. Current transfer learning methods always assume at least one of (i)
source and target task label spaces overlap, (ii) source datasets are
available, and (iii) target network architectures are consistent with source
ones. However, holding these assumptions is difficult in practical settings
because the target task rarely has the same labels as the source task, the
source dataset access is restricted due to storage costs and privacy, and the
target architecture is often specialized to each task. To transfer source
knowledge without these assumptions, we propose a transfer learning method that
uses deep generative models and is composed of the following two stages: pseudo
pre-training (PP) and pseudo semi-supervised learning (P-SSL). PP trains a
target architecture with an artificial dataset synthesized by using conditional
source generative models. P-SSL applies SSL algorithms to labeled target data
and unlabeled pseudo samples, which are generated by cascading the source
classifier and generative models to condition them with target samples. Our
experimental results indicate that our method can outperform the baselines of
scratch training and knowledge distillation.",https://github.com/huggingface/pytorch-pretrained-BigGAN,-1
HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance,0.802133,"Marker-less monocular 3D human motion capture (MoCap) with scene interactions
is a challenging research topic relevant for extended reality, robotics and
virtual avatar generation. Due to the inherent depth ambiguity of monocular
settings, 3D motions captured with existing methods often contain severe
artefacts such as incorrect body-scene inter-penetrations, jitter and body
floating. To tackle these issues, we propose HULC, a new approach for 3D human
MoCap which is aware of the scene geometry. HULC estimates 3D poses and dense
body-environment surface contacts for improved 3D localisations, as well as the
absolute scale of the subject. Furthermore, we introduce a 3D pose trajectory
optimisation based on a novel pose manifold sampling that resolves erroneous
body-environment inter-penetrations. Although the proposed method requires less
structured inputs compared to existing scene-aware monocular MoCap algorithms,
it produces more physically-plausible poses: HULC significantly and
consistently outperforms the existing approaches in various experiments and on
different metrics. Project page: https://vcai.mpi-inf.mpg.de/projects/HULC/.",None,-1
Physical Reasoning in an Open World,0.0683882,"Most work on physical reasoning, both in artificial intelligence and in
cognitive science, has focused on closed-world reasoning, in which it is
assumed that the problem specification specifies all relevant objects and
substance, all their relations in an initial situation, and all exogenous
events. However, in many situations, it is important to do open-world
reasoning; that is, making valid conclusions from very incomplete information.
We have implemented in Prolog an open-world reasoner for a toy microworld of
containers that can be loaded, unloaded, sealed, unsealed, carried, and dumped.",https://github.com/Jennifercheukyin/Physical-Reasoning-in-Open-World,-1
NTULM: Enriching Social Media Text Representations with Non-Textual Units,0.408569,"On social media, additional context is often present in the form of
annotations and meta-data such as the post's author, mentions, Hashtags, and
hyperlinks. We refer to these annotations as Non-Textual Units (NTUs). We posit
that NTUs provide social context beyond their textual semantics and leveraging
these units can enrich social media text representations. In this work we
construct an NTU-centric social heterogeneous network to co-embed NTUs. We then
principally integrate these NTU embeddings into a large pretrained language
model by fine-tuning with these additional units. This adds context to noisy
short-text social media. Experiments show that utilizing NTU-augmented text
representations significantly outperforms existing text-only baselines by 2-5\%
relative points on many downstream tasks highlighting the importance of context
to social media NLP. We also highlight that including NTU context into the
initial layers of language model alongside text is better than using it after
the text embedding is generated. Our work leads to the generation of holistic
general purpose social media content embedding.",None,-1
BioTABQA: Instruction Learning for Biomedical Table Question Answering,0.780816,"Table Question Answering (TQA) is an important but under-explored task. Most
of the existing QA datasets are in unstructured text format and only few of
them use tables as the context. To the best of our knowledge, none of TQA
datasets exist in the biomedical domain where tables are frequently used to
present information. In this paper, we first curate a table question answering
dataset, BioTABQA, using 22 templates and the context from a biomedical
textbook on differential diagnosis. BioTABQA can not only be used to teach a
model how to answer questions from tables but also evaluate how a model
generalizes to unseen questions, an important scenario for biomedical
applications. To achieve the generalization evaluation, we divide the templates
into 17 training and 5 cross-task evaluations. Then, we develop two baselines
using single and multi-tasks learning on BioTABQA. Furthermore, we explore
instructional learning, a recent technique showing impressive generalizing
performance. Experimental results show that our instruction-tuned model
outperforms single and multi-task baselines on an average by ~23% and ~6%
across various evaluation settings, and more importantly, instruction-tuned
model outperforms baselines by ~5% on cross-tasks.",None,14059
Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence,0.715379,"The logical negation property (LNP), which implies generating different
predictions for semantically opposite inputs, is an important property that a
trustworthy language model must satisfy. However, much recent evidence shows
that large-size pre-trained language models (PLMs) do not satisfy this
property. In this paper, we perform experiments using probing tasks to assess
PLM's LNP understanding. Unlike previous studies that only examined negation
expressions, we expand the boundary of the investigation to lexical semantics.
Through experiments, we observe that PLMs violate the LNP frequently. To
alleviate the issue, we propose a novel intermediate training task, names
meaning-matching, designed to directly learn a meaning-text correspondence,
instead of relying on the distributional hypothesis. Through multiple
experiments, we find that the task enables PLMs to learn lexical semantic
information. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm
that it is a safe intermediate task that guarantees a similar or better
performance of downstream tasks. Finally, we observe that our proposed approach
outperforms our previous counterparts despite its time and resource efficiency.",https://github.com/MJ-Jang/beyond-distributional,-1
From Zero to Production: Baltic-Ukrainian Machine Translation Systems to Aid Refugees,0.0711584,"In this paper, we examine the development and usage of six low-resource
machine translation systems translating between the Ukrainian language and each
of the official languages of the Baltic states. We developed these systems in
reaction to the escalating Ukrainian refugee crisis caused by the Russian
military aggression in Ukraine in the hope that they might be helpful for
refugees and public administrations. Now, two months after MT systems were made
public, we analyze their usage patterns and statistics. Our findings show that
the Latvian-Ukrainian and Lithuanian-Ukrainian systems are integrated into the
public services of Baltic states, leading to more than 127 million translated
sentences for the Lithuanian-Ukrainian system. Motivated by these findings, we
further enhance our MT systems by better Ukrainian toponym translation and
publish an improved version of the Lithuanian-Ukrainian system.",None,-1
Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching,0.704124,"Ontology Matching (OM) plays an important role in many domains such as
bioinformatics and the Semantic Web, and its research is becoming increasingly
popular, especially with the application of machine learning (ML) techniques.
Although the Ontology Alignment Evaluation Initiative (OAEI) represents an
impressive effort for the systematic evaluation of OM systems, it still suffers
from several limitations including limited evaluation of subsumption mappings,
suboptimal reference mappings, and limited support for the evaluation of
ML-based systems. To tackle these limitations, we introduce five new biomedical
OM tasks involving ontologies extracted from Mondo and UMLS. Each task includes
both equivalence and subsumption matching; the quality of reference mappings is
ensured by human curation, ontology pruning, etc.; and a comprehensive
evaluation framework is proposed to measure OM performance from various
perspectives for both ML-based and non-ML-based OM systems. We report
evaluation results for OM systems of different types to demonstrate the usage
of these resources, all of which are publicly available as part of the new
BioML track at OAEI 2022.",https://github.com/KRR-Oxford/DeepOnto,-1
Improving Low-Resource Speech Recognition with Pretrained Speech Models: Continued Pretraining vs. Semi-Supervised Training,0.553124,"Self-supervised Transformer based models, such as wav2vec 2.0 and HuBERT,
have produced significant improvements over existing approaches to automatic
speech recognition (ASR). This is evident in the performance of the wav2vec 2.0
based pretrained XLSR-53 model across many languages when fine-tuned with
available labeled data. However, the performance from finetuning these models
can be dependent on the amount of in-language or similar-to-in-language data
included in the pretraining dataset. In this paper we investigate continued
pretraining (CoPT) with unlabeled in-language audio data on the XLSR-53
pretrained model in several low-resource languages. CoPT is more
computationally efficient than semi-supervised training (SST), the standard
approach of utilizing unlabeled data in ASR, since it omits the need for
pseudo-labeling of the unlabeled data. We show CoPT results in word error rates
(WERs), equal to or slightly better than using SST. In addition, we show that
using the CoPT model for pseudo-labeling, and using these labels in SST,
results in further improvements in WER.",None,604
GaIA: Graphical Information Gain based Attention Network for Weakly Supervised Point Cloud Semantic Segmentation,0.735116,"While point cloud semantic segmentation is a significant task in 3D scene
understanding, this task demands a time-consuming process of fully annotating
labels. To address this problem, recent studies adopt a weakly supervised
learning approach under the sparse annotation. Different from the existing
studies, this study aims to reduce the epistemic uncertainty measured by the
entropy for a precise semantic segmentation. We propose the graphical
information gain based attention network called GaIA, which alleviates the
entropy of each point based on the reliable information. The graphical
information gain discriminates the reliable point by employing relative entropy
between target point and its neighborhoods. We further introduce anchor-based
additive angular margin loss, ArcPoint. The ArcPoint optimizes the unlabeled
points containing high entropy towards semantically similar classes of the
labeled points on hypersphere space. Experimental results on S3DIS and
ScanNet-v2 datasets demonstrate our framework outperforms the existing weakly
supervised methods. We have released GaIA at https://github.com/Karel911/GaIA.",https://github.com/Karel911/GaIA,-1
AAM-Gym: Artificial Intelligence Testbed for Advanced Air Mobility,0.616116,"We introduce AAM-Gym, a research and development testbed for Advanced Air
Mobility (AAM). AAM has the potential to revolutionize travel by reducing
ground traffic and emissions by leveraging new types of aircraft such as
electric vertical take-off and landing (eVTOL) aircraft and new advanced
artificial intelligence (AI) algorithms. Validation of AI algorithms require
representative AAM scenarios, as well as a fast time simulation testbed to
evaluate their performance. Until now, there has been no such testbed available
for AAM to enable a common research platform for individuals in government,
industry, or academia. MIT Lincoln Laboratory has developed AAM-Gym to address
this gap by providing an ecosystem to develop, train, and validate new and
established AI algorithms across a wide variety of AAM use-cases. In this
paper, we use AAM-Gym to study the performance of two reinforcement learning
algorithms on an AAM use-case, separation assurance in AAM corridors. The
performance of the two algorithms is demonstrated based on a series of metrics
provided by AAM-Gym, showing the testbed's utility to AAM research.",None,-1
Computing and Exploiting Document Structure to Improve Unsupervised Extractive Summarization of Legal Case Decisions,0.554215,"Though many algorithms can be used to automatically summarize legal case
decisions, most fail to incorporate domain knowledge about how important
sentences in a legal decision relate to a representation of its document
structure. For example, analysis of a legal case summarization dataset
demonstrates that sentences serving different types of argumentative roles in
the decision appear in different sections of the document. In this work, we
propose an unsupervised graph-based ranking model that uses a reweighting
algorithm to exploit properties of the document structure of legal case
decisions. We also explore the impact of using different methods to compute the
document structure. Results on the Canadian Legal Case Law dataset show that
our proposed method outperforms several strong baselines.",https://github.com/cs329yangzhong/DocumentStructureLegalSum,-1
Processing the structure of documents: Logical Layout Analysis of historical newspapers in French,0.495934,"Background. In recent years, libraries and archives led important
digitisation campaigns that opened the access to vast collections of historical
documents. While such documents are often available as XML ALTO documents, they
lack information about their logical structure. In this paper, we address the
problem of Logical Layout Analysis applied to historical documents in French.
We propose a rule-based method, that we evaluate and compare with two
Machine-Learning models, namely RIPPER and Gradient Boosting. Our data set
contains French newspapers, periodicals and magazines, published in the first
half of the twentieth century in the Franche-Comt\'e Region. Results. Our
rule-based system outperforms the two other models in nearly all evaluations.
It has especially better Recall results, indicating that our system covers more
types of every logical label than the other two models. When comparing RIPPER
with Gradient Boosting, we can observe that Gradient Boosting has better
Precision scores but RIPPER has better Recall scores. Conclusions. The
evaluation shows that our system outperforms the two Machine Learning models,
and provides significantly higher Recall. It also confirms that our system can
be used to produce annotated data sets that are large enough to envisage
Machine Learning or Deep Learning approaches for the task of Logical Layout
Analysis. Combining rules and Machine Learning models into hybrid systems could
potentially provide even better performances. Furthermore, as the layout in
historical documents evolves rapidly, one possible solution to overcome this
problem would be to apply Rule Learning algorithms to bootstrap rule sets
adapted to different publication periods.",None,703
Uncertainty-aware Personal Assistant for Making Personalized Privacy Decisions,0.683977,"Many software systems, such as online social networks enable users to share
information about themselves. While the action of sharing is simple, it
requires an elaborate thought process on privacy: what to share, with whom to
share, and for what purposes. Thinking about these for each piece of content to
be shared is tedious. Recent approaches to tackle this problem build personal
assistants that can help users by learning what is private over time and
recommending privacy labels such as private or public to individual content
that a user considers sharing. However, privacy is inherently ambiguous and
highly personal. Existing approaches to recommend privacy decisions do not
address these aspects of privacy sufficiently. Ideally, a personal assistant
should be able to adjust its recommendation based on a given user, considering
that user's privacy understanding. Moreover, the personal assistant should be
able to assess when its recommendation would be uncertain and let the user make
the decision on her own. Accordingly, this paper proposes a personal assistant
that uses evidential deep learning to classify content based on its privacy
label. An important characteristic of the personal assistant is that it can
model its uncertainty in its decisions explicitly, determine that it does not
know the answer, and delegate from making a recommendation when its uncertainty
is high. By factoring in the user's own understanding of privacy, such as risk
factors or own labels, the personal assistant can personalize its
recommendations per user. We evaluate our proposed personal assistant using a
well-known data set. Our results show that our personal assistant can
accurately identify uncertain cases, personalize them to its user's needs, and
thus helps users preserve their privacy well.",None,-1
Support-set based Multi-modal Representation Enhancement for Video Captioning,0.0651341,"Video captioning is a challenging task that necessitates a thorough
comprehension of visual scenes. Existing methods follow a typical one-to-one
mapping, which concentrates on a limited sample space while ignoring the
intrinsic semantic associations between samples, resulting in rigid and
uninformative expressions. To address this issue, we propose a novel and
flexible framework, namely Support-set based Multi-modal Representation
Enhancement (SMRE) model, to mine rich information in a semantic subspace
shared between samples. Specifically, we propose a Support-set Construction
(SC) module to construct a support-set to learn underlying connections between
samples and obtain semantic-related visual elements. During this process, we
design a Semantic Space Transformation (SST) module to constrain relative
distance and administrate multi-modal interactions in a self-supervised way.
Extensive experiments on MSVD and MSR-VTT datasets demonstrate that our SMRE
achieves state-of-the-art performance.",https://github.com/SMRE-CV/SMRE,-1
Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video,0.707822,"We present HandAvatar, a novel representation for hand animation and
rendering, which can generate smoothly compositional geometry and
self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as
a high-resolution mesh topology to fit personalized hand shapes. Sequentially,
we decompose hand geometry into per-bone rigid parts, and then re-compose
paired geometry encodings to derive an across-part consistent occupancy field.
As for texture modeling, we propose a self-occlusion-aware shading field
(SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record
albedo information under a wide variety of hand poses. Moreover, directed soft
occupancy is designed to describe the ray-to-surface relation, which is
leveraged to generate an illumination field for the disentanglement of
pose-independent albedo and pose-dependent illumination. Trained from monocular
video data, our HandAvatar can perform free-pose hand animation and rendering
while at the same time achieving superior appearance fidelity. We also
demonstrate that HandAvatar provides a route for hand appearance editing.
Project website: https://seanchenxy.github.io/HandAvatarWeb.",None,2845
PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection,0.796125,"Recent years have witnessed a trend of applying context frames to boost the
performance of object detection as video object detection. Existing methods
usually aggregate features at one stroke to enhance the feature. These methods,
however, usually lack spatial information from neighboring frames and suffer
from insufficient feature aggregation. To address the issues, we perform a
progressive way to introduce both temporal information and spatial information
for an integrated enhancement. The temporal information is introduced by the
temporal feature aggregation model (TFAM), by conducting an attention mechanism
between the context frames and the target frame (i.e., the frame to be
detected). Meanwhile, we employ a Spatial Transition Awareness Model (STAM) to
convey the location transition information between each context frame and
target frame. Built upon a transformer-based detector DETR, our PTSEFormer also
follows an end-to-end fashion to avoid heavy post-processing procedures while
achieving 88.1% mAP on the ImageNet VID dataset. Codes are available at
https://github.com/Hon-Wong/PTSEFormer.",https://github.com/Hon-Wong/PTSEFormer,-1
Breadth-First Pipeline Parallelism,0.089531,"We introduce Breadth-First Pipeline Parallelism, a novel training schedule
which optimizes the combination of pipeline and data parallelism. Breadth-First
Pipeline Parallelism lowers training time, cost and memory usage by combining a
high GPU utilization with a small batch size per GPU, and by making use of
fully sharded data parallelism. Experimentally, we observed an increase of up
to 43% in training throughput for a 52 billion-parameter model using a small
batch size per GPU compared to Megatron-LM, which would reduce the training
time and cost by the same amount on a large GPU cluster.",None,-1
System Resilience through Health Monitoring and Reconfiguration,0.161429,"We demonstrate an end-to-end framework to improve the resilience of man-made
systems to unforeseen events. The framework is based on a physics-based digital
twin model and three modules tasked with real-time fault diagnosis, prognostics
and reconfiguration. The fault diagnosis module uses model-based diagnosis
algorithms to detect and isolate faults and generates interventions in the
system to disambiguate uncertain diagnosis solutions. We scale up the fault
diagnosis algorithm to the required real-time performance through the use of
parallelization and surrogate models of the physics-based digital twin. The
prognostics module tracks the fault progressions and trains the online
degradation models to compute remaining useful life of system components. In
addition, we use the degradation models to assess the impact of the fault
progression on the operational requirements. The reconfiguration module uses
PDDL-based planning endowed with semantic attachments to adjust the system
controls so that the fault impact on the system operation is minimized. We
define a resilience metric and use the example of a fuel system model to
demonstrate how the metric improves with our framework.",None,-1
Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors,0.778976,"We present a method for inferring diverse 3D models of human-object
interactions from images. Reasoning about how humans interact with objects in
complex scenes from a single 2D image is a challenging task given ambiguities
arising from the loss of information through projection. In addition, modeling
3D interactions requires the generalization ability towards diverse object
categories and interaction types. We propose an action-conditioned modeling of
interactions that allows us to infer diverse 3D arrangements of humans and
objects without supervision on contact regions or 3D scene geometry. Our method
extracts high-level commonsense knowledge from large language models (such as
GPT-3), and applies them to perform 3D reasoning of human-object interactions.
Our key insight is priors extracted from large language models can help in
reasoning about human-object contacts from textural prompts only. We
quantitatively evaluate the inferred 3D models on a large human-object
interaction dataset and show how our method leads to better 3D reconstructions.
We further qualitatively evaluate the effectiveness of our method on real
images and demonstrate its generalizability towards interaction types and
object categories.",https://github.com/open-mmlab/mmpose,-1
Layout Aware Inpainting for Automated Furniture Removal in Indoor Scenes,0.593599,"We address the problem of detecting and erasing furniture from a wide angle
photograph of a room. Inpainting large regions of an indoor scene often results
in geometric inconsistencies of background elements within the inpaint mask. To
address this problem, we utilize perceptual information (e.g. instance
segmentation, and room layout) to produce a geometrically consistent empty
version of a room. We share important details to make this system viable, such
as per-plane inpainting, automatic rectification, and texture refinement. We
provide detailed ablation along with qualitative examples, justifying our
design choices. We show an application of our system by removing real furniture
from a room and redecorating it with virtual furniture.",None,-1
Monte Carlo Tree Search Algorithms for Risk-Aware and Multi-Objective Reinforcement Learning,0.533053,"In many risk-aware and multi-objective reinforcement learning settings, the
utility of the user is derived from a single execution of a policy. In these
settings, making decisions based on the average future returns is not suitable.
For example, in a medical setting a patient may only have one opportunity to
treat their illness. Making decisions using just the expected future returns --
known in reinforcement learning as the value -- cannot account for the
potential range of adverse or positive outcomes a decision may have. Therefore,
we should use the distribution over expected future returns differently to
represent the critical information that the agent requires at decision time by
taking both the future and accrued returns into consideration. In this paper,
we propose two novel Monte Carlo tree search algorithms. Firstly, we present a
Monte Carlo tree search algorithm that can compute policies for nonlinear
utility functions (NLU-MCTS) by optimising the utility of the different
possible returns attainable from individual policy executions, resulting in
good policies for both risk-aware and multi-objective settings. Secondly, we
propose a distributional Monte Carlo tree search algorithm (DMCTS) which
extends NLU-MCTS. DMCTS computes an approximate posterior distribution over the
utility of the returns, and utilises Thompson sampling during planning to
compute policies in risk-aware and multi-objective settings. Both algorithms
outperform the state-of-the-art in multi-objective reinforcement learning for
the expected utility of the returns.",None,-1
NeuPhysics: Editable Neural Geometry and Physics from Monocular Videos,0.790482,"We present a method for learning 3D geometry and physics parameters of a
dynamic scene from only a monocular RGB video input. To decouple the learning
of underlying scene geometry from dynamic motion, we represent the scene as a
time-invariant signed distance function (SDF) which serves as a reference
frame, along with a time-conditioned deformation field. We further bridge this
neural geometry representation with a differentiable physics simulator by
designing a two-way conversion between the neural field and its corresponding
hexahedral mesh, enabling us to estimate physics parameters from the source
video by minimizing a cycle consistency loss. Our method also allows a user to
interactively edit 3D objects from the source video by modifying the recovered
hexahedral mesh, and propagating the operation back to the neural field
representation. Experiments show that our method achieves superior mesh and
video reconstruction of dynamic scenes compared to competing Neural Field
approaches, and we provide extensive examples which demonstrate its ability to
extract useful 3D representations from videos captured with consumer-grade
cameras.",https://sites.google.com/view/neuphysics,-1
Learning to Execute Actions or Ask Clarification Questions,0.82288,"Collaborative tasks are ubiquitous activities where a form of communication
is required in order to reach a joint goal. Collaborative building is one of
such tasks. We wish to develop an intelligent builder agent in a simulated
building environment (Minecraft) that can build whatever users wish to build by
just talking to the agent. In order to achieve this goal, such agents need to
be able to take the initiative by asking clarification questions when further
information is needed. Existing works on Minecraft Corpus Dataset only learn to
execute instructions neglecting the importance of asking for clarifications. In
this paper, we extend the Minecraft Corpus Dataset by annotating all builder
utterances into eight types, including clarification questions, and propose a
new builder agent model capable of determining when to ask or execute
instructions. Experimental results show that our model achieves
state-of-the-art performance on the collaborative building task with a
substantial improvement. We also define two new tasks, the learning to ask task
and the joint learning task. The latter consists of solving both collaborating
building and learning to ask tasks jointly.",https://github.com/ZhengxiangShi/LearnToAsk,-1
Quantifying syntax similarity with a polynomial representation of dependency trees,0.533744,"We introduce a graph polynomial that distinguishes tree structures to
represent dependency grammar and a measure based on the polynomial
representation to quantify syntax similarity. The polynomial encodes accurate
and comprehensive information about the dependency structure and dependency
relations of words in a sentence. We apply the polynomial-based methods to
analyze sentences in the Parallel Universal Dependencies treebanks.
Specifically, we compare the syntax of sentences and their translations in
different languages, and we perform a syntactic typology study of available
languages in the Parallel Universal Dependencies treebanks. We also demonstrate
and discuss the potential of the methods in measuring syntax diversity of
corpora.",https://github.com/pliumath/dependencies,-1
Whodunit? Learning to Contrast for Authorship Attribution,0.773398,"Authorship attribution is the task of identifying the author of a given text.
The key is finding representations that can differentiate between authors.
Existing approaches typically use manually designed features that capture a
dataset's content and style, but these approaches are dataset-dependent and
yield inconsistent performance across corpora. In this work, we propose
\textit{learning} author-specific representations by fine-tuning pre-trained
generic language representations with a contrastive objective (Contra-X). We
show that Contra-X learns representations that form highly separable clusters
for different authors. It advances the state-of-the-art on multiple human and
machine authorship attribution benchmarks, enabling improvements of up to 6.8%
over cross-entropy fine-tuning. However, we find that Contra-X improves overall
accuracy at the cost of sacrificing performance for some authors. Resolving
this tension will be an important direction for future work. To the best of our
knowledge, we are the first to integrate contrastive learning with pre-trained
language model fine-tuning for authorship attribution.",https://github.com/BoAi01/Contra-X.git,-1
Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition,0.77845,"The research and applications of multimodal emotion recognition have become
increasingly popular recently. However, multimodal emotion recognition faces
the challenge of lack of data. To solve this problem, we propose to use
transfer learning which leverages state-of-the-art pre-trained models including
wav2vec 2.0 and BERT for this task. Multi-level fusion approaches including
coattention-based early fusion and late fusion with the models trained on both
embeddings are explored. Also, a multi-granularity framework which extracts not
only frame-level speech embeddings but also segment-level embeddings including
phone, syllable and word-level speech embeddings is proposed to further boost
the performance. By combining our coattention-based early fusion model and late
fusion model with the multi-granularity feature extraction framework, we obtain
result that outperforms best baseline approaches by 1.3% unweighted accuracy
(UA) on the IEMOCAP dataset.",None,-1
Compositional Law Parsing with Latent Random Functions,0.603469,"Human cognition has compositionality. We understand a scene by decomposing
the scene into different concepts (e.g., shape and position of an object) and
learning the respective laws of these concepts, which may be either natural
(e.g., laws of motion) or man-made (e.g., laws of a game). The automatic
parsing of these laws indicates the model's ability to understand the scene,
which makes law parsing play a central role in many visual tasks. This paper
proposes a deep latent variable model for Compositional LAw Parsing (CLAP),
which achieves the human-like compositionality ability through an
encoding-decoding architecture to represent concepts in the scene as latent
variables. CLAP employs concept-specific latent random functions instantiated
with Neural Processes to capture the law of concepts. Our experimental results
demonstrate that CLAP outperforms the baseline methods in multiple visual tasks
such as intuitive physics, abstract visual reasoning, and scene representation.
The law manipulation experiments illustrate CLAP's interpretability by
modifying specific latent random functions on samples. For example, CLAP learns
the laws of position-changing and appearance constancy from the moving balls in
a scene, making it possible to exchange laws between samples or compose
existing laws into novel laws.",https://github.com/FudanVI/generative-abstract-reasoning/tree/main/clap,-1
The Frost Hollow Experiments: Pavlovian Signalling as a Path to Coordination and Communication Between Agents,0.221329,"Learned communication between agents is a powerful tool when approaching
decision-making problems that are hard to overcome by any single agent in
isolation. However, continual coordination and communication learning between
machine agents or human-machine partnerships remains a challenging open
problem. As a stepping stone toward solving the continual communication
learning problem, in this paper we contribute a multi-faceted study into what
we term Pavlovian signalling -- a process by which learned, temporally extended
predictions made by one agent inform decision-making by another agent with
different perceptual access to their shared environment. We seek to establish
how different temporal processes and representational choices impact Pavlovian
signalling between learning agents. To do so, we introduce a partially
observable decision-making domain we call the Frost Hollow. In this domain a
prediction learning agent and a reinforcement learning agent are coupled into a
two-part decision-making system that seeks to acquire sparse reward while
avoiding time-conditional hazards. We evaluate two domain variations: 1)
machine prediction and control learning in a linear walk, and 2) a prediction
learning machine interacting with a human participant in a virtual reality
environment. Our results showcase the speed of learning for Pavlovian
signalling, the impact that different temporal representations do (and do not)
have on agent-agent coordination, and how temporal aliasing impacts agent-agent
and human-agent interactions differently. As a main contribution, we establish
Pavlovian signalling as a natural bridge between fixed signalling paradigms and
fully adaptive communication learning. Our results therefore point to an
actionable, constructivist path towards continual communication learning
between reinforcement learning agents, with potential impact in a range of
real-world settings.",None,84402
Detection of Fights in Videos: A Comparison Study of Anomaly Detection and Action Recognition,0.308443,"Detection of fights is an important surveillance application in videos. Most
existing methods use supervised binary action recognition. Since frame-level
annotations are very hard to get for anomaly detection, weakly supervised
learning using multiple instance learning is widely used. This paper explores
the detection of fights in videos as one special type of anomaly detection and
as binary action recognition. We use the UBI-Fight and NTU-CCTV-Fight datasets
for most of the study since they have frame-level annotations. We find that the
anomaly detection has similar or even better performance than the action
recognition. Furthermore, we study to use anomaly detection as a toolbox to
generate training datasets for action recognition in an iterative way
conditioned on the performance of the anomaly detection. Experiment results
should show that we achieve state-of-the-art performance on three fight
detection datasets.",None,-1
A method for ethical AI in Defence: A case study on developing trustworthy autonomous systems,0.879198,"What does it mean to be responsible and responsive when developing and
deploying trusted autonomous systems in Defence? In this short reflective
article, we describe a case study of building a trusted autonomous system -
Athena AI - within an industry-led, government-funded project with diverse
collaborators and stakeholders. Using this case study, we draw out lessons on
the value and impact of embedding responsible research and innovation-aligned,
ethics-by-design approaches and principles throughout the development of
technology at high translation readiness levels.",None,-1
Mildly Conservative Q-Learning for Offline Reinforcement Learning,0.854359,"Offline reinforcement learning (RL) defines the task of learning from a
static logged dataset without continually interacting with the environment. The
distribution shift between the learned policy and the behavior policy makes it
necessary for the value function to stay conservative such that
out-of-distribution (OOD) actions will not be severely overestimated. However,
existing approaches, penalizing the unseen actions or regularizing with the
behavior policy, are too pessimistic, which suppresses the generalization of
the value function and hinders the performance improvement. This paper explores
mild but enough conservatism for offline learning while not harming
generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD
actions are actively trained by assigning them proper pseudo Q values. We
theoretically show that MCQ induces a policy that behaves at least as well as
the behavior policy and no erroneous overestimation will occur for OOD actions.
Experimental results on the D4RL benchmarks demonstrate that MCQ achieves
remarkable performance compared with prior work. Furthermore, MCQ shows
superior generalization ability when transferring from offline to online, and
significantly outperforms baselines. Our code is publicly available at
https://github.com/dmksjfl/MCQ.",https://github.com/dmksjfl/MCQ,-1
MABEL: Attenuating Gender Bias using Textual Entailment Data,0.651926,"Pre-trained language models encode undesirable social biases, which are
further exacerbated in downstream use. To this end, we propose MABEL (a Method
for Attenuating Gender Bias using Entailment Labels), an intermediate
pre-training approach for mitigating gender bias in contextualized
representations. Key to our approach is the use of a contrastive learning
objective on counterfactually augmented, gender-balanced entailment pairs from
natural language inference (NLI) datasets. We also introduce an alignment
regularizer that pulls identical entailment pairs along opposite gender
directions closer. We extensively evaluate our approach on intrinsic and
extrinsic metrics, and show that MABEL outperforms previous task-agnostic
debiasing approaches in terms of fairness. It also preserves task performance
after fine-tuning on downstream tasks. Together, these findings demonstrate the
suitability of NLI data as an effective means of bias mitigation, as opposed to
only using unlabeled sentences in the literature. Finally, we identify that
existing approaches often use evaluation settings that are insufficient or
inconsistent. We make an effort to reproduce and compare previous methods, and
call for unifying the evaluation settings across gender debiasing methods for
better future comparison.",https://github.com/princeton-nlp/MABEL,-1
Multimodal Image Fusion based on Hybrid CNN-Transformer and Non-local Cross-modal Attention,0.0450109,"The fusion of images taken by heterogeneous sensors helps to enrich the
information and improve the quality of imaging. In this article, we present a
hybrid model consisting of a convolutional encoder and a Transformer-based
decoder to fuse multimodal images. In the encoder, a non-local cross-modal
attention block is proposed to capture both local and global dependencies of
multiple source images. A branch fusion module is designed to adaptively fuse
the features of the two branches. We embed a Transformer module with linear
complexity in the decoder to enhance the reconstruction capability of the
proposed network. Qualitative and quantitative experiments demonstrate the
effectiveness of the proposed method by comparing it with existing
state-of-the-art fusion models. The source code of our work is available at
https://github.com/pandayuanyu/HCFusion.",https://github.com/pandayuanyu/HCFusion,4962
Coupling User Preference with External Rewards to Enable Driver-centered and Resource-aware EV Charging Recommendation,0.32696,"Electric Vehicle (EV) charging recommendation that both accommodates user
preference and adapts to the ever-changing external environment arises as a
cost-effective strategy to alleviate the range anxiety of private EV drivers.
Previous studies focus on centralized strategies to achieve optimized resource
allocation, particularly useful for privacy-indifferent taxi fleets and
fixed-route public transits. However, private EV driver seeks a more
personalized and resource-aware charging recommendation that is tailor-made to
accommodate the user preference (when and where to charge) yet sufficiently
adaptive to the spatiotemporal mismatch between charging supply and demand.
Here we propose a novel Regularized Actor-Critic (RAC) charging recommendation
approach that would allow each EV driver to strike an optimal balance between
the user preference (historical charging pattern) and the external reward
(driving distance and wait time). Experimental results on two real-world
datasets demonstrate the unique features and superior performance of our
approach to the competing methods.",https://github.com/cyli2019/RAC-for-EV-Charging-Rec.,-1
MetaASSIST: Robust Dialogue State Tracking with Meta Learning,0.22356,"Existing dialogue datasets contain lots of noise in their state annotations.
Such noise can hurt model training and ultimately lead to poor generalization
performance. A general framework named ASSIST has recently been proposed to
train robust dialogue state tracking (DST) models. It introduces an auxiliary
model to generate pseudo labels for the noisy training set. These pseudo labels
are combined with vanilla labels by a common fixed weighting parameter to train
the primary DST model. Notwithstanding the improvements of ASSIST on DST,
tuning the weighting parameter is challenging. Moreover, a single parameter
shared by all slots and all instances may be suboptimal. To overcome these
limitations, we propose a meta learning-based framework MetaASSIST to
adaptively learn the weighting parameter. Specifically, we propose three
schemes with varying degrees of flexibility, ranging from slot-wise to both
slot-wise and instance-wise, to convert the weighting parameter into learnable
functions. These functions are trained in a meta-learning manner by taking the
validation set as meta data. Experimental results demonstrate that all three
schemes can achieve competitive performance. Most impressively, we achieve a
state-of-the-art joint goal accuracy of 80.10% on MultiWOZ 2.4.",https://github.com/smartyfh/,-1
Emotion detection of social data: APIs comparative study,0.368082,"The development of emotion detection technology has emerged as a highly
valuable possibility in the corporate sector due to the nearly limitless uses
of this new discipline, particularly with the unceasing propagation of social
data. In recent years, the electronic marketplace has witnessed the
establishment of a large number of start-up businesses with an almost sole
focus on building new commercial and open-source tools and APIs for emotion
detection and recognition. Yet, these tools and APIs must be continuously
reviewed and evaluated, and their performances should be reported and
discussed. There is a lack of research to empirically compare current emotion
detection technologies in terms of the results obtained from each model using
the same textual dataset. Also, there is a lack of comparative studies that
apply benchmark comparison to social data. This study compares eight
technologies; IBM Watson NLU, ParallelDots, Symanto-Ekman, Crystalfeel, Text to
Emotion, Senpy, Textprobe, and NLP Cloud. The comparison was undertaken using
two different datasets. The emotions from the chosen datasets were then derived
using the incorporated APIs. The performance of these APIs was assessed using
the aggregated scores that they delivered as well as the theoretically proven
evaluation metrics such as the micro-average of accuracy, classification error,
precision, recall, and f1-score. Lastly, the assessment of these APIs
incorporating the evaluation measures is reported and discussed.",None,-1
Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds,0.362814,"Despite impressive successes, deep reinforcement learning (RL) systems still
fall short of human performance on generalization to new tasks and environments
that differ from their training. As a benchmark tailored for studying RL
generalization, we introduce Avalon, a set of tasks in which embodied agents in
highly diverse procedural 3D worlds must survive by navigating terrain, hunting
or gathering food, and avoiding hazards. Avalon is unique among existing RL
benchmarks in that the reward function, world dynamics, and action space are
the same for every task, with tasks differentiated solely by altering the
environment; its 20 tasks, ranging in complexity from eat and throw to hunt and
navigate, each create worlds in which the agent must perform specific skills in
order to survive. This setup enables investigations of generalization within
tasks, between tasks, and to compositional tasks that require combining skills
learned from previous tasks. Avalon includes a highly efficient simulator, a
library of baselines, and a benchmark with scoring metrics evaluated against
hundreds of hours of human performance, all of which are open-source and
publicly available. We find that standard RL baselines make progress on most
tasks but are still far from human performance, suggesting Avalon is
challenging enough to advance the quest for generalizable RL.",https://generallyintelligent.com/avalon,-1
"BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer",0.0918965,"Memes are prevalent on the internet and continue to grow and evolve alongside
our culture. An automatic understanding of memes propagating on the internet
can shed light on the general sentiment and cultural attitudes of people. In
this work, we present team BLUE's solution for the second edition of the
MEMOTION shared task. We showcase two approaches for meme classification (i.e.
sentiment, humour, offensive, sarcasm and motivation levels) using a text-only
method using BERT, and a Multi-Modal-Multi-Task transformer network that
operates on both the meme image and its caption to output the final scores. In
both approaches, we leverage state-of-the-art pretrained models for text (BERT,
Sentence Transformer) and image processing (EfficientNetV4, CLIP). Through our
efforts, we obtain first place in task A, second place in task B and third
place in task C. In addition, our team obtained the highest average score for
all three tasks.",None,-1
Creativity in translation: machine translation as a constraint for literary texts,0.730876,"This article presents the results of a study involving the translation of a
short story by Kurt Vonnegut from English to Catalan and Dutch using three
modalities: machine-translation (MT), post-editing (PE) and translation without
aid (HT). Our aim is to explore creativity, understood to involve novelty and
acceptability, from a quantitative perspective. The results show that HT has
the highest creativity score, followed by PE, and lastly, MT, and this is
unanimous from all reviewers. A neural MT system trained on literary data does
not currently have the necessary capabilities for a creative translation; it
renders literal solutions to translation problems. More importantly, using MT
to post-edit raw output constrains the creativity of translators, resulting in
a poorer translation often not fit for publication, according to experts.",None,-1
TCE at Qur'an QA 2022: Arabic Language Question Answering Over Holy Qur'an Using a Post-Processed Ensemble of BERT-based Models,0.224954,"In recent years, we witnessed great progress in different tasks of natural
language understanding using machine learning. Question answering is one of
these tasks which is used by search engines and social media platforms for
improved user experience. Arabic is the language of the Holy Qur'an; the sacred
text for 1.8 billion people across the world. Arabic is a challenging language
for Natural Language Processing (NLP) due to its complex structures. In this
article, we describe our attempts at OSACT5 Qur'an QA 2022 Shared Task, which
is a question answering challenge on the Holy Qur'an in Arabic. We propose an
ensemble learning model based on Arabic variants of BERT models. In addition,
we perform post-processing to enhance the model predictions. Our system
achieves a Partial Reciprocal Rank (pRR) score of 56.6% on the official test
set.",https://github.com/mohammed-elkomy/quran-qa,-1
PSP-HDRI$+$: A Synthetic Dataset Generator for Pre-Training of Human-Centric Computer Vision Models,0.733332,"We introduce a new synthetic data generator PSP-HDRI$+$ that proves to be a
superior pre-training alternative to ImageNet and other large-scale synthetic
data counterparts. We demonstrate that pre-training with our synthetic data
will yield a more general model that performs better than alternatives even
when tested on out-of-distribution (OOD) sets. Furthermore, using ablation
studies guided by person keypoint estimation metrics with an off-the-shelf
model architecture, we show how to manipulate our synthetic data generator to
further improve model performance.",https://github.com/Unity-Technologies/PeopleSansPeople,-1
Towards customizable reinforcement learning agents: Enabling preference specification through online vocabulary expansion,0.291826,"There is a growing interest in developing automated agents that can work
alongside humans. In addition to completing the assigned task, such an agent
will undoubtedly be expected to behave in a manner that is preferred by the
human. This requires the human to communicate their preferences to the agent.
To achieve this, the current approaches either require the users to specify the
reward function or the preference is interactively learned from queries that
ask the user to compare behavior. The former approach can be challenging if the
internal representation used by the agent is inscrutable to the human while the
latter is unnecessarily cumbersome for the user if their preference can be
specified more easily in symbolic terms. In this work, we propose PRESCA
(PREference Specification through Concept Acquisition), a system that allows
users to specify their preferences in terms of concepts that they understand.
PRESCA maintains a set of such concepts in a shared vocabulary. If the relevant
concept is not in the shared vocabulary, then it is learned. To make learning a
new concept more feedback efficient, PRESCA leverages causal associations
between the target concept and concepts that are already known. In addition, we
use a novel data augmentation approach to further reduce required feedback. We
evaluate PRESCA by using it on a Minecraft environment and show that it can
effectively align the agent with the user's preference.",None,17546
BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach,0.99964,"Bilevel optimization (BO) is useful for solving a variety of important
machine learning problems including but not limited to hyperparameter
optimization, meta-learning, continual learning, and reinforcement learning.
Conventional BO methods need to differentiate through the low-level
optimization process with implicit differentiation, which requires expensive
calculations related to the Hessian matrix. There has been a recent quest for
first-order methods for BO, but the methods proposed to date tend to be
complicated and impractical for large-scale deep learning applications. In this
work, we propose a simple first-order BO algorithm that depends only on
first-order gradient information, requires no implicit differentiation, and is
practical and efficient for large-scale non-convex functions in deep learning.
We provide non-asymptotic convergence analysis of the proposed method to
stationary points for non-convex objectives and present empirical results that
show its superior practical performance.",https://github.com/JunjieYang97/stocBiO,-1
A Model of Anaphoric Ambiguities using Sheaf Theoretic Quantum-like Contextuality and BERT,0.25823,"Ambiguities of natural language do not preclude us from using it and context
helps in getting ideas across. They, nonetheless, pose a key challenge to the
development of competent machines to understand natural language and use it as
humans do. Contextuality is an unparalleled phenomenon in quantum mechanics,
where different mathematical formalisms have been put forwards to understand
and reason about it. In this paper, we construct a schema for anaphoric
ambiguities that exhibits quantum-like contextuality. We use a recently
developed criterion of sheaf-theoretic contextuality that is applicable to
signalling models. We then take advantage of the neural word embedding engine
BERT to instantiate the schema to natural language examples and extract
probability distributions for the instances. As a result, plenty of
sheaf-contextual examples were discovered in the natural language corpora BERT
utilises. Our hope is that these examples will pave the way for future research
and for finding ways to extend applications of quantum computing to natural
language processing.",None,-1
Lipschitz-constrained Unsupervised Skill Discovery,0.902285,"We study the problem of unsupervised skill discovery, whose goal is to learn
a set of diverse and useful skills with no external reward. There have been a
number of skill discovery methods based on maximizing the mutual information
(MI) between skills and states. However, we point out that their MI objectives
usually prefer static skills to dynamic ones, which may hinder the application
for downstream tasks. To address this issue, we propose Lipschitz-constrained
Skill Discovery (LSD), which encourages the agent to discover more diverse,
dynamic, and far-reaching skills. Another benefit of LSD is that its learned
representation function can be utilized for solving goal-following downstream
tasks even in a zero-shot manner - i.e., without further training or complex
planning. Through experiments on various MuJoCo robotic locomotion and
manipulation environments, we demonstrate that LSD outperforms previous
approaches in terms of skill diversity, state space coverage, and performance
on seven downstream tasks including the challenging task of following multiple
goals on Humanoid. Our code and videos are available at
https://shpark.me/projects/lsd/.",https://vision.snu.ac.kr/projects/lsd/,-1
Patch-wise Contrastive Style Learning for Instagram Filter Removal,0.811488,"Image-level corruptions and perturbations degrade the performance of CNNs on
different downstream vision tasks. Social media filters are one of the most
common resources of various corruptions and perturbations for real-world visual
analysis applications. The negative effects of these distractive factors can be
alleviated by recovering the original images with their pure style for the
inference of the downstream vision tasks. Assuming these filters substantially
inject a piece of additional style information to the social media images, we
can formulate the problem of recovering the original versions as a reverse
style transfer problem. We introduce Contrastive Instagram Filter Removal
Network (CIFR), which enhances this idea for Instagram filter removal by
employing a novel multi-layer patch-wise contrastive style learning mechanism.
Experiments show our proposed strategy produces better qualitative and
quantitative results than the previous studies. Moreover, we present the
results of our additional experiments for proposed architecture within
different settings. Finally, we present the inference outputs and quantitative
comparison of filtered and recovered images on localization and segmentation
tasks to encourage the main motivation for this problem.",https://github.com/birdortyedi/cifr-pytorch,-1
Robust Calibration with Multi-domain Temperature Scaling,0.918521,"Uncertainty quantification is essential for the reliable deployment of
machine learning models to high-stakes application domains. Uncertainty
quantification is all the more challenging when training distribution and test
distribution are different, even the distribution shifts are mild. Despite the
ubiquity of distribution shifts in real-world applications, existing
uncertainty quantification approaches mainly study the in-distribution setting
where the train and test distributions are the same. In this paper, we develop
a systematic calibration model to handle distribution shifts by leveraging data
from multiple domains. Our proposed method -- multi-domain temperature scaling
-- uses the heterogeneity in the domains to improve calibration robustness
under distribution shift. Through experiments on three benchmark data sets, we
find our proposed method outperforms existing methods as measured on both
in-distribution and out-of-distribution test sets.",None,-1
Constrained Dynamic Movement Primitives for Safe Learning of Motor Skills,0.744271,"Dynamic movement primitives are widely used for learning skills which can be
demonstrated to a robot by a skilled human or controller. While their
generalization capabilities and simple formulation make them very appealing to
use, they possess no strong guarantees to satisfy operational safety
constraints for a task. In this paper, we present constrained dynamic movement
primitives (CDMP) which can allow for constraint satisfaction in the robot
workspace. We present a formulation of a non-linear optimization to perturb the
DMP forcing weights regressed by locally-weighted regression to admit a Zeroing
Barrier Function (ZBF), which certifies workspace constraint satisfaction. We
demonstrate the proposed CDMP under different constraints on the end-effector
movement such as obstacle avoidance and workspace constraints on a physical
robot. A video showing the implementation of the proposed algorithm using
different manipulators in different environments could be found here
https://youtu.be/hJegJJkJfys.",None,-1
Reweighting Strategy based on Synthetic Data Identification for Sentence Similarity,0.0411565,"Semantically meaningful sentence embeddings are important for numerous tasks
in natural language processing. To obtain such embeddings, recent studies
explored the idea of utilizing synthetically generated data from pretrained
language models (PLMs) as a training corpus. However, PLMs often generate
sentences much different from the ones written by human. We hypothesize that
treating all these synthetic examples equally for training deep neural networks
can have an adverse effect on learning semantically meaningful embeddings. To
analyze this, we first train a classifier that identifies machine-written
sentences, and observe that the linguistic features of the sentences identified
as written by a machine are significantly different from those of human-written
sentences. Based on this, we propose a novel approach that first trains the
classifier to measure the importance of each sentence. The distilled
information from the classifier is then used to train a reliable sentence
embedding model. Through extensive evaluation on four real-world datasets, we
demonstrate that our model trained on synthetic data generalizes well and
outperforms the existing baselines. Our implementation is publicly available at
https://github.com/ddehun/coling2022_reweighting_sts.",https://github.com/ddehun/coling2022_reweighting_sts,-1
Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings,0.668811,"Previous knowledge graph embedding approaches usually map entities to
representations and utilize score functions to predict the target entities, yet
they typically struggle to reason rare or emerging unseen entities. In this
paper, we propose kNN-KGE, a new knowledge graph embedding approach with
pre-trained language models, by linearly interpolating its entity distribution
with k-nearest neighbors. We compute the nearest neighbors based on the
distance in the entity embedding space from the knowledge store. Our approach
can allow rare or emerging entities to be memorized explicitly rather than
implicitly in model parameters. Experimental results demonstrate that our
approach can improve inductive and transductive link prediction results and
yield better performance for low-resource settings with only a few triples,
which might be easier to reason via explicit memory. Code is available at
https://github.com/zjunlp/KNN-KG.",https://github.com/zjunlp/KNN-KG,-1
BITE: Textual Backdoor Attacks with Iterative Trigger Injection,0.831134,"Backdoor attacks have become an emerging threat to NLP systems. By providing
poisoned training data, the adversary can embed a ""backdoor"" into the victim
model, which allows input instances satisfying certain textual patterns (e.g.,
containing a keyword) to be predicted as a target label of the adversary's
choice. In this paper, we demonstrate that it is possible to design a backdoor
attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a
high attack success rate). We propose BITE, a backdoor attack that poisons the
training data to establish strong correlations between the target label and a
set of ""trigger words"". These trigger words are iteratively identified and
injected into the target-label instances through natural word-level
perturbations. The poisoned training data instruct the victim model to predict
the target label on inputs containing trigger words, forming the backdoor.
Experiments on four text classification datasets show that our proposed attack
is significantly more effective than baseline methods while maintaining decent
stealthiness, raising alarm on the usage of untrusted training data. We further
propose a defense method named DeBITE based on potential trigger word removal,
which outperforms existing methods in defending against BITE and generalizes
well to handling other backdoor attacks.",https://github.com/INK-USC/BITE,-1
Learning Neural Radiance Fields from Multi-View Geometry,0.326213,"We present a framework, called MVG-NeRF, that combines classical Multi-View
Geometry algorithms and Neural Radiance Fields (NeRF) for image-based 3D
reconstruction. NeRF has revolutionized the field of implicit 3D
representations, mainly due to a differentiable volumetric rendering
formulation that enables high-quality and geometry-aware novel view synthesis.
However, the underlying geometry of the scene is not explicitly constrained
during training, thus leading to noisy and incorrect results when extracting a
mesh with marching cubes. To this end, we propose to leverage pixelwise depths
and normals from a classical 3D reconstruction pipeline as geometric priors to
guide NeRF optimization. Such priors are used as pseudo-ground truth during
training in order to improve the quality of the estimated underlying surface.
Moreover, each pixel is weighted by a confidence value based on the
forward-backward reprojection error for additional robustness. Experimental
results on real-world data demonstrate the effectiveness of this approach in
obtaining clean 3D meshes from images, while maintaining competitive
performances in novel view synthesis.",None,-1
A Close Look into the Calibration of Pre-trained Language Models,0.846799,"Pre-trained language models (PLMs) may fail in giving reliable estimates of
their predictive uncertainty. We take a close look into this problem, aiming to
answer two questions: (1) Do PLMs learn to become calibrated in the training
process? (2) How effective are existing calibration methods? For the first
question, we conduct fine-grained control experiments to study the dynamic
change in PLMs' calibration performance in training. We consider six factors as
control variables, including dataset difficulty, available training samples,
training steps, the number of tunable parameters, model scale, and pretraining.
We observe a consistent change in calibration performance across six factors.
We find that PLMs don't learn to become calibrated in training, evidenced by
the continual increase in confidence, no matter whether the predictions are
correct or not. We highlight that our finding somewhat contradicts two
established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining
improves model calibration. Next, we study the effectiveness of existing
calibration methods in mitigating the overconfidence issue. Besides unlearnable
calibration methods (e.g., label smoothing), we adapt and extend two recently
proposed learnable methods that directly collect data to train models to have
reasonable confidence estimations. Experimental results show that learnable
methods significantly reduce PLMs' confidence in wrong predictions. The code is
available at \url{https://github.com/lifan-yuan/PLMCalibration}.",https://github.com/lifan-yuan/PLMCalibration,-1
Synthesis of Stabilizing Recurrent Equilibrium Network Controllers,0.517235,"We propose a parameterization of a nonlinear dynamic controller based on the
recurrent equilibrium network, a generalization of the recurrent neural
network. We derive constraints on the parameterization under which the
controller guarantees exponential stability of a partially observed dynamical
system with sector bounded nonlinearities. Finally, we present a method to
synthesize this controller using projected policy gradient methods to maximize
a reward function with arbitrary structure. The projection step involves the
solution of convex optimization problems. We demonstrate the proposed method
with simulated examples of controlling nonlinear plants, including plants
modeled with neural networks.",https://github.com/neelayjunnarkar/stabilizing-ren,-1
JuryGCN: Quantifying Jackknife Uncertainty on Graph Convolutional Networks,0.831292,"Graph Convolutional Network (GCN) has exhibited strong empirical performance
in many real-world applications. The vast majority of existing works on GCN
primarily focus on the accuracy while ignoring how confident or uncertain a GCN
is with respect to its predictions. Despite being a cornerstone of trustworthy
graph mining, uncertainty quantification on GCN has not been well studied and
the scarce existing efforts either fail to provide deterministic quantification
or have to change the training procedure of GCN by introducing additional
parameters or architectures. In this paper, we propose the first
frequentist-based approach named JuryGCN in quantifying the uncertainty of GCN,
where the key idea is to quantify the uncertainty of a node as the width of
confidence interval by a jackknife estimator. Moreover, we leverage the
influence functions to estimate the change in GCN parameters without
re-training to scale up the computation. The proposed JuryGCN is capable of
quantifying uncertainty deterministically without modifying the GCN
architecture or introducing additional parameters. We perform extensive
experimental evaluation on real-world datasets in the tasks of both active
learning and semi-supervised node classification, which demonstrate the
efficacy of the proposed method.",None,-1
SmoothNets: Optimizing CNN architecture design for differentially private deep learning,0.298179,"The arguably most widely employed algorithm to train deep neural networks
with Differential Privacy is DPSGD, which requires clipping and noising of
per-sample gradients. This introduces a reduction in model utility compared to
non-private training. Empirically, it can be observed that this accuracy
degradation is strongly dependent on the model architecture. We investigated
this phenomenon and, by combining components which exhibit good individual
performance, distilled a new model architecture termed SmoothNet, which is
characterised by increased robustness to the challenges of DP-SGD training.
Experimentally, we benchmark SmoothNet against standard architectures on two
benchmark datasets and observe that our architecture outperforms others,
reaching an accuracy of 73.5\% on CIFAR-10 at $\varepsilon=7.0$ and 69.2\% at
$\varepsilon=7.0$ on ImageNette, a state-of-the-art result compared to prior
architectural modifications for DP.",https://github.com/fastai/imagenette,-1
Provable Safe Reinforcement Learning with Binary Feedback,0.221671,"Safety is a crucial necessity in many applications of reinforcement learning
(RL), whether robotic, automotive, or medical. Many existing approaches to safe
RL rely on receiving numeric safety feedback, but in many cases this feedback
can only take binary values; that is, whether an action in a given state is
safe or unsafe. This is particularly true when feedback comes from human
experts. We therefore consider the problem of provable safe RL when given
access to an offline oracle providing binary feedback on the safety of state,
action pairs. We provide a novel meta algorithm, SABRE, which can be applied to
any MDP setting given access to a blackbox PAC RL algorithm for that setting.
SABRE applies concepts from active learning to reinforcement learning to
provably control the number of queries to the safety oracle. SABRE works by
iteratively exploring the state space to find regions where the agent is
currently uncertain about safety. Our main theoretical results shows that,
under appropriate technical assumptions, SABRE never takes unsafe actions
during training, and is guaranteed to return a near-optimal safe policy with
high probability. We provide a discussion of how our meta-algorithm may be
applied to various settings studied in both theoretical and empirical
frameworks.",None,-1
POQue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events,0.126722,"Knowledge about outcomes is critical for complex event understanding but is
hard to acquire. We show that by pre-identifying a participant in a complex
event, crowd workers are able to (1) infer the collective impact of salient
events that make up the situation, (2) annotate the volitional engagement of
participants in causing the situation, and (3) ground the outcome of the
situation in state changes of the participants. By creating a multi-step
interface and a careful quality control strategy, we collect a high quality
annotated dataset of 8K short newswire narratives and ROCStories with high
inter-annotator agreement (0.74-0.96 weighted Fleiss Kappa). Our dataset, POQue
(Participant Outcome Questions), enables the exploration and development of
models that address multiple aspects of semantic understanding. Experimentally,
we show that current language models lag behind human performance in subtle
ways through our task formulations that target abstract and specific
comprehension of a complex event, its outcome, and a participant's influence
over the event culmination.",https://github.com/saiumbc/POQue,-1
Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations,0.342612,"Recent work on explainable NLP has shown that few-shot prompting can enable
large pretrained language models (LLMs) to generate grammatical and factual
natural language explanations for data labels. In this work, we study the
connection between explainability and sample hardness by investigating the
following research question - ""Are LLMs and humans equally good at explaining
data labels for both easy and hard samples?"" We answer this question by first
collecting human-written explanations in the form of generalizable commonsense
rules on the task of Winograd Schema Challenge (Winogrande dataset). We compare
these explanations with those generated by GPT-3 while varying the hardness of
the test samples as well as the in-context samples. We observe that (1) GPT-3
explanations are as grammatical as human explanations regardless of the
hardness of the test samples, (2) for easy examples, GPT-3 generates highly
supportive explanations but human explanations are more generalizable, and (3)
for hard examples, human explanations are significantly better than GPT-3
explanations both in terms of label-supportiveness and generalizability
judgements. We also find that hardness of the in-context examples impacts the
quality of GPT-3 explanations. Finally, we show that the supportiveness and
generalizability aspects of human explanations are also impacted by sample
hardness, although by a much smaller margin than models. Supporting code and
data are available at https://github.com/swarnaHub/ExplanationHardness",https://github.com/swarnaHub/ExplanationHardness,-1
"Early Recall, Late Precision: Multi-Robot Semantic Object Mapping under Operational Constraints in Perceptually-Degraded Environments",0.394469,"Semantic object mapping in uncertain, perceptually degraded environments
during long-range multi-robot autonomous exploration tasks such as
search-and-rescue is important and challenging. During such missions, high
recall is desirable to avoid missing true target objects and high precision is
also critical to avoid wasting valuable operational time on false positives.
Given recent advancements in visual perception algorithms, the former is
largely solvable autonomously, but the latter is difficult to address without
the supervision of a human operator. However, operational constraints such as
mission time, computational requirements, mesh network bandwidth and so on, can
make the operator's task infeasible unless properly managed. We propose the
Early Recall, Late Precision (EaRLaP) semantic object mapping pipeline to solve
this problem. EaRLaP was used by Team CoSTAR in DARPA Subterranean Challenge,
where it successfully detected all the artifacts encountered by the team of
robots. We will discuss these results and performance of the EaRLaP on various
datasets.",None,-1
Pre-training Language Models with Deterministic Factual Knowledge,0.408802,"Previous works show that Pre-trained Language Models (PLMs) can capture
factual knowledge. However, some analyses reveal that PLMs fail to perform it
robustly, e.g., being sensitive to the changes of prompts when extracting
factual knowledge. To mitigate this issue, we propose to let PLMs learn the
deterministic relationship between the remaining context and the masked
content. The deterministic relationship ensures that the masked factual content
can be deterministically inferable based on the existing clues in the context.
That would provide more stable patterns for PLMs to capture factual knowledge
than randomly masking. Two pre-training tasks are further introduced to
motivate PLMs to rely on the deterministic relationship when filling masks.
Specifically, we use an external Knowledge Base (KB) to identify deterministic
relationships and continuously pre-train PLMs with the proposed methods. The
factual knowledge probing experiments indicate that the continuously
pre-trained PLMs achieve better robustness in factual knowledge capturing.
Further experiments on question-answering datasets show that trying to learn a
deterministic relationship with the proposed methods can also help other
knowledge-intensive tasks.",https://github.com/informagi/REL,-1
Omnifont Persian OCR System Using Primitives,0.715982,"In this paper, we introduce a model-based omnifont Persian OCR system. The
system uses a set of 8 primitive elements as structural features for
recognition. First, the scanned document is preprocessed. After normalizing the
preprocessed image, text rows and sub-words are separated and then thinned.
After recognition of dots in sub-words, strokes are extracted and primitive
elements of each sub-word are recognized using the strokes. Finally, the
primitives are compared with a predefined set of character identification
vectors in order to identify sub-word characters. The separation and
recognition steps of the system are concurrent, eliminating unavoidable errors
of independent separation of letters. The system has been tested on documents
with 14 standard Persian fonts in 6 sizes. The achieved precision is 97.06%.",None,-1
A Large Scale Search Dataset for Unbiased Learning to Rank,0.910732,"The unbiased learning to rank (ULTR) problem has been greatly advanced by
recent deep learning techniques and well-designed debias algorithms. However,
promising results on the existing benchmark datasets may not be extended to the
practical scenario due to the following disadvantages observed from those
popular benchmark datasets: (1) outdated semantic feature extraction where
state-of-the-art large scale pre-trained language models like BERT cannot be
exploited due to the missing of the original text;(2) incomplete display
features for in-depth study of ULTR, e.g., missing the displayed abstract of
documents for analyzing the click necessary bias; (3) lacking real-world user
feedback, leading to the prevalence of synthetic datasets in the empirical
study. To overcome the above disadvantages, we introduce the Baidu-ULTR
dataset. It involves randomly sampled 1.2 billion searching sessions and 7,008
expert annotated queries, which is orders of magnitude larger than the existing
ones. Baidu-ULTR provides:(1) the original semantic feature and a pre-trained
language model for easy usage; (2) sufficient display information such as
position, displayed height, and displayed abstract, enabling the comprehensive
study of different biases with advanced techniques such as causal discovery and
meta-learning; and (3) rich user feedback on search result pages (SERPs) like
dwelling time, allowing for user engagement optimization and promoting the
exploration of multi-task learning in ULTR. In this paper, we present the
design principle of Baidu-ULTR and the performance of benchmark ULTR algorithms
on this new data resource, favoring the exploration of ranking for long-tail
queries and pre-training tasks for ranking. The Baidu-ULTR dataset and
corresponding baseline implementation are available at
https://github.com/ChuXiaokai/baidu_ultr_dataset.",https://github.com/ChuXiaokai/baidu_ultr_dataset,-1
Graph-based Extractive Explainer for Recommendations,0.187303,"Explanations in a recommender system assist users in making informed
decisions among a set of recommended items. Great research attention has been
devoted to generating natural language explanations to depict how the
recommendations are generated and why the users should pay attention to them.
However, due to different limitations of those solutions, e.g., template-based
or generation-based, it is hard to make the explanations easily perceivable,
reliable and personalized at the same time.
  In this work, we develop a graph attentive neural network model that
seamlessly integrates user, item, attributes, and sentences for
extraction-based explanation. The attributes of items are selected as the
intermediary to facilitate message passing for user-item specific evaluation of
sentence relevance. And to balance individual sentence relevance, overall
attribute coverage, and content redundancy, we solve an integer linear
programming problem to make the final selection of sentences. Extensive
empirical evaluations against a set of state-of-the-art baseline methods on two
benchmark review datasets demonstrated the generation quality of the proposed
solution.",None,-1
Nearest Neighbor Non-autoregressive Text Generation,0.159313,"Non-autoregressive (NAR) models can generate sentences with less computation
than autoregressive models but sacrifice generation quality. Previous studies
addressed this issue through iterative decoding. This study proposes using
nearest neighbors as the initial state of an NAR decoder and editing them
iteratively. We present a novel training strategy to learn the edit operations
on neighbors to improve NAR text generation. Experimental results show that the
proposed method (NeighborEdit) achieves higher translation quality (1.69 points
higher than the vanilla Transformer) with fewer decoding iterations
(one-eighteenth fewer iterations) on the JRC-Acquis En-De dataset, the common
benchmark dataset for machine translation using nearest neighbors. We also
confirm the effectiveness of the proposed method on a data-to-text task
(WikiBio). In addition, the proposed method outperforms an NAR baseline on the
WMT'14 En-De dataset. We also report analysis on neighbor examples used in the
proposed method.",https://github.com/pytorch/fairseq,-1
WebtoonMe: A Data-Centric Approach for Full-Body Portrait Stylization,0.864665,"Full-body portrait stylization, which aims to translate portrait photography
into a cartoon style, has drawn attention recently. However, most methods have
focused only on converting face regions, restraining the feasibility of use in
real-world applications. A recently proposed two-stage method expands the
rendering area to full bodies, but the outputs are less plausible and fail to
achieve quality robustness of non-face regions. Furthermore, they cannot
reflect diverse skin tones. In this study, we propose a data-centric solution
to build a production-level full-body portrait stylization system. Based on the
two-stage scheme, we construct a novel and advanced dataset preparation
paradigm that can effectively resolve the aforementioned problems. Experiments
reveal that with our pipeline, high-quality portrait stylization can be
achieved without additional losses or architectural changes.",https://github.com/Sxela/ArcaneGAN,-1
BERT for Long Documents: A Case Study of Automated ICD Coding,0.116262,"Transformer models have achieved great success across many NLP problems.
However, previous studies in automated ICD coding concluded that these models
fail to outperform some of the earlier solutions such as CNN-based models. In
this paper we challenge this conclusion. We present a simple and scalable
method to process long text with the existing transformer models such as BERT.
We show that this method significantly improves the previous results reported
for transformer models in ICD coding, and is able to outperform one of the
prominent CNN-based methods.",None,-1
TCTrack: Temporal Contexts for Aerial Tracking,0.991593,"Temporal contexts among consecutive frames are far from being fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier.",https://github.com/vision4robotics/TCTrack,-1
Does Simultaneous Speech Translation need Simultaneous Models?,0.793159,"In simultaneous speech translation (SimulST), finding the best trade-off
between high translation quality and low latency is a challenging task. To meet
the latency constraints posed by the different application scenarios, multiple
dedicated SimulST models are usually trained and maintained, generating high
computational costs. In this paper, motivated by the increased social and
environmental impact caused by these costs, we investigate whether a single
model trained offline can serve not only the offline but also the simultaneous
task without the need for any additional training or adaptation. Experiments on
en->{de, es} indicate that, aside from facilitating the adoption of
well-established offline techniques and architectures without affecting
latency, the offline solution achieves similar or better translation quality
compared to the same model trained in simultaneous settings, as well as being
competitive with the SimulST state of the art.",https://github.com/hlt-mt/,9038
SkillNet-NLG: General-Purpose Natural Language Generation with a Sparsely Activated Approach,0.164437,"We present SkillNet-NLG, a sparsely activated approach that handles many
natural language generation tasks with one model. Different from traditional
dense models that always activate all the parameters, SkillNet-NLG selectively
activates relevant parts of the parameters to accomplish a task, where the
relevance is controlled by a set of predefined skills. The strength of such
model design is that it provides an opportunity to precisely adapt relevant
skills to learn new tasks effectively. We evaluate on Chinese natural language
generation tasks. Results show that, with only one model file, SkillNet-NLG
outperforms previous best performance methods on four of five tasks.
SkillNet-NLG performs better than two multi-task learning baselines (a dense
model and a Mixture-of-Expert model) and achieves comparable performance to
task-specific models. Lastly, SkillNet-NLG surpasses baseline systems when
being adapted to new tasks.",https://github.com/huggingface/transformers,-1
DigNet: Digging Clues from Local-Global Interactive Graph for Aspect-level Sentiment Classification,0.746621,"In aspect-level sentiment classification (ASC), state-of-the-art models
encode either syntax graph or relation graph to capture the local syntactic
information or global relational information. Despite the advantages of syntax
and relation graphs, they have respective shortages which are neglected,
limiting the representation power in the graph modeling process. To resolve
their limitations, we design a novel local-global interactive graph, which
marries their advantages by stitching the two graphs via interactive edges. To
model this local-global interactive graph, we propose a novel neural network
termed DigNet, whose core module is the stacked local-global interactive (LGI)
layers performing two processes: intra-graph message passing and cross-graph
message passing. In this way, the local syntactic and global relational
information can be reconciled as a whole in understanding the aspect-level
sentiment. Concretely, we design two variants of local-global interactive
graphs with different kinds of interactive edges and three variants of LGI
layers. We conduct experiments on several public benchmark datasets and the
results show that we outperform previous best scores by 3\%, 2.32\%, and 6.33\%
in terms of Macro-F1 on Lap14, Res14, and Res15 datasets, respectively,
confirming the effectiveness and superiority of the proposed local-global
interactive graph and DigNet.",None,-1
Are Current Task-oriented Dialogue Systems Able to Satisfy Impolite Users?,0.383945,"Task-oriented dialogue (TOD) systems have assisted users on many tasks,
including ticket booking and service inquiries. While existing TOD systems have
shown promising performance in serving customer needs, these systems mostly
assume that users would interact with the dialogue agent politely. This
assumption is unrealistic as impatient or frustrated customers may also
interact with TOD systems impolitely. This paper aims to address this research
gap by investigating impolite users' effects on TOD systems. Specifically, we
constructed an impolite dialogue corpus and conducted extensive experiments to
evaluate the state-of-the-art TOD systems on our impolite dialogue corpus. Our
experimental results show that existing TOD systems are unable to handle
impolite user utterances. We also present a data augmentation method to improve
TOD performance in impolite dialogues. Nevertheless, handling impolite
dialogues remains a very challenging research task. We hope by releasing the
impolite dialogue corpus and establishing the benchmark evaluations, more
researchers are encouraged to investigate this new challenging research task.",None,-1
BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,1.0,"We present BlenderBot 3, a 175B parameter dialogue model capable of
open-domain conversation with access to the internet and a long-term memory,
and having been trained on a large number of user defined tasks. We release
both the model weights and code, and have also deployed the model on a public
web page to interact with organic users. This technical report describes how
the model was built (architecture, model and training scheme), and details of
its deployment, including safety mechanisms. Human evaluations show its
superiority to existing open-domain dialogue agents, including its predecessors
(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for
continual learning using the data collected from deployment, which will also be
publicly released. The goal of this research program is thus to enable the
community to study ever-improving responsible agents that learn through
interaction.",https://www.parl.ai/projects/bb3,-1
Searching for Structure in Unfalsifiable Claims,0.161338,"Social media platforms give rise to an abundance of posts and comments on
every topic imaginable. Many of these posts express opinions on various aspects
of society, but their unfalsifiable nature makes them ill-suited to
fact-checking pipelines. In this work, we aim to distill such posts into a
small set of narratives that capture the essential claims related to a given
topic. Understanding and visualizing these narratives can facilitate more
informed debates on social media. As a first step towards systematically
identifying the underlying narratives on social media, we introduce PAPYER, a
fine-grained dataset of online comments related to hygiene in public restrooms,
which contains a multitude of unfalsifiable claims. We present a
human-in-the-loop pipeline that uses a combination of machine and human kernels
to discover the prevailing narratives and show that this pipeline outperforms
recent large transformer models and state-of-the-art unsupervised topic models.",None,-1
Gradient-Guided Importance Sampling for Learning Binary Energy-Based Models,0.864665,"Learning energy-based models (EBMs) is known to be difficult especially on
discrete data where gradient-based learning strategies cannot be applied
directly. Although ratio matching is a sound method to learn discrete EBMs, it
suffers from expensive computation and excessive memory requirements, thereby
resulting in difficulties in learning EBMs on high-dimensional data. Motivated
by these limitations, in this study, we propose ratio matching with
gradient-guided importance sampling (RMwGGIS). Particularly, we use the
gradient of the energy function w.r.t. the discrete data space to approximately
construct the provably optimal proposal distribution, which is subsequently
used by importance sampling to efficiently estimate the original ratio matching
objective. We perform experiments on density modeling over synthetic discrete
data, graph generation, and training Ising models to evaluate our proposed
method. The experimental results demonstrate that our method can significantly
alleviate the limitations of ratio matching, perform more effectively in
practice, and scale to high-dimensional problems. Our implementation is
available at https://github.com/divelab/RMwGGIS.",https://github.com/divelab/RMwGGIS,-1
Deep Neural Patchworks: Coping with Large Segmentation Tasks,0.117645,"Convolutional neural networks are the way to solve arbitrary image
segmentation tasks. However, when images are large, memory demands often exceed
the available resources, in particular on a common GPU. Especially in
biomedical imaging, where 3D images are common, the problems are apparent. A
typical approach to solve this limitation is to break the task into smaller
subtasks by dividing images into smaller image patches. Another approach, if
applicable, is to look at the 2D image sections separately, and to solve the
problem in 2D. Often, the loss of global context makes such approaches less
effective; important global information might not be present in the current
image patch, or the selected 2D image section. Here, we propose Deep Neural
Patchworks (DNP), a segmentation framework that is based on hierarchical and
nested stacking of patch-based networks that solves the dilemma between global
context and memory limitations.",https://bitbucket.org/reisert/patchwork/wiki/Home,-1
DARER: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition,0.827567,"The task of joint dialog sentiment classification (DSC) and act recognition
(DAR) aims to simultaneously predict the sentiment label and act label for each
utterance in a dialog. In this paper, we put forward a new framework which
models the explicit dependencies via integrating \textit{prediction-level
interactions} other than semantics-level interactions, more consistent with
human intuition. Besides, we propose a speaker-aware temporal graph (SATG) and
a dual-task relational temporal graph (DRTG) to introduce \textit{temporal
relations} into dialog understanding and dual-task reasoning. To implement our
framework, we propose a novel model dubbed DARER, which first generates the
context-, speaker- and temporal-sensitive utterance representations via
modeling SATG, then conducts recurrent dual-task relational reasoning on DRTG,
in which process the estimated label distributions act as key clues in
prediction-level interactions. Experiment results show that DARER outperforms
existing models by large margins while requiring much less computation resource
and costing less training time. Remarkably, on DSC task in Mastodon, DARER
gains a relative improvement of about 25% over previous best model in terms of
F1, with less than 50% parameters and about only 60% required GPU memory.",https://github.com/XingBowen714/DARER,-1
COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning,0.930748,"Graph contrastive learning (GCL) improves graph representation learning,
leading to SOTA on various downstream tasks. The graph augmentation step is a
vital but scarcely studied step of GCL. In this paper, we show that the node
embedding obtained via the graph augmentations is highly biased, somewhat
limiting contrastive models from learning discriminative features for
downstream tasks. Thus, instead of investigating graph augmentation in the
input space, we alternatively propose to perform augmentations on the hidden
features (feature augmentation). Inspired by so-called matrix sketching, we
propose COSTA, a novel COvariance-preServing feaTure space Augmentation
framework for GCL, which generates augmented features by maintaining a ""good
sketch"" of original features. To highlight the superiority of feature
augmentation with COSTA, we investigate a single-view setting (in addition to
multi-view one) which conserves memory and computations. We show that the
feature augmentation with COSTA achieves comparable/better results than graph
augmentation based models.",None,-1
SimA: Simple Softmax-free Attention for Vision Transformers,0.23239,"Recently, vision transformers have become very popular. However, deploying
them in many applications is computationally expensive partly due to the
Softmax layer in the attention block. We introduce a simple but effective,
Softmax-free attention block, SimA, which normalizes query and key matrices
with simple $\ell_1$-norm instead of using Softmax layer. Then, the attention
block in SimA is a simple multiplication of three matrices, so SimA can
dynamically change the ordering of the computation at the test time to achieve
linear computation on the number of tokens or the number of channels. We
empirically show that SimA applied to three SOTA variations of transformers,
DeiT, XCiT, and CvT, results in on-par accuracy compared to the SOTA models,
without any need for Softmax layer. Interestingly, changing SimA from
multi-head to single-head has only a small effect on the accuracy, which
simplifies the attention block further. The code is available here:
https://github.com/UCDvision/sima",https://github.com/UCDvision/sima,-1
Feedback Gradient Descent: Efficient and Stable Optimization with Orthogonality for DNNs,0.596777,"The optimization with orthogonality has been shown useful in training deep
neural networks (DNNs). To impose orthogonality on DNNs, both computational
efficiency and stability are important. However, existing methods utilizing
Riemannian optimization or hard constraints can only ensure stability while
those using soft constraints can only improve efficiency. In this paper, we
propose a novel method, named Feedback Gradient Descent (FGD), to our
knowledge, the first work showing high efficiency and stability simultaneously.
FGD induces orthogonality based on the simple yet indispensable Euler
discretization of a continuous-time dynamical system on the tangent bundle of
the Stiefel manifold. In particular, inspired by a numerical integration method
on manifolds called Feedback Integrators, we propose to instantiate it on the
tangent bundle of the Stiefel manifold for the first time. In the extensive
image classification experiments, FGD comprehensively outperforms the existing
state-of-the-art methods in terms of accuracy, efficiency, and stability.",None,-1
Extending Logic Explained Networks to Text Classification,0.266963,"Recently, Logic Explained Networks (LENs) have been proposed as
explainable-by-design neural models providing logic explanations for their
predictions. However, these models have only been applied to vision and tabular
data, and they mostly favour the generation of global explanations, while local
ones tend to be noisy and verbose. For these reasons, we propose LENp,
improving local explanations by perturbing input words, and we test it on text
classification. Our results show that (i) LENp provides better local
explanations than LIME in terms of sensitivity and faithfulness, and (ii) logic
explanations are more useful and user-friendly than feature scoring provided by
LIME as attested by a human survey.",None,-1
Machine Learning-Based User Scheduling in Integrated Satellite-HAPS-Ground Networks,0.339288,"Integrated space-air-ground networks promise to offer a valuable solution
space for empowering the sixth generation of communication networks (6G),
particularly in the context of connecting the unconnected and ultraconnecting
the connected. Such digital inclusion thrive makes resource management
problems, especially those accounting for load-balancing considerations, of
particular interest. The conventional model-based optimization methods,
however, often fail to meet the real-time processing and quality-of-service
needs, due to the high heterogeneity of the space-air-ground networks, and the
typical complexity of the classical algorithms. Given the premises of
artificial intelligence at automating wireless networks design and the
large-scale heterogeneity of non-terrestrial networks, this paper focuses on
showcasing the prospects of machine learning in the context of user scheduling
in integrated space-air-ground communications. The paper first overviews the
most relevant state-of-the art in the context of machine learning applications
to the resource allocation problems, with a dedicated attention to
space-air-ground networks. The paper then proposes, and shows the benefit of,
one specific use case that uses ensembling deep neural networks for optimizing
the user scheduling policies in integrated space-high altitude platform station
(HAPS)-ground networks. Finally, the paper sheds light on the challenges and
open issues that promise to spur the integration of machine learning in
space-air-ground networks, namely, online HAPS power adaptation, learning-based
channel sensing, data-driven multi-HAPSs resource management, and intelligent
flying taxis-empowered systems.",None,-1
The Problem of Semantic Shift in Longitudinal Monitoring of Social Media: A Case Study on Mental Health During the COVID-19 Pandemic,0.0454806,"Social media allows researchers to track societal and cultural changes over
time based on language analysis tools. Many of these tools rely on statistical
algorithms which need to be tuned to specific types of language. Recent studies
have shown the absence of appropriate tuning, specifically in the presence of
semantic shift, can hinder robustness of the underlying methods. However,
little is known about the practical effect this sensitivity may have on
downstream longitudinal analyses. We explore this gap in the literature through
a timely case study: understanding shifts in depression during the course of
the COVID-19 pandemic. We find that inclusion of only a small number of
semantically-unstable features can promote significant changes in longitudinal
estimates of our target outcome. At the same time, we demonstrate that a
recently-introduced method for measuring semantic shift may be used to
proactively identify failure points of language-based models and, in turn,
improve predictive generalization.",https://github.com/kharrigian/semantic-shift-websci-2022,-1
Teaching Where to Look: Attention Similarity Knowledge Distillation for Low Resolution Face Recognition,0.817373,"Deep learning has achieved outstanding performance for face recognition
benchmarks, but performance reduces significantly for low resolution (LR)
images. We propose an attention similarity knowledge distillation approach,
which transfers attention maps obtained from a high resolution (HR) network as
a teacher into an LR network as a student to boost LR recognition performance.
Inspired by humans being able to approximate an object's region from an LR
image based on prior knowledge obtained from HR images, we designed the
knowledge distillation loss using the cosine similarity to make the student
network's attention resemble the teacher network's attention. Experiments on
various LR face related benchmarks confirmed the proposed method generally
improved recognition performances on LR settings, outperforming
state-of-the-art results by simply transferring well-constructed attention
maps. The code and pretrained models are publicly available in the
https://github.com/gist-ailab/teaching-where-to-look.",https://github.com/gist-ailab/teaching-where-to-look,-1
Semantic features of object concepts generated with GPT-3,0.325566,"Semantic features have been playing a central role in investigating the
nature of our conceptual representations. Yet the enormous time and effort
required to empirically sample and norm features from human raters has
restricted their use to a limited set of manually curated concepts. Given
recent promising developments with transformer-based language models, here we
asked whether it was possible to use such models to automatically generate
meaningful lists of properties for arbitrary object concepts and whether these
models would produce features similar to those found in humans. To this end, we
probed a GPT-3 model to generate semantic features for 1,854 objects and
compared automatically-generated features to existing human feature norms.
GPT-3 generated many more features than humans, yet showed a similar
distribution in the types of generated features. Generated feature norms
rivaled human norms in predicting similarity, relatedness, and category
membership, while variance partitioning demonstrated that these predictions
were driven by similar variance in humans and GPT-3. Together, these results
highlight the potential of large language models to capture important facets of
human knowledge and yield a new approach for automatically generating
interpretable feature sets, thus drastically expanding the potential use of
semantic features in psychological and linguistic studies.",https://github.com/ViCCo-Group/semantic-features-gpt-3,-1
Towards Proactive Information Retrieval in Noisy Text with Wikipedia Concepts,0.178575,"Extracting useful information from the user history to clearly understand
informational needs is a crucial feature of a proactive information retrieval
system. Regarding understanding information and relevance, Wikipedia can
provide the background knowledge that an intelligent system needs. This work
explores how exploiting the context of a query using Wikipedia concepts can
improve proactive information retrieval on noisy text. We formulate two models
that use entity linking to associate Wikipedia topics with the relevance model.
Our experiments around a podcast segment retrieval task demonstrate that there
is a clear signal of relevance in Wikipedia concepts while a ranking model can
improve precision by incorporating them. We also find Wikifying the background
context of a query can help disambiguate the meaning of the query, further
helping proactive information retrieval.",None,-1
Transformers in Action: Weakly Supervised Action Segmentation,0.0747763,"The video action segmentation task is regularly explored under weaker forms
of supervision, such as transcript supervision, where a list of actions is
easier to obtain than dense frame-wise labels. In this formulation, the task
presents various challenges for sequence modeling approaches due to the
emphasis on action transition points, long sequence lengths, and frame
contextualization, making the task well-posed for transformers. Given
developments enabling transformers to scale linearly, we demonstrate through
our architecture how they can be applied to improve action alignment accuracy
over the equivalent RNN-based models with the attention mechanism focusing
around salient action transition regions. Additionally, given the recent focus
on inference-time transcript selection, we propose a supplemental transcript
embedding approach to select transcripts more quickly at inference-time.
Furthermore, we subsequently demonstrate how this approach can also improve the
overall segmentation performance. Finally, we evaluate our proposed methods
across the benchmark datasets to better understand the applicability of
transformers and the importance of transcript selection on this video-driven
weakly-supervised task.",None,78284
Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in Dialog Systems,0.592743,"Dialog systems are often designed or trained to output human-like responses.
However, some responses may be impossible for a machine to truthfully say (e.g.
""that movie made me cry""). Highly anthropomorphic responses might make users
uncomfortable or implicitly deceive them into thinking they are interacting
with a human. We collect human ratings on the feasibility of approximately 900
two-turn dialogs sampled from 9 diverse data sources. Ratings are for two
hypothetical machine embodiments: a futuristic humanoid robot and a digital
assistant. We find that for some data-sources commonly used to train dialog
systems, 20-30% of utterances are not viewed as possible for a machine. Rating
is marginally affected by machine embodiment. We explore qualitative and
quantitative reasons for these ratings. Finally, we build classifiers and
explore how modeling configuration might affect output permissibly, and discuss
implications for building less falsely anthropomorphic dialog systems.",https://github.com/DNGros/Robots-Dont-Cry,-1
Agile Maneuvers in Legged Robots: a Predictive Control Approach,0.826171,"Planning and execution of agile locomotion maneuvers have been a longstanding
challenge in legged robotics. It requires to derive motion plans and local
feedback policies in real-time to handle the nonholonomy of the kinetic
momenta. To achieve so, we propose a hybrid predictive controller that
considers the robot's actuation limits and full-body dynamics. It combines the
feedback policies with tactile information to locally predict future actions.
It converges within a few milliseconds thanks to a feasibility-driven approach.
Our predictive controller enables ANYmal robots to generate agile maneuvers in
realistic scenarios. A crucial element is to track the local feedback policies
as, in contrast to whole-body control, they achieve the desired angular
momentum. To the best of our knowledge, our predictive controller is the first
to handle actuation limits, generate agile locomotion maneuvers, and execute
optimal feedback policies for low level torque control without the use of a
separate whole-body controller.",None,4536
NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages,0.935154,"Natural language processing (NLP) has a significant impact on society via
technologies such as machine translation and search engines. Despite its
success, NLP technology is only widely available for high-resource languages
such as English and Chinese, while it remains inaccessible to many languages
due to the unavailability of data resources and benchmarks. In this work, we
focus on developing resources for languages in Indonesia. Despite being the
second most linguistically diverse country, most languages in Indonesia are
categorized as endangered and some are even extinct. We develop the first-ever
parallel resource for 10 low-resource languages in Indonesia. Our resource
includes datasets, a multi-task benchmark, and lexicons, as well as a parallel
Indonesian-English dataset. We provide extensive analyses and describe the
challenges when creating such resources. We hope that our work can spark NLP
research on Indonesian and other underrepresented languages.",https://github.com/andria009/IndonesianSentimentLexicon,32819
MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model,0.999818,"Human motion modeling is important for many modern graphics applications,
which typically require professional skills. In order to remove the skill
barriers for laymen, recent motion generation methods can directly generate
human motions conditioned on natural languages. However, it remains challenging
to achieve diverse and fine-grained motion generation with various text inputs.
To address this problem, we propose MotionDiffuse, the first diffusion
model-based text-driven motion generation framework, which demonstrates several
desired properties over existing methods. 1) Probabilistic Mapping. Instead of
a deterministic language-motion mapping, MotionDiffuse generates motions
through a series of denoising steps in which variations are injected. 2)
Realistic Synthesis. MotionDiffuse excels at modeling complicated data
distribution and generating vivid motion sequences. 3) Multi-Level
Manipulation. MotionDiffuse responds to fine-grained instructions on body
parts, and arbitrary-length motion synthesis with time-varied text prompts. Our
experiments show MotionDiffuse outperforms existing SoTA methods by convincing
margins on text-driven motion generation and action-conditioned motion
generation. A qualitative analysis further demonstrates MotionDiffuse's
controllability for comprehensive motion generation. Homepage:
https://mingyuan-zhang.github.io/projects/MotionDiffuse.html",https://mingyuan-zhang.github.io/projects/MotionDiffuse.html,-1
A general-purpose method for applying Explainable AI for Anomaly Detection,0.320443,"The need for explainable AI (XAI) is well established but relatively little
has been published outside of the supervised learning paradigm. This paper
focuses on a principled approach to applying explainability and
interpretability to the task of unsupervised anomaly detection. We argue that
explainability is principally an algorithmic task and interpretability is
principally a cognitive task, and draw on insights from the cognitive sciences
to propose a general-purpose method for practical diagnosis using explained
anomalies. We define Attribution Error, and demonstrate, using real-world
labeled datasets, that our method based on Integrated Gradients (IG) yields
significantly lower attribution errors than alternative methods.",None,-1
Skeleton-based Action Recognition via Temporal-Channel Aggregation,0.883219,"Skeleton-based action recognition methods are limited by the semantic
extraction of spatio-temporal skeletal maps. However, current methods have
difficulty in effectively combining features from both temporal and spatial
graph dimensions and tend to be thick on one side and thin on the other. In
this paper, we propose a Temporal-Channel Aggregation Graph Convolutional
Networks (TCA-GCN) to learn spatial and temporal topologies dynamically and
efficiently aggregate topological features in different temporal and channel
dimensions for skeleton-based action recognition. We use the Temporal
Aggregation module to learn temporal dimensional features and the Channel
Aggregation module to efficiently combine spatial dynamic channel-wise
topological features with temporal dynamic topological features. In addition,
we extract multi-scale skeletal features on temporal modeling and fuse them
with an attention mechanism. Extensive experiments show that our model results
outperform state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and
NW-UCLA datasets.",None,-1
PARSRec: Explainable Personalized Attention-fused Recurrent Sequential Recommendation Using Session Partial Actions,0.254194,"The emerging meta- and multi-verse landscape is yet another step towards the
more prevalent use of already ubiquitous online markets. In such markets,
recommender systems play critical roles by offering items of interest to the
users, thereby narrowing down a vast search space that comprises hundreds of
thousands of products. Recommender systems are usually designed to learn common
user behaviors and rely on them for inference. This approach, while effective,
is oblivious to subtle idiosyncrasies that differentiate humans from each
other. Focusing on this observation, we propose an architecture that relies on
common patterns as well as individual behaviors to tailor its recommendations
for each person. Simulations under a controlled environment show that our
proposed model learns interpretable personalized user behaviors. Our empirical
results on Nielsen Consumer Panel dataset indicate that the proposed approach
achieves up to 27.9% performance improvement compared to the state-of-the-art.",https://github.com/ehgh/PARSRec,-1
Retrieval of surgical phase transitions using reinforcement learning,0.696355,"In minimally invasive surgery, surgical workflow segmentation from video
analysis is a well studied topic. The conventional approach defines it as a
multi-class classification problem, where individual video frames are
attributed a surgical phase label.
  We introduce a novel reinforcement learning formulation for offline phase
transition retrieval. Instead of attempting to classify every video frame, we
identify the timestamp of each phase transition. By construction, our model
does not produce spurious and noisy phase transitions, but contiguous phase
blocks. We investigate two different configurations of this model. The first
does not require processing all frames in a video (only <60% and <20% of frames
in 2 different applications), while producing results slightly under the
state-of-the-art accuracy. The second configuration processes all video frames,
and outperforms the state-of-the art at a comparable computational cost.
  We compare our method against the recent top-performing frame-based
approaches TeCNO and Trans-SVNet on the public dataset Cholec80 and also on an
in-house dataset of laparoscopic sacrocolpopexy. We perform both a frame-based
(accuracy, precision, recall and F1-score) and an event-based (event ratio)
evaluation of our algorithms.",None,-1
Pareto Pairwise Ranking for Fairness Enhancement of Recommender Systems,0.199193,"Learning to rank is an effective recommendation approach since its
introduction around 2010. Famous algorithms such as Bayesian Personalized
Ranking and Collaborative Less is More Filtering have left deep impact in both
academia and industry. However, most learning to rank approaches focus on
improving technical accuracy metrics such as AUC, MRR and NDCG. Other
evaluation metrics of recommender systems like fairness have been largely
overlooked until in recent years. In this paper, we propose a new learning to
rank algorithm named Pareto Pairwise Ranking. We are inspired by the idea of
Bayesian Personalized Ranking and power law distribution. We show that our
algorithm is competitive with other algorithms when evaluated on technical
accuracy metrics. What is more important, in our experiment section we
demonstrate that Pareto Pairwise Ranking is the most fair algorithm in
comparison with 9 other contemporary algorithms.",None,-1
The Importance of Credo in Multiagent Learning,0.189882,"We propose a model for multi-objective optimization, a credo, for agents in a
system that are configured into multiple groups (i.e., teams). Our model of
credo regulates how agents optimize their behavior for the groups they belong
to. We evaluate credo in the context of challenging social dilemmas with
reinforcement learning agents. Our results indicate that the interests of
teammates, or the entire system, are not required to be fully aligned for
achieving globally beneficial outcomes. We identify two scenarios without full
common interest that achieve high equality and significantly higher mean
population rewards compared to when the interests of all agents are aligned.",https://github.com/eugenevinitsky/sequential_social_dilemma_games/,-1
RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction,0.908302,"In document-level event extraction (DEE) task, event arguments always scatter
across sentences (across-sentence issue) and multiple events may lie in one
document (multi-event issue). In this paper, we argue that the relation
information of event arguments is of great significance for addressing the
above two issues, and propose a new DEE framework which can model the relation
dependencies, called Relation-augmented Document-level Event Extraction
(ReDEE). More specifically, this framework features a novel and tailored
transformer, named as Relation-augmented Attention Transformer (RAAT). RAAT is
scalable to capture multi-scale and multi-amount argument relations. To further
leverage relation information, we introduce a separate event relation
prediction task and adopt multi-task learning method to explicitly enhance
event extraction performance. Extensive experiments demonstrate the
effectiveness of the proposed method, which can achieve state-of-the-art
performance on two public datasets. Our code is available at https://github.
com/TencentYoutuResearch/RAAT.",https://github.com/TencentYoutuResearch/RAAT,-1
Modeling Information Change in Science Communication with Semantically Matched Paraphrases,0.797138,"Whether the media faithfully communicate scientific information has long been
a core issue to the science community. Automatically identifying paraphrased
scientific findings could enable large-scale tracking and analysis of
information changes in the science communication process, but this requires
systems to understand the similarity between scientific information across
multiple domains. To this end, we present the SCIENTIFIC PARAPHRASE AND
INFORMATION CHANGE DATASET (SPICED), the first paraphrase dataset of scientific
findings annotated for degree of information change. SPICED contains 6,000
scientific finding pairs extracted from news stories, social media discussions,
and full texts of original papers. We demonstrate that SPICED poses a
challenging task and that models trained on SPICED improve downstream
performance on evidence retrieval for fact checking of real-world scientific
claims. Finally, we show that models trained on SPICED can reveal large-scale
trends in the degrees to which people and organizations faithfully communicate
new scientific findings. Data, code, and pre-trained models are available at
http://www.copenlu.com/publication/2022_emnlp_wright/.",http://www.copenlu.com/publication/2022_emnlp_wright/,-1
Towards Stroke Patients' Upper-limb Automatic Motor Assessment Using Smartwatches,0.853605,"Assessing the physical condition in rehabilitation scenarios is a challenging
problem, since it involves Human Activity Recognition (HAR) and kinematic
analysis methods. In addition, the difficulties increase in unconstrained
rehabilitation scenarios, which are much closer to the real use cases. In
particular, our aim is to design an upper-limb assessment pipeline for stroke
patients using smartwatches. We focus on the HAR task, as it is the first part
of the assessing pipeline. Our main target is to automatically detect and
recognize four key movements inspired by the Fugl-Meyer assessment scale, which
are performed in both constrained and unconstrained scenarios. In addition to
the application protocol and dataset, we propose two detection and
classification baseline methods. We believe that the proposed framework,
dataset and baseline results will serve to foster this research field.",None,-1
Quantifying Robustness to Adversarial Word Substitutions,0.10347,"Deep-learning-based NLP models are found to be vulnerable to word
substitution perturbations. Before they are widely adopted, the fundamental
issues of robustness need to be addressed. Along this line, we propose a formal
framework to evaluate word-level robustness. First, to study safe regions for a
model, we introduce robustness radius which is the boundary where the model can
resist any perturbation. As calculating the maximum robustness radius is
computationally hard, we estimate its upper and lower bound. We repurpose
attack methods as ways of seeking upper bound and design a pseudo-dynamic
programming algorithm for a tighter upper bound. Then verification method is
utilized for a lower bound. Further, for evaluating the robustness of regions
outside a safe radius, we reexamine robustness from another view:
quantification. A robustness metric with a rigorous statistical guarantee is
introduced to measure the quantification of adversarial examples, which
indicates the model's susceptibility to perturbations outside the safe radius.
The metric helps us figure out why state-of-the-art models like BERT can be
easily fooled by a few word substitutions, but generalize well in the presence
of real-world noises.",None,8526
Are disentangled representations all you need to build speaker anonymization systems?,0.615406,"Speech signals contain a lot of sensitive information, such as the speaker's
identity, which raises privacy concerns when speech data get collected. Speaker
anonymization aims to transform a speech signal to remove the source speaker's
identity while leaving the spoken content unchanged. Current methods perform
the transformation by relying on content/speaker disentanglement and voice
conversion. Usually, an acoustic model from an automatic speech recognition
system extracts the content representation while an x-vector system extracts
the speaker representation. Prior work has shown that the extracted features
are not perfectly disentangled. This paper tackles how to improve features
disentanglement, and thus the converted anonymized speech. We propose enhancing
the disentanglement by removing speaker information from the acoustic model
using vector quantization. Evaluation done using the VoicePrivacy 2022 toolkit
showed that vector quantization helps conceal the original speaker identity
while maintaining utility for speech recognition.",https://colab.research.google.com/github/deep-privacy/SA-toolkit/blob/master/SA-colab.ipynb,-1
Pretraining is All You Need for Image-to-Image Translation,0.98777,"We propose to use pretraining to boost general image-to-image translation.
Prior image-to-image translation methods usually need dedicated architectural
design and train individual translation models from scratch, struggling for
high-quality generation of complex scenes, especially when paired training data
are not abundant. In this paper, we regard each image-to-image translation
problem as a downstream task and introduce a simple and generic framework that
adapts a pretrained diffusion model to accommodate various kinds of
image-to-image translation. We also propose adversarial training to enhance the
texture synthesis in the diffusion model training, in conjunction with
normalized guidance sampling to improve the generation quality. We present
extensive empirical comparison across various tasks on challenging benchmarks
such as ADE20K, COCO-Stuff, and DIODE, showing the proposed pretraining-based
image-to-image translation (PITI) is capable of synthesizing images of
unprecedented realism and faithfulness.",None,-1
Motion-aware Dynamic Graph Neural Network for Video Compressive Sensing,0.205459,"Video snapshot compressive imaging (SCI) utilizes a 2D detector to capture
sequential video frames and compresses them into a single measurement. Various
reconstruction methods have been developed to recover the high-speed video
frames from the snapshot measurement. However, most existing reconstruction
methods are incapable of capturing long-range spatial and temporal
dependencies, which are critical for video processing. In this paper, we
propose a flexible and robust approach based on graph neural network (GNN) to
efficiently model non-local interactions between pixels in space as well as
time regardless of the distance. Specifically, we develop a motion-aware
dynamic GNN for better video representation, i.e., represent each pixel as the
aggregation of relative nodes under the guidance of frame-by-frame motions,
which consists of motion-aware dynamic sampling, cross-scale node sampling and
graph aggregation. Extensive results on both simulation and real data
demonstrate both the effectiveness and efficiency of the proposed approach, and
the visualization clearly illustrates the intrinsic dynamic sampling operations
of our proposed model for boosting the video SCI reconstruction results. The
code and models will be released to the public.",None,-1
Policy Optimization over General State and Action Spaces,0.397804,"Reinforcement learning (RL) problems over general state and action spaces are
notoriously challenging. In contrast to the tableau setting, one can not
enumerate all the states and then iteratively update the policies for each
state. This prevents the application of many well-studied RL methods especially
those with provable convergence guarantees. In this paper, we first present a
substantial generalization of the recently developed policy mirror descent
method to deal with general state and action spaces. We introduce new
approaches to incorporate function approximation into this method, so that we
do not need to use explicit policy parameterization at all. Moreover, we
present a novel policy dual averaging method for which possibly simpler
function approximation techniques can be applied. We establish linear
convergence rate to global optimality or sublinear convergence to stationarity
for these methods applied to solve different classes of RL problems under exact
policy evaluation. We then define proper notions of the approximation errors
for policy evaluation and investigate their impact on the convergence of these
methods applied to general-state RL problems with either finite-action or
continuous-action spaces. To the best of our knowledge, the development of
these algorithmic frameworks as well as their convergence analysis appear to be
new in the literature.",None,11470
Knowledge Transfer from Answer Ranking to Answer Generation,0.243627,"Recent studies show that Question Answering (QA) based on Answer Sentence
Selection (AS2) can be improved by generating an improved answer from the top-k
ranked answer sentences (termed GenQA). This allows for synthesizing the
information from multiple candidates into a concise, natural-sounding answer.
However, creating large-scale supervised training data for GenQA models is very
challenging. In this paper, we propose to train a GenQA model by transferring
knowledge from a trained AS2 model, to overcome the aforementioned issue.
First, we use an AS2 model to produce a ranking over answer candidates for a
set of questions. Then, we use the top ranked candidate as the generation
target, and the next k top ranked candidates as context for training a GenQA
model. We also propose to use the AS2 model prediction scores for loss
weighting and score-conditioned input/output shaping, to aid the knowledge
transfer. Our evaluation on three public and one large industrial datasets
demonstrates the superiority of our approach over the AS2 baseline, and GenQA
trained using supervised data.",https://github.com/amazon-research/wqa-genqa-knowledge-transfer,14771
Cognitive Ledger Project: Towards Building Personal Digital Twins Through Cognitive Blockchain,0.28408,"The Cognitive Ledger Project is an effort to develop a modular system for
turning users' personal data into structured information and machine learning
models based on a blockchain-based infrastructure. In this work-in-progress
paper, we propose a cognitive architecture for cognitive digital twins. The
suggested design embraces a cognitive blockchain (Cognitive ledger) at its
core. The architecture includes several modules that turn users' activities in
the digital environment into reusable knowledge objects and artificial
intelligence that one day can work together to form the cognitive digital twin
of users.",None,-1
CCPT: Automatic Gameplay Testing and Validation with Curiosity-Conditioned Proximal Trajectories,0.678188,"This paper proposes a novel deep reinforcement learning algorithm to perform
automatic analysis and detection of gameplay issues in complex 3D navigation
environments. The Curiosity-Conditioned Proximal Trajectories (CCPT) method
combines curiosity and imitation learning to train agents to methodically
explore in the proximity of known trajectories derived from expert
demonstrations. We show how CCPT can explore complex environments, discover
gameplay issues and design oversights in the process, and recognize and
highlight them directly to game designers. We further demonstrate the
effectiveness of the algorithm in a novel 3D navigation environment which
reflects the complexity of modern AAA video games. Our results show a higher
level of coverage and bug discovery than baselines methods, and it hence can
provide a valuable tool for game designers to identify issues in game design
automatically.",None,-1
Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words,0.465183,"The standard BERT adopts subword-based tokenization, which may break a word
into two or more wordpieces (e.g., converting ""lossless"" to ""loss"" and ""less"").
This will bring inconvenience in following situations: (1) what is the best way
to obtain the contextual vector of a word that is divided into multiple
wordpieces? (2) how to predict a word via cloze test without knowing the number
of wordpieces in advance? In this work, we explore the possibility of
developing BERT-style pretrained model over a vocabulary of words instead of
wordpieces. We call such word-level BERT model as WordBERT. We train models
with different vocabulary sizes, initialization configurations and languages.
Results show that, compared to standard wordpiece-based BERT, WordBERT makes
significant improvements on cloze test and machine reading comprehension. On
many other natural language understanding tasks, including POS tagging,
chunking and NER, WordBERT consistently performs better than BERT. Model
analysis indicates that the major advantage of WordBERT over BERT lies in the
understanding for low-frequency words and rare words. Furthermore, since the
pipeline is language-independent, we train WordBERT for Chinese language and
obtain significant gains on five natural language understanding datasets.
Lastly, the analyse on inference speed illustrates WordBERT has comparable time
cost to BERT in natural language understanding tasks.",None,-1
Unbiased Knowledge Distillation for Recommendation,0.478845,"As a promising solution for model compression, knowledge distillation (KD)
has been applied in recommender systems (RS) to reduce inference latency.
Traditional solutions first train a full teacher model from the training data,
and then transfer its knowledge (\ie \textit{soft labels}) to supervise the
learning of a compact student model. However, we find such a standard
distillation paradigm would incur serious bias issue -- popular items are more
heavily recommended after the distillation. This effect prevents the student
model from making accurate and fair recommendations, decreasing the
effectiveness of RS.
  In this work, we identify the origin of the bias in KD -- it roots in the
biased soft labels from the teacher, and is further propagated and intensified
during the distillation. To rectify this, we propose a new KD method with a
stratified distillation strategy. It first partitions items into multiple
groups according to their popularity, and then extracts the ranking knowledge
within each group to supervise the learning of the student. Our method is
simple and teacher-agnostic -- it works on distillation stage without affecting
the training of the teacher model. We conduct extensive theoretical and
empirical studies to validate the effectiveness of our proposal. We release our
code at: https://github.com/chengang95/UnKD.",None,-1
Can GAN-induced Attribute Manipulations Impact Face Recognition?,0.0978514,"Impact due to demographic factors such as age, sex, race, etc., has been
studied extensively in automated face recognition systems. However, the impact
of \textit{digitally modified} demographic and facial attributes on face
recognition is relatively under-explored. In this work, we study the effect of
attribute manipulations induced via generative adversarial networks (GANs) on
face recognition performance. We conduct experiments on the CelebA dataset by
intentionally modifying thirteen attributes using AttGAN and STGAN and
evaluating their impact on two deep learning-based face verification methods,
ArcFace and VGGFace. Our findings indicate that some attribute manipulations
involving eyeglasses and digital alteration of sex cues can significantly
impair face recognition by up to 73% and need further analysis.",None,-1
BoxShrink: From Bounding Boxes to Segmentation Masks,0.197227,"One of the core challenges facing the medical image computing community is
fast and efficient data sample labeling. Obtaining fine-grained labels for
segmentation is particularly demanding since it is expensive, time-consuming,
and requires sophisticated tools. On the contrary, applying bounding boxes is
fast and takes significantly less time than fine-grained labeling, but does not
produce detailed results. In response, we propose a novel framework for
weakly-supervised tasks with the rapid and robust transformation of bounding
boxes into segmentation masks without training any machine learning model,
coined BoxShrink. The proposed framework comes in two variants -
rapid-BoxShrink for fast label transformations, and robust-BoxShrink for more
precise label transformations. An average of four percent improvement in IoU is
found across several models when being trained using BoxShrink in a
weakly-supervised setting, compared to using only bounding box annotations as
inputs on a colonoscopy image data set. We open-sourced the code for the
proposed framework and published it online.",https://github.com/michaelgroeger/boxshrink,-1
GBC: An Efficient and Adaptive Clustering Algorithm Based on Granular-Ball,0.177886,"Existing clustering methods are based on a single granularity of information,
such as the distance and density of each data. This most fine-grained based
approach is usually inefficient and susceptible to noise. Inspired by adaptive
process of granular-ball division and differentiation, we present a novel
clustering approach that retains the speed and efficiency of K-means clustering
while out-performing time-tested density clustering approaches widely used in
industry today. Our simple, robust, adaptive granular-ball clustering method
can efficiently recognize clusters with unknown and complex shapes without the
use of extra parameters. Moreover, the proposed method provides an efficient,
adaptive way to depict the world, and will promote the research and development
of adaptive and efficient AI technologies, especially density computing models,
and improve the efficiency of many existing clustering methods.",None,-1
UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition,0.955418,"Multimodal sentiment analysis (MSA) and emotion recognition in conversation
(ERC) are key research topics for computers to understand human behaviors. From
a psychological perspective, emotions are the expression of affect or feelings
during a short period, while sentiments are formed and held for a longer
period. However, most existing works study sentiment and emotion separately and
do not fully exploit the complementary knowledge behind the two. In this paper,
we propose a multimodal sentiment knowledge-sharing framework (UniMSE) that
unifies MSA and ERC tasks from features, labels, and models. We perform
modality fusion at the syntactic and semantic levels and introduce contrastive
learning between modalities and samples to better capture the difference and
consistency between sentiments and emotions. Experiments on four public
benchmark datasets, MOSI, MOSEI, MELD, and IEMOCAP, demonstrate the
effectiveness of the proposed method and achieve consistent improvements
compared with state-of-the-art methods.",https://github.com/LeMei/UniMSE,-1
Multi-Grid Redundant Bounding Box Annotation for Accurate Object Detection,0.0327656,"Modern leading object detectors are either two-stage or one-stage networks
repurposed from a deep CNN-based backbone classifier network. YOLOv3 is one
such very-well known state-of-the-art one-shot detector that takes in an input
image and divides it into an equal-sized grid matrix. The grid cell having the
center of an object is the one responsible for detecting the particular object.
This paper presents a new mathematical approach that assigns multiple grids per
object for accurately tight-fit bounding box prediction. We also propose an
effective offline copy-paste data augmentation for object detection. Our
proposed method significantly outperforms some current state-of-the-art object
detectors with a prospect for further better performance.",None,-1
Unified Speech-Text Pre-training for Speech Translation and Recognition,0.867796,"We describe a method to jointly pre-train speech and text in an
encoder-decoder modeling framework for speech translation and recognition. The
proposed method incorporates four self-supervised and supervised subtasks for
cross modality learning. A self-supervised speech subtask leverages unlabelled
speech data, and a (self-)supervised text to text subtask makes use of abundant
text training data. Two auxiliary supervised speech tasks are included to unify
speech and text modeling space. Our contribution lies in integrating linguistic
information from the text corpus into the speech pre-training. Detailed
analysis reveals learning interference among subtasks. Two pre-training
configurations for speech translation and recognition, respectively, are
presented to alleviate subtask interference. Our experiments show the proposed
method can effectively fuse speech and text information into one model. It
achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the
MuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the
Librispeech speech recognition task.",https://github.com/pytorch/fairseq/tree/main/examples/speech_text_joint_to_text,-1
On the Evaluation of Answer-Agnostic Paragraph-level Multi-Question Generation,0.0283878,"We study the task of predicting a set of salient questions from a given
paragraph without any prior knowledge of the precise answer. We make two main
contributions. First, we propose a new method to evaluate a set of predicted
questions against the set of references by using the Hungarian algorithm to
assign predicted questions to references before scoring the assigned pairs. We
show that our proposed evaluation strategy has better theoretical and practical
properties compared to prior methods because it can properly account for the
coverage of references. Second, we compare different strategies to utilize a
pre-trained seq2seq model to generate and select a set of questions related to
a given paragraph. The code is available.",https://github.com/JRC1995/QuestionGenerationPub,-1
Sim2Real Instance-Level Style Transfer for 6D Pose Estimation,0.312469,"In recent years, synthetic data has been widely used in the training of 6D
pose estimation networks, in part because it automatically provides perfect
annotation at low cost. However, there are still non-trivial domain gaps, such
as differences in textures/materials, between synthetic and real data. These
gaps have a measurable impact on performance. To solve this problem, we
introduce a simulation to reality (sim2real) instance-level style transfer for
6D pose estimation network training. Our approach transfers the style of target
objects individually, from synthetic to real, without human intervention. This
improves the quality of synthetic data for training pose estimation networks.
We also propose a complete pipeline from data collection to the training of a
pose estimation network and conduct extensive evaluation on a real-world
robotic platform. Our evaluation shows significant improvement achieved by our
method in both pose estimation performance and the realism of images adapted by
the style transfer.",None,-1
UniCLIP: Unified Framework for Contrastive Language-Image Pre-training,0.600511,"Pre-training vision-language models with contrastive objectives has shown
promising results that are both scalable to large uncurated datasets and
transferable to many downstream applications. Some following works have
targeted to improve data efficiency by adding self-supervision terms, but
inter-domain (image-text) contrastive loss and intra-domain (image-image)
contrastive loss are defined on individual spaces in those works, so many
feasible combinations of supervision are overlooked. To overcome this issue, we
propose UniCLIP, a Unified framework for Contrastive Language-Image
Pre-training. UniCLIP integrates the contrastive loss of both inter-domain
pairs and intra-domain pairs into a single universal space. The discrepancies
that occur when integrating contrastive loss between different domains are
resolved by the three key components of UniCLIP: (1) augmentation-aware feature
embedding, (2) MP-NCE loss, and (3) domain dependent similarity measure.
UniCLIP outperforms previous vision-language pre-training methods on various
single- and multi-modality downstream tasks. In our experiments, we show that
each component that comprises UniCLIP contributes well to the final
performance.",https://github.com/Sense-GVT/DeCLIP,-1
An LSTM model for Twitter Sentiment Analysis,0.649342,"Sentiment analysis on social media such as Twitter provides organizations and
individuals an effective way to monitor public emotions towards them and their
competitors. As a result, sentiment analysis has become an important and
challenging task. In this work, we have collected seven publicly available and
manually annotated twitter sentiment datasets. We create a new training and
testing dataset from the collected datasets. We develop an LSTM model to
classify sentiment of a tweet and evaluate the model with the new dataset.",None,-1
Protecting Celebrities from DeepFake with Identity Consistency Transformer,0.985963,"In this work we propose Identity Consistency Transformer, a novel face
forgery detection method that focuses on high-level semantics, specifically
identity information, and detecting a suspect face by finding identity
inconsistency in inner and outer face regions. The Identity Consistency
Transformer incorporates a consistency loss for identity consistency
determination. We show that Identity Consistency Transformer exhibits superior
generalization ability not only across different datasets but also across
various types of image degradation forms found in real-world applications
including deepfake videos. The Identity Consistency Transformer can be easily
enhanced with additional identity information when such information is
available, and for this reason it is especially well-suited for detecting face
forgeries involving celebrities. Code will be released at
\url{https://github.com/LightDXY/ICT_DeepFake}",https://github.com/LightDXY/ICT_DeepFake,-1
Self-Supervised Visual Place Recognition by Mining Temporal and Feature Neighborhoods,0.42838,"Visual place recognition (VPR) using deep networks has achieved
state-of-the-art performance. However, most of them require a training set with
ground truth sensor poses to obtain positive and negative samples of each
observation's spatial neighborhood for supervised learning. When such
information is unavailable, temporal neighborhoods from a sequentially
collected data stream could be exploited for self-supervised training, although
we find its performance suboptimal. Inspired by noisy label learning, we
propose a novel self-supervised framework named \textit{TF-VPR} that uses
temporal neighborhoods and learnable feature neighborhoods to discover unknown
spatial neighborhoods. Our method follows an iterative training paradigm which
alternates between: (1) representation learning with data augmentation, (2)
positive set expansion to include the current feature space neighbors, and (3)
positive set contraction via geometric verification. We conduct comprehensive
experiments on both simulated and real datasets, with either RGB images or
point clouds as inputs. The results show that our method outperforms our
baselines in recall rate, robustness, and heading diversity, a novel metric we
propose for VPR. Our code and datasets can be found at
https://ai4ce.github.io/TF-VPR/.",https://ai4ce.github.io/TF-VPR/,-1
"What to Read in a Contract? Party-Specific Summarization of Legal Obligations, Entitlements, and Prohibitions",0.200323,"Reviewing and comprehending key obligations, entitlements, and prohibitions
in legal contracts can be a tedious task due to their length and
domain-specificity. Furthermore, the key rights and duties requiring review
vary for each contracting party. In this work, we propose a new task of
party-specific extractive summarization for legal contracts to facilitate
faster reviewing and improved comprehension of rights and duties. To facilitate
this, we curate a dataset comprising of party-specific pairwise importance
comparisons annotated by legal experts, covering ~293K sentence pairs that
include obligations, entitlements, and prohibitions extracted from lease
agreements. Using this dataset, we train a pairwise importance ranker and
propose a pipeline-based extractive summarization system that generates a
party-specific contract summary. We establish the need for incorporating
domain-specific notion of importance during summarization by comparing our
system against various baselines using both automatic and human evaluation
methods",None,-1
Bayesian Optimization under Stochastic Delayed Feedback,0.685853,"Bayesian optimization (BO) is a widely-used sequential method for
zeroth-order optimization of complex and expensive-to-compute black-box
functions. The existing BO methods assume that the function evaluation
(feedback) is available to the learner immediately or after a fixed delay. Such
assumptions may not be practical in many real-life problems like online
recommendations, clinical trials, and hyperparameter tuning where feedback is
available after a random delay. To benefit from the experimental
parallelization in these problems, the learner needs to start new function
evaluations without waiting for delayed feedback. In this paper, we consider
the BO under stochastic delayed feedback problem. We propose algorithms with
sub-linear regret guarantees that efficiently address the dilemma of selecting
new function queries while waiting for randomly delayed feedback. Building on
our results, we also make novel contributions to batch BO and contextual
Gaussian process bandits. Experiments on synthetic and real-life datasets
verify the performance of our algorithms.",https://github.com/daizhongxiang/BO-SDF,-1
Towards a Cleaner Document-Oriented Multilingual Crawled Corpus,0.99851,"The need for raw large raw corpora has dramatically increased in recent years
with the introduction of transfer learning and semi-supervised learning methods
to Natural Language Processing. And while there have been some recent attempts
to manually curate the amount of data necessary to train large language models,
the main way to obtain this data is still through automatic web crawling. In
this paper we take the existing multilingual web corpus OSCAR and its pipeline
Ungoliant that extracts and classifies data from Common Crawl at the line
level, and propose a set of improvements and automatic annotations in order to
produce a new document-oriented version of OSCAR that could prove more suitable
to pre-train large generative language models as well as hopefully other
applications in Natural Language Processing and Digital Humanities.",https://github.com/oscar-corpus/oscar-tools,9732
Deep Learning Opacity in Scientific Discovery,0.673327,"Philosophers have recently focused on critical, epistemological challenges
that arise from the opacity of deep neural networks. One might conclude from
this literature that doing good science with opaque models is exceptionally
challenging, if not impossible. Yet, this is hard to square with the recent
boom in optimism for AI in science alongside a flood of recent scientific
breakthroughs driven by AI methods. In this paper, I argue that the disconnect
between philosophical pessimism and scientific optimism is driven by a failure
to examine how AI is actually used in science. I show that, in order to
understand the epistemic justification for AI-powered breakthroughs,
philosophers must examine the role played by deep learning as part of a wider
process of discovery. The philosophical distinction between the 'context of
discovery' and the 'context of justification' is helpful in this regard. I
demonstrate the importance of attending to this distinction with two cases
drawn from the scientific literature, and show that epistemic opacity need not
diminish AI's capacity to lead scientists to significant and justifiable
breakthroughs.",None,-1
Multi-Task Learning Framework for Extracting Emotion Cause Span and Entailment in Conversations,0.696856,"Predicting emotions expressed in text is a well-studied problem in the NLP
community. Recently there has been active research in extracting the cause of
an emotion expressed in text. Most of the previous work has done causal emotion
entailment in documents. In this work, we propose neural models to extract
emotion cause span and entailment in conversations. For learning such models,
we use RECCON dataset, which is annotated with cause spans at the utterance
level. In particular, we propose MuTEC, an end-to-end Multi-Task learning
framework for extracting emotions, emotion cause, and entailment in
conversations. This is in contrast to existing baseline models that use ground
truth emotions to extract the cause. MuTEC performs better than the baselines
for most of the data folds provided in the dataset.",https://github.com/Exploration-Lab/MuTEC,-1
Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model,0.642142,"Augmenting pretrained language models with retrievers has shown promise in
effectively solving common NLP problems, such as language modeling and question
answering. In this paper, we evaluate the strengths and weaknesses of popular
retriever-augmented language models, namely kNN-LM, REALM, DPR + FiD,
Contriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved
statements across different tasks. Our findings indicate that the simple
similarity metric employed by retrievers is insufficient for retrieving all the
necessary statements for reasoning. Additionally, the language models do not
exhibit strong reasoning even when provided with only the required statements.
Furthermore, when combined with imperfect retrievers, the performance of the
language models becomes even worse, e.g., Flan-T5's performance drops by 28.6%
when retrieving 5 statements using Contriever. While larger language models
improve performance, there is still a substantial room for enhancement. Our
further analysis indicates that multihop retrieve-and-read is promising for
large language models like GPT-3.5, but does not generalize to other language
models like Flan-T5-xxl.",https://github.com/McGill-NLP/retriever-lm-reasoning,-1
Adaptive Local-Component-aware Graph Convolutional Network for One-shot Skeleton-based Action Recognition,0.721547,"Skeleton-based action recognition receives increasing attention because the
skeleton representations reduce the amount of training data by eliminating
visual information irrelevant to actions. To further improve the sample
efficiency, meta-learning-based one-shot learning solutions were developed for
skeleton-based action recognition. These methods find the nearest neighbor
according to the similarity between instance-level global average embedding.
However, such measurement holds unstable representativity due to inadequate
generalized learning on local invariant and noisy features, while intuitively,
more fine-grained recognition usually relies on determining key local body
movements. To address this limitation, we present the Adaptive
Local-Component-aware Graph Convolutional Network, which replaces the
comparison metric with a focused sum of similarity measurements on aligned
local embedding of action-critical spatial/temporal segments. Comprehensive
one-shot experiments on the public benchmark of NTU-RGB+D 120 indicate that our
method provides a stronger representation than the global embedding and helps
our model reach state-of-the-art.",None,-1
Holistic Sentence Embeddings for Better Out-of-Distribution Detection,0.308835,"Detecting out-of-distribution (OOD) instances is significant for the safe
deployment of NLP models. Among recent textual OOD detection works based on
pretrained language models (PLMs), distance-based methods have shown superior
performance. However, they estimate sample distance scores in the last-layer
CLS embedding space and thus do not make full use of linguistic information
underlying in PLMs. To address the issue, we propose to boost OOD detection by
deriving more holistic sentence embeddings. On the basis of the observations
that token averaging and layer combination contribute to improving OOD
detection, we propose a simple embedding approach named Avg-Avg, which averages
all token representations from each intermediate layer as the sentence
embedding and significantly surpasses the state-of-the-art on a comprehensive
suite of benchmarks by a 9.33% FAR95 margin. Furthermore, our analysis
demonstrates that it indeed helps preserve general linguistic knowledge in
fine-tuned PLMs and substantially benefits detecting background shifts. The
simple yet effective embedding method can be applied to fine-tuned PLMs with
negligible extra costs, providing a free gain in OOD detection. Our code is
available at https://github.com/lancopku/Avg-Avg.",https://github.com/lancopku/Avg-Avg,-1
HCL-TAT: A Hybrid Contrastive Learning Method for Few-shot Event Detection with Task-Adaptive Threshold,0.521845,"Conventional event detection models under supervised learning settings suffer
from the inability of transfer to newly-emerged event types owing to lack of
sufficient annotations. A commonly-adapted solution is to follow a
identify-then-classify manner, which first identifies the triggers and then
converts the classification task via a few-shot learning paradigm. However,
these methods still fall far short of expectations due to: (i) insufficient
learning of discriminative representations in low-resource scenarios, and (ii)
trigger misidentification caused by the overlap of the learned representations
of triggers and non-triggers. To address the problems, in this paper, we
propose a novel Hybrid Contrastive Learning method with a Task-Adaptive
Threshold (abbreviated as HCLTAT), which enables discriminative representation
learning with a two-view contrastive loss (support-support and
prototype-query), and devises a easily-adapted threshold to alleviate
misidentification of triggers. Extensive experiments on the benchmark dataset
FewEvent demonstrate the superiority of our method to achieve better results
compared to the state-of-the-arts. All the code and data of this paper will be
available for online public access.",https://github.com/231sm/Low_Resource_KBP,-1
DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models,0.952997,"We study the way DALLE-2 maps symbols (words) in the prompt to their
references (entities or properties of entities in the generated image). We show
that in stark contrast to the way human process language, DALLE-2 does not
follow the constraint that each word has a single role in the interpretation,
and sometimes re-use the same symbol for different purposes. We collect a set
of stimuli that reflect the phenomenon: we show that DALLE-2 depicts both
senses of nouns with multiple senses at once; and that a given word can modify
the properties of two distinct entities in the image, or can be depicted as one
object and also modify the properties of another object, creating a semantic
leakage of properties between entities. Taken together, our study highlights
the differences between DALLE-2 and human language processing and opens an
avenue for future study on the inductive biases of text-to-image models.",https://github.com/RoyiRa/,-1
End-to-End Speech to Intent Prediction to improve E-commerce Customer Support Voicebot in Hindi and English,0.2387,"Automation of on-call customer support relies heavily on accurate and
efficient speech-to-intent (S2I) systems. Building such systems using
multi-component pipelines can pose various challenges because they require
large annotated datasets, have higher latency, and have complex deployment.
These pipelines are also prone to compounding errors. To overcome these
challenges, we discuss an end-to-end (E2E) S2I model for customer support
voicebot task in a bilingual setting. We show how we can solve E2E intent
classification by leveraging a pre-trained automatic speech recognition (ASR)
model with slight modification and fine-tuning on small annotated datasets.
Experimental results show that our best E2E model outperforms a conventional
pipeline by a relative ~27% on the F1 score.",None,-1
Unsupervised Image Representation Learning with Deep Latent Particles,0.0742353,"We propose a new representation of visual data that disentangles object
position from appearance. Our method, termed Deep Latent Particles (DLP),
decomposes the visual input into low-dimensional latent ``particles'', where
each particle is described by its spatial location and features of its
surrounding region. To drive learning of such representations, we follow a
VAE-based approach and introduce a prior for particle positions based on a
spatial-softmax architecture, and a modification of the evidence lower bound
loss inspired by the Chamfer distance between particles. We demonstrate that
our DLP representations are useful for downstream tasks such as unsupervised
keypoint (KP) detection, image manipulation, and video prediction for scenes
composed of multiple dynamic objects. In addition, we show that our
probabilistic interpretation of the problem naturally provides uncertainty
estimates for particle locations, which can be used for model selection, among
other tasks. Videos and code are available:
https://taldatech.github.io/deep-latent-particles-web/",https://github.com/taldatech/deep-latent-particles-pytorch,-1
Deep Latent-Variable Models for Text Generation,0.0977558,"Text generation aims to produce human-like natural language output for
down-stream tasks. It covers a wide range of applications like machine
translation, document summarization, dialogue generation and so on. Recently
deep neural network-based end-to-end architectures have been widely adopted.
The end-to-end approach conflates all sub-modules, which used to be designed by
complex handcrafted rules, into a holistic encode-decode architecture. Given
enough training data, it is able to achieve state-of-the-art performance yet
avoiding the need of language/domain-dependent knowledge. Nonetheless, deep
learning models are known to be extremely data-hungry, and text generated from
them usually suffer from low diversity, interpretability and controllability.
As a result, it is difficult to trust the output from them in real-life
applications. Deep latent-variable models, by specifying the probabilistic
distribution over an intermediate latent process, provide a potential way of
addressing these problems while maintaining the expressive power of deep neural
networks. This dissertation presents how deep latent-variable models can
improve over the standard encoder-decoder model for text generation.",https://github.com/chin-gyou/controllable-selection,-1
Dense Learning based Semi-Supervised Object Detection,0.864572,"Semi-supervised object detection (SSOD) aims to facilitate the training and
deployment of object detectors with the help of a large amount of unlabeled
data. Though various self-training based and consistency-regularization based
SSOD methods have been proposed, most of them are anchor-based detectors,
ignoring the fact that in many real-world applications anchor-free detectors
are more demanded. In this paper, we intend to bridge this gap and propose a
DenSe Learning (DSL) based anchor-free SSOD algorithm. Specifically, we achieve
this goal by introducing several novel techniques, including an Adaptive
Filtering strategy for assigning multi-level and accurate dense pixel-wise
pseudo-labels, an Aggregated Teacher for producing stable and precise
pseudo-labels, and an uncertainty-consistency-regularization term among scales
and shuffled patches for improving the generalization capability of the
detector. Extensive experiments are conducted on MS-COCO and PASCAL-VOC, and
the results show that our proposed DSL method records new state-of-the-art SSOD
performance, surpassing existing methods by a large margin. Codes can be found
at \textcolor{blue}{https://github.com/chenbinghui1/DSL}.",https://github.com/chenbinghui1/DSL,-1
On the Effect of Information Asymmetry in Human-AI Teams,0.518506,"Over the last years, the rising capabilities of artificial intelligence (AI)
have improved human decision-making in many application areas. Teaming between
AI and humans may even lead to complementary team performance (CTP), i.e., a
level of performance beyond the ones that can be reached by AI or humans
individually. Many researchers have proposed using explainable AI (XAI) to
enable humans to rely on AI advice appropriately and thereby reach CTP.
However, CTP is rarely demonstrated in previous work as often the focus is on
the design of explainability, while a fundamental prerequisite -- the presence
of complementarity potential between humans and AI -- is often neglected.
Therefore, we focus on the existence of this potential for effective human-AI
decision-making. Specifically, we identify information asymmetry as an
essential source of complementarity potential, as in many real-world
situations, humans have access to different contextual information. By
conducting an online experiment, we demonstrate that humans can use such
contextual information to adjust the AI's decision, finally resulting in CTP.",None,-1
Neural Embeddings for Text,0.102422,"We propose a new kind of embedding for natural language text that deeply
represents semantic meaning. Standard text embeddings use the outputs from
hidden layers of a pretrained language model. In our method, we let a language
model learn from the text and then literally pick its brain, taking the actual
weights of the model's neurons to generate a vector. We call this
representation of the text a neural embedding. We confirm the ability of this
representation to reflect semantics of the text by an analysis of its behavior
on several datasets, and by a comparison of neural embedding with state of the
art sentence embeddings.",None,-1
Data-Efficient Finetuning Using Cross-Task Nearest Neighbors,0.244016,"Obtaining labeled data to train a model for a task of interest is often
expensive. Prior work shows training models on multitask data augmented with
task descriptions (prompts) effectively transfers knowledge to new tasks.
Towards efficiently building task-specific models, we assume access to a small
number (32-1000) of unlabeled target-task examples and use those to retrieve
the most similar labeled examples from a large pool of multitask data augmented
with prompts. Compared to the current practice of finetuning models on
uniformly sampled prompted multitask data (e.g.: FLAN, T0), our approach of
finetuning on cross-task nearest neighbors is significantly more
data-efficient. Using only 2% of the data from the P3 pool without any labeled
target-task data, our models outperform strong baselines trained on all
available data by 3-30% on 12 out of 14 datasets representing held-out tasks
including legal and scientific document QA. Similarly, models trained on
cross-task nearest neighbors from SuperNaturalInstructions, representing about
5% of the pool, obtain comparable performance to state-of-the-art models on 12
held-out tasks from that pool. Moreover, the models produced by our approach
also provide a better initialization than single multitask finetuned models for
few-shot finetuning on target-task data, as shown by a 2-23% relative
improvement over few-shot finetuned T0-3B models on 8 datasets.",https://github.com/allenai/data-efficient-finetuning,-1
A Generative Approach for Script Event Prediction via Contrastive Fine-tuning,0.426601,"Script event prediction aims to predict the subsequent event given the
context. This requires the capability to infer the correlations between events.
Recent works have attempted to improve event correlation reasoning by using
pretrained language models and incorporating external knowledge~(e.g.,
discourse relations). Though promising results have been achieved, some
challenges still remain. First, the pretrained language models adopted by
current works ignore event-level knowledge, resulting in an inability to
capture the correlations between events well. Second, modeling correlations
between events with discourse relations is limited because it can only capture
explicit correlations between events with discourse markers, and cannot capture
many implicit correlations. To this end, we propose a novel generative approach
for this task, in which a pretrained language model is fine-tuned with an
event-centric pretraining objective and predicts the next event within a
generative paradigm. Specifically, we first introduce a novel event-level blank
infilling strategy as the learning objective to inject event-level knowledge
into the pretrained language model, and then design a likelihood-based
contrastive loss for fine-tuning the generative model. Instead of using an
additional prediction layer, we perform prediction by using sequence
likelihoods generated by the generative model. Our approach models correlations
between events in a soft way without any external knowledge. The
likelihood-based prediction eliminates the need to use additional networks to
make predictions and is somewhat interpretable since it scores each word in the
event. Experimental results on the multi-choice narrative cloze~(MCNC) task
demonstrate that our approach achieves better results than other
state-of-the-art baselines. Our code will be available at
https://github.com/zhufq00/mcnc.",https://github.com/zhufq00/mcnc,-1
Semantic-Oriented Unlabeled Priming for Large-Scale Language Models,0.429592,"Due to the high costs associated with finetuning large language models,
various recent works propose to adapt them to specific tasks without any
parameter updates through in-context learning. Unfortunately, for in-context
learning there is currently no way to leverage unlabeled data, which is often
much easier to obtain in large quantities than labeled examples. In this work,
we therefore investigate ways to make use of unlabeled examples to improve the
zero-shot performance of pretrained language models without any finetuning: We
introduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies
examples by retrieving semantically similar unlabeled examples, assigning
labels to them in a zero-shot fashion, and then using them for in-context
learning. We also propose bag-of-contexts priming, a new priming strategy that
is more suitable for our setting and enables the usage of more examples than
fit into the context window.",None,-1
TAS-NIR: A VIS+NIR Dataset for Fine-grained Semantic Segmentation in Unstructured Outdoor Environments,0.376858,"Vegetation Indices based on paired images of the visible color spectrum (VIS)
and near infrared spectrum (NIR) have been widely used in remote sensing
applications. These vegetation indices are extended for their application in
autonomous driving in unstructured outdoor environments. In this domain we can
combine traditional vegetation indices like the Normalized Difference
Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI) with Convolutional
Neural Networks (CNNs) pre-trained on available VIS datasets. By laying a focus
on learning calibrated CNN outputs, we can provide an approach to fuse known
hand-crafted image features with CNN predictions for different domains as well.
The method is evaluated on a VIS+NIR dataset of semantically annotated images
in unstructured outdoor environments. The dataset is available at
mucar3.de/iros2022-ppniv-tas-nir.",None,-1
LightDepth: A Resource Efficient Depth Estimation Approach for Dealing with Ground Truth Sparsity via Curriculum Learning,0.102606,"Advances in neural networks enable tackling complex computer vision tasks
such as depth estimation of outdoor scenes at unprecedented accuracy. Promising
research has been done on depth estimation. However, current efforts are
computationally resource-intensive and do not consider the resource constraints
of autonomous devices, such as robots and drones. In this work, we present a
fast and battery-efficient approach for depth estimation. Our approach devises
model-agnostic curriculum-based learning for depth estimation. Our experiments
show that the accuracy of our model performs on par with the state-of-the-art
models, while its response time outperforms other models by 71%. All codes are
available online at https://github.com/fatemehkarimii/LightDepth.",https://github.com/fatemehkarimii/LightDepth,-1
Geolocation estimation of target vehicles using image processing and geometric computation,0.0861289,"Estimating vehicles' locations is one of the key components in intelligent
traffic management systems (ITMSs) for increasing traffic scene awareness.
Traditionally, stationary sensors have been employed in this regard. The
development of advanced sensing and communication technologies on modern
vehicles (MVs) makes it feasible to use such vehicles as mobile sensors to
estimate the traffic data of observed vehicles. This study aims to explore the
capabilities of a monocular camera mounted on an MV in order to estimate the
geolocation of the observed vehicle in a global positioning system (GPS)
coordinate system. We proposed a new methodology by integrating deep learning,
image processing, and geometric computation to address the observed-vehicle
localization problem. To evaluate our proposed methodology, we developed new
algorithms and tested them using real-world traffic data. The results indicated
that our proposed methodology and algorithms could effectively estimate the
observed vehicle's latitude and longitude dynamically.",None,-1
Swin MAE: Masked Autoencoders for Small Datasets,0.6089,"The development of deep learning models in medical image analysis is majorly
limited by the lack of large-sized and well-annotated datasets. Unsupervised
learning does not require labels and is more suitable for solving medical image
analysis problems. However, most of the current unsupervised learning methods
need to be applied to large datasets. To make unsupervised learning applicable
to small datasets, we proposed Swin MAE, which is a masked autoencoder with
Swin Transformer as its backbone. Even on a dataset of only a few thousand
medical images and without using any pre-trained models, Swin MAE is still able
to learn useful semantic features purely from images. It can equal or even
slightly outperform the supervised model obtained by Swin Transformer trained
on ImageNet in terms of the transfer learning results of downstream tasks. The
code is publicly available at https://github.com/Zian-Xu/Swin-MAE.",https://github.com/Zian-Xu/Swin-MAE,-1
Language Agnostic Code-Mixing Data Augmentation by Predicting Linguistic Patterns,0.206554,"In this work, we focus on intrasentential code-mixing and propose several
different Synthetic Code-Mixing (SCM) data augmentation methods that outperform
the baseline on downstream sentiment analysis tasks across various amounts of
labeled gold data. Most importantly, our proposed methods demonstrate that
strategically replacing parts of sentences in the matrix language with a
constant mask significantly improves classification accuracy, motivating
further linguistic insights into the phenomenon of code-mixing. We test our
data augmentation method in a variety of low-resource and cross-lingual
settings, reaching up to a relative improvement of 7.73% on the extremely
scarce English-Malayalam dataset. We conclude that the code-switch pattern in
code-mixing sentences is also important for the model to learn. Finally, we
propose a language-agnostic SCM algorithm that is cheap yet extremely helpful
for low-resource languages.",None,-1
Training Language Models with Memory Augmentation,0.935083,"Recent work has improved language models (LMs) remarkably by equipping them
with a non-parametric memory component. However, most existing approaches only
introduce mem-ories at testing time or represent them using a separately
trained encoder, resulting in suboptimal training of the language model. In
this work, we present TRIME, a novel yet simple training approach designed for
training LMs with memory augmentation. Our approach uses a training objective
that directly takes in-batch examples as accessible memory. We also present new
methods for memory construction and data batching, which are used for adapting
to different sets of memories--local, long-term, and external memory--at
testing time. We evaluate TRIME on multiple language modeling and machine
translation benchmarks and show that it is able to achieve significant
improvements across all the settings. Concretely, TRIME reduces the perplexity
from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory
set from the training corpus. Compared to standard LM training, TRIME adds
negligible computational overhead and is compatible with different neural
architectures, making it a versatile solution for training memory-augmented
LMs.",https://github.com/princeton-nlp/TRIME,49836
A Robust Learning Methodology for Uncertainty-aware Scientific Machine Learning models,0.460016,"Robust learning is an important issue in Scientific Machine Learning (SciML).
There are several works in the literature addressing this topic. However, there
is an increasing demand for methods that can simultaneously consider all the
different uncertainty components involved in SciML model identification. Hence,
this work proposes a comprehensive methodology for uncertainty evaluation of
the SciML that also considers several possible sources of uncertainties
involved in the identification process. The uncertainties considered in the
proposed method are the absence of theory and causal models, the sensitiveness
to data corruption or imperfection, and the computational effort. Therefore, it
was possible to provide an overall strategy for the uncertainty-aware models in
the SciML field. The methodology is validated through a case study, developing
a Soft Sensor for a polymerization reactor. The results demonstrated that the
identified Soft Sensor are robust for uncertainties, corroborating with the
consistency of the proposed approach.",None,712
Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours,0.424159,"Text classification can be useful in many real-world scenarios, saving a lot
of time for end users. However, building a custom classifier typically requires
coding skills and ML knowledge, which poses a significant barrier for many
potential users. To lift this barrier, we introduce Label Sleuth, a free open
source system for labeling and creating text classifiers. This system is unique
for (a) being a no-code system, making NLP accessible to non-experts, (b)
guiding users through the entire labeling process until they obtain a custom
classifier, making the process efficient -- from cold start to classifier in a
few hours, and (c) being open for configuration and extension by developers. By
open sourcing Label Sleuth we hope to build a community of users and developers
that will broaden the utilization of NLP models.",https://github.com/heartexlabs/label-studio,9733
Learning to Fold Real Garments with One Arm: A Case Study in Cloud-Based Robotics Research,0.469098,"Autonomous fabric manipulation is a longstanding challenge in robotics, but
evaluating progress is difficult due to the cost and diversity of robot
hardware. Using Reach, a cloud robotics platform that enables low-latency
remote execution of control policies on physical robots, we present the first
systematic benchmarking of fabric manipulation algorithms on physical hardware.
We develop 4 novel learning-based algorithms that model expert actions,
keypoints, reward functions, and dynamic motions, and we compare these against
4 learning-free and inverse dynamics algorithms on the task of folding a
crumpled T-shirt with a single robot arm. The entire lifecycle of data
collection, model training, and policy evaluation is performed remotely without
physical access to the robot workcell. Results suggest a new algorithm
combining imitation learning with analytic methods achieves 84% of human-level
performance on the folding task. See
https://sites.google.com/berkeley.edu/cloudfolding for all data, code, models,
and supplemental material.",https://github.com/google-research/pyreach,167441
"Artificial Intelligence for Cybersecurity: Threats, Attacks and Mitigation",0.835668,"With the advent of the digital era, every day-to-day task is automated due to
technological advances. However, technology has yet to provide people with
enough tools and safeguards. As the internet connects more-and-more devices
around the globe, the question of securing the connected devices grows at an
even spiral rate. Data thefts, identity thefts, fraudulent transactions,
password compromises, and system breaches are becoming regular everyday news.
The surging menace of cyber-attacks got a jolt from the recent advancements in
Artificial Intelligence. AI is being applied in almost every field of different
sciences and engineering. The intervention of AI not only automates a
particular task but also improves efficiency by many folds. So it is evident
that such a scrumptious spread would be very appetizing to cybercriminals. Thus
the conventional cyber threats and attacks are now ``intelligent"" threats. This
article discusses cybersecurity and cyber threats along with both conventional
and intelligent ways of defense against cyber-attacks. Furthermore finally, end
the discussion with the potential prospects of the future of AI in
cybersecurity.",None,-1
gBuilder: A Scalable Knowledge Graph Construction System for Unstructured Corpus,0.35939,"We design a user-friendly and scalable knowledge graph construction (KGC)
system for extracting structured knowledge from the unstructured corpus.
Different from existing KGC systems, gBuilder provides a flexible and
user-defined pipeline to embrace the rapid development of IE models. More
built-in template-based or heuristic operators and programmable operators are
available for adapting to data from different domains. Furthermore, we also
design a cloud-based self-adaptive task scheduling for gBuilder to ensure its
scalability on large-scale knowledge graph construction. Experimental
evaluation demonstrates the ability of gBuilder to organize multiple
information extraction models for knowledge graph construction in a uniform
platform, and confirms its high scalability on large-scale KGC tasks.",None,-1
End-to-End Image-Based Fashion Recommendation,0.703351,"In fashion-based recommendation settings, incorporating the item image
features is considered a crucial factor, and it has shown significant
improvements to many traditional models, including but not limited to matrix
factorization, auto-encoders, and nearest neighbor models. While there are
numerous image-based recommender approaches that utilize dedicated deep neural
networks, comparisons to attribute-aware models are often disregarded despite
their ability to be easily extended to leverage items' image features. In this
paper, we propose a simple yet effective attribute-aware model that
incorporates image features for better item representation learning in item
recommendation tasks. The proposed model utilizes items' image features
extracted by a calibrated ResNet50 component. We present an ablation study to
compare incorporating the image features using three different techniques into
the recommender system component that can seamlessly leverage any available
items' attributes. Experiments on two image-based real-world recommender
systems datasets show that the proposed model significantly outperforms all
state-of-the-art image-based models.",https://github.com/Shereen-Elsayed/ImgRec,-1
TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval,0.919095,"Text-Video retrieval is a task of great practical value and has received
increasing attention, among which learning spatial-temporal video
representation is one of the research hotspots. The video encoders in the
state-of-the-art video retrieval models usually directly adopt the pre-trained
vision backbones with the network structure fixed, they therefore can not be
further improved to produce the fine-grained spatial-temporal video
representation. In this paper, we propose Token Shift and Selection Network
(TS2-Net), a novel token shift and selection transformer architecture, which
dynamically adjusts the token sequence and selects informative tokens in both
temporal and spatial dimensions from input video samples. The token shift
module temporally shifts the whole token features back-and-forth across
adjacent frames, to preserve the complete token representation and capture
subtle movements. Then the token selection module selects tokens that
contribute most to local spatial semantics. Based on thorough experiments, the
proposed TS2-Net achieves state-of-the-art performance on major text-video
retrieval benchmarks, including new records on MSRVTT, VATEX, LSMDC,
ActivityNet, and DiDeMo.",https://github.com/yuqi657/ts2_net,-1
QFF: Quantized Fourier Features for Neural Field Representations,0.209651,"Multilayer perceptrons (MLPs) learn high frequencies slowly. Recent
approaches encode features in spatial bins to improve speed of learning
details, but at the cost of larger model size and loss of continuity. Instead,
we propose to encode features in bins of Fourier features that are commonly
used for positional encoding. We call these Quantized Fourier Features (QFF).
As a naturally multiresolution and periodic representation, our experiments
show that using QFF can result in smaller model size, faster training, and
better quality outputs for several applications, including Neural Image
Representations (NIR), Neural Radiance Field (NeRF) and Signed Distance
Function (SDF) modeling. QFF are easy to code, fast to compute, and serve as a
simple drop-in addition to many neural field representations.",None,-1
Hierarchical Semi-Supervised Contrastive Learning for Contamination-Resistant Anomaly Detection,0.548656,"Anomaly detection aims at identifying deviant samples from the normal data
distribution. Contrastive learning has provided a successful way to sample
representation that enables effective discrimination on anomalies. However,
when contaminated with unlabeled abnormal samples in training set under
semi-supervised settings, current contrastive-based methods generally 1) ignore
the comprehensive relation between training data, leading to suboptimal
performance, and 2) require fine-tuning, resulting in low efficiency. To
address the above two issues, in this paper, we propose a novel hierarchical
semi-supervised contrastive learning (HSCL) framework, for
contamination-resistant anomaly detection. Specifically, HSCL hierarchically
regulates three complementary relations: sample-to-sample, sample-to-prototype,
and normal-to-abnormal relations, enlarging the discrimination between normal
and abnormal samples with a comprehensive exploration of the contaminated data.
Besides, HSCL is an end-to-end learning approach that can efficiently learn
discriminative representations without fine-tuning. HSCL achieves
state-of-the-art performance in multiple scenarios, such as one-class
classification and cross-dataset detection. Extensive ablation studies further
verify the effectiveness of each considered relation. The code is available at
https://github.com/GaoangW/HSCL.",https://github.com/GaoangW/HSCL,35971
Reinforcement Learning with Prior Policy Guidance for Motion Planning of Dual-Arm Free-Floating Space Robot,0.948033,"Reinforcement learning methods as a promising technique have achieved
superior results in the motion planning of free-floating space robots. However,
due to the increase in planning dimension and the intensification of system
dynamics coupling, the motion planning of dual-arm free-floating space robots
remains an open challenge. In particular, the current study cannot handle the
task of capturing a non-cooperative object due to the lack of the pose
constraint of the end-effectors. To address the problem, we propose a novel
algorithm, EfficientLPT, to facilitate RL-based methods to improve planning
accuracy efficiently. Our core contributions are constructing a mixed policy
with prior knowledge guidance and introducing infinite norm to build a more
reasonable reward function. Furthermore, our method successfully captures a
rotating object with different spinning speeds.",None,-1
QCRI's COVID-19 Disinformation Detector: A System to Fight the COVID-19 Infodemic in Social Media,0.258738,"Fighting the ongoing COVID-19 infodemic has been declared as one of the most
important focus areas by the World Health Organization since the onset of the
COVID-19 pandemic. While the information that is consumed and disseminated
consists of promoting fake cures, rumors, and conspiracy theories to spreading
xenophobia and panic, at the same time there is information (e.g., containing
advice, promoting cure) that can help different stakeholders such as
policy-makers. Social media platforms enable the infodemic and there has been
an effort to curate the content on such platforms, analyze and debunk them.
While a majority of the research efforts consider one or two aspects (e.g.,
detecting factuality) of such information, in this study we focus on a
multifaceted approach, including an
API,\url{https://app.swaggerhub.com/apis/yifan2019/Tanbih/0.8.0/} and a demo
system,\url{https://covid19.tanbih.org}, which we made freely and publicly
available. We believe that this will facilitate researchers and different
stakeholders. A screencast of the API services and demo is
available.\url{https://youtu.be/zhbcSvxEKMk}",https://github.com/GateNLP/CANTM,-1
CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models,0.461353,"We motivate and introduce CHARD: Clinical Health-Aware Reasoning across
Dimensions, to investigate the capability of text generation models to act as
implicit clinical knowledge bases and generate free-flow textual explanations
about various health-related conditions across several dimensions. We collect
and present an associated dataset, CHARDat, consisting of explanations about 52
health conditions across three clinical dimensions. We conduct extensive
experiments using BART and T5 along with data augmentation, and perform
automatic, human, and qualitative analyses. We show that while our models can
perform decently, CHARD is very challenging with strong potential for further
exploration.",https://github.com/styfeng/CHARD,-1
A Better Choice: Entire-space Datasets for Aspect Sentiment Triplet Extraction,0.242394,"Aspect sentiment triplet extraction (ASTE) aims to extract aspect term,
sentiment and opinion term triplets from sentences. Since the initial datasets
used to evaluate models on ASTE had flaws, several studies later corrected the
initial datasets and released new versions of the datasets independently. As a
result, different studies select different versions of datasets to evaluate
their methods, which makes ASTE-related works hard to follow. In this paper, we
analyze the relation between different versions of datasets and suggest that
the entire-space version should be used for ASTE. Besides the sentences
containing triplets and the triplets in the sentences, the entire-space version
additionally includes the sentences without triplets and the aspect terms which
do not belong to any triplets. Hence, the entire-space version is consistent
with real-world scenarios and evaluating models on the entire-space version can
better reflect the models' performance in real-world scenarios. In addition,
experimental results show that evaluating models on non-entire-space datasets
inflates the performance of existing models and models trained on the
entire-space version can obtain better performance.",https://github.com/l294265421/entire-space-aste,-1
Automatic Summarization of Russian Texts: Comparison of Extractive and Abstractive Methods,0.108884,"The development of large and super-large language models, such as GPT-3, T5,
Switch Transformer, ERNIE, etc., has significantly improved the performance of
text generation. One of the important research directions in this area is the
generation of texts with arguments. The solution of this problem can be used in
business meetings, political debates, dialogue systems, for preparation of
student essays. One of the main domains for these applications is the economic
sphere. The key problem of the argument text generation for the Russian
language is the lack of annotated argumentation corpora. In this paper, we use
translated versions of the Argumentative Microtext, Persuasive Essays and UKP
Sentential corpora to fine-tune RuBERT model. Further, this model is used to
annotate the corpus of economic news by argumentation. Then the annotated
corpus is employed to fine-tune the ruGPT-3 model, which generates argument
texts. The results show that this approach improves the accuracy of the
argument generation by more than 20 percentage points (63.2% vs. 42.5%)
compared to the original ruGPT-3 model.",https://github.com/sberbank-ai/ru-gpts,-1
A comparison of several AI techniques for authorship attribution on Romanian texts,0.355266,"Determining the author of a text is a difficult task. Here we compare
multiple AI techniques for classifying literary texts written by multiple
authors by taking into account a limited number of speech parts (prepositions,
adverbs, and conjunctions). We also introduce a new dataset composed of texts
written in the Romanian language on which we have run the algorithms. The
compared methods are Artificial Neural Networks, Support Vector Machines, Multi
Expression Programming, Decision Trees with C5.0, and k-Nearest Neighbour.
Numerical experiments show, first of all, that the problem is difficult, but
some algorithms are able to generate decent errors on the test set.",https://github.com/sanda-avram/ROST-source-code,2427
Self-supervised Contrastive Learning for Audio-Visual Action Recognition,0.181817,"The underlying correlation between audio and visual modalities can be
utilized to learn supervised information for unlabeled videos. In this paper,
we propose an end-to-end self-supervised framework named Audio-Visual
Contrastive Learning (AVCL), to learn discriminative audio-visual
representations for action recognition. Specifically, we design an attention
based multi-modal fusion module (AMFM) to fuse audio and visual modalities. To
align heterogeneous audio-visual modalities, we construct a novel
co-correlation guided representation alignment module (CGRA). To learn
supervised information from unlabeled videos, we propose a novel
self-supervised contrastive learning module (SelfCL). Furthermore, we build a
new audio-visual action recognition dataset named Kinetics-Sounds100.
Experimental results on Kinetics-Sounds32 and Kinetics-Sounds100 datasets
demonstrate the superiority of our AVCL over the state-of-the-art methods on
large-scale action recognition benchmark.",None,-1
A Novel Self-Knowledge Distillation Approach with Siamese Representation Learning for Action Recognition,0.339619,"Knowledge distillation is an effective transfer of knowledge from a heavy
network (teacher) to a small network (student) to boost students' performance.
Self-knowledge distillation, the special case of knowledge distillation, has
been proposed to remove the large teacher network training process while
preserving the student's performance. This paper introduces a novel
Self-knowledge distillation approach via Siamese representation learning, which
minimizes the difference between two representation vectors of the two
different views from a given sample. Our proposed method, SKD-SRL, utilizes
both soft label distillation and the similarity of representation vectors.
Therefore, SKD-SRL can generate more consistent predictions and representations
in various views of the same data point. Our benchmark has been evaluated on
various standard datasets. The experimental results have shown that SKD-SRL
significantly improves the accuracy compared to existing supervised learning
and knowledge distillation methods regardless of the networks.",None,-1
Sequence Prediction Under Missing Data : An RNN Approach Without Imputation,0.0419506,"Missing data scenarios are very common in ML applications in general and
time-series/sequence applications are no exceptions. This paper pertains to a
novel Recurrent Neural Network (RNN) based solution for sequence prediction
under missing data. Our method is distinct from all existing approaches. It
tries to encode the missingness patterns in the data directly without trying to
impute data either before or during model building. Our encoding is lossless
and achieves compression. It can be employed for both sequence classification
and forecasting. We focus on forecasting here in a general context of
multi-step prediction in presence of possible exogenous inputs. In particular,
we propose novel variants of Encoder-Decoder (Seq2Seq) RNNs for this. The
encoder here adopts the above mentioned pattern encoding, while at the decoder
which has a different structure, multiple variants are feasible. We demonstrate
the utility of our proposed architecture via multiple experiments on both
single and multiple sequence (real) data-sets. We consider both scenarios where
(i)data is naturally missing and (ii)data is synthetically masked.",None,-1
Connecting the Dots: Floorplan Reconstruction Using Two-Level Queries,0.462832,"We address 2D floorplan reconstruction from 3D scans. Existing approaches
typically employ heuristically designed multi-stage pipelines. Instead, we
formulate floorplan reconstruction as a single-stage structured prediction
task: find a variable-size set of polygons, which in turn are variable-length
sequences of ordered vertices. To solve it we develop a novel Transformer
architecture that generates polygons of multiple rooms in parallel, in a
holistic manner without hand-crafted intermediate stages. The model features
two-level queries for polygons and corners, and includes polygon matching to
make the network end-to-end trainable. Our method achieves a new
state-of-the-art for two challenging datasets, Structured3D and SceneCAD, along
with significantly faster inference than previous methods. Moreover, it can
readily be extended to predict additional information, i.e., semantic room
types and architectural elements like doors and windows. Our code and models
are available at: https://github.com/ywyue/RoomFormer.",https://github.com/ywyue/RoomFormer,-1
MASALA: Modelling and Analysing the Semantics of Adpositions in Linguistic Annotation of Hindi,0.110732,"We present a completed, publicly available corpus of annotated semantic
relations of adpositions and case markers in Hindi. We used the multilingual
SNACS annotation scheme, which has been applied to a variety of typologically
diverse languages. Building on past work examining linguistic problems in SNACS
annotation, we use language models to attempt automatic labelling of SNACS
supersenses in Hindi and achieve results competitive with past work on English.
We look towards upstream applications in semantic role labelling and extension
to related languages such as Gujarati.",https://github.com/aryamanarora/carmls-hi,-1
RMGN: A Regional Mask Guided Network for Parser-free Virtual Try-on,0.693199,"Virtual try-on(VTON) aims at fitting target clothes to reference person
images, which is widely adopted in e-commerce.Existing VTON approaches can be
narrowly categorized into Parser-Based(PB) and Parser-Free(PF) by whether
relying on the parser information to mask the persons' clothes and synthesize
try-on images. Although abandoning parser information has improved the
applicability of PF methods, the ability of detail synthesizing has also been
sacrificed. As a result, the distraction from original cloth may persistin
synthesized images, especially in complicated postures and high resolution
applications. To address the aforementioned issue, we propose a novel PF method
named Regional Mask Guided Network(RMGN). More specifically, a regional mask is
proposed to explicitly fuse the features of target clothes and reference
persons so that the persisted distraction can be eliminated. A posture
awareness loss and a multi-level feature extractor are further proposed to
handle the complicated postures and synthesize high resolution images.
Extensive experiments demonstrate that our proposed RMGN outperforms both
state-of-the-art PB and PF methods.Ablation studies further verify the
effectiveness ofmodules in RMGN.",https://github.com/jokerlc/RMGN-VITON,-1
Does Corpus Quality Really Matter for Low-Resource Languages?,0.326311,"The vast majority of non-English corpora are derived from automatically
filtered versions of CommonCrawl. While prior work has identified major issues
on the quality of these datasets (Kreutzer et al., 2021), it is not clear how
this impacts downstream performance. Taking representation learning in Basque
as a case study, we explore tailored crawling (manually identifying and
scraping websites with high-quality content) as an alternative to filtering
CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque
portion of popular multilingual corpora like CC100 and mC4, yet it has a much
higher quality according to native annotators. For instance, 66% of documents
are rated as high-quality for EusCrawl, in contrast with <33% for both mC4 and
CC100. Nevertheless, we obtain similar results on downstream NLU tasks
regardless of the corpus used for pre-training. Our work suggests that NLU
performance in low-resource languages is not primarily constrained by the
quality of the data, and other factors like corpus size and domain coverage can
play a more important role.",None,-1
PARAGEN : A Parallel Generation Toolkit,0.455307,"PARAGEN is a PyTorch-based NLP toolkit for further development on parallel
generation. PARAGEN provides thirteen types of customizable plugins, helping
users to experiment quickly with novel ideas across model architectures,
optimization, and learning strategies. We implement various features, such as
unlimited data loading and automatic model selection, to enhance its industrial
usage. ParaGen is now deployed to support various research and industry
applications at ByteDance. PARAGEN is available at
https://github.com/bytedance/ParaGen.",https://github.com/bytedance/ParaGen,-1
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers,1.0,"We propose an efficient design of Transformer-based models for multivariate
time series forecasting and self-supervised representation learning. It is
based on two key components: (i) segmentation of time series into
subseries-level patches which are served as input tokens to Transformer; (ii)
channel-independence where each channel contains a single univariate time
series that shares the same embedding and Transformer weights across all the
series. Patching design naturally has three-fold benefit: local semantic
information is retained in the embedding; computation and memory usage of the
attention maps are quadratically reduced given the same look-back window; and
the model can attend longer history. Our channel-independent patch time series
Transformer (PatchTST) can improve the long-term forecasting accuracy
significantly when compared with that of SOTA Transformer-based models. We also
apply our model to self-supervised pre-training tasks and attain excellent
fine-tuning performance, which outperforms supervised training on large
datasets. Transferring of masked pre-trained representation on one dataset to
others also produces SOTA forecasting accuracy. Code is available at:
https://github.com/yuqinie98/PatchTST.",None,-1
AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,0.69306,"This paper proposes AdaTest, a novel adaptive test pattern generation
framework for efficient and reliable Hardware Trojan (HT) detection. HT is a
backdoor attack that tampers with the design of victim integrated circuits
(ICs). AdaTest improves the existing HT detection techniques in terms of
scalability and accuracy of detecting smaller Trojans in the presence of noise
and variations. To achieve high trigger coverage, AdaTest leverages
Reinforcement Learning (RL) to produce a diverse set of test inputs.
Particularly, we progressively generate test vectors with high reward values in
an iterative manner. In each iteration, the test set is evaluated and
adaptively expanded as needed. Furthermore, AdaTest integrates adaptive
sampling to prioritize test samples that provide more information for HT
detection, thus reducing the number of samples while improving the sample
quality for faster exploration. We develop AdaTest with a Software/Hardware
co-design principle and provide an optimized on-chip architecture solution.
AdaTest's architecture minimizes the hardware overhead in two ways:(i)
Deploying circuit emulation on programmable hardware to accelerate reward
evaluation of the test input; (ii) Pipelining each computation stage in AdaTest
by automatically constructing auxiliary circuit for test input generation,
reward evaluation, and adaptive sampling. We evaluate AdaTest's performance on
various HT benchmarks and compare it with two prior works that use logic
testing for HT detection. Experimental results show that AdaTest engenders up
to two orders of test generation speedup and two orders of test set size
reduction compared to the prior works while achieving the same level or higher
Trojan detection rate.",None,31252
Transfer Deep Reinforcement Learning-based Large-scale V2G Continuous Charging Coordination with Renewable Energy Sources,0.312389,"Due to the increasing popularity of electric vehicles (EVs) and the
technological advancement of EV electronics, the vehicle-to-grid (V2G)
technique and large-scale scheduling algorithms have been developed to achieve
a high level of renewable energy and power grid stability. This paper proposes
a deep reinforcement learning (DRL) method for the continuous
charging/discharging coordination strategy in aggregating large-scale EVs in
V2G mode with renewable energy sources (RES). The DRL coordination strategy can
efficiently optimize the electric vehicle aggregator's (EVA's) real-time
charging/discharging power with the state of charge (SOC) constraints of the
EVA and the individual EV. Compared with uncontrolled charging, the load
variance is reduced by 97.37$\%$ and the charging cost by 76.56$\%$. The DRL
coordination strategy further demonstrates outstanding transfer learning
ability to microgrids with RES and large-scale EVA, as well as the complicated
weekly scheduling. The DRL coordination strategy demonstrates flexible,
adaptable, and scalable performance for the large-scale V2G under realistic
operating conditions.",None,-1
Structured Local Radiance Fields for Human Avatar Modeling,0.693802,"It is extremely challenging to create an animatable clothed human avatar from
RGB videos, especially for loose clothes due to the difficulties in motion
modeling. To address this problem, we introduce a novel representation on the
basis of recent neural scene rendering techniques. The core of our
representation is a set of structured local radiance fields, which are anchored
to the pre-defined nodes sampled on a statistical human body template. These
local radiance fields not only leverage the flexibility of implicit
representation in shape and appearance modeling, but also factorize cloth
deformations into skeleton motions, node residual translations and the dynamic
detail variations inside each individual radiance field. To learn our
representation from RGB data and facilitate pose generalization, we propose to
learn the node translations and the detail variations in a conditional
generative latent space. Overall, our method enables automatic construction of
animatable human avatars for various types of clothes without the need for
scanning subject-specific templates, and can generate realistic images with
dynamic details for novel poses. Experiment show that our method outperforms
state-of-the-art methods both qualitatively and quantitatively.",https://github.com/zju3dv/EasyMocap,-1
A Portable Multiscopic Camera for Novel View and Time Synthesis in Dynamic Scenes,0.0689087,"We present a portable multiscopic camera system with a dedicated model for
novel view and time synthesis in dynamic scenes. Our goal is to render
high-quality images for a dynamic scene from any viewpoint at any time using
our portable multiscopic camera. To achieve such novel view and time synthesis,
we develop a physical multiscopic camera equipped with five cameras to train a
neural radiance field (NeRF) in both time and spatial domains for dynamic
scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal
coordinate, and 2D viewing direction) to view-dependent and time-varying
emitted radiance and volume density. Volume rendering is applied to render a
photo-realistic image at a specified camera pose and time. To improve the
robustness of our physical camera, we propose a camera parameter optimization
module and a temporal frame interpolation module to promote information
propagation across time. We conduct experiments on both real-world and
synthetic datasets to evaluate our system, and the results show that our
approach outperforms alternative solutions qualitatively and quantitatively.
Our code and dataset are available at https://yuenfuilau.github.io.",https://github.com/avinashpaliwal/Super-SloMo,-1
Seq2Seq Surrogates of Epidemic Models to Facilitate Bayesian Inference,0.205691,"Epidemic models are powerful tools in understanding infectious disease.
However, as they increase in size and complexity, they can quickly become
computationally intractable. Recent progress in modelling methodology has shown
that surrogate models can be used to emulate complex epidemic models with a
high-dimensional parameter space. We show that deep sequence-to-sequence
(seq2seq) models can serve as accurate surrogates for complex epidemic models
with sequence based model parameters, effectively replicating seasonal and
long-term transmission dynamics. Once trained, our surrogate can predict
scenarios a several thousand times faster than the original model, making them
ideal for policy exploration. We demonstrate that replacing a traditional
epidemic model with a learned simulator facilitates robust Bayesian inference.",None,88105
Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance,0.937823,"Human-translated text displays distinct features from naturally written text
in the same language. This phenomena, known as translationese, has been argued
to confound the machine translation (MT) evaluation. Yet, we find that existing
work on translationese neglects some important factors and the conclusions are
mostly correlational but not causal. In this work, we collect CausalMT, a
dataset where the MT training data are also labeled with the human translation
directions. We inspect two critical factors, the train-test direction match
(whether the human translation directions in the training and test sets are
aligned), and data-model direction match (whether the model learns in the same
direction as the human translation direction in the dataset). We show that
these two factors have a large causal effect on the MT performance, in addition
to the test-model direction mismatch highlighted by existing work on the impact
of translationese. In light of our findings, we provide a set of suggestions
for MT training and evaluation. Our code and data are at
https://github.com/EdisonNi-hku/CausalMT",https://github.com/EdisonNi-hku/CausalMT,-1
Learning Deep Sensorimotor Policies for Vision-based Autonomous Drone Racing,0.717279,"Autonomous drones can operate in remote and unstructured environments,
enabling various real-world applications. However, the lack of effective
vision-based algorithms has been a stumbling block to achieving this goal.
Existing systems often require hand-engineered components for state estimation,
planning, and control. Such a sequential design involves laborious tuning,
human heuristics, and compounding delays and errors. This paper tackles the
vision-based autonomous-drone-racing problem by learning deep sensorimotor
policies. We use contrastive learning to extract robust feature representations
from the input images and leverage a two-stage learning-by-cheating framework
for training a neural network policy. The resulting policy directly infers
control commands with feature representations learned from raw images, forgoing
the need for globally-consistent state estimation, trajectory planning, and
handcrafted control design. Our experimental results indicate that our
vision-based policy can achieve the same level of racing performance as the
state-based policy while being robust against different visual disturbances and
distractors. We believe this work serves as a stepping-stone toward developing
intelligent vision-based autonomous systems that control the drone purely from
image inputs, like human pilots.",None,-1
Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations,0.624212,"Recently, weakly-supervised image segmentation using weak annotations like
scribbles has gained great attention, since such annotations are much easier to
obtain compared to time-consuming and label-intensive labeling at the
pixel/voxel level. However, because scribbles lack structure information of
region of interest (ROI), existing scribble-based methods suffer from poor
boundary localization. Furthermore, most current methods are designed for 2D
image segmentation, which do not fully leverage the volumetric information if
directly applied to image slices. In this paper, we propose a scribble-based
volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image
segmentation and improves boundary prediction. To achieve this, we augment a
2.5D attention UNet with a proposed label propagation module to extend semantic
information from scribbles and a combination of static and active boundary
prediction to learn ROI's boundary and regularize its shape. Extensive
experiments on three public datasets demonstrate Scribble2D5 significantly
outperforms current scribble-based methods and approaches the performance of
fully-supervised ones. Our code is available online.",https://github.com/Qybc/Scribble2D5,-1
Human-centric Image Cropping with Partition-aware and Content-preserving Features,0.511555,"Image cropping aims to find visually appealing crops in an image, which is an
important yet challenging task. In this paper, we consider a specific and
practical application: human-centric image cropping, which focuses on the
depiction of a person. To this end, we propose a human-centric image cropping
method with two novel feature designs for the candidate crop: partition-aware
feature and content-preserving feature. For partition-aware feature, we divide
the whole image into nine partitions based on the human bounding box and treat
different partitions in a candidate crop differently conditioned on the human
information. For content-preserving feature, we predict a heatmap indicating
the important content to be included in a good crop, and extract the geometric
relation between the heatmap and a candidate crop. Extensive experiments
demonstrate that our method can perform favorably against state-of-the-art
image cropping methods on human-centric image cropping task. Code is available
at https://github.com/bcmi/Human-Centric-Image-Cropping.",https://github.com/bcmi/Human-Centric-Image-Cropping,-1
Learning Visuo-Haptic Skewering Strategies for Robot-Assisted Feeding,0.996701,"Acquiring food items with a fork poses an immense challenge to a
robot-assisted feeding system, due to the wide range of material properties and
visual appearances present across food groups. Deformable foods necessitate
different skewering strategies than firm ones, but inferring such
characteristics for several previously unseen items on a plate remains
nontrivial. Our key insight is to leverage visual and haptic observations
during interaction with an item to rapidly and reactively plan skewering
motions. We learn a generalizable, multimodal representation for a food item
from raw sensory inputs which informs the optimal skewering strategy. Given
this representation, we propose a zero-shot framework to sense visuo-haptic
properties of a previously unseen item and reactively skewer it, all within a
single interaction. Real-robot experiments with foods of varying levels of
visual and textural diversity demonstrate that our multimodal policy
outperforms baselines which do not exploit both visual and haptic cues or do
not reactively plan. Across 6 plates of different food items, our proposed
framework achieves 71% success over 69 skewering attempts total. Supplementary
material, datasets, code, and videos are available on our website:
https://sites.google.com/view/hapticvisualnet-corl22/home",None,-1
A Multi-label Continual Learning Framework to Scale Deep Learning Approaches for Packaging Equipment Monitoring,0.336297,"Continual Learning aims to learn from a stream of tasks, being able to
remember at the same time both new and old tasks. While many approaches were
proposed for single-class classification, multi-label classification in the
continual scenario remains a challenging problem. For the first time, we study
multi-label classification in the Domain Incremental Learning scenario.
Moreover, we propose an efficient approach that has a logarithmic complexity
with regard to the number of tasks, and can be applied also in the Class
Incremental Learning scenario. We validate our approach on a real-world
multi-label Alarm Forecasting problem from the packaging industry. For the sake
of reproducibility, the dataset and the code used for the experiments are
publicly available.",https://github.com/dallepezze/bat-ocdm,-1
Rethinking Audio-visual Synchronization for Active Speaker Detection,0.404529,"Active speaker detection (ASD) systems are important modules for analyzing
multi-talker conversations. They aim to detect which speakers or none are
talking in a visual scene at any given time. Existing research on ASD does not
agree on the definition of active speakers. We clarify the definition in this
work and require synchronization between the audio and visual speaking
activities. This clarification of definition is motivated by our extensive
experiments, through which we discover that existing ASD methods fail in
modeling the audio-visual synchronization and often classify unsynchronized
videos as active speaking. To address this problem, we propose a cross-modal
contrastive learning strategy and apply positional encoding in attention
modules for supervised ASD models to leverage the synchronization cue.
Experimental results suggest that our model can successfully detect
unsynchronized speaking as not speaking, addressing the limitation of current
models.",https://github.com/urkax/SyncTalkNet,30069
Environmental Claim Detection,0.34932,"To transition to a green economy, environmental claims made by companies must
be reliable, comparable, and verifiable. To analyze such claims at scale,
automated methods are needed to detect them in the first place. However, there
exist no datasets or models for this. Thus, this paper introduces the task of
environmental claim detection. To accompany the task, we release an
expert-annotated dataset and models trained on this dataset. We preview one
potential application of such models: We detect environmental claims made in
quarterly earning calls and find that the number of environmental claims has
steadily increased since the Paris Agreement in 2015.",https://github.com/dominiksinsaarland/environmental_claims,-1
"Continual Learning, Fast and Slow",0.568865,"According to the Complementary Learning Systems (CLS)
theory~\cite{mcclelland1995there} in neuroscience, humans do effective
\emph{continual learning} through two complementary systems: a fast learning
system centered on the hippocampus for rapid learning of the specifics,
individual experiences; and a slow learning system located in the neocortex for
the gradual acquisition of structured knowledge about the environment.
Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a
general continual learning framework comprising a fast learning system for
supervised learning of pattern-separated representation from specific tasks and
a slow learning system for representation learning of task-agnostic general
representation via Self-Supervised Learning (SSL). DualNets can seamlessly
incorporate both representation types into a holistic framework to facilitate
better continual learning in deep neural networks. Via extensive experiments,
we demonstrate the promising results of DualNets on a wide range of continual
learning protocols, ranging from the standard offline, task-aware setting to
the challenging online, task-free scenario. Notably, on the
CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly
different visual images, DualNets can achieve competitive performance with
existing state-of-the-art dynamic architecture
strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive
ablation studies to validate DualNets efficacy, robustness, and scalability.
Code will be made available at \url{https://github.com/phquang/DualNet}.",https://github.com/phquang/DualNet,-1
Finetuning BERT on Partially Annotated NER Corpora,0.0354409,"Most Named Entity Recognition (NER) models operate under the assumption that
training datasets are fully labelled. While it is valid for established
datasets like CoNLL 2003 and OntoNotes, sometimes it is not feasible to obtain
the complete dataset annotation. These situations may occur, for instance,
after selective annotation of entities for cost reduction. This work presents
an approach to finetuning BERT on such partially labelled datasets using
self-supervision and label preprocessing. Our approach outperforms the previous
LSTM-based label preprocessing baseline, significantly improving the
performance on poorly labelled datasets. We demonstrate that following our
approach while finetuning RoBERTa on CoNLL 2003 dataset with only 10% of total
entities labelled is enough to reach the performance of the baseline trained on
the same dataset with 50% of the entities labelled.",https://github.com/ViktorooReps/guided-bond,-1
Structural Prior Guided Generative Adversarial Transformers for Low-Light Image Enhancement,0.22672,"We propose an effective Structural Prior guided Generative Adversarial
Transformer (SPGAT) to solve low-light image enhancement. Our SPGAT mainly
contains a generator with two discriminators and a structural prior estimator
(SPE). The generator is based on a U-shaped Transformer which is used to
explore non-local information for better clear image restoration. The SPE is
used to explore useful structures from images to guide the generator for better
structural detail estimation. To generate more realistic images, we develop a
new structural prior guided adversarial learning method by building the skip
connections between the generator and discriminators so that the discriminators
can better discriminate between real and fake features. Finally, we propose a
parallel windows-based Swin Transformer block to aggregate different level
hierarchical features for high-quality image restoration. Experimental results
demonstrate that the proposed SPGAT performs favorably against recent
state-of-the-art methods on both synthetic and real-world datasets.",None,-1
Visualize Before You Write: Imagination-Guided Open-Ended Text Generation,0.394969,"Recent advances in text-to-image synthesis make it possible to visualize
machine imaginations for a given context. On the other hand, when generating
text, human writers are gifted at creative visualization, which enhances their
writings by forming imaginations as blueprints before putting down the stories
in words. Inspired by such a cognitive process, we ask the natural question of
whether we can endow machines with the same ability to utilize visual
information and construct a general picture of the context to guide text
generation. In this work, we propose iNLG that uses machine-generated images to
guide language models in open-ended text generation. The experiments and
analyses demonstrate the effectiveness of iNLG on open-ended text generation
tasks, including text completion, story generation, and concept-to-text
generation in both few-shot and full-data scenarios. Both automatic metrics and
human evaluations verify that the text snippets generated by our iNLG are
coherent and informative while displaying minor degeneration.",https://github.com/VegB/iNLG,-1
Risk-Driven Design of Perception Systems,0.403213,"Modern autonomous systems rely on perception modules to process complex
sensor measurements into state estimates. These estimates are then passed to a
controller, which uses them to make safety-critical decisions. It is therefore
important that we design perception systems to minimize errors that reduce the
overall safety of the system. We develop a risk-driven approach to designing
perception systems that accounts for the effect of perceptual errors on the
performance of the fully-integrated, closed-loop system. We formulate a risk
function to quantify the effect of a given perceptual error on overall safety,
and show how we can use it to design safer perception systems by including a
risk-dependent term in the loss function and generating training data in
risk-sensitive regions. We evaluate our techniques on a realistic vision-based
aircraft detect and avoid application and show that risk-driven design reduces
collision risk by 37% over a baseline system.",https://github.com/sisl/RiskDrivenPerception,-1
Blueprint Separable Residual Network for Efficient Image Super-Resolution,0.953037,"Recent advances in single image super-resolution (SISR) have achieved
extraordinary performance, but the computational cost is too heavy to apply in
edge devices. To alleviate this problem, many novel and effective solutions
have been proposed. Convolutional neural network (CNN) with the attention
mechanism has attracted increasing attention due to its efficiency and
effectiveness. However, there is still redundancy in the convolution operation.
In this paper, we propose Blueprint Separable Residual Network (BSRN)
containing two efficient designs. One is the usage of blueprint separable
convolution (BSConv), which takes place of the redundant convolution operation.
The other is to enhance the model ability by introducing more effective
attention modules. The experimental results show that BSRN achieves
state-of-the-art performance among existing efficient SR methods. Moreover, a
smaller variant of our model BSRN-S won the first place in model complexity
track of NTIRE 2022 Efficient SR Challenge. The code is available at
https://github.com/xiaom233/BSRN.",https://github.com/xiaom233/BSRN,-1
Interactive Visual Reasoning under Uncertainty,0.134398,"One of the fundamental cognitive abilities of humans is to quickly resolve
uncertainty by generating hypotheses and testing them via active trials.
Encountering a novel phenomenon accompanied by ambiguous cause-effect
relationships, humans make hypotheses against data, conduct inferences from
observation, test their theory via experimentation, and correct the proposition
if inconsistency arises. These iterative processes persist until the underlying
mechanism becomes clear. In this work, we devise the IVRE (pronounced as
""ivory"") environment for evaluating artificial agents' reasoning ability under
uncertainty. IVRE is an interactive environment featuring rich scenarios
centered around Blicket detection. Agents in IVRE are placed into environments
with various ambiguous action-effect pairs and asked to determine each object's
role. They are encouraged to propose effective and efficient experiments to
validate their hypotheses based on observations and actively gather new
information. The game ends when all uncertainties are resolved or the maximum
number of trials is consumed. By evaluating modern artificial agents in IVRE,
we notice a clear failure of today's learning methods compared to humans. Such
inefficacy in interactive reasoning ability under uncertainty calls for future
research in building human-like intelligence.",None,-1
Distilling Inter-Class Distance for Semantic Segmentation,0.671495,"Knowledge distillation is widely adopted in semantic segmentation to reduce
the computation cost.The previous knowledge distillation methods for semantic
segmentation focus on pixel-wise feature alignment and intra-class feature
variation distillation, neglecting to transfer the knowledge of the inter-class
distance in the feature space, which is important for semantic segmentation. To
address this issue, we propose an Inter-class Distance Distillation (IDD)
method to transfer the inter-class distance in the feature space from the
teacher network to the student network. Furthermore, semantic segmentation is a
position-dependent task,thus we exploit a position information distillation
module to help the student network encode more position information. Extensive
experiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show
that our method is helpful to improve the accuracy of semantic segmentation
models and achieves the state-of-the-art performance. E.g. it boosts the
benchmark model(""PSPNet+ResNet18"") by 7.50% in accuracy on the Cityscapes
dataset.",None,-1
Consent as a Foundation for Responsible Autonomy,0.692958,"This paper focuses on a dynamic aspect of responsible autonomy, namely, to
make intelligent agents be responsible at run time. That is, it considers
settings where decision making by agents impinges upon the outcomes perceived
by other agents. For an agent to act responsibly, it must accommodate the
desires and other attitudes of its users and, through other agents, of their
users.
  The contribution of this paper is twofold. First, it provides a conceptual
analysis of consent, its benefits and misuses, and how understanding consent
can help achieve responsible autonomy. Second, it outlines challenges for AI
(in particular, for agents and multiagent systems) that merit investigation to
form as a basis for modeling consent in multiagent systems and applying consent
to achieve responsible autonomy.",None,31895
Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET,0.801211,"Neural metrics have achieved impressive correlation with human judgements in
the evaluation of machine translation systems, but before we can safely
optimise towards such metrics, we should be aware of (and ideally eliminate)
biases toward bad translations that receive high scores. Our experiments show
that sample-based Minimum Bayes Risk decoding can be used to explore and
quantify such weaknesses. When applying this strategy to COMET for en-de and
de-en, we find that COMET models are not sensitive enough to discrepancies in
numbers and named entities. We further show that these biases are hard to fully
remove by simply training on additional synthetic data and release our code and
data for facilitating further experiments.",https://github.com/Unbabel/COMET,-1
VCSE: Time-Domain Visual-Contextual Speaker Extraction Network,0.399588,"Speaker extraction seeks to extract the target speech in a multi-talker
scenario given an auxiliary reference. Such reference can be auditory, i.e., a
pre-recorded speech, visual, i.e., lip movements, or contextual, i.e., phonetic
sequence. References in different modalities provide distinct and complementary
information that could be fused to form top-down attention on the target
speaker. Previous studies have introduced visual and contextual modalities in a
single model. In this paper, we propose a two-stage time-domain
visual-contextual speaker extraction network named VCSE, which incorporates
visual and self-enrolled contextual cues stage by stage to take full advantage
of every modality. In the first stage, we pre-extract a target speech with
visual cues and estimate the underlying phonetic sequence. In the second stage,
we refine the pre-extracted target speech with the self-enrolled contextual
cues. Experimental results on the real-world Lip Reading Sentences 3 (LRS3)
database demonstrate that our proposed VCSE network consistently outperforms
other state-of-the-art baselines.",https://github.com/ZhengkunTian/OpenTransformer,-1
Compositional Generalisation with Structured Reordering and Fertility Layers,0.299989,"Seq2seq models have been shown to struggle with compositional generalisation,
i.e. generalising to new and potentially more complex structures than seen
during training. Taking inspiration from grammar-based models that excel at
compositional generalisation, we present a flexible end-to-end differentiable
neural model that composes two structural operations: a fertility step, which
we introduce in this work, and a reordering step based on previous work (Wang
et al., 2021). To ensure differentiability, we use the expected value of each
step. Our model outperforms seq2seq models by a wide margin on challenging
compositional splits of realistic semantic parsing tasks that require
generalisation to longer examples. It also compares favourably to other models
targeting compositional generalisation.",https://github.com/namednil/f-then-r,-1
Marginal Contrastive Correspondence for Guided Image Generation,0.342639,"Exemplar-based image translation establishes dense correspondences between a
conditional input and an exemplar (from two different domains) for leveraging
detailed exemplar styles to achieve realistic image translation. Existing work
builds the cross-domain correspondences implicitly by minimizing feature-wise
distances across the two domains. Without explicit exploitation of
domain-invariant features, this approach may not reduce the domain gap
effectively which often leads to sub-optimal correspondences and image
translation. We design a Marginal Contrastive Learning Network (MCL-Net) that
explores contrastive learning to learn domain-invariant features for realistic
exemplar-based image translation. Specifically, we design an innovative
marginal contrastive loss that guides to establish dense correspondences
explicitly. Nevertheless, building correspondence with domain-invariant
semantics alone may impair the texture patterns and lead to degraded texture
generation. We thus design a Self-Correlation Map (SCM) that incorporates scene
structures as auxiliary information which improves the built correspondences
substantially. Quantitative and qualitative experiments on multifarious image
translation tasks show that the proposed method outperforms the
state-of-the-art consistently.",None,-1
TO-FLOW: Efficient Continuous Normalizing Flows with Temporal Optimization adjoint with Moving Speed,0.267885,"Continuous normalizing flows (CNFs) construct invertible mappings between an
arbitrary complex distribution and an isotropic Gaussian distribution using
Neural Ordinary Differential Equations (neural ODEs). It has not been tractable
on large datasets due to the incremental complexity of the neural ODE training.
Optimal Transport theory has been applied to regularize the dynamics of the ODE
to speed up training in recent works. In this paper, a temporal optimization is
proposed by optimizing the evolutionary time for forward propagation of the
neural ODE training. In this appoach, we optimize the network weights of the
CNF alternately with evolutionary time by coordinate descent. Further with
temporal regularization, stability of the evolution is ensured. This approach
can be used in conjunction with the original regularization approach. We have
experimentally demonstrated that the proposed approach can significantly
accelerate training without sacrifying performance over baseline models.",None,-1
Conditional Generation with a Question-Answering Blueprint,0.977926,"The ability to convey relevant and faithful information is critical for many
tasks in conditional generation and yet remains elusive for neural seq-to-seq
models whose outputs often reveal hallucinations and fail to correctly cover
important details. In this work, we advocate planning as a useful intermediate
representation for rendering conditional generation less opaque and more
grounded. Our work proposes a new conceptualization of text plans as a sequence
of question-answer (QA) pairs. We enhance existing datasets (e.g., for
summarization) with a QA blueprint operating as a proxy for both content
selection (i.e.,~what to say) and planning (i.e.,~in what order). We obtain
blueprints automatically by exploiting state-of-the-art question generation
technology and convert input-output pairs into input-blueprint-output tuples.
We develop Transformer-based models, each varying in how they incorporate the
blueprint in the generated output (e.g., as a global plan or iteratively).
Evaluation across metrics and datasets demonstrates that blueprint models are
more factual than alternatives which do not resort to planning and allow
tighter control of the generation output.",None,-1
Extending Temporal Data Augmentation for Video Action Recognition,0.264164,"Pixel space augmentation has grown in popularity in many Deep Learning areas,
due to its effectiveness, simplicity, and low computational cost. Data
augmentation for videos, however, still remains an under-explored research
topic, as most works have been treating inputs as stacks of static images
rather than temporally linked series of data. Recently, it has been shown that
involving the time dimension when designing augmentations can be superior to
its spatial-only variants for video action recognition. In this paper, we
propose several novel enhancements to these techniques to strengthen the
relationship between the spatial and temporal domains and achieve a deeper
level of perturbations. The video action recognition results of our techniques
outperform their respective variants in Top-1 and Top-5 settings on the UCF-101
and the HMDB-51 datasets.",None,-1
Build-a-Bot: Teaching Conversational AI Using a Transformer-Based Intent Recognition and Question Answering Architecture,0.137616,"As artificial intelligence (AI) becomes a prominent part of modern life, AI
literacy is becoming important for all citizens, not just those in technology
careers. Previous research in AI education materials has largely focused on the
introduction of terminology as well as AI use cases and ethics, but few allow
students to learn by creating their own machine learning models. Therefore,
there is a need for enriching AI educational tools with more adaptable and
flexible platforms for interested educators with any level of technical
experience to utilize within their teaching material. As such, we propose the
development of an open-source tool (Build-a-Bot) for students and teachers to
not only create their own transformer-based chatbots based on their own course
material, but also learn the fundamentals of AI through the model creation
process. The primary concern of this paper is the creation of an interface for
students to learn the principles of artificial intelligence by using a natural
language pipeline to train a customized model to answer questions based on
their own school curriculums. The model uses contexts given by their
instructor, such as chapters of a textbook, to answer questions and is deployed
on an interactive chatbot/voice agent. The pipeline teaches students data
collection, data augmentation, intent recognition, and question answering by
having them work through each of these processes while creating their AI agent,
diverging from previous chatbot work where students and teachers use the bots
as black-boxes with no abilities for customization or the bots lack AI
capabilities, with the majority of dialogue scripts being rule-based. In
addition, our tool is designed to make each step of this pipeline intuitive for
students at a middle-school level. Further work primarily lies in providing our
tool to schools and seeking student and teacher evaluations.",None,-1
How to Adapt Pre-trained Vision-and-Language Models to a Text-only Input?,0.0829631,"Current language models have been criticised for learning language from text
alone without connection between words and their meaning. Consequently,
multimodal training has been proposed as a way for creating models with better
language understanding by providing the lacking connection. We focus on
pre-trained multimodal vision-and-language (VL) models for which there already
are some results on their language understanding capabilities. An unresolved
issue with evaluating the linguistic skills of these models, however, is that
there is no established method for adapting them to text-only input without
out-of-distribution uncertainty. To find the best approach, we investigate and
compare seven possible methods for adapting three different pre-trained VL
models to text-only input. Our evaluations on both GLUE and Visual Property
Norms (VPN) show that care should be put into adapting VL models to zero-shot
text-only tasks, while the models are less sensitive to how we adapt them to
non-zero-shot tasks. We also find that the adaptation methods perform
differently for different models and that unimodal model counterparts perform
on par with the VL models regardless of adaptation, indicating that current VL
models do not necessarily gain better language understanding from their
multimodal training.",https://github.com/lovhag/adapt-pre-trained-VL-models-to-text,-1
Detecting Driver Drowsiness as an Anomaly Using LSTM Autoencoders,0.54876,"In this paper, an LSTM autoencoder-based architecture is utilized for
drowsiness detection with ResNet-34 as feature extractor. The problem is
considered as anomaly detection for a single subject; therefore, only the
normal driving representations are learned and it is expected that drowsiness
representations, yielding higher reconstruction losses, are to be distinguished
according to the knowledge of the network. In our study, the confidence levels
of normal and anomaly clips are investigated through the methodology of label
assignment such that training performance of LSTM autoencoder and
interpretation of anomalies encountered during testing are analyzed under
varying confidence rates. Our method is experimented on NTHU-DDD and
benchmarked with a state-of-the-art anomaly detection method for driver
drowsiness. Results show that the proposed model achieves detection rate of
0.8740 area under curve (AUC) and is able to provide significant improvements
on certain scenarios.",None,2076
Adapting Pretrained Text-to-Text Models for Long Text Sequences,0.758419,"We present an empirical study of adapting an existing pretrained text-to-text
model for long-sequence inputs. Through a comprehensive study along three axes
of the pretraining pipeline -- model architecture, optimization objective, and
pretraining corpus, we propose an effective recipe to build long-context models
from existing short-context models. Specifically, we replace the full attention
in transformers with pooling-augmented blockwise attention, and pretrain the
model with a masked-span prediction task with spans of varying length. In terms
of the pretraining corpus, we find that using randomly concatenated
short-documents from a large open-domain corpus results in better performance
than using existing long document corpora which are typically limited in their
domain coverage. With these findings, we build a long-context model that
achieves competitive performance on long-text QA tasks and establishes the new
state of the art on five long-text summarization datasets, often outperforming
previous methods with larger model sizes. Our code has been released at
https://github.com/facebookresearch/bart_ls.",https://github.com/facebookresearch/bart_ls,-1
EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance,0.998944,"Although current neural text-to-speech (TTS) models are able to generate
high-quality speech, intensity controllable emotional TTS is still a
challenging task. Most existing methods need external optimizations for
intensity calculation, leading to suboptimal results or degraded quality. In
this paper, we propose EmoDiff, a diffusion-based TTS model where emotion
intensity can be manipulated by a proposed soft-label guidance technique
derived from classifier guidance. Specifically, instead of being guided with a
one-hot vector for the specified emotion, EmoDiff is guided with a soft label
where the value of the specified emotion and \textit{Neutral} is set to
$\alpha$ and $1-\alpha$ respectively. The $\alpha$ here represents the emotion
intensity and can be chosen from 0 to 1. Our experiments show that EmoDiff can
precisely control the emotion intensity while maintaining high voice quality.
Moreover, diverse speech with specified emotion intensity can be generated by
sampling in the reverse denoising process.",None,8854
DFA: Dynamic Feature Aggregation for Efficient Video Object Detection,0.622218,"Video object detection is a fundamental yet challenging task in computer
vision. One practical solution is to take advantage of temporal information
from the video and apply feature aggregation to enhance the object features in
each frame. Though effective, those existing methods always suffer from low
inference speeds because they use a fixed number of frames for feature
aggregation regardless of the input frame. Therefore, this paper aims to
improve the inference speed of the current feature aggregation-based video
object detectors while maintaining their performance. To achieve this goal, we
propose a vanilla dynamic aggregation module that adaptively selects the frames
for feature enhancement. Then, we extend the vanilla dynamic aggregation module
to a more effective and reconfigurable deformable version. Finally, we
introduce inplace distillation loss to improve the representations of objects
aggregated with fewer frames. Extensive experimental results validate the
effectiveness and efficiency of our proposed methods: On the ImageNet VID
benchmark, integrated with our proposed methods, FGFA and SELSA can improve the
inference speed by 31% and 76% respectively while getting comparable
performance on accuracy.",https://github.com/open-mmlab/mmtracking,-1
Rethinking Graph Convolutional Networks in Knowledge Graph Completion,0.876026,"Graph convolutional networks (GCNs) -- which are effective in modeling graph
structures -- have been increasingly popular in knowledge graph completion
(KGC). GCN-based KGC models first use GCNs to generate expressive entity
representations and then use knowledge graph embedding (KGE) models to capture
the interactions among entities and relations. However, many GCN-based KGC
models fail to outperform state-of-the-art KGE models though introducing
additional computational complexity. This phenomenon motivates us to explore
the real effect of GCNs in KGC. Therefore, in this paper, we build upon
representative GCN-based KGC models and introduce variants to find which factor
of GCNs is critical in KGC. Surprisingly, we observe from experiments that the
graph structure modeling in GCNs does not have a significant impact on the
performance of KGC models, which is in contrast to the common belief. Instead,
the transformations for entity representations are responsible for the
performance improvements. Based on the observation, we propose a simple yet
effective framework named LTE-KGE, which equips existing KGE models with
linearly transformed entity embeddings. Experiments demonstrate that LTE-KGE
models lead to similar performance improvements with GCN-based KGC methods,
while being more computationally efficient. These results suggest that existing
GCNs are unnecessary for KGC, and novel GCN-based KGC models should count on
more ablation studies to validate their effectiveness. The code of all the
experiments is available on GitHub at https://github.com/MIRALab-USTC/GCN4KGC.",https://github.com/MIRALab-USTC/GCN4KGC,2225
SciNLI: A Corpus for Natural Language Inference on Scientific Text,0.745651,"Existing Natural Language Inference (NLI) datasets, while being instrumental
in the advancement of Natural Language Understanding (NLU) research, are not
related to scientific text. In this paper, we introduce SciNLI, a large dataset
for NLI that captures the formality in scientific text and contains 107,412
sentence pairs extracted from scholarly papers on NLP and computational
linguistics. Given that the text used in scientific literature differs vastly
from the text used in everyday language both in terms of vocabulary and
sentence structure, our dataset is well suited to serve as a benchmark for the
evaluation of scientific NLU models. Our experiments show that SciNLI is harder
to classify than the existing NLI datasets. Our best performing model with
XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23%
showing that there is substantial room for improvement.",https://github.com/msadat3/SciNLI,6491
Structured Pruning Learns Compact and Accurate Models,0.999305,"The growing size of neural language models has led to increased attention in
model compression. The two predominant approaches are pruning, which gradually
removes weights from a pre-trained model, and distillation, which trains a
smaller compact model to match a larger one. Pruning methods can significantly
reduce the model size but hardly achieve large speedups as distillation.
However, distillation methods require large amounts of unlabeled data and are
expensive to train. In this work, we propose a task-specific structured pruning
method CoFi (Coarse- and Fine-grained Pruning), which delivers highly
parallelizable subnetworks and matches the distillation methods in both
accuracy and latency, without resorting to any unlabeled data. Our key insight
is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads
and hidden units) modules, which controls the pruning decision of each
parameter with masks of different granularity. We also devise a layerwise
distillation strategy to transfer knowledge from unpruned to pruned models
during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi
yields models with over 10x speedups with a small accuracy drop, showing its
effectiveness and efficiency compared to previous pruning and distillation
approaches.",https://github.com/princeton-nlp/CoFiPruning,-1
Multiple Object Tracking Challenge Technical Report for Team MT_IoT,0.262924,"This is a brief technical report of our proposed method for Multiple-Object
Tracking (MOT) Challenge in Complex Environments. In this paper, we treat the
MOT task as a two-stage task including human detection and trajectory matching.
Specifically, we designed an improved human detector and associated most of
detection to guarantee the integrity of the motion trajectory. We also propose
a location-wise matching matrix to obtain more accurate trace matching. Without
any model merging, our method achieves 66.672 HOTA and 93.971 MOTA on the
DanceTrack challenge dataset.",None,-1
Explainable Action Advising for Multi-Agent Reinforcement Learning,0.316183,"Action advising is a knowledge transfer technique for reinforcement learning
based on the teacher-student paradigm. An expert teacher provides advice to a
student during training in order to improve the student's sample efficiency and
policy performance. Such advice is commonly given in the form of state-action
pairs. However, it makes it difficult for the student to reason with and apply
to novel states. We introduce Explainable Action Advising, in which the teacher
provides action advice as well as associated explanations indicating why the
action was chosen. This allows the student to self-reflect on what it has
learned, enabling advice generalization and leading to improved sample
efficiency and learning performance - even in environments where the teacher is
sub-optimal. We empirically show that our framework is effective in both
single-agent and multi-agent scenarios, yielding improved policy returns and
convergence rates when compared to state-of-the-art methods",https://github.com/sophieyueguo/explainable_action_advising,-1
Towards Federated Long-Tailed Learning,0.466454,"Data privacy and class imbalance are the norm rather than the exception in
many machine learning tasks. Recent attempts have been launched to, on one
side, address the problem of learning from pervasive private data, and on the
other side, learn from long-tailed data. However, both assumptions might hold
in practical applications, while an effective method to simultaneously
alleviate both issues is yet under development. In this paper, we focus on
learning with long-tailed (LT) data distributions under the context of the
popular privacy-preserved federated learning (FL) framework. We characterize
three scenarios with different local or global long-tailed data distributions
in the FL framework, and highlight the corresponding challenges. The
preliminary results under different scenarios reveal that substantial future
work are of high necessity to better resolve the characterized federated
long-tailed learning tasks.",None,-1
What Dense Graph Do You Need for Self-Attention?,0.059273,"Transformers have made progress in miscellaneous tasks, but suffer from
quadratic computational and memory complexities. Recent works propose sparse
Transformers with attention on sparse graphs to reduce complexity and remain
strong performance. While effective, the crucial parts of how dense a graph
needs to be to perform well are not fully explored. In this paper, we propose
Normalized Information Payload (NIP), a graph scoring function measuring
information transfer on graph, which provides an analysis tool for trade-offs
between performance and complexity. Guided by this theoretical analysis, we
present Hypercube Transformer, a sparse Transformer that models token
interactions in a hypercube and shows comparable or even better results with
vanilla Transformer while yielding $O(N\log N)$ complexity with sequence length
$N$. Experiments on tasks requiring various sequence lengths lay validation for
our graph function well.",https://github.com/yxzwang/Normalized-Information-Payload,-1
BERT4Loc: BERT for Location -- POI Recommender System,0.243857,"Recommending points of interest (POIs) is a challenging task that requires
extracting comprehensive location data from location-based social media
platforms. To provide effective location-based recommendations, it's important
to analyze users' historical behavior and preferences. In this study, we
present a sophisticated location-aware recommendation system that uses
Bidirectional Encoder Representations from Transformers (BERT) to offer
personalized location-based suggestions. Our model combines location
information and user preferences to provide more relevant recommendations
compared to models that predict the next POI in a sequence. Our experiments on
two benchmark dataset show that our BERT-based model outperforms various
state-of-the-art sequential models. Moreover, we see the effectiveness of the
proposed model for quality through additional experiments.",None,7807
Face segmentation: A comparison between visible and thermal images,0.158039,"Face segmentation is a first step for face biometric systems. In this paper
we present a face segmentation algorithm for thermographic images. This
algorithm is compared with the classic Viola and Jones algorithm used for
visible images. Experimental results reveal that, when segmenting a
multispectral (visible and thermal) face database, the proposed algorithm is
more than 10 times faster, while the accuracy of face segmentation in thermal
images is higher than in case of Viola-Jones",None,-1
Non-Uniformly Terminating Chase: Size and Complexity,0.950213,"The chase procedure, originally introduced for checking implication of
database constraints, and later on used for computing data exchange solutions,
has recently become a central algorithmic tool in rule-based ontological
reasoning. In this context, a key problem is non-uniform chase termination:
does the chase of a database w.r.t. a rule-based ontology terminate? And if
this is the case, what is the size of the result of the chase? We focus on
guarded tuple-generating dependencies (TGDs), which form a robust rule-based
ontology language, and study the above central questions for the semi-oblivious
version of the chase. One of our main findings is that non-uniform
semi-oblivious chase termination for guarded TGDs is feasible in polynomial
time w.r.t. the database, and the size of the result of the chase (whenever is
finite) is linear w.r.t. the database. Towards our results concerning
non-uniform chase termination, we show that basic techniques such as
simplification and linearization, originally introduced in the context of
ontological query answering, can be safely applied to the chase termination
problem.",None,-1
Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts,0.19483,"Despite the advances in digital healthcare systems offering curated
structured knowledge, much of the critical information still lies in large
volumes of unlabeled and unstructured clinical texts. These texts, which often
contain protected health information (PHI), are exposed to information
extraction tools for downstream applications, risking patient identification.
Existing works in de-identification rely on using large-scale annotated corpora
in English, which often are not suitable in real-world multilingual settings.
Pre-trained language models (LM) have shown great potential for cross-lingual
transfer in low-resource settings. In this work, we empirically show the
few-shot cross-lingual transfer property of LMs for named entity recognition
(NER) and apply it to solve a low-resource and real-world challenge of
code-mixed (Spanish-Catalan) clinical notes de-identification in the stroke
domain. We annotate a gold evaluation dataset to assess few-shot setting
performance where we only use a few hundred labeled examples for training. Our
model improves the zero-shot F1-score from 73.7% to 91.2% on the gold
evaluation set when adapting Multilingual BERT (mBERT) (Devlin et al., 2019)
from the MEDDOCAN (Marimon et al., 2019) corpus with our few-shot cross-lingual
target corpus. When generalized to an out-of-sample test set, the best model
achieves a human-evaluation F1-score of 97.2%.",https://github.com/suamin/T2NER,196
Custom Structure Preservation in Face Aging,0.932335,"In this work, we propose a novel architecture for face age editing that can
produce structural modifications while maintaining relevant details present in
the original image. We disentangle the style and content of the input image and
propose a new decoder network that adopts a style-based strategy to combine the
style and content representations of the input image while conditioning the
output on the target age. We go beyond existing aging methods allowing users to
adjust the degree of structure preservation in the input image during
inference. To this purpose, we introduce a masking mechanism, the CUstom
Structure Preservation module, that distinguishes relevant regions in the input
image from those that should be discarded. CUSP requires no additional
supervision. Finally, our quantitative and qualitative analysis which include a
user study, show that our method outperforms prior art and demonstrates the
effectiveness of our strategy regarding image editing and adjustable structure
preservation. Code and pretrained models are available at
https://github.com/guillermogotre/CUSP.",https://github.com/guillermogotre/CUSP,-1
LCP-dropout: Compression-based Multiple Subword Segmentation for Neural Machine Translation,0.606678,"In this study, we propose a simple and effective preprocessing method for
subword segmentation based on a data compression algorithm. Compression-based
subword segmentation has recently attracted significant attention as a
preprocessing method for training data in Neural Machine Translation. Among
them, BPE/BPE-dropout is one of the fastest and most effective method compared
to conventional approaches. However, compression-based approach has a drawback
in that generating multiple segmentations is difficult due to the determinism.
To overcome this difficulty, we focus on a probabilistic string algorithm,
called locally-consistent parsing (LCP), that has been applied to achieve
optimum compression. Employing the probabilistic mechanism of LCP, we propose
LCP-dropout for multiple subword segmentation that improves BPE/BPE-dropout,
and show that it outperforms various baselines in learning from especially
small training data.",https://github.com/moses-smt/mosesdecoder,-1
Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,0.694067,"Despite their strong performance on many tasks, pre-trained language models
have been shown to struggle on out-of-distribution compositional
generalization. Meanwhile, recent work has shown considerable improvements on
many NLP tasks from model scaling. Can scaling up model size also improve
compositional generalization in semantic parsing? We evaluate encoder-decoder
models up to 11B parameters and decoder-only models up to 540B parameters, and
compare model scaling curves for three different methods for applying a
pre-trained language model to a new task: fine-tuning all parameters, prompt
tuning, and in-context learning. We observe that fine-tuning generally has flat
or negative scaling curves on out-of-distribution compositional generalization
in semantic parsing evaluations. In-context learning has positive scaling
curves, but is generally outperformed by much smaller fine-tuned models.
Prompt-tuning can outperform fine-tuning, suggesting further potential
improvements from scaling as it exhibits a more positive scaling curve.
Additionally, we identify several error trends that vary with model scale. For
example, larger models are generally better at modeling the syntax of the
output space, but are also more prone to certain types of overfitting. Overall,
our study highlights limitations of current techniques for effectively
leveraging model scale for compositional generalization, while our analysis
also suggests promising directions for future work.",https://github.com/microsoft/compositional-generalization-span-level-attention,20836
Transformer Embeddings of Irregularly Spaced Events and Their Participants,0.658745,"The neural Hawkes process (Mei & Eisner, 2017) is a generative model of
irregularly spaced sequences of discrete events. To handle complex domains with
many event types, Mei et al. (2020a) further consider a setting in which each
event in the sequence updates a deductive database of facts (via
domain-specific pattern-matching rules); future events are then conditioned on
the database contents. They show how to convert such a symbolic system into a
neuro-symbolic continuous-time generative model, in which each database fact
and the possible event has a time-varying embedding that is derived from its
symbolic provenance.
  In this paper, we modify both models, replacing their recurrent LSTM-based
architectures with flatter attention-based architectures (Vaswani et al.,
2017), which are simpler and more parallelizable. This does not appear to hurt
our accuracy, which is comparable to or better than that of the original models
as well as (where applicable) previous attention-based methods (Zuo et al.,
2020; Zhang et al., 2020a).",https://github.com/yangalan123/anhp-andtt,-1
Faces: AI Blitz XIII Solutions,0.176362,"AI Blitz XIII Faces challenge hosted on www.aicrowd.com platform consisted of
five problems: Sentiment Classification, Age Prediction, Mask Prediction, Face
Recognition, and Face De-Blurring. Our team GLaDOS took second place. Here we
present our solutions and results. Code implementation:
https://github.com/ndrwmlnk/ai-blitz-xiii",https://github.com/ndrwmlnk/ai-blitz-xiii,-1
Omnivore: A Single Model for Many Visual Modalities,0.926519,"Prior work has studied different visual modalities in isolation and developed
separate architectures for recognition of images, videos, and 3D data. Instead,
in this paper, we propose a single model which excels at classifying images,
videos, and single-view 3D data using exactly the same model parameters. Our
'Omnivore' model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities.
Omnivore is simple to train, uses off-the-shelf standard datasets, and performs
at-par or better than modality-specific models of the same size. A single
Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN
RGB-D. After finetuning, our models outperform prior work on a variety of
vision tasks and generalize across modalities. Omnivore's shared visual
representation naturally enables cross-modal recognition without access to
correspondences between modalities. We hope our results motivate researchers to
model visual modalities together.",None,111734
VQ-Flows: Vector Quantized Local Normalizing Flows,0.379527,"Normalizing flows provide an elegant approach to generative modeling that
allows for efficient sampling and exact density evaluation of unknown data
distributions. However, current techniques have significant limitations in
their expressivity when the data distribution is supported on a low-dimensional
manifold or has a non-trivial topology. We introduce a novel statistical
framework for learning a mixture of local normalizing flows as ""chart maps""
over the data manifold. Our framework augments the expressivity of recent
approaches while preserving the signature property of normalizing flows, that
they admit exact density evaluation. We learn a suitable atlas of charts for
the data manifold via a vector quantized auto-encoder (VQ-AE) and the
distributions over them using a conditional flow. We validate experimentally
that our probabilistic framework enables existing approaches to better model
data distributions over complex manifolds.",None,-1
PAL: Persona-Augmented Emotional Support Conversation Generation,0.951647,"Due to the lack of human resources for mental health support, there is an
increasing demand for employing conversational agents for support. Recent work
has demonstrated the effectiveness of dialogue models in providing emotional
support. As previous studies have demonstrated that seekers' persona is an
important factor for effective support, we investigate whether there are
benefits to modeling such information in dialogue models for support. In this
paper, our empirical analysis verifies that persona has an important impact on
emotional support. Therefore, we propose a framework for dynamically inferring
and modeling seekers' persona. We first train a model for inferring the
seeker's persona from the conversation history. Accordingly, we propose PAL, a
model that leverages persona information and, in conjunction with our
strategy-based controllable generation method, provides personalized emotional
support. Automatic and manual evaluations demonstrate that PAL achieves
state-of-the-art results, outperforming the baselines on the studied benchmark.
Our code and data are publicly available at https://github.com/chengjl19/PAL.",https://github.com/chengjl19/PAL,-1
Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization,0.814644,"Vision transformers (ViTs) are emerging with significantly improved accuracy
in computer vision tasks. However, their complex architecture and enormous
computation/storage demand impose urgent needs for new hardware accelerator
design methodology. This work proposes an FPGA-aware automatic ViT acceleration
framework based on the proposed mixed-scheme quantization. To the best of our
knowledge, this is the first FPGA-based ViT acceleration framework exploring
model quantization. Compared with state-of-the-art ViT quantization work
(algorithmic approach only without hardware acceleration), our quantization
achieves 0.47% to 1.36% higher Top-1 accuracy under the same bit-width.
Compared with the 32-bit floating-point baseline FPGA accelerator, our
accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS
vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.",None,-1
Experiencer-Specific Emotion and Appraisal Prediction,0.685414,"Emotion classification in NLP assigns emotions to texts, such as sentences or
paragraphs. With texts like ""I felt guilty when he cried"", focusing on the
sentence level disregards the standpoint of each participant in the situation:
the writer (""I"") and the other entity (""he"") could in fact have different
affective states. The emotions of different entities have been considered only
partially in emotion semantic role labeling, a task that relates semantic roles
to emotion cue words. Proposing a related task, we narrow the focus on the
experiencers of events, and assign an emotion (if any holds) to each of them.
To this end, we represent each emotion both categorically and with appraisal
variables, as a psychological access to explaining why a person develops a
particular emotion. On an event description corpus, our experiencer-aware
models of emotions and appraisals outperform the experiencer-agnostic
baselines, showing that disregarding event participants is an
oversimplification for the emotion detection task.",https://www.ims.uni-stuttgart.de/data/appraisalemotion,-1
Unbiased Heterogeneous Scene Graph Generation with Relation-aware Message Passing Neural Network,0.762315,"Recent scene graph generation (SGG) frameworks have focused on learning
complex relationships among multiple objects in an image. Thanks to the nature
of the message passing neural network (MPNN) that models high-order
interactions between objects and their neighboring objects, they are dominant
representation learning modules for SGG. However, existing MPNN-based
frameworks assume the scene graph as a homogeneous graph, which restricts the
context-awareness of visual relations between objects. That is, they overlook
the fact that the relations tend to be highly dependent on the objects with
which the relations are associated. In this paper, we propose an unbiased
heterogeneous scene graph generation (HetSGG) framework that captures
relation-aware context using message passing neural networks. We devise a novel
message passing layer, called relation-aware message passing neural network
(RMP), that aggregates the contextual information of an image considering the
predicate type between objects. Our extensive evaluations demonstrate that
HetSGG outperforms state-of-the-art methods, especially outperforming on tail
predicate classes.",https://github.com/KanghoonYoon/hetsgg-torch,-1
SecureBERT: A Domain-Specific Language Model for Cybersecurity,0.424602,"Natural Language Processing (NLP) has recently gained wide attention in
cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber
automation. Increased connection and automation have revolutionized the world's
economic and cultural infrastructures, while they have introduced risks in
terms of cyber attacks. CTI is information that helps cybersecurity analysts
make intelligent security decisions, that is often delivered in the form of
natural language text, which must be transformed to machine readable format
through an automated procedure before it can be used for automated security
measures.
  This paper proposes SecureBERT, a cybersecurity language model capable of
capturing text connotations in cybersecurity text (e.g., CTI) and therefore
successful in automation for many critical cybersecurity tasks that would
otherwise rely on human expertise and time-consuming manual efforts. SecureBERT
has been trained using a large corpus of cybersecurity text.To make SecureBERT
effective not just in retaining general English understanding, but also when
applied to text with cybersecurity implications, we developed a customized
tokenizer as well as a method to alter pre-trained weights. The SecureBERT is
evaluated using the standard Masked Language Model (MLM) test as well as two
additional standard NLP tasks. Our evaluation studies show that
SecureBERT\footnote{\url{https://github.com/ehsanaghaei/SecureBERT}}
outperforms existing similar models, confirming its capability for solving
crucial NLP tasks in cybersecurity.",https://github.com/ehsanaghaei/SecureBERT,-1
PromptBoosting: Black-Box Text Classification with Ten Forward Passes,0.715876,"We describe PromptBoosting, a query-efficient procedure for building a text
classifier from a neural language model (LM) without access to the LM's
parameters, gradients, or hidden representations. This form of ""black-box""
classifier training has become increasingly important as the cost of training
and inference in large-scale LMs grows. But existing black-box LM classifier
learning approaches are themselves computationally inefficient, typically
specializing LMs to the target task by searching in a large space of (discrete
or continuous) prompts using zeroth-order optimization methods. Instead of
directly optimizing in prompt space, PromptBoosting obtains a small pool of
prompts via a gradient-free approach and then constructs a large pool of weak
learners by pairing these prompts with different elements of the LM's output
distribution. These weak learners are then ensembled using the AdaBoost
algorithm. The entire learning process requires only a small number of forward
passes and no backward pass. Experiments show that PromptBoosting achieves
state-of-the-art performance in multiple black-box few-shot classification
tasks, and matches or outperforms full fine-tuning in both few-shot and
standard learning paradigms, while training 10x faster than existing black-box
methods.",https://github.com/UCSB-NLP-Chang/PromptBoosting,-1
"Env-Aware Anomaly Detection: Ignore Style Changes, Stay True to Content!",0.559414,"We introduce a formalization and benchmark for the unsupervised anomaly
detection task in the distribution-shift scenario. Our work builds upon the
iWildCam dataset, and, to the best of our knowledge, we are the first to
propose such an approach for visual data. We empirically validate that
environment-aware methods perform better in such cases when compared with the
basic Empirical Risk Minimization (ERM). We next propose an extension for
generating positive samples for contrastive methods that considers the
environment labels when training, improving the ERM baseline score by 8.7%.",None,-1
SpanProto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition,0.722004,"Few-shot Named Entity Recognition (NER) aims to identify named entities with
very little annotated data. Previous methods solve this problem based on
token-wise classification, which ignores the information of entity boundaries,
and inevitably the performance is affected by the massive non-entity tokens. To
this end, we propose a seminal span-based prototypical network (SpanProto) that
tackles few-shot NER via a two-stage approach, including span extraction and
mention classification. In the span extraction stage, we transform the
sequential tags into a global boundary matrix, enabling the model to focus on
the explicit boundary information. For mention classification, we leverage
prototypical learning to capture the semantic representations for each labeled
span and make the model better adapt to novel-class entities. To further
improve the model performance, we split out the false positives generated by
the span extractor but not labeled in the current episode set, and then present
a margin-based loss to separate them from each prototype region. Experiments
over multiple benchmarks demonstrate that our model outperforms strong
baselines by a large margin.",https://github.com/alibaba/EasyNLP,-1
Object-Guided Day-Night Visual Localization in Urban Scenes,0.436467,"We introduce Object-Guided Localization (OGuL) based on a novel method of
local-feature matching. Direct matching of local features is sensitive to
significant changes in illumination. In contrast, object detection often
survives severe changes in lighting conditions. The proposed method first
detects semantic objects and establishes correspondences of those objects
between images. Object correspondences provide local coarse alignment of the
images in the form of a planar homography. These homographies are consequently
used to guide the matching of local features. Experiments on standard urban
localization datasets (Aachen, Extended-CMU-Season, RobotCar-Season) show that
OGuL significantly improves localization results with as simple local features
as SIFT, and its performance competes with the state-of-the-art CNN-based
methods trained for day-to-night localization.",None,-1
Learning Implicit Templates for Point-Based Clothed Human Modeling,0.986355,"We present FITE, a First-Implicit-Then-Explicit framework for modeling human
avatars in clothing. Our framework first learns implicit surface templates
representing the coarse clothing topology, and then employs the templates to
guide the generation of point sets which further capture pose-dependent
clothing deformations such as wrinkles. Our pipeline incorporates the merits of
both implicit and explicit representations, namely, the ability to handle
varying topology and the ability to efficiently capture fine details. We also
propose diffused skinning to facilitate template training especially for loose
clothing, and projection-based pose-encoding to extract pose information from
mesh templates without predefined UV map or connectivity. Our code is publicly
available at https://github.com/jsnln/fite.",https://github.com/jsnln/fite,-1
Scalable Planning and Learning Framework Development for Swarm-to-Swarm Engagement Problems,0.864665,"Development of guidance, navigation and control frameworks/algorithms for
swarms attracted significant attention in recent years. That being said,
algorithms for planning swarm allocations/trajectories for engaging with enemy
swarms is largely an understudied problem. Although small-scale scenarios can
be addressed with tools from differential game theory, existing approaches fail
to scale for large-scale multi-agent pursuit evasion (PE) scenarios. In this
work, we propose a reinforcement learning (RL) based framework to decompose to
large-scale swarm engagement problems into a number of independent multi-agent
pursuit-evasion games. We simulate a variety of multi-agent PE scenarios, where
finite time capture is guaranteed under certain conditions. The calculated PE
statistics are provided as a reward signal to the high level allocation layer,
which uses an RL algorithm to allocate controlled swarm units to eliminate
enemy swarm units with maximum efficiency. We verify our approach in
large-scale swarm-to-swarm engagement simulations.",None,-1
Video Anomaly Detection via Prediction Network with Enhanced Spatio-Temporal Memory Exchange,0.292573,"Video anomaly detection is a challenging task because most anomalies are
scarce and non-deterministic. Many approaches investigate the reconstruction
difference between normal and abnormal patterns, but neglect that anomalies do
not necessarily correspond to large reconstruction errors. To address this
issue, we design a Convolutional LSTM Auto-Encoder prediction framework with
enhanced spatio-temporal memory exchange using bi-directionalilty and a
higher-order mechanism. The bi-directional structure promotes learning the
temporal regularity through forward and backward predictions. The unique
higher-order mechanism further strengthens spatial information interaction
between the encoder and the decoder. Considering the limited receptive fields
in Convolutional LSTMs, we also introduce an attention module to highlight
informative features for prediction. Anomalies are eventually identified by
comparing the frames with their corresponding predictions. Evaluations on three
popular benchmarks show that our framework outperforms most existing
prediction-based anomaly detection methods.",None,-1
Data augmentation on graphs for table type classification,0.09767,"Tables are widely used in documents because of their compact and structured
representation of information. In particular, in scientific papers, tables can
sum up novel discoveries and summarize experimental results, making the
research comparable and easily understandable by scholars. Since the layout of
tables is highly variable, it would be useful to interpret their content and
classify them into categories. This could be helpful to directly extract
information from scientific papers, for instance comparing performance of some
models given their paper result tables. In this work, we address the
classification of tables using a Graph Neural Network, exploiting the table
structure for the message passing algorithm in use. We evaluate our model on a
subset of the Tab2Know dataset. Since it contains few examples manually
annotated, we propose data augmentation techniques directly on the table graph
structures. We achieve promising preliminary results, proposing a data
augmentation method suitable for graph-based table representation.",https://github.com/tabulapdf/tabula,-1
Neural apparent BRDF fields for multiview photometric stereo,0.44494,"We propose to tackle the multiview photometric stereo problem using an
extension of Neural Radiance Fields (NeRFs), conditioned on light source
direction. The geometric part of our neural representation predicts surface
normal direction, allowing us to reason about local surface reflectance. The
appearance part of our neural representation is decomposed into a neural
bidirectional reflectance function (BRDF), learnt as part of the fitting
process, and a shadow prediction network (conditioned on light source
direction) allowing us to model the apparent BRDF. This balance of learnt
components with inductive biases based on physical image formation models
allows us to extrapolate far from the light source and viewer directions
observed during training. We demonstrate our approach on a multiview
photometric stereo benchmark and show that competitive performance can be
obtained with the neural density representation of a NeRF.",None,-1
HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction,0.99988,"We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,
to catalyze the research of category-level human-object interaction. HOI4D
consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by
4 participants interacting with 800 different object instances from 16
categories over 610 different indoor rooms. Frame-wise annotations for panoptic
segmentation, motion segmentation, 3D hand pose, category-level object pose and
hand action have also been provided, together with reconstructed object meshes
and scene point clouds. With HOI4D, we establish three benchmarking tasks to
promote category-level HOI from 4D visual signals including semantic
segmentation of 4D dynamic point cloud sequences, category-level object pose
tracking, and egocentric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and
produces great research opportunities.",None,-1
Holistic Transformer: A Joint Neural Network for Trajectory Prediction and Decision-Making of Autonomous Vehicles,0.453538,"Trajectory prediction and behavioral decision-making are two important tasks
for autonomous vehicles that require good understanding of the environmental
context; behavioral decisions are better made by referring to the outputs of
trajectory predictions. However, most current solutions perform these two tasks
separately. Therefore, a joint neural network that combines multiple cues is
proposed and named as the holistic transformer to predict trajectories and make
behavioral decisions simultaneously. To better explore the intrinsic
relationships between cues, the network uses existing knowledge and adopts
three kinds of attention mechanisms: the sparse multi-head type for reducing
noise impact, feature selection sparse type for optimally using partial prior
knowledge, and multi-head with sigmoid activation type for optimally using
posteriori knowledge. Compared with other trajectory prediction models, the
proposed model has better comprehensive performance and good interpretability.
Perceptual noise robustness experiments demonstrate that the proposed model has
good noise robustness. Thus, simultaneous trajectory prediction and behavioral
decision-making combining multiple cues can reduce computational costs and
enhance semantic relationships between scenes and agents.",None,-1
Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering,0.900871,"Recent works on knowledge base question answering (KBQA) retrieve subgraphs
for easier reasoning. A desired subgraph is crucial as a small one may exclude
the answer but a large one might introduce more noises. However, the existing
retrieval is either heuristic or interwoven with the reasoning, causing
reasoning on the partial subgraphs, which increases the reasoning bias when the
intermediate supervision is missing. This paper proposes a trainable subgraph
retriever (SR) decoupled from the subsequent reasoning process, which enables a
plug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive
experiments demonstrate SR achieves significantly better retrieval and QA
performance than existing retrieval methods. Via weakly supervised pre-training
as well as the end-to-end fine-tuning, SRl achieves new state-of-the-art
performance when combined with NSM, a subgraph-oriented reasoner, for
embedding-based KBQA methods.",https://github.com/RUCKBReasoning/SubgraphRetrievalKBQA,-1
Semi-supervised New Event Type Induction and Description via Contrastive Loss-Enforced Batch Attention,0.575568,"Most event extraction methods have traditionally relied on an annotated set
of event types. However, creating event ontologies and annotating supervised
training data are expensive and time-consuming. Previous work has proposed
semi-supervised approaches which leverage seen (annotated) types to learn how
to automatically discover new event types. State-of-the-art methods, both
semi-supervised or fully unsupervised, use a form of reconstruction loss on
specific tokens in a context. In contrast, we present a novel approach to
semi-supervised new event type induction using a masked contrastive loss, which
learns similarities between event mentions by enforcing an attention mechanism
over the data minibatch. We further disentangle the discovered clusters by
approximating the underlying manifolds in the data, which allows us to increase
normalized mutual information and Fowlkes-Mallows scores by over 20% absolute.
Building on these clustering results, we extend our approach to two new tasks:
predicting the type name of the discovered clusters and linking them to
FrameNet frames.",None,-1
Event Transformer. A sparse-aware solution for efficient event data processing,0.917554,"Event cameras are sensors of great interest for many applications that run in
low-resource and challenging environments. They log sparse illumination changes
with high temporal resolution and high dynamic range, while they present
minimal power consumption. However, top-performing methods often ignore
specific event-data properties, leading to the development of generic but
computationally expensive algorithms. Efforts toward efficient solutions
usually do not achieve top-accuracy results for complex tasks. This work
proposes a novel framework, Event Transformer (EvT), that effectively takes
advantage of event-data properties to be highly efficient and accurate. We
introduce a new patch-based event representation and a compact transformer-like
architecture to process it. EvT is evaluated on different event-based
benchmarks for action and gesture recognition. Evaluation results show better
or comparable accuracy to the state-of-the-art while requiring significantly
less computation resources, which makes EvT able to work with minimal latency
both on GPU and CPU.",https://github.com/AlbertoSabater/EventTransformer,6081
Object Delineation in Satellite Images,0.304626,"Machine learning is being widely applied to analyze satellite data with
problems such as classification and feature detection. Unlike traditional image
processing algorithms, geospatial applications need to convert the detected
objects from a raster form to a geospatial vector form to further analyze it.
This gem delivers a simple and light-weight algorithm for delineating the
pixels that are marked by ML algorithms to extract geospatial objects from
satellite images. The proposed algorithm is exact and users can further apply
simplification and approximation based on the application needs.",None,-1
Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training,0.600442,"In cross-lingual speech synthesis, the speech in various languages can be
synthesized for a monoglot speaker. Normally, only the data of monoglot
speakers are available for model training, thus the speaker similarity is
relatively low between the synthesized cross-lingual speech and the native
language recordings. Based on the multilingual transformer text-to-speech
model, this paper studies a multi-task learning framework to improve the
cross-lingual speaker similarity. To further improve the speaker similarity,
joint training with a speaker classifier is proposed. Here, a scheme similar to
parallel scheduled sampling is proposed to train the transformer model
efficiently to avoid breaking the parallel training mechanism when introducing
joint training. By using multi-task learning and speaker classifier joint
training, in subjective and objective evaluations, the cross-lingual speaker
similarity can be consistently improved for both the seen and unseen speakers
in the training set.",None,-1
Ethical Design of Computers: From Semiconductors to IoT and Artificial Intelligence,0.249199,"Computing systems are tightly integrated today into our professional, social,
and private lives. An important consequence of this growing ubiquity of
computing is that it can have significant ethical implications of which
computing professionals should take account. In most real-world scenarios, it
is not immediately obvious how particular technical choices during the design
and use of computing systems could be viewed from an ethical perspective. This
article provides a perspective on the ethical challenges within semiconductor
chip design, IoT applications, and the increasing use of artificial
intelligence in the design processes, tools, and hardware-software stacks of
these systems.",None,21094
Uncertainty Quantification for Competency Assessment of Autonomous Agents,0.205691,"For safe and reliable deployment in the real world, autonomous agents must
elicit appropriate levels of trust from human users. One method to build trust
is to have agents assess and communicate their own competencies for performing
given tasks. Competency depends on the uncertainties affecting the agent,
making accurate uncertainty quantification vital for competency assessment. In
this work, we show how ensembles of deep generative models can be used to
quantify the agent's aleatoric and epistemic uncertainties when forecasting
task outcomes as part of competency assessment.",None,-1
On three types of $L$-fuzzy $β$-covering-based rough sets,0.372911,"In this paper, we mainly construct three types of $L$-fuzzy
$\beta$-covering-based rough set models and study the axiom sets, matrix
representations and interdependency of these three pairs of $L$-fuzzy
$\beta$-covering-based rough approximation operators. Firstly, we propose three
pairs of $L$-fuzzy $\beta$-covering-based rough approximation operators by
introducing the concepts such as $\beta$-degree of intersection and
$\beta$-subsethood degree, which are generalizations of degree of intersection
and subsethood degree, respectively. And then, the axiom set for each of these
$L$-fuzzy $\beta$-covering-based rough approximation operator is investigated.
Thirdly, we give the matrix representations of three types of $L$-fuzzy
$\beta$-covering-based rough approximation operators, which make it valid to
calculate the $L$-fuzzy $\beta$-covering-based lower and upper rough
approximation operators through operations on matrices. Finally, the
interdependency of the three pairs of rough approximation operators based on
$L$-fuzzy $\beta$-covering is studied by using the notion of reducible elements
and independent elements. In other words, we present the necessary and
sufficient conditions under which two $L$-fuzzy $\beta$-coverings can generate
the same lower and upper rough approximation operations.",None,7646
MOVE: Unsupervised Movable Object Segmentation and Detection,0.560396,"We introduce MOVE, a novel method to segment objects without any form of
supervision. MOVE exploits the fact that foreground objects can be shifted
locally relative to their initial position and result in realistic
(undistorted) new images. This property allows us to train a segmentation model
on a dataset of images without annotation and to achieve state of the art
(SotA) performance on several evaluation datasets for unsupervised salient
object detection and segmentation. In unsupervised single object discovery,
MOVE gives an average CorLoc improvement of 7.2% over the SotA, and in
unsupervised class-agnostic object detection it gives a relative AP improvement
of 53% on average. Our approach is built on top of self-supervised features
(e.g. from DINO or MAE), an inpainting network (based on the Masked
AutoEncoder) and adversarial training.",None,-1
Dimension-adaptive machine-learning-based quantum state reconstruction,0.0842046,"We introduce an approach for performing quantum state reconstruction on
systems of $n$ qubits using a machine-learning-based reconstruction system
trained exclusively on $m$ qubits, where $m\geq n$. This approach removes the
necessity of exactly matching the dimensionality of a system under
consideration with the dimension of a model used for training. We demonstrate
our technique by performing quantum state reconstruction on randomly sampled
systems of one, two, and three qubits using machine-learning-based methods
trained exclusively on systems containing at least one additional qubit. The
reconstruction time required for machine-learning-based methods scales
significantly more favorably than the training time; hence this technique can
offer an overall savings of resources by leveraging a single neural network for
dimension-variable state reconstruction, obviating the need to train dedicated
machine-learning systems for each Hilbert space.",https://github.com/slohani-ai/machine-learning-for-physical-sciences,-1
Enhanced Deep Animation Video Interpolation,0.30902,"Existing learning-based frame interpolation algorithms extract consecutive
frames from high-speed natural videos to train the model. Compared to natural
videos, cartoon videos are usually in a low frame rate. Besides, the motion
between consecutive cartoon frames is typically nonlinear, which breaks the
linear motion assumption of interpolation algorithms. Thus, it is unsuitable
for generating a training set directly from cartoon videos. For better adapting
frame interpolation algorithms from nature video to animation video, we present
AutoFI, a simple and effective method to automatically render training data for
deep animation video interpolation. AutoFI takes a layered architecture to
render synthetic data, which ensures the assumption of linear motion.
Experimental results show that AutoFI performs favorably in training both DAIN
and ANIN. However, most frame interpolation algorithms will still fail in
error-prone areas, such as fast motion or large occlusion. Besides AutoFI, we
also propose a plug-and-play sketch-based post-processing module, named SktFI,
to refine the final results using user-provided sketches manually. With AutoFI
and SktFI, the interpolated animation frames show high perceptual quality.",https://github.com/laomao0/AutoSktFI,-1
A Benchmark for Out of Distribution Detection in Point Cloud 3D Semantic Segmentation,0.122161,"Safety-critical applications like autonomous driving use Deep Neural Networks
(DNNs) for object detection and segmentation. The DNNs fail to predict when
they observe an Out-of-Distribution (OOD) input leading to catastrophic
consequences. Existing OOD detection methods were extensively studied for image
inputs but have not been explored much for LiDAR inputs. So in this study, we
proposed two datasets for benchmarking OOD detection in 3D semantic
segmentation. We used Maximum Softmax Probability and Entropy scores generated
using Deep Ensembles and Flipout versions of RandLA-Net as OOD scores. We
observed that Deep Ensembles out perform Flipout model in OOD detection with
greater AUROC scores for both datasets.",None,-1
A Song of (Dis)agreement: Evaluating the Evaluation of Explainable Artificial Intelligence in Natural Language Processing,0.570345,"There has been significant debate in the NLP community about whether or not
attention weights can be used as an explanation - a mechanism for interpreting
how important each input token is for a particular prediction. The validity of
""attention as explanation"" has so far been evaluated by computing the rank
correlation between attention-based explanations and existing feature
attribution explanations using LSTM-based models. In our work, we (i) compare
the rank correlation between five more recent feature attribution methods and
two attention-based methods, on two types of NLP tasks, and (ii) extend this
analysis to also include transformer-based models. We find that attention-based
explanations do not correlate strongly with any recent feature attribution
methods, regardless of the model or task. Furthermore, we find that none of the
tested explanations correlate strongly with one another for the
transformer-based model, leading us to question the underlying assumption that
we should measure the validity of attention-based explanations based on how
well they correlate with existing feature attribution explanation methods.
After conducting experiments on five datasets using two different models, we
argue that the community should stop using rank correlation as an evaluation
metric for attention-based explanations. We suggest that researchers and
practitioners should instead test various explanation methods and employ a
human-in-the-loop process to determine if the explanations align with human
intuition for the particular use case at hand.",None,-1
3DMODT: Attention-Guided Affinities for Joint Detection & Tracking in 3D Point Clouds,0.369262,"We propose a method for joint detection and tracking of multiple objects in
3D point clouds, a task conventionally treated as a two-step process comprising
object detection followed by data association. Our method embeds both steps
into a single end-to-end trainable network eliminating the dependency on
external object detectors. Our model exploits temporal information employing
multiple frames to detect objects and track them in a single network, thereby
making it a utilitarian formulation for real-world scenarios. Computing
affinity matrix by employing features similarity across consecutive point cloud
scans forms an integral part of visual tracking. We propose an attention-based
refinement module to refine the affinity matrix by suppressing erroneous
correspondences. The module is designed to capture the global context in
affinity matrix by employing self-attention within each affinity matrix and
cross-attention across a pair of affinity matrices. Unlike competing
approaches, our network does not require complex post-processing algorithms,
and processes raw LiDAR frames to directly output tracking results. We
demonstrate the effectiveness of our method on the three tracking benchmarks:
JRDB, Waymo, and KITTI. Experimental evaluations indicate the ability of our
model to generalize well across datasets.",None,-1
Online Coreference Resolution for Dialogue Processing: Improving Mention-Linking on Real-Time Conversations,0.26113,"This paper suggests a direction of coreference resolution for online decoding
on actively generated input such as dialogue, where the model accepts an
utterance and its past context, then finds mentions in the current utterance as
well as their referents, upon each dialogue turn. A baseline and four
incremental-updated models adapted from the mention-linking paradigm are
proposed for this new setting, which address different aspects including the
singletons, speaker-grounded encoding and cross-turn mention contextualization.
Our approach is assessed on three datasets: Friends, OntoNotes, and BOLT.
Results show that each aspect brings out steady improvement, and our best
models outperform the baseline by over 10%, presenting an effective system for
this setting. Further analysis highlights the task characteristics, such as the
significance of addressing the mention recall.",https://github.com/emorynlp/elit,-1
Transformers are Sample-Efficient World Models,0.792866,"Deep reinforcement learning agents are notoriously sample inefficient, which
considerably limits their application to real-world problems. Recently, many
model-based methods have been designed to address this issue, with learning in
the imagination of a world model being one of the most prominent approaches.
However, while virtually unlimited interaction with a simulated environment
sounds appealing, the world model has to be accurate over extended periods of
time. Motivated by the success of Transformers in sequence modeling tasks, we
introduce IRIS, a data-efficient agent that learns in a world model composed of
a discrete autoencoder and an autoregressive Transformer. With the equivalent
of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean
human normalized score of 1.046, and outperforms humans on 10 out of 26 games,
setting a new state of the art for methods without lookahead search. To foster
future research on Transformers and world models for sample-efficient
reinforcement learning, we release our code and models at
https://github.com/eloialonso/iris.",https://github.com/eloialonso/iris,-1
Unknown-Aware Domain Adversarial Learning for Open-Set Domain Adaptation,0.896969,"Open-Set Domain Adaptation (OSDA) assumes that a target domain contains
unknown classes, which are not discovered in a source domain. Existing domain
adversarial learning methods are not suitable for OSDA because distribution
matching with $\textit{unknown}$ classes leads to negative transfer. Previous
OSDA methods have focused on matching the source and the target distribution by
only utilizing $\textit{known}$ classes. However, this $\textit{known}$-only
matching may fail to learn the target-$\textit{unknown}$ feature space.
Therefore, we propose Unknown-Aware Domain Adversarial Learning (UADAL), which
$\textit{aligns}$ the source and the target-$\textit{known}$ distribution while
simultaneously $\textit{segregating}$ the target-$\textit{unknown}$
distribution in the feature alignment procedure. We provide theoretical
analyses on the optimized state of the proposed $\textit{unknown-aware}$
feature alignment, so we can guarantee both $\textit{alignment}$ and
$\textit{segregation}$ theoretically. Empirically, we evaluate UADAL on the
benchmark datasets, which shows that UADAL outperforms other methods with
better feature alignments by reporting state-of-the-art performances.",https://github.com/JoonHo-Jang/UADAL,-1
Unsupervised 4D LiDAR Moving Object Segmentation in Stationary Settings with Multivariate Occupancy Time Series,0.607655,"In this work, we address the problem of unsupervised moving object
segmentation (MOS) in 4D LiDAR data recorded from a stationary sensor, where no
ground truth annotations are involved. Deep learning-based state-of-the-art
methods for LiDAR MOS strongly depend on annotated ground truth data, which is
expensive to obtain and scarce in existence. To close this gap in the
stationary setting, we propose a novel 4D LiDAR representation based on
multivariate time series that relaxes the problem of unsupervised MOS to a time
series clustering problem. More specifically, we propose modeling the change in
occupancy of a voxel by a multivariate occupancy time series (MOTS), which
captures spatio-temporal occupancy changes on the voxel level and its
surrounding neighborhood. To perform unsupervised MOS, we train a neural
network in a self-supervised manner to encode MOTS into voxel-level feature
representations, which can be partitioned by a clustering algorithm into moving
or stationary. Experiments on stationary scenes from the Raw KITTI dataset show
that our fully unsupervised approach achieves performance that is comparable to
that of supervised state-of-the-art approaches.",https://github.com/thkreutz/umosmots,-1
Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP),0.983459,"Contrastively trained language-image models such as CLIP, ALIGN, and BASIC
have demonstrated unprecedented robustness to multiple challenging natural
distribution shifts. Since these language-image models differ from previous
training approaches in several ways, an important question is what causes the
large robustness gains. We answer this question via a systematic experimental
investigation. Concretely, we study five different possible causes for the
robustness gains: (i) the training set size, (ii) the training distribution,
(iii) language supervision at training time, (iv) language supervision at test
time, and (v) the contrastive loss function. Our experiments show that the more
diverse training distribution is the main cause for the robustness gains, with
the other factors contributing little to no robustness. Beyond our experimental
results, we also introduce ImageNet-Captions, a version of ImageNet with
original text annotations from Flickr, to enable further controlled experiments
of language-image training.",None,-1
Neural Weight Search for Scalable Task Incremental Learning,0.0929746,"Task incremental learning aims to enable a system to maintain its performance
on previously learned tasks while learning new tasks, solving the problem of
catastrophic forgetting. One promising approach is to build an individual
network or sub-network for future tasks. However, this leads to an ever-growing
memory due to saving extra weights for new tasks and how to address this issue
has remained an open problem in task incremental learning. In this paper, we
introduce a novel Neural Weight Search technique that designs a fixed search
space where the optimal combinations of frozen weights can be searched to build
new models for novel tasks in an end-to-end manner, resulting in scalable and
controllable memory growth. Extensive experiments on two benchmarks, i.e.,
Split-CIFAR-100 and CUB-to-Sketches, show our method achieves state-of-the-art
performance with respect to both average inference accuracy and total memory
cost.",https://github.com/JianJiangKCL/NeuralWeightSearch,-1
On the Effect of Anticipation on Reading Times,0.726785,"Over the past two decades, numerous studies have demonstrated how less
predictable (i.e., higher surprisal) words take more time to read. In general,
these studies have implicitly assumed the reading process is purely responsive:
Readers observe a new word and allocate time to process it as required. We
argue that prior results are also compatible with a reading process that is at
least partially anticipatory: Readers could make predictions about a future
word and allocate time to process it based on their expectation. In this work,
we operationalize this anticipation as a word's contextual entropy. We assess
the effect of anticipation on reading by comparing how well surprisal and
contextual entropy predict reading times on four naturalistic reading datasets:
two self-paced and two eye-tracking. Experimentally, across datasets and
analyses, we find substantial evidence for effects of contextual entropy over
surprisal on a word's reading time (RT): in fact, entropy is sometimes better
than surprisal in predicting a word's RT. Spillover effects, however, are
generally not captured by entropy, but only by surprisal. Further, we
hypothesize four cognitive mechanisms through which contextual entropy could
impact RTs -- three of which we are able to design experiments to analyze.
Overall, our results support a view of reading that is not just responsive, but
also anticipatory.",https://github.com/rycolab/anticipation-on-reading-times,-1
Model-based Reinforcement Learning with Multi-step Plan Value Estimation,0.415363,"A promising way to improve the sample efficiency of reinforcement learning is
model-based methods, in which many explorations and evaluations can happen in
the learned models to save real-world samples. However, when the learned model
has a non-negligible model error, sequential steps in the model are hard to be
accurately evaluated, limiting the model's utilization. This paper proposes to
alleviate this issue by introducing multi-step plans to replace multi-step
actions for model-based RL. We employ the multi-step plan value estimation,
which evaluates the expected discounted return after executing a sequence of
action plans at a given state, and updates the policy by directly computing the
multi-step policy gradient via plan value estimation. The new model-based
reinforcement learning algorithm MPPVE (Model-based Planning Policy Learning
with Multi-step Plan Value Estimation) shows a better utilization of the
learned model and achieves a better sample efficiency than state-of-the-art
model-based RL approaches.",None,-1
An anomaly detection approach for backdoored neural networks: face recognition as a case study,0.169007,"Backdoor attacks allow an attacker to embed functionality jeopardizing proper
behavior of any algorithm, machine learning or not. This hidden functionality
can remain inactive for normal use of the algorithm until activated by the
attacker. Given how stealthy backdoor attacks are, consequences of these
backdoors could be disastrous if such networks were to be deployed for
applications as critical as border or access control. In this paper, we propose
a novel backdoored network detection method based on the principle of anomaly
detection, involving access to the clean part of the training data and the
trained network. We highlight its promising potential when considering various
triggers, locations and identity pairs, without the need to make any
assumptions on the nature of the backdoor and its setup. We test our method on
a novel dataset of backdoored networks and report detectability results with
perfect scores.",https://gitlab.idiap.ch/bob/bob.paper.backdoors_anomaly_detection.biosig2022,18877
Learning to Estimate Shapley Values with Vision Transformers,0.61673,"Transformers have become a default architecture in computer vision, but
understanding what drives their predictions remains a challenging problem.
Current explanation approaches rely on attention values or input gradients, but
these provide a limited view of a model's dependencies. Shapley values offer a
theoretically sound alternative, but their computational cost makes them
impractical for large, high-dimensional models. In this work, we aim to make
Shapley values practical for vision transformers (ViTs). To do so, we first
leverage an attention masking approach to evaluate ViTs with partial
information, and we then develop a procedure to generate Shapley value
explanations via a separate, learned explainer model. Our experiments compare
Shapley values to many baseline methods (e.g., attention rollout, GradCAM,
LRP), and we find that our approach provides more accurate explanations than
existing methods for ViTs.",https://github.com/suinleelab/vit-shapley,-1
Towards Developing Safety Assurance Cases for Learning-Enabled Medical Cyber-Physical Systems,0.139441,"Machine Learning (ML) technologies have been increasingly adopted in Medical
Cyber-Physical Systems (MCPS) to enable smart healthcare. Assuring the safety
and effectiveness of learning-enabled MCPS is challenging, as such systems must
account for diverse patient profiles and physiological dynamics and handle
operational uncertainties. In this paper, we develop a safety assurance case
for ML controllers in learning-enabled MCPS, with an emphasis on establishing
confidence in the ML-based predictions. We present the safety assurance case in
detail for Artificial Pancreas Systems (APS) as a representative application of
learning-enabled MCPS, and provide a detailed analysis by implementing a deep
neural network for the prediction in APS. We check the sufficiency of the ML
data and analyze the correctness of the ML-based prediction using formal
verification. Finally, we outline open research problems based on our
experience in this paper.",https://github.com/jxx123/simglucose,-1
ST-MoE: Designing Stable and Transferable Sparse Expert Models,0.835356,"Scale has opened new frontiers in natural language processing -- but at a
high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have
been proposed as an energy efficient path to even larger and more capable
language models. But advancing the state-of-the-art across a broad set of
natural language tasks has been hindered by training instabilities and
uncertain quality during fine-tuning. Our work focuses on these issues and acts
as a design guide. We conclude by scaling a sparse model to 269B parameters,
with a computational cost comparable to a 32B dense encoder-decoder Transformer
(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,
a sparse model achieves state-of-the-art performance in transfer learning,
across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC
Challenge), summarization (XSum, CNN-DM), closed book question answering
(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,
ANLI R3).",https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py,-1
ViGAT: Bottom-up event recognition and explanation in video using factorized graph attention network,0.391036,"In this paper a pure-attention bottom-up approach, called ViGAT, that
utilizes an object detector together with a Vision Transformer (ViT) backbone
network to derive object and frame features, and a head network to process
these features for the task of event recognition and explanation in video, is
proposed. The ViGAT head consists of graph attention network (GAT) blocks
factorized along the spatial and temporal dimensions in order to capture
effectively both local and long-term dependencies between objects or frames.
Moreover, using the weighted in-degrees (WiDs) derived from the adjacency
matrices at the various GAT blocks, we show that the proposed architecture can
identify the most salient objects and frames that explain the decision of the
network. A comprehensive evaluation study is performed, demonstrating that the
proposed approach provides state-of-the-art results on three large, publicly
available video datasets (FCVID, Mini-Kinetics, ActivityNet).",https://github.com/bmezaris/ViGAT,-1
CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation,0.873409,"Practical dialog systems need to deal with various knowledge sources, noisy
user expressions, and the shortage of annotated data. To better solve the above
problems, we propose CGoDial, new challenging and comprehensive Chinese
benchmark for multi-domain Goal-oriented Dialog evaluation. It contains 96,763
dialog sessions and 574,949 dialog turns totally, covering three datasets with
different knowledge sources: 1) a slot-based dialog (SBD) dataset with
table-formed knowledge, 2) a flow-based dialog (FBD) dataset with tree-formed
knowledge, and a retrieval-based dialog (RBD) dataset with candidate-formed
knowledge. To bridge the gap between academic benchmarks and spoken dialog
scenarios, we either collect data from real conversations or add spoken
features to existing datasets via crowd-sourcing. The proposed experimental
settings include the combinations of training with either the entire training
set or a few-shot training set, and testing with either the standard test set
or a hard test subset, which can assess model capabilities in terms of general
prediction, fast adaptability and reliable robustness.",https://github.com/Alibaba/DAMO-ConvAI/cgodial,-1
Utilizing distilBert transformer model for sentiment classification of COVID-19's Persian open-text responses,0.0876528,"The COVID-19 pandemic has caused drastic alternations in human life in all
aspects. The government's laws in this regard affected the lifestyle of all
people. Due to this fact studying the sentiment of individuals is essential to
be aware of the future impacts of the coming pandemics. To contribute to this
aim, we proposed an NLP (Natural Language Processing) model to analyze
open-text answers in a survey in Persian and detect positive and negative
feelings of the people in Iran. In this study, a distilBert transformer model
was applied to take on this task. We deployed three approaches to perform the
comparison, and our best model could gain accuracy: 0.824, Precision: 0.824,
Recall: 0.798, and F1 score: 0.804.",None,-1
Do Inpainting Yourself: Generative Facial Inpainting Guided by Exemplars,0.0313218,"We present EXE-GAN, a novel exemplar-guided facial inpainting framework using
generative adversarial networks. Our approach can not only preserve the quality
of the input facial image but also complete the image with exemplar-like facial
attributes. We achieve this by simultaneously leveraging the global style of
the input image, the stochastic style generated from the random latent code,
and the exemplar style of exemplar image. We introduce a novel attribute
similarity metric to encourage networks to learn the style of facial attributes
from the exemplar in a self-supervised way. To guarantee the natural transition
across the boundaries of inpainted regions, we introduce a novel spatial
variant gradient backpropagation technique to adjust the loss gradients based
on the spatial location. Extensive evaluations and practical applications on
public CelebA-HQ and FFHQ datasets validate the superiority of EXE-GAN in terms
of the visual quality in facial inpainting.",https://github.com/open-mmlab/mmediting,-1
Towards a Grounded Theory of Causation for Embodied AI,0.212008,"There exist well-developed frameworks for causal modelling, but these require
rather a lot of human domain expertise to define causal variables and perform
interventions. In order to enable autonomous agents to learn abstract causal
models through interactive experience, the existing theoretical foundations
need to be extended and clarified. Existing frameworks give no guidance
regarding variable choice / representation, and more importantly, give no
indication as to which behaviour policies or physical transformations of state
space shall count as interventions. The framework sketched in this paper
describes actions as transformations of state space, for instance induced by an
agent running a policy. This makes it possible to describe in a uniform way
both transformations of the micro-state space and abstract models thereof, and
say when the latter is veridical / grounded / natural. We then introduce
(causal) variables, define a mechanism as an invariant predictor, and say when
an action can be viewed as a ``surgical intervention'', thus bringing the
objective of causal representation \& intervention skill learning into clearer
focus.",None,-1
Object Localization under Single Coarse Point Supervision,0.604999,"Point-based object localization (POL), which pursues high-performance object
sensing under low-cost data annotation, has attracted increased attention.
However, the point annotation mode inevitably introduces semantic variance for
the inconsistency of annotated points. Existing POL methods heavily reply on
accurate key-point annotations which are difficult to define. In this study, we
propose a POL method using coarse point annotations, relaxing the supervision
signals from accurate key points to freely spotted points. To this end, we
propose a coarse point refinement (CPR) approach, which to our best knowledge
is the first attempt to alleviate semantic variance from the perspective of
algorithm. CPR constructs point bags, selects semantic-correlated points, and
produces semantic center points through multiple instance learning (MIL). In
this way, CPR defines a weakly supervised evolution procedure, which ensures
training high-performance object localizer under coarse point supervision.
Experimental results on COCO, DOTA and our proposed SeaPerson dataset validate
the effectiveness of the CPR approach. The dataset and code will be available
at https://github.com/ucas-vg/PointTinyBenchmark/.",https://github.com/ucas-vg/PointTinyBenchmark/,-1
"Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report",0.971254,"In September 2021, the ""One Hundred Year Study on Artificial Intelligence""
project (AI100) issued the second report of its planned long-term periodic
assessment of artificial intelligence (AI) and its impact on society. It was
written by a panel of 17 study authors, each of whom is deeply rooted in AI
research, chaired by Michael Littman of Brown University. The report, entitled
""Gathering Strength, Gathering Storms,"" answers a set of 14 questions probing
critical areas of AI development addressing the major risks and dangers of AI,
its effects on society, its public perception and the future of the field. The
report concludes that AI has made a major leap from the lab to people's lives
in recent years, which increases the urgency to understand its potential
negative effects. The questions were developed by the AI100 Standing Committee,
chaired by Peter Stone of the University of Texas at Austin, consisting of a
group of AI leaders with expertise in computer science, sociology, ethics,
economics, and other disciplines.",None,-1
"Great Expectations: Unsupervised Inference of Suspense, Surprise and Salience in Storytelling",0.166864,"Stories interest us not because they are a sequence of mundane and
predictable events but because they have drama and tension. Crucial to creating
dramatic and exciting stories are surprise and suspense. The thesis trains a
series of deep learning models via only reading stories, a self-supervised (or
unsupervised) system. Narrative theory methods (rules and procedures) are
applied to the knowledge built into deep learning models to directly infer
salience, surprise, and salience in stories. Extensions add memory and external
knowledge from story plots and from Wikipedia to infer salience on novels such
as Great Expectations and plays such as Macbeth. Other work adapts the models
as a planning system for generating original stories.
  The thesis finds that applying the narrative theory to deep learning models
can align with the typical reader. In follow-up work, the insights could help
improve computer models for tasks such as automatic story writing and
assistance for writing, summarising or editing stories. Moreover, the approach
of applying narrative theory to the inherent qualities built in a system that
learns itself (self-supervised) from reading from books, watching videos, and
listening to audio is much cheaper and more adaptable to other domains and
tasks. Progress is swift in improving self-supervised systems. As such, the
thesis's relevance is that applying domain expertise with these systems may be
a more productive approach for applying machine learning in many areas of
interest.",https://github.com/dwlmt/story-fragments/,-1
Automatic Semantic Modeling for Structural Data Source with the Prior Knowledge from Knowledge Base,0.203854,"A critical step in sharing semantic content online is to map the structural
data source to a public domain ontology. This problem is denoted as the
Relational-To-Ontology Mapping Problem (Rel2Onto). A huge effort and expertise
are required for manually modeling the semantics of data. Therefore, an
automatic approach for learning the semantics of a data source is desirable.
Most of the existing work studies the semantic annotation of source attributes.
However, although critical, the research for automatically inferring the
relationships between attributes is very limited. In this paper, we propose a
novel method for semantically annotating structured data sources using machine
learning, graph matching and modified frequent subgraph mining to amend the
candidate model. In our work, Knowledge graph is used as prior knowledge. Our
evaluation shows that our approach outperforms two state-of-the-art solutions
in tricky cases where only a few semantic models are known.",https://github.com/Zaiwen/ModelCorrection,-1
TOKEN is a MASK: Few-shot Named Entity Recognition with Pre-trained Language Models,0.28003,"Transferring knowledge from one domain to another is of practical importance
for many tasks in natural language processing, especially when the amount of
available data in the target domain is limited. In this work, we propose a
novel few-shot approach to domain adaptation in the context of Named Entity
Recognition (NER). We propose a two-step approach consisting of a variable base
module and a template module that leverages the knowledge captured in
pre-trained language models with the help of simple descriptive patterns. Our
approach is simple yet versatile and can be applied in few-shot and zero-shot
settings. Evaluating our lightweight approach across a number of different
datasets shows that it can boost the performance of state-of-the-art baselines
by 2-5% F1-score.",None,-1
SuperCon: Supervised Contrastive Learning for Imbalanced Skin Lesion Classification,0.292919,"Convolutional neural networks (CNNs) have achieved great success in skin
lesion classification. A balanced dataset is required to train a good model.
However, due to the appearance of different skin lesions in practice, severe or
even deadliest skin lesion types (e.g., melanoma) naturally have quite small
amount represented in a dataset. In that, classification performance
degradation occurs widely, it is significantly important to have CNNs that work
well on class imbalanced skin lesion image dataset. In this paper, we propose
SuperCon, a two-stage training strategy to overcome the class imbalance problem
on skin lesion classification. It contains two stages: (i) representation
training that tries to learn a feature representation that closely aligned
among intra-classes and distantly apart from inter-classes, and (ii) classifier
fine-tuning that aims to learn a classifier that correctly predict the label
based on the learnt representations. In the experimental evaluation, extensive
comparisons have been made among our approach and other existing approaches on
skin lesion benchmark datasets. The results show that our two-stage training
strategy effectively addresses the class imbalance classification problem, and
significantly improves existing works in terms of F1-score and AUC score,
resulting in state-of-the-art performance.",https://github.com/keyu07/SuperCon_ISIC,-1
Multi-level Consistency Learning for Semi-supervised Domain Adaptation,0.586545,"Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from
a fully labeled source domain to a scarcely labeled target domain. In this
paper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA.
Specifically, our MCL regularizes the consistency of different views of target
domain samples at three levels: (i) at inter-domain level, we robustly and
accurately align the source and target domains using a prototype-based optimal
transport method that utilizes the pros and cons of different views of target
samples; (ii) at intra-domain level, we facilitate the learning of both
discriminative and compact target feature representations by proposing a novel
class-wise contrastive clustering loss; (iii) at sample level, we follow
standard practice and improve the prediction accuracy by conducting a
consistency-based self-training. Empirically, we verified the effectiveness of
our MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet,
and Office-Home datasets, and the experimental results demonstrate that our MCL
framework achieves the state-of-the-art performance.",https://github.com/chester256/MCL,30637
Generalizing Math Word Problem Solvers via Solution Diversification,0.48373,"Current math word problem (MWP) solvers are usually Seq2Seq models trained by
the (one-problem; one-solution) pairs, each of which is made of a problem
description and a solution showing reasoning flow to get the correct answer.
However, one MWP problem naturally has multiple solution equations. The
training of an MWP solver with (one-problem; one-solution) pairs excludes other
correct solutions, and thus limits the generalizability of the MWP solver. One
feasible solution to this limitation is to augment multiple solutions to a
given problem. However, it is difficult to collect diverse and accurate augment
solutions through human efforts. In this paper, we design a new training
framework for an MWP solver by introducing a solution buffer and a solution
discriminator. The buffer includes solutions generated by an MWP solver to
encourage the training data diversity. The discriminator controls the quality
of buffered solutions to participate in training. Our framework is flexibly
applicable to a wide setting of fully, semi-weakly and weakly supervised
training for all Seq2Seq MWP solvers. We conduct extensive experiments on a
benchmark dataset Math23k and a new dataset named Weak12k, and show that our
framework improves the performance of various MWP solvers under different
settings by generating correct and diverse solutions.",https://github.com/LZhenwen/Solution Diversity,-1
Evaluating Explainability for Graph Neural Networks,0.887414,"As post hoc explanations are increasingly used to understand the behavior of
graph neural networks (GNNs), it becomes crucial to evaluate the quality and
reliability of GNN explanations. However, assessing the quality of GNN
explanations is challenging as existing graph datasets have no or unreliable
ground-truth explanations for a given task. Here, we introduce a synthetic
graph data generator, ShapeGGen, which can generate a variety of benchmark
datasets (e.g., varying graph sizes, degree distributions, homophilic vs.
heterophilic graphs) accompanied by ground-truth explanations. Further, the
flexibility to generate diverse synthetic datasets and corresponding
ground-truth explanations allows us to mimic the data generated by various
real-world applications. We include ShapeGGen and several real-world graph
datasets into an open-source graph explainability library, GraphXAI. In
addition to synthetic and real-world graph datasets with ground-truth
explanations, GraphXAI provides data loaders, data processing functions,
visualizers, GNN model implementations, and evaluation metrics to benchmark the
performance of GNN explainability methods.",https://github.com/mims-harvard/GraphXAI,-1
Probing for the Usage of Grammatical Number,0.913343,"A central quest of probing is to uncover how pre-trained models encode a
linguistic property within their representations. An encoding, however, might
be spurious-i.e., the model might not rely on it when making predictions. In
this paper, we try to find encodings that the model actually uses, introducing
a usage-based probing setup. We first choose a behavioral task which cannot be
solved without using the linguistic property. Then, we attempt to remove the
property by intervening on the model's representations. We contend that, if an
encoding is used by the model, its removal should harm the performance on the
chosen behavioral task. As a case study, we focus on how BERT encodes
grammatical number, and on how it uses this encoding to solve the number
agreement task. Experimentally, we find that BERT relies on a linear encoding
of grammatical number to produce the correct behavioral output. We also find
that BERT uses a separate encoding of grammatical number for nouns and verbs.
Finally, we identify in which layers information about grammatical number is
transferred from a noun to its head verb.",None,-1
Robust Planning for Human-Robot Joint Tasks with Explicit Reasoning on Human Mental State,0.306612,"We consider the human-aware task planning problem where a human-robot team is
given a shared task with a known objective to achieve. Recent approaches tackle
it by modeling it as a team of independent, rational agents, where the robot
plans for both agents' (shared) tasks. However, the robot knows that humans
cannot be administered like artificial agents, so it emulates and predicts the
human's decisions, actions, and reactions. Based on earlier approaches, we
describe a novel approach to solve such problems, which models and uses
execution-time observability conventions. Abstractly, this modeling is based on
situation assessment, which helps our approach capture the evolution of
individual agents' beliefs and anticipate belief divergences that arise in
practice. It decides if and when belief alignment is needed and achieves it
with communication. These changes improve the solver's performance: (a)
communication is effectively used, and (b) robust for more realistic and
challenging problems.",None,-1
LobsDICE: Offline Learning from Observation via Stationary Distribution Correction Estimation,0.345768,"We consider the problem of learning from observation (LfO), in which the
agent aims to mimic the expert's behavior from the state-only demonstrations by
experts. We additionally assume that the agent cannot interact with the
environment but has access to the action-labeled transition data collected by
some agents with unknown qualities. This offline setting for LfO is appealing
in many real-world scenarios where the ground-truth expert actions are
inaccessible and the arbitrary environment interactions are costly or risky. In
this paper, we present LobsDICE, an offline LfO algorithm that learns to
imitate the expert policy via optimization in the space of stationary
distributions. Our algorithm solves a single convex minimization problem, which
minimizes the divergence between the two state-transition distributions induced
by the expert and the agent policy. Through an extensive set of offline LfO
tasks, we show that LobsDICE outperforms strong baseline methods.",https://github.com/secury/optidice,-1
Worldwide city transport typology prediction with sentence-BERT based supervised learning via Wikipedia,0.649246,"An overwhelming majority of the world's human population lives in urban areas
and cities. Understanding a city's transportation typology is immensely
valuable for planners and policy makers whose decisions can potentially impact
millions of city residents. Despite the value of understanding a city's
typology, labeled data (city and it's typology) is scarce, and spans at most a
few hundred cities in the current transportation literature. To break this
barrier, we propose a supervised machine learning approach to predict a city's
typology given the information in its Wikipedia page. Our method leverages
recent breakthroughs in natural language processing, namely sentence-BERT, and
shows how the text-based information from Wikipedia can be effectively used as
a data source for city typology prediction tasks that can be applied to over
2000 cities worldwide. We propose a novel method for low-dimensional city
representation using a city's Wikipedia page, which makes supervised learning
of city typology labels tractable even with a few hundred labeled samples.
These features are used with labeled city samples to train binary classifiers
(logistic regression) for four different city typologies: (i) congestion, (ii)
auto-heavy, (iii) transit-heavy, and (iv) bike-friendly cities resulting in
reasonably high AUC scores of 0.87, 0.86, 0.61 and 0.94 respectively. Our
approach provides sufficient flexibility for incorporating additional variables
in the city typology models and can be applied to study other city typologies
as well. Our findings can assist a diverse group of stakeholders in
transportation and urban planning fields, and opens up new opportunities for
using text-based information from Wikipedia (or similar platforms) as data
sources in such fields.",None,-1
Monotonic segmental attention for automatic speech recognition,0.463446,"We introduce a novel segmental-attention model for automatic speech
recognition. We restrict the decoder attention to segments to avoid quadratic
runtime of global attention, better generalize to long sequences, and
eventually enable streaming. We directly compare global-attention and different
segmental-attention modeling variants. We develop and compare two separate
time-synchronous decoders, one specifically taking the segmental nature into
account, yielding further improvements. Using time-synchronous decoding for
segmental models is novel and a step towards streaming applications. Our
experiments show the importance of a length model to predict the segment
boundaries. The final best segmental-attention model using segmental decoding
performs better than global-attention, in contrast to other monotonic attention
approaches in the literature. Further, we observe that the segmental model
generalizes much better to long sequences of up to several minutes.",https://github.com/rwth-i6/returnn-experiments/tree/master/,-1
SpeedFolding: Learning Efficient Bimanual Folding of Garments,0.952553,"Folding garments reliably and efficiently is a long standing challenge in
robotic manipulation due to the complex dynamics and high dimensional
configuration space of garments. An intuitive approach is to initially
manipulate the garment to a canonical smooth configuration before folding. In
this work, we develop SpeedFolding, a reliable and efficient bimanual system,
which given user-defined instructions as folding lines, manipulates an
initially crumpled garment to (1) a smoothed and (2) a folded configuration.
Our primary contribution is a novel neural network architecture that is able to
predict pairs of gripper poses to parameterize a diverse set of bimanual action
primitives. After learning from 4300 human-annotated and self-supervised
actions, the robot is able to fold garments from a random initial configuration
in under 120s on average with a success rate of 93%. Real-world experiments
show that the system is able to generalize to unseen garments of different
color, shape, and stiffness. While prior work achieved 3-6 Folds Per Hour
(FPH), SpeedFolding achieves 30-40 FPH.",None,-1
Leveraging Language for Accelerated Learning of Tool Manipulation,0.59889,"Robust and generalized tool manipulation requires an understanding of the
properties and affordances of different tools. We investigate whether
linguistic information about a tool (e.g., its geometry, common uses) can help
control policies adapt faster to new tools for a given task. We obtain diverse
descriptions of various tools in natural language and use pre-trained language
models to generate their feature representations. We then perform
language-conditioned meta-learning to learn policies that can efficiently adapt
to new tools given their corresponding text descriptions. Our results
demonstrate that combining linguistic information and meta-learning
significantly accelerates tool learning in several manipulation tasks including
pushing, lifting, sweeping, and hammering.",None,-1
On Rate-Distortion Theory in Capacity-Limited Cognition & Reinforcement Learning,0.303438,"Throughout the cognitive-science literature, there is widespread agreement
that decision-making agents operating in the real world do so under limited
information-processing capabilities and without access to unbounded cognitive
or computational resources. Prior work has drawn inspiration from this fact and
leveraged an information-theoretic model of such behaviors or policies as
communication channels operating under a bounded rate constraint. Meanwhile, a
parallel line of work also capitalizes on the same principles from
rate-distortion theory to formalize capacity-limited decision making through
the notion of a learning target, which facilitates Bayesian regret bounds for
provably-efficient learning algorithms. In this paper, we aim to elucidate this
latter perspective by presenting a brief survey of these information-theoretic
models of capacity-limited decision making in biological and artificial agents.",None,-1
Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text,0.759209,"As text generated by large language models proliferates, it becomes vital to
understand how humans engage with such text, and whether or not they are able
to detect when the text they are reading did not originate with a human writer.
Prior work on human detection of generated text focuses on the case where an
entire passage is either human-written or machine-generated. In this paper, we
study a more realistic setting where text begins as human-written and
transitions to being generated by state-of-the-art neural language models. We
show that, while annotators often struggle at this task, there is substantial
variance in annotator skill and that given proper incentives, annotators can
improve at this task over time. Furthermore, we conduct a detailed comparison
study and analyze how a variety of variables (model size, decoding strategy,
fine-tuning, prompt genre, etc.) affect human detection performance. Finally,
we collect error annotations from our participants and use them to show that
certain textual genres influence models to make different types of errors and
that certain sentence-level features correlate highly with annotator selection.
We release the RoFT dataset: a collection of over 21,000 human annotations
paired with error classifications to encourage future work in human detection
and evaluation of generated text.",https://github.com/liamdugan/human-detection,-1
Towards Rapid Prototyping and Comparability in Active Learning for Deep Object Detection,0.132611,"Active learning as a paradigm in deep learning is especially important in
applications involving intricate perception tasks such as object detection
where labels are difficult and expensive to acquire. Development of active
learning methods in such fields is highly computationally expensive and time
consuming which obstructs the progression of research and leads to a lack of
comparability between methods. In this work, we propose and investigate a
sandbox setup for rapid development and transparent evaluation of active
learning in deep object detection. Our experiments with commonly used
configurations of datasets and detection architectures found in the literature
show that results obtained in our sandbox environment are representative of
results on standard configurations. The total compute time to obtain results
and assess the learning behavior can thereby be reduced by factors of up to 14
when comparing with Pascal VOC and up to 32 when comparing with BDD100k. This
allows for testing and evaluating data acquisition and labeling strategies in
under half a day and contributes to the transparency and development speed in
the field of active learning for object detection.",None,-1
CV 3315 Is All You Need : Semantic Segmentation Competition,0.0840408,"This competition focus on Urban-Sense Segmentation based on the vehicle
camera view. Class highly unbalanced Urban-Sense images dataset challenge the
existing solutions and further studies. Deep Conventional neural network-based
semantic segmentation methods such as encoder-decoder architecture and
multi-scale and pyramid-based approaches become flexible solutions applicable
to real-world applications. In this competition, we mainly review the
literature and conduct experiments on transformer-driven methods especially
SegFormer, to achieve an optimal trade-off between performance and efficiency.
For example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,
and the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple
factors, including individual case failure analysis, individual class
performance, training pressure and efficiency estimation, the final candidate
model for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU
evaluated on the testing set. Checkout our code implementation at
https://vmv.re/cv3315.",https://vmv.re/cv3315,-1
Standing on the Shoulders of Giant Frozen Language Models,0.325353,"Huge pretrained language models (LMs) have demonstrated surprisingly good
zero-shot capabilities on a wide variety of tasks. This gives rise to the
appealing vision of a single, versatile model with a wide range of
functionalities across disparate applications. However, current leading
techniques for leveraging a ""frozen"" LM -- i.e., leaving its weights untouched
-- still often underperform fine-tuning approaches which modify these weights
in a task-dependent way. Those, in turn, suffer forgetfulness and compromise
versatility, suggesting a tradeoff between performance and versatility. The
main message of this paper is that current frozen-model techniques such as
prompt tuning are only the tip of the iceberg, and more powerful methods for
leveraging frozen LMs can do just as well as fine tuning in challenging domains
without sacrificing the underlying model's versatility. To demonstrate this, we
introduce three novel methods for leveraging frozen models: input-dependent
prompt tuning, frozen readers, and recursive LMs, each of which vastly improves
on current frozen-model approaches. Indeed, some of our methods even outperform
fine-tuning approaches in domains currently dominated by the latter. The
computational cost of each method is higher than that of existing frozen model
methods, but still negligible relative to a single pass through a huge frozen
LM. Each of these methods constitutes a meaningful contribution in its own
right, but by presenting these contributions together we aim to convince the
reader of a broader message that goes beyond the details of any given method:
that frozen models have untapped potential and that fine-tuning is often
unnecessary.",None,27124
Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation,0.644137,"Most dominant neural machine translation (NMT) models are restricted to make
predictions only according to the local context of preceding words in a
left-to-right manner. Although many previous studies try to incorporate global
information into NMT models, there still exist limitations on how to
effectively exploit bidirectional global context. In this paper, we propose a
Confidence Based Bidirectional Global Context Aware (CBBGCA) training framework
for NMT, where the NMT model is jointly trained with an auxiliary conditional
masked language model (CMLM). The training consists of two stages: (1)
multi-task joint training; (2) confidence based knowledge distillation. At the
first stage, by sharing encoder parameters, the NMT model is additionally
supervised by the signal from the CMLM decoder that contains bidirectional
global contexts. Moreover, at the second stage, using the CMLM as teacher, we
further pertinently incorporate bidirectional global context to the NMT model
on its unconfidently-predicted target words via knowledge distillation.
Experimental results show that our proposed CBBGCA training framework
significantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on
three large-scale translation datasets, namely WMT'14 English-to-German, WMT'19
Chinese-to-English and WMT'14 English-to-French, respectively.",None,-1
Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents,0.475873,"Text semantic matching is a fundamental task that has been widely used in
various scenarios, such as community question answering, information retrieval,
and recommendation. Most state-of-the-art matching models, e.g., BERT, directly
perform text comparison by processing each word uniformly. However, a query
sentence generally comprises content that calls for different levels of
matching granularity. Specifically, keywords represent factual information such
as action, entity, and event that should be strictly matched, while intents
convey abstract concepts and ideas that can be paraphrased into various
expressions. In this work, we propose a simple yet effective training strategy
for text semantic matching in a divide-and-conquer manner by disentangling
keywords from intents. Our approach can be easily combined with pre-trained
language models (PLM) without influencing their inference efficiency, achieving
stable performance improvements against a wide range of PLMs on three
benchmarks.",https://github.com/RowitZou/DC-Match,-1
Improving Sentiment Analysis By Emotion Lexicon Approach on Vietnamese Texts,0.304787,"The sentiment analysis task has various applications in practice. In the
sentiment analysis task, words and phrases that represent positive and negative
emotions are important. Finding out the words that represent the emotion from
the text can improve the performance of the classification models for the
sentiment analysis task. In this paper, we propose a methodology that combines
the emotion lexicon with the classification model to enhance the accuracy of
the models. Our experimental results show that the emotion lexicon combined
with the classification model improves the performance of models.",None,-1
DexTransfer: Real World Multi-fingered Dexterous Grasping with Minimal Human Demonstrations,0.626191,"Teaching a multi-fingered dexterous robot to grasp objects in the real world
has been a challenging problem due to its high dimensional state and action
space. We propose a robot-learning system that can take a small number of human
demonstrations and learn to grasp unseen object poses given partially occluded
observations. Our system leverages a small motion capture dataset and generates
a large dataset with diverse and successful trajectories for a multi-fingered
robot gripper. By adding domain randomization, we show that our dataset
provides robust grasping trajectories that can be transferred to a policy
learner. We train a dexterous grasping policy that takes the point clouds of
the object as input and predicts continuous actions to grasp objects from
different initial robot states. We evaluate the effectiveness of our system on
a 22-DoF floating Allegro Hand in simulation and a 23-DoF Allegro robot hand
with a KUKA arm in real world. The policy learned from our dataset can
generalize well on unseen object poses in both simulation and the real world",https://github.com/mmatl/pyrender,-1
Learning Job Titles Similarity from Noisy Skill Labels,0.308528,"Measuring semantic similarity between job titles is an essential
functionality for automatic job recommendations. This task is usually
approached using supervised learning techniques, which requires training data
in the form of equivalent job title pairs. In this paper, we instead propose an
unsupervised representation learning method for training a job title similarity
model using noisy skill labels. We show that it is highly effective for tasks
such as text ranking and job normalization.",https://github.com/rabihzbib/jobtitlesimilarity_dataset,-1
Objects Can Move: 3D Change Detection by Geometric Transformation Constistency,0.161008,"AR/VR applications and robots need to know when the scene has changed. An
example is when objects are moved, added, or removed from the scene. We propose
a 3D object discovery method that is based only on scene changes. Our method
does not need to encode any assumptions about what is an object, but rather
discovers objects by exploiting their coherent move. Changes are initially
detected as differences in the depth maps and segmented as objects if they
undergo rigid motions. A graph cut optimization propagates the changing labels
to geometrically consistent regions. Experiments show that our method achieves
state-of-the-art performance on the 3RScan dataset against competitive
baselines. The source code of our method can be found at
https://github.com/katadam/ObjectsCanMove.",https://github.com/katadam/ObjectsCanMove,-1
Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning,0.963419,"Controlled automated story generation seeks to generate natural language
stories satisfying constraints from natural language critiques or preferences.
Existing methods to control for story preference utilize prompt engineering
which is labor intensive and often inconsistent. They may also use
logit-manipulation methods which require annotated datasets to exist for the
desired attributes. To address these issues, we first train a contrastive
bi-encoder model to align stories with corresponding human critiques, named
CARP, building a general purpose preference model. This is subsequently used as
a reward function to fine-tune a generative language model via reinforcement
learning. However, simply fine-tuning a generative language model with a
contrastive reward model does not always reliably result in a story generation
system capable of generating stories that meet user preferences. To increase
story generation robustness we further fine-tune the contrastive reward model
using a prompt-learning technique. A human participant study is then conducted
comparing generations from our full system, ablations, and two baselines. We
show that the full fine-tuning pipeline results in a story generator preferred
over a LLM 20x as large as well as logit-based methods. This motivates the use
of contrastive learning for general purpose human preference modeling.",https://github.com/lvwerra/trl/,19820
Bayesian Optimisation for Robust Model Predictive Control under Model Parameter Uncertainty,0.0510481,"We propose an adaptive optimisation approach for tuning stochastic model
predictive control (MPC) hyper-parameters while jointly estimating probability
distributions of the transition model parameters based on performance rewards.
In particular, we develop a Bayesian optimisation (BO) algorithm with a
heteroscedastic noise model to deal with varying noise across the MPC
hyper-parameter and dynamics model parameter spaces. Typical homoscedastic
noise models are unrealistic for tuning MPC since stochastic controllers are
inherently noisy, and the level of noise is affected by their hyper-parameter
settings. We evaluate the proposed optimisation algorithm in simulated control
and robotics tasks where we jointly infer control and dynamics parameters.
Experimental results demonstrate that our approach leads to higher cumulative
rewards and more stable controllers.",None,-1
Transductive Decoupled Variational Inference for Few-Shot Classification,0.276674,"The versatility to learn from a handful of samples is the hallmark of human
intelligence. Few-shot learning is an endeavour to transcend this capability
down to machines. Inspired by the promise and power of probabilistic deep
learning, we propose a novel variational inference network for few-shot
classification (coined as TRIDENT) to decouple the representation of an image
into semantic and label latent variables, and simultaneously infer them in an
intertwined fashion. To induce task-awareness, as part of the inference
mechanics of TRIDENT, we exploit information across both query and support
images of a few-shot task using a novel built-in attention-based transductive
feature extraction module (we call AttFEX). Our extensive experimental results
corroborate the efficacy of TRIDENT and demonstrate that, using the simplest of
backbones, it sets a new state-of-the-art in the most commonly adopted datasets
miniImageNet and tieredImageNet (offering up to 4% and 5% improvements,
respectively), as well as for the recent challenging cross-domain miniImagenet
--> CUB scenario offering a significant margin (up to 20% improvement) beyond
the best existing cross-domain baselines. Code and experimentation can be found
in our GitHub repository: https://github.com/anujinho/trident",https://github.com/anujinho/trident,-1
Multi-View Document Representation Learning for Open-Domain Dense Retrieval,0.783766,"Dense retrieval has achieved impressive advances in first-stage retrieval
from a large-scale document collection, which is built on bi-encoder
architecture to produce single vector representation of query and document.
However, a document can usually answer multiple potential queries from
different views. So the single vector representation of a document is hard to
match with multi-view queries, and faces a semantic mismatch problem. This
paper proposes a multi-view document representation learning framework, aiming
to produce multi-view embeddings to represent documents and enforce them to
align with different queries. First, we propose a simple yet effective method
of generating multiple embeddings through viewers. Second, to prevent
multi-view embeddings from collapsing to the same one, we further propose a
global-local loss with annealed temperature to encourage the multiple viewers
to better align with different potential queries. Experiments show our method
outperforms recent works and achieves state-of-the-art results.",None,-1
On-Board Pedestrian Trajectory Prediction Using Behavioral Features,0.562441,"This paper presents a novel approach to pedestrian trajectory prediction for
on-board camera systems, which utilizes behavioral features of pedestrians that
can be inferred from visual observations. Our proposed method, called
Behavior-Aware Pedestrian Trajectory Prediction (BA-PTP), processes multiple
input modalities, i.e. bounding boxes, body and head orientation of pedestrians
as well as their pose, with independent encoding streams. The encodings of each
stream are fused using a modality attention mechanism, resulting in a final
embedding that is used to predict future bounding boxes in the image.
  In experiments on two datasets for pedestrian behavior prediction, we
demonstrate the benefit of using behavioral features for pedestrian trajectory
prediction and evaluate the effectiveness of the proposed encoding strategy.
Additionally, we investigate the relevance of different behavioral features on
the prediction performance based on an ablation study.",None,-1
Exploiting Global and Local Hierarchies for Hierarchical Text Classification,0.306679,"Hierarchical text classification aims to leverage label hierarchy in
multi-label text classification. Existing methods encode label hierarchy in a
global view, where label hierarchy is treated as the static hierarchical
structure containing all labels. Since global hierarchy is static and
irrelevant to text samples, it makes these methods hard to exploit hierarchical
information. Contrary to global hierarchy, local hierarchy as a structured
labels hierarchy corresponding to each text sample. It is dynamic and relevant
to text samples, which is ignored in previous methods. To exploit global and
local hierarchies,we propose Hierarchy-guided BERT with Global and Local
hierarchies (HBGL), which utilizes the large-scale parameters and prior
language knowledge of BERT to model both global and local
hierarchies.Moreover,HBGL avoids the intentional fusion of semantic and
hierarchical modules by directly modeling semantic and hierarchical information
with BERT.Compared with the state-of-the-art method HGCLR,our method achieves
significant improvement on three benchmark datasets.",http://github.com/kongds/,15937
Unsupervised Face Morphing Attack Detection via Self-paced Anomaly Detection,0.688597,"The supervised-learning-based morphing attack detection (MAD) solutions
achieve outstanding success in dealing with attacks from known morphing
techniques and known data sources. However, given variations in the morphing
attacks, the performance of supervised MAD solutions drops significantly due to
the insufficient diversity and quantity of the existing MAD datasets. To
address this concern, we propose a completely unsupervised MAD solution via
self-paced anomaly detection (SPL-MAD) by leveraging the existing large-scale
face recognition (FR) datasets and the unsupervised nature of convolutional
autoencoders. Using general FR datasets that might contain unintentionally and
unlabeled manipulated samples to train an autoencoder can lead to a diverse
reconstruction behavior of attack and bona fide samples. We analyze this
behavior empirically to provide a solid theoretical ground for designing our
unsupervised MAD solution. This also results in proposing to integrate our
adapted modified self-paced learning paradigm to enhance the reconstruction
error separability between the bona fide and attack samples in a completely
unsupervised manner. Our experimental results on a diverse set of MAD
evaluation datasets show that the proposed unsupervised SPL-MAD solution
outperforms the overall performance of a wide range of supervised MAD solutions
and provides higher generalizability on unknown attacks.",https://github.com/meilfang/SPL-MAD,-1
NFLAT: Non-Flat-Lattice Transformer for Chinese Named Entity Recognition,0.864484,"Recently, Flat-LAttice Transformer (FLAT) has achieved great success in
Chinese Named Entity Recognition (NER). FLAT performs lexical enhancement by
constructing flat lattices, which mitigates the difficulties posed by blurred
word boundaries and the lack of word semantics. In FLAT, the positions of
starting and ending characters are used to connect a matching word. However,
this method is likely to match more words when dealing with long texts,
resulting in long input sequences. Therefore, it significantly increases the
memory and computational costs of the self-attention module. To deal with this
issue, we advocate a novel lexical enhancement method, InterFormer, that
effectively reduces the amount of computational and memory costs by
constructing non-flat lattices. Furthermore, with InterFormer as the backbone,
we implement NFLAT for Chinese NER. NFLAT decouples lexicon fusion and context
feature encoding. Compared with FLAT, it reduces unnecessary attention
calculations in ""word-character"" and ""word-word"". This reduces the memory usage
by about 50% and can use more extensive lexicons or higher batches for network
training. The experimental results obtained on several well-known benchmarks
demonstrate the superiority of the proposed method over the state-of-the-art
hybrid (character-word) models.",None,-1
Interpretable Molecular Graph Generation via Monotonic Constraints,0.626729,"Designing molecules with specific properties is a long-lasting research
problem and is central to advancing crucial domains such as drug discovery and
material science. Recent advances in deep graph generative models treat
molecule design as graph generation problems which provide new opportunities
toward the breakthrough of this long-lasting problem. Existing models, however,
have many shortcomings, including poor interpretability and controllability
toward desired molecular properties. This paper focuses on new methodologies
for molecule generation with interpretable and controllable deep generative
models, by proposing new monotonically-regularized graph variational
autoencoders. The proposed models learn to represent the molecules with latent
variables and then learn the correspondence between them and molecule
properties parameterized by polynomial functions. To further improve the
intepretability and controllability of molecule generation towards desired
properties, we derive new objectives which further enforce monotonicity of the
relation between some latent variables and target molecule properties such as
toxicity and clogP. Extensive experimental evaluation demonstrates the
superiority of the proposed framework on accuracy, novelty, disentanglement,
and control towards desired molecular properties. The code is open-source at
https://anonymous.4open.science/r/MDVAE-FD2C.",None,-1
Synthesizing Personalized Non-speech Vocalization from Discrete Speech Representations,0.684499,"We formulated non-speech vocalization (NSV) modeling as a text-to-speech task
and verified its viability. Specifically, we evaluated the phonetic
expressivity of HUBERT speech units on NSVs and verified our model's ability to
control over speaker timbre even though the training data is speaker few-shot.
In addition, we substantiated that the heterogeneity in recording conditions is
the major obstacle for NSV modeling. Finally, we discussed five improvements
over our method for future research. Audio samples of synthesized NSVs are
available on our demo page: https://resemble-ai.github.io/reLaugh.",None,4373
Adversarial and Random Transformations for Robust Domain Adaptation and Generalization,0.252622,"Data augmentation has been widely used to improve generalization in training
deep neural networks. Recent works show that using worst-case transformations
or adversarial augmentation strategies can significantly improve the accuracy
and robustness. However, due to the non-differentiable properties of image
transformations, searching algorithms such as reinforcement learning or
evolution strategy have to be applied, which are not computationally practical
for large scale problems. In this work, we show that by simply applying
consistency training with random data augmentation, state-of-the-art results on
domain adaptation (DA) and generalization (DG) can be obtained. To further
improve the accuracy and robustness with adversarial examples, we propose a
differentiable adversarial data augmentation method based on spatial
transformer networks (STN). The combined adversarial and random transformations
based method outperforms the state-of-the-art on multiple DA and DG benchmark
datasets. Besides, the proposed method shows desirable robustness to
corruption, which is also validated on commonly used datasets.",None,-1
Global Matching with Overlapping Attention for Optical Flow Estimation,0.980582,"Optical flow estimation is a fundamental task in computer vision. Recent
direct-regression methods using deep neural networks achieve remarkable
performance improvement. However, they do not explicitly capture long-term
motion correspondences and thus cannot handle large motions effectively. In
this paper, inspired by the traditional matching-optimization methods where
matching is introduced to handle large displacements before energy-based
optimizations, we introduce a simple but effective global matching step before
the direct regression and develop a learning-based matching-optimization
framework, namely GMFlowNet. In GMFlowNet, global matching is efficiently
calculated by applying argmax on 4D cost volumes. Additionally, to improve the
matching quality, we propose patch-based overlapping attention to extract large
context features. Extensive experiments demonstrate that GMFlowNet outperforms
RAFT, the most popular optimization-only method, by a large margin and achieves
state-of-the-art performance on standard benchmarks. Thanks to the matching and
overlapping attention, GMFlowNet obtains major improvements on the predictions
for textureless regions and large motions. Our code is made publicly available
at https://github.com/xiaofeng94/GMFlowNet",https://github.com/xiaofeng94/GMFlowNet,-1
Revisiting Discrete Soft Actor-Critic,0.815925,"We study the adaption of soft actor-critic (SAC) from continuous action space
to discrete action space. We revisit vanilla SAC and provide an in-depth
understanding of its Q value underestimation and performance instability issues
when applied to discrete settings. We thereby propose entropy-penalty and
double average Q-learning with Q-clip to address these issues. Extensive
experiments on typical benchmarks with discrete action space, including Atari
games and a large-scale MOBA game, show the efficacy of our proposed method.
Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.",https://github.com/coldsummerday/Revisiting-Discrete-SAC,-1
Kratt: Developing an Automatic Subject Indexing Tool for The National Library of Estonia,0.303119,"Manual subject indexing in libraries is a time-consuming and costly process
and the quality of the assigned subjects is affected by the cataloguer's
knowledge on the specific topics contained in the book. Trying to solve these
issues, we exploited the opportunities arising from artificial intelligence to
develop Kratt: a prototype of an automatic subject indexing tool. Kratt is able
to subject index a book independent of its extent and genre with a set of
keywords present in the Estonian Subject Thesaurus. It takes Kratt
approximately 1 minute to subject index a book, outperforming humans 10-15
times. Although the resulting keywords were not considered satisfactory by the
cataloguers, the ratings of a small sample of regular library users showed more
promise. We also argue that the results can be enhanced by including a bigger
corpus for training the model and applying more careful preprocessing
techniques.",None,-1
SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control,0.451112,"Despite the growing success of diffusion models in continuous-valued domains
(e.g., images), similar efforts for discrete domains such as text have yet to
match the performance of autoregressive language models. In this work, we
present SSD-LM -- a diffusion-based language model with two key design choices.
First, SSD-LM is semi-autoregressive, iteratively generating blocks of text,
allowing for flexible output length at decoding time while enabling local
bidirectional context updates. Second, it is simplex-based, performing
diffusion on the natural vocabulary space rather than a learned latent space,
allowing us to incorporate classifier guidance and modular control using
off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on
unconstrained text generation benchmarks, and show that it matches or
outperforms strong autoregressive GPT-2 models across standard quality and
diversity metrics, while vastly outperforming diffusion-based baselines. On
controlled text generation, SSD-LM also outperforms competitive baselines, with
an extra advantage in modularity.",https://github.com/xhan77/ssd-lm,-1
Chemotaxis of sea urchin sperm cells through deep reinforcement learning,0.540249,"By imitating biological microswimmers, microrobots can be designed to
accomplish targeted delivery of cargos and biomedical manipulations at
microscale. However, it is still a great challenge to enable microrobots to
maneuver in a complex environment. Machine learning algorithms offer a tool to
boost mobility and flexibility of a synthetic microswimmer, hence could help us
design truly smart microrobots. In this work, we investigate how a model of sea
urchin sperm cell can self-learn chemotactic motion in a chemoattractant
concentration field. We employ an artificial neural network to act as a
decision-making agent and facilitate the sperm cell to discover efficient
maneuver strategies through a deep reinforcement learning (DRL) algorithm. Our
results show that chemotactic behaviours, very similar to the realistic ones,
can be achieved by the DRL utilizing only limited environmental information. In
most cases, the DRL algorithm discovers more efficient strategies than the
human-devised one. Furthermore, the DRL can even utilize an external
disturbance to facilitate the chemotactic motion if the extra flow information
is also taken into account by the artificial neural network. Our results
provide insights to the chemotactic process of sea urchin sperm cells and also
prepare guidance for the intelligent maneuver of microrobots.",None,1256
What Makes Data-to-Text Generation Hard for Pretrained Language Models?,0.179337,"Expressing natural language descriptions of structured facts or relations --
data-to-text generation (D2T) -- increases the accessibility of structured
knowledge repositories. Previous work shows that pre-trained language
models(PLMs) perform remarkably well on this task after fine-tuning on a
significant amount of task-specific training data. On the other hand, while
auto-regressive PLMs can generalize from a few task examples, their efficacy at
D2T is largely unexplored. Furthermore, we have an incomplete understanding of
the limits of PLMs on D2T.
  In this work, we conduct an empirical study of both fine-tuned and
auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their
performance as a function of the amount of task-specific data and how these
data are incorporated into the models: zero and few-shot learning, and
fine-tuning of model weights. In addition, we probe the limits of PLMs by
measuring performance on subsets of the evaluation data: novel predicates and
abstractive test examples. To improve the performance on these subsets, we
investigate two techniques: providing predicate descriptions in the context and
re-ranking generated candidates by information reflected in the source.
Finally, we conduct a human evaluation of model errors and show that D2T
generation tasks would benefit from datasets with more careful manual curation.",https://github.com/UKPLab/plms-graph2text,28189
Deep Surrogate of Modular Multi Pump using Active Learning,0.0570714,"Due to the high cost and reliability of sensors, the designers of a pump
reduce the needed number of sensors for the estimation of the feasible
operating point as much as possible. The major challenge to obtain a good
estimation is the low amount of data available. Using this amount of data, the
performance of the estimation method is not enough to satisfy the client
requests. To solve this problem of scarcity of data, getting high quality data
is important to obtain a good estimation. Based on these considerations, we
develop an active learning framework for estimating the operating point of a
Modular Multi Pump used in energy field. In particular we focus on the
estimation of the surge distance. We apply Active learning to estimate the
surge distance with minimal dataset. Results report that active learning is a
valuable technique also for real application.",None,-1
SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views,0.991195,"We introduce SparseNeuS, a novel neural rendering based method for the task
of surface reconstruction from multi-view images. This task becomes more
difficult when only sparse images are provided as input, a scenario where
existing neural reconstruction approaches usually produce incomplete or
distorted results. Moreover, their inability of generalizing to unseen new
scenes impedes their application in practice. Contrarily, SparseNeuS can
generalize to new scenes and work well with sparse images (as few as 2 or 3).
SparseNeuS adopts signed distance function (SDF) as the surface representation,
and learns generalizable priors from image features by introducing geometry
encoding volumes for generic surface prediction. Moreover, several strategies
are introduced to effectively leverage sparse views for high-quality
reconstruction, including 1) a multi-level geometry reasoning framework to
recover the surfaces in a coarse-to-fine manner; 2) a multi-scale color
blending scheme for more reliable color prediction; 3) a consistency-aware
fine-tuning scheme to control the inconsistent regions caused by occlusion and
noise. Extensive experiments demonstrate that our approach not only outperforms
the state-of-the-art methods, but also exhibits good efficiency,
generalizability, and flexibility.",https://www.xxlong.site/SparseNeuSarXiv:2206.05737v2,-1
Near Perfect GAN Inversion,0.430688,"To edit a real photo using Generative Adversarial Networks (GANs), we need a
GAN inversion algorithm to identify the latent vector that perfectly reproduces
it. Unfortunately, whereas existing inversion algorithms can synthesize images
similar to real photos, they cannot generate the identical clones needed in
most applications. Here, we derive an algorithm that achieves near perfect
reconstructions of photos. Rather than relying on encoder- or
optimization-based methods to find an inverse mapping on a fixed generator
$G(\cdot)$, we derive an approach to locally adjust $G(\cdot)$ to more
optimally represent the photos we wish to synthesize. This is done by locally
tweaking the learned mapping $G(\cdot)$ s.t. $\| {\bf x} - G({\bf z})
\|<\epsilon$, with ${\bf x}$ the photo we wish to reproduce, ${\bf z}$ the
latent vector, $\|\cdot\|$ an appropriate metric, and $\epsilon > 0$ a small
scalar. We show that this approach can not only produce synthetic images that
are indistinguishable from the real photos we wish to replicate, but that these
images are readily editable. We demonstrate the effectiveness of the derived
algorithm on a variety of datasets including human faces, animals, and cars,
and discuss its importance for diversity and inclusion.",https://github.com/davisking/dlibofStyleGAN2,-1
ClearPose: Large-scale Transparent Object Dataset and Benchmark,0.877727,"Transparent objects are ubiquitous in household settings and pose distinct
challenges for visual sensing and perception systems. The optical properties of
transparent objects leave conventional 3D sensors alone unreliable for object
depth and pose estimation. These challenges are highlighted by the shortage of
large-scale RGB-Depth datasets focusing on transparent objects in real-world
settings. In this work, we contribute a large-scale real-world RGB-Depth
transparent object dataset named ClearPose to serve as a benchmark dataset for
segmentation, scene-level depth completion and object-centric pose estimation
tasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth
frames and 5M instance annotations covering 63 household objects. The dataset
includes object categories commonly used in daily life under various lighting
and occluding conditions as well as challenging test scenarios such as cases of
occlusion by opaque or translucent objects, non-planar orientations, presence
of liquids, etc. We benchmark several state-of-the-art depth completion and
object pose estimation deep neural networks on ClearPose. The dataset and
benchmarking source code is available at https://github.com/opipari/ClearPose.",https://github.com/opipari/ClearPose,-1
Improving Sample Efficiency of Value Based Models Using Attention and Vision Transformers,0.195881,"Much of recent Deep Reinforcement Learning success is owed to the neural
architecture's potential to learn and use effective internal representations of
the world. While many current algorithms access a simulator to train with a
large amount of data, in realistic settings, including while playing games that
may be played against people, collecting experience can be quite costly. In
this paper, we introduce a deep reinforcement learning architecture whose
purpose is to increase sample efficiency without sacrificing performance. We
design this architecture by incorporating advances achieved in recent years in
the field of Natural Language Processing and Computer Vision. Specifically, we
propose a visually attentive model that uses transformers to learn a
self-attention mechanism on the feature maps of the state representation, while
simultaneously optimizing return. We demonstrate empirically that this
architecture improves sample complexity for several Atari environments, while
also achieving better performance in some of the games.",None,-1
MotionBERT: A Unified Perspective on Learning Human Motion Representations,0.809985,"We present a unified perspective on tackling various human-centric video
tasks by learning human motion representations from large-scale and
heterogeneous data resources. Specifically, we propose a pretraining stage in
which a motion encoder is trained to recover the underlying 3D motion from
noisy partial 2D observations. The motion representations acquired in this way
incorporate geometric, kinematic, and physical knowledge about human motion,
which can be easily transferred to multiple downstream tasks. We implement the
motion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer)
neural network. It could capture long-range spatio-temporal relationships among
the skeletal joints comprehensively and adaptively, exemplified by the lowest
3D pose estimation error so far when trained from scratch. Furthermore, our
proposed framework achieves state-of-the-art performance on all three
downstream tasks by simply finetuning the pretrained motion encoder with a
simple regression head (1-2 layers), which demonstrates the versatility of the
learned motion representations. Code and models are available at
https://motionbert.github.io/",https://motionbert.github.io/,-1
"Evaluating State of the Art, Forecasting Ensembles- and Meta-learning Strategies for Model Fusion",0.402565,"Techniques of hybridisation and ensemble learning are popular model fusion
techniques for improving the predictive power of forecasting methods. With
limited research that instigates combining these two promising approaches, this
paper focuses on the utility of the Exponential-Smoothing-Recurrent Neural
Network (ES-RNN) in the pool of base models for different ensembles. We compare
against some state of the art ensembling techniques and arithmetic model
averaging as a benchmark. We experiment with the M4 forecasting data set of
100,000 time-series, and the results show that the Feature-based Forecast Model
Averaging (FFORMA), on average, is the best technique for late data fusion with
the ES-RNN. However, considering the M4's Daily subset of data, stacking was
the only successful ensemble at dealing with the case where all base model
performances are similar. Our experimental results indicate that we attain
state of the art forecasting results compared to N-BEATS as a benchmark. We
conclude that model averaging is a more robust ensemble than model selection
and stacking strategies. Further, the results show that gradient boosting is
superior for implementing ensemble learning strategies.",https://github.com/Pieter-Cawood/FFORMA-ESRNN,-1
Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems,0.364833,"Stochastic optimization has found wide applications in minimizing objective
functions in machine learning, which motivates a lot of theoretical studies to
understand its practical success. Most of existing studies focus on the
convergence of optimization errors, while the generalization analysis of
stochastic optimization is much lagging behind. This is especially the case for
nonconvex and nonsmooth problems often encountered in practice. In this paper,
we initialize a systematic stability and generalization analysis of stochastic
optimization on nonconvex and nonsmooth problems. We introduce novel
algorithmic stability measures and establish their quantitative connection on
the gap between population gradients and empirical gradients, which is then
further extended to study the gap between the Moreau envelope of the empirical
risk and that of the population risk. To our knowledge, these quantitative
connection between stability and generalization in terms of either gradients or
Moreau envelopes have not been studied in the literature. We introduce a class
of sampling-determined algorithms, for which we develop bounds for three
stability measures. Finally, we apply these discussions to derive error bounds
for stochastic gradient descent and its adaptive variant, where we show how to
achieve an implicit regularization by tuning the step sizes and the number of
iterations.",None,-1
Seeing a Rose in Five Thousand Ways,0.111091,"What is a rose, visually? A rose comprises its intrinsics, including the
distribution of geometry, texture, and material specific to its object
category. With knowledge of these intrinsic properties, we may render roses of
different sizes and shapes, in different poses, and under different lighting
conditions. In this work, we build a generative model that learns to capture
such object intrinsics from a single image, such as a photo of a bouquet. Such
an image includes multiple instances of an object type. These instances all
share the same intrinsics, but appear different due to a combination of
variance within these intrinsics and differences in extrinsic factors, such as
pose and illumination. Experiments show that our model successfully learns
object intrinsics (distribution of geometry, texture, and material) for a wide
range of objects, each from a single Internet image. Our method achieves
superior results on multiple downstream tasks, including intrinsic image
decomposition, shape and image generation, view synthesis, and relighting.",None,35102
Character-Aware Models Improve Visual Text Rendering,0.746574,"Current image generation models struggle to reliably produce well-formed
visual text. In this paper, we investigate a key contributing factor: popular
text-to-image models lack character-level input features, making it much harder
to predict a word's visual makeup as a series of glyphs. To quantify this
effect, we conduct a series of experiments comparing character-aware vs.
character-blind text encoders. In the text-only domain, we find that
character-aware models provide large gains on a novel spelling task
(WikiSpell). Applying our learnings to the visual domain, we train a suite of
image generation models, and show that character-aware variants outperform
their character-blind counterparts across a range of novel text rendering tasks
(our DrawText benchmark). Our models set a much higher state-of-the-art on
visual spelling, with 30+ point accuracy gains over competitors on rare words,
despite training on far fewer examples.",None,-1
Using Large Language Models to Generate Engaging Captions for Data Visualizations,0.293842,"Creating compelling captions for data visualizations has been a longstanding
challenge. Visualization researchers are typically untrained in journalistic
reporting and hence the captions that are placed below data visualizations tend
to be not overly engaging and rather just stick to basic observations about the
data. In this work we explore the opportunities offered by the newly emerging
crop of large language models (LLM) which use sophisticated deep learning
technology to produce human-like prose. We ask, can these powerful software
devices be purposed to produce engaging captions for generic data
visualizations like a scatterplot. It turns out that the key challenge lies in
designing the most effective prompt for the LLM, a task called prompt
engineering. We report on first experiments using the popular LLM GPT-3 and
deliver some promising results.",None,-1
Migrating Face Swap to Mobile Devices: A lightweight Framework and A Supervised Training Solution,0.330432,"Existing face swap methods rely heavily on large-scale networks for adequate
capacity to generate visually plausible results, which inhibits its
applications on resource-constraint platforms. In this work, we propose
MobileFSGAN, a novel lightweight GAN for face swap that can run on mobile
devices with much fewer parameters while achieving competitive performance. A
lightweight encoder-decoder structure is designed especially for image
synthesis tasks, which is only 10.2MB and can run on mobile devices at a
real-time speed. To tackle the unstability of training such a small network, we
construct the FSTriplets dataset utilizing facial attribute editing techniques.
FSTriplets provides source-target-result training triplets, yielding
pixel-level labels thus for the first time making the training process
supervised. We also designed multi-scale gradient losses for efficient
back-propagation, resulting in faster and better convergence. Experimental
results show that our model reaches comparable performance towards
state-of-the-art methods, while significantly reducing the number of network
parameters. Codes and the dataset have been released.",https://github.com/HoiM/MobileFSGANbasedongenerativeadversarialnetworks,-1
Region Aware Video Object Segmentation with Deep Motion Modeling,0.34186,"Current semi-supervised video object segmentation (VOS) methods usually
leverage the entire features of one frame to predict object masks and update
memory. This introduces significant redundant computations. To reduce
redundancy, we present a Region Aware Video Object Segmentation (RAVOS)
approach that predicts regions of interest (ROIs) for efficient object
segmentation and memory storage. RAVOS includes a fast object motion tracker to
predict their ROIs in the next frame. For efficient segmentation, object
features are extracted according to the ROIs, and an object decoder is designed
for object-level segmentation. For efficient memory storage, we propose motion
path memory to filter out redundant context by memorizing the features within
the motion path of objects between two frames. Besides RAVOS, we also propose a
large-scale dataset, dubbed OVOS, to benchmark the performance of VOS models
under occlusions. Evaluation on DAVIS and YouTube-VOS benchmarks and our new
OVOS dataset show that our method achieves state-of-the-art performance with
significantly faster inference time, e.g., 86.1 J&F at 42 FPS on DAVIS and 84.4
J&F at 23 FPS on YouTube-VOS.",None,-1
Alterfactual Explanations -- The Relevance of Irrelevance for Explaining AI Systems,0.259863,"Explanation mechanisms from the field of Counterfactual Thinking are a
widely-used paradigm for Explainable Artificial Intelligence (XAI), as they
follow a natural way of reasoning that humans are familiar with. However, all
common approaches from this field are based on communicating information about
features or characteristics that are especially important for an AI's decision.
We argue that in order to fully understand a decision, not only knowledge about
relevant features is needed, but that the awareness of irrelevant information
also highly contributes to the creation of a user's mental model of an AI
system. Therefore, we introduce a new way of explaining AI systems. Our
approach, which we call Alterfactual Explanations, is based on showing an
alternative reality where irrelevant features of an AI's input are altered. By
doing so, the user directly sees which characteristics of the input data can
change arbitrarily without influencing the AI's decision. We evaluate our
approach in an extensive user study, revealing that it is able to significantly
contribute to the participants' understanding of an AI. We show that
alterfactual explanations are suited to convey an understanding of different
aspects of the AI's reasoning than established counterfactual explanation
methods.",None,-1
Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning,0.992297,"Providing Emotional Support (ES) to soothe people in emotional distress is an
essential capability in social interactions. Most existing researches on
building ES conversation systems only considered single-turn interactions with
users, which was over-simplified. In comparison, multi-turn ES conversation
systems can provide ES more effectively, but face several new technical
challenges, including: (1) how to adopt appropriate support strategies to
achieve the long-term dialogue goal of comforting the user's emotion; (2) how
to dynamically model the user's state. In this paper, we propose a novel system
MultiESC to address these issues. For strategy planning, drawing inspiration
from the A* search algorithm, we propose lookahead heuristics to estimate the
future user feedback after using particular strategies, which helps to select
strategies that can lead to the best long-term effects. For user state
modeling, MultiESC focuses on capturing users' subtle emotional expressions and
understanding their emotion causes. Extensive experiments show that MultiESC
significantly outperforms competitive baselines in both dialogue generation and
strategy planning. Our codes are available at
https://github.com/lwgkzl/MultiESC.",https://github.com/lwgkzl/MultiESC,-1
Generative Design Ideation: A Natural Language Generation Approach,0.332314,"This paper aims to explore a generative approach for knowledge-based design
ideation by applying the latest pre-trained language models in artificial
intelligence (AI). Specifically, a method of fine-tuning the generative
pre-trained transformer using the USPTO patent database is proposed. The
AI-generated ideas are not only in concise and understandable language but also
able to synthesize the target design with external knowledge sources with
controllable knowledge distance. The method is tested in a case study of
rolling toy design and the results show good performance in generating ideas of
varied novelty with near-field and far-field source knowledge.",None,-1
Graph Neural Networks with Trainable Adjacency Matrices for Fault Diagnosis on Multivariate Sensor Data,0.201327,"Timely detected anomalies in the chemical technological processes, as well as
the earliest detection of the cause of the fault, significantly reduce the
production cost in the industrial factories. Data on the state of the
technological process and the operation of production equipment are received by
a large number of different sensors. To better predict the behavior of the
process and equipment, it is necessary not only to consider the behavior of the
signals in each sensor separately, but also to take into account their
correlation and hidden relationships with each other. Graph-based data
representation helps with this. The graph nodes can be represented as data from
the different sensors, and the edges can display the influence of these data on
each other. In this work, the possibility of applying graph neural networks to
the problem of fault diagnosis in a chemical process is studied. It was
proposed to construct a graph during the training of graph neural network. This
allows to train models on data where the dependencies between the sensors are
not known in advance. In this work, several methods for obtaining adjacency
matrices were considered, as well as their quality was studied. It has also
been proposed to use multiple adjacency matrices in one model. We showed
state-of-the-art performance on the fault diagnosis task with the Tennessee
Eastman Process dataset. The proposed graph neural networks outperformed the
results of recurrent neural networks.",None,-1
Dialect-robust Evaluation of Generated Text,0.353535,"Evaluation metrics that are not robust to dialect variation make it
impossible to tell how well systems perform for many groups of users, and can
even penalize systems for producing text in lower-resource dialects. However,
currently, there exists no way to quantify how metrics respond to change in the
dialect of a generated utterance. We thus formalize dialect robustness and
dialect awareness as goals for NLG evaluation metrics. We introduce a suite of
methods and corresponding statistical tests one can use to assess metrics in
light of the two goals. Applying the suite to current state-of-the-art metrics,
we demonstrate that they are not dialect-robust and that semantic perturbations
frequently lead to smaller decreases in a metric than the introduction of
dialect features. As a first step to overcome this limitation, we propose a
training schema, NANO, which introduces regional and language information to
the pretraining process of a metric. We demonstrate that NANO provides a
size-efficient way for models to improve the dialect robustness while
simultaneously improving their performance on the standard metric benchmark.",https://github.com/google-research/bleurt,-1
DIMBA: Discretely Masked Black-Box Attack in Single Object Tracking,0.746282,"The adversarial attack can force a CNN-based model to produce an incorrect
output by craftily manipulating human-imperceptible input. Exploring such
perturbations can help us gain a deeper understanding of the vulnerability of
neural networks, and provide robustness to deep learning against miscellaneous
adversaries. Despite extensive studies focusing on the robustness of image,
audio, and NLP, works on adversarial examples of visual object tracking --
especially in a black-box manner -- are quite lacking. In this paper, we
propose a novel adversarial attack method to generate noises for single object
tracking under black-box settings, where perturbations are merely added on
initial frames of tracking sequences, which is difficult to be noticed from the
perspective of a whole video clip. Specifically, we divide our algorithm into
three components and exploit reinforcement learning for localizing important
frame patches precisely while reducing unnecessary computational queries
overhead. Compared to existing techniques, our method requires fewer queries on
initialized frames of a video to manipulate competitive or even better attack
performance. We test our algorithm in both long-term and short-term datasets,
including OTB100, VOT2018, UAV123, and LaSOT. Extensive experiments demonstrate
the effectiveness of our method on three mainstream types of trackers:
discrimination, Siamese-based, and reinforcement learning-based trackers.",None,-1
SleepyWheels: An Ensemble Model for Drowsiness Detection leading to Accident Prevention,0.02361,"Around 40 percent of accidents related to driving on highways in India occur
due to the driver falling asleep behind the steering wheel. Several types of
research are ongoing to detect driver drowsiness but they suffer from the
complexity and cost of the models. In this paper, SleepyWheels a revolutionary
method that uses a lightweight neural network in conjunction with facial
landmark identification is proposed to identify driver fatigue in real time.
SleepyWheels is successful in a wide range of test scenarios, including the
lack of facial characteristics while covering the eye or mouth, the drivers
varying skin tones, camera placements, and observational angles. It can work
well when emulated to real time systems. SleepyWheels utilized EfficientNetV2
and a facial landmark detector for identifying drowsiness detection. The model
is trained on a specially created dataset on driver sleepiness and it achieves
an accuracy of 97 percent. The model is lightweight hence it can be further
deployed as a mobile application for various platforms.",https://github.com/jominjose14/SleepyWheels,-1
HealthE: Classifying Entities in Online Textual Health Advice,0.128808,"The processing of entities in natural language is essential to many medical
NLP systems. Unfortunately, existing datasets vastly under-represent the
entities required to model public health relevant texts such as health advice
often found on sites like WebMD. People rely on such information for personal
health management and clinically relevant decision making. In this work, we
release a new annotated dataset, HealthE, consisting of 6,756 health advice.
HealthE has a more granular label space compared to existing medical NER
corpora and contains annotation for diverse health phrases. Additionally, we
introduce a new health entity classification model, EP S-BERT, which leverages
textual context patterns in the classification of entity classes. EP S-BERT
provides a 4-point increase in F1 score over the nearest baseline and a
34-point increase in F1 when compared to off-the-shelf medical NER tools
trained to extract disease and medication mentions from clinical texts. All
code and data are publicly available on Github.",None,-1
Continual Learning Beyond a Single Model,0.183611,"A growing body of research in continual learning focuses on the catastrophic
forgetting problem. While many attempts have been made to alleviate this
problem, the majority of the methods assume a single model in the continual
learning setup. In this work, we question this assumption and show that
employing ensemble models can be a simple yet effective method to improve
continual performance. However, ensembles' training and inference costs can
increase significantly as the number of models grows. Motivated by this
limitation, we study different ensemble models to understand their benefits and
drawbacks in continual learning scenarios. Finally, to overcome the high
compute cost of ensembles, we leverage recent advances in neural network
subspace to propose a computationally cheap algorithm with similar runtime to a
single model yet enjoying the performance benefits of ensembles.",None,-1
Quantum computing overview: discrete vs. continuous variable models,0.0728847,"In this Near Intermediate-Scale Quantum era, there are two types of near-term
quantum devices available on cloud: superconducting quantum processing units
(QPUs) based on the discrete variable model and linear optics (photonics) QPUs
based on the continuous variable (CV) model. Quantum computation in the
discrete variable model is performed in a finite dimensional quantum state
space and the CV model in an infinite dimensional space. In implementing
quantum algorithms, the CV model offers more quantum gates that are not
available in the discrete variable model. CV-based photonic quantum computers
provide additional flexibility of controlling the length of the output vectors
of quantum circuits, using different methods of measurement and the notion of
cutoff dimension.",None,-1
Offensive Language Detection on Twitter,0.048928,"Detection of offensive language in social media is one of the key challenges
for social media. Researchers have proposed many advanced methods to accomplish
this task. In this report, we try to use the learnings from their approach and
incorporate our ideas to improve upon them. We have successfully achieved an
accuracy of 74% in classifying offensive tweets. We also list upcoming
challenges in the abusive content detection in the social media world.",https://github.com/TeamHG-Memex/eli5,-1
Target-Driven Structured Transformer Planner for Vision-Language Navigation,0.525488,"Vision-language navigation is the task of directing an embodied agent to
navigate in 3D scenes with natural language instructions. For the agent,
inferring the long-term navigation target from visual-linguistic clues is
crucial for reliable path planning, which, however, has rarely been studied
before in literature. In this article, we propose a Target-Driven Structured
Transformer Planner (TD-STP) for long-horizon goal-guided and room layout-aware
navigation. Specifically, we devise an Imaginary Scene Tokenization mechanism
for explicit estimation of the long-term target (even located in unexplored
environments). In addition, we design a Structured Transformer Planner which
elegantly incorporates the explored room layout into a neural attention
architecture for structured and global planning. Experimental results
demonstrate that our TD-STP substantially improves previous best methods'
success rate by 2% and 5% on the test set of R2R and REVERIE benchmarks,
respectively. Our code is available at https://github.com/YushengZhao/TD-STP .",https://github.com/YushengZhao/TD-STP,-1
Towards Automatic Construction of Filipino WordNet: Word Sense Induction and Synset Induction Using Sentence Embeddings,0.203829,"Wordnets are indispensable tools for various natural language processing
applications. Unfortunately, wordnets get outdated, and producing or updating
wordnets can be slow and costly in terms of time and resources. This problem
intensifies for low-resource languages. This study proposes a method for word
sense induction and synset induction using only two linguistic resources,
namely, an unlabeled corpus and a sentence embeddings-based language model. The
resulting sense inventory and synonym sets can be used in automatically
creating a wordnet. We applied this method on a corpus of Filipino text. The
sense inventory and synsets were evaluated by matching them with the sense
inventory of the machine translated Princeton WordNet, as well as comparing the
synsets to the Filipino WordNet. This study empirically shows that the 30% of
the induced word senses are valid and 40% of the induced synsets are valid in
which 20% are novel synsets.",None,-1
Attention Based Neural Networks for Wireless Channel Estimation,0.76809,"In this paper, we deploy the self-attention mechanism to achieve improved
channel estimation for orthogonal frequency-division multiplexing waveforms in
the downlink. Specifically, we propose a new hybrid encoder-decoder structure
(called HA02) for the first time which exploits the attention mechanism to
focus on the most important input information. In particular, we implement a
transformer encoder block as the encoder to achieve the sparsity in the input
features and a residual neural network as the decoder respectively, inspired by
the success of the attention mechanism. Using 3GPP channel models, our
simulations show superior estimation performance compared with other candidate
neural network methods for channel estimation.",None,18255
Towards Abstractive Grounded Summarization of Podcast Transcripts,0.472924,"Podcasts have recently shown a rapid rise in popularity. Summarization of
podcast transcripts is of practical benefit to both content providers and
consumers. It helps consumers to quickly decide whether they will listen to the
podcasts and reduces the cognitive load of content providers to write
summaries. Nevertheless, podcast summarization faces significant challenges
including factual inconsistencies with respect to the inputs. The problem is
exacerbated by speech disfluencies and recognition errors in transcripts of
spoken language. In this paper, we explore a novel abstractive summarization
method to alleviate these challenges. Specifically, our approach learns to
produce an abstractive summary while grounding summary segments in specific
portions of the transcript to allow for full inspection of summary details. We
conduct a series of analyses of the proposed approach on a large podcast
dataset and show that the approach can achieve promising results. Grounded
summaries bring clear benefits in locating the summary and transcript segments
that contain inconsistent information, and hence significantly improve
summarization quality in both automatic and human evaluation metrics.",https://github.com/tencent-ailab/GrndPodcastSum,-1
"Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision",0.298488,"Contrastive Language-Image Pretraining (CLIP) has emerged as a novel paradigm
to learn visual models from language supervision. While researchers continue to
push the frontier of CLIP, reproducing these works remains challenging. This is
because researchers do not choose consistent training recipes and even use
different data, hampering the fair comparison between different methods. In
this work, we propose CLIP-benchmark, a first attempt to evaluate, analyze, and
benchmark CLIP and its variants. We conduct a comprehensive analysis of three
key factors: data, supervision, and model architecture. We find considerable
intuitive or counter-intuitive insights: (1). Data quality has a significant
impact on performance. (2). Certain supervision has different effects for
Convolutional Networks (ConvNets) and Vision Transformers (ViT). Applying more
proper supervision can effectively improve the performance of CLIP. (3).
Curtailing the text encoder reduces the training cost but not much affect the
final performance. Moreover, we further combine DeCLIP with FILIP, bringing us
the strongest variant DeFILIP. The CLIP-benchmark would be released at:
https://github.com/Sense-GVT/DeCLIP for future CLIP research.",https://github.com/Sense-GVT/DeCLIP,-1
Depth-Wise Attention (DWAtt): A Layer Fusion Method for Data-Efficient Classification,0.104263,"Language Models pretrained on large textual data have been shown to encode
different types of knowledge simultaneously. Traditionally, only the features
from the last layer are used when adapting to new tasks or data. We put forward
that, when using or finetuning deep pretrained models, intermediate layer
features that may be relevant to the downstream task are buried too deep to be
used efficiently in terms of needed samples or steps. To test this, we propose
a new layer fusion method: Depth-Wise Attention (DWAtt), to help re-surface
signals from non-final layers. We compare DWAtt to a basic concatenation-based
layer fusion method (Concat), and compare both to a deeper model baseline --
all kept within a similar parameter budget. Our findings show that DWAtt and
Concat are more step- and sample-efficient than the baseline, especially in the
few-shot setting. DWAtt outperforms Concat on larger data sizes. On CoNLL-03
NER, layer fusion shows 3.68--9.73% F1 gain at different few-shot sizes. The
layer fusion models presented significantly outperform the baseline in various
training scenarios with different data sizes, architectures, and training
constraints.",https://github.com/munael/dwatt-depth_wise_attention-lrec_coling_2024,-1
Quantifying Harm,0.136782,"In a companion paper (Beckers et al. 2022), we defined a qualitative notion
of harm: either harm is caused, or it is not. For practical applications, we
often need to quantify harm; for example, we may want to choose the lest
harmful of a set of possible interventions. We first present a quantitative
definition of harm in a deterministic context involving a single individual,
then we consider the issues involved in dealing with uncertainty regarding the
context and going from a notion of harm for a single individual to a notion of
""societal harm"", which involves aggregating the harm to individuals. We show
that the ""obvious"" way of doing this (just taking the expected harm for an
individual and then summing the expected harm over all individuals can lead to
counterintuitive or inappropriate answers, and discuss alternatives, drawing on
work from the decision-theory literature.",None,2215
Learning Perception-Aware Agile Flight in Cluttered Environments,0.77599,"Recently, neural control policies have outperformed existing model-based
planning-and-control methods for autonomously navigating quadrotors through
cluttered environments in minimum time. However, they are not perception aware,
a crucial requirement in vision-based navigation due to the camera's limited
field of view and the underactuated nature of a quadrotor. We propose a
learning-based system that achieves perception-aware, agile flight in cluttered
environments. Our method combines imitation learning with reinforcement
learning (RL) by leveraging a privileged learning-by-cheating framework. Using
RL, we first train a perception-aware teacher policy with full-state
information to fly in minimum time through cluttered environments. Then, we use
imitation learning to distill its knowledge into a vision-based student policy
that only perceives the environment via a camera. Our approach tightly couples
perception and control, showing a significant advantage in computation speed
(10 times faster) and success rate. We demonstrate the closed-loop control
performance using hardware-in-the-loop simulation.",None,-1
Separable Self-attention for Mobile Vision Transformers,0.916178,"Mobile vision transformers (MobileViT) can achieve state-of-the-art
performance across several mobile vision tasks, including classification and
detection. Though these models have fewer parameters, they have high latency as
compared to convolutional neural network-based models. The main efficiency
bottleneck in MobileViT is the multi-headed self-attention (MHA) in
transformers, which requires $O(k^2)$ time complexity with respect to the
number of tokens (or patches) $k$. Moreover, MHA requires costly operations
(e.g., batch-wise matrix multiplication) for computing self-attention,
impacting latency on resource-constrained devices. This paper introduces a
separable self-attention method with linear complexity, i.e. $O(k)$. A simple
yet effective characteristic of the proposed method is that it uses
element-wise operations for computing self-attention, making it a good choice
for resource-constrained devices. The improved model, MobileViTv2, is
state-of-the-art on several mobile vision tasks, including ImageNet object
classification and MS-COCO object detection. With about three million
parameters, MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet
dataset, outperforming MobileViT by about 1% while running $3.2\times$ faster
on a mobile device.
  Our source code is available at: \url{https://github.com/apple/ml-cvnets}",https://github.com/apple/ml-cvnets,-1
Gradient-Based Constrained Sampling from Language Models,0.452526,"Large pretrained language models generate fluent text but are notoriously
hard to controllably sample from. In this work, we study constrained sampling
from such language models: generating text that satisfies user-defined
constraints, while maintaining fluency and the model's performance in a
downstream task. We propose MuCoLa -- a sampling procedure that combines the
log-likelihood of the language model with arbitrary (differentiable)
constraints in a single energy function, and then generates samples in a
non-autoregressive manner. Specifically, it initializes the entire output
sequence with noise and follows a Markov chain defined by Langevin Dynamics
using the gradients of the energy function. We evaluate MuCoLa on text
generation with soft and hard constraints as well as their combinations
obtaining significant improvements over competitive baselines for toxicity
avoidance, sentiment control, and keyword-guided generation.",https://github.com/Sachin19/mucoco/tree/sampling,-1
Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics,0.567238,"Few images on the Web receive alt-text descriptions that would make them
accessible to blind and low vision (BLV) users. Image-based NLG systems have
progressed to the point where they can begin to address this persistent
societal problem, but these systems will not be fully successful unless we
evaluate them on metrics that guide their development correctly. Here, we argue
against current referenceless metrics -- those that don't rely on
human-generated ground-truth descriptions -- on the grounds that they do not
align with the needs of BLV users. The fundamental shortcoming of these metrics
is that they do not take context into account, whereas contextual information
is highly valued by BLV users. To substantiate these claims, we present a study
with BLV participants who rated descriptions along a variety of dimensions. An
in-depth analysis reveals that the lack of context-awareness makes current
referenceless metrics inadequate for advancing image accessibility. As a
proof-of-concept, we provide a contextual version of the referenceless metric
CLIPScore which begins to address the disconnect to the BLV data. An accessible
HTML version of this paper is available at
https://elisakreiss.github.io/contextual-description-evaluation/paper/reflessmetrics.html",https://github.com/elisakreiss/contextual-description-evaluation,38178
Boundary-Guided Camouflaged Object Detection,0.764698,"Camouflaged object detection (COD), segmenting objects that are elegantly
blended into their surroundings, is a valuable yet challenging task. Existing
deep-learning methods often fall into the difficulty of accurately identifying
the camouflaged object with complete and fine object structure. To this end, in
this paper, we propose a novel boundary-guided network (BGNet) for camouflaged
object detection. Our method explores valuable and extra object-related edge
semantics to guide representation learning of COD, which forces the model to
generate features that highlight object structure, thereby promoting
camouflaged object detection of accurate boundary localization. Extensive
experiments on three challenging benchmark datasets demonstrate that our BGNet
significantly outperforms the existing 18 state-of-the-art methods under four
widely-used evaluation metrics. Our code is publicly available at:
https://github.com/thograce/BGNet.",None,-1
"Interventions, Where and How? Experimental Design for Causal Models at Scale",0.738378,"Causal discovery from observational and interventional data is challenging
due to limited data and non-identifiability: factors that introduce uncertainty
in estimating the underlying structural causal model (SCM). Selecting
experiments (interventions) based on the uncertainty arising from both factors
can expedite the identification of the SCM. Existing methods in experimental
design for causal discovery from limited data either rely on linear assumptions
for the SCM or select only the intervention target. This work incorporates
recent advances in Bayesian causal discovery into the Bayesian optimal
experimental design framework, allowing for active causal discovery of large,
nonlinear SCMs while selecting both the interventional target and the value. We
demonstrate the performance of the proposed method on synthetic graphs
(Erdos-R\`enyi, Scale Free) for both linear and nonlinear SCMs as well as on
the \emph{in-silico} single-cell gene regulatory network dataset, DREAM.",https://github.com/yannadani/cbed,-1
Speaker adaptation for Wav2vec2 based dysarthric ASR,0.622052,"Dysarthric speech recognition has posed major challenges due to lack of
training data and heavy mismatch in speaker characteristics. Recent ASR systems
have benefited from readily available pretrained models such as wav2vec2 to
improve the recognition performance. Speaker adaptation using fMLLR and
xvectors have provided major gains for dysarthric speech with very little
adaptation data. However, integration of wav2vec2 with fMLLR features or
xvectors during wav2vec2 finetuning is yet to be explored. In this work, we
propose a simple adaptation network for fine-tuning wav2vec2 using fMLLR
features. The adaptation network is also flexible to handle other speaker
adaptive features such as xvectors. Experimental analysis show steady
improvements using our proposed approach across all impairment severity levels
and attains 57.72\% WER for high severity in UASpeech dataset. We also
performed experiments on German dataset to substantiate the consistency of our
proposed approach across diverse domains.",None,-1
Inharmonious Region Localization via Recurrent Self-Reasoning,0.0898163,"Synthetic images created by image editing operations are prevalent, but the
color or illumination inconsistency between the manipulated region and
background may make it unrealistic. Thus, it is important yet challenging to
localize the inharmonious region to improve the quality of synthetic image.
Inspired by the classic clustering algorithm, we aim to group pixels into two
clusters: inharmonious cluster and background cluster by inserting a novel
Recurrent Self-Reasoning (RSR) module into the bottleneck of UNet structure.
The mask output from RSR module is provided for the decoder as attention
guidance. Finally, we adaptively combine the masks from RSR and the decoder to
form our final mask. Experimental results on the image harmonization dataset
demonstrate that our method achieves competitive performance both
quantitatively and qualitatively.",None,-1
A Unified Framework for Multi-intent Spoken Language Understanding with prompting,0.0285613,"Multi-intent Spoken Language Understanding has great potential for widespread
implementation. Jointly modeling Intent Detection and Slot Filling in it
provides a channel to exploit the correlation between intents and slots.
However, current approaches are apt to formulate these two sub-tasks
differently, which leads to two issues: 1) It hinders models from effective
extraction of shared features. 2) Pretty complicated structures are involved to
enhance expression ability while causing damage to the interpretability of
frameworks. In this work, we describe a Prompt-based Spoken Language
Understanding (PromptSLU) framework, to intuitively unify two sub-tasks into
the same form by offering a common pre-trained Seq2Seq model. In detail, ID and
SF are completed by concisely filling the utterance into task-specific prompt
templates as input, and sharing output formats of key-value pairs sequence.
Furthermore, variable intents are predicted first, then naturally embedded into
prompts to guide slot-value pairs inference from a semantic perspective.
Finally, we are inspired by prevalent multi-task learning to introduce an
auxiliary sub-task, which helps to learn relationships among provided labels.
Experiment results show that our framework outperforms several state-of-the-art
baselines on two public datasets.",None,-1
Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition,0.382229,"Continual Learning for Named Entity Recognition (CL-NER) aims to learn a
growing number of entity types over time from a stream of data. However, simply
learning Other-Class in the same way as new entity types amplifies the
catastrophic forgetting and leads to a substantial performance drop. The main
cause behind this is that Other-Class samples usually contain old entity types,
and the old knowledge in these Other-Class samples is not preserved properly.
Thanks to the causal inference, we identify that the forgetting is caused by
the missing causal effect from the old data. To this end, we propose a unified
causal framework to retrieve the causality from both new entity types and
Other-Class. Furthermore, we apply curriculum learning to mitigate the impact
of label noise and introduce a self-adaptive weight for balancing the causal
effects between new entity types and Other-Class. Experimental results on three
benchmark datasets show that our method outperforms the state-of-the-art method
by a large margin. Moreover, our method can be combined with the existing
state-of-the-art methods to improve the performance in CL-NER",https://github.com/zzz47zzz/CFNER,-1
Neural Architecture Search for Dense Prediction Tasks in Computer Vision,0.444779,"The success of deep learning in recent years has lead to a rising demand for
neural network architecture engineering. As a consequence, neural architecture
search (NAS), which aims at automatically designing neural network
architectures in a data-driven manner rather than manually, has evolved as a
popular field of research. With the advent of weight sharing strategies across
architectures, NAS has become applicable to a much wider range of problems. In
particular, there are now many publications for dense prediction tasks in
computer vision that require pixel-level predictions, such as semantic
segmentation or object detection. These tasks come with novel challenges, such
as higher memory footprints due to high-resolution data, learning multi-scale
representations, longer training times, and more complex and larger neural
architectures. In this manuscript, we provide an overview of NAS for dense
prediction tasks by elaborating on these novel challenges and surveying ways to
address them to ease future research and application of existing methods to
novel problems.",None,-1
A Hierarchical Bayesian Approach to Inverse Reinforcement Learning with Symbolic Reward Machines,0.596133,"A misspecified reward can degrade sample efficiency and induce undesired
behaviors in reinforcement learning (RL) problems. We propose symbolic reward
machines for incorporating high-level task knowledge when specifying the reward
signals. Symbolic reward machines augment existing reward machine formalism by
allowing transitions to carry predicates and symbolic reward outputs. This
formalism lends itself well to inverse reinforcement learning, whereby the key
challenge is determining appropriate assignments to the symbolic values from a
few expert demonstrations. We propose a hierarchical Bayesian approach for
inferring the most likely assignments such that the concretized reward machine
can discriminate expert demonstrated trajectories from other trajectories with
high accuracy. Experimental results show that learned reward machines can
significantly improve training efficiency for complex RL tasks and generalize
well across different task environment configurations.",None,2911
Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation,0.898629,"Optimizing behaviors for dexterous manipulation has been a longstanding
challenge in robotics, with a variety of methods from model-based control to
model-free reinforcement learning having been previously explored in
literature. Perhaps one of the most powerful techniques to learn complex
manipulation strategies is imitation learning. However, collecting and learning
from demonstrations in dexterous manipulation is quite challenging. The
complex, high-dimensional action-space involved with multi-finger control often
leads to poor sample efficiency of learning-based methods. In this work, we
propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning
framework for dexterous manipulation. DIME only requires a single RGB camera to
observe a human operator and teleoperate our robotic hand. Once demonstrations
are collected, DIME employs standard imitation learning methods to train
dexterous manipulation policies. On both simulation and real robot benchmarks
we demonstrate that DIME can be used to solve complex, in-hand manipulation
tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro
hand. Our framework along with pre-collected demonstrations is publicly
available at https://nyu-robot-learning.github.io/dime.",https://nyu-robot-learning.github.io/dime,-1
Transformer based Urdu Handwritten Text Optical Character Reader,0.437774,"Extracting Handwritten text is one of the most important components of
digitizing information and making it available for large scale setting.
Handwriting Optical Character Reader (OCR) is a research problem in computer
vision and natural language processing computing, and a lot of work has been
done for English, but unfortunately, very little work has been done for low
resourced languages such as Urdu. Urdu language script is very difficult
because of its cursive nature and change of shape of characters based on it's
relative position, therefore, a need arises to propose a model which can
understand complex features and generalize it for every kind of handwriting
style. In this work, we propose a transformer based Urdu Handwritten text
extraction model. As transformers have been very successful in Natural Language
Understanding task, we explore them further to understand complex Urdu
Handwriting.",None,272
"On the interplay of adversarial robustness and architecture components: patches, convolution and attention",0.30133,"In recent years novel architecture components for image classification have
been developed, starting with attention and patches used in transformers. While
prior works have analyzed the influence of some aspects of architecture
components on the robustness to adversarial attacks, in particular for vision
transformers, the understanding of the main factors is still limited. We
compare several (non)-robust classifiers with different architectures and study
their properties, including the effect of adversarial training on the
interpretability of the learnt features and robustness to unseen threat models.
An ablation from ResNet to ConvNeXt reveals key architectural changes leading
to almost $10\%$ higher $\ell_\infty$-robustness.",https://github.com/libffcv/ffcv-imagenet,-1
Items from Psychometric Tests as Training Data for Personality Profiling Models of Twitter Users,0.678684,"Machine-learned models for author profiling in social media often rely on
data acquired via self-reporting-based psychometric tests (questionnaires)
filled out by social media users. This is an expensive but accurate data
collection strategy. Another, less costly alternative, which leads to
potentially more noisy and biased data, is to rely on labels inferred from
publicly available information in the profiles of the users, for instance
self-reported diagnoses or test results. In this paper, we explore a third
strategy, namely to directly use a corpus of items from validated psychometric
tests as training data. Items from psychometric tests often consist of
sentences from an I-perspective (e.g., ""I make friends easily.""). Such corpora
of test items constitute 'small data', but their availability for many concepts
is a rich resource. We investigate this approach for personality profiling, and
evaluate BERT classifiers fine-tuned on such psychometric test items for the
big five personality traits (openness, conscientiousness, extraversion,
agreeableness, neuroticism) and analyze various augmentation strategies
regarding their potential to address the challenges coming with such a small
corpus. Our evaluation on a publicly available Twitter corpus shows a
comparable performance to in-domain training for 4/5 personality traits with
T5-based data augmentation.",None,-1
Online Decision Transformer,0.999009,"Recent work has shown that offline reinforcement learning (RL) can be
formulated as a sequence modeling problem (Chen et al., 2021; Janner et al.,
2021) and solved via approaches similar to large-scale language modeling.
However, any practical instantiation of RL also involves an online component,
where policies pretrained on passive offline datasets are finetuned via
taskspecific interactions with the environment. We propose Online Decision
Transformers (ODT), an RL algorithm based on sequence modeling that blends
offline pretraining with online finetuning in a unified framework. Our
framework uses sequence-level entropy regularizers in conjunction with
autoregressive modeling objectives for sample-efficient exploration and
finetuning. Empirically, we show that ODT is competitive with the
state-of-the-art in absolute performance on the D4RL benchmark but shows much
more significant gains during the finetuning procedure.",https://github.com/kzl/decision-transformer,-1
Mathematical Cookbook for Snapshot Compressive Imaging,0.432806,"The author intends to provide you with a beautiful, elegant, user-friendly
cookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the
cookbook is composed of introduction, conventional optimization, and deep
equilibrium models. The latest releases are strongly recommended! For any other
questions, suggestions, or comments, feel free to email the author.",None,120
MTEB: Massive Text Embedding Benchmark,0.985442,"Text embeddings are commonly evaluated on a small set of datasets from a
single task not covering their possible applications to other tasks. It is
unclear whether state-of-the-art embeddings on semantic textual similarity
(STS) can be equally well applied to other tasks like clustering or reranking.
This makes progress in the field difficult to track, as various models are
constantly being proposed without proper evaluation. To solve this problem, we
introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding
tasks covering a total of 58 datasets and 112 languages. Through the
benchmarking of 33 models on MTEB, we establish the most comprehensive
benchmark of text embeddings to date. We find that no particular text embedding
method dominates across all tasks. This suggests that the field has yet to
converge on a universal text embedding method and scale it up sufficiently to
provide state-of-the-art results on all embedding tasks. MTEB comes with
open-source code and a public leaderboard at
https://github.com/embeddings-benchmark/mteb.",None,-1
Identifying Fixation and Saccades in Virtual Reality,0.0546458,"Gaze recognition can significantly reduce the amount of eye movement data for
a better understanding of cognitive and visual processing. Gaze recognition is
an essential precondition for eye-based interaction applications in virtual
reality. However, the three-dimensional characteristics of virtual reality
environments also pose new challenges to existing recognition algorithms. Based
on seven evaluation metrics and the Overall score (the mean of the seven
normalized metric values), we obtain optimal parameters of three existing
recognition algorithms (Velocity-Threshold Identification, Dispersion-Threshold
Identification, and Velocity & Dispersion-Threshold Identification) and our
modified Velocity & Dispersion-Threshold Identification algorithm. We compare
the performance of these four algorithms with optimal parameters. The results
show that our modified Velocity & Dispersion-Threshold Identification performs
the best. The impact of interface complexity on classification results is also
preliminarily explored. The results show that the algorithms are not sensitive
to interface complexity.",None,-1
Computational historical linguistics and language diversity in South Asia,0.837177,"South Asia is home to a plethora of languages, many of which severely lack
access to new language technologies. This linguistic diversity also results in
a research environment conducive to the study of comparative, contact, and
historical linguistics -- fields which necessitate the gathering of extensive
data from many languages. We claim that data scatteredness (rather than
scarcity) is the primary obstacle in the development of South Asian language
technology, and suggest that the study of language history is uniquely aligned
with surmounting this obstacle. We review recent developments in and at the
intersection of South Asian NLP and historical-comparative linguistics,
describing our and others' current efforts in this area. We also offer new
strategies towards breaking the data barrier.",None,-1
Robust Double-Encoder Network for RGB-D Panoptic Segmentation,0.66072,"Perception is crucial for robots that act in real-world environments, as
autonomous systems need to see and understand the world around them to act
properly. Panoptic segmentation provides an interpretation of the scene by
computing a pixelwise semantic label together with instance IDs. In this paper,
we address panoptic segmentation using RGB-D data of indoor scenes. We propose
a novel encoder-decoder neural network that processes RGB and depth separately
through two encoders. The features of the individual encoders are progressively
merged at different resolutions, such that the RGB features are enhanced using
complementary depth information. We propose a novel merging approach called
ResidualExcite, which reweighs each entry of the feature map according to its
importance. With our double-encoder architecture, we are robust to missing
cues. In particular, the same model can train and infer on RGB-D, RGB-only, and
depth-only input data, without the need to train specialized models. We
evaluate our method on publicly available datasets and show that our approach
achieves superior results compared to other common approaches for panoptic
segmentation.",https://github.com/PRBonn/PS-res-excite,-1
CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities,0.473595,"An increasing number of monitoring systems have been developed in smart
cities to ensure that real-time operations of a city satisfy safety and
performance requirements. However, many existing city requirements are written
in English with missing, inaccurate, or ambiguous information. There is a high
demand for assisting city policy makers in converting human-specified
requirements to machine-understandable formal specifications for monitoring
systems. To tackle this limitation, we build CitySpec, the first intelligent
assistant system for requirement specification in smart cities. To create
CitySpec, we first collect over 1,500 real-world city requirements across
different domains from over 100 cities and extract city-specific knowledge to
generate a dataset of city vocabulary with 3,061 words. We also build a
translation model and enhance it through requirement synthesis and develop a
novel online learning framework with validation under uncertainty. The
evaluation results on real-world city requirements show that CitySpec increases
the sentence-level accuracy of requirement specification from 59.02% to 86.64%,
and has strong adaptability to a new city and a new domain (e.g., F1 score for
requirements in Seattle increases from 77.6% to 93.75% with online learning).",None,-1
Coordinating CAV Swarms at Intersections with a Deep Learning Model,0.449175,"Connected and automated vehicles (CAVs) are viewed as a special kind of
robots that have the potential to significantly improve the safety and
efficiency of traffic. In contrast to many swarm robotics studies that are
demonstrated in labs by employing a small number of robots, CAV studies aims to
achieve cooperative driving of unceasing robot swarm flows. However, how to get
the optimal passing order of such robot swarm flows even for a signal-free
intersection is an NP-hard problem (specifically, enumerating based algorithm
takes days to find the optimal solution to a 20-CAV scenario). Here, we
introduce a novel cooperative driving algorithm (AlphaOrder) that combines
offline deep learning and online tree searching to find a near-optimal passing
order in real-time. AlphaOrder builds a pointer network model from solved
scenarios and generates near-optimal passing orders instantaneously for new
scenarios. Furthermore, our approach provides a general approach to managing
preemptive resource sharing between swarm robotics (e.g., scheduling multiple
automated guided vehicles (AGVs) and unmanned aerial vehicles (UAVs) at
conflicting areas",None,-1
RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks,0.729292,"Temporal/spatial receptive fields of models play an important role in
sequential/spatial tasks. Large receptive fields facilitate long-term
relations, while small receptive fields help to capture the local details.
Existing methods construct models with hand-designed receptive fields in
layers. Can we effectively search for receptive field combinations to replace
hand-designed patterns? To answer this question, we propose to find better
receptive field combinations through a global-to-local search scheme. Our
search scheme exploits both global search to find the coarse combinations and
local search to get the refined receptive field combinations further. The
global search finds possible coarse combinations other than human-designed
patterns. On top of the global search, we propose an expectation-guided
iterative local search scheme to refine combinations effectively. Our RF-Next
models, plugging receptive field search to various models, boost the
performance on many tasks, e.g., temporal action segmentation, object
detection, instance segmentation, and speech synthesis. The source code is
publicly available on http://mmcheng.net/rfnext.",http://mmcheng.net/rfnext,-1
Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning,0.599205,"Non-exemplar class-incremental learning is to recognize both the old and new
classes when old class samples cannot be saved. It is a challenging task since
representation optimization and feature retention can only be achieved under
supervision from new classes. To address this problem, we propose a novel
self-sustaining representation expansion scheme. Our scheme consists of a
structure reorganization strategy that fuses main-branch expansion and
side-branch updating to maintain the old features, and a main-branch
distillation scheme to transfer the invariant knowledge. Furthermore, a
prototype selection mechanism is proposed to enhance the discrimination between
the old and new classes by selectively incorporating new samples into the
distillation process. Extensive experiments on three benchmarks demonstrate
significant incremental performance, outperforming the state-of-the-art methods
by a margin of 3%, 3% and 6%, respectively.",None,-1
"V3GAN: Decomposing Background, Foreground and Motion for Video Generation",0.157259,"Video generation is a challenging task that requires modeling plausible
spatial and temporal dynamics in a video. Inspired by how humans perceive a
video by grouping a scene into moving and stationary components, we propose a
method that decomposes the task of video generation into the synthesis of
foreground, background and motion. Foreground and background together describe
the appearance, whereas motion specifies how the foreground moves in a video
over time. We propose V3GAN, a novel three-branch generative adversarial
network where two branches model foreground and background information, while
the third branch models the temporal information without any supervision. The
foreground branch is augmented with our novel feature-level masking layer that
aids in learning an accurate mask for foreground and background separation. To
encourage motion consistency, we further propose a shuffling loss for the video
discriminator. Extensive quantitative and qualitative analysis on synthetic as
well as real-world benchmark datasets demonstrates that V3GAN outperforms the
state-of-the-art methods by a significant margin.",None,546
Extracting Effective Subnetworks with Gumbel-Softmax,0.104179,"Large and performant neural networks are often overparameterized and can be
drastically reduced in size and complexity thanks to pruning. Pruning is a
group of methods, which seeks to remove redundant or unnecessary weights or
groups of weights in a network. These techniques allow the creation of
lightweight networks, which are particularly critical in embedded or mobile
applications. In this paper, we devise an alternative pruning method that
allows extracting effective subnetworks from larger untrained ones. Our method
is stochastic and extracts subnetworks by exploring different topologies which
are sampled using Gumbel Softmax. The latter is also used to train probability
distributions which measure the relevance of weights in the sampled topologies.
The resulting subnetworks are further enhanced using a highly efficient
rescaling mechanism that reduces training time and improves performance.
Extensive experiments conducted on CIFAR show the outperformance of our
subnetwork extraction method against the related work.",https://github.com/N0ciple/ASLP,-1
Do ever larger octopi still amplify reporting biases? Evidence from judgments of typical colour,0.178424,"Language models (LMs) trained on raw texts have no direct access to the
physical world. Gordon and Van Durme (2013) point out that LMs can thus suffer
from reporting bias: texts rarely report on common facts, instead focusing on
the unusual aspects of a situation. If LMs are only trained on text corpora and
naively memorise local co-occurrence statistics, they thus naturally would
learn a biased view of the physical world. While prior studies have repeatedly
verified that LMs of smaller scales (e.g., RoBERTa, GPT-2) amplify reporting
bias, it remains unknown whether such trends continue when models are scaled
up. We investigate reporting bias from the perspective of colour in larger
language models (LLMs) such as PaLM and GPT-3. Specifically, we query LLMs for
the typical colour of objects, which is one simple type of perceptually
grounded physical common sense. Surprisingly, we find that LLMs significantly
outperform smaller LMs in determining an object's typical colour and more
closely track human judgments, instead of overfitting to surface patterns
stored in texts. This suggests that very large models of language alone are
able to overcome certain types of reporting bias that are characterized by
local co-occurrences.",https://github.com/google-research/language/tree/master/language/octopus-llm,-1
Shadow-Background-Noise 3D Spatial Decomposition Using Sparse Low-Rank Gaussian Properties for Video-SAR Moving Target Shadow Enhancement,0.95926,"Moving target shadows among video synthetic aperture radar (Video-SAR) images
are always interfered by low scattering backgrounds and cluttered noises,
causing poor detec-tion-tracking accuracy. Thus, a shadow-background-noise 3D
spatial decomposition (SBN-3D-SD) model is proposed to enhance shadows for
higher detection-tracking accuracy. It leverages the sparse property of
shadows, the low-rank property of back-grounds, and the Gaussian property of
noises to perform 3D spatial three-decomposition. It separates shadows from
back-grounds and noises by the alternating direction method of multi-pliers
(ADMM). Results on the Sandia National Laboratories (SNL) data verify its
effectiveness. It boosts the shadow saliency from the qualitative and
quantitative evaluation. It boosts the shadow detection accuracy of Faster
R-CNN, RetinaNet and YOLOv3. It also boosts the shadow tracking accuracy of
TransTrack, FairMOT and ByteTrack.",None,-1
Controllable Fake Document Infilling for Cyber Deception,0.532892,"Recent works in cyber deception study how to deter malicious intrusion by
generating multiple fake versions of a critical document to impose costs on
adversaries who need to identify the correct information. However, existing
approaches are context-agnostic, resulting in sub-optimal and unvaried outputs.
We propose a novel context-aware model, Fake Document Infilling (FDI), by
converting the problem to a controllable mask-then-infill procedure. FDI masks
important concepts of varied lengths in the document, then infills a realistic
but fake alternative considering both the previous and future contexts. We
conduct comprehensive evaluations on technical documents and news stories.
Results show that FDI outperforms the baselines in generating highly believable
fakes with moderate modification to protect critical information and deceive
adversaries.",https://github.com/snowood1/FDI,-1
SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation,0.535058,"Feature regression is a simple way to distill large neural network models to
smaller ones. We show that with simple changes to the network architecture,
regression can outperform more complex state-of-the-art approaches for
knowledge distillation from self-supervised models. Surprisingly, the addition
of a multi-layer perceptron head to the CNN backbone is beneficial even if used
only during distillation and discarded in the downstream task. Deeper
non-linear projections can thus be used to accurately mimic the teacher without
changing inference architecture and time. Moreover, we utilize independent
projection heads to simultaneously distill multiple teacher networks. We also
find that using the same weakly augmented image as input for both teacher and
student networks aids distillation. Experiments on ImageNet dataset demonstrate
the efficacy of the proposed changes in various self-supervised distillation
settings.",https://github.com/UCDvision/simreg,-1
TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition,0.531379,"Creation of 3D content by stylization is a promising yet challenging problem
in computer vision and graphics research. In this work, we focus on stylizing
photorealistic appearance renderings of a given surface mesh of arbitrary
topology. Motivated by the recent surge of cross-modal supervision of the
Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which
transfers the appearance style of a given 3D shape according to a text prompt
in a photorealistic manner. Technically, we propose to disentangle the
appearance style as the spatially varying bidirectional reflectance
distribution function, the local geometric variation, and the lighting
condition, which are jointly optimized, via supervision of the CLIP loss, by a
spherical Gaussians based differentiable renderer. As such, TANGO enables
photorealistic 3D style transfer by automatically predicting reflectance
effects even for bare, low-quality meshes, without training on a task-specific
dataset. Extensive experiments show that TANGO outperforms existing methods of
text-driven 3D style transfer in terms of photorealistic quality, consistency
of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and
results are available at our project webpage https://cyw-3d.github.io/tango/.",https://cyw-3d.github.io/tango,-1
Learning functional sections in medical conversations: iterative pseudo-labeling and human-in-the-loop approach,0.156919,"Medical conversations between patients and medical professionals have
implicit functional sections, such as ""history taking"", ""summarization"",
""education"", and ""care plan."" In this work, we are interested in learning to
automatically extract these sections. A direct approach would require
collecting large amounts of expert annotations for this task, which is
inherently costly due to the contextual inter-and-intra variability between
these sections. This paper presents an approach that tackles the problem of
learning to classify medical dialogue into functional sections without
requiring a large number of annotations. Our approach combines pseudo-labeling
and human-in-the-loop. First, we bootstrap using weak supervision with
pseudo-labeling to generate dialogue turn-level pseudo-labels and train a
transformer-based model, which is then applied to individual sentences to
create noisy sentence-level labels. Second, we iteratively refine
sentence-level labels using a cluster-based human-in-the-loop approach. Each
iteration requires only a few dozen annotator decisions. We evaluate the
results on an expert-annotated dataset of 100 dialogues and find that while our
models start with 69.5% accuracy, we can iteratively improve it to 82.5%. The
code used to perform all experiments described in this paper can be found here:
https://github.com/curai/curai-research/tree/main/functional-sections.",https://github.com/curai/curai-research/tree/main/functional-sections.,-1
Accurate Action Recommendation for Smart Home via Two-Level Encoders and Commonsense Knowledge,0.222751,"How can we accurately recommend actions for users to control their devices at
home? Action recommendation for smart home has attracted increasing attention
due to its potential impact on the markets of virtual assistants and Internet
of Things (IoT). However, designing an effective action recommender system for
smart home is challenging because it requires handling context correlations,
considering both queried contexts and previous histories of users, and dealing
with capricious intentions in history. In this work, we propose SmartSense, an
accurate action recommendation method for smart home. For individual action,
SmartSense summarizes its device control and its temporal contexts in a
self-attentive manner, to reflect the importance of the correlation between
them. SmartSense then summarizes sequences of users considering queried
contexts in a query-attentive manner to extract the query-related patterns from
the sequential actions. SmartSense also transfers the commonsense knowledge
from routine data to better handle intentions in action sequences. As a result,
SmartSense addresses all three main challenges of action recommendation for
smart home, and achieves the state-of-the-art performance giving up to 9.8%
higher mAP@1 than the best competitor.",https://github.com/snudatalab/SmartSense,-1
Improving Simultaneous Machine Translation with Monolingual Data,0.448988,"Simultaneous machine translation (SiMT) is usually done via sequence-level
knowledge distillation (Seq-KD) from a full-sentence neural machine translation
(NMT) model. However, there is still a significant performance gap between NMT
and SiMT. In this work, we propose to leverage monolingual data to improve
SiMT, which trains a SiMT student on the combination of bilingual data and
external monolingual data distilled by Seq-KD. Preliminary experiments on En-Zh
and En-Ja news domain corpora demonstrate that monolingual data can
significantly improve translation quality (e.g., +3.15 BLEU on En-Zh). Inspired
by the behavior of human simultaneous interpreters, we propose a novel
monolingual sampling strategy for SiMT, considering both chunk length and
monotonicity. Experimental results show that our sampling strategy consistently
outperforms the random sampling strategy (and other conventional typical NMT
monolingual sampling strategies) by avoiding the key problem of SiMT --
hallucination, and has better scalability. We achieve +0.72 BLEU improvements
on average against random sampling on En-Zh and En-Ja. Data and codes can be
found at https://github.com/hexuandeng/Mono4SiMT.",https://github.com/hexuandeng/Mono4SiMT,123783
LwPosr: Lightweight Efficient Fine-Grained Head Pose Estimation,0.425235,"This paper presents a lightweight network for head pose estimation (HPE)
task. While previous approaches rely on convolutional neural networks, the
proposed network \textit{LwPosr} uses mixture of depthwise separable
convolutional (DSC) and transformer encoder layers which are structured in two
streams and three stages to provide fine-grained regression for predicting head
poses. The quantitative and qualitative demonstration is provided to show that
the proposed network is able to learn head poses efficiently while using less
parameter space. Extensive ablations are conducted using three open-source
datasets namely 300W-LP, AFLW2000, and BIWI datasets. To our knowledge, (1)
\textit{LwPosr} is the lightest network proposed for estimating head poses
compared to both keypoints-based and keypoints-free approaches; (2) it sets a
benchmark for both overperforming the previous lightweight network on mean
absolute error and on reducing number of parameters; (3) it is first of its
kind to use mixture of DSCs and transformer encoders for HPE. This approach is
suitable for mobile devices which require lightweight networks.",None,-1
Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers,0.298557,"Recently, RNN-Transducers have achieved remarkable results on various
automatic speech recognition tasks. However, lattice-free sequence
discriminative training methods, which obtain superior performance in hybrid
models, are rarely investigated in RNN-Transducers. In this work, we propose
three lattice-free training objectives, namely lattice-free maximum mutual
information, lattice-free segment-level minimum Bayes risk, and lattice-free
minimum Bayes risk, which are used for the final posterior output of the
phoneme-based neural transducer with a limited context dependency. Compared to
criteria using N-best lists, lattice-free methods eliminate the decoding step
for hypotheses generation during training, which leads to more efficient
training. Experimental results show that lattice-free methods gain up to 6.5%
relative improvement in word error rate compared to a sequence-level
cross-entropy trained model. Compared to the N-best-list based minimum Bayes
risk objectives, lattice-free methods gain 40% - 70% relative training time
speedup with a small degradation in performance.",None,-1
Multi-modal Video Chapter Generation,0.137801,"Chapter generation becomes practical technique for online videos nowadays.
The chapter breakpoints enable users to quickly find the parts they want and
get the summative annotations. However, there is no public method and dataset
for this task. To facilitate the research along this direction, we introduce a
new dataset called Chapter-Gen, which consists of approximately 10k
user-generated videos with annotated chapter information. Our data collection
procedure is fast, scalable and does not require any additional manual
annotation. On top of this dataset, we design an effective baseline specificlly
for video chapters generation task. which captures two aspects of a
video,including visual dynamics and narration text. It disentangles local and
global video features for localization and title generation respectively. To
parse the long video efficiently, a skip sliding window mechanism is designed
to localize potential chapters. And a cross attention multi-modal fusion module
is developed to aggregate local features for title generation. Our experiments
demonstrate that the proposed framework achieves superior results over existing
methods which illustrate that the method design for similar task cannot be
transfered directly even after fine-tuning. Code and dataset are available at
https://github.com/czt117/MVCG.",https://github.com/czt117/MVCG,2561
Synthetic Distracted Driving (SynDD2) dataset for analyzing distracted behaviors and various gaze zones of a driver,0.773011,"This article presents a synthetic distracted driving (SynDD2 - a continuum of
SynDD1) dataset for machine learning models to detect and analyze drivers'
various distracted behavior and different gaze zones. We collected the data in
a stationary vehicle using three in-vehicle cameras positioned at locations: on
the dashboard, near the rearview mirror, and on the top right-side window
corner. The dataset contains two activity types: distracted activities and gaze
zones for each participant, and each activity type has two sets: without
appearance blocks and with appearance blocks such as wearing a hat or
sunglasses. The order and duration of each activity for each participant are
random. In addition, the dataset contains manual annotations for each activity,
having its start and end time annotated. Researchers could use this dataset to
evaluate the performance of machine learning algorithms to classify various
distracting activities and gaze zones of drivers.",None,-1
Object-wise Masked Autoencoders for Fast Pre-training,0.332349,"Self-supervised pre-training for images without labels has recently achieved
promising performance in image classification. The success of transformer-based
methods, ViT and MAE, draws the community's attention to the design of backbone
architecture and self-supervised task. In this work, we show that current
masked image encoding models learn the underlying relationship between all
objects in the whole scene, instead of a single object representation.
Therefore, those methods bring a lot of compute time for self-supervised
pre-training. To solve this issue, we introduce a novel object selection and
division strategy to drop non-object patches for learning object-wise
representations by selective reconstruction with interested region masks. We
refer to this method ObjMAE. Extensive experiments on four commonly-used
datasets demonstrate the effectiveness of our model in reducing the compute
cost by 72% while achieving competitive performance. Furthermore, we
investigate the inter-object and intra-object relationship and find that the
latter is crucial for self-supervised pre-training.",https://github.com/facebookresearch/clevr-dataset-gen,-1
RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset for Natural Disaster Damage Assessment,0.063903,"Recent advancements in computer vision and deep learning techniques have
facilitated notable progress in scene understanding, thereby assisting rescue
teams in achieving precise damage assessment. In this paper, we present
RescueNet, a meticulously curated high-resolution post-disaster dataset that
includes detailed classification and semantic segmentation annotations. This
dataset aims to facilitate comprehensive scene understanding in the aftermath
of natural disasters. RescueNet comprises post-disaster images collected after
Hurricane Michael, obtained using Unmanned Aerial Vehicles (UAVs) from multiple
impacted regions. The uniqueness of RescueNet lies in its provision of
high-resolution post-disaster imagery, accompanied by comprehensive annotations
for each image. Unlike existing datasets that offer annotations limited to
specific scene elements such as buildings, RescueNet provides pixel-level
annotations for all classes, including buildings, roads, pools, trees, and
more. Furthermore, we evaluate the utility of the dataset by implementing
state-of-the-art segmentation models on RescueNet, demonstrating its value in
enhancing existing methodologies for natural disaster damage assessment.",https://github.com/BinaLab/RescueNet-A-High-Resolution-Post-Disaster-UAV-Dataset-for-Semantic-Segmentation/tree/main,1500
Label Semantics for Few Shot Named Entity Recognition,0.969235,"We study the problem of few shot learning for named entity recognition.
Specifically, we leverage the semantic information in the names of the labels
as a way of giving the model additional signal and enriched priors. We propose
a neural architecture that consists of two BERT encoders, one to encode the
document and its tokens and another one to encode each of the labels in natural
language format. Our model learns to match the representations of named
entities computed by the first encoder with label representations computed by
the second encoder. The label semantics signal is shown to support improved
state-of-the-art results in multiple few shot NER benchmarks and on-par
performance in standard benchmarks. Our model is especially effective in low
resource settings.",None,-1
"A Distant Supervision Corpus for Extracting Biomedical Relationships Between Chemicals, Diseases and Genes",0.451174,"We introduce ChemDisGene, a new dataset for training and evaluating
multi-class multi-label document-level biomedical relation extraction models.
Our dataset contains 80k biomedical research abstracts labeled with mentions of
chemicals, diseases, and genes, portions of which human experts labeled with 18
types of biomedical relationships between these entities (intended for
evaluation), and the remainder of which (intended for training) has been
distantly labeled via the CTD database with approximately 78\% accuracy. In
comparison to similar preexisting datasets, ours is both substantially larger
and cleaner; it also includes annotations linking mentions to their entities.
We also provide three baseline deep neural network relation extraction models
trained and evaluated on our new dataset.",https://github.com/chanzuckerberg/ChemDisGene,-1
Multi-Document Summarization with Centroid-Based Pretraining,0.882681,"In Multi-Document Summarization (MDS), the input can be modeled as a set of
documents, and the output is its summary. In this paper, we focus on
pretraining objectives for MDS. Specifically, we introduce a novel pretraining
objective, which involves selecting the ROUGE-based centroid of each document
cluster as a proxy for its summary. Our objective thus does not require human
written summaries and can be utilized for pretraining on a dataset consisting
solely of document sets. Through zero-shot, few-shot, and fully supervised
experiments on multiple MDS datasets, we show that our model Centrum is better
or comparable to a state-of-the-art model. We make the pretrained and
fine-tuned models freely available to the research community
https://github.com/ratishsp/centrum.",https://github.com/ratishsp/centrum,-1
Object Permanence Emerges in a Random Walk along Memory,0.102205,"This paper proposes a self-supervised objective for learning representations
that localize objects under occlusion - a property known as object permanence.
A central question is the choice of learning signal in cases of total
occlusion. Rather than directly supervising the locations of invisible objects,
we propose a self-supervised objective that requires neither human annotation,
nor assumptions about object dynamics. We show that object permanence can
emerge by optimizing for temporal coherence of memory: we fit a Markov walk
along a space-time graph of memories, where the states in each time step are
non-Markovian features from a sequence encoder. This leads to a memory
representation that stores occluded objects and predicts their motion, to
better localize them. The resulting model outperforms existing approaches on
several datasets of increasing complexity and realism, despite requiring
minimal supervision, and hence being broadly applicable.",None,-1
"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish",0.328077,"The availability of compute and data to train larger and larger language
models increases the demand for robust methods of benchmarking the true
progress of LM training. Recent years witnessed significant progress in
standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or
KILT have become de facto standard tools to compare large language models.
Following the trend to replicate GLUE for other languages, the KLEJ benchmark
has been released for Polish. In this paper, we evaluate the progress in
benchmarking for low-resourced languages. We note that only a handful of
languages have such comprehensive benchmarks. We also note the gap in the
number of tasks being evaluated by benchmarks for resource-rich English/Chinese
and the rest of the world. In this paper, we introduce LEPISZCZE (the Polish
word for glew, the Middle English predecessor of glue), a new, comprehensive
benchmark for Polish NLP with a large variety of tasks and high-quality
operationalization of the benchmark. We design LEPISZCZE with flexibility in
mind. Including new models, datasets, and tasks is as simple as possible while
still offering data versioning and model tracking. In the first run of the
benchmark, we test 13 experiments (task and dataset pairs) based on the five
most recent LMs for Polish. We use five datasets from the Polish benchmark and
add eight novel datasets. As the paper's main contribution, apart from
LEPISZCZE, we provide insights and experiences learned while creating the
benchmark for Polish as the blueprint to design similar benchmarks for other
low-resourced languages.",https://github.com/CLARIN-PL/LEPISZCZE,-1
PolyU-BPCoMa: A Dataset and Benchmark Towards Mobile Colorized Mapping Using a Backpack Multisensorial System,0.479679,"Constructing colorized point clouds from mobile laser scanning and images is
a fundamental work in surveying and mapping. It is also an essential
prerequisite for building digital twins for smart cities. However, existing
public datasets are either in relatively small scales or lack accurate
geometrical and color ground truth. This paper documents a multisensorial
dataset named PolyU-BPCoMA which is distinctively positioned towards mobile
colorized mapping. The dataset incorporates resources of 3D LiDAR, spherical
imaging, GNSS and IMU on a backpack platform. Color checker boards are pasted
in each surveyed area as targets and ground truth data are collected by an
advanced terrestrial laser scanner (TLS). 3D geometrical and color information
can be recovered in the colorized point clouds produced by the backpack system
and the TLS, respectively. Accordingly, we provide an opportunity to benchmark
the mapping and colorization accuracy simultaneously for a mobile
multisensorial system. The dataset is approximately 800 GB in size covering
both indoor and outdoor environments. The dataset and development kits are
available at https://github.com/chenpengxin/PolyU-BPCoMa.git.",https://github.com/chenpengxin/PolyU-BPCoMa.git,-1
BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate Speech in Different Social Contexts,0.763829,"Social media platforms and online streaming services have spawned a new breed
of Hate Speech (HS). Due to the massive amount of user-generated content on
these sites, modern machine learning techniques are found to be feasible and
cost-effective to tackle this problem. However, linguistically diverse datasets
covering different social contexts in which offensive language is typically
used are required to train generalizable models. In this paper, we identify the
shortcomings of existing Bangla HS datasets and introduce a large manually
labeled dataset BD-SHS that includes HS in different social contexts. The
labeling criteria were prepared following a hierarchical annotation process,
which is the first of its kind in Bangla HS to the best of our knowledge. The
dataset includes more than 50,200 offensive comments crawled from online social
networking sites and is at least 60% larger than any existing Bangla HS
datasets. We present the benchmark result of our dataset by training different
NLP models resulting in the best one achieving an F1-score of 91.0%. In our
experiments, we found that a word embedding trained exclusively using 1.47
million comments from social media and streaming sites consistently resulted in
better modeling of HS detection in comparison to other pre-trained embeddings.
Our dataset and all accompanying codes is publicly available at
github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media",https://github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media,-1
Data-driven prediction of Air Traffic Controllers reactions to resolving conflicts,0.692534,"With the aim to enhance automation in conflict detection and resolution
(CD&R) tasks in the Air Traffic Management domain, in this paper we propose
deep learning techniques (DL) that can learn models of Air Traffic Controllers'
(ATCO) reactions in resolving conflicts that can violate separation minimum
constraints among aircraft trajectories: This implies learning when the ATCO
will react towards resolving a conflict, and how he/she will react. Timely
reactions, to which this paper aims, focus on when do reactions happen, aiming
to predict the trajectory points, as the trajectory evolves, that the ATCO
issues a conflict resolution action, while also predicting the type of
resolution action (if any). Towards this goal, the paper formulates the ATCO
reactions prediction problem for CD&R, and presents DL methods that can model
ATCO timely reactions and evaluates these methods in real-world data sets,
showing their efficacy in prediction with very high accuracy.",None,4539
Learn what matters: cross-domain imitation learning with task-relevant embeddings,0.613829,"We study how an autonomous agent learns to perform a task from demonstrations
in a different domain, such as a different environment or different agent. Such
cross-domain imitation learning is required to, for example, train an
artificial agent from demonstrations of a human expert. We propose a scalable
framework that enables cross-domain imitation learning without access to
additional demonstrations or further domain knowledge. We jointly train the
learner agent's policy and learn a mapping between the learner and expert
domains with adversarial training. We effect this by using a mutual information
criterion to find an embedding of the expert's state space that contains
task-relevant information and is invariant to domain specifics. This step
significantly simplifies estimating the mapping between the learner and expert
domains and hence facilitates end-to-end learning. We demonstrate successful
transfer of policies between considerably different domains, without extra
supervision such as additional demonstrations, and in situations where other
methods fail.",https://github.com/HumanCompatibleAI/seals,-1
RepMix: Representation Mixing for Robust Attribution of Synthesized Images,0.779171,"Rapid advances in Generative Adversarial Networks (GANs) raise new challenges
for image attribution; detecting whether an image is synthetic and, if so,
determining which GAN architecture created it. Uniquely, we present a solution
to this task capable of 1) matching images invariant to their semantic content;
2) robust to benign transformations (changes in quality, resolution, shape,
etc.) commonly encountered as images are re-shared online. In order to
formalize our research, a challenging benchmark, Attribution88, is collected
for robust and practical image attribution. We then propose RepMix, our GAN
fingerprinting technique based on representation mixing and a novel loss. We
validate its capability of tracing the provenance of GAN-generated images
invariant to the semantic content of the image and also robust to
perturbations. We show our approach improves significantly from existing GAN
fingerprinting works on both semantic generalization and robustness. Data and
code are available at https://github.com/TuBui/image_attribution.",https://github.com/TuBui/image-attribution,-1
Fusing Convolutional Neural Network and Geometric Constraint for Image-based Indoor Localization,0.82384,"This paper proposes a new image-based localization framework that explicitly
localizes the camera/robot by fusing Convolutional Neural Network (CNN) and
sequential images' geometric constraints. The camera is localized using a
single or few observed images and training images with 6-degree-of-freedom pose
labels. A Siamese network structure is adopted to train an image descriptor
network, and the visually similar candidate image in the training set is
retrieved to localize the testing image geometrically. Meanwhile, a
probabilistic motion model predicts the pose based on a constant velocity
assumption. The two estimated poses are finally fused using their uncertainties
to yield an accurate pose prediction. This method leverages the geometric
uncertainty and is applicable in indoor scenarios predominated by diffuse
illumination. Experiments on simulation and real data sets demonstrate the
efficiency of our proposed method. The results further show that combining the
CNN-based framework with geometric constraint achieves better accuracy when
compared with CNN-only methods, especially when the training data size is
small.",None,-1
The Legal Argument Reasoning Task in Civil Procedure,0.573754,"We present a new NLP task and dataset from the domain of the U.S. civil
procedure. Each instance of the dataset consists of a general introduction to
the case, a particular question, and a possible solution argument, accompanied
by a detailed analysis of why the argument applies in that case. Since the
dataset is based on a book aimed at law students, we believe that it represents
a truly complex task for benchmarking modern legal language models. Our
baseline evaluation shows that fine-tuning a legal transformer provides some
advantage over random baseline models, but our analysis reveals that the actual
ability to infer legal arguments remains a challenging open research question.",https://github.com/trusthlt/legal-argument-reasoning-task,-1
R4D: Utilizing Reference Objects for Long-Range Distance Estimation,0.298276,"Estimating the distance of objects is a safety-critical task for autonomous
driving. Focusing on short-range objects, existing methods and datasets neglect
the equally important long-range objects. In this paper, we introduce a
challenging and under-explored task, which we refer to as Long-Range Distance
Estimation, as well as two datasets to validate new methods developed for this
task. We then proposeR4D, the first framework to accurately estimate the
distance of long-range objects by using references with known distances in the
scene. Drawing inspiration from human perception, R4D builds a graph by
connecting a target object to all references. An edge in the graph encodes the
relative distance information between a pair of target and reference objects.
An attention module is then used to weigh the importance of reference objects
and combine them into one target object distance prediction. Experiments on the
two proposed datasets demonstrate the effectiveness and robustness of R4D by
showing significant improvements compared to existing baselines. We are looking
to make the proposed dataset, Waymo OpenDataset - Long-Range Labels, available
publicly at waymo.com/open/download.",None,-1
Generative Multiplane Images: Making a 2D GAN 3D-Aware,0.814304,"What is really needed to make an existing 2D GAN 3D-aware? To answer this
question, we modify a classical GAN, i.e., StyleGANv2, as little as possible.
We find that only two modifications are absolutely necessary: 1) a multiplane
image style generator branch which produces a set of alpha maps conditioned on
their depth; 2) a pose-conditioned discriminator. We refer to the generated
output as a 'generative multiplane image' (GMPI) and emphasize that its
renderings are not only high-quality but also guaranteed to be view-consistent,
which makes GMPIs different from many prior works. Importantly, the number of
alpha maps can be dynamically adjusted and can differ between training and
inference, alleviating memory concerns and enabling fast training of GMPIs in
less than half a day at a resolution of $1024^2$. Our findings are consistent
across three challenging and common high-resolution datasets, including FFHQ,
AFHQv2, and MetFaces.",https://github.com/apple/ml-gmpi,-1
HEATGait: Hop-Extracted Adjacency Technique in Graph Convolution based Gait Recognition,0.152832,"Biometric authentication using gait has become a promising field due to its
unobtrusive nature. Recent approaches in model-based gait recognition
techniques utilize spatio-temporal graphs for the elegant extraction of gait
features. However, existing methods often rely on multi-scale operators for
extracting long-range relationships among joints resulting in biased weighting.
In this paper, we present HEATGait, a gait recognition system that improves the
existing multi-scale graph convolution by efficient hop-extraction technique to
alleviate the issue. Combined with preprocessing and augmentation techniques,
we propose a powerful feature extractor that utilizes ResGCN to achieve
state-of-the-art performance in model-based gait recognition on the CASIA-B
gait dataset.",None,-1
Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training,0.15688,"In this paper, we present a cross-modal recipe retrieval framework,
Transformer-based Network for Large Batch Training (TNLBT), which is inspired
by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).
TNLBT aims to accomplish retrieval tasks while generating images from recipe
embeddings. We apply the Hierarchical Transformer-based recipe text encoder,
the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial
network architecture to enable better cross-modal embedding learning for recipe
texts and images. In addition, we use self-supervised learning to exploit the
rich information in the recipe texts having no corresponding images. Since
contrastive learning could benefit from a larger batch size according to the
recent literature on self-supervised learning, we adopt a large batch size
during training and have validated its effectiveness. In the experiments, the
proposed framework significantly outperformed the current state-of-the-art
frameworks in both cross-modal recipe retrieval and image generation tasks on
the benchmark Recipe1M. This is the first work which confirmed the
effectiveness of large batch training on cross-modal recipe embeddings.",https://github.com/mseitzer/pytorch-fid,-1
Extractive Summarization of Legal Decisions using Multi-task Learning and Maximal Marginal Relevance,0.854222,"Summarizing legal decisions requires the expertise of law practitioners,
which is both time- and cost-intensive. This paper presents techniques for
extractive summarization of legal decisions in a low-resource setting using
limited expert annotated data. We test a set of models that locate relevant
content using a sequential model and tackle redundancy by leveraging maximal
marginal relevance to compose summaries. We also demonstrate an implicit
approach to help train our proposed models generate more informative summaries.
Our multi-task learning model variant leverages rhetorical role identification
as an auxiliary task to further improve the summarizer. We perform extensive
experiments on datasets containing legal decisions from the US Board of
Veterans' Appeals and conduct quantitative and expert-ranked evaluations of our
models. Our results show that the proposed approaches can achieve ROUGE scores
vis-\`a-vis expert extracted summaries that match those achieved by
inter-annotator comparison.",https://github.com/LLTLab/VetClaims-JSON,-1
"Less Data, More Knowledge: Building Next Generation Semantic Communication Networks",0.999941,"Semantic communication is viewed as a revolutionary paradigm that can
potentially transform how we design and operate wireless communication systems.
However, despite a recent surge of research activities in this area, the
research landscape remains limited. In this tutorial, we present the first
rigorous vision of a scalable end-to-end semantic communication network that is
founded on novel concepts from artificial intelligence (AI), causal reasoning,
and communication theory. We first discuss how the design of semantic
communication networks requires a move from data-driven networks towards
knowledge-driven ones. Subsequently, we highlight the necessity of creating
semantic representations of data that satisfy the key properties of minimalism,
generalizability, and efficiency so as to do more with less. We then explain
how those representations can form the basis a so-called semantic language. By
using semantic representation and languages, we show that the traditional
transmitter and receiver now become a teacher and apprentice. Then, we define
the concept of reasoning by investigating the fundamentals of causal
representation learning and their role in designing semantic communication
networks. We demonstrate that reasoning faculties are majorly characterized by
the ability to capture causal and associational relationships in datastreams.
For such reasoning-driven networks, we propose novel and essential semantic
communication metrics that include new ""reasoning capacity"" measures that could
go beyond Shannon's bound to capture the convergence of computing and
communication. Finally, we explain how semantic communications can be scaled to
large-scale networks (6G and beyond). In a nutshell, we expect this tutorial to
provide a comprehensive reference on how to properly build, analyze, and deploy
future semantic communication networks.",None,70013
Pointillism: Accurate 3D bounding box estimation with multi-radars,0.933622,"Autonomous perception requires high-quality environment sensing in the form
of 3D bounding boxes of dynamic objects. The primary sensors used in automotive
systems are light-based cameras and LiDARs. However, they are known to fail in
adverse weather conditions. Radars can potentially solve this problem as they
are barely affected by adverse weather conditions. However, specular
reflections of wireless signals cause poor performance of radar point clouds.
We introduce Pointillism, a system that combines data from multiple spatially
separated radars with an optimal separation to mitigate these problems. We
introduce a novel concept of Cross Potential Point Clouds, which uses the
spatial diversity induced by multiple radars and solves the problem of noise
and sparsity in radar point clouds. Furthermore, we present the design of
RP-net, a novel deep learning architecture, designed explicitly for radar's
sparse data distribution, to enable accurate 3D bounding box estimation. The
spatial techniques designed and proposed in this paper are fundamental to
radars point cloud distribution and would benefit other radar sensing
applications.",None,-1
On the Use of BERT for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation,0.990629,"In recent years, pre-trained models have become dominant in most natural
language processing (NLP) tasks. However, in the area of Automated Essay
Scoring (AES), pre-trained models such as BERT have not been properly used to
outperform other deep learning models such as LSTM. In this paper, we introduce
a novel multi-scale essay representation for BERT that can be jointly learned.
We also employ multiple losses and transfer learning from out-of-domain essays
to further improve the performance. Experiment results show that our approach
derives much benefit from joint learning of multi-scale essay representation
and obtains almost the state-of-the-art result among all deep learning models
in the ASAP task. Our multi-scale essay representation also generalizes well to
CommonLit Readability Prize data set, which suggests that the novel text
representation proposed in this paper may be a new and effective choice for
long-text tasks.",https://github.com/lingochamp/Multi-Scale-BERT-AES,-1
DialMed: A Dataset for Dialogue-based Medication Recommendation,0.389351,"Medication recommendation is a crucial task for intelligent healthcare
systems. Previous studies mainly recommend medications with electronic health
records (EHRs). However, some details of interactions between doctors and
patients may be ignored or omitted in EHRs, which are essential for automatic
medication recommendation. Therefore, we make the first attempt to recommend
medications with the conversations between doctors and patients. In this work,
we construct DIALMED, the first high-quality dataset for medical dialogue-based
medication recommendation task. It contains 11,996 medical dialogues related to
16 common diseases from 3 departments and 70 corresponding common medications.
Furthermore, we propose a Dialogue structure and Disease knowledge aware
Network (DDN), where a QA Dialogue Graph mechanism is designed to model the
dialogue structure and the knowledge graph is used to introduce external
disease knowledge. The extensive experimental results demonstrate that the
proposed method is a promising solution to recommend medications with medical
dialogues. The dataset and code are available at
https://github.com/f-window/DialMed.",https://github.com/f-window/DialMed,-1
Multi-Agent Deep Reinforcement Learning for Cost- and Delay-Sensitive Virtual Network Function Placement and Routing,0.6559,"This paper proposes an effective and novel multiagent deep reinforcement
learning (MADRL)-based method for solving the joint virtual network function
(VNF) placement and routing (P&R), where multiple service requests with
differentiated demands are delivered at the same time. The differentiated
demands of the service requests are reflected by their delay- and
cost-sensitive factors. We first construct a VNF P&R problem to jointly
minimize a weighted sum of service delay and resource consumption cost, which
is NP-complete. Then, the joint VNF P&R problem is decoupled into two iterative
subtasks: placement subtask and routing subtask. Each subtask consists of
multiple concurrent parallel sequential decision processes. By invoking the
deep deterministic policy gradient method and multi-agent technique, an
MADRL-P&R framework is designed to perform the two subtasks. The new joint
reward and internal rewards mechanism is proposed to match the goals and
constraints of the placement and routing subtasks. We also propose the
parameter migration-based model-retraining method to deal with changing network
topologies. Corroborated by experiments, the proposed MADRL-P&R framework is
superior to its alternatives in terms of service cost and delay, and offers
higher flexibility for personalized service demands. The parameter
migration-based model-retraining method can efficiently accelerate convergence
under moderate network topology changes.",None,-1
Anomaly localization for copy detection patterns through print estimations,0.223598,"Copy detection patterns (CDP) are recent technologies for protecting products
from counterfeiting. However, in contrast to traditional copy fakes, deep
learning-based fakes have shown to be hardly distinguishable from originals by
traditional authentication systems. Systems based on classical supervised
learning and digital templates assume knowledge of fake CDP at training time
and cannot generalize to unseen types of fakes. Authentication based on printed
copies of originals is an alternative that yields better results even for
unseen fakes and simple authentication metrics but comes at the impractical
cost of acquisition and storage of printed copies. In this work, to overcome
these shortcomings, we design a machine learning (ML) based authentication
system that only requires digital templates and printed original CDP for
training, whereas authentication is based solely on digital templates, which
are used to estimate original printed codes. The obtained results show that the
proposed system can efficiently authenticate original and detect fake CDP by
accurately locating the anomalies in the fake CDP. The empirical evaluation of
the authentication system under investigation is performed on the original and
ML-based fakes CDP printed on two industrial printers.",https://gitlab.unige.ch/Brian.Pulfer/,-1
The Open corpus of the Veps and Karelian languages: overview and applications,0.148962,"A growing priority in the study of Baltic-Finnic languages of the Republic of
Karelia has been the methods and tools of corpus linguistics. Since 2016,
linguists, mathematicians, and programmers at the Karelian Research Centre have
been working with the Open Corpus of the Veps and Karelian Languages (VepKar),
which is an extension of the Veps Corpus created in 2009. The VepKar corpus
comprises texts in Karelian and Veps, multifunctional dictionaries linked to
them, and software with an advanced system of search using various criteria of
the texts (language, genre, etc.) and numerous linguistic categories (lexical
and grammatical search in texts was implemented thanks to the generator of word
forms that we created earlier). A corpus of 3000 texts was compiled, texts were
uploaded and marked up, the system for classifying texts into languages,
dialects, types and genres was introduced, and the word-form generator was
created. Future plans include developing a speech module for working with audio
recordings and a syntactic tagging module using morphological analysis outputs.
Owing to continuous functional advancements in the corpus manager and ongoing
VepKar enrichment with new material and text markup, users can handle a wide
range of scientific and applied tasks. In creating the universal national
VepKar corpus, its developers and managers strive to preserve and exhibit as
fully as possible the state of the Veps and Karelian languages in the 19th-21st
centuries.",https://github.com/componavt/sanahelmi,-1
Hyperbolic Image Segmentation,0.748847,"For image segmentation, the current standard is to perform pixel-level
optimization and inference in Euclidean output embedding spaces through linear
hyperplanes. In this work, we show that hyperbolic manifolds provide a valuable
alternative for image segmentation and propose a tractable formulation of
hierarchical pixel-level classification in hyperbolic space. Hyperbolic Image
Segmentation opens up new possibilities and practical benefits for
segmentation, such as uncertainty estimation and boundary information for free,
zero-label generalization, and increased performance in low-dimensional output
embeddings.",https://github.com/MinaGhadimiAtigh/HyperbolicImageSegmentation,-1
Adaptive Weighted Guided Image Filtering for Depth Enhancement in Shape-From-Focus,0.306067,"Existing shape from focus (SFF) techniques cannot preserve depth edges and
fine structural details from a sequence of multi-focus images. Moreover, noise
in the sequence of multi-focus images affects the accuracy of the depth map. In
this paper, a novel depth enhancement algorithm for the SFF based on an
adaptive weighted guided image filtering (AWGIF) is proposed to address the
above issues. The AWGIF is applied to decompose an initial depth map which is
estimated by the traditional SFF into a base layer and a detail layer. In order
to preserve the edges accurately in the refined depth map, the guidance image
is constructed from the multi-focus image sequence, and the coefficient of the
AWGIF is utilized to suppress the noise while enhancing the fine depth details.
Experiments on real and synthetic objects demonstrate the superiority of the
proposed algorithm in terms of anti-noise, and the ability to preserve depth
edges and fine structural details compared to existing methods.",None,-1
GIAOTracker: A comprehensive framework for MCMOT with global information and optimizing strategies in VisDrone 2021,0.86646,"In recent years, algorithms for multiple object tracking tasks have benefited
from great progresses in deep models and video quality. However, in challenging
scenarios like drone videos, they still suffer from problems, such as small
objects, camera movements and view changes. In this paper, we propose a new
multiple object tracker, which employs Global Information And some Optimizing
strategies, named GIAOTracker. It consists of three stages, i.e., online
tracking, global link and post-processing. Given detections in every frame, the
first stage generates reliable tracklets using information of camera motion,
object motion and object appearance. Then they are associated into trajectories
by exploiting global clues and refined through four post-processing methods.
With the effectiveness of the three stages, GIAOTracker achieves
state-of-the-art performance on the VisDrone MOT dataset and wins the 3rd place
in the VisDrone2021 MOT Challenge.",https://github.com/ultralytics/yolov5,-1
Wavelet Feature Maps Compression for Image-to-Image CNNs,0.0652204,"Convolutional Neural Networks (CNNs) are known for requiring extensive
computational resources, and quantization is among the best and most common
methods for compressing them. While aggressive quantization (i.e., less than
4-bits) performs well for classification, it may cause severe performance
degradation in image-to-image tasks such as semantic segmentation and depth
estimation. In this paper, we propose Wavelet Compressed Convolution (WCC) -- a
novel approach for high-resolution activation maps compression integrated with
point-wise convolutions, which are the main computational cost of modern
architectures. To this end, we use an efficient and hardware-friendly
Haar-wavelet transform, known for its effectiveness in image compression, and
define the convolution on the compressed activation map. We experiment with
various tasks that benefit from high-resolution input. By combining WCC with
light quantization, we achieve compression rates equivalent to 1-4bit
activation quantization with relatively small and much more graceful
degradation in performance. Our code is available at
https://github.com/BGUCompSci/WaveletCompressedConvolution.",https://github.com/BGUCompSci/WaveletCompressedConvolution,-1
"Surround-view Fisheye BEV-Perception for Valet Parking: Dataset, Baseline and Distortion-insensitive Multi-task Framework",0.354069,"Surround-view fisheye perception under valet parking scenes is fundamental
and crucial in autonomous driving. Environmental conditions in parking lots
perform differently from the common public datasets, such as imperfect light
and opacity, which substantially impacts on perception performance. Most
existing networks based on public datasets may generalize suboptimal results on
these valet parking scenes, also affected by the fisheye distortion. In this
article, we introduce a new large-scale fisheye dataset called Fisheye Parking
Dataset(FPD) to promote the research in dealing with diverse real-world
surround-view parking cases. Notably, our compiled FPD exhibits excellent
characteristics for different surround-view perception tasks. In addition, we
also propose our real-time distortion-insensitive multi-task framework Fisheye
Perception Network (FPNet), which improves the surround-view fisheye BEV
perception by enhancing the fisheye distortion operation and multi-task
lightweight designs. Extensive experiments validate the effectiveness of our
approach and the dataset's exceptional generalizability.",None,-1
Learning to Approximate: Auto Direction Vector Set Generation for Hypervolume Contribution Approximation,0.0531673,"Hypervolume contribution is an important concept in evolutionary
multi-objective optimization (EMO). It involves in hypervolume-based EMO
algorithms and hypervolume subset selection algorithms. Its main drawback is
that it is computationally expensive in high-dimensional spaces, which limits
its applicability to many-objective optimization. Recently, an R2 indicator
variant (i.e., $R_2^{\text{HVC}}$ indicator) is proposed to approximate the
hypervolume contribution. The $R_2^{\text{HVC}}$ indicator uses line segments
along a number of direction vectors for hypervolume contribution approximation.
It has been shown that different direction vector sets lead to different
approximation quality. In this paper, we propose \textit{Learning to
Approximate (LtA)}, a direction vector set generation method for the
$R_2^{\text{HVC}}$ indicator. The direction vector set is automatically learned
from training data. The learned direction vector set can then be used in the
$R_2^{\text{HVC}}$ indicator to improve its approximation quality. The
usefulness of the proposed LtA method is examined by comparing it with other
commonly-used direction vector set generation methods for the
$R_2^{\text{HVC}}$ indicator. Experimental results suggest the superiority of
LtA over the other methods for generating high quality direction vector sets.",https://github.com/HisaoLabSUSTC/GAHSS,-1
StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts,0.65591,"Inferring spatial relations in natural language is a crucial ability an
intelligent system should possess. The bAbI dataset tries to capture tasks
relevant to this domain (task 17 and 19). However, these tasks have several
limitations. Most importantly, they are limited to fixed expressions, they are
limited in the number of reasoning steps required to solve them, and they fail
to test the robustness of models to input that contains irrelevant or redundant
information. In this paper, we present a new Question-Answering dataset called
StepGame for robust multi-hop spatial reasoning in texts. Our experiments
demonstrate that state-of-the-art models on the bAbI dataset struggle on the
StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented
Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental
results on both datasets show that our model outperforms all the baselines with
superior generalization and robustness performance.",None,-1
Probing Pre-Trained Language Models for Cross-Cultural Differences in Values,0.997702,"Language embeds information about social, cultural, and political values
people hold. Prior work has explored social and potentially harmful biases
encoded in Pre-Trained Language models (PTLMs). However, there has been no
systematic study investigating how values embedded in these models vary across
cultures. In this paper, we introduce probes to study which values across
cultures are embedded in these models, and whether they align with existing
theories and cross-cultural value surveys. We find that PTLMs capture
differences in values across cultures, but those only weakly align with
established value surveys. We discuss implications of using mis-aligned models
in cross-cultural settings, as well as ways of aligning PTLMs with value
surveys.",None,-1
Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds,0.945978,"Discovering latent topics from text corpora has been studied for decades.
Many existing topic models adopt a fully unsupervised setting, and their
discovered topics may not cater to users' particular interests due to their
inability of leveraging user guidance. Although there exist seed-guided topic
discovery approaches that leverage user-provided seeds to discover
topic-representative terms, they are less concerned with two factors: (1) the
existence of out-of-vocabulary seeds and (2) the power of pre-trained language
models (PLMs). In this paper, we generalize the task of seed-guided topic
discovery to allow out-of-vocabulary seeds. We propose a novel framework, named
SeeTopic, wherein the general knowledge of PLMs and the local semantics learned
from the input corpus can mutually benefit each other. Experiments on three
real datasets from different domains demonstrate the effectiveness of SeeTopic
in terms of topic coherence, accuracy, and diversity.",None,-1
Revealing interactions between HVDC cross-area flows and frequency stability with explainable AI,0.607329,"The energy transition introduces more volatile energy sources into the power
grids. In this context, power transfer between different synchronous areas
through High Voltage Direct Current (HVDC) links becomes increasingly
important. Such links can balance volatile generation by enabling long-distance
transport or by leveraging their fast control behavior. Here, we investigate
the interaction of power imbalances - represented through the power grid
frequency - and power flows on HVDC links between synchronous areas in Europe.
We use explainable machine learning to identify key dependencies and
disentangle the interaction of critical features. Our results show that
market-based HVDC flows introduce deterministic frequency deviations, which
however can be mitigated through strict ramping limits. Moreover, varying HVDC
operation modes strongly affect the interaction with the grid. In particular,
we show that load-frequency control via HVDC links can both have control-like
or disturbance-like impacts on frequency stability.",None,-1
Privacy-Preserving Model Upgrades with Bidirectional Compatible Training in Image Retrieval,0.447372,"The task of privacy-preserving model upgrades in image retrieval desires to
reap the benefits of rapidly evolving new models without accessing the raw
gallery images. A pioneering work introduced backward-compatible training,
where the new model can be directly deployed in a backfill-free manner, i.e.,
the new query can be directly compared to the old gallery features. Despite a
possible solution, its improvement in sequential model upgrades is gradually
limited by the fixed and under-quality old gallery embeddings. To this end, we
propose a new model upgrade paradigm, termed Bidirectional Compatible Training
(BiCT), which will upgrade the old gallery embeddings by forward-compatible
training towards the embedding space of the backward-compatible new model. We
conduct comprehensive experiments to verify the prominent improvement by BiCT
and interestingly observe that the inconspicuous loss weight of backward
compatibility actually plays an essential role for both backward and forward
retrieval performance. To summarize, we introduce a new and valuable problem
named privacy-preserving model upgrades, with a proper solution BiCT. Several
intriguing insights are further proposed to get the most out of our method.",None,-1
Transformers are Adaptable Task Planners,0.628072,"Every home is different, and every person likes things done in their
particular way. Therefore, home robots of the future need to both reason about
the sequential nature of day-to-day tasks and generalize to user's preferences.
To this end, we propose a Transformer Task Planner(TTP) that learns high-level
actions from demonstrations by leveraging object attribute-based
representations. TTP can be pre-trained on multiple preferences and shows
generalization to unseen preferences using a single demonstration as a prompt
in a simulated dishwasher loading task. Further, we demonstrate real-world dish
rearrangement using TTP with a Franka Panda robotic arm, prompted using a
single human demonstration.",None,-1
Self-Configuring nnU-Nets Detect Clouds in Satellite Images,0.219173,"Cloud detection is a pivotal satellite image pre-processing step that can be
performed both on the ground and on board a satellite to tag useful images. In
the latter case, it can help to reduce the amount of data to downlink by
pruning the cloudy areas, or to make a satellite more autonomous through
data-driven acquisition re-scheduling of the cloudy areas. We approach this
important task with nnU-Nets, a self-reconfigurable framework able to perform
meta-learning of a segmentation network over various datasets. Our experiments,
performed over Sentinel-2 and Landsat-8 multispectral images revealed that
nnU-Nets deliver state-of-the-art cloud segmentation performance without any
manual design. Our approach was ranked within the top 7% best solutions (across
847 participating teams) in the On Cloud N: Cloud Cover Detection Challenge,
where we reached the Jaccard index of 0.882 over more than 10k unseen
Sentinel-2 image patches (the winners obtained 0.897, whereas the baseline
U-Net with the ResNet-34 backbone used as an encoder: 0.817, and the classic
Sentinel-2 image thresholding: 0.652).",https://gitlab.com/jnalepa/nnUNets_for_clouds,-1
Perturbation Augmentation for Fairer NLP,0.892518,"Unwanted and often harmful social biases are becoming ever more salient in
NLP research, affecting both models and datasets. In this work, we ask whether
training on demographically perturbed data leads to fairer language models. We
collect a large dataset of human annotated text perturbations and train a
neural perturbation model, which we show outperforms heuristic alternatives. We
find that (i) language models (LMs) pre-trained on demographically perturbed
corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE
datasets exhibit less demographic bias on downstream tasks, and (iii) fairness
improvements do not come at the expense of performance on downstream tasks.
Lastly, we discuss outstanding questions about how best to evaluate the
(un)fairness of large language models. We hope that this exploration of neural
demographic perturbation will help drive more improvement towards fairer NLP.",https://github.com/facebookresearch/ParlAI,-1
Linear Array Network for Low-light Image Enhancement,0.163402,"Convolution neural networks (CNNs) based methods have dominated the low-light
image enhancement tasks due to their outstanding performance. However, the
convolution operation is based on a local sliding window mechanism, which is
difficult to construct the long-range dependencies of the feature maps.
Meanwhile, the self-attention based global relationship aggregation methods
have been widely used in computer vision, but these methods are difficult to
handle high-resolution images because of the high computational complexity. To
solve this problem, this paper proposes a Linear Array Self-attention (LASA)
mechanism, which uses only two 2-D feature encodings to construct 3-D global
weights and then refines feature maps generated by convolution layers. Based on
LASA, Linear Array Network (LAN) is proposed, which is superior to the existing
state-of-the-art (SOTA) methods in both RGB and RAW based low-light enhancement
tasks with a smaller amount of parameters. The code is released in
https://github.com/cuiziteng/LASA_enhancement.",https://github.com/cuiziteng/LASA,-1
"Self-supervision and Learnable STRFs for Age, Emotion, and Country Prediction",0.0718638,"This work presents a multitask approach to the simultaneous estimation of
age, country of origin, and emotion given vocal burst audio for the 2022 ICML
Expressive Vocalizations Challenge ExVo-MultiTask track. The method of choice
utilized a combination of spectro-temporal modulation and self-supervised
features, followed by an encoder-decoder network organized in a multitask
paradigm. We evaluate the complementarity between the tasks posed by examining
independent task-specific and joint models, and explore the relative strengths
of different feature sets. We also introduce a simple score fusion mechanism to
leverage the complementarity of different feature sets for this task.
  We find that robust data preprocessing in conjunction with score fusion over
spectro-temporal receptive field and HuBERT models achieved our best
ExVo-MultiTask test score of 0.412.",https://github.com/snakers4/silero-vad,-1
Modelling Business Agreements in the Multimodal Transportation Domain through Ontological Smart Contracts,0.112715,"The blockchain technology provides integrity and reliability of the
information, thus offering a suitable solution to guarantee trustability in a
multi-stakeholder scenario that involves actors defining business agreements.
The Ride2Rail project investigated the use of the blockchain to record as smart
contracts the agreements between different stakeholders defined in a multimodal
transportation domain. Modelling an ontology to represent the smart contracts
enables the possibility of having a machine-readable and interoperable
representation of the agreements. On one hand, the underlying blockchain
ensures trust in the execution of the contracts, on the other hand, their
ontological representation facilitates the retrieval of information within the
ecosystem. The paper describes the development of the Ride2Rail Ontology for
Agreements to showcase how the concept of an ontological smart contract,
defined in the OASIS ontology, can be applied to a specific domain. The usage
of the designed ontology is discussed by describing the modelling as
ontological smart contracts of business agreements defined in a ride-sharing
scenario.",https://github.com/Ride2Rail/agreement-ledger-ontology,-1
Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization,0.775762,"Neural architecture search (NAS) and network pruning are widely studied
efficient AI techniques, but not yet perfect. NAS performs exhaustive candidate
architecture search, incurring tremendous search cost. Though (structured)
pruning can simply shrink model dimension, it remains unclear how to decide the
per-layer sparsity automatically and optimally. In this work, we revisit the
problem of layer-width optimization and propose Pruning-as-Search (PaS), an
end-to-end channel pruning method to search out desired sub-network
automatically and efficiently. Specifically, we add a depth-wise binary
convolution to learn pruning policies directly through gradient descent. By
combining the structural reparameterization and PaS, we successfully searched
out a new family of VGG-like and lightweight networks, which enable the
flexibility of arbitrary width with respect to each layer instead of each
stage. Experimental results show that our proposed architecture outperforms
prior arts by around $1.0\%$ top-1 accuracy under similar inference speed on
ImageNet-1000 classification task. Furthermore, we demonstrate the
effectiveness of our width search on complex tasks including instance
segmentation and image translation. Code and models are released.",None,-1
PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales,0.64001,"Neural language models (LMs) have achieved impressive results on various
language-based reasoning tasks by utilizing latent knowledge encoded in their
own pretrained parameters. To make this reasoning process more explicit, recent
works retrieve a rationalizing LM's internal knowledge by training or prompting
it to generate free-text rationales, which can be used to guide task
predictions made by either the same LM or a separate reasoning LM. However,
rationalizing LMs require expensive rationale annotation and/or computation,
without any assurance that their generated rationales improve LM task
performance or faithfully reflect LM decision-making. In this paper, we propose
PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns
to faithfully reason over rationales via counterfactual regularization. First,
PINTO maps out a suitable reasoning process for the task input by prompting a
frozen rationalizing LM to generate a free-text rationale. Second, PINTO's
reasoning LM is fine-tuned to solve the task using the generated rationale as
context, while regularized to output less confident predictions when the
rationale is perturbed. Across four datasets, we show that PINTO significantly
improves the generalization ability of the reasoning LM, yielding higher
performance on both in-distribution and out-of-distribution test sets. Also, we
find that PINTO's rationales are more faithful to its task predictions than
those generated by competitive baselines.",https://github.com/wangpf3/pinto-faithful-language-reasoning,-1
Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation,0.99605,"In recent years, machine learning models have rapidly become better at
generating clinical consultation notes; yet, there is little work on how to
properly evaluate the generated consultation notes to understand the impact
they may have on both the clinician using them and the patient's clinical
safety. To address this we present an extensive human evaluation study of
consultation notes where 5 clinicians (i) listen to 57 mock consultations, (ii)
write their own notes, (iii) post-edit a number of automatically generated
notes, and (iv) extract all the errors, both quantitative and qualitative. We
then carry out a correlation study with 18 automatic quality metrics and the
human judgements. We find that a simple, character-based Levenshtein distance
metric performs on par if not better than common model-based metrics like
BertScore. All our findings and annotations are open-sourced.",https://github.com/babylonhealth/primock57,-1
Approximating Constraint Manifolds Using Generative Models for Sampling-Based Constrained Motion Planning,0.114776,"Sampling-based motion planning under task constraints is challenging because
the null-measure constraint manifold in the configuration space makes rejection
sampling extremely inefficient, if not impossible. This paper presents a
learning-based sampling strategy for constrained motion planning problems. We
investigate the use of two well-known deep generative models, the Conditional
Variational Autoencoder (CVAE) and the Conditional Generative Adversarial Net
(CGAN), to generate constraint-satisfying sample configurations. Instead of
precomputed graphs, we use generative models conditioned on constraint
parameters for approximating the constraint manifold. This approach allows for
the efficient drawing of constraint-satisfying samples online without any need
for modification of available sampling-based motion planning algorithms. We
evaluate the efficiency of these two generative models in terms of their
sampling accuracy and coverage of sampling distribution. Simulations and
experiments are also conducted for different constraint tasks on two robotic
platforms.",None,9510
Prompt-driven efficient Open-set Semi-supervised Learning,0.303082,"Open-set semi-supervised learning (OSSL) has attracted growing interest,
which investigates a more practical scenario where out-of-distribution (OOD)
samples are only contained in unlabeled data. Existing OSSL methods like
OpenMatch learn an OOD detector to identify outliers, which often update all
modal parameters (i.e., full fine-tuning) to propagate class information from
labeled data to unlabeled ones. Currently, prompt learning has been developed
to bridge gaps between pre-training and fine-tuning, which shows higher
computational efficiency in several downstream tasks. In this paper, we propose
a prompt-driven efficient OSSL framework, called OpenPrompt, which can
propagate class information from labeled to unlabeled data with only a small
number of trainable parameters. We propose a prompt-driven joint space learning
mechanism to detect OOD data by maximizing the distribution gap between ID and
OOD samples in unlabeled data, thereby our method enables the outliers to be
detected in a new way. The experimental results on three public datasets show
that OpenPrompt outperforms state-of-the-art methods with less than 1% of
trainable parameters. More importantly, OpenPrompt achieves a 4% improvement in
terms of AUROC on outlier detection over a fully supervised model on CIFAR10.",None,-1
Semantic optical fiber communication system,0.572202,"The current optical communication systems minimize bit or symbol errors
without considering the semantic meaning behind digital bits, thus transmitting
a lot of unnecessary information. We propose and experimentally demonstrate a
semantic optical fiber communication (SOFC) system. Instead of encoding
information into bits for transmission, semantic information is extracted from
the source using deep learning. The generated semantic symbols are then
directly transmitted through an optical fiber. Compared with the bit-based
structure, the SOFC system achieved higher information compression and a more
stable performance, especially in the low received optical power regime, and
enhanced the robustness against optical link impairments. This work introduces
an intelligent optical communication system at the human analytical thinking
level, which is a significant step toward a breakthrough in the current optical
communication architecture.",None,-1
KGRGRL: A User's Permission Reasoning Method Based on Knowledge Graph Reward Guidance Reinforcement Learning,0.130642,"In general, multiple domain cyberspace security assessments can be
implemented by reasoning user's permissions. However, while existing methods
include some information from the physical and social domains, they do not
provide a comprehensive representation of cyberspace. Existing reasoning
methods are also based on expert-given rules, resulting in inefficiency and a
low degree of intelligence. To address this challenge, we create a Knowledge
Graph (KG) of multiple domain cyberspace in order to provide a standard
semantic description of the multiple domain cyberspace. Following that, we
proposed a user's permissions reasoning method based on reinforcement learning.
All permissions in cyberspace are represented as nodes, and an agent is trained
to find all permissions that user can have according to user's initial
permissions and cyberspace KG. We set 10 reward setting rules based on the
features of cyberspace KG in the reinforcement learning of reward information
setting, so that the agent can better locate user's all permissions and avoid
blindly finding user's permissions. The results of the experiments showed that
the proposed method can successfully reason about user's permissions and
increase the intelligence level of the user's permissions reasoning method. At
the same time, the F1 value of the proposed method is 6% greater than that of
the Translating Embedding (TransE) method.",None,-1
Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media,0.454069,"Social media is a breeding ground for threat narratives and related
conspiracy theories. In these, an outside group threatens the integrity of an
inside group, leading to the emergence of sharply defined group identities:
Insiders -- agents with whom the authors identify and Outsiders -- agents who
threaten the insiders. Inferring the members of these groups constitutes a
challenging new NLP task: (i) Information is distributed over many
poorly-constructed posts; (ii) Threats and threat agents are highly contextual,
with the same post potentially having multiple agents assigned to membership in
either group; (iii) An agent's identity is often implicit and transitive; and
(iv) Phrases used to imply Outsider status often do not follow common negative
sentiment patterns. To address these challenges, we define a novel
Insider-Outsider classification task. Because we are not aware of any
appropriate existing datasets or attendant models, we introduce a labeled
dataset (CT5K) and design a model (NP2IO) to address this task. NP2IO leverages
pretrained language modeling to classify Insiders and Outsiders. NP2IO is shown
to be robust, generalizing to noun phrases not seen during training, and
exceeding the performance of non-trivial baseline models by $20\%$.",https://github.com/heartexlabs/label-studio,-1
Graph-in-Graph Network for Automatic Gene Ontology Description Generation,0.460365,"Gene Ontology (GO) is the primary gene function knowledge base that enables
computational tasks in biomedicine. The basic element of GO is a term, which
includes a set of genes with the same function. Existing research efforts of GO
mainly focus on predicting gene term associations. Other tasks, such as
generating descriptions of new terms, are rarely pursued. In this paper, we
propose a novel task: GO term description generation. This task aims to
automatically generate a sentence that describes the function of a GO term
belonging to one of the three categories, i.e., molecular function, biological
process, and cellular component. To address this task, we propose a
Graph-in-Graph network that can efficiently leverage the structural information
of GO. The proposed network introduces a two-layer graph: the first layer is a
graph of GO terms where each node is also a graph (gene graph). Such a
Graph-in-Graph network can derive the biological functions of GO terms and
generate proper descriptions. To validate the effectiveness of the proposed
network, we build three large-scale benchmark datasets. By incorporating the
proposed Graph-in-Graph network, the performances of seven different
sequence-to-sequence models can be substantially boosted across all evaluation
metrics, with up to 34.7%, 14.5%, and 39.1% relative improvements in BLEU,
ROUGE-L, and METEOR, respectively.",None,-1
Smart Multi-tenant Federated Learning,0.242747,"Federated learning (FL) is an emerging distributed machine learning method
that empowers in-situ model training on decentralized edge devices. However,
multiple simultaneous training activities could overload resource-constrained
devices. In this work, we propose a smart multi-tenant FL system, MuFL, to
effectively coordinate and execute simultaneous training activities. We first
formalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and
introduce a vanilla multi-tenant FL system that trains activities sequentially
to form baselines. Then, we propose two approaches to optimize multi-tenant FL:
1) activity consolidation merges training activities into one activity with a
multi-task architecture; 2) after training it for rounds, activity splitting
divides it into groups by employing affinities among activities such that
activities within a group have better synergy. Extensive experiments
demonstrate that MuFL outperforms other methods while consuming 40% less
energy. We hope this work will inspire the community to further study and
optimize multi-tenant FL.",https://github.com/tstandley/taskgrouping,20521
The Sound of Bounding-Boxes,0.0958485,"In the task of audio-visual sound source separation, which leverages visual
information for sound source separation, identifying objects in an image is a
crucial step prior to separating the sound source. However, existing methods
that assign sound on detected bounding boxes suffer from a problem that their
approach heavily relies on pre-trained object detectors. Specifically, when
using these existing methods, it is required to predetermine all the possible
categories of objects that can produce sound and use an object detector
applicable to all such categories. To tackle this problem, we propose a fully
unsupervised method that learns to detect objects in an image and separate
sound source simultaneously. As our method does not rely on any pre-trained
detector, our method is applicable to arbitrary categories without any
additional annotation. Furthermore, although being fully unsupervised, we found
that our method performs comparably in separation accuracy.",https://github.com/openimages,-1
DOM-LM: Learning Generalizable Representations for HTML Documents,0.947392,"HTML documents are an important medium for disseminating information on the
Web for human consumption. An HTML document presents information in multiple
text formats including unstructured text, structured key-value pairs, and
tables. Effective representation of these documents is essential for machine
understanding to enable a wide range of applications, such as Question
Answering, Web Search, and Personalization. Existing work has either
represented these documents using visual features extracted by rendering them
in a browser, which is typically computationally expensive, or has simply
treated them as plain text documents, thereby failing to capture useful
information presented in their HTML structure. We argue that the text and HTML
structure together convey important semantics of the content and therefore
warrant a special treatment for their representation learning. In this paper,
we introduce a novel representation learning approach for web pages, dubbed
DOM-LM, which addresses the limitations of existing approaches by encoding both
text and DOM tree structure with a transformer-based encoder and learning
generalizable representations for HTML documents via self-supervised
pre-training. We evaluate DOM-LM on a variety of webpage understanding tasks,
including Attribute Extraction, Open Information Extraction, and Question
Answering. Our extensive experiments show that DOM-LM consistently outperforms
all baselines designed for these tasks. In particular, DOM-LM demonstrates
better generalization performance both in few-shot and zero-shot settings,
making it attractive for making it suitable for real-world application settings
with limited labeled data.",None,-1
Universal Conditional Masked Language Pre-training for Neural Machine Translation,0.46641,"Pre-trained sequence-to-sequence models have significantly improved Neural
Machine Translation (NMT). Different from prior works where pre-trained models
usually adopt an unidirectional decoder, this paper demonstrates that
pre-training a sequence-to-sequence model but with a bidirectional decoder can
produce notable performance gains for both Autoregressive and
Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked
language model pre-trained on large-scale bilingual and monolingual corpora in
many languages. We also introduce two simple but effective methods to enhance
the CeMAT, aligned code-switching & masking and dynamic dual-masking. We
conduct extensive experiments and show that our CeMAT can achieve significant
performance improvement for all scenarios from low- to extremely high-resource
languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on
average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it
can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the
best of our knowledge, this is the first work to pre-train a unified model for
fine-tuning on both NMT tasks. Code, data, and pre-trained models are available
at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT.",https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT,19622
Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning,0.798809,"Novel category discovery aims at adapting models trained on known categories
to novel categories. Previous works only focus on the scenario where known and
novel categories are of the same granularity. In this paper, we investigate a
new practical scenario called Fine-grained Category Discovery under
Coarse-grained supervision (FCDC). FCDC aims at discovering fine-grained
categories with only coarse-grained labeled data, which can adapt models to
categories of different granularity from known ones and reduce significant
labeling cost. It is also a challenging task since supervised training on
coarse-grained categories tends to focus on inter-class distance (distance
between coarse-grained classes) but ignore intra-class distance (distance
between fine-grained sub-classes) which is essential for separating
fine-grained categories. Considering most current methods cannot transfer
knowledge from coarse-grained level to fine-grained level, we propose a
hierarchical weighted self-contrastive network by building a novel weighted
self-contrastive module and combining it with supervised learning in a
hierarchical manner. Extensive experiments on public datasets show both
effectiveness and efficiency of our model over compared methods. Code and data
are available at https://github.com/Lackel/Hierarchical_Weighted_SCL.",https://github.com/Lackel/Hierarchical_Weighted_SCL,-1
"Towards Rich, Portable, and Large-Scale Pedestrian Data Collection",0.0723942,"Recently, pedestrian behavior research has shifted towards machine learning
based methods and converged on the topic of modeling pedestrian interactions.
For this, a large-scale dataset that contains rich information is needed. We
propose a data collection system that is portable, which facilitates accessible
large-scale data collection in diverse environments. We also couple the system
with a semi-autonomous labeling pipeline for fast trajectory label production.
We further introduce the first batch of dataset from the ongoing data
collection effort -- the TBD pedestrian dataset. Compared with existing
pedestrian datasets, our dataset contains three components: human verified
labels grounded in the metric space, a combination of top-down and perspective
views, and naturalistic human behavior in the presence of a socially
appropriate ""robot"".",https://tbd.ri.cmu.edu/tbd-social-navigation-datasets,-1
High-Res Facial Appearance Capture from Polarized Smartphone Images,0.74146,"We propose a novel method for high-quality facial texture reconstruction from
RGB images using a novel capturing routine based on a single smartphone which
we equip with an inexpensive polarization foil. Specifically, we turn the
flashlight into a polarized light source and add a polarization filter on top
of the camera. Leveraging this setup, we capture the face of a subject with
cross-polarized and parallel-polarized light. For each subject, we record two
short sequences in a dark environment under flash illumination with different
light polarization using the modified smartphone. Based on these observations,
we reconstruct an explicit surface mesh of the face using structure from
motion. We then exploit the camera and light co-location within a
differentiable renderer to optimize the facial textures using an
analysis-by-synthesis approach. Our method optimizes for high-resolution normal
textures, diffuse albedo, and specular albedo using a coarse-to-fine
optimization scheme. We show that the optimized textures can be used in a
standard rendering pipeline to synthesize high-quality photo-realistic 3D
digital humans in novel environments.",None,-1
Bridging the Gap between Reality and Ideality of Entity Matching: A Revisiting and Benchmark Re-Construction,0.269737,"Entity matching (EM) is the most critical step for entity resolution (ER).
While current deep learningbased methods achieve very impressive performance on
standard EM benchmarks, their realworld application performance is much
frustrating. In this paper, we highlight that such the gap between reality and
ideality stems from the unreasonable benchmark construction process, which is
inconsistent with the nature of entity matching and therefore leads to biased
evaluations of current EM approaches. To this end, we build a new EM corpus and
re-construct EM benchmarks to challenge critical assumptions implicit in the
previous benchmark construction process by step-wisely changing the restricted
entities, balanced labels, and single-modal records in previous benchmarks into
open entities, imbalanced labels, and multimodal records in an open
environment. Experimental results demonstrate that the assumptions made in the
previous benchmark construction process are not coincidental with the open
environment, which conceal the main challenges of the task and therefore
significantly overestimate the current progress of entity matching. The
constructed benchmarks and code are publicly released",https://github.com/tshu-w/ember,-1
Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models,0.826226,"Neural network models have achieved state-of-the-art performance on
grapheme-to-phoneme (G2P) conversion. However, their performance relies on
large-scale pronunciation dictionaries, which may not be available for a lot of
languages. Inspired by the success of the pre-trained language model BERT, this
paper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which
is built by self-supervised training on a large, language-specific word list
with only grapheme information. Furthermore, two approaches are developed to
incorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e.,
fine-tuning GBERT or fusing GBERT into the Transformer model by attention.
Experimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean
datasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our
GBERT-based G2P models under both medium-resource and low-resource data
conditions.",https://github.com/ldong1111/GraphemeBERT,-1
Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation,0.144059,"Neural Machine Translation (NMT) has reached a level of maturity to be
recognized as the premier method for the translation between different
languages and aroused interest in different research areas, including software
engineering. A key step to validate the robustness of the NMT models consists
in evaluating the performance of the models on adversarial inputs, i.e., inputs
obtained from the original ones by adding small amounts of perturbation.
However, when dealing with the specific task of the code generation (i.e., the
generation of code starting from a description in natural language), it has not
yet been defined an approach to validate the robustness of the NMT models. In
this work, we address the problem by identifying a set of perturbations and
metrics tailored for the robustness assessment of such models. We present a
preliminary experimental evaluation, showing what type of perturbations affect
the model the most and deriving useful insights for future directions.",None,-1
Towards Open Set Video Anomaly Detection,0.463687,"Open Set Video Anomaly Detection (OpenVAD) aims to identify abnormal events
from video data where both known anomalies and novel ones exist in testing.
Unsupervised models learned solely from normal videos are applicable to any
testing anomalies but suffer from a high false positive rate. In contrast,
weakly supervised methods are effective in detecting known anomalies but could
fail in an open world. We develop a novel weakly supervised method for the
OpenVAD problem by integrating evidential deep learning (EDL) and normalizing
flows (NFs) into a multiple instance learning (MIL) framework. Specifically, we
propose to use graph neural networks and triplet loss to learn discriminative
features for training the EDL classifier, where the EDL is capable of
identifying the unknown anomalies by quantifying the uncertainty. Moreover, we
develop an uncertainty-aware selection strategy to obtain clean anomaly
instances and a NFs module to generate the pseudo anomalies. Our method is
superior to existing approaches by inheriting the advantages of both the
unsupervised NFs and the weakly-supervised MIL framework. Experimental results
on multiple real-world video datasets show the effectiveness of our method.",None,5177
How stable are Transferability Metrics evaluations?,0.709996,"Transferability metrics is a maturing field with increasing interest, which
aims at providing heuristics for selecting the most suitable source models to
transfer to a given target dataset, without fine-tuning them all. However,
existing works rely on custom experimental setups which differ across papers,
leading to inconsistent conclusions about which transferability metrics work
best. In this paper we conduct a large-scale study by systematically
constructing a broad range of 715k experimental setup variations. We discover
that even small variations to an experimental setup lead to different
conclusions about the superiority of a transferability metric over another.
Then we propose better evaluations by aggregating across many experiments,
enabling to reach more stable conclusions. As a result, we reveal the
superiority of LogME at selecting good source datasets to transfer from in a
semantic segmentation scenario, NLEEP at selecting good source architectures in
an image classification scenario, and GBC at determining which target task
benefits most from a given source model. Yet, no single transferability metric
works best in all scenarios.",https://github.com/google-research/google-research/tree/master/stable transfer,-1
ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select,0.64656,"We study the problem of extracting N-ary relation tuples from scientific
articles. This task is challenging because the target knowledge tuples can
reside in multiple parts and modalities of the document. Our proposed method
ReSel decomposes this task into a two-stage procedure that first retrieves the
most relevant paragraph/table and then selects the target entity from the
retrieved component. For the high-level retrieval stage, ReSel designs a simple
and effective feature set, which captures multi-level lexical and semantic
similarities between the query and components. For the low-level selection
stage, ReSel designs a cross-modal entity correlation graph along with a
multi-view architecture, which models both semantic and document-structural
relations between entities. Our experiments on three scientific information
extraction datasets show that ReSel outperforms state-of-the-art baselines
significantly.",https://github.com/night-chen/ReSel,-1
Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning,0.999563,"How to train deep neural networks (DNNs) to generalize well is a central
concern in deep learning, especially for severely overparameterized networks
nowadays. In this paper, we propose an effective method to improve the model
generalization by additionally penalizing the gradient norm of loss function
during optimization. We demonstrate that confining the gradient norm of loss
function could help lead the optimizers towards finding flat minima. We
leverage the first-order approximation to efficiently implement the
corresponding gradient to fit well in the gradient descent framework. In our
experiments, we confirm that when using our methods, generalization performance
of various models could be improved on different datasets. Also, we show that
the recent sharpness-aware minimization method (Foret et al., 2021) is a
special, but not the best, case of our method, where the best case of our
method could give new state-of-art performance on these tasks. Code is
available at {https://github.com/zhaoyang-0204/gnp}.",https://github.com/zhaoyang-0204/gnp,-1
Causal Fairness Analysis,0.794597,"Decision-making systems based on AI and machine learning have been used
throughout a wide range of real-world scenarios, including healthcare, law
enforcement, education, and finance. It is no longer far-fetched to envision a
future where autonomous systems will be driving entire business decisions and,
more broadly, supporting large-scale decision-making infrastructure to solve
society's most challenging problems. Issues of unfairness and discrimination
are pervasive when decisions are being made by humans, and remain (or are
potentially amplified) when decisions are made using machines with little
transparency, accountability, and fairness. In this paper, we introduce a
framework for \textit{causal fairness analysis} with the intent of filling in
this gap, i.e., understanding, modeling, and possibly solving issues of
fairness in decision-making settings. The main insight of our approach will be
to link the quantification of the disparities present on the observed data with
the underlying, and often unobserved, collection of causal mechanisms that
generate the disparity in the first place, challenge we call the Fundamental
Problem of Causal Fairness Analysis (FPCFA). In order to solve the FPCFA, we
study the problem of decomposing variations and empirical measures of fairness
that attribute such variations to structural mechanisms and different units of
the population. Our effort culminates in the Fairness Map, which is the first
systematic attempt to organize and explain the relationship between different
criteria found in the literature. Finally, we study which causal assumptions
are minimally needed for performing causal fairness analysis and propose a
Fairness Cookbook, which allows data scientists to assess the existence of
disparate impact and disparate treatment.",None,-1
Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks,0.712479,"Iterative text revision improves text quality by fixing grammatical errors,
rephrasing for better readability or contextual appropriateness, or
reorganizing sentence structures throughout a document. Most recent research
has focused on understanding and classifying different types of edits in the
iterative revision process from human-written text instead of building accurate
and robust systems for iterative text revision. In this work, we aim to build
an end-to-end text revision system that can iteratively generate helpful edits
by explicitly detecting editable spans (where-to-edit) with their corresponding
edit intents and then instructing a revision model to revise the detected edit
spans. Leveraging datasets from other related text editing NLP tasks, combined
with the specification of editable spans, leads our system to more accurately
model the process of iterative text refinement, as evidenced by empirical
results and human evaluations. Our system significantly outperforms previous
baselines on our text revision tasks and other standard text revision tasks,
including grammatical error correction, text simplification, sentence fusion,
and style transfer. Through extensive qualitative and quantitative analysis, we
make vital connections between edit intentions and writing quality, and better
computational modeling of iterative text revisions.",https://github.com/vipulraheja/iterater,-1
SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations,0.950911,"Accurate mapping of large-scale environments is an essential building block
of most outdoor autonomous systems. Challenges of traditional mapping methods
include the balance between memory consumption and mapping accuracy. This paper
addresses the problem of achieving large-scale 3D reconstruction using implicit
representations built from 3D LiDAR measurements. We learn and store implicit
features through an octree-based, hierarchical structure, which is sparse and
extensible. The implicit features can be turned into signed distance values
through a shallow neural network. We leverage binary cross entropy loss to
optimize the local features with the 3D measurements as supervision. Based on
our implicit representation, we design an incremental mapping system with
regularization to tackle the issue of forgetting in continual learning. Our
experiments show that our 3D reconstructions are more accurate, complete, and
memory-efficient than current state-of-the-art 3D mapping methods.",https://github.com/PRBonn/SHINE_mapping,-1
Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record,0.247186,"Objective: Social determinants of health (SDOH) impact health outcomes and
are documented in the electronic health record (EHR) through structured data
and unstructured clinical notes. However, clinical notes often contain more
comprehensive SDOH information, detailing aspects such as status, severity, and
temporality. This work has two primary objectives: i) develop a natural
language processing (NLP) information extraction model to capture detailed SDOH
information and ii) evaluate the information gain achieved by applying the SDOH
extractor to clinical narratives and combining the extracted representations
with existing structured data.
  Materials and Methods: We developed a novel SDOH extractor using a deep
learning entity and relation extraction architecture to characterize SDOH
across various dimensions. In an EHR case study, we applied the SDOH extractor
to a large clinical data set with 225,089 patients and 430,406 notes with
social history sections and compared the extracted SDOH information with
existing structured data.
  Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the
EHR case study, we found extracted SDOH information complements existing
structured data with 32% of homeless patients, 19% of current tobacco users,
and 10% of drug users only having these health risk factors documented in the
clinical narrative.
  Conclusions: Utilizing EHR data to identify SDOH health risk factors and
social needs may improve patient care and outcomes. Semantic representations of
text-encoded SDOH information can augment existing structured data, and this
more comprehensive SDOH representation can assist health systems in identifying
and addressing these social needs.",https://github.com/uw-bionlp/mspert,8863
Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem,0.440457,"Math word problem solver requires both precise relation reasoning about
quantities in the text and reliable generation for the diverse equation.
Current sequence-to-tree or relation extraction methods regard this only from a
fixed view, struggling to simultaneously handle complex semantics and diverse
equations. However, human solving naturally involves two consistent reasoning
views: top-down and bottom-up, just as math equations also can be expressed in
multiple equivalent forms: pre-order and post-order. We propose a multi-view
consistent contrastive learning for a more complete semantics-to-equation
mapping. The entire process is decoupled into two independent but consistent
views: top-down decomposition and bottom-up construction, and the two reasoning
views are aligned in multi-granularity for consistency, enhancing global
generation and precise reasoning. Experiments on multiple datasets across two
languages show our approach significantly outperforms the existing baselines,
especially on complex problems. We also show after consistent alignment,
multi-view can absorb the merits of both views and generate more diverse
results consistent with the mathematical laws.",https://github.com/zwq2018/,2217
What do tokens know about their characters and how do they know it?,0.873198,"Pre-trained language models (PLMs) that use subword tokenization schemes can
succeed at a variety of language tasks that require character-level
information, despite lacking explicit access to the character composition of
tokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe),
we probe what word pieces encode about character-level information by training
classifiers to predict the presence or absence of a particular alphabetical
character in a token, based on its embedding (e.g., probing whether the model
embedding for ""cat"" encodes that it contains the character ""a""). We find that
these models robustly encode character-level information and, in general,
larger models perform better at the task. We show that these results generalize
to characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic).
Then, through a series of experiments and analyses, we investigate the
mechanisms through which PLMs acquire English-language character information
during training and argue that this knowledge is acquired through multiple
phenomena, including a systematic relationship between particular characters
and particular parts of speech, as well as natural variability in the
tokenization of related strings.",https://github.com/ayushk4/character-probing-pytorch,-1
A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,0.784893,"Deep reinforcement learning is a promising approach to learning policies in
uncontrolled environments that do not require domain knowledge. Unfortunately,
due to sample inefficiency, deep RL applications have primarily focused on
simulated environments. In this work, we demonstrate that the recent
advancements in machine learning algorithms and libraries combined with a
carefully tuned robot controller lead to learning quadruped locomotion in only
20 minutes in the real world. We evaluate our approach on several indoor and
outdoor terrains which are known to be challenging for classical model-based
controllers. We observe the robot to be able to learn walking gait consistently
on all of these terrains. Finally, we evaluate our design decisions in a
simulated environment.",None,-1
Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal,0.708596,"Transformer-based large language models are trained to make predictions about
the next word by aggregating representations of previous tokens through their
self-attention mechanism. In the field of cognitive modeling, such attention
patterns have recently been interpreted as embodying the process of cue-based
retrieval, in which attention over multiple targets is taken to generate
interference and latency during retrieval. Under this framework, this work
first defines an entropy-based predictor that quantifies the diffuseness of
self-attention, as well as distance-based predictors that capture the
incremental change in attention patterns across timesteps. Moreover, following
recent studies that question the informativeness of attention weights, we also
experiment with alternative methods for incorporating vector norms into
attention weights. Regression experiments using predictors calculated from the
GPT-2 language model show that these predictors deliver a substantially better
fit to held-out self-paced reading and eye-tracking data over a rigorous
baseline including GPT-2 surprisal. Additionally, the distance-based predictors
generally demonstrated higher predictive power, with effect sizes of up to 6.59
ms per standard deviation on self-paced reading times (compared to 2.82 ms for
surprisal) and 1.05 ms per standard deviation on eye-gaze durations (compared
to 3.81 ms for surprisal).",https://github.com/byungdoh/attn_dist,-1
Scalable Multi-view Clustering with Graph Filtering,0.431922,"With the explosive growth of multi-source data, multi-view clustering has
attracted great attention in recent years. Most existing multi-view methods
operate in raw feature space and heavily depend on the quality of original
feature representation. Moreover, they are often designed for feature data and
ignore the rich topology structure information. Accordingly, in this paper, we
propose a generic framework to cluster both attribute and graph data with
heterogeneous features. It is capable of exploring the interplay between
feature and structure. Specifically, we first adopt graph filtering technique
to eliminate high-frequency noise to achieve a clustering-friendly smooth
representation. To handle the scalability challenge, we develop a novel
sampling strategy to improve the quality of anchors. Extensive experiments on
attribute and graph benchmarks demonstrate the superiority of our approach with
respect to state-of-the-art approaches.",https://github.com/EricliuLiang/SMC,-1
Classification of Hyperspectral Images Using SVM with Shape-adaptive Reconstruction and Smoothed Total Variation,0.56087,"In this work, a novel algorithm called SVM with Shape-adaptive Reconstruction
and Smoothed Total Variation (SaR-SVM-STV) is introduced to classify
hyperspectral images, which makes full use of spatial and spectral information.
The Shape-adaptive Reconstruction (SaR) is introduced to preprocess each pixel
based on the Pearson Correlation between pixels in its shape-adaptive (SA)
region. Support Vector Machines (SVMs) are trained to estimate the pixel-wise
probability maps of each class. Then the Smoothed Total Variation (STV) model
is applied to denoise and generate the final classification map. Experiments
show that SaR-SVM-STV outperforms the SVM-STV method with a few training
labels, demonstrating the significance of reconstructing hyperspectral images
before classification.",https://github.com/ckn3/SA-Recon,-1
Patents Phrase to Phrase Semantic Matching Dataset,0.531412,"There are many general purpose benchmark datasets for Semantic Textual
Similarity but none of them are focused on technical concepts found in patents
and scientific publications. This work aims to fill this gap by presenting a
new human rated contextual phrase to phrase matching dataset. The entire
dataset contains close to $50,000$ rated phrase pairs, each with a CPC
(Cooperative Patent Classification) class as a context. This paper describes
the dataset and some baseline models.",None,-1
NeuMan: Neural Human Radiance Field from a Single Video,0.995569,"Photorealistic rendering and reposing of humans is important for enabling
augmented reality experiences. We propose a novel framework to reconstruct the
human and the scene that can be rendered with novel human poses and views from
just a single in-the-wild video. Given a video captured by a moving camera, we
train two NeRF models: a human NeRF model and a scene NeRF model. To train
these models, we rely on existing methods to estimate the rough geometry of the
human and the scene. Those rough geometry estimates allow us to create a
warping field from the observation space to the canonical pose-independent
space, where we train the human model in. Our method is able to learn subject
specific details, including cloth wrinkles and accessories, from just a 10
seconds video clip, and to provide high quality renderings of the human under
novel poses, from novel views, together with the background.",https://github.com/apple/ml-neuman,-1
SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems,0.702603,"We design deep neural networks (DNNs) and corresponding networks' splittings
to distribute DNNs' workload to camera sensors and a centralized aggregator on
head mounted devices to meet system performance targets in inference accuracy
and latency under the given hardware resource constraints. To achieve an
optimal balance among computation, communication, and performance, a
split-aware neural architecture search framework, SplitNets, is introduced to
conduct model designing, splitting, and communication reduction simultaneously.
We further extend the framework to multi-view systems for learning to fuse
inputs from multiple camera sensors with optimal performance and systemic
efficiency. We validate SplitNets for single-view system on ImageNet as well as
multi-view system on 3D classification, and show that the SplitNets framework
achieves state-of-the-art (SOTA) performance and system latency compared with
existing approaches.",None,-1
Generating Compressed Combinatory Proof Structures -- An Approach to Automated First-Order Theorem Proving,0.312119,"Representing a proof tree by a combinator term that reduces to the tree lets
subtle forms of duplication within the tree materialize as duplicated subterms
of the combinator term. In a DAG representation of the combinator term these
straightforwardly factor into shared subgraphs. To search for proofs,
combinator terms can be enumerated, like clausal tableaux, interwoven with
unification of formulas that are associated with nodes of the enumerated
structures. To restrict the search space, the enumeration can be based on proof
schemas defined as parameterized combinator terms. We introduce here this
""combinator term as proof structure"" approach to automated first-order proving,
present an implementation and first experimental results. The approach builds
on a term view of proof structures rooted in condensed detachment and the
connection method. It realizes features known from the connection structure
calculus, which has not been implemented so far.",None,-1
Quantitative AI Risk Assessments: Opportunities and Challenges,0.768873,"Although AI-based systems are increasingly being leveraged to provide value
to organizations, individuals, and society, significant attendant risks have
been identified. These risks have led to proposed regulations, litigation, and
general societal concerns.
  As with any promising technology, organizations want to benefit from the
positive capabilities of AI technology while reducing the risks. The best way
to reduce risks is to implement comprehensive AI lifecycle governance where
policies and procedures are described and enforced during the design,
development, deployment, and monitoring of an AI system. While support for
comprehensive governance is beginning to emerge, organizations often need to
identify the risks of deploying an already-built model without knowledge of how
it was constructed or access to its original developers.
  Such an assessment will quantitatively assess the risks of an existing model
in a manner analogous to how a home inspector might assess the energy
efficiency of an already-built home or a physician might assess overall patient
health based on a battery of tests. This paper explores the concept of a
quantitative AI Risk Assessment, exploring the opportunities, challenges, and
potential impacts of such an approach, and discussing how it might improve AI
regulations.",None,-1
Tractable Boolean and Arithmetic Circuits,0.69657,"Tractable Boolean and arithmetic circuits have been studied extensively in AI
for over two decades now. These circuits were initially proposed as ""compiled
objects,"" meant to facilitate logical and probabilistic reasoning, as they
permit various types of inference to be performed in linear-time and a
feed-forward fashion like neural networks. In more recent years, the role of
tractable circuits has significantly expanded as they became a computational
and semantical backbone for some approaches that aim to integrate knowledge,
reasoning and learning. In this article, we review the foundations of tractable
circuits and some associated milestones, while focusing on their core
properties and techniques that make them particularly useful for the broad aims
of neuro-symbolic AI.",None,-1
BigColor: Colorization using a Generative Color Prior for Natural Images,0.820194,"For realistic and vivid colorization, generative priors have recently been
exploited. However, such generative priors often fail for in-the-wild complex
images due to their limited representation space. In this paper, we propose
BigColor, a novel colorization approach that provides vivid colorization for
diverse in-the-wild images with complex structures. While previous generative
priors are trained to synthesize both image structures and colors, we learn a
generative color prior to focus on color synthesis given the spatial structure
of an image. In this way, we reduce the burden of synthesizing image structures
from the generative prior and expand its representation space to cover diverse
images. To this end, we propose a BigGAN-inspired encoder-generator network
that uses a spatial feature map instead of a spatially-flattened BigGAN latent
code, resulting in an enlarged representation space. Our method enables robust
colorization for diverse inputs in a single forward pass, supports arbitrary
input resolutions, and provides multi-modal colorization results. We
demonstrate that BigColor significantly outperforms existing methods especially
on in-the-wild images with complex structures.",https://github.com/jantic/DeOldify,-1
Ditto: Building Digital Twins of Articulated Objects from Interaction,0.930759,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto",https://ut-austin-rpl.github.io/Ditto/,18716
Assessment of contextualised representations in detecting outcome phrases in clinical trials,0.200472,"Automating the recognition of outcomes reported in clinical trials using
machine learning has a huge potential of speeding up access to evidence
necessary in healthcare decision-making. Prior research has however
acknowledged inadequate training corpora as a challenge for the Outcome
detection (OD) task. Additionally, several contextualized representations like
BERT and ELMO have achieved unparalleled success in detecting various diseases,
genes, proteins, and chemicals, however, the same cannot be emphatically stated
for outcomes, because these models have been relatively under-tested and
studied for the OD task. We introduce ""EBM-COMET"", a dataset in which 300
PubMed abstracts are expertly annotated for clinical outcomes. Unlike prior
related datasets that use arbitrary outcome classifications, we use labels from
a taxonomy recently published to standardize outcome classifications. To
extract outcomes, we fine-tune a variety of pre-trained contextualized
representations, additionally, we use frozen contextualized and
context-independent representations in our custom neural model augmented with
clinically informed Part-Of-Speech embeddings and a cost-sensitive loss
function. We adopt strict evaluation for the trained models by rewarding them
for correctly identifying full outcome phrases rather than words within the
entities i.e. given an outcome ""systolic blood pressure"", the models are
rewarded a classification score only when they predict all 3 words in sequence,
otherwise, they are not rewarded. We observe our best model (BioBERT) achieve
81.5\% F1, 81.3\% sensitivity and 98.0\% specificity. We reach a consensus on
which contextualized representations are best suited for detecting outcomes
from clinical-trial abstracts. Furthermore, our best model outperforms scores
published on the original EBM-NLP dataset leader-board scores.",https://github.com/MichealAbaho/ODP-tagger,-1
Boosting human decision-making with AI-generated decision aids,0.285486,"Human decision-making is plagued by many systematic errors. Many of these
errors can be avoided by providing decision aids that guide decision-makers to
attend to the important information and integrate it according to a rational
decision strategy. Designing such decision aids used to be a tedious manual
process. Advances in cognitive science might make it possible to automate this
process in the future. We recently introduced machine learning methods for
discovering optimal strategies for human decision-making automatically and an
automatic method for explaining those strategies to people. Decision aids
constructed by this method were able to improve human decision-making. However,
following the descriptions generated by this method is very tedious. We
hypothesized that this problem can be overcome by conveying the automatically
discovered decision strategy as a series of natural language instructions for
how to reach a decision. Experiment 1 showed that people do indeed understand
such procedural instructions more easily than the decision aids generated by
our previous method. Encouraged by this finding, we developed an algorithm for
translating the output of our previous method into procedural instructions. We
applied the improved method to automatically generate decision aids for a
naturalistic planning task (i.e., planning a road trip) and a naturalistic
decision task (i.e., choosing a mortgage). Experiment 2 showed that these
automatically generated decision-aids significantly improved people's
performance in planning a road trip and choosing a mortgage. These findings
suggest that AI-powered boosting might have potential for improving human
decision-making in the real world.",https://github.com/RationalityEnhancement/InterpretableStrategyDiscovery,-1
Evaluating Feature Attribution Methods in the Image Domain,0.225253,"Feature attribution maps are a popular approach to highlight the most
important pixels in an image for a given prediction of a model. Despite a
recent growth in popularity and available methods, little attention is given to
the objective evaluation of such attribution maps. Building on previous work in
this domain, we investigate existing metrics and propose new variants of
metrics for the evaluation of attribution maps. We confirm a recent finding
that different attribution metrics seem to measure different underlying
concepts of attribution maps, and extend this finding to a larger selection of
attribution metrics. We also find that metric results on one dataset do not
necessarily generalize to other datasets, and methods with desirable
theoretical properties such as DeepSHAP do not necessarily outperform
computationally cheaper alternatives. Based on these findings, we propose a
general benchmarking approach to identify the ideal feature attribution method
for a given use case. Implementations of attribution metrics and our
experiments are available online.",https://github.com/zoeparman/benchmark-general-imaging,-1
PedRecNet: Multi-task deep neural network for full 3D human pose and orientation estimation,0.504285,"We present a multitask network that supports various deep neural network
based pedestrian detection functions. Besides 2D and 3D human pose, it also
supports body and head orientation estimation based on full body bounding box
input. This eliminates the need for explicit face recognition. We show that the
performance of 3D human pose estimation and orientation estimation is
comparable to the state-of-the-art. Since very few data sets exist for 3D human
pose and in particular body and head orientation estimation based on full body
data, we further show the benefit of particular simulation data to train the
network. The network architecture is relatively simple, yet powerful, and
easily adaptable for further research and applications.",https://github.com/noboevbo/PedRec,-1
LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval,0.623964,"Retrieval models based on dense representations in semantic space have become
an indispensable branch for first-stage retrieval. These retrievers benefit
from surging advances in representation learning towards compressive global
sequence-level embeddings. However, they are prone to overlook local salient
phrases and entity mentions in texts, which usually play pivot roles in
first-stage retrieval. To mitigate this weakness, we propose to make a dense
retriever align a well-performing lexicon-aware representation model. The
alignment is achieved by weakened knowledge distillations to enlighten the
retriever via two aspects -- 1) a lexicon-augmented contrastive objective to
challenge the dense encoder and 2) a pair-wise rank-consistent regularization
to make dense model's behavior incline to the other. We evaluate our model on
three public benchmarks, which shows that with a comparable lexicon-aware
retriever as the teacher, our proposed dense one can bring consistent and
significant improvements, and even outdo its teacher. In addition, we found our
improvement on the dense retriever is complementary to the standard ranker
distillation, which can further lift state-of-the-art performance.",None,-1
Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding,0.69234,"Inspired by the impressive performance of recent face image editing methods,
several studies have been naturally proposed to extend these methods to the
face video editing task. One of the main challenges here is temporal
consistency among edited frames, which is still unresolved. To this end, we
propose a novel face video editing framework based on diffusion autoencoders
that can successfully extract the decomposed features - for the first time as a
face video editing model - of identity and motion from a given video. This
modeling allows us to edit the video by simply manipulating the temporally
invariant feature to the desired direction for the consistency. Another unique
strength of our model is that, since our model is based on diffusion models, it
can satisfy both reconstruction and edit capabilities at the same time, and is
robust to corner cases in wild face videos (e.g. occluded faces) unlike the
existing GAN-based methods.",https://diff-video-ae.github.io,-1
AI Ethics Issues in Real World: Evidence from AI Incident Database,0.680139,"With the powerful performance of Artificial Intelligence (AI) also comes
prevalent ethical issues. Though governments and corporations have curated
multiple AI ethics guidelines to curb unethical behavior of AI, the effect has
been limited, probably due to the vagueness of the guidelines. In this paper,
we take a closer look at how AI ethics issues take place in real world, in
order to have a more in-depth and nuanced understanding of different ethical
issues as well as their social impact. With a content analysis of AI Incident
Database, which is an effort to prevent repeated real world AI failures by
cataloging incidents, we identified 13 application areas which often see
unethical use of AI, with intelligent service robots, language/vision models
and autonomous driving taking the lead. Ethical issues appear in 8 different
forms, from inappropriate use and racial discrimination, to physical safety and
unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI
practitioners with a practical guideline when trying to deploy AI applications
ethically.",None,-1
META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI,0.521167,"Task-oriented dialogue (TOD) systems have been widely used by mobile phone
intelligent assistants to accomplish tasks such as calendar scheduling or hotel
reservation. Current TOD systems usually focus on multi-turn text/speech
interaction, then they would call back-end APIs designed for TODs to perform
the task. However, this API-based architecture greatly limits the
information-searching capability of intelligent assistants and may even lead to
task failure if TOD-specific APIs are not available or the task is too
complicated to be executed by the provided APIs. In this paper, we propose a
new TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A
GUI-TOD system can directly perform GUI operations on real APPs and execute
tasks without invoking TOD-specific backend APIs. Furthermore, we release
META-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile
GUI. We also propose a multi-model action prediction and response model, which
show promising results on META-GUI. The dataset, codes and leaderboard are
publicly available.",https://x-lance.github.io/,-1
BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment,0.731738,"This work addresses the Burst Super-Resolution (BurstSR) task using a new
architecture, which requires restoring a high-quality image from a sequence of
noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in
BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can
significantly improve the capability of extracting inter-frame information and
reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided
Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin
Transformer Blocks and Groups as our main backbone. More specifically, we
combine optical flows and deformable convolutions, hence our BSRT can handle
misalignment and aggregate the potential texture information in multi-frames
more efficiently. In addition, our Transformer-based structure can capture
long-range dependency to further improve the performance. The evaluation on
both synthetic and real-world tracks demonstrates that our approach achieves a
new state-of-the-art in BurstSR task. Further, our BSRT wins the championship
in the NTIRE2022 Burst Super-Resolution Challenge.",https://github.com/Algolzw/BSRT,-1
Boundary Smoothing for Named Entity Recognition,0.903493,"Neural named entity recognition (NER) models may easily encounter the
over-confidence issue, which degrades the performance and calibration. Inspired
by label smoothing and driven by the ambiguity of boundary annotation in NER
engineering, we propose boundary smoothing as a regularization technique for
span-based neural NER models. It re-assigns entity probabilities from annotated
spans to the surrounding ones. Built on a simple but strong baseline, our model
achieves results better than or competitive with previous state-of-the-art
systems on eight well-known NER benchmarks. Further empirical analysis suggests
that boundary smoothing effectively mitigates over-confidence, improves model
calibration, and brings flatter neural minima and more smoothed loss
landscapes.",https://github.com/syuoni/eznlp,-1
Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation,0.992143,"Human evaluation is the foundation upon which the evaluation of both
summarization systems and automatic metrics rests. However, existing human
evaluation studies for summarization either exhibit a low inter-annotator
agreement or have insufficient scale, and an in-depth analysis of human
evaluation is lacking. Therefore, we address the shortcomings of existing
summarization evaluation along the following axes: (1) We propose a modified
summarization salience protocol, Atomic Content Units (ACUs), which is based on
fine-grained semantic units and allows for a high inter-annotator agreement.
(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large
human evaluation dataset consisting of 22,000 summary-level annotations over 28
top-performing systems on three datasets. (3) We conduct a comparative study of
four human evaluation protocols, underscoring potential confounding factors in
evaluation setups. (4) We evaluate 50 automatic metrics and their variants
using the collected human annotations across evaluation protocols and
demonstrate how our benchmark leads to more statistically stable and
significant results. The metrics we benchmarked include recent methods based on
large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings
have important implications for evaluating LLMs, as we show that LLMs adjusted
by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,
which is affected by the annotators' prior, input-agnostic preferences, calling
for more robust, targeted evaluation methods.",https://github.com/Yale-LILY/ROSE,41181
Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs,0.0329622,"Answering factual questions with temporal intent over knowledge graphs
(temporal KGQA) attracts rising attention in recent years. In the generation of
temporal queries, existing KGQA methods ignore the fact that some intrinsic
connections between events can make them temporally related, which may limit
their capability. We systematically analyze the possible interpretation of
temporal constraints and conclude the interpretation structures as the Semantic
Framework of Temporal Constraints, SF-TCons. Based on the semantic framework,
we propose a temporal question answering method, SF-TQA, which generates query
graphs by exploring the relevant facts of mentioned entities, where the
exploring process is restricted by SF-TCons. Our evaluations show that SF-TQA
significantly outperforms existing methods on two benchmarks over different
knowledge graphs.",None,-1
Universal Adaptive Data Augmentation,0.371904,"Existing automatic data augmentation (DA) methods either ignore updating DA's
parameters according to the target model's state during training or adopt
update strategies that are not effective enough. In this work, we design a
novel data augmentation strategy called ""Universal Adaptive Data Augmentation""
(UADA). Different from existing methods, UADA would adaptively update DA's
parameters according to the target model's gradient information during
training: given a pre-defined set of DA operations, we randomly decide types
and magnitudes of DA operations for every data batch during training, and
adaptively update DA's parameters along the gradient direction of the loss
concerning DA's parameters. In this way, UADA can increase the training loss of
the target networks, and the target networks would learn features from harder
samples to improve the generalization. Moreover, UADA is very general and can
be utilized in numerous tasks, e.g., image classification, semantic
segmentation and object detection. Extensive experiments with various models
are conducted on CIFAR-10, CIFAR-100, ImageNet, tiny-ImageNet, Cityscapes, and
VOC07+12 to prove the significant performance improvements brought by UADA.",None,-1
A.I. Robustness: a Human-Centered Perspective on Technological Challenges and Opportunities,0.262764,"Despite the impressive performance of Artificial Intelligence (AI) systems,
their robustness remains elusive and constitutes a key issue that impedes
large-scale adoption. Robustness has been studied in many domains of AI, yet
with different interpretations across domains and contexts. In this work, we
systematically survey the recent progress to provide a reconciled terminology
of concepts around AI robustness. We introduce three taxonomies to organize and
describe the literature both from a fundamental and applied point of view: 1)
robustness by methods and approaches in different phases of the machine
learning pipeline; 2) robustness for specific model architectures, tasks, and
systems; and in addition, 3) robustness assessment methodologies and insights,
particularly the trade-offs with other trustworthiness properties. Finally, we
identify and discuss research gaps and opportunities and give an outlook on the
field. We highlight the central role of humans in evaluating and enhancing AI
robustness, considering the necessary knowledge humans can provide, and discuss
the need for better understanding practices and developing supportive tools in
the future.",None,8667
Goal-Conditioned Reinforcement Learning: Problems and Solutions,0.994954,"Goal-conditioned reinforcement learning (GCRL), related to a set of complex
RL problems, trains an agent to achieve different goals under particular
scenarios. Compared to the standard RL solutions that learn a policy solely
depending on the states or observations, GCRL additionally requires the agent
to make decisions according to different goals. In this survey, we provide a
comprehensive overview of the challenges and algorithms for GCRL. Firstly, we
answer what the basic problems are studied in this field. Then, we explain how
goals are represented and present how existing solutions are designed from
different points of view. Finally, we make the conclusion and discuss potential
future prospects that recent researches focus on.",https://github.com/apexrl/,-1
CLIPascene: Scene Sketching with Different Types and Levels of Abstraction,0.579664,"In this paper, we present a method for converting a given scene image into a
sketch using different types and multiple levels of abstraction. We distinguish
between two types of abstraction. The first considers the fidelity of the
sketch, varying its representation from a more precise portrayal of the input
to a looser depiction. The second is defined by the visual simplicity of the
sketch, moving from a detailed depiction to a sparse sketch. Using an explicit
disentanglement into two abstraction axes -- and multiple levels for each one
-- provides users additional control over selecting the desired sketch based on
their personal goals and preferences. To form a sketch at a given level of
fidelity and simplification, we train two MLP networks. The first network
learns the desired placement of strokes, while the second network learns to
gradually remove strokes from the sketch without harming its recognizability
and semantics. Our approach is able to generate sketches of complex scenes
including those with complex backgrounds (e.g., natural and urban settings) and
subjects (e.g., animals and people) while depicting gradual abstractions of the
input scene in terms of fidelity and simplicity.",None,-1
Exemplar-free Online Continual Learning,0.752234,"Targeted for real world scenarios, online continual learning aims to learn
new tasks from sequentially available data under the condition that each data
is observed only once by the learner. Though recent works have made remarkable
achievements by storing part of learned task data as exemplars for knowledge
replay, the performance is greatly relied on the size of stored exemplars while
the storage consumption is a significant constraint in continual learning. In
addition, storing exemplars may not always be feasible for certain applications
due to privacy concerns. In this work, we propose a novel exemplar-free method
by leveraging nearest-class-mean (NCM) classifier where the class mean is
estimated during training phase on all data seen so far through online mean
update criteria. We focus on image classification task and conduct extensive
experiments on benchmark datasets including CIFAR-100 and Food-1k. The results
demonstrate that our method without using any exemplar outperforms
state-of-the-art exemplar-based approaches with large margins under standard
protocol (20 exemplars per class) and is able to achieve competitive
performance even with larger exemplar size (100 exemplars per class).",None,-1
Deep-Attack over the Deep Reinforcement Learning,0.680989,"Recent adversarial attack developments have made reinforcement learning more
vulnerable, and different approaches exist to deploy attacks against it, where
the key is how to choose the right timing of the attack. Some work tries to
design an attack evaluation function to select critical points that will be
attacked if the value is greater than a certain threshold. This approach makes
it difficult to find the right place to deploy an attack without considering
the long-term impact. In addition, there is a lack of appropriate indicators of
assessment during attacks. To make the attacks more intelligent as well as to
remedy the existing problems, we propose the reinforcement learning-based
attacking framework by considering the effectiveness and stealthy
spontaneously, while we also propose a new metric to evaluate the performance
of the attack model in these two aspects. Experimental results show the
effectiveness of our proposed model and the goodness of our proposed evaluation
metric. Furthermore, we validate the transferability of the model, and also its
robustness under the adversarial training.",None,-1
CtlGAN: Few-shot Artistic Portraits Generation with Contrastive Transfer Learning,0.92756,"Generating artistic portraits is a challenging problem in computer vision.
Existing portrait stylization models that generate good quality results are
based on Image-to-Image Translation and require abundant data from both source
and target domains. However, without enough data, these methods would result in
overfitting. In this work, we propose CtlGAN, a new few-shot artistic portraits
generation model with a novel contrastive transfer learning strategy. We adapt
a pretrained StyleGAN in the source domain to a target artistic domain with no
more than 10 artistic faces. To reduce overfitting to the few training
examples, we introduce a novel Cross-Domain Triplet loss which explicitly
encourages the target instances generated from different latent codes to be
distinguishable. We propose a new encoder which embeds real faces into Z+ space
and proposes a dual-path training strategy to better cope with the adapted
decoder and eliminate the artifacts. Extensive qualitative, quantitative
comparisons and a user study show our method significantly outperforms
state-of-the-arts under 10-shot and 1-shot settings and generates high quality
artistic portraits. The code will be made publicly available.",None,-1
MRI-GAN: A Generalized Approach to Detect DeepFakes using Perceptual Image Assessment,0.0577344,"DeepFakes are synthetic videos generated by swapping a face of an original
image with the face of somebody else. In this paper, we describe our work to
develop general, deep learning-based models to classify DeepFake content. We
propose a novel framework for using Generative Adversarial Network (GAN)-based
models, we call MRI-GAN, that utilizes perceptual differences in images to
detect synthesized videos. We test our MRI-GAN approach and a
plain-frames-based model using the DeepFake Detection Challenge Dataset. Our
plain frames-based-model achieves 91% test accuracy and a model which uses our
MRI-GAN framework with Structural Similarity Index Measurement (SSIM) for the
perceptual differences achieves 74% test accuracy. The results of MRI-GAN are
preliminary and may be improved further by modifying the choice of loss
function, tuning hyper-parameters, or by using a more advanced perceptual
similarity metric.",https://github.com/pratikpv/mri_gan_deepfake,-1
CATs are Fuzzy PETs: A Corpus and Analysis of Potentially Euphemistic Terms,0.408779,"Euphemisms have not received much attention in natural language processing,
despite being an important element of polite and figurative language.
Euphemisms prove to be a difficult topic, not only because they are subject to
language change, but also because humans may not agree on what is a euphemism
and what is not. Nevertheless, the first step to tackling the issue is to
collect and analyze examples of euphemisms. We present a corpus of potentially
euphemistic terms (PETs) along with example texts from the GloWbE corpus.
Additionally, we present a subcorpus of texts where these PETs are not being
used euphemistically, which may be useful for future applications. We also
discuss the results of multiple analyses run on the corpus. Firstly, we find
that sentiment analysis on the euphemistic texts supports that PETs generally
decrease negative and offensive sentiment. Secondly, we observe cases of
disagreement in an annotation task, where humans are asked to label PETs as
euphemistic or not in a subset of our corpus text examples. We attribute the
disagreement to a variety of potential reasons, including if the PET was a
commonly accepted term (CAT).",https://github.com/marsgav/euphemism_project,-1
Fine-tuned Sentiment Analysis of COVID-19 Vaccine-Related Social Media Data: Comparative Study,0.61043,"This study investigated and compared public sentiment related to COVID-19
vaccines expressed on two popular social media platforms, Reddit and Twitter,
harvested from January 1, 2020, to March 1, 2022. To accomplish this task, we
created a fine-tuned DistilRoBERTa model to predict sentiments of approximately
9.5 million Tweets and 70 thousand Reddit comments. To fine-tune our model, our
team manually labeled the sentiment of 3600 Tweets and then augmented our
dataset by the method of back-translation. Text sentiment for each social media
platform was then classified with our fine-tuned model using Python and the
Huggingface sentiment analysis pipeline. Our results determined that the
average sentiment expressed on Twitter was more negative (52% positive) than
positive and the sentiment expressed on Reddit was more positive than negative
(53% positive). Though average sentiment was found to vary between these social
media platforms, both displayed similar behavior related to sentiment shared at
key vaccine-related developments during the pandemic. Considering this similar
trend in shared sentiment demonstrated across social media platforms, Twitter
and Reddit continue to be valuable data sources that public health officials
can utilize to strengthen vaccine confidence and combat misinformation. As the
spread of misinformation poses a range of psychological and psychosocial risks
(anxiety, fear, etc.), there is an urgency in understanding the public
perspective and attitude toward shared falsities. Comprehensive educational
delivery systems tailored to the population's expressed sentiments that
facilitate digital literacy, health information-seeking behavior, and precision
health promotion could aid in clarifying such misinformation.",None,-1
Controllable Dialogue Simulation with In-Context Learning,0.868075,"Building dialogue systems requires a large corpus of annotated dialogues.
Such datasets are usually created via crowdsourcing, which is expensive and
time-consuming. In this paper, we propose \textsc{Dialogic}, a novel dialogue
simulation method based on large language model in-context learning to automate
dataset creation. Seeded with a few annotated dialogues, \textsc{Dialogic}
automatically selects in-context examples for demonstration and prompts GPT-3
to generate new dialogues and annotations in a controllable way. Our method can
rapidly expand a small set of dialogue data with minimum or zero \textit{human
involvement} and \textit{parameter update} and is thus much more cost-efficient
and time-saving than crowdsourcing. Experimental results on the MultiWOZ
dataset demonstrate that training a model on the simulated dialogues leads to
even better performance than using the same amount of human-generated dialogues
under the challenging low-resource settings, with as few as 85 dialogues as a
seed. When enough data is available, our method can still serve as an effective
data augmentation method. Human evaluation results also show that our simulated
dialogues have near-human fluency and annotation accuracy. The code and data
are available at \textbf{\url{https://github.com/Leezekun/dialogic}}.",https://github.com/Leezekun/dialogic,-1
Ham2Pose: Animating Sign Language Notation into Pose Sequences,0.89581,"Translating spoken languages into Sign languages is necessary for open
communication between the hearing and hearing-impaired communities. To achieve
this goal, we propose the first method for animating a text written in
HamNoSys, a lexical Sign language notation, into signed pose sequences. As
HamNoSys is universal by design, our proposed method offers a generic solution
invariant to the target Sign language. Our method gradually generates pose
predictions using transformer encoders that create meaningful representations
of the text and poses while considering their spatial and temporal information.
We use weak supervision for the training process and show that our method
succeeds in learning from partial and inaccurate data. Additionally, we offer a
new distance measurement that considers missing keypoints, to measure the
distance between pose sequences using DTW-MJE. We validate its correctness
using AUTSL, a large-scale Sign language dataset, show that it measures the
distance between pose sequences more accurately than existing measurements, and
use it to assess the quality of our generated pose sequences. Code for the data
pre-processing, the model, and the distance measurement is publicly released
for future research.",https://github.com/AmitMY/pose-format,3241
SummScore: A Comprehensive Evaluation Metric for Summary Quality Based on Cross-Encoder,0.171948,"Text summarization models are often trained to produce summaries that meet
human quality requirements. However, the existing evaluation metrics for
summary text are only rough proxies for summary quality, suffering from low
correlation with human scoring and inhibition of summary diversity. To solve
these problems, we propose SummScore, a comprehensive metric for summary
quality evaluation based on CrossEncoder. Firstly, by adopting the
original-summary measurement mode and comparing the semantics of the original
text, SummScore gets rid of the inhibition of summary diversity. With the help
of the text-matching pre-training Cross-Encoder, SummScore can effectively
capture the subtle differences between the semantics of summaries. Secondly, to
improve the comprehensiveness and interpretability, SummScore consists of four
fine-grained submodels, which measure Coherence, Consistency, Fluency, and
Relevance separately. We use semi-supervised multi-rounds of training to
improve the performance of our model on extremely limited annotated data.
Extensive experiments show that SummScore significantly outperforms existing
evaluation metrics in the above four dimensions in correlation with human
scoring. We also provide the quality evaluation results of SummScore on 16
mainstream summarization models for later research.",None,4451
Prosodic Alignment for off-screen automatic dubbing,0.833214,"The goal of automatic dubbing is to perform speech-to-speech translation
while achieving audiovisual coherence. This entails isochrony, i.e.,
translating the original speech by also matching its prosodic structure into
phrases and pauses, especially when the speaker's mouth is visible. In previous
work, we introduced a prosodic alignment model to address isochrone or
on-screen dubbing. In this work, we extend the prosodic alignment model to also
address off-screen dubbing that requires less stringent synchronization
constraints. We conduct experiments on four dubbing directions - English to
French, Italian, German and Spanish - on a publicly available collection of TED
Talks and on publicly available YouTube videos. Empirical results show that
compared to our previous work the extended prosodic alignment model provides
significantly better subjective viewing experience on videos in which on-screen
and off-screen automatic dubbing is applied for sentences with speakers mouth
visible and not visible, respectively.",None,-1
CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI Models,0.174673,"As the practicality of Artificial Intelligence (AI) and Machine Learning (ML)
based techniques grow, there is an ever increasing threat of adversarial
attacks. There is a need to red team this ecosystem to identify system
vulnerabilities, potential threats, characterize properties that will enhance
system robustness, and encourage the creation of effective defenses. A
secondary need is to share this AI security threat intelligence between
different stakeholders like, model developers, users, and AI/ML security
professionals. In this paper, we create and describe a prototype system CTI4AI,
to overcome the need to methodically identify and share AI/ML specific
vulnerabilities and threat intelligence.",None,-1
Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition,0.442272,"The practice of fine-tuning Pre-trained Language Models (PLMs) from general
or domain-specific data to a specific task with limited resources, has gained
popularity within the field of natural language processing (NLP). In this work,
we re-visit this assumption and carry out an investigation in clinical NLP,
specifically Named Entity Recognition on drugs and their related attributes. We
compare Transformer models that are trained from scratch to fine-tuned
BERT-based LLMs namely BERT, BioBERT, and ClinicalBERT. Furthermore, we examine
the impact of an additional CRF layer on such models to encourage contextual
learning. We use n2c2-2018 shared task data for model development and
evaluations. The experimental outcomes show that 1) CRF layers improved all
language models; 2) referring to BIO-strict span level evaluation using
macro-average F1 score, although the fine-tuned LLMs achieved 0.83+ scores, the
TransformerCRF model trained from scratch achieved 0.78+, demonstrating
comparable performances with much lower cost - e.g. with 39.80\% less training
parameters; 3) referring to BIO-strict span-level evaluation using
weighted-average F1 score, ClinicalBERT-CRF, BERT-CRF, and TransformerCRF
exhibited lower score differences, with 97.59\%/97.44\%/96.84\% respectively.
4) applying efficient training by down-sampling for better data distribution
further reduced the training cost and need for data, while maintaining similar
scores - i.e. around 0.02 points lower compared to using the full dataset. Our
models will be hosted at \url{https://github.com/HECTA-UoM/TransformerCRF}",https://github.com/HECTA-UoM/TransformerCRF,-1
SVG Vector Font Generation for Chinese Characters with Transformer,0.615773,"Designing fonts for Chinese characters is highly labor-intensive and
time-consuming. While the latest methods successfully generate the English
alphabet vector font, despite the high demand for automatic font generation,
Chinese vector font generation has been an unsolved problem owing to its
complex shape and numerous characters. This study addressed the problem of
automatically generating Chinese vector fonts from only a single style and
content reference. We proposed a novel network architecture with Transformer
and loss functions to capture structural features without differentiable
rendering. Although the dataset range was still limited to the sans-serif
family, we successfully generated the Chinese vector font for the first time
using the proposed method.",None,-1
Improved Evaluation and Generation of Grid Layouts using Distance Preservation Quality and Linear Assignment Sorting,0.576825,"Images sorted by similarity enables more images to be viewed simultaneously,
and can be very useful for stock photo agencies or e-commerce applications.
Visually sorted grid layouts attempt to arrange images so that their proximity
on the grid corresponds as closely as possible to their similarity. Various
metrics exist for evaluating such arrangements, but there is low experimental
evidence on correlation between human perceived quality and metric value. We
propose Distance Preservation Quality (DPQ) as a new metric to evaluate the
quality of an arrangement. Extensive user testing revealed stronger correlation
of DPQ with user-perceived quality and performance in image retrieval tasks
compared to other metrics. In addition, we introduce Fast Linear Assignment
Sorting (FLAS) as a new algorithm for creating visually sorted grid layouts.
FLAS achieves very good sorting qualities while improving run time and
computational resources.",https://github.com/Visual-Computing/LAS_FLAS,-1
M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation,0.21993,"End-to-end speech-to-text translation models are often initialized with
pre-trained speech encoder and pre-trained text decoder. This leads to a
significant training gap between pre-training and fine-tuning, largely due to
the modality differences between speech outputs from the encoder and text
inputs to the decoder. In this work, we aim to bridge the modality gap between
speech and text to improve translation quality. We propose M-Adapter, a novel
Transformer-based module, to adapt speech representations to text. While
shrinking the speech sequence, M-Adapter produces features desired for
speech-to-text translation via modelling global and local dependencies of a
speech sequence. Our experimental results show that our model outperforms a
strong baseline by up to 1 BLEU score on the Must-C En$\rightarrow$DE
dataset.\footnote{Our code is available at
https://github.com/mingzi151/w2v2-st.}",https://github.com/mingzi151/w2v2-st,-1
PSDoodle: Searching for App Screens via Interactive Sketching,0.592237,"Keyword-based mobile screen search does not account for screen content and
fails to operate as a universal tool for all levels of users. Visual searching
(e.g., image, sketch) is structured and easy to adopt. Current visual search
approaches count on a complete screen and are therefore slow and tedious.
PSDoodle employs a deep neural network to recognize partial screen element
drawings instantly on a digital drawing interface and shows results in
real-time. PSDoodle is the first tool that utilizes partial sketches and
searches for screens in an interactive iterative way. PSDoodle supports
different drawing styles and retrieves search results that are relevant to the
user's sketch query. A short video demonstration is available online at:
https://youtu.be/3cVLHFm5pY4",https://github.com/soumikmohianuta/PSDoodle,2830
"Runtime Analysis for the NSGA-II: Proving, Quantifying, and Explaining the Inefficiency For Many Objectives",0.502374,"The NSGA-II is one of the most prominent algorithms to solve multi-objective
optimization problems. Despite numerous successful applications, several
studies have shown that the NSGA-II is less effective for larger numbers of
objectives. In this work, we use mathematical runtime analyses to rigorously
demonstrate and quantify this phenomenon. We show that even on the simple
$m$-objective generalization of the discrete OneMinMax benchmark, where every
solution is Pareto optimal, the NSGA-II also with large population sizes cannot
compute the full Pareto front (objective vectors of all Pareto optima) in
sub-exponential time when the number of objectives is at least three. The
reason for this unexpected behavior lies in the fact that in the computation of
the crowding distance, the different objectives are regarded independently.
This is not a problem for two objectives, where any sorting of a pair-wise
incomparable set of solutions according to one objective is also such a sorting
according to the other objective (in the inverse order).",None,-1
Meta-Learning Sparse Compression Networks,0.604255,"Recent work in Deep Learning has re-imagined the representation of data as
functions mapping from a coordinate space to an underlying continuous signal.
When such functions are approximated by neural networks this introduces a
compelling alternative to the more common multi-dimensional array
representation. Recent work on such Implicit Neural Representations (INRs) has
shown that - following careful architecture search - INRs can outperform
established compression methods such as JPEG (e.g. Dupont et al., 2021). In
this paper, we propose crucial steps towards making such ideas scalable:
Firstly, we employ state-of-the-art network sparsification techniques to
drastically improve compression. Secondly, introduce the first method allowing
for sparsification to be employed in the inner-loop of commonly used
Meta-Learning algorithms, drastically improving both compression and the
computational cost of learning INRs. The generality of this formalism allows us
to present results on diverse data modalities such as images, manifolds, signed
distance functions, 3D shapes and scenes, several of which establish new
state-of-the-art results.",None,-1
Retrieval Augmentation for Commonsense Reasoning: A Unified Approach,0.448554,"A common thread of retrieval-augmented methods in the existing literature
focuses on retrieving encyclopedic knowledge, such as Wikipedia, which
facilitates well-defined entity and relation spaces that can be modeled.
However, applying such methods to commonsense reasoning tasks faces two unique
challenges, i.e., the lack of a general large-scale corpus for retrieval and a
corresponding effective commonsense retriever. In this paper, we systematically
investigate how to leverage commonsense knowledge retrieval to improve
commonsense reasoning tasks. We proposed a unified framework of
retrieval-augmented commonsense reasoning (called RACo), including a newly
constructed commonsense corpus with over 20 million documents and novel
strategies for training a commonsense retriever. We conducted experiments on
four different commonsense reasoning tasks. Extensive evaluation results showed
that our proposed RACo can significantly outperform other knowledge-enhanced
method counterparts, achieving new SoTA performance on the CommonGen and CREAK
leaderboards.",https://github.com/wyu97/RACo,-1
Inference and dynamic decision-making for deteriorating systems with probabilistic dependencies through Bayesian networks and deep reinforcement learning,0.912096,"In the context of modern environmental and societal concerns, there is an
increasing demand for methods able to identify management strategies for civil
engineering systems, minimizing structural failure risks while optimally
planning inspection and maintenance (I&M) processes. Most available methods
simplify the I&M decision problem to the component level due to the
computational complexity associated with global optimization methodologies
under joint system-level state descriptions. In this paper, we propose an
efficient algorithmic framework for inference and decision-making under
uncertainty for engineering systems exposed to deteriorating environments,
providing optimal management strategies directly at the system level. In our
approach, the decision problem is formulated as a factored partially observable
Markov decision process, whose dynamics are encoded in Bayesian network
conditional structures. The methodology can handle environments under equal or
general, unequal deterioration correlations among components, through Gaussian
hierarchical structures and dynamic Bayesian networks. In terms of policy
optimization, we adopt a deep decentralized multi-agent actor-critic (DDMAC)
reinforcement learning approach, in which the policies are approximated by
actor neural networks guided by a critic network. By including deterioration
dependence in the simulated environment, and by formulating the cost model at
the system level, DDMAC policies intrinsically consider the underlying
system-effects. This is demonstrated through numerical experiments conducted
for both a 9-out-of-10 system and a steel frame under fatigue deterioration.
Results demonstrate that DDMAC policies offer substantial benefits when
compared to state-of-the-art heuristic approaches. The inherent consideration
of system-effects by DDMAC strategies is also interpreted based on the learned
policies.",None,622
Elastic Monte Carlo Tree Search with State Abstraction for Strategy Game Playing,0.25627,"Strategy video games challenge AI agents with their combinatorial search
space caused by complex game elements. State abstraction is a popular technique
that reduces the state space complexity. However, current state abstraction
methods for games depend on domain knowledge, making their application to new
games expensive. State abstraction methods that require no domain knowledge are
studied extensively in the planning domain. However, no evidence shows they
scale well with the complexity of strategy games. In this paper, we propose
Elastic MCTS, an algorithm that uses state abstraction to play strategy games.
In Elastic MCTS, the nodes of the tree are clustered dynamically, first grouped
together progressively by state abstraction, and then separated when an
iteration threshold is reached. The elastic changes benefit from efficient
searching brought by state abstraction but avoid the negative influence of
using state abstraction for the whole search. To evaluate our method, we make
use of the general strategy games platform Stratega to generate scenarios of
varying complexity. Results show that Elastic MCTS outperforms MCTS baselines
with a large margin, while reducing the tree size by a factor of $10$. Code can
be found at: https://github.com/egg-west/Stratega",https://github.com/egg-west/Stratega,-1
Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning,0.801799,"Advances in artificial intelligence often stem from the development of new
environments that abstract real-world situations into a form where research can
be done conveniently. This paper contributes such an environment based on ideas
inspired by elementary Microeconomics. Agents learn to produce resources in a
spatially complex world, trade them with one another, and consume those that
they prefer. We show that the emergent production, consumption, and pricing
behaviors respond to environmental conditions in the directions predicted by
supply and demand shifts in Microeconomics. We also demonstrate settings where
the agents' emergent prices for goods vary over space, reflecting the local
abundance of goods. After the price disparities emerge, some agents then
discover a niche of transporting goods between regions with different
prevailing prices -- a profitable strategy because they can buy goods where
they are cheap and sell them where they are expensive. Finally, in a series of
ablation experiments, we investigate how choices in the environmental rewards,
bartering actions, agent architecture, and ability to consume tradable goods
can either aid or inhibit the emergence of this economic behavior. This work is
part of the environment development branch of a research program that aims to
build human-like artificial general intelligence through multi-agent
interactions in simulated societies. By exploring which environment features
are needed for the basic phenomena of elementary microeconomics to emerge
automatically from learning, we arrive at an environment that differs from
those studied in prior multi-agent reinforcement learning work along several
dimensions. For example, the model incorporates heterogeneous tastes and
physical abilities, and agents negotiate with one another as a grounded form of
communication.",https://github.com/deepmind/meltingpot,-1
Best-$k$ Search Algorithm for Neural Text Generation,0.0761536,"Modern natural language generation paradigms require a good decoding strategy
to obtain quality sequences out of the model. Beam search yields high-quality
but low diversity outputs; stochastic approaches suffer from high variance and
sometimes low quality, but the outputs tend to be more natural and creative. In
this work, we propose a deterministic search algorithm balancing both quality
and diversity. We first investigate the vanilla best-first search (BFS)
algorithm and then propose the Best-$k$ Search algorithm. Inspired by BFS, we
greedily expand the top $k$ nodes, instead of only the first node, to boost
efficiency and diversity. Upweighting recently discovered nodes accompanied by
heap pruning ensures the completeness of the search procedure. Experiments on
four NLG tasks, including question generation, commonsense generation, text
summarization, and translation, show that best-$k$ search yields more diverse
and natural outputs compared to strong baselines, while our approach maintains
high text quality. The proposed algorithm is parameter-free, lightweight,
efficient, and easy to use.",None,-1
Q-ViT: Fully Differentiable Quantization for Vision Transformer,0.808892,"In this paper, we propose a fully differentiable quantization method for
vision transformer (ViT) named as Q-ViT, in which both of the quantization
scales and bit-widths are learnable parameters. Specifically, based on our
observation that heads in ViT display different quantization robustness, we
leverage head-wise bit-width to squeeze the size of Q-ViT while preserving
performance. In addition, we propose a novel technique named switchable scale
to resolve the convergence problem in the joint training of quantization scales
and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to
3-bit without heavy performance drop. Moreover, we analyze the quantization
robustness of every architecture component of ViT and show that the Multi-head
Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key
aspects for ViT quantization. This study provides some insights for further
research about ViT quantization. Extensive experiments on different ViT models,
such as DeiT and Swin Transformer show the effectiveness of our quantization
method. In particular, our method outperforms the state-of-the-art uniform
quantization method by 1.5% on DeiT-Tiny.",https://github.com/zhexinli/Q-ViT-DeiT,-1
OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering,0.819046,"The information in tables can be an important complement to text, making
table-based question answering (QA) systems of great value. The intrinsic
complexity of handling tables often adds an extra burden to both model design
and data annotation. In this paper, we aim to develop a simple table-based QA
model with minimal annotation effort. Motivated by the fact that table-based QA
requires both alignment between questions and tables and the ability to perform
complicated reasoning over multiple table elements, we propose an omnivorous
pretraining approach that consumes both natural and synthetic data to endow
models with these respective abilities. Specifically, given freely available
tables, we leverage retrieval to pair them with relevant natural sentences for
mask-based pretraining, and synthesize NL questions by converting SQL sampled
from tables for pretraining with a QA loss. We perform extensive experiments in
both few-shot and full settings, and the results clearly demonstrate the
superiority of our model OmniTab, with the best multitasking approach achieving
an absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively,
also establishing a new state-of-the-art on WikiTableQuestions. Detailed
ablations and analyses reveal different characteristics of natural and
synthetic data, shedding light on future directions in omnivorous pretraining.
Code, pretraining data, and pretrained models are available at
https://github.com/jzbjyb/OmniTab.",https://github.com/jzbjyb/OmniTab,-1
SumREN: Summarizing Reported Speech about Events in News,0.558095,"A primary objective of news articles is to establish the factual record for
an event, frequently achieved by conveying both the details of the specified
event (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and
how people reacted to it (i.e., reported statements). However, existing work on
news summarization almost exclusively focuses on the event details. In this
work, we propose the novel task of summarizing the reactions of different
speakers, as expressed by their reported statements, to a given event. To this
end, we create a new multi-document summarization benchmark, SUMREN, comprising
745 summaries of reported statements from various public figures obtained from
633 news articles discussing 132 events. We propose an automatic silver
training data generation approach for our task, which helps smaller models like
BART achieve GPT-3 level performance on this task. Finally, we introduce a
pipeline-based framework for summarizing reported speech, which we empirically
show to generate summaries that are more abstractive and factual than baseline
query-focused summarization approaches.",https://github.com/amazon-science/SumREN,-1
A Proposal for Foley Sound Synthesis Challenge,0.838957,"""Foley"" refers to sound effects that are added to multimedia during
post-production to enhance its perceived acoustic properties, e.g., by
simulating the sounds of footsteps, ambient environmental sounds, or visible
objects on the screen. While foley is traditionally produced by foley artists,
there is increasing interest in automatic or machine-assisted techniques
building upon recent advances in sound synthesis and generative models. To
foster more participation in this growing research area, we propose a challenge
for automatic foley synthesis. Through case studies on successful previous
challenges in audio and machine learning, we set the goals of the proposed
challenge: rigorous, unified, and efficient evaluation of different foley
synthesis systems, with an overarching goal of drawing active participation
from the research community. We outline the details and design considerations
of a foley sound synthesis challenge, including task definition, dataset
requirements, and evaluation criteria.",None,-1
Class-Incremental Lifelong Learning in Multi-Label Classification,0.192851,"Existing class-incremental lifelong learning studies only the data is with
single-label, which limits its adaptation to multi-label data. This paper
studies Lifelong Multi-Label (LML) classification, which builds an online
class-incremental classifier in a sequential multi-label classification data
stream. Training on the data with Partial Labels in LML classification may
result in more serious Catastrophic Forgetting in old classes. To solve the
problem, the study proposes an Augmented Graph Convolutional Network (AGCN)
with a built Augmented Correlation Matrix (ACM) across sequential partial-label
tasks. The results of two benchmarks show that the method is effective for LML
classification and reducing forgetting.",https://github.com/Kaile-Du/AGCN,-1
Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series,0.0702589,"Deep Learning and transfer learning models are being used to generate time
series forecasts; however, there is scarce evidence about their performance
prediction that it is more evident for monthly time series. The purpose of this
paper is to compare Deep Learning models with transfer learning and without
transfer learning and other traditional methods used for monthly forecasts to
answer three questions about the suitability of Deep Learning and Transfer
Learning to generate predictions of time series. Time series of M4 and M3
competitions were used for the experiments. The results suggest that deep
learning models based on TCN, LSTM, and CNN with transfer learning tend to
surpass the performance prediction of other traditional methods. On the other
hand, TCN and LSTM, trained directly on the target time series, got similar or
better performance than traditional methods for some forecast horizons.",https://github.com/martin12cr/DatasetsDeepTransfer,-1
WiCV 2021: The Eighth Women In Computer Vision Workshop,0.864665,"In this paper, we present the details of Women in Computer Vision Workshop -
WiCV 2021, organized alongside the virtual CVPR 2021. It provides a voice to a
minority (female) group in the computer vision community and focuses on
increasing the visibility of these researchers, both in academia and industry.
WiCV believes that such an event can play an important role in lowering the
gender imbalance in the field of computer vision. WiCV is organized each year
where it provides a)~opportunity for collaboration between researchers from
minority groups, b)~mentorship to female junior researchers, c)~financial
support to presenters to overcome monetary burden and d)~large and diverse
choice of role models, who can serve as examples to younger researchers at the
beginning of their careers. In this paper, we present a report on the workshop
program, trends over the past years, a summary of statistics regarding
presenters, attendees, and sponsorship for the WiCV 2021 workshop.",None,-1
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning,0.798955,"Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.",https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt,-1
Lexical semantics enhanced neural word embeddings,0.133078,"Current breakthroughs in natural language processing have benefited
dramatically from neural language models, through which distributional
semantics can leverage neural data representations to facilitate downstream
applications. Since neural embeddings use context prediction on word
co-occurrences to yield dense vectors, they are inevitably prone to capture
more semantic association than semantic similarity. To improve vector space
models in deriving semantic similarity, we post-process neural word embeddings
through deep metric learning, through which we can inject lexical-semantic
relations, including syn/antonymy and hypo/hypernymy, into a distributional
space. We introduce hierarchy-fitting, a novel semantic specialization approach
to modelling semantic similarity nuances inherently stored in the IS-A
hierarchies. Hierarchy-fitting attains state-of-the-art results on the common-
and rare-word benchmark datasets for deriving semantic similarity from neural
word embeddings. It also incorporates an asymmetric distance function to
specialize hypernymy's directionality explicitly, through which it
significantly improves vanilla embeddings in multiple evaluation tasks of
detecting hypernymy and directionality without negative impacts on semantic
similarity judgement. The results demonstrate the efficacy of hierarchy-fitting
in specializing neural embeddings with semantic relations in late fusion,
potentially expanding its applicability to aggregating heterogeneous data and
various knowledge resources for learning multimodal semantic spaces.",None,-1
Cross-modal Contrastive Learning for Speech Translation,0.93309,"How can we learn unified representations for spoken utterances and their
written text? Learning similar representations for semantically similar speech
and text is important for speech translation. To this end, we propose ConST, a
cross-modal contrastive learning method for end-to-end speech-to-text
translation. We evaluate ConST and a variety of previous baselines on a popular
benchmark MuST-C. Experiments show that the proposed ConST consistently
outperforms the previous methods on, and achieves an average BLEU of 29.4. The
analysis further verifies that ConST indeed closes the representation gap of
different modalities -- its learned representation improves the accuracy of
cross-modal speech-text retrieval from 4% to 88%. Code and models are available
at https://github.com/ReneeYe/ConST.",https://github.com/ReneeYe/ConST,3502
Semantic Search for Large Scale Clinical Ontologies,0.0963721,"Finding concepts in large clinical ontologies can be challenging when queries
use different vocabularies. A search algorithm that overcomes this problem is
useful in applications such as concept normalisation and ontology matching,
where concepts can be referred to in different ways, using different synonyms.
In this paper, we present a deep learning based approach to build a semantic
search system for large clinical ontologies. We propose a Triplet-BERT model
and a method that generates training data directly from the ontologies. The
model is evaluated using five real benchmark data sets and the results show
that our approach achieves high results on both free text to concept and
concept to concept searching tasks, and outperforms all baseline methods.",https://github.com/dmis-lab/biobert,2314
Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts,0.150036,"Recent works suggest that transformer models are capable of multi-tasking on
diverse NLP tasks and adapting to new tasks efficiently. However, the potential
of these multi-task models may be limited as they use the same set of
parameters for all tasks. In contrast, humans tackle tasks in a more flexible
way, by making proper presumptions on what skills and knowledge are relevant
and executing only the necessary computations. Inspired by this, we propose to
use task-level mixture-of-expert models, which has a collection of transformer
layers (i.e., experts) and a router component that chooses from these experts
dynamically and flexibly. We find that these models help improve the average
performance gain (ARG) metric by 2.6% when adapting to unseen tasks in the
few-shot setting and by 5.6% in the zero-shot generalization setting. Further,
we show that the learned routing decisions partly rediscover human
categorization of NLP tasks -- certain experts are strongly associated with
extractive tasks, some with classification tasks, and some with tasks requiring
world knowledge.",https://github.com/INK-USC/CrossTaskMoE,378
AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation,0.665693,"Dense retrievers have made significant strides in text retrieval and
open-domain question answering, even though most achievements were made
possible only with large amounts of human supervision. In this work, we aim to
develop unsupervised methods by proposing two methods that create pseudo
query-document pairs and train dense retrieval models in an annotation-free and
scalable manner: query extraction and transferred query generation. The former
method produces pseudo queries by selecting salient spans from the original
document. The latter utilizes generation models trained for other NLP tasks
(e.g., summarization) to produce pseudo queries. Extensive experiments show
that models trained with the proposed augmentation methods can perform
comparably well (or better) to multiple strong baselines. Combining those
strategies leads to further improvements, achieving the state-of-the-art
performance of unsupervised dense retrieval on both BEIR and ODQA datasets.",None,-1
Modeling Complex Dialogue Mappings via Sentence Semantic Segmentation Guided Conditional Variational Auto-Encoder,0.0487326,"Complex dialogue mappings (CDM), including one-to-many and many-to-one
mappings, tend to make dialogue models generate incoherent or dull responses,
and modeling these mappings remains a huge challenge for neural dialogue
systems. To alleviate these problems, methods like introducing external
information, reconstructing the optimization function, and manipulating data
samples are proposed, while they primarily focus on avoiding training with CDM,
inevitably weakening the model's ability of understanding CDM in human
conversations and limiting further improvements in model performance. This
paper proposes a Sentence Semantic \textbf{Seg}mentation guided
\textbf{C}onditional \textbf{V}ariational \textbf{A}uto-\textbf{E}ncoder
(SegCVAE) method which can model and take advantages of the CDM data.
Specifically, to tackle the incoherent problem caused by one-to-many, SegCVAE
uses response-related prominent semantics to constrained the latent variable.
To mitigate the non-diverse problem brought by many-to-one, SegCVAE segments
multiple prominent semantics to enrich the latent variables. Three novel
components, Internal Separation, External Guidance, and Semantic Norms, are
proposed to achieve SegCVAE. On dialogue generation tasks, both the automatic
and human evaluation results show that SegCVAE achieves new state-of-the-art
performance.",None,-1
Towards Better Document-level Relation Extraction via Iterative Inference,0.609229,"Document-level relation extraction (RE) aims to extract the relations between
entities from the input document that usually containing many
difficultly-predicted entity pairs whose relations can only be predicted
through relational inference. Existing methods usually directly predict the
relations of all entity pairs of input document in a one-pass manner, ignoring
the fact that predictions of some entity pairs heavily depend on the predicted
results of other pairs. To deal with this issue, in this paper, we propose a
novel document-level RE model with iterative inference. Our model is mainly
composed of two modules: 1) a base module expected to provide preliminary
relation predictions on entity pairs; 2) an inference module introduced to
refine these preliminary predictions by iteratively dealing with
difficultly-predicted entity pairs depending on other pairs in an easy-to-hard
manner. Unlike previous methods which only consider feature information of
entity pairs, our inference module is equipped with two Extended Cross
Attention units, allowing it to exploit both feature information and previous
predictions of entity pairs during relational inference. Furthermore, we adopt
a two-stage strategy to train our model. At the first stage, we only train our
base module. During the second stage, we train the whole model, where
contrastive learning is introduced to enhance the training of inference module.
Experimental results on three commonly-used datasets show that our model
consistently outperforms other competitive baselines.",https://github.com/DeepLearnXMU/DocRE-II,-1
Data-driven Approach to Differentiating between Depression and Dementia from Noisy Speech and Language Data,0.412422,"A significant number of studies apply acoustic and linguistic characteristics
of human speech as prominent markers of dementia and depression. However,
studies on discriminating depression from dementia are rare. Co-morbid
depression is frequent in dementia and these clinical conditions share many
overlapping symptoms, but the ability to distinguish between depression and
dementia is essential as depression is often curable. In this work, we
investigate the ability of clustering approaches in distinguishing between
depression and dementia from human speech. We introduce a novel aggregated
dataset, which combines narrative speech data from multiple conditions, i.e.,
Alzheimer's disease, mild cognitive impairment, healthy control, and
depression. We compare linear and non-linear clustering approaches and show
that non-linear clustering techniques distinguish better between distinct
disease clusters. Our interpretability analysis shows that the main
differentiating symptoms between dementia and depression are acoustic
abnormality, repetitiveness (or circularity) of speech, word finding
difficulty, coherence impairment, and differences in lexical complexity and
richness.",https://github.com/vu-minh/mlteam-lime-for-tsne,-1
Pixel-Level Equalized Matching for Video Object Segmentation,0.0701702,"Feature similarity matching, which transfers the information of the reference
frame to the query frame, is a key component in semi-supervised video object
segmentation. If surjective matching is adopted, background distractors can
easily occur and degrade the performance. Bijective matching mechanisms try to
prevent this by restricting the amount of information being transferred to the
query frame, but have two limitations: 1) surjective matching cannot be fully
leveraged as it is transformed to bijective matching at test time; and 2)
test-time manual tuning is required for searching the optimal hyper-parameters.
To overcome these limitations while ensuring reliable information transfer, we
introduce an equalized matching mechanism. To prevent the reference frame
information from being overly referenced, the potential contribution to the
query frame is equalized by simply applying a softmax operation along with the
query. On public benchmark datasets, our proposed approach achieves a
comparable performance to state-of-the-art methods.",None,-1
Memory-free Online Change-point Detection: A Novel Neural Network Approach,0.747131,"Change-point detection (CPD), which detects abrupt changes in the data
distribution, is recognized as one of the most significant tasks in time series
analysis. Despite the extensive literature on offline CPD, unsupervised online
CPD still suffers from major challenges, including scalability, hyperparameter
tuning, and learning constraints. To mitigate some of these challenges, in this
paper, we propose a novel deep learning approach for unsupervised online CPD
from multi-dimensional time series, named Adaptive LSTM-Autoencoder
Change-Point Detection (ALACPD). ALACPD exploits an LSTM-autoencoder-based
neural network to perform unsupervised online CPD. It continuously adapts to
the incoming samples without keeping the previously received input, thus being
memory-free. We perform an extensive evaluation on several real-world time
series CPD benchmarks. We show that ALACPD, on average, ranks first among
state-of-the-art CPD algorithms in terms of quality of the time series
segmentation, and it is on par with the best performer in terms of the accuracy
of the estimated change-points. The implementation of ALACPD is available
online on Github\footnote{\url{https://github.com/zahraatashgahi/ALACPD}}.",https://github.com/zahraatashgahi/ALACPD,-1
Learning Across Domains and Devices: Style-Driven Source-Free Domain Adaptation in Clustered Federated Learning,0.649403,"Federated Learning (FL) has recently emerged as a possible way to tackle the
domain shift in real-world Semantic Segmentation (SS) without compromising the
private nature of the collected data. However, most of the existing works on FL
unrealistically assume labeled data in the remote clients. Here we propose a
novel task (FFREEDA) in which the clients' data is unlabeled and the server
accesses a source labeled dataset for pre-training only. To solve FFREEDA, we
propose LADD, which leverages the knowledge of the pre-trained model by
employing self-supervision with ad-hoc regularization techniques for local
training and introducing a novel federated clustered aggregation scheme based
on the clients' style. Our experiments show that our algorithm is able to
efficiently tackle the new task outperforming existing approaches. The code is
available at https://github.com/Erosinho13/LADD.",https://github.com/Erosinho13/LADD,-1
TaSPM: Targeted Sequential Pattern Mining,0.671764,"Sequential pattern mining (SPM) is an important technique of pattern mining,
which has many applications in reality. Although many efficient sequential
pattern mining algorithms have been proposed, there are few studies can focus
on target sequences. Targeted querying sequential patterns can not only reduce
the number of sequences generated by SPM, but also improve the efficiency of
users in performing pattern analysis. The current algorithms available on
targeted sequence querying are based on specific scenarios and cannot be
generalized to other applications. In this paper, we formulate the problem of
targeted sequential pattern mining and propose a generic framework namely
TaSPM, based on the fast CM-SPAM algorithm. What's more, to improve the
efficiency of TaSPM on large-scale datasets and multiple-items-based sequence
datasets, we propose several pruning strategies to reduce meaningless
operations in mining processes. Totally four pruning strategies are designed in
TaSPM, and hence it can terminate unnecessary pattern extensions quickly and
achieve better performance. Finally, we conduct extensive experiments on
different datasets to compare the existing SPM algorithms with TaSPM.
Experiments show that the novel targeted mining algorithm TaSPM can achieve
faster running time and less memory consumption.",None,-1
Universal Domain Adaptive Object Detector,0.539209,"Universal domain adaptive object detection (UniDAOD)is more challenging than
domain adaptive object detection (DAOD) since the label space of the source
domain may not be the same as that of the target and the scale of objects in
the universal scenarios can vary dramatically (i.e, category shift and scale
shift). To this end, we propose US-DAF, namely Universal Scale-Aware Domain
Adaptive Faster RCNN with Multi-Label Learning, to reduce the negative transfer
effect during training while maximizing transferability as well as
discriminability in both domains under a variety of scales. Specifically, our
method is implemented by two modules: 1) We facilitate the feature alignment of
common classes and suppress the interference of private classes by designing a
Filter Mechanism module to overcome the negative transfer caused by category
shift. 2) We fill the blank of scale-aware adaptation in object detection by
introducing a new Multi-Label Scale-Aware Adapter to perform individual
alignment between the corresponding scale for two domains. Experiments show
that US-DAF achieves state-of-the-art results on three scenarios (i.e,
Open-Set, Partial-Set, and Closed-Set) and yields 7.1% and 5.9% relative
improvement on benchmark datasets Clipart1k and Watercolor in particular.",None,-1
PHEMEPlus: Enriching Social Media Rumour Verification with External Evidence,0.432169,"Work on social media rumour verification utilises signals from posts, their
propagation and users involved. Other lines of work target identifying and
fact-checking claims based on information from Wikipedia, or trustworthy news
articles without considering social media context. However works combining the
information from social media with external evidence from the wider web are
lacking. To facilitate research in this direction, we release a novel dataset,
PHEMEPlus, an extension of the PHEME benchmark, which contains social media
conversations as well as relevant external evidence for each rumour. We
demonstrate the effectiveness of incorporating such evidence in improving
rumour verification models. Additionally, as part of the evidence collection,
we evaluate various ways of query formulation to identify the most effective
method.",https://github.com/JohnNLP/PhemePlus,-1
A Web Application for Experimenting and Validating Remote Measurement of Vital Signs,0.490844,"With a surge in online medical advising remote monitoring of patient vitals
is required. This can be facilitated with the Remote Photoplethysmography
(rPPG) techniques that compute vital signs from facial videos. It involves
processing video frames to obtain skin pixels, extracting the cardiac data from
it and applying signal processing filters to extract the Blood Volume Pulse
(BVP) signal. Different algorithms are applied to the BVP signal to estimate
the various vital signs. We implemented a web application framework to measure
a person's Heart Rate (HR), Heart Rate Variability (HRV), Oxygen Saturation
(SpO2), Respiration Rate (RR), Blood Pressure (BP), and stress from the face
video. The rPPG technique is highly sensitive to illumination and motion
variation. The web application guides the users to reduce the noise due to
these variations and thereby yield a cleaner BVP signal. The accuracy and
robustness of the framework was validated with the help of volunteers.",None,-1
From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering,0.756422,"Multi-hop question answering (QA) is a challenging task requiring QA systems
to perform complex reasoning over multiple documents and provide supporting
facts together with the exact answer. Existing works tend to utilize
graph-based reasoning and question decomposition to obtain the reasoning chain,
which inevitably introduces additional complexity and cumulative error to the
system. To address the above issue, we propose a simple yet effective novel
framework, From Easy to Hard (FE2H), to remove distracting information and
obtain better contextual representations for the multi-hop QA task. Inspired by
the iterative document selection process and the progressive learning custom of
humans, FE2H divides both the document selector and reader into two stages
following an easy-to-hard manner. Specifically, we first select the document
most relevant to the question and then utilize the question together with this
document to select other pertinent documents. As for the QA phase, our reader
is first trained on a single-hop QA dataset and then transferred into the
multi-hop QA task. We comprehensively evaluate our model on the popular
multi-hop QA benchmark HotpotQA. Experimental results demonstrate that our
method ourperforms all other methods in the leaderboard of HotpotQA (distractor
setting).",None,-1
Connection-minimal Abduction in EL via Translation to FOL -- Technical Report,0.909282,"Abduction in description logics finds extensions of a knowledge base to make
it entail an observation. As such, it can be used to explain why the
observation does not follow, to repair incomplete knowledge bases, and to
provide possible explanations for unexpected observations. We consider TBox
abduction in the lightweight description logic EL, where the observation is a
concept inclusion and the background knowledge is a TBox, i.e., a set of
concept inclusions. To avoid useless answers, such problems usually come with
further restrictions on the solution space and/or minimality criteria that help
sort the chaff from the grain. We argue that existing minimality notions are
insufficient, and introduce connection minimality. This criterion follows
Occam's razor by rejecting hypotheses that use concept inclusions unrelated to
the problem at hand. We show how to compute a special class of
connection-minimal hypotheses in a sound and complete way. Our technique is
based on a translation to first-order logic, and constructs hypotheses based on
prime implicates. We evaluate a prototype implementation of our approach on
ontologies from the medical domain.",None,-1
Music-driven Dance Regeneration with Controllable Key Pose Constraints,0.167237,"In this paper, we propose a novel framework for music-driven dance motion
synthesis with controllable key pose constraint. In contrast to methods that
generate dance motion sequences only based on music without any other
controllable conditions, this work targets on synthesizing high-quality dance
motion driven by music as well as customized poses performed by users. Our
model involves two single-modal transformer encoders for music and motion
representations and a cross-modal transformer decoder for dance motions
generation. The cross-modal transformer decoder achieves the capability of
synthesizing smooth dance motion sequences, which keeps a consistency with key
poses at corresponding positions, by introducing the local neighbor position
embedding. Such mechanism makes the decoder more sensitive to key poses and the
corresponding positions. Our dance synthesis model achieves satisfactory
performance both on quantitative and qualitative evaluations with extensive
experiments, which demonstrates the effectiveness of our proposed method.",None,-1
Flow-Adapter Architecture for Unsupervised Machine Translation,0.453305,"In this work, we propose a flow-adapter architecture for unsupervised NMT. It
leverages normalizing flows to explicitly model the distributions of
sentence-level latent representations, which are subsequently used in
conjunction with the attention mechanism for the translation task. The primary
novelties of our model are: (a) capturing language-specific sentence
representations separately for each language using normalizing flows and (b)
using a simple transformation of these latent representations for translating
from one language to another. This architecture allows for unsupervised
training of each language independently. While there is prior work on latent
variables for supervised MT, to the best of our knowledge, this is the first
work that uses latent variables and normalizing flows for unsupervised MT. We
obtain competitive results on several unsupervised MT benchmarks.",https://github.com/facebookresearch/XLM/blob/main/get-data-nmt.sh,-1
Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach for Rotating Machinery exploiting Augmented Synthetic Data,0.557199,"Artificial Intelligence (AI) is one of the approaches that has been proposed
to analyze the collected data (e.g., vibration signals) providing a diagnosis
of the asset's operating condition. It is known that models trained with
labeled data (supervised) achieve excellent results, but two main problems make
their application in production processes difficult: (i) impossibility or long
time to obtain a sample of all operational conditions (since faults seldom
happen) and (ii) high cost of experts to label all acquired data. Another
limitating factor for the applicability of AI approaches in this context is the
lack of interpretability of the models (black-boxes), which reduces the
confidence of the diagnosis and trust/adoption from users. To overcome these
problems, a new generic and interpretable approach for classifying faults in
rotating machinery based on transfer learning from augmented synthetic data to
real rotating machinery is here proposed, namelly FaultD-XAI (Fault Diagnosis
using eXplainable AI). To provide scalability using transfer learning,
synthetic vibration signals are created mimicking the characteristic behavior
of failures in operation. The application of Gradient-weighted Class Activation
Mapping (Grad-CAM) with 1D Convolutional Neural Network (1D CNN) allows the
interpretation of results, supporting the user in decision making and
increasing diagnostic confidence. The proposed approach not only obtained
promising diagnostic performance, but was also able to learn characteristics
used by experts to identify conditions in a source domain and apply them in
another target domain. The experimental results suggest a promising approach on
exploiting transfer learning, synthetic data and explainable artificial
intelligence for fault diagnosis. Lastly, to guarantee reproducibility and
foster research in the field, the developed dataset is made publicly available.",None,-1
Learning to Prove Trigonometric Identities,0.18601,"Automatic theorem proving with deep learning methods has attracted attentions
recently. In this paper, we construct an automatic proof system for
trigonometric identities. We define the normalized form of trigonometric
identities, design a set of rules for the proof and put forward a method which
can generate theoretically infinite trigonometric identities. Our goal is not
only to complete the proof, but to complete the proof in as few steps as
possible. For this reason, we design a model to learn proof data generated by
random BFS (rBFS), and it is proved theoretically and experimentally that the
model can outperform rBFS after a simple imitation learning. After further
improvement through reinforcement learning, we get AutoTrig, which can give
proof steps for identities in almost as short steps as BFS (theoretically
shortest method), with a time cost of only one-thousandth. In addition,
AutoTrig also beats Sympy, Matlab and human in the synthetic dataset, and
performs well in many generalization tasks.",None,-1
Exploration via Elliptical Episodic Bonuses,0.770185,"In recent years, a number of reinforcement learning (RL) methods have been
proposed to explore complex environments which differ across episodes. In this
work, we show that the effectiveness of these methods critically relies on a
count-based episodic term in their exploration bonus. As a result, despite
their success in relatively simple, noise-free settings, these methods fall
short in more realistic scenarios where the state space is vast and prone to
noise. To address this limitation, we introduce Exploration via Elliptical
Episodic Bonuses (E3B), a new method which extends count-based episodic bonuses
to continuous state spaces and encourages an agent to explore states that are
diverse under a learned embedding within each episode. The embedding is learned
using an inverse dynamics model in order to capture controllable aspects of the
environment. Our method sets a new state-of-the-art across 16 challenging tasks
from the MiniHack suite, without requiring task-specific inductive biases. E3B
also matches existing methods on sparse reward, pixel-based VizDoom
environments, and outperforms existing methods in reward-free exploration on
Habitat, demonstrating that it can scale to high-dimensional pixel-based
observations and realistic environments.",https://github.com/facebookresearch/e3b,-1
Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories,0.404387,"Obtaining photorealistic reconstructions of objects from sparse views is
inherently ambiguous and can only be achieved by learning suitable
reconstruction priors. Earlier works on sparse rigid object reconstruction
successfully learned such priors from large datasets such as CO3D. In this
paper, we extend this approach to dynamic objects. We use cats and dogs as a
representative example and introduce Common Pets in 3D (CoP3D), a collection of
crowd-sourced videos showing around 4,200 distinct pets. CoP3D is one of the
first large-scale datasets for benchmarking non-rigid 3D reconstruction ""in the
wild"". We also propose Tracker-NeRF, a method for learning 4D reconstruction
from our dataset. At test time, given a small number of video frames of an
unseen object, Tracker-NeRF predicts the trajectories of its 3D points and
generates new views, interpolating viewpoint and time. Results on CoP3D reveal
significantly better non-rigid new-view synthesis performance than existing
baselines.",None,-1
Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences,0.635942,"Generating complex behaviors that satisfy the preferences of non-expert users
is a crucial requirement for AI agents. Interactive reward learning from
trajectory comparisons (a.k.a. RLHF) is one way to allow non-expert users to
convey complex objectives by expressing preferences over short clips of agent
behaviors. Even though this parametric method can encode complex tacit
knowledge present in the underlying tasks, it implicitly assumes that the human
is unable to provide richer feedback than binary preference labels, leading to
intolerably high feedback complexity and poor user experience. While providing
a detailed symbolic closed-form specification of the objectives might be
tempting, it is not always feasible even for an expert user. However, in most
cases, humans are aware of how the agent should change its behavior along
meaningful axes to fulfill their underlying purpose, even if they are not able
to fully specify task objectives symbolically. Using this as motivation, we
introduce the notion of Relative Behavioral Attributes, which allows the users
to tweak the agent behavior through symbolic concepts (e.g., increasing the
softness or speed of agents' movement). We propose two practical methods that
can learn to model any kind of behavioral attributes from ordered behavior
clips. We demonstrate the effectiveness of our methods on four tasks with nine
different behavioral attributes, showing that once the attributes are learned,
end users can produce desirable agent behaviors relatively effortlessly, by
providing feedback just around ten times. This is over an order of magnitude
less than that required by the popular learning-from-human-preferences
baselines. The supplementary video and source code are available at:
https://guansuns.github.io/pages/rba.",None,-1
Data Feedback Loops: Model-driven Amplification of Dataset Biases,0.649881,"Datasets scraped from the internet have been critical to the successes of
large-scale machine learning. Yet, this very success puts the utility of future
internet-derived datasets at potential risk, as model outputs begin to replace
human annotations as a source of supervision.
  In this work, we first formalize a system where interactions with one model
are recorded as history and scraped as training data in the future. We then
analyze its stability over time by tracking changes to a test-time bias
statistic (e.g. gender bias of model predictions). We find that the degree of
bias amplification is closely linked to whether the model's outputs behave like
samples from the training distribution, a behavior which we characterize and
define as consistent calibration. Experiments in three conditional prediction
scenarios - image classification, visual role-labeling, and language generation
- demonstrate that models that exhibit a sampling-like behavior are more
calibrated and thus more stable. Based on this insight, we propose an
intervention to help calibrate and stabilize unstable feedback systems.
  Code is available at https://github.com/rtaori/data_feedback.",https://github.com/rtaori/data_feedback,-1
Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation,0.692525,"We introduce Bi-SimCut: a simple but effective training strategy to boost
neural machine translation (NMT) performance. It consists of two procedures:
bidirectional pretraining and unidirectional finetuning. Both procedures
utilize SimCut, a simple regularization method that forces the consistency
between the output distributions of the original and the cutoff sentence pairs.
Without leveraging extra dataset via back-translation or integrating
large-scale pretrained model, Bi-SimCut achieves strong translation performance
across five translation benchmarks (data sizes range from 160K to 20.2M): BLEU
scores of 31.16 for en -> de and 38.37 for de -> en on the IWSLT14 dataset,
30.78 for en -> de and 35.15 for de -> en on the WMT14 dataset, and 27.17 for
zh -> en on the WMT17 dataset. SimCut is not a new method, but a version of
Cutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be
considered as a perturbation-based method. Given the universality and
simplicity of SimCut and Bi-SimCut, we believe they can serve as strong
baselines for future NMT research.",https://github.com/gpengzhi/Bi-SimCut,-1
Semiconductor Defect Detection by Hybrid Classical-Quantum Deep Learning,0.934265,"With the rapid development of artificial intelligence and autonomous driving
technology, the demand for semiconductors is projected to rise substantially.
However, the massive expansion of semiconductor manufacturing and the
development of new technology will bring many defect wafers. If these defect
wafers have not been correctly inspected, the ineffective semiconductor
processing on these defect wafers will cause additional impact to our
environment, such as excessive carbon dioxide emission and energy consumption.
In this paper, we utilize the information processing advantages of quantum
computing to promote the defect learning defect review (DLDR). We propose a
classical-quantum hybrid algorithm for deep learning on near-term quantum
processors. By tuning parameters implemented on it, quantum circuit driven by
our framework learns a given DLDR task, include of wafer defect map
classification, defect pattern classification, and hotspot detection. In
addition, we explore parametrized quantum circuits with different
expressibility and entangling capacities. These results can be used to build a
future roadmap to develop circuit-based quantum deep learning for semiconductor
defect detection.",None,10320
Color Invariant Skin Segmentation,0.114144,"This paper addresses the problem of automatically detecting human skin in
images without reliance on color information. A primary motivation of the work
has been to achieve results that are consistent across the full range of skin
tones, even while using a training dataset that is significantly biased toward
lighter skin tones. Previous skin-detection methods have used color cues almost
exclusively, and we present a new approach that performs well in the absence of
such information. A key aspect of the work is dataset repair through
augmentation that is applied strategically during training, with the goal of
color invariant feature learning to enhance generalization. We have
demonstrated the concept using two architectures, and experimental results show
improvements in both precision and recall for most Fitzpatrick skin tones in
the benchmark ECU dataset. We further tested the system with the RFW dataset to
show that the proposed method performs much more consistently across different
ethnicities, thereby reducing the chance of bias based on skin color. To
demonstrate the effectiveness of our work, extensive experiments were performed
on grayscale images as well as images obtained under unconstrained illumination
and with artificial filters. Source code:
https://github.com/HanXuMartin/Color-Invariant-Skin-Segmentation",https://github.com/HanXuMartin/Color-Invariant-Skin-Segmentation1,-1
Multimodal Transformer for Nursing Activity Recognition,0.805429,"In an aging population, elderly patient safety is a primary concern at
hospitals and nursing homes, which demands for increased nurse care. By
performing nurse activity recognition, we can not only make sure that all
patients get an equal desired care, but it can also free nurses from manual
documentation of activities they perform, leading to a fair and safe place of
care for the elderly. In this work, we present a multimodal transformer-based
network, which extracts features from skeletal joints and acceleration data,
and fuses them to perform nurse activity recognition. Our method achieves
state-of-the-art performance of 81.8% accuracy on the benchmark dataset
available for nurse activity recognition from the Nurse Care Activity
Recognition Challenge. We perform ablation studies to show that our fusion
model is better than single modality transformer variants (using only
acceleration or skeleton joints data). Our solution also outperforms
state-of-the-art ST-GCN, GRU and other classical hand-crafted-feature-based
classifier solutions by a margin of 1.6%, on the NCRC dataset. Code is
available at \url{https://github.com/Momilijaz96/MMT_for_NCRC}.",https://github.com/Momilijaz96/MMT_for_NCRC,-1
Monitoring Diversity of AI Conferences: Lessons Learnt and Future Challenges in the DivinAI Project,0.0550487,"DivinAI is an open and collaborative initiative promoted by the European
Commission's Joint Research Centre to measure and monitor diversity indicators
related to AI conferences, with special focus on gender balance, geographical
representation, and presence of academia vs companies. This paper summarizes
the main achievements and lessons learnt during the first year of life of the
DivinAI project, and proposes a set of recommendations for its further
development and maintenance by the AI community.",None,-1
Introducing the Welsh Text Summarisation Dataset and Baseline Systems,0.328176,"Welsh is an official language in Wales and is spoken by an estimated 884,300
people (29.2% of the population of Wales). Despite this status and estimated
increase in speaker numbers since the last (2011) census, Welsh remains a
minority language undergoing revitalization and promotion by Welsh Government
and relevant stakeholders. As part of the effort to increase the availability
of Welsh digital technology, this paper introduces the first Welsh
summarisation dataset, which we provide freely for research purposes to help
advance the work on Welsh text summarization. The dataset was created by Welsh
speakers by manually summarising Welsh Wikipedia articles. In addition, the
paper discusses the implementation and evaluation of different summarisation
systems for Welsh. The summarization systems and results will serve as
benchmarks for the development of summarises in other minority language
contexts.",https://github.com/UCREL/welsh-summarization-dataset,-1
BadPrompt: Backdoor Attacks on Continuous Prompts,0.861906,"The prompt-based learning paradigm has gained much research attention
recently. It has achieved state-of-the-art performance on several NLP tasks,
especially in the few-shot scenarios. While steering the downstream tasks, few
works have been reported to investigate the security problems of the
prompt-based models. In this paper, we conduct the first study on the
vulnerability of the continuous prompt learning algorithm to backdoor attacks.
We observe that the few-shot scenarios have posed a great challenge to backdoor
attacks on the prompt-based models, limiting the usability of existing NLP
backdoor methods. To address this challenge, we propose BadPrompt, a
lightweight and task-adaptive algorithm, to backdoor attack continuous prompts.
Specially, BadPrompt first generates candidate triggers which are indicative
for predicting the targeted label and dissimilar to the samples of the
non-targeted labels. Then, it automatically selects the most effective and
invisible trigger for each sample with an adaptive trigger optimization
algorithm. We evaluate the performance of BadPrompt on five datasets and two
continuous prompt models. The results exhibit the abilities of BadPrompt to
effectively attack continuous prompts while maintaining high performance on the
clean test sets, outperforming the baseline models by a large margin. The
source code of BadPrompt is publicly available at
https://github.com/papersPapers/BadPrompt.",https://github.com/papersPapers/BadPrompt,-1
Stochastic Market Games,0.109485,"Some of the most relevant future applications of multi-agent systems like
autonomous driving or factories as a service display mixed-motive scenarios,
where agents might have conflicting goals. In these settings agents are likely
to learn undesirable outcomes in terms of cooperation under independent
learning, such as overly greedy behavior. Motivated from real world societies,
in this work we propose to utilize market forces to provide incentives for
agents to become cooperative. As demonstrated in an iterated version of the
Prisoner's Dilemma, the proposed market formulation can change the dynamics of
the game to consistently learn cooperative policies. Further we evaluate our
approach in spatially and temporally extended settings for varying numbers of
agents. We empirically find that the presence of markets can improve both the
overall result and agent individual returns via their trading activities.",None,-1
Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation,0.832176,"Sign language gloss translation aims to translate the sign glosses into
spoken language texts, which is challenging due to the scarcity of labeled
gloss-text parallel data. Back translation (BT), which generates
pseudo-parallel data by translating in-domain spoken language texts into sign
glosses, has been applied to alleviate the data scarcity problem. However, the
lack of large-scale high-quality domain spoken language text data limits the
effect of BT. In this paper, to overcome the limitation, we propose a Prompt
based domain text Generation (PGEN) approach to produce the large-scale
in-domain spoken language text data. Specifically, PGEN randomly concatenates
sentences from the original in-domain spoken language text data as prompts to
induce a pre-trained language model (i.e., GPT-2) to generate spoken language
texts in a similar style. Experimental results on three benchmarks of sign
language gloss translation in varied languages demonstrate that BT with spoken
language texts generated by PGEN significantly outperforms the compared
methods. In addition, as the scale of spoken language texts generated by PGEN
increases, the BT technique can achieve further improvements, demonstrating the
effectiveness of our approach. We release the code and data for facilitating
future research in this field.",https://github.com/Atrewin/PGen,-1
Overlap-guided Gaussian Mixture Models for Point Cloud Registration,0.436437,"Probabilistic 3D point cloud registration methods have shown competitive
performance in overcoming noise, outliers, and density variations. However,
registering point cloud pairs in the case of partial overlap is still a
challenge. This paper proposes a novel overlap-guided probabilistic
registration approach that computes the optimal transformation from matched
Gaussian Mixture Model (GMM) parameters. We reformulate the registration
problem as the problem of aligning two Gaussian mixtures such that a
statistical discrepancy measure between the two corresponding mixtures is
minimized. We introduce a Transformer-based detection module to detect
overlapping regions, and represent the input point clouds using GMMs by guiding
their alignment through overlap scores computed by this detection module.
Experiments show that our method achieves superior registration accuracy and
efficiency than state-of-the-art methods when handling point clouds with
partial overlap and different densities on synthetic and real-world datasets.
https://github.com/gfmei/ogmm",https://github.com/gfmei/ogmm,-1
MTet: Multi-domain Translation for English and Vietnamese,0.253422,"We introduce MTet, the largest publicly available parallel corpus for
English-Vietnamese translation. MTet consists of 4.2M high-quality training
sentence pairs and a multi-domain test set refined by the Vietnamese research
community. Combining with previous works on English-Vietnamese translation, we
grow the existing parallel dataset to 6.2M sentence pairs. We also release the
first pretrained model EnViT5 for English and Vietnamese languages. Combining
both resources, our model significantly outperforms previous state-of-the-art
results by up to 2 points in translation BLEU score, while being 1.6 times
smaller.",https://github.com/stefan-it/nmt-en-vi,-1
Contrastive Bayesian Analysis for Deep Metric Learning,0.42565,"Recent methods for deep metric learning have been focusing on designing
different contrastive loss functions between positive and negative pairs of
samples so that the learned feature embedding is able to pull positive samples
of the same class closer and push negative samples from different classes away
from each other. In this work, we recognize that there is a significant
semantic gap between features at the intermediate feature layer and class
labels at the final output layer. To bridge this gap, we develop a contrastive
Bayesian analysis to characterize and model the posterior probabilities of
image labels conditioned by their features similarity in a contrastive learning
setting. This contrastive Bayesian analysis leads to a new loss function for
deep metric learning. To improve the generalization capability of the proposed
method onto new classes, we further extend the contrastive Bayesian loss with a
metric variance constraint. Our experimental results and ablation studies
demonstrate that the proposed contrastive Bayesian metric learning method
significantly improves the performance of deep metric learning in both
supervised and pseudo-supervised scenarios, outperforming existing methods by a
large margin.",https://github.com/kanshichao/CBML,-1
Nonparametric Masked Language Modeling,0.768225,"Existing language models (LMs) predict tokens with a softmax over a finite
vocabulary, which can make it difficult to predict rare tokens or phrases. We
introduce NPM, the first nonparametric masked language model that replaces this
softmax with a nonparametric distribution over every phrase in a reference
corpus. NPM fills in the [MASK] solely from retrieving a token from a text
corpus. We show that NPM can be efficiently trained with a contrastive
objective and an in-batch approximation to full corpus retrieval. Zero-shot
evaluation on 16 tasks including classification, fact probing and question
answering demonstrates that NPM outperforms significantly larger parametric
models, either with or without a retrieve-and-generate approach. It is
particularly better at dealing with rare patterns (word senses or facts) and
predicting rare or nearly unseen words (e.g., non-Latin script). We release the
model and code at github.com/facebookresearch/NPM.",https://github.com/facebookresearch/NPM,-1
Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework,0.532069,"Simultaneous machine translation (SiMT) starts translating while receiving
the streaming source inputs, and hence the source sentence is always incomplete
during translating. Different from the full-sentence MT using the conventional
seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,
which forces each target word to only align with a partial source prefix to
adapt to the incomplete source in streaming inputs. However, the source words
in the front positions are always illusoryly considered more important since
they appear in more prefixes, resulting in position bias, which makes the model
pay more attention on the front source positions in testing. In this paper, we
first analyze the phenomenon of position bias in SiMT, and develop a
Length-Aware Framework to reduce the position bias by bridging the structural
gap between SiMT and full-sentence MT. Specifically, given the streaming
inputs, we first predict the full-sentence length and then fill the future
source position with positional encoding, thereby turning the streaming inputs
into a pseudo full-sentence. The proposed framework can be integrated into most
existing SiMT methods to further improve performance. Experiments on two
representative SiMT methods, including the state-of-the-art adaptive policy,
show that our method successfully reduces the position bias and thereby
achieves better SiMT performance.",https://github.com/pytorch/fairseq/tree/master/examples/simultaneous_translation,-1
Simple and Effective Knowledge-Driven Query Expansion for QA-Based Product Attribute Extraction,0.338282,"A key challenge in attribute value extraction (AVE) from e-commerce sites is
how to handle a large number of attributes for diverse products. Although this
challenge is partially addressed by a question answering (QA) approach which
finds a value in product data for a given query (attribute), it does not work
effectively for rare and ambiguous queries. We thus propose simple
knowledge-driven query expansion based on possible answers (values) of a query
(attribute) for QA-based AVE. We retrieve values of a query (attribute) from
the training data to expand the query. We train a model with two tricks,
knowledge dropout and knowledge token mixing, which mimic the imperfection of
the value knowledge in testing. Experimental results on our cleaned version of
AliExpress dataset show that our method improves the performance of AVE (+6.08
macro F1), especially for rare and ambiguous attributes (+7.82 and +6.86 macro
F1, respectively).",https://github.com/lanmanok/ACL19_Scaling_Up_Open_Tagging,-1
Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing,0.790389,"Entity typing aims at predicting one or more words that describe the type(s)
of a specific mention in a sentence. Due to shortcuts from surface patterns to
annotated entity labels and biased training, existing entity typing models are
subject to the problem of spurious correlations. To comprehensively investigate
the faithfulness and reliability of entity typing methods, we first
systematically define distinct kinds of model biases that are reflected mainly
from spurious correlations. Particularly, we identify six types of existing
model biases, including mention-context bias, lexical overlapping bias, named
entity bias, pronoun bias, dependency bias, and overgeneralization bias. To
mitigate model biases, we then introduce a counterfactual data augmentation
method. By augmenting the original training set with their debiased
counterparts, models are forced to fully comprehend sentences and discover the
fundamental cues for entity typing, rather than relying on spurious
correlations for shortcuts. Experimental results on the UFET dataset show our
counterfactual data augmentation approach helps improve generalization of
different entity typing models with consistently better performance on both the
original and debiased test sets.",https://github.com/luka-group/DiagnoseET,-1
BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets,0.737404,"The free flow of information has been accelerated by the rapid development of
social media technology. There has been a significant social and psychological
impact on the population due to the outbreak of Coronavirus disease (COVID-19).
The COVID-19 pandemic is one of the current events being discussed on social
media platforms. In order to safeguard societies from this pandemic, studying
people's emotions on social media is crucial. As a result of their particular
characteristics, sentiment analysis of texts like tweets remains challenging.
Sentiment analysis is a powerful text analysis tool. It automatically detects
and analyzes opinions and emotions from unstructured data. Texts from a wide
range of sources are examined by a sentiment analysis tool, which extracts
meaning from them, including emails, surveys, reviews, social media posts, and
web articles. To evaluate sentiments, natural language processing (NLP) and
machine learning techniques are used, which assign weights to entities, topics,
themes, and categories in sentences or phrases. Machine learning tools learn
how to detect sentiment without human intervention by examining examples of
emotions in text. In a pandemic situation, analyzing social media texts to
uncover sentimental trends can be very helpful in gaining a better
understanding of society's needs and predicting future trends. We intend to
study society's perception of the COVID-19 pandemic through social media using
state-of-the-art BERT and Deep CNN models. The superiority of BERT models over
other deep models in sentiment analysis is evident and can be concluded from
the comparison of the various research studies mentioned in this article.",None,-1
A Simple Strategy to Provable Invariance via Orbit Mapping,0.208256,"Many applications require robustness, or ideally invariance, of neural
networks to certain transformations of input data. Most commonly, this
requirement is addressed by training data augmentation, using adversarial
training, or defining network architectures that include the desired invariance
by design. In this work, we propose a method to make network architectures
provably invariant with respect to group actions by choosing one element from a
(possibly continuous) orbit based on a fixed criterion. In a nutshell, we
intend to 'undo' any possible transformation before feeding the data into the
actual network. Further, we empirically analyze the properties of different
approaches which incorporate invariance via training or architecture, and
demonstrate the advantages of our method in terms of robustness and
computational efficiency. In particular, we investigate the robustness with
respect to rotations of images (which can hold up to discretization artifacts)
as well as the provable orientation and scaling invariance of 3D point cloud
classification.",https://github.com/QUVA-Lab/e2cnn_experiments,3895
Cross-modal Learning for Image-Guided Point Cloud Shape Completion,0.487635,"In this paper we explore the recent topic of point cloud completion, guided
by an auxiliary image. We show how it is possible to effectively combine the
information from the two modalities in a localized latent space, thus avoiding
the need for complex point cloud reconstruction methods from single views used
by the state-of-the-art. We also investigate a novel weakly-supervised setting
where the auxiliary image provides a supervisory signal to the training process
by using a differentiable renderer on the completed point cloud to measure
fidelity in the image space. Experiments show significant improvements over
state-of-the-art supervised methods for both unimodal and multimodal
completion. We also show the effectiveness of the weakly-supervised approach
which outperforms a number of supervised methods and is competitive with the
latest supervised models only exploiting point cloud information.",https://github.com/diegovalsesia/XMFnet,-1
On the Road to Online Adaptation for Semantic Image Segmentation,0.936674,"We propose a new problem formulation and a corresponding evaluation framework
to advance research on unsupervised domain adaptation for semantic image
segmentation. The overall goal is fostering the development of adaptive
learning systems that will continuously learn, without supervision, in
ever-changing environments. Typical protocols that study adaptation algorithms
for segmentation models are limited to few domains, adaptation happens offline,
and human intervention is generally required, at least to annotate data for
hyper-parameter tuning. We argue that such constraints are incompatible with
algorithms that can continuously adapt to different real-world situations. To
address this, we propose a protocol where models need to learn online, from
sequences of temporally correlated images, requiring continuous, frame-by-frame
adaptation. We accompany this new protocol with a variety of baselines to
tackle the proposed formulation, as well as an extensive analysis of their
behaviors, which can serve as a starting point for future research.",https://github.com/naver/oasis,-1
Gym-saturation: an OpenAI Gym environment for saturation provers,0.438439,"`gym-saturation` is an OpenAI Gym environment for reinforcement learning (RL)
agents capable of proving theorems. Currently, only theorems written in a
formal language of the Thousands of Problems for Theorem Provers (TPTP) library
in clausal normal form (CNF) are supported. `gym-saturation` implements the
'given clause' algorithm (similar to the one used in Vampire and E Prover).
Being written in Python, `gym-saturation` was inspired by PyRes. In contrast to
the monolithic architecture of a typical Automated Theorem Prover (ATP),
`gym-saturation` gives different agents opportunities to select clauses
themselves and train from their experience. Combined with a particular agent,
`gym-saturation` can work as an ATP. Even with a non trained agent based on
heuristics, `gym-saturation` can find refutations for 688 (of 8257) CNF
problems from TPTP v7.5.0.",None,20
CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations,0.0322677,"Although reinforcement learning has found widespread use in dense reward
settings, training autonomous agents with sparse rewards remains challenging.
To address this difficulty, prior work has shown promising results when using
not only task-specific demonstrations but also task-agnostic albeit somewhat
related demonstrations. In most cases, the available demonstrations are
distilled into an implicit prior, commonly represented via a single deep net.
Explicit priors in the form of a database that can be queried have also been
shown to lead to encouraging results. To better benefit from available
demonstrations, we develop a method to Combine Explicit and Implicit Priors
(CEIP). CEIP exploits multiple implicit priors in the form of normalizing flows
in parallel to form a single complex prior. Moreover, CEIP uses an effective
explicit retrieval and push-forward mechanism to condition the implicit priors.
In three challenging environments, we find the proposed CEIP method to improve
upon sophisticated state-of-the-art techniques.",https://github.com/289371298/CEIP,-1
A general-purpose material property data extraction pipeline from large polymer corpora using Natural Language Processing,0.842864,"The ever-increasing number of materials science articles makes it hard to
infer chemistry-structure-property relations from published literature. We used
natural language processing (NLP) methods to automatically extract material
property data from the abstracts of polymer literature. As a component of our
pipeline, we trained MaterialsBERT, a language model, using 2.4 million
materials science abstracts, which outperforms other baseline models in three
out of five named entity recognition datasets when used as the encoder for
text. Using this pipeline, we obtained ~300,000 material property records from
~130,000 abstracts in 60 hours. The extracted data was analyzed for a diverse
range of applications such as fuel cells, supercapacitors, and polymer solar
cells to recover non-trivial insights. The data extracted through our pipeline
is made available through a web platform at https://polymerscholar.org which
can be used to locate material property data recorded in abstracts
conveniently. This work demonstrates the feasibility of an automatic pipeline
that starts from published literature and ends with a complete set of extracted
material property information.",None,-1
Robust Trajectory Prediction against Adversarial Attacks,0.790037,"Trajectory prediction using deep neural networks (DNNs) is an essential
component of autonomous driving (AD) systems. However, these methods are
vulnerable to adversarial attacks, leading to serious consequences such as
collisions. In this work, we identify two key ingredients to defend trajectory
prediction models against adversarial attacks including (1) designing effective
adversarial training methods and (2) adding domain-specific data augmentation
to mitigate the performance degradation on clean data. We demonstrate that our
method is able to improve the performance by 46% on adversarial data and at the
cost of only 3% performance degradation on clean data, compared to the model
trained with clean data. Additionally, compared to existing robust methods, our
method can improve performance by 21% on adversarial examples and 9% on clean
data. Our robust model is evaluated with a planner to study its downstream
impacts. We demonstrate that our model can significantly reduce the severe
accident rates (e.g., collisions and off-road driving).",https://robustav.github.io/RobustTraj,-1
DocBed: A Multi-Stage OCR Solution for Documents with Complex Layouts,0.716781,"Digitization of newspapers is of interest for many reasons including
preservation of history, accessibility and search ability, etc. While
digitization of documents such as scientific articles and magazines is
prevalent in literature, one of the main challenges for digitization of
newspaper lies in its complex layout (e.g. articles spanning multiple columns,
text interrupted by images) analysis, which is necessary to preserve human
read-order. This work provides a major breakthrough in the digitization of
newspapers on three fronts: first, releasing a dataset of 3000 fully-annotated,
real-world newspaper images from 21 different U.S. states representing an
extensive variety of complex layouts for document layout analysis; second,
proposing layout segmentation as a precursor to existing optical character
recognition (OCR) engines, where multiple state-of-the-art image segmentation
models and several post-processing methods are explored for document layout
segmentation; third, providing a thorough and structured evaluation protocol
for isolated layout segmentation and end-to-end OCR.",https://github.com/facebookresearch/detectron2,-1
Boosting word frequencies in authorship attribution,0.251628,"In this paper, I introduce a simple method of computing relative word
frequencies for authorship attribution and similar stylometric tasks. Rather
than computing relative frequencies as the number of occurrences of a given
word divided by the total number of tokens in a text, I argue that a more
efficient normalization factor is the total number of relevant tokens only. The
notion of relevant words includes synonyms and, usually, a few dozen other
words in some ways semantically similar to a word in question. To determine
such a semantic background, one of word embedding models can be used. The
proposed method outperforms classical most-frequent-word approaches
substantially, usually by a few percentage points depending on the input
settings.",https://github.com/computationalstylistics/word_frequencies,-1
FvOR: Robust Joint Shape and Pose Optimization for Few-view Object Reconstruction,0.816554,"Reconstructing an accurate 3D object model from a few image observations
remains a challenging problem in computer vision. State-of-the-art approaches
typically assume accurate camera poses as input, which could be difficult to
obtain in realistic settings. In this paper, we present FvOR, a learning-based
object reconstruction method that predicts accurate 3D models given a few
images with noisy input poses. The core of our approach is a fast and robust
multi-view reconstruction algorithm to jointly refine 3D geometry and camera
pose estimation using learnable neural network modules. We provide a thorough
benchmark of state-of-the-art approaches for this problem on ShapeNet. Our
approach achieves best-in-class results. It is also two orders of magnitude
faster than the recent optimization-based approach IDR. Our code is released at
\url{https://github.com/zhenpeiyang/FvOR/}",https://github.com/zhenpeiyang/FvOR/,-1
Exploring Dimensionality Reduction Techniques in Multilingual Transformers,0.213116,"Both in scientific literature and in industry,, Semantic and context-aware
Natural Language Processing-based solutions have been gaining importance in
recent years. The possibilities and performance shown by these models when
dealing with complex Language Understanding tasks is unquestionable, from
conversational agents to the fight against disinformation in social networks.
In addition, considerable attention is also being paid to developing
multilingual models to tackle the language bottleneck. The growing need to
provide more complex models implementing all these features has been
accompanied by an increase in their size, without being conservative in the
number of dimensions required. This paper aims to give a comprehensive account
of the impact of a wide variety of dimensional reduction techniques on the
performance of different state-of-the-art multilingual Siamese Transformers,
including unsupervised dimensional reduction techniques such as linear and
nonlinear feature extraction, feature selection, and manifold techniques. In
order to evaluate the effects of these techniques, we considered the
multilingual extended version of Semantic Textual Similarity Benchmark (mSTSb)
and two different baseline approaches, one using the pre-trained version of
several models and another using their fine-tuned STS version. The results
evidence that it is possible to achieve an average reduction in the number of
dimensions of $91.58\% \pm 2.59\%$ and $54.65\% \pm 32.20\%$, respectively.
This work has also considered the consequences of dimensionality reduction for
visualization purposes. The results of this study will significantly contribute
to the understanding of how different tuning approaches affect performance on
semantic-aware tasks and how dimensional reduction techniques deal with the
high-dimensional embeddings computed for the STS task and their potential for
highly demanding NLP tasks",None,-1
Revisiting the Practical Effectiveness of Constituency Parse Extraction from Pre-trained Language Models,0.258598,"Constituency Parse Extraction from Pre-trained Language Models (CPE-PLM) is a
recent paradigm that attempts to induce constituency parse trees relying only
on the internal knowledge of pre-trained language models. While attractive in
the perspective that similar to in-context learning, it does not require
task-specific fine-tuning, the practical effectiveness of such an approach
still remains unclear, except that it can function as a probe for investigating
language models' inner workings. In this work, we mathematically reformulate
CPE-PLM and propose two advanced ensemble methods tailored for it,
demonstrating that the new parsing paradigm can be competitive with common
unsupervised parsers by introducing a set of heterogeneous PLMs combined using
our techniques. Furthermore, we explore some scenarios where the trees
generated by CPE-PLM are practically useful. Specifically, we show that CPE-PLM
is more effective than typical supervised parsers in few-shot settings.",https://github.com/nikitakit/self-attentive-parser,-1
Facing Changes: Continual Entity Alignment for Growing Knowledge Graphs,0.427999,"Entity alignment is a basic and vital technique in knowledge graph (KG)
integration. Over the years, research on entity alignment has resided on the
assumption that KGs are static, which neglects the nature of growth of
real-world KGs. As KGs grow, previous alignment results face the need to be
revisited while new entity alignment waits to be discovered. In this paper, we
propose and dive into a realistic yet unexplored setting, referred to as
continual entity alignment. To avoid retraining an entire model on the whole
KGs whenever new entities and triples come, we present a continual alignment
method for this task. It reconstructs an entity's representation based on
entity adjacency, enabling it to generate embeddings for new entities quickly
and inductively using their existing neighbors. It selects and replays partial
pre-aligned entity pairs to train only parts of KGs while extracting
trustworthy alignment for knowledge augmentation. As growing KGs inevitably
contain non-matchable entities, different from previous works, the proposed
method employs bidirectional nearest neighbor matching to find new entity
alignment and update old alignment. Furthermore, we also construct new datasets
by simulating the growth of multilingual DBpedia. Extensive experiments
demonstrate that our continual alignment method is more effective than
baselines based on retraining or inductive learning.",https://github.com/nju-websoft/ContEA,-1
Hyperbolic Relevance Matching for Neural Keyphrase Extraction,0.392573,"Keyphrase extraction is a fundamental task in natural language processing and
information retrieval that aims to extract a set of phrases with important
information from a source document. Identifying important keyphrase is the
central component of the keyphrase extraction task, and its main challenge is
how to represent information comprehensively and discriminate importance
accurately. In this paper, to address these issues, we design a new hyperbolic
matching model (HyperMatch) to represent phrases and documents in the same
hyperbolic space and explicitly estimate the phrase-document relevance via the
Poincar\'e distance as the important score of each phrase. Specifically, to
capture the hierarchical syntactic and semantic structure information,
HyperMatch takes advantage of the hidden representations in multiple layers of
RoBERTa and integrates them as the word embeddings via an adaptive mixing
layer. Meanwhile, considering the hierarchical structure hidden in the
document, HyperMatch embeds both phrases and documents in the same hyperbolic
space via a hyperbolic phrase encoder and a hyperbolic document encoder. This
strategy can further enhance the estimation of phrase-document relevance due to
the good properties of hyperbolic space. In this setting, the keyphrase
extraction can be taken as a matching problem and effectively implemented by
minimizing a hyperbolic margin-based triplet loss. Extensive experiments are
conducted on six benchmarks and demonstrate that HyperMatch outperforms the
state-of-the-art baselines.",https://github.com/MySong7NLPer/HyperMatch,-1
"Improving Depression estimation from facial videos with face alignment, training optimization and scheduling",0.324765,"Deep learning models have shown promising results in recognizing depressive
states using video-based facial expressions. While successful models typically
leverage using 3D-CNNs or video distillation techniques, the different use of
pretraining, data augmentation, preprocessing, and optimization techniques
across experiments makes it difficult to make fair architectural comparisons.
We propose instead to enhance two simple models based on ResNet-50 that use
only static spatial information by using two specific face alignment methods
and improved data augmentation, optimization, and scheduling techniques. Our
extensive experiments on benchmark datasets obtain similar results to
sophisticated spatio-temporal models for single streams, while the score-level
fusion of two different streams outperforms state-of-the-art methods. Our
findings suggest that specific modifications in the preprocessing and training
process result in noticeable differences in the performance of the models and
could hide the actual originally attributed to the use of different neural
network architectures.",None,-1
Query-based Industrial Analytics over Knowledge Graphs with Ontology Reshaping,0.706386,"Industrial analytics that includes among others equipment diagnosis and
anomaly detection heavily relies on integration of heterogeneous production
data. Knowledge Graphs (KGs) as the data format and ontologies as the unified
data schemata are a prominent solution that offers high quality data
integration and a convenient and standardised way to exchange data and to layer
analytical applications over it. However, poor design of ontologies of high
degree of mismatch between them and industrial data naturally lead to KGs of
low quality that impede the adoption and scalability of industrial analytics.
Indeed, such KGs substantially increase the training time of writing queries
for users, consume high volume of storage for redundant information, and are
hard to maintain and update. To address this problem we propose an ontology
reshaping approach to transform ontologies into KG schemata that better reflect
the underlying data and thus help to construct better KGs. In this poster we
present a preliminary discussion of our on-going research, evaluate our
approach with a rich set of SPARQL queries on real-world industry data at Bosch
and discuss our findings.",None,-1
Transformer-based Global 3D Hand Pose Estimation in Two Hands Manipulating Objects Scenarios,0.128105,"This report describes our 1st place solution to ECCV 2022 challenge on Human
Body, Hands, and Activities (HBHA) from Egocentric and Multi-view Cameras (hand
pose estimation). In this challenge, we aim to estimate global 3D hand poses
from the input image where two hands and an object are interacting on the
egocentric viewpoint. Our proposed method performs end-to-end multi-hand pose
estimation via transformer architecture. In particular, our method robustly
estimates hand poses in a scenario where two hands interact. Additionally, we
propose an algorithm that considers hand scales to robustly estimate the
absolute depth. The proposed algorithm works well even when the hand sizes are
various for each person. Our method attains 14.4 mm (left) and 15.9 mm (right)
errors for each hand in the test set.",None,-1
AiATrack: Attention in Attention for Transformer Visual Tracking,0.984408,"Transformer trackers have achieved impressive advancements recently, where
the attention mechanism plays an important role. However, the independent
correlation computation in the attention mechanism could result in noisy and
ambiguous attention weights, which inhibits further performance improvement. To
address this issue, we propose an attention in attention (AiA) module, which
enhances appropriate correlations and suppresses erroneous ones by seeking
consensus among all correlation vectors. Our AiA module can be readily applied
to both self-attention blocks and cross-attention blocks to facilitate feature
aggregation and information propagation for visual tracking. Moreover, we
propose a streamlined Transformer tracking framework, dubbed AiATrack, by
introducing efficient feature reuse and target-background embeddings to make
full use of temporal references. Experiments show that our tracker achieves
state-of-the-art performance on six tracking benchmarks while running at a
real-time speed.",https://github.com/Little-Podi/AiATrack,-1
Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages,0.652001,"Multilingual Pretrained Language Models (MPLMs) have shown their strong
multilinguality in recent empirical cross-lingual transfer studies. In this
paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)
pipeline to improve the zero-shot performance on low-resource languages (LRLs)
by augmenting the context with semantically similar sentences retrieved from a
high-resource language (HRL) as prompts. PARC improves the zero-shot
performance on three downstream tasks (binary sentiment classification, topic
categorization and natural language inference) with multilingual parallel test
sets across 10 LRLs covering 6 language families in both unlabeled settings
(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the
finetuning baseline by 3.7%. We find a significant positive correlation between
cross-lingual transfer performance on one side, and the similarity between the
high- and low-resource languages as well as the amount of low-resource
pretraining data on the other side. A robustness analysis suggests that PARC
has the potential to achieve even stronger performance with more powerful
MPLMs.",https://github.com/ercong21/parc,-1
Unified Vision and Language Prompt Learning,0.635048,"Prompt tuning, a parameter- and data-efficient transfer learning paradigm
that tunes only a small number of parameters in a model's input space, has
become a trend in the vision community since the emergence of large
vision-language models like CLIP. We present a systematic study on two
representative prompt tuning methods, namely text prompt tuning and visual
prompt tuning. A major finding is that none of the unimodal prompt tuning
methods performs consistently well: text prompt tuning fails on data with high
intra-class visual variances while visual prompt tuning cannot handle low
inter-class variances. To combine the best from both worlds, we propose a
simple approach called Unified Prompt Tuning (UPT), which essentially learns a
tiny neural network to jointly optimize prompts across different modalities.
Extensive experiments on over 11 vision datasets show that UPT achieves a
better trade-off than the unimodal counterparts on few-shot learning
benchmarks, as well as on domain generalization benchmarks. Code and models
will be released to facilitate future research.",https://github.com/yuhangzang/UPT,-1
Explanations from Large Language Models Make Small Reasoners Better,0.607495,"Integrating free-text explanations to in-context learning of large language
models (LLM) is shown to elicit strong reasoning capabilities along with
reasonable explanations. In this paper, we consider the problem of leveraging
the explanations generated by LLM to improve the training of small reasoners,
which are more favorable in real-production deployment due to their low cost.
We systematically explore three explanation generation approaches from LLM and
utilize a multi-task learning framework to facilitate small models to acquire
strong reasoning power together with explanation generation capabilities.
Experiments on multiple reasoning tasks show that our method can consistently
and significantly outperform finetuning baselines across different settings,
and even perform better than finetuning/prompting a 60x larger GPT-3 (175B)
model by up to 9.5% in accuracy. As a side benefit, human evaluation further
shows that our method can generate high-quality explanations to justify its
predictions, moving towards the goal of explainable AI.",https://github.com/eladsegal/strategyqa,-1
E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning,0.44811,"The ability to recognize analogies is fundamental to human cognition.
Existing benchmarks to test word analogy do not reveal the underneath process
of analogical reasoning of neural models. Holding the belief that models
capable of reasoning should be right for the right reasons, we propose a
first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning
benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in
English) problems sourced from the Civil Service Exams, which require intensive
background knowledge to solve. More importantly, we design a free-text
explanation scheme to explain whether an analogy should be drawn, and manually
annotate them for each and every question and candidate answer. Empirical
results suggest that this benchmark is very challenging for some
state-of-the-art models for both explanation generation and analogical question
answering tasks, which invites further research in this area.",https://github.com/Tiiiger/bert_score,-1
Bias-Eliminated Semantic Refinement for Any-Shot Learning,0.166073,"When training samples are scarce, the semantic embedding technique, ie,
describing class labels with attributes, provides a condition to generate
visual features for unseen objects by transferring the knowledge from seen
objects. However, semantic descriptions are usually obtained in an external
paradigm, such as manual annotation, resulting in weak consistency between
descriptions and visual features. In this paper, we refine the coarse-grained
semantic description for any-shot learning tasks, ie, zero-shot learning (ZSL),
generalized zero-shot learning (GZSL), and few-shot learning (FSL). A new
model, namely, the semantic refinement Wasserstein generative adversarial
network (SRWGAN) model, is designed with the proposed multihead representation
and hierarchical alignment techniques. Unlike conventional methods, semantic
refinement is performed with the aim of identifying a bias-eliminated condition
for disjoint-class feature generation and is applicable in both inductive and
transductive settings. We extensively evaluate model performance on six
benchmark datasets and observe state-of-the-art results for any-shot learning;
eg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset
and 82.2% harmonic accuracy for the Oxford Flowers (FLO) dataset in the
standard GZSL setting. Various visualizations are also provided to show the
bias-eliminated generation of SRWGAN. Our code is available.",https://github.com/LiangjunFeng/SRWGAN,8266
CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation,0.40863,"Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed
inputs during training -- helps reduce model reliance on spurious correlations
and improves generalization to out-of-distribution (OOD) data. Prior work on
generating counterfactuals only considered restricted classes of perturbations,
limiting their effectiveness. We present COunterfactual Generation via
Retrieval and Editing (CORE), a retrieval-augmented generation framework for
creating diverse counterfactual perturbations for CDA. For each training
example, CORE first performs a dense retrieval over a task-related unlabeled
text corpus using a learned bi-encoder and extracts relevant counterfactual
excerpts. CORE then incorporates these into prompts to a large language model
with few-shot learning capabilities, for counterfactual editing. Conditioning
language model edits on naturally occurring data results in diverse
perturbations. Experiments on natural language inference and sentiment analysis
benchmarks show that CORE counterfactuals are more effective at improving
generalization to OOD data compared to other DA approaches. We also show that
the CORE retrieval framework can be used to encourage diversity in manually
authored perturbations",https://github.com/tanay2001/CORE,-1
Trust-based Consensus in Multi-Agent Reinforcement Learning Systems,0.178665,"An often neglected issue in multi-agent reinforcement learning (MARL) is the
potential presence of unreliable agents in the environment whose deviations
from expected behavior can prevent a system from accomplishing its intended
tasks. In particular, consensus is a fundamental underpinning problem of
cooperative distributed multi-agent systems. Consensus requires different
agents, situated in a decentralized communication network, to reach an
agreement out of a set of initial proposals that they put forward.
Learning-based agents should adopt a protocol that allows them to reach
consensus despite having one or more unreliable agents in the system. This
paper investigates the problem of unreliable agents in MARL, considering
consensus as case study. Echoing established results in the distributed systems
literature, our experiments show that even a moderate fraction of such agents
can greatly impact the ability of reaching consensus in a networked
environment. We propose Reinforcement Learning-based Trusted Consensus (RLTC),
a decentralized trust mechanism, in which agents can independently decide which
neighbors to communicate with. We empirically demonstrate that our trust
mechanism is able to deal with unreliable agents effectively, as evidenced by
higher consensus success rates.",None,-1
Progressive Continual Learning for Spoken Keyword Spotting,0.369448,"Catastrophic forgetting is a thorny challenge when updating keyword spotting
(KWS) models after deployment. To tackle such challenges, we propose a
progressive continual learning strategy for small-footprint spoken keyword
spotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a
network instantiator to generate the task-specific sub-networks for remembering
previously learned keywords. As a result, the PCL-KWS approach incrementally
learns new keywords without forgetting prior knowledge. Besides, the
keyword-aware network scaling mechanism of PCL-KWS constrains the growth of
model parameters while achieving high performance. Experimental results show
that after learning five new tasks sequentially, our proposed PCL-KWS approach
archives the new state-of-the-art performance of 92.8% average accuracy for all
the tasks on Google Speech Command dataset compared with other baselines.",None,-1
Danish Airs and Grounds: A Dataset for Aerial-to-Street-Level Place Recognition and Localization,0.736403,"Place recognition and visual localization are particularly challenging in
wide baseline configurations. In this paper, we contribute with the
\emph{Danish Airs and Grounds} (DAG) dataset, a large collection of
street-level and aerial images targeting such cases. Its main challenge lies in
the extreme viewing-angle difference between query and reference images with
consequent changes in illumination and perspective. The dataset is larger and
more diverse than current publicly available data, including more than 50 km of
road in urban, suburban and rural areas. All images are associated with
accurate 6-DoF metadata that allows the benchmarking of visual localization
methods.
  We also propose a map-to-image re-localization pipeline, that first estimates
a dense 3D reconstruction from the aerial images and then matches query
street-level images to street-level renderings of the 3D model. The dataset can
be downloaded at: https://frederikwarburg.github.io/DAG",None,-1
FJMP: Factorized Joint Multi-Agent Motion Prediction over Learned Directed Acyclic Interaction Graphs,0.851459,"Predicting the future motion of road agents is a critical task in an
autonomous driving pipeline. In this work, we address the problem of generating
a set of scene-level, or joint, future trajectory predictions in multi-agent
driving scenarios. To this end, we propose FJMP, a Factorized Joint Motion
Prediction framework for multi-agent interactive driving scenarios. FJMP models
the future scene interaction dynamics as a sparse directed interaction graph,
where edges denote explicit interactions between agents. We then prune the
graph into a directed acyclic graph (DAG) and decompose the joint prediction
task into a sequence of marginal and conditional predictions according to the
partial ordering of the DAG, where joint future trajectories are decoded using
a directed acyclic graph neural network (DAGNN). We conduct experiments on the
INTERACTION and Argoverse 2 datasets and demonstrate that FJMP produces more
accurate and scene-consistent joint trajectory predictions than non-factorized
approaches, especially on the most interactive and kinematically interesting
agents. FJMP ranks 1st on the multi-agent test leaderboard of the INTERACTION
dataset.",None,-1
Measuring and Improving the Use of Graph Information in Graph Neural Networks,0.709562,"Graph neural networks (GNNs) have been widely used for representation
learning on graph data. However, there is limited understanding on how much
performance GNNs actually gain from graph data. This paper introduces a
context-surrounding GNN framework and proposes two smoothness metrics to
measure the quantity and quality of information obtained from graph data. A new
GNN model, called CS-GNN, is then designed to improve the use of graph
information based on the smoothness values of a graph. CS-GNN is shown to
achieve better performance than existing methods in different types of real
graphs.",None,-1
Dataless Knowledge Fusion by Merging Weights of Language Models,0.991309,"Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios.",https://github.com/bloomberg/dataless-model-merging,-1
Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction,0.594096,"Multimodal named entity recognition and relation extraction (MNER and MRE) is
a fundamental and crucial branch in information extraction. However, existing
approaches for MNER and MRE usually suffer from error sensitivity when
irrelevant object images incorporated in texts. To deal with these issues, we
propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for
visual-enhanced entity and relation extraction, aiming to achieve more
effective and robust performance. Specifically, we regard visual representation
as pluggable visual prefix to guide the textual representation for error
insensitive forecasting decision. We further propose a dynamic gated
aggregation strategy to achieve hierarchical multi-scaled visual features as
visual prefix for fusion. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our method, and achieve state-of-the-art
performance. Code is available in https://github.com/zjunlp/HVPNeT.",https://github.com/zjunlp/HVPNeT,-1
MusIAC: An extensible generative framework for Music Infilling Applications with multi-level Control,0.148334,"We present a novel music generation framework for music infilling, with a
user friendly interface. Infilling refers to the task of generating musical
sections given the surrounding multi-track music. The proposed
transformer-based framework is extensible for new control tokens as the added
music control tokens such as tonal tension per bar and track polyphony level in
this work. We explore the effects of including several musically meaningful
control tokens, and evaluate the results using objective metrics related to
pitch and rhythm. Our results demonstrate that adding additional control tokens
helps to generate music with stronger stylistic similarities to the original
music. It also provides the user with more control to change properties like
the music texture and tonal tension in each bar compared to previous research
which only provided control for track density. We present the model in a Google
Colab notebook to enable interactive generation.",https://github.com/ruiguo-bio/MusIAC,-1
Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology,0.533482,"Histopathology whole slide images (WSIs) can reveal significant
inter-hospital variability such as illumination, color or optical artifacts.
These variations, caused by the use of different scanning protocols across
medical centers (staining, scanner), can strongly harm algorithms
generalization on unseen protocols. This motivates development of new methods
to limit such drop of performances. In this paper, to enhance robustness on
unseen target protocols, we propose a new test-time data augmentation based on
multi domain image-to-image translation. It allows to project images from
unseen protocol into each source domain before classifying them and ensembling
the predictions. This test-time augmentation method results in a significant
boost of performances for domain generalization. To demonstrate its
effectiveness, our method has been evaluated on 2 different histopathology
tasks where it outperforms conventional domain generalization, standard H&E
specific color augmentation/normalization and standard test-time augmentation
techniques. Our code is publicly available at
https://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling.",https://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling,-1
LEATHER: A Framework for Learning to Generate Human-like Text in Dialogue,0.0780578,"Algorithms for text-generation in dialogue can be misguided. For example, in
task-oriented settings, reinforcement learning that optimizes only task-success
can lead to abysmal lexical diversity. We hypothesize this is due to poor
theoretical understanding of the objectives in text-generation and their
relation to the learning process (i.e., model training). To this end, we
propose a new theoretical framework for learning to generate text in dialogue.
Compared to existing theories of learning, our framework allows for analysis of
the multi-faceted goals inherent to text-generation. We use our framework to
develop theoretical guarantees for learners that adapt to unseen data. As an
example, we apply our theory to study data-shift within a cooperative learning
algorithm proposed for the GuessWhat?! visual dialogue game. From this insight,
we propose a new algorithm, and empirically, we demonstrate our proposal
improves both task-success and human-likeness of the generated text. Finally,
we show statistics from our theory are empirically predictive of multiple
qualities of the generated dialogue, suggesting our theory is useful for
model-selection when human evaluations are not available.",https://github.com/anthonysicilia/LEATHER-AACL2022,-1
SAVCHOI: Detecting Suspicious Activities using Dense Video Captioning with Human Object Interactions,0.0547102,"Detecting suspicious activities in surveillance videos is a longstanding
problem in real-time surveillance that leads to difficulties in detecting
crimes. Hence, we propose a novel approach for detecting and summarizing
suspicious activities in surveillance videos. We have also created ground truth
summaries for the UCF-Crime video dataset. We modify a pre-existing approach
for this task by leveraging the Human-Object Interaction (HOI) model for the
Visual features in the Bi-Modal Transformer. Further, we validate our approach
against the existing state-of-the-art algorithms for the Dense Video Captioning
task for the ActivityNet Captions dataset. We observe that this formulation for
Dense Captioning performs significantly better than other discussed BMT-based
approaches for BLEU@1, BLEU@2, BLEU@3, BLEU@4, and METEOR. We further perform a
comparative analysis of the dataset and the model to report the findings based
on different NMS thresholds (searched using Genetic Algorithms). Here, our
formulation outperforms all the models for BLEU@1, BLEU@2, BLEU@3, and most
models for BLEU@4 and METEOR falling short of only ADV-INF Global by 25% and
0.5%, respectively.",https://github.com/v-iashin/BMT,246
RankGen: Improving Text Generation with Large Ranking Models,0.585302,"Given an input sequence (or prefix), modern language models often assign high
probabilities to output sequences that are repetitive, incoherent, or
irrelevant to the prefix; as such, model-generated text also contains such
artifacts. To address these issues we present RankGen, a 1.2B parameter encoder
model for English that scores model generations given a prefix. RankGen can be
flexibly incorporated as a scoring function in beam search and used to decode
from any pretrained language model. We train RankGen using large-scale
contrastive learning to map a prefix close to the ground-truth sequence that
follows it and far away from two types of negatives: (1) random sequences from
the same document as the prefix, and (2) sequences generated from a large
language model conditioned on the prefix. Experiments across four different
language models (345M-11B parameters) and two domains show that RankGen
significantly outperforms decoding algorithms like nucleus, top-k, and typical
sampling, as well as contrastive decoding and search, on both automatic metrics
(85.0 vs 77.3 MAUVE over nucleus) as well as human evaluations with English
writers (74.5% human preference over nucleus sampling). Analysis reveals that
RankGen outputs are more relevant to the prefix and improve continuity and
coherence compared to baselines. We release our model checkpoints, code, and
human preference data with explanations to facilitate future research.",https://github.com/martiansideofthemoon/rankgen,-1
Learning Point Processes using Recurrent Graph Network,0.075783,"We present a novel Recurrent Graph Network (RGN) approach for predicting
discrete marked event sequences by learning the underlying complex stochastic
process. Using the framework of Point Processes, we interpret a marked discrete
event sequence as the superposition of different sequences each of a unique
type. The nodes of the Graph Network use LSTM to incorporate past information
whereas a Graph Attention Network (GAT Network) introduces strong inductive
biases to capture the interaction between these different types of events. By
changing the self-attention mechanism from attending over past events to
attending over event types, we obtain a reduction in time and space complexity
from $\mathcal{O}(N^2)$ (total number of events) to
$\mathcal{O}(|\mathcal{Y}|^2)$ (number of event types). Experiments show that
the proposed approach improves performance in log-likelihood, prediction and
goodness-of-fit tasks with lower time and space complexity compared to
state-of-the art Transformer based architectures.",None,-1
Learning Uncertainty with Artificial Neural Networks for Improved Predictive Process Monitoring,0.513304,"The inability of artificial neural networks to assess the uncertainty of
their predictions is an impediment to their widespread use. We distinguish two
types of learnable uncertainty: model uncertainty due to a lack of training
data and noise-induced observational uncertainty. Bayesian neural networks use
solid mathematical foundations to learn the model uncertainties of their
predictions. The observational uncertainty can be calculated by adding one
layer to these networks and augmenting their loss functions. Our contribution
is to apply these uncertainty concepts to predictive process monitoring tasks
to train uncertainty-based models to predict the remaining time and outcomes.
Our experiments show that uncertainty estimates allow more and less accurate
predictions to be differentiated and confidence intervals to be constructed in
both regression and classification tasks. These conclusions remain true even in
early stages of running processes. Moreover, the deployed techniques are fast
and produce more accurate predictions. The learned uncertainty could increase
users' confidence in their process prediction systems, promote better
cooperation between humans and these systems, and enable earlier
implementations with smaller datasets.",None,-1
Generating Multiple-Length Summaries via Reinforcement Learning for Unsupervised Sentence Summarization,0.633891,"Sentence summarization shortens given texts while maintaining core contents
of the texts. Unsupervised approaches have been studied to summarize texts
without human-written summaries. However, recent unsupervised models are
extractive, which remove words from texts and thus they are less flexible than
abstractive summarization. In this work, we devise an abstractive model based
on reinforcement learning without ground-truth summaries. We formulate the
unsupervised summarization based on the Markov decision process with rewards
representing the summary quality. To further enhance the summary quality, we
develop a multi-summary learning mechanism that generates multiple summaries
with varying lengths for a given text, while making the summaries mutually
enhance each other. Experimental results show that the proposed model
substantially outperforms both abstractive and extractive models, yet
frequently generating new words not contained in input texts.",https://github.com/dmhyun/MSRP,-1
Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images,0.95893,"In this paper, we present a generalizable model-free 6-DoF object pose
estimator called Gen6D. Existing generalizable pose estimators either need
high-quality object models or require additional depth maps or object masks in
test time, which significantly limits their application scope. In contrast, our
pose estimator only requires some posed images of the unseen object and is able
to accurately predict the poses of the object in arbitrary environments. Gen6D
consists of an object detector, a viewpoint selector and a pose refiner, all of
which do not require the 3D object model and can generalize to unseen objects.
Experiments show that Gen6D achieves state-of-the-art results on two model-free
datasets: the MOPED dataset and a new GenMOP dataset collected by us. In
addition, on the LINEMOD dataset, Gen6D achieves competitive results compared
with instance-specific pose estimators. Project page:
https://liuyuan-pal.github.io/Gen6D/.",https://liuyuan-pal.github.io/Gen6D/,-1
InterTrack: Interaction Transformer for 3D Multi-Object Tracking,0.388691,"3D multi-object tracking (MOT) is a key problem for autonomous vehicles,
required to perform well-informed motion planning in dynamic environments.
Particularly for densely occupied scenes, associating existing tracks to new
detections remains challenging as existing systems tend to omit critical
contextual information. Our proposed solution, InterTrack, introduces the
Interaction Transformer for 3D MOT to generate discriminative object
representations for data association. We extract state and shape features for
each track and detection, and efficiently aggregate global information via
attention. We then perform a learned regression on each track/detection feature
pair to estimate affinities, and use a robust two-stage data association and
track management approach to produce the final tracks. We validate our approach
on the nuScenes 3D MOT benchmark, where we observe significant improvements,
particularly on classes with small physical sizes and clustered objects. As of
submission, InterTrack ranks 1st in overall AMOTA among methods using
CenterPoint detections.",https://github.com/open-mmlab/OpenPCDet,-1
Investigation of Data Augmentation Techniques for Disordered Speech Recognition,0.609811,"Disordered speech recognition is a highly challenging task. The underlying
neuro-motor conditions of people with speech disorders, often compounded with
co-occurring physical disabilities, lead to the difficulty in collecting large
quantities of speech required for system development. This paper investigates a
set of data augmentation techniques for disordered speech recognition,
including vocal tract length perturbation (VTLP), tempo perturbation and speed
perturbation. Both normal and disordered speech were exploited in the
augmentation process. Variability among impaired speakers in both the original
and augmented data was modeled using learning hidden unit contributions (LHUC)
based speaker adaptive training. The final speaker adapted system constructed
using the UASpeech corpus and the best augmentation approach based on speed
perturbation produced up to 2.92% absolute (9.3% relative) word error rate
(WER) reduction over the baseline system without data augmentation, and gave an
overall WER of 26.37% on the test set containing 16 dysarthric speakers.",None,-1
PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition,0.929234,"The widely studied task of Natural Language Inference (NLI) requires a system
to recognize whether one piece of text is textually entailed by another, i.e.
whether the entirety of its meaning can be inferred from the other. In current
NLI datasets and models, textual entailment relations are typically defined on
the sentence- or paragraph-level. However, even a simple sentence often
contains multiple propositions, i.e. distinct units of meaning conveyed by the
sentence. As these propositions can carry different truth values in the context
of a given premise, we argue for the need to recognize the textual entailment
relation of each proposition in a sentence individually.
  We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert
human raters. Our dataset structure resembles the tasks of (1) segmenting
sentences within a document to the set of propositions, and (2) classifying the
entailment relation of each proposition with respect to a different yet
topically-aligned document, i.e. documents describing the same event or entity.
We establish strong baselines for the segmentation and entailment tasks.
Through case studies on summary hallucination detection and document-level NLI,
we demonstrate that our conceptual framework is potentially useful for
understanding and explaining the compositionality of NLI labels.",https://github.com/google-research-datasets/propsegment,-1
AutoAttention: Automatic Field Pair Selection for Attention in User Behavior Modeling,0.358099,"In Click-through rate (CTR) prediction models, a user's interest is usually
represented as a fixed-length vector based on her history behaviors. Recently,
several methods are proposed to learn an attentive weight for each user
behavior and conduct weighted sum pooling. However, these methods only manually
select several fields from the target item side as the query to interact with
the behaviors, neglecting the other target item fields, as well as user and
context fields. Directly including all these fields in the attention may
introduce noise and deteriorate the performance. In this paper, we propose a
novel model named AutoAttention, which includes all item/user/context side
fields as the query, and assigns a learnable weight for each field pair between
behavior fields and query fields. Pruning on these field pairs via these
learnable weights lead to automatic field pair selection, so as to identify and
remove noisy field pairs. Though including more fields, the computation cost of
AutoAttention is still low due to using a simple attention function and field
pair selection. Extensive experiments on the public dataset and Tencent's
production dataset demonstrate the effectiveness of the proposed approach.",https://github.com/waydrow/AutoAttention,-1
DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization,0.525208,"LiDAR mapping is important yet challenging in self-driving and mobile
robotics. To tackle such a global point cloud registration problem, DeepMapping
converts the complex map estimation into a self-supervised training of simple
deep networks. Despite its broad convergence range on small datasets,
DeepMapping still cannot produce satisfactory results on large-scale datasets
with thousands of frames. This is due to the lack of loop closures and exact
cross-frame point correspondences, and the slow convergence of its global
localization network. We propose DeepMapping2 by adding two novel techniques to
address these issues: (1) organization of training batch based on map topology
from loop closing, and (2) self-supervised local-to-global point consistency
loss leveraging pairwise registration. Our experiments and ablation studies on
public datasets (KITTI, NCLT, and Nebula) demonstrate the effectiveness of our
method.",None,-1
Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control,0.60274,"Hierarchical Imitation Learning (HIL) has been proposed to recover
highly-complex behaviors in long-horizon tasks from expert demonstrations by
modeling the task hierarchy with the option framework. Existing methods either
overlook the causal relationship between the subtask and its corresponding
policy or cannot learn the policy in an end-to-end fashion, which leads to
suboptimality. In this work, we develop a novel HIL algorithm based on
Adversarial Inverse Reinforcement Learning and adapt it with the
Expectation-Maximization algorithm in order to directly recover a hierarchical
policy from the unannotated demonstrations. Further, we introduce a directed
information term to the objective function to enhance the causality and propose
a Variational Autoencoder framework for learning with our objectives in an
end-to-end fashion. Theoretical justifications and evaluations on challenging
robotic control tasks are provided to show the superiority of our algorithm.
The codes are available at https://github.com/LucasCJYSDL/HierAIRL.",https://github.com/LucasCJYSDL/HierAIRL,-1
On the Usefulness of the Fit-on-the-Test View on Evaluating Calibration of Classifiers,0.273109,"Every uncalibrated classifier has a corresponding true calibration map that
calibrates its confidence. Deviations of this idealistic map from the identity
map reveal miscalibration. Such calibration errors can be reduced with many
post-hoc calibration methods which fit some family of calibration maps on a
validation dataset. In contrast, evaluation of calibration with the expected
calibration error (ECE) on the test set does not explicitly involve fitting.
However, as we demonstrate, ECE can still be viewed as if fitting a family of
functions on the test data. This motivates the fit-on-the-test view on
evaluation: first, approximate a calibration map on the test data, and second,
quantify its distance from the identity. Exploiting this view allows us to
unlock missed opportunities: (1) use the plethora of post-hoc calibration
methods for evaluating calibration; (2) tune the number of bins in ECE with
cross-validation. Furthermore, we introduce: (3) benchmarking on pseudo-real
data where the true calibration map can be estimated very precisely; and (4)
novel calibration and evaluation methods using new calibration map families PL
and PL3.",https://github.com/markus93/fit-on-the-test,4116
MonoJSG: Joint Semantic and Geometric Cost Volume for Monocular 3D Object Detection,0.847963,"Due to the inherent ill-posed nature of 2D-3D projection, monocular 3D object
detection lacks accurate depth recovery ability. Although the deep neural
network (DNN) enables monocular depth-sensing from high-level learned features,
the pixel-level cues are usually omitted due to the deep convolution mechanism.
To benefit from both the powerful feature representation in DNN and pixel-level
geometric constraints, we reformulate the monocular object depth estimation as
a progressive refinement problem and propose a joint semantic and geometric
cost volume to model the depth error. Specifically, we first leverage neural
networks to learn the object position, dimension, and dense normalized 3D
object coordinates. Based on the object depth, the dense coordinates patch
together with the corresponding object features is reprojected to the image
space to build a cost volume in a joint semantic and geometric error manner.
The final depth is obtained by feeding the cost volume to a refinement network,
where the distribution of semantic and geometric error is regularized by direct
depth supervision. Through effectively mitigating depth error by the refinement
framework, we achieve state-of-the-art results on both the KITTI and Waymo
datasets.",https://github.com/lianqing11/MonoJSG,-1
Unfooling Perturbation-Based Post Hoc Explainers,0.778764,"Monumental advancements in artificial intelligence (AI) have lured the
interest of doctors, lenders, judges, and other professionals. While these
high-stakes decision-makers are optimistic about the technology, those familiar
with AI systems are wary about the lack of transparency of its decision-making
processes. Perturbation-based post hoc explainers offer a model agnostic means
of interpreting these systems while only requiring query-level access. However,
recent work demonstrates that these explainers can be fooled adversarially.
This discovery has adverse implications for auditors, regulators, and other
sentinels. With this in mind, several natural questions arise - how can we
audit these black box systems? And how can we ascertain that the auditee is
complying with the audit in good faith? In this work, we rigorously formalize
this problem and devise a defense against adversarial attacks on
perturbation-based explainers. We propose algorithms for the detection
(CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by our
novel conditional anomaly detection approach, KNN-CAD. We demonstrate that our
approach successfully detects whether a black box system adversarially conceals
its decision-making process and mitigates the adversarial attack on real-world
data for the prevalent explainers, LIME and SHAP.",https://github.com/craymichael/unfooling,-1
Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion,0.353761,"Transformer-based pre-trained models like BERT have achieved great progress
on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also
shown general benefits in multiple NLP tasks. However, how to efficiently
integrate dependency prior structure into pre-trained models to better model
complex semantic matching relations is still unsettled. In this paper, we
propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion
\textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency
structure into pre-trained models and adaptively fuses it with semantic
information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a
structure-sensitive paradigm to construct a dependency matrix for calibrating
attention weights. It adopts an adaptive fusion module to integrate the
obtained dependency information and the original semantic signals. Moreover,
DAFA reconstructs the attention calculation flow and provides better
interpretability. By applying it on BERT, our method achieves state-of-the-art
or competitive performance on 10 public datasets, demonstrating the benefits of
adaptively fusing dependency structure in semantic matching task.",None,-1
Nested Named Entity Recognition from Medical Texts: An Adaptive Shared Network Architecture with Attentive CRF,0.173795,"Recognizing useful named entities plays a vital role in medical information
processing, which helps drive the development of medical area research. Deep
learning methods have achieved good results in medical named entity recognition
(NER). However, we find that existing methods face great challenges when
dealing with the nested named entities. In this work, we propose a novel
method, referred to as ASAC, to solve the dilemma caused by the nested
phenomenon, in which the core idea is to model the dependency between different
categories of entity recognition. The proposed method contains two key modules:
the adaptive shared (AS) part and the attentive conditional random field (ACRF)
module. The former part automatically assigns adaptive weights across each task
to achieve optimal recognition accuracy in the multi-layer network. The latter
module employs the attention operation to model the dependency between
different entities. In this way, our model could learn better entity
representations by capturing the implicit distinctions and relationships
between different categories of entities. Extensive experiments on public
datasets verify the effectiveness of our method. Besides, we also perform
ablation analyses to deeply understand our methods.",None,27779
Training Semantic Descriptors for Image-Based Localization,0.173355,"Vision based solutions for the localization of vehicles have become popular
recently. We employ an image retrieval based visual localization approach. The
database images are kept with GPS coordinates and the location of the retrieved
database image serves as an approximate position of the query image. We show
that localization can be performed via descriptors solely extracted from
semantically segmented images. It is reliable especially when the environment
is subjected to severe illumination and seasonal changes. Our experiments
reveal that the localization performance of a semantic descriptor can increase
up to the level of state-of-the-art RGB image based methods.",None,-1
Contextual Information and Commonsense Based Prompt for Emotion Recognition in Conversation,0.299399,"Emotion recognition in conversation (ERC) aims to detect the emotion for each
utterance in a given conversation. The newly proposed ERC models have leveraged
pre-trained language models (PLMs) with the paradigm of pre-training and
fine-tuning to obtain good performance. However, these models seldom exploit
PLMs' advantages thoroughly, and perform poorly for the conversations lacking
explicit emotional expressions. In order to fully leverage the latent knowledge
related to the emotional expressions in utterances, we propose a novel ERC
model CISPER with the new paradigm of prompt and language model (LM) tuning.
Specifically, CISPER is equipped with the prompt blending the contextual
information and commonsense related to the interlocutor's utterances, to
achieve ERC more effectively. Our extensive experiments demonstrate CISPER's
superior performance over the state-of-the-art ERC models, and the
effectiveness of leveraging these two kinds of significant prompt information
for performance gains. To reproduce our experimental results conveniently,
CISPER's sourcecode and the datasets have been shared at
https://github.com/DeqingYang/CISPER.",https://github.com/DeqingYang/CISPER,-1
Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks,0.637302,"Early diagnosis and intervention are clinically considered the paramount part
of treating cerebral palsy (CP), so it is essential to design an efficient and
interpretable automatic prediction system for CP. We highlight a significant
difference between CP infants' frequency of human movement and that of the
healthy group, which improves prediction performance. However, the existing
deep learning-based methods did not use the frequency information of infants'
movement for CP prediction. This paper proposes a frequency attention informed
graph convolutional network and validates it on two consumer-grade RGB video
datasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency
attention module aids in improving both classification performance and system
interpretability. In addition, we design a frequency-binning method that
retains the critical frequency of the human joint position data while filtering
the noise. Our prediction performance achieves state-of-the-art research on
both datasets. Our work demonstrates the effectiveness of frequency information
in supporting the prediction of CP non-intrusively and provides a way for
supporting the early diagnosis of CP in the resource-limited regions where the
clinical resources are not abundant.",https://github.com/zhz95/FAIGCN,3498
Brain tumor detection using artificial convolutional neural networks,0.565486,"In this paper, a convolutional neural network (CNN) was used to classify NMR
images of human brains with 4 different types of tumors: meningioma, glioma and
pituitary gland tumors. During the training phase of this project, an accuracy
of 100% was obtained, meanwhile, in the evaluation phase the precision was 96%.",None,-1
Demystifying Unsupervised Semantic Correspondence Estimation,0.174499,"We explore semantic correspondence estimation through the lens of
unsupervised learning. We thoroughly evaluate several recently proposed
unsupervised methods across multiple challenging datasets using a standardized
evaluation protocol where we vary factors such as the backbone architecture,
the pre-training strategy, and the pre-training and finetuning datasets. To
better understand the failure modes of these methods, and in order to provide a
clearer path for improvement, we provide a new diagnostic framework along with
a new performance metric that is better suited to the semantic matching task.
Finally, we introduce a new unsupervised correspondence approach which utilizes
the strength of pre-trained features while encouraging better matches during
training. This results in significantly better matching performance compared to
current state-of-the-art methods.",None,-1
AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation,0.296505,"Training a generalizable 3D part segmentation network is quite challenging
but of great importance in real-world applications. To tackle this problem,
some works design task-specific solutions by translating human understanding of
the task to machine's learning process, which faces the risk of missing the
optimal strategy since machines do not necessarily understand in the exact
human way. Others try to use conventional task-agnostic approaches designed for
domain generalization problems with no task prior knowledge considered. To
solve the above issues, we propose AutoGPart, a generic method enabling
training generalizable 3D part segmentation networks with the task prior
considered. AutoGPart builds a supervision space with geometric prior knowledge
encoded, and lets the machine to search for the optimal supervisions from the
space for a specific segmentation task automatically. Extensive experiments on
three generalizable 3D part segmentation tasks are conducted to demonstrate the
effectiveness and versatility of AutoGPart. We demonstrate that the performance
of segmentation networks using simple backbones can be significantly improved
when trained with supervisions searched by our method.",https://autogpart.github.io,-1
ARMOR: A Model-based Framework for Improving Arbitrary Baseline Policies with Offline Data,0.328462,"We propose a new model-based offline RL framework, called Adversarial Models
for Offline Reinforcement Learning (ARMOR), which can robustly learn policies
to improve upon an arbitrary baseline policy regardless of data coverage. Based
on the concept of relative pessimism, ARMOR is designed to optimize for the
worst-case relative performance when facing uncertainty. In theory, we prove
that the learned policy of ARMOR never degrades the performance of the baseline
policy with any admissible hyperparameter, and can learn to compete with the
best policy within data coverage when the hyperparameter is well tuned, and the
baseline policy is supported by the data. Such a robust policy improvement
property makes ARMOR especially suitable for building real-world learning
systems, because in practice ensuring no performance degradation is imperative
before considering any benefit learning can bring.",None,-1
Neurodevelopmental Phenotype Prediction: A State-of-the-Art Deep Learning Model,0.199115,"A major challenge in medical image analysis is the automated detection of
biomarkers from neuroimaging data. Traditional approaches, often based on image
registration, are limited in capturing the high variability of cortical
organisation across individuals. Deep learning methods have been shown to be
successful in overcoming this difficulty, and some of them have even
outperformed medical professionals on certain datasets. In this paper, we apply
a deep neural network to analyse the cortical surface data of neonates, derived
from the publicly available Developing Human Connectome Project (dHCP). Our
goal is to identify neurodevelopmental biomarkers and to predict gestational
age at birth based on these biomarkers. Using scans of preterm neonates
acquired around the term-equivalent age, we were able to investigate the impact
of preterm birth on cortical growth and maturation during late gestation.
Besides reaching state-of-the-art prediction accuracy, the proposed model has
much fewer parameters than the baselines, and its error stays low on both
unregistered and registered cortical surfaces.",https://github.com/daniel-unyi-42/Neurodevelopmental-Phenotype-Prediction,-1
Optimal estimation of Gaussian DAG models,0.761446,"We study the optimal sample complexity of learning a Gaussian directed
acyclic graph (DAG) from observational data. Our main results establish the
minimax optimal sample complexity for learning the structure of a linear
Gaussian DAG model in two settings of interest: 1) Under equal variances
without knowledge of the true ordering, and 2) For general linear models given
knowledge of the ordering. In both cases the sample complexity is $n\asymp
q\log(d/q)$, where $q$ is the maximum number of parents and $d$ is the number
of nodes. We further make comparisons with the classical problem of learning
(undirected) Gaussian graphical models, showing that under the equal variance
assumption, these two problems share the same optimal sample complexity. In
other words, at least for Gaussian models with equal error variances, learning
a directed graphical model is statistically no more difficult than learning an
undirected graphical model. Our results also extend to more general
identification assumptions as well as subgaussian errors.",https://github.com/WY-Chen/EqVarDAG/blob/master/R/EqVarDAG_HD_TD.R,-1
FrOoDo: Framework for Out-of-Distribution Detection,0.0913083,"FrOoDo is an easy-to-use and flexible framework for Out-of-Distribution
detection tasks in digital pathology. It can be used with PyTorch
classification and segmentation models, and its modular design allows for easy
extension. The goal is to automate the task of OoD Evaluation such that
research can focus on the main goal of either designing new models, new methods
or evaluating a new dataset. The code can be found at
https://github.com/MECLabTUDA/FrOoDo.",None,-1
End-to-end model for named entity recognition from speech without paired training data,0.753686,"Recent works showed that end-to-end neural approaches tend to become very
popular for spoken language understanding (SLU). Through the term end-to-end,
one considers the use of a single model optimized to extract semantic
information directly from the speech signal. A major issue for such models is
the lack of paired audio and textual data with semantic annotation. In this
paper, we propose an approach to build an end-to-end neural model to extract
semantic information in a scenario in which zero paired audio data is
available. Our approach is based on the use of an external model trained to
generate a sequence of vectorial representations from text. These
representations mimic the hidden representations that could be generated inside
an end-to-end automatic speech recognition (ASR) model by processing a speech
signal. An SLU neural module is then trained using these representations as
input and the annotated text as output. Last, the SLU module replaces the top
layers of the ASR model to achieve the construction of the end-to-end model.
Our experiments on named entity recognition, carried out on the QUAERO corpus,
show that this approach is very promising, getting better results than a
comparable cascade approach or than the use of synthetic voices.",https://github.com/mdhaffar/Named-Entity-Recognition,-1
CVAE-H: Conditionalizing Variational Autoencoders via Hypernetworks and Trajectory Forecasting for Autonomous Driving,0.253888,"The task of predicting stochastic behaviors of road agents in diverse
environments is a challenging problem for autonomous driving. To best
understand scene contexts and produce diverse possible future states of the
road agents adaptively in different environments, a prediction model should be
probabilistic, multi-modal, context-driven, and general. We present
Conditionalizing Variational AutoEncoders via Hypernetworks (CVAE-H); a
conditional VAE that extensively leverages hypernetwork and performs generative
tasks for high-dimensional problems like the prediction task. We first evaluate
CVAE-H on simple generative experiments to show that CVAE-H is probabilistic,
multi-modal, context-driven, and general. Then, we demonstrate that the
proposed model effectively solves a self-driving prediction problem by
producing accurate predictions of road agents in various environments.",None,-1
Streaming Intended Query Detection using E2E Modeling for Continued Conversation,0.0566598,"In voice-enabled applications, a predetermined hotword isusually used to
activate a device in order to attend to the query.However, speaking queries
followed by a hotword each timeintroduces a cognitive burden in continued
conversations. Toavoid repeating a hotword, we propose a streaming
end-to-end(E2E) intended query detector that identifies the utterancesdirected
towards the device and filters out other utterancesnot directed towards device.
The proposed approach incor-porates the intended query detector into the E2E
model thatalready folds different components of the speech recognitionpipeline
into one neural network.The E2E modeling onspeech decoding and intended query
detection also allows us todeclare a quick intended query detection based on
early partialrecognition result, which is important to decrease latencyand make
the system responsive. We demonstrate that theproposed E2E approach yields a
22% relative improvement onequal error rate (EER) for the detection accuracy
and 600 mslatency improvement compared with an independent intendedquery
detector. In our experiment, the proposed model detectswhether the user is
talking to the device with a 8.7% EERwithin 1.4 seconds of median latency after
user starts speaking.",None,-1
Immersive Text Game and Personality Classification,0.0120734,"We designed and built a game called \textit{Immersive Text Game}, which
allows the player to choose a story and a character, and interact with other
characters in the story in an immersive manner of dialogues. The game is based
on several latest models, including text generation language model, information
extraction model, commonsense reasoning model, and psychology evaluation model.
In the past, similar text games usually let players choose from limited actions
instead of answering on their own, and not every time what characters said are
determined by the player. Through the combination of these models and elaborate
game mechanics and modes, the player will find some novel experiences as driven
through the storyline.",None,-1
A Call for Clarity in Beam Search: How It Works and When It Stops,0.142604,"Text generation with beam search has proven successful in a wide range of
applications. We point out that, though largely overlooked in the literature,
the commonly-used implementation of beam decoding (e.g., Hugging Face
Transformers and fairseq) uses a first come, first served heuristic: it keeps a
set of already completed sequences over time steps and stops when the size of
this set reaches the beam size. Based on this finding, we introduce a patience
factor, a simple modification to this beam decoding implementation, that
generalizes the stopping criterion and provides flexibility to the depth of
search. Empirical results demonstrate that adjusting this patience factor
improves decoding performance of strong pretrained models on news text
summarization and machine translation over diverse language pairs, with a
negligible inference slowdown. Our approach only modifies one line of code and
can be thus readily incorporated in any implementation. Further, we find that
different versions of beam decoding result in large performance differences in
summarization, demonstrating the need for clarity in specifying the beam search
implementation in research work. Our code will be available upon publication.",None,-1
A Benchmark dataset for predictive maintenance,0.553488,"The paper describes the MetroPT data set, an outcome of a eXplainable
Predictive Maintenance (XPM) project with an urban metro public transportation
service in Porto, Portugal. The data was collected in 2022 that aimed to
evaluate machine learning methods for online anomaly detection and failure
prediction. By capturing several analogic sensor signals (pressure,
temperature, current consumption), digital signals (control signals, discrete
signals), and GPS information (latitude, longitude, and speed), we provide a
dataset that can be easily used to evaluate online machine learning methods.
This dataset contains some interesting characteristics and can be a good
benchmark for predictive maintenance models.",None,-1
A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss,0.226583,"For readability assessment, traditional methods mainly employ machine
learning classifiers with hundreds of linguistic features. Although the deep
learning model has become the prominent approach for almost all NLP tasks, it
is less explored for readability assessment. In this paper, we propose a
BERT-based model with feature projection and length-balanced loss (BERT-FP-LBL)
for readability assessment. Specially, we present a new difficulty knowledge
guided semi-supervised method to extract topic features to complement the
traditional linguistic features. From the linguistic features, we employ
projection filtering to extract orthogonal features to supplement BERT
representations. Furthermore, we design a new length-balanced loss to handle
the greatly varying length distribution of data. Our model achieves
state-of-the-art performances on two English benchmark datasets and one dataset
of Chinese textbooks, and also achieves the near-perfect accuracy of 99\% on
one English dataset. Moreover, our proposed model obtains comparable results
with human experts in consistency test.",https://github.com/liwb1219/zhfeat,1295
CaSS: A Channel-aware Self-supervised Representation Learning Framework for Multivariate Time Series Classification,0.246933,"Self-supervised representation learning of Multivariate Time Series (MTS) is
a challenging task and attracts increasing research interests in recent years.
Many previous works focus on the pretext task of self-supervised learning and
usually neglect the complex problem of MTS encoding, leading to unpromising
results. In this paper, we tackle this challenge from two aspects: encoder and
pretext task, and propose a unified channel-aware self-supervised learning
framework CaSS. Specifically, we first design a new Transformer-based encoder
Channel-aware Transformer (CaT) to capture the complex relationships between
different time channels of MTS. Second, we combine two novel pretext tasks Next
Trend Prediction (NTP) and Contextual Similarity (CS) for the self-supervised
representation learning with our proposed encoder. Extensive experiments are
conducted on several commonly used benchmark datasets. The experimental results
show that our framework achieves new state-of-the-art comparing with previous
self-supervised MTS representation learning methods (up to +7.70\% improvement
on LSST dataset) and can be well applied to the downstream MTS classification.",None,-1
MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds,0.261831,"Semantic segmentation of 3D point cloud is an essential task for autonomous
driving environment perception. The pipeline of most pointwise point cloud
semantic segmentation methods includes points sampling, neighbor searching,
feature aggregation, and classification. Neighbor searching method like
K-nearest neighbors algorithm, KNN, has been widely applied. However, the
complexity of KNN is always a bottleneck of efficiency. In this paper, we
propose an end-to-end neural architecture, Multiple View Pointwise Net,
MVP-Net, to efficiently and directly infer large-scale outdoor point cloud
without KNN or any complex pre/postprocessing. Instead, assumption-based space
filling curves and multi-rotation of point cloud methods are introduced to
point feature aggregation and receptive field expanding. Numerical experiments
show that the proposed MVP-Net is 11 times faster than the most efficient
pointwise semantic segmentation method RandLA-Net and achieves the same
accuracy on the large-scale benchmark SemanticKITTI dataset.",None,-1
Code-Switching without Switching: Language Agnostic End-to-End Speech Translation,0.687652,"We propose a) a Language Agnostic end-to-end Speech Translation model (LAST),
and b) a data augmentation strategy to increase code-switching (CS)
performance. With increasing globalization, multiple languages are increasingly
used interchangeably during fluent speech. Such CS complicates traditional
speech recognition and translation, as we must recognize which language was
spoken first and then apply a language-dependent recognizer and subsequent
translation component to generate the desired target language output. Such a
pipeline introduces latency and errors. In this paper, we eliminate the need
for that, by treating speech recognition and translation as one unified
end-to-end speech translation problem. By training LAST with both input
languages, we decode speech into one target language, regardless of the input
language. LAST delivers comparable recognition and speech translation accuracy
in monolingual usage, while reducing latency and error rate considerably when
CS is observed.",None,-1
EGCR: Explanation Generation for Conversational Recommendation,0.0254912,"Growing attention has been paid in Conversational Recommendation System
(CRS), which works as a conversation-based and recommendation task-oriented
tool to provide items of interest and explore user preference. However,
existing work in CRS fails to explicitly show the reasoning logic to users and
the whole CRS still remains a black box. Therefore we propose a novel
end-to-end framework named Explanation Generation for Conversational
Recommendation (EGCR) based on generating explanations for conversational
agents to explain why they make the action. EGCR incorporates user reviews to
enhance the item representation and increase the informativeness of the whole
conversation. To the best of our knowledge, this is the first framework for
explainable conversational recommendation on real-world datasets. Moreover, we
evaluate EGCR on one benchmark conversational recommendation datasets and
achieve better performance on both recommendation accuracy and conversation
quality than other state-of-the art models. Finally, extensive experiments
demonstrate that generated explanations are not only having high quality and
explainability, but also making CRS more trustworthy. We will make our code
available to contribute to the CRS community",None,-1
"MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction",0.979835,"The diverse relationships among real-world events, including coreference,
temporal, causal, and subevent relations, are fundamental to understanding
natural languages. However, two drawbacks of existing datasets limit event
relation extraction (ERE) tasks: (1) Small scale. Due to the annotation
complexity, the data scale of existing datasets is limited, which cannot well
train and evaluate data-hungry models. (2) Absence of unified annotation.
Different types of event relations naturally interact with each other, but
existing datasets only cover limited relation types at once, which prevents
models from taking full advantage of relation interactions. To address these
issues, we construct a unified large-scale human-annotated ERE dataset
MAVEN-ERE with improved annotation schemes. It contains 103,193 event
coreference chains, 1,216,217 temporal relations, 57,992 causal relations, and
15,841 subevent relations, which is larger than existing datasets of all the
ERE tasks by at least an order of magnitude. Experiments show that ERE on
MAVEN-ERE is quite challenging, and considering relation interactions with
joint learning can improve performances. The dataset and source codes can be
obtained from https://github.com/THU-KEG/MAVEN-ERE.",https://github.com/THU-KEG/MAVEN-ERE,-1
Effective Image Tampering Localization with Multi-Scale ConvNeXt Feature Fusion,0.431287,"With the widespread use of powerful image editing tools, image tampering
becomes easy and realistic. Existing image forensic methods still face
challenges of low generalization performance and robustness. In this letter, we
propose an effective image tampering localization scheme based on ConvNeXt
network and multi-scale feature fusion. Stacked ConvNeXt blocks are used as an
encoder to capture hierarchical multi-scale features, which are then fused in
decoder for locating tampered pixels accurately. Combined loss and effective
data augmentation are adopted to further improve the model performance.
Extensive experimental results show that localization performance of our
proposed scheme outperforms other state-of-the-art ones. The source code will
be available at https://github.com/ZhuHC98/ITL-SSN.",https://github.com/ZhuHC98/ITL-SSN,-1
A generative grammar of cooking,0.274094,"Cooking is a uniquely human endeavor for transforming raw ingredients into
delicious dishes. Over centuries, cultures worldwide have evolved diverse
cooking practices ingrained in their culinary traditions. Recipes, thus, are
cultural capsules that capture culinary knowledge in elaborate cooking
protocols. While simple quantitative models have probed the patterns in recipe
composition and the process of cuisine evolution, unlike other cultural quirks
such as language, the principles of cooking remain hitherto unexplored. The
fundamental rules that drive the act of cooking, shaping recipe composition and
cuisine architecture, are unclear. Here we present a generative grammar of
cooking that captures the underlying culinary logic. By studying an extensive
repository of structured recipes, we identify core concepts and rules that
together forge a combinatorial system for culinary synthesis. Building on the
body of work done in the context of language, the demonstration of a logically
consistent generative framework offers profound insights into the act of
cooking. Given the central role of food in nutrition and lifestyle disorders,
culinary grammar provides leverage to improve public health through dietary
interventions beyond applications for creative pursuits such as novel recipe
generation.",None,-1
JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning,0.272866,"In online job marketplaces, it is important to establish a well-defined job
title taxonomy for various downstream tasks (e.g., job recommendation, users'
career analysis, and turnover prediction). Job Title Normalization (JTN) is
such a cleaning step to classify user-created non-standard job titles into
normalized ones. However, solving the JTN problem is non-trivial with
challenges: (1) semantic similarity of different job titles, (2) non-normalized
user-created job titles, and (3) large-scale and long-tailed job titles in
real-world applications. To this end, we propose a novel solution, named JAMES,
that constructs three unique embeddings (i.e., graph, contextual, and
syntactic) of a target job title to effectively capture its various traits. We
further propose a multi-aspect co-attention mechanism to attentively combine
these embeddings, and employ neural logical reasoning representations to
collaboratively estimate similarities between messy job titles and normalized
job titles in a reasoning space. To evaluate JAMES, we conduct comprehensive
experiments against ten competing models on a large-scale real-world dataset
with over 350,000 job titles. Our experimental results show that JAMES
significantly outperforms the best baseline by 10.06% in Precision@10 and by
17.52% in NDCG@10, respectively.",None,13066
PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion,0.417023,"This paper presents a parameter-lite transfer learning approach of pretrained
language models (LM) for knowledge graph (KG) completion. Instead of
finetuning, which modifies all LM parameters, we only tune a few new parameters
while keeping the original LM parameters fixed. We establish this via
reformulating KG completion as a ""fill-in-the-blank"" task, and introducing a
parameter-lite encoder on top of the original LMs. We show that, by tuning far
fewer parameters than finetuning, LMs transfer non-trivially to most tasks and
reach competitiveness with prior state-of-the-art approaches. For instance, we
outperform the fully finetuning approaches on a KG completion benchmark by
tuning only 1% of the parameters. The code and datasets are available at
\url{https://github.com/yuanyehome/PALT}.",https://github.com/yuanyehome/PALT,-1
Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer,0.876453,"Videos are created to express emotion, exchange information, and share
experiences. Video synthesis has intrigued researchers for a long time. Despite
the rapid progress driven by advances in visual synthesis, most existing
studies focus on improving the frames' quality and the transitions between
them, while little progress has been made in generating longer videos. In this
paper, we present a method that builds on 3D-VQGAN and transformers to generate
videos with thousands of frames. Our evaluation shows that our model trained on
16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse,
and Taichi-HD datasets can generate diverse, coherent, and high-quality long
videos. We also showcase conditional extensions of our approach for generating
meaningful long videos by incorporating temporal information with text and
audio. Videos and code can be found at
https://songweige.github.io/projects/tats/index.html.",https://songweige.github.io/projects/tats,-1
Semantic properties of English nominal pluralization: Insights from word embeddings,0.718231,"Semantic differentiation of nominal pluralization is grammaticalized in many
languages. For example, plural markers may only be relevant for human nouns.
English does not appear to make such distinctions. Using distributional
semantics, we show that English nominal pluralization exhibits semantic
clusters. For instance, pluralization of fruit words is more similar to one
another and less similar to pluralization of other semantic classes. Therefore,
reduction of the meaning shift in plural formation to the addition of an
abstract plural meaning is too simplistic. A semantically informed method,
called CosClassAvg, is introduced that outperforms pluralization methods in
distributional semantics which assume plural formation amounts to the addition
of a fixed plural vector. In comparison with our approach, a method from
compositional distributional semantics, called FRACSS, predicted plural vectors
that were more similar to the corpus-extracted plural vectors in terms of
direction but not vector length. A modeling study reveals that the observed
difference between the two predicted semantic spaces by CosClassAvg and FRACSS
carries over to how well a computational model of the listener can understand
previously unencountered plural forms. Mappings from word forms, represented
with triphone vectors, to predicted semantic vectors are more productive when
CosClassAvg-generated semantic vectors are employed as gold standard vectors
instead of FRACSS-generated vectors.",None,-1
CoNFies: Controllable Neural Face Avatars,0.281492,"Neural Radiance Fields (NeRF) are compelling techniques for modeling dynamic
3D scenes from 2D image collections. These volumetric representations would be
well suited for synthesizing novel facial expressions but for two problems.
First, deformable NeRFs are object agnostic and model holistic movement of the
scene: they can replay how the motion changes over time, but they cannot alter
it in an interpretable way. Second, controllable volumetric representations
typically require either time-consuming manual annotations or 3D supervision to
provide semantic meaning to the scene. We propose a controllable neural
representation for face self-portraits (CoNFies), that solves both of these
problems within a common framework, and it can rely on automated processing. We
use automated facial action recognition (AFAR) to characterize facial
expressions as a combination of action units (AU) and their intensities. AUs
provide both the semantic locations and control labels for the system. CoNFies
outperformed competing methods for novel view and expression synthesis in terms
of visual and anatomic fidelity of expressions.",None,-1
OntoSeer -- A Recommendation System to Improve the Quality of Ontologies,0.285999,"Building an ontology is not only a time-consuming process, but it is also
confusing, especially for beginners and the inexperienced. Although ontology
developers can take the help of domain experts in building an ontology, they
are not readily available in several cases for a variety of reasons. Ontology
developers have to grapple with several questions related to the choice of
classes, properties, and the axioms that should be included. Apart from this,
there are aspects such as modularity and reusability that should be taken care
of. From among the thousands of publicly available ontologies and vocabularies
in repositories such as Linked Open Vocabularies (LOV) and BioPortal, it is
hard to know the terms (classes and properties) that can be reused in the
development of an ontology. A similar problem exists in implementing the right
set of ontology design patterns (ODPs) from among the several available.
Generally, ontology developers make use of their experience in handling these
issues, and the inexperienced ones have a hard time. In order to bridge this
gap, we propose a tool named OntoSeer, that monitors the ontology development
process and provides suggestions in real-time to improve the quality of the
ontology under development. It can provide suggestions on the naming
conventions to follow, vocabulary to reuse, ODPs to implement, and axioms to be
added to the ontology. OntoSeer has been implemented as a Prot\'eg\'e plug-in.",https://github.com/kracr/ontoseer,-1
Depth Estimation with Simplified Transformer,0.681898,"Transformer and its variants have shown state-of-the-art results in many
vision tasks recently, ranging from image classification to dense prediction.
Despite of their success, limited work has been reported on improving the model
efficiency for deployment in latency-critical applications, such as autonomous
driving and robotic navigation. In this paper, we aim at improving upon the
existing transformers in vision, and propose a method for self-supervised
monocular Depth Estimation with Simplified Transformer (DEST), which is
efficient and particularly suitable for deployment on GPU-based platforms.
Through strategic design choices, our model leads to significant reduction in
model size, complexity, as well as inference latency, while achieving superior
accuracy as compared to state-of-the-art. We also show that our design
generalize well to other dense prediction task without bells and whistles.",None,-1
Large Language Models and the Reverse Turing Test,0.320092,"Large Language Models (LLMs) have been transformative. They are pre-trained
foundational models that are self-supervised and can be adapted with fine
tuning to a wide range of natural language tasks, each of which previously
would have required a separate network model. This is one step closer to the
extraordinary versatility of human language. GPT-3 and more recently LaMDA can
carry on dialogs with humans on many topics after minimal priming with a few
examples. However, there has been a wide range of reactions and debate on
whether these LLMs understand what they are saying or exhibit signs of
intelligence. This high variance is exhibited in three interviews with LLMs
reaching wildly different conclusions. A new possibility was uncovered that
could explain this divergence. What appears to be intelligence in LLMs may in
fact be a mirror that reflects the intelligence of the interviewer, a
remarkable twist that could be considered a Reverse Turing Test. If so, then by
studying interviews we may be learning more about the intelligence and beliefs
of the interviewer than the intelligence of the LLMs. As LLMs become more
capable they may transform the way we interact with machines and how they
interact with each other. Increasingly, LLMs are being coupled with
sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A
road map for achieving artificial general autonomy is outlined with seven major
improvements inspired by brain systems. LLMs could be used to uncover new
insights into brain function by downloading brain data during natural
behaviors.",None,-1
MBGDT:Robust Mini-Batch Gradient Descent,0.132788,"In high dimensions, most machine learning method perform fragile even there
are a little outliers. To address this, we hope to introduce a new method with
the base learner, such as Bayesian regression or stochastic gradient descent to
solve the problem of the vulnerability in the model. Because the mini-batch
gradient descent allows for a more robust convergence than the batch gradient
descent, we work a method with the mini-batch gradient descent, called
Mini-Batch Gradient Descent with Trimming (MBGDT). Our method show state-of-art
performance and have greater robustness than several baselines when we apply
our method in designed dataset.",None,-1
Syntax Controlled Knowledge Graph-to-Text Generation with Order and Semantic Consistency,0.325421,"The knowledge graph (KG) stores a large amount of structural knowledge, while
it is not easy for direct human understanding. Knowledge graph-to-text
(KG-to-text) generation aims to generate easy-to-understand sentences from the
KG, and at the same time, maintains semantic consistency between generated
sentences and the KG. Existing KG-to-text generation methods phrase this task
as a sequence-to-sequence generation task with linearized KG as input and
consider the consistency issue of the generated texts and KG through a simple
selection between decoded sentence word and KG node word at each time step.
However, the linearized KG order is commonly obtained through a heuristic
search without data-driven optimization. In this paper, we optimize the
knowledge description order prediction under the order supervision extracted
from the caption and further enhance the consistency of the generated sentences
and KG through syntactic and semantic regularization. We incorporate the
Part-of-Speech (POS) syntactic tags to constrain the positions to copy words
from the KG and employ a semantic context scoring function to evaluate the
semantic fitness for each word in its local context when decoding each word in
the generated sentence. Extensive experiments are conducted on two datasets,
WebNLG and DART, and achieve state-of-the-art performances.",None,-1
Multilingual Persuasion Detection: Video Games as an Invaluable Data Source for NLP,0.391081,"Role-playing games (RPGs) have a considerable amount of text in video game
dialogues. Quite often this text is semi-annotated by the game developers. In
this paper, we extract a multilingual dataset of persuasive dialogue from
several RPGs. We show the viability of this data in building a persuasion
detection system using a natural language processing (NLP) model called BERT.
We believe that video games have a lot of unused potential as a datasource for
a variety of NLP tasks. The code and data described in this paper are available
on Zenodo.",None,-1
Cycle-Consistent Counterfactuals by Latent Transformations,0.593448,"CounterFactual (CF) visual explanations try to find images similar to the
query image that change the decision of a vision system to a specified outcome.
Existing methods either require inference-time optimization or joint training
with a generative adversarial model which makes them time-consuming and
difficult to use in practice. We propose a novel approach, Cycle-Consistent
Counterfactuals by Latent Transformations (C3LT), which learns a latent
transformation that automatically generates visual CFs by steering in the
latent space of generative models. Our method uses cycle consistency between
the query and CF latent representations which helps our training to find better
solutions. C3LT can be easily plugged into any state-of-the-art pretrained
generative network. This enables our method to generate high-quality and
interpretable CF images at high resolution such as those in ImageNet. In
addition to several established metrics for evaluating CF explanations, we
introduce a novel metric tailored to assess the quality of the generated CF
examples and validate the effectiveness of our method on an extensive set of
experiments.",https://github.com/IBM/Contrastive-Explanation-Method,8748
MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning,0.849612,"The deep learning community has witnessed an exponentially growing interest
in self-supervised learning (SSL). However, it still remains unexplored how to
build a framework for learning useful representations of raw music waveforms in
a self-supervised manner. In this work, we design Music2Vec, a framework
exploring different SSL algorithmic components and tricks for music audio
recordings. Our model achieves comparable results to the state-of-the-art
(SOTA) music SSL model Jukebox, despite being significantly smaller with less
than 2% of parameters of the latter. The model will be released on
Huggingface(Please refer to: https://huggingface.co/m-a-p/music2vec-v1)",https://huggingface.co/m-a-p/music2vec-v1,-1
Efficient yet Competitive Speech Translation: FBK@IWSLT2022,0.688462,"The primary goal of this FBK's systems submission to the IWSLT 2022 offline
and simultaneous speech translation tasks is to reduce model training costs
without sacrificing translation quality. As such, we first question the need of
ASR pre-training, showing that it is not essential to achieve competitive
results. Second, we focus on data filtering, showing that a simple method that
looks at the ratio between source and target characters yields a quality
improvement of 1 BLEU. Third, we compare different methods to reduce the
detrimental effect of the audio segmentation mismatch between training data
manually segmented at sentence level and inference data that is automatically
segmented. Towards the same goal of training cost reduction, we participate in
the simultaneous task with the same model trained for offline ST. The
effectiveness of our lightweight training strategy is shown by the high score
obtained on the MuST-C en-de corpus (26.7 BLEU) and is confirmed in
high-resource data conditions by a 1.6 BLEU improvement on the IWSLT2020 test
set over last year's winning system.",https://github.com/hlt-mt/FBK-fairseq,-1
Semantically Proportional Patchmix for Few-Shot Learning,0.0229563,"Few-shot learning aims to classify unseen classes with only a limited number
of labeled data. Recent works have demonstrated that training models with a
simple transfer learning strategy can achieve competitive results in few-shot
classification. Although excelling at distinguishing training data, these
models are not well generalized to unseen data, probably due to insufficient
feature representations on evaluation. To tackle this issue, we propose
Semantically Proportional Patchmix (SePPMix), in which patches are cut and
pasted among training images and the ground truth labels are mixed
proportionally to the semantic information of the patches. In this way, we can
improve the generalization ability of the model by regional dropout effect
without introducing severe label noise. To learn more robust representations of
data, we further take rotate transformation on the mixed images and predict
rotations as a rule-based regularizer. Extensive experiments on prevalent
few-shot benchmarks have shown the effectiveness of our proposed method.",None,-1
MoSE: Modality Split and Ensemble for Multimodal Knowledge Graph Completion,0.821856,"Multimodal knowledge graph completion (MKGC) aims to predict missing entities
in MKGs. Previous works usually share relation representation across
modalities. This results in mutual interference between modalities during
training, since for a pair of entities, the relation from one modality probably
contradicts that from another modality. Furthermore, making a unified
prediction based on the shared relation representation treats the input in
different modalities equally, while their importance to the MKGC task should be
different. In this paper, we propose MoSE, a Modality Split representation
learning and Ensemble inference framework for MKGC. Specifically, in the
training phase, we learn modality-split relation embeddings for each modality
instead of a single modality-shared one, which alleviates the modality
interference. Based on these embeddings, in the inference phase, we first make
modality-split predictions and then exploit various ensemble methods to combine
the predictions with different weights, which models the modality importance
dynamically. Experimental results on three KG datasets show that MoSE
outperforms state-of-the-art MKGC methods. Codes are available at
https://github.com/OreOZhao/MoSE4MKGC.",https://github.com/OreOZhao/MoSE4MKGC,-1
Collaborating Domain-shared and Target-specific Feature Clustering for Cross-domain 3D Action Recognition,0.550491,"In this work, we consider the problem of cross-domain 3D action recognition
in the open-set setting, which has been rarely explored before. Specifically,
there is a source domain and a target domain that contain the skeleton
sequences with different styles and categories, and our purpose is to cluster
the target data by utilizing the labeled source data and unlabeled target data.
For such a challenging task, this paper presents a novel approach dubbed CoDT
to collaboratively cluster the domain-shared features and target-specific
features. CoDT consists of two parallel branches. One branch aims to learn
domain-shared features with supervised learning in the source domain, while the
other is to learn target-specific features using contrastive learning in the
target domain. To cluster the features, we propose an online clustering
algorithm that enables simultaneous promotion of robust pseudo label generation
and feature clustering. Furthermore, to leverage the complementarity of
domain-shared features and target-specific features, we propose a novel
collaborative clustering strategy to enforce pair-wise relationship consistency
between the two branches. We conduct extensive experiments on multiple
cross-domain 3D action recognition datasets, and the results demonstrate the
effectiveness of our method.",None,-1
Voxel-informed Language Grounding,0.219405,"Natural language applied to natural 2D images describes a fundamentally 3D
world. We present the Voxel-informed Language Grounder (VLG), a language
grounding model that leverages 3D geometric information in the form of voxel
maps derived from the visual input using a volumetric reconstruction model. We
show that VLG significantly improves grounding accuracy on SNARE, an object
reference game task. At the time of writing, VLG holds the top place on the
SNARE leaderboard, achieving SOTA results with a 2.0% absolute improvement.",https://github.com/snaredataset/snare,-1
Neural Enhanced Belief Propagation for Data Association in Multiobject Tracking,0.571702,"Situation-aware technologies enabled by multiobject tracking (MOT) methods
will create new services and applications in fields such as autonomous
navigation and applied ocean sciences. Belief propagation (BP) is a
state-of-the-art method for Bayesian MOT but fully relies on a statistical
model and preprocessed sensor measurements. In this paper, we establish a
hybrid method for model-based and data-driven MOT. The proposed neural enhanced
belief propagation (NEBP) approach complements BP by information learned from
raw sensor data with the goal to improve data association and to reject false
alarm measurements. We evaluate the performance of our NEBP approach for MOT on
the nuScenes autonomous driving dataset and demonstrate that it can outperform
state-of-the-art reference methods.",None,-1
Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities,0.999974,"Assembly101 is a new procedural activity dataset featuring 4321 videos of
people assembling and disassembling 101 ""take-apart"" toy vehicles. Participants
work without fixed instructions, and the sequences feature rich and natural
variations in action ordering, mistakes, and corrections. Assembly101 is the
first multi-view action dataset, with simultaneous static (8) and egocentric
(4) recordings. Sequences are annotated with more than 100K coarse and 1M
fine-grained action segments, and 18M 3D hand poses. We benchmark on three
action understanding tasks: recognition, anticipation and temporal
segmentation. Additionally, we propose a novel task of detecting mistakes. The
unique recording format and rich set of annotations allow us to investigate
generalization to new toys, cross-view transfer, long-tailed distributions, and
pose vs. appearance. We envision that Assembly101 will serve as a new challenge
to investigate various activity understanding problems.",https://assembly-101.github.io/,-1
Panoramic Human Activity Recognition,0.687657,"To obtain a more comprehensive activity understanding for a crowded scene, in
this paper, we propose a new problem of panoramic human activity recognition
(PAR), which aims to simultaneous achieve the individual action, social group
activity, and global activity recognition. This is a challenging yet practical
problem in real-world applications. For this problem, we develop a novel
hierarchical graph neural network to progressively represent and model the
multi-granularity human activities and mutual social relations for a crowd of
people. We further build a benchmark to evaluate the proposed method and other
existing related methods. Experimental results verify the rationality of the
proposed PAR problem, the effectiveness of our method and the usefulness of the
benchmark. We will release the source code and benchmark to the public for
promoting the study on this problem.",https://github.com/RuizeHan/PAR,10875
Semantic-Preserving Adversarial Code Comprehension,0.0783221,"Based on the tremendous success of pre-trained language models (PrLMs) for
source code comprehension tasks, current literature studies either ways to
further improve the performance (generalization) of PrLMs, or their robustness
against adversarial attacks. However, they have to compromise on the trade-off
between the two aspects and none of them consider improving both sides in an
effective and practical way. To fill this gap, we propose Semantic-Preserving
Adversarial Code Embeddings (SPACE) to find the worst-case semantic-preserving
attacks while forcing the model to predict the correct labels under these worst
cases. Experiments and analysis demonstrate that SPACE can stay robust against
state-of-the-art attacks while boosting the performance of PrLMs for code.",https://github.com/EricLee8/SPACE,-1
Effective and Efficient Training for Sequential Recommendation using Recency Sampling,0.675775,"Many modern sequential recommender systems use deep neural networks, which
can effectively estimate the relevance of items but require a lot of time to
train. Slow training increases expenses, hinders product development timescales
and prevents the model from being regularly updated to adapt to changing user
preferences. Training such sequential models involves appropriately sampling
past user interactions to create a realistic training objective. The existing
training objectives have limitations. For instance, next item prediction never
uses the beginning of the sequence as a learning target, thereby potentially
discarding valuable data. On the other hand, the item masking used by BERT4Rec
is only weakly related to the goal of the sequential recommendation; therefore,
it requires much more time to obtain an effective model. Hence, we propose a
novel Recency-based Sampling of Sequences training objective that addresses
both limitations. We apply our method to various recent and state-of-the-art
model architectures - such as GRU4Rec, Caser, and SASRec. We show that the
models enhanced with our method can achieve performances exceeding or very
close to stateof-the-art BERT4Rec, but with much less training time.",None,-1
Multi-Sentence Knowledge Selection in Open-Domain Dialogue,0.472729,"Incorporating external knowledge sources effectively in conversations is a
longstanding problem in open-domain dialogue research. The existing literature
on open-domain knowledge selection is limited and makes certain brittle
assumptions on knowledge sources to simplify the overall task (Dinan et al.,
2019), such as the existence of a single relevant knowledge sentence per
context. In this work, we evaluate the existing state of open-domain
conversation knowledge selection, showing where the existing methodologies
regarding data and evaluation are flawed. We then improve on them by proposing
a new framework for collecting relevant knowledge, and create an augmented
dataset based on the Wizard of Wikipedia (WOW) corpus, which we call WOW++.
WOW++ averages 8 relevant knowledge sentences per dialogue context, embracing
the inherent ambiguity of open-domain dialogue knowledge selection. We then
benchmark various knowledge ranking algorithms on this augmented dataset with
both intrinsic evaluation and extrinsic measures of response quality, showing
that neural rerankers that use WOW++ can outperform rankers trained on standard
datasets.",https://github.com/alexa/wow-plus-plus,-1
Learning Parameters for a Generalized Vidale-Wolfe Response Model with Flexible Ad Elasticity and Word-of-Mouth,0.162623,"In this research, we investigate a generalized form of Vidale-Wolfe (GVW)
model. One key element of our modeling work is that the GVW model contains two
useful indexes representing advertiser's elasticity and the word-of-mouth (WoM)
effect, respectively. Moreover, we discuss some desirable properties of the GVW
model, and present a deep neural network (DNN)-based estimation method to learn
its parameters. Furthermore, based on three realworld datasets, we conduct
computational experiments to validate the GVW model and identified properties.
In addition, we also discuss potential advantages of the GVW model over
econometric models. The research outcome shows that both the ad elasticity
index and the WoM index have significant influences on advertising responses,
and the GVW model has potential advantages over econometric models of
advertising, in terms of several interesting phenomena drawn from practical
advertising situations. The GVW model and its deep learning-based estimation
method provide a basis to support big data-driven advertising analytics and
decision makings; in the meanwhile, identified properties and experimental
findings of this research illuminate critical managerial insights for
advertisers in various advertising forms.",None,22251
How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?,0.284726,"A multilingual tokenizer is a fundamental component of multilingual neural
machine translation. It is trained from a multilingual corpus. Since a skewed
data distribution is considered to be harmful, a sampling strategy is usually
used to balance languages in the corpus. However, few works have systematically
answered how language imbalance in tokenizer training affects downstream
performance. In this work, we analyze how translation performance changes as
the data ratios among languages vary in the tokenizer training corpus. We find
that while relatively better performance is often observed when languages are
more equally sampled, the downstream performance is more robust to language
imbalance than we usually expected. Two features, UNK rate and closeness to the
character level, can warn of poor downstream performance before performing the
task. We also distinguish language sampling for tokenizer training from
sampling for model training and show that the model is more sensitive to the
latter.",https://github.com/ngoyal2707/sacrebleu/tree/adding_spm_tokenized_bleu,-1
VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners,0.634628,"We explore an efficient approach to establish a foundational video-text
model. We present VideoCoCa that maximally reuses a pretrained image-text
contrastive captioner (CoCa) model and adapt it to video-text tasks with
minimal extra training. While previous works adapt image-text models with
various cross-frame fusion modules, we find that the generative attentional
pooling and contrastive attentional pooling layers in CoCa are instantly
adaptable to flattened frame embeddings, yielding state-of-the-art results on
zero-shot video classification and zero-shot text-to-video retrieval.
Furthermore, we explore lightweight finetuning on top of VideoCoCa, and achieve
strong results on video question-answering and video captioning.",None,-1
Unified Semantic Typing with Meaningful Label Inference,0.864046,"Semantic typing aims at classifying tokens or spans of interest in a textual
context into semantic categories such as relations, entity types, and event
types. The inferred labels of semantic categories meaningfully interpret how
machines understand components of text. In this paper, we present UniST, a
unified framework for semantic typing that captures label semantics by
projecting both inputs and labels into a joint semantic embedding space. To
formulate different lexical and relational semantic typing tasks as a unified
task, we incorporate task descriptions to be jointly encoded with the input,
allowing UniST to be adapted to different tasks without introducing
task-specific model components. UniST optimizes a margin ranking loss such that
the semantic relatedness of the input and labels is reflected from their
embedding similarity. Our experiments demonstrate that UniST achieves strong
performance across three semantic typing tasks: entity typing, relation
classification and event typing. Meanwhile, UniST effectively transfers
semantic knowledge of labels and substantially improves generalizability on
inferring rarely seen and unseen types. In addition, multiple semantic typing
tasks can be jointly trained within the unified framework, leading to a single
compact multi-tasking model that performs comparably to dedicated single-task
models, while offering even better transferability.",https://github.com/luka-group/UniST,-1
"Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI",0.307079,"Users' physical safety is an increasing concern as the market for intelligent
systems continues to grow, where unconstrained systems may recommend users
dangerous actions that can lead to serious injury. Covertly unsafe text is an
area of particular interest, as such text may arise from everyday scenarios and
are challenging to detect as harmful. We propose FARM, a novel framework
leveraging external knowledge for trustworthy rationale generation in the
context of safety. In particular, FARM foveates on missing knowledge to qualify
the information required to reason in specific scenarios and retrieves this
information with attribution to trustworthy sources. This knowledge is used to
both classify the safety of the original text and generate human-interpretable
rationales, shedding light on the risk of systems to specific user groups and
helping both stakeholders manage the risks of their systems and policymakers to
provide concrete safeguards for consumer safety. Our experiments show that FARM
obtains state-of-the-art results on the SafeText dataset, showing absolute
improvement in safety classification accuracy by 5.9%.",https://github.com/alexmeigz/FARM,-1
Non-Linear Pairwise Language Mappings for Low-Resource Multilingual Acoustic Model Fusion,0.334307,"Multilingual speech recognition has drawn significant attention as an
effective way to compensate data scarcity for low-resource languages.
End-to-end (e2e) modelling is preferred over conventional hybrid systems,
mainly because of no lexicon requirement. However, hybrid DNN-HMMs still
outperform e2e models in limited data scenarios. Furthermore, the problem of
manual lexicon creation has been alleviated by publicly available trained
models of grapheme-to-phoneme (G2P) and text to IPA transliteration for a lot
of languages. In this paper, a novel approach of hybrid DNN-HMM acoustic models
fusion is proposed in a multilingual setup for the low-resource languages.
Posterior distributions from different monolingual acoustic models, against a
target language speech signal, are fused together. A separate regression neural
network is trained for each source-target language pair to transform posteriors
from source acoustic model to the target language. These networks require very
limited data as compared to the ASR training. Posterior fusion yields a
relative gain of 14.65% and 6.5% when compared with multilingual and
monolingual baselines respectively. Cross-lingual model fusion shows that the
comparable results can be achieved without using posteriors from the language
dependent ASR.",None,-1
Exploring Patch-wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks,0.775642,"Recently, contrastive learning-based image translation methods have been
proposed, which contrasts different spatial locations to enhance the spatial
correspondence. However, the methods often ignore the diverse semantic relation
within the images. To address this, here we propose a novel semantic relation
consistency (SRC) regularization along with the decoupled contrastive learning,
which utilize the diverse semantics by focusing on the heterogeneous semantics
between the image patches of a single image. To further improve the
performance, we present a hard negative mining by exploiting the semantic
relation. We verified our method for three tasks: single-modal and multi-modal
image translations, and GAN compression task for image translation.
Experimental results confirmed the state-of-art performance of our method in
all the three tasks.",https://github.com/mit-han-lab/gan-compression,-1
Unsupervised Fish Trajectory Tracking and Segmentation,0.0787009,"DNN for fish tracking and segmentation based on high-quality labels is
expensive. Alternative unsupervised approaches rely on spatial and temporal
variations that naturally occur in video data to generate noisy
pseudo-ground-truth labels. These pseudo-labels are used to train a multi-task
deep neural network. In this paper, we propose a three-stage framework for
robust fish tracking and segmentation, where the first stage is an optical flow
model, which generates the pseudo labels using spatial and temporal consistency
between frames. In the second stage, a self-supervised model refines the
pseudo-labels incrementally. In the third stage, the refined labels are used to
train a segmentation network. No human annotations are used during the training
or inference. Extensive experiments are performed to validate our method on
three public underwater video datasets and to demonstrate that it is highly
effective for video annotation and segmentation. We also evaluate the
robustness of our framework to different imaging conditions and discuss the
limitations of our current implementation.",None,-1
Wav2Vec-Aug: Improved self-supervised training with limited data,0.345182,"Self-supervised learning (SSL) of speech representations has received much
attention over the last few years but most work has focused on languages and
domains with an abundance of unlabeled data. However, for many languages there
is a shortage even in the unlabeled data which limits the effectiveness of SSL.
In this work, we focus on the problem of applying SSL to domains with limited
available data by leveraging data augmentation for Wav2Vec 2.0 pretraining.
Further, we propose improvements to each component of the model which result in
a combined relative word error rate (WER) improvement of up to 13% compared to
Wav2Vec 2.0 on Librispeech test-clean / other.",https://github.com/facebookresearch/WavAugment,-1
CrowdMLP: Weakly-Supervised Crowd Counting via Multi-Granularity MLP,0.735075,"Existing state-of-the-art crowd counting algorithms rely excessively on
location-level annotations, which are burdensome to acquire. When only
count-level (weak) supervisory signals are available, it is arduous and
error-prone to regress total counts due to the lack of explicit spatial
constraints. To address this issue, a novel and efficient counter (referred to
as CrowdMLP) is presented, which probes into modelling global dependencies of
embeddings and regressing total counts by devising a multi-granularity MLP
regressor. In specific, a locally-focused pre-trained frontend is cascaded to
extract crude feature maps with intrinsic spatial cues, which prevent the model
from collapsing into trivial outcomes. The crude embeddings, along with raw
crowd scenes, are tokenized at different granularity levels. The
multi-granularity MLP then proceeds to mix tokens at the dimensions of
cardinality, channel, and spatial for mining global information. An effective
proxy task, namely Split-Counting, is also proposed to evade the barrier of
limited samples and the shortage of spatial hints in a self-supervised manner.
Extensive experiments demonstrate that CrowdMLP significantly outperforms
existing weakly-supervised counting algorithms and performs on par with
state-of-the-art location-level supervised approaches.",None,-1
A Novel Approach to Fairness in Automated Decision-Making using Affective Normalization,0.0525721,"Any decision, such as one about who to hire, involves two components. First,
a rational component, i.e., they have a good education, they speak clearly.
Second, an affective component, based on observables such as visual features of
race and gender, and possibly biased by stereotypes. Here we propose a method
for measuring the affective, socially biased, component, thus enabling its
removal. That is, given a decision-making process, these affective measurements
remove the affective bias in the decision, rendering it fair across a set of
categories defined by the method itself. We thus propose that this may solve
three key problems in intersectional fairness: (1) the definition of categories
over which fairness is a consideration; (2) an infinite regress into smaller
and smaller groups; and (3) ensuring a fair distribution based on basic human
rights or other prior information. The primary idea in this paper is that
fairness biases can be measured using affective coherence, and that this can be
used to normalize outcome mappings. We aim for this conceptual work to expose a
novel method for handling fairness problems that uses emotional coherence as an
independent measure of bias that goes beyond statistical parity.",None,-1
"Learning Robotic Navigation from Experience: Principles, Methods, and Recent Results",0.675131,"Navigation is one of the most heavily studied problems in robotics, and is
conventionally approached as a geometric mapping and planning problem. However,
real-world navigation presents a complex set of physical challenges that defies
simple geometric abstractions. Machine learning offers a promising way to go
beyond geometry and conventional planning, allowing for navigational systems
that make decisions based on actual prior experience. Such systems can reason
about traversability in ways that go beyond geometry, accounting for the
physical outcomes of their actions and exploiting patterns in real-world
environments. They can also improve as more data is collected, potentially
providing a powerful network effect. In this article, we present a general
toolkit for experiential learning of robotic navigation skills that unifies
several recent approaches, describe the underlying design principles, summarize
experimental results from several of our recent papers, and discuss open
problems and directions for future work.",None,-1
Fuzzy granular approximation classifier,0.236033,"In this article, a new Fuzzy Granular Approximation Classifier (FGAC) is
introduced. The classifier is based on the previously introduced concept of the
granular approximation and its multi-class classification case. The classifier
is instance-based and its biggest advantage is its local transparency i.e., the
ability to explain every individual prediction it makes. We first develop the
FGAC for the binary classification case and the multi-class classification case
and we discuss its variation that includes the Ordered Weighted Average (OWA)
operators. Those variations of the FGAC are then empirically compared with
other locally transparent ML methods. At the end, we discuss the transparency
of the FGAC and its advantage over other locally transparent methods. We
conclude that while the FGAC has similar predictive performance to other
locally transparent ML models, its transparency can be superior in certain
cases.",https://github.com/markopalangetic/FGAC_experiments,28553
Meta-Learning Adversarial Bandits,0.235794,"We study online learning with bandit feedback across multiple tasks, with the
goal of improving average performance across tasks if they are similar
according to some natural task-similarity measure. As the first to target the
adversarial setting, we design a unified meta-algorithm that yields
setting-specific guarantees for two important cases: multi-armed bandits (MAB)
and bandit linear optimization (BLO). For MAB, the meta-algorithm tunes the
initialization, step-size, and entropy parameter of the Tsallis-entropy
generalization of the well-known Exp3 method, with the task-averaged regret
provably improving if the entropy of the distribution over estimated
optima-in-hindsight is small. For BLO, we learn the initialization, step-size,
and boundary-offset of online mirror descent (OMD) with self-concordant barrier
regularizers, showing that task-averaged regret varies directly with a measure
induced by these functions on the interior of the action space. Our adaptive
guarantees rely on proving that unregularized follow-the-leader combined with
multiplicative weights is enough to online learn a non-smooth and non-convex
sequence of affine functions of Bregman divergences that upper-bound the regret
of OMD.",None,-1
B-SMART: A Reference Architecture for Artificially Intelligent Autonomic Smart Buildings,0.884224,"The pervasive application of artificial intelligence and machine learning
algorithms is transforming many industries and aspects of the human experience.
One very important industry trend is the move to convert existing human
dwellings to smart buildings, and to create new smart buildings. Smart
buildings aim to mitigate climate change by reducing energy consumption and
associated carbon emissions. To accomplish this, they leverage artificial
intelligence, big data, and machine learning algorithms to learn and optimize
system performance. These fields of research are currently very rapidly
evolving and advancing, but there has been very little guidance to help
engineers and architects working on smart buildings apply artificial
intelligence algorithms and technologies in a systematic and effective manner.
In this paper we present B-SMART: the first reference architecture for
autonomic smart buildings. B-SMART facilitates the application of artificial
intelligence techniques and technologies to smart buildings by decoupling
conceptually distinct layers of functionality and organizing them into an
autonomic control loop. We also present a case study illustrating how B-SMART
can be applied to accelerate the introduction of artificial intelligence into
an existing smart building.",None,-1
Learning Disentangled Textual Representations via Statistical Measures of Similarity,0.67503,"When working with textual data, a natural application of disentangled
representations is fair classification where the goal is to make predictions
without being biased (or influenced) by sensitive attributes that may be
present in the data (e.g., age, gender or race). Dominant approaches to
disentangle a sensitive attribute from textual representations rely on learning
simultaneously a penalization term that involves either an adversarial loss
(e.g., a discriminator) or an information measure (e.g., mutual information).
However, these methods require the training of a deep neural network with
several parameter updates for each update of the representation model. As a
matter of fact, the resulting nested optimization loop is both time consuming,
adding complexity to the optimization dynamic, and requires a fine
hyperparameter selection (e.g., learning rates, architecture). In this work, we
introduce a family of regularizers for learning disentangled representations
that do not require training. These regularizers are based on statistical
measures of similarity between the conditional probability distributions with
respect to the sensitive attributes. Our novel regularizers do not require
additional training, are faster and do not involve additional tuning while
achieving better results both when combined with pretrained and randomly
initialized text encoders.",https://github.com/PierreColombo/TORNADO,-1
Omni-Granular Ego-Semantic Propagation for Self-Supervised Graph Representation Learning,0.498948,"Unsupervised/self-supervised graph representation learning is critical for
downstream node- and graph-level classification tasks. Global structure of
graphs helps discriminating representations and existing methods mainly utilize
the global structure by imposing additional supervisions. However, their global
semantics are usually invariant for all nodes/graphs and they fail to
explicitly embed the global semantics to enrich the representations. In this
paper, we propose Omni-Granular Ego-Semantic Propagation for Self-Supervised
Graph Representation Learning (OEPG). Specifically, we introduce
instance-adaptive global-aware ego-semantic descriptors, leveraging the first-
and second-order feature differences between each node/graph and hierarchical
global clusters of the entire graph dataset. The descriptors can be explicitly
integrated into local graph convolution as new neighbor nodes. Besides, we
design an omni-granular normalization on the whole scales and hierarchies of
the ego-semantic to assign attentional weight to each descriptor from an
omni-granular perspective. Specialized pretext tasks and cross-iteration
momentum update are further developed for local-global mutual adaptation. In
downstream tasks, OEPG consistently achieves the best performance with a 2%~6%
accuracy gain on multiple datasets cross scales and domains. Notably, OEPG also
generalizes to quantity- and topology-imbalance scenarios.",None,-1
4D-StOP: Panoptic Segmentation of 4D LiDAR using Spatio-temporal Object Proposal Generation and Aggregation,0.79649,"In this work, we present a new paradigm, called 4D-StOP, to tackle the task
of 4D Panoptic LiDAR Segmentation. 4D-StOP first generates spatio-temporal
proposals using voting-based center predictions, where each point in the 4D
volume votes for a corresponding center. These tracklet proposals are further
aggregated using learned geometric features. The tracklet aggregation method
effectively generates a video-level 4D scene representation over the entire
space-time volume. This is in contrast to existing end-to-end trainable
state-of-the-art approaches which use spatio-temporal embeddings that are
represented by Gaussian probability distributions. Our voting-based tracklet
generation method followed by geometric feature-based aggregation generates
significantly improved panoptic LiDAR segmentation quality when compared to
modeling the entire 4D volume using Gaussian probability distributions. 4D-StOP
achieves a new state-of-the-art when applied to the SemanticKITTI test dataset
with a score of 63.9 LSTQ, which is a large (+7%) improvement compared to
current best-performing end-to-end trainable methods. The code and pre-trained
models are available at: https://github.com/LarsKreuzberg/4D-StOP.",https://github.com/LarsKreuzberg/4D-StOP,35042
Deep Learning in Business Analytics: A Clash of Expectations and Reality,0.976897,"Our fast-paced digital economy shaped by global competition requires
increased data-driven decision-making based on artificial intelligence (AI) and
machine learning (ML). The benefits of deep learning (DL) are manifold, but it
comes with limitations that have - so far - interfered with widespread industry
adoption. This paper explains why DL - despite its popularity - has
difficulties speeding up its adoption within business analytics. It is shown -
by a mixture of content analysis and empirical study - that the adoption of
deep learning is not only affected by computational complexity, lacking big
data architecture, lack of transparency (black-box), and skill shortage, but
also by the fact that DL does not outperform traditional ML models in the case
of structured datasets with fixed-length feature vectors. Deep learning should
be regarded as a powerful addition to the existing body of ML models instead of
a one size fits all solution.",None,-1
Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control,0.739434,"Abstractive summarization systems leveraging pre-training language models
have achieved superior results on benchmark datasets. However, such models have
been shown to be more prone to hallucinate facts that are unfaithful to the
input context. In this paper, we propose a method to remedy entity-level
extrinsic hallucinations with Entity Coverage Control (ECC). We first compute
entity coverage precision and prepend the corresponding control code for each
training example, which implicitly guides the model to recognize faithfulness
contents in the training phase. We further extend our method via intermediate
fine-tuning on large but noisy data extracted from Wikipedia to unlock
zero-shot summarization. We show that the proposed method leads to more
faithful and salient abstractive summarization in supervised fine-tuning and
zero-shot settings according to our experimental results on three benchmark
datasets XSum, Pubmed, and SAMSum of very different domains and styles.",None,-1
Discovering Nonlinear PDEs from Scarce Data with Physics-encoded Learning,0.364339,"There have been growing interests in leveraging experimental measurements to
discover the underlying partial differential equations (PDEs) that govern
complex physical phenomena. Although past research attempts have achieved great
success in data-driven PDE discovery, the robustness of the existing methods
cannot be guaranteed when dealing with low-quality measurement data. To
overcome this challenge, we propose a novel physics-encoded discrete learning
framework for discovering spatiotemporal PDEs from scarce and noisy data. The
general idea is to (1) firstly introduce a novel deep convolutional-recurrent
network, which can encode prior physics knowledge (e.g., known PDE terms,
assumed PDE structure, initial/boundary conditions, etc.) while remaining
flexible on representation capability, to accurately reconstruct high-fidelity
data, and (2) perform sparse regression with the reconstructed data to identify
the explicit form of the governing PDEs. We validate our method on three
nonlinear PDE systems. The effectiveness and superiority of the proposed method
over baseline models are demonstrated.",None,-1
Federated Continual Learning for Text Classification via Selective Inter-client Transfer,0.244548,"In this work, we combine the two paradigms: Federated Learning (FL) and
Continual Learning (CL) for text classification task in cloud-edge continuum.
The objective of Federated Continual Learning (FCL) is to improve deep learning
models over life time at each client by (relevant and efficient) knowledge
transfer without sharing data. Here, we address challenges in minimizing
inter-client interference while knowledge sharing due to heterogeneous tasks
across clients in FCL setup. In doing so, we propose a novel framework,
Federated Selective Inter-client Transfer (FedSeIT) which selectively combines
model parameters of foreign clients. To further maximize knowledge transfer, we
assess domain overlap and select informative tasks from the sequence of
historical tasks at each foreign client while preserving privacy. Evaluating
against the baselines, we show improved performance, a gain of (average) 12.4\%
in text classification over a sequence of tasks using five datasets from
diverse domains. To the best of our knowledge, this is the first work that
applies FCL to NLP.",https://github.com/RaiPranav/FCL-FedSeIT,-1
Autofocusing+: Noise-Resilient Motion Correction in Magnetic Resonance Imaging,0.122949,"Image corruption by motion artifacts is an ingrained problem in Magnetic
Resonance Imaging (MRI). In this work, we propose a neural network-based
regularization term to enhance Autofocusing, a classic optimization-based
method to remove motion artifacts. The method takes the best of both worlds:
the optimization-based routine iteratively executes the blind demotion and deep
learning-based prior penalizes for unrealistic restorations and speeds up the
convergence. We validate the method on three models of motion trajectories,
using synthetic and real noisy data. The method proves resilient to noise and
anatomic structure variation, outperforming the state-of-the-art demotion
methods.",None,-1
Enhanced Physics-Informed Neural Networks with Augmented Lagrangian Relaxation Method (AL-PINNs),0.510209,"Physics-Informed Neural Networks (PINNs) have become a prominent application
of deep learning in scientific computation, as they are powerful approximators
of solutions to nonlinear partial differential equations (PDEs). There have
been numerous attempts to facilitate the training process of PINNs by adjusting
the weight of each component of the loss function, called adaptive
loss-balancing algorithms. In this paper, we propose an Augmented Lagrangian
relaxation method for PINNs (AL-PINNs). We treat the initial and boundary
conditions as constraints for the optimization problem of the PDE residual. By
employing Augmented Lagrangian relaxation, the constrained optimization problem
becomes a sequential max-min problem so that the learnable parameters $\lambda$
adaptively balance each loss component. Our theoretical analysis reveals that
the sequence of minimizers of the proposed loss functions converges to an
actual solution for the Helmholtz, viscous Burgers, and Klein--Gordon
equations. We demonstrate through various numerical experiments that AL-PINNs
yield a much smaller relative error compared with that of state-of-the-art
adaptive loss-balancing algorithms.",https://github.com/HwijaeSon/AL-PINNs,-1
SSformer: A Lightweight Transformer for Semantic Segmentation,0.212154,"It is well believed that Transformer performs better in semantic segmentation
compared to convolutional neural networks. Nevertheless, the original Vision
Transformer may lack of inductive biases of local neighborhoods and possess a
high time complexity. Recently, Swin Transformer sets a new record in various
vision tasks by using hierarchical architecture and shifted windows while being
more efficient. However, as Swin Transformer is specifically designed for image
classification, it may achieve suboptimal performance on dense prediction-based
segmentation task. Further, simply combing Swin Transformer with existing
methods would lead to the boost of model size and parameters for the final
segmentation model. In this paper, we rethink the Swin Transformer for semantic
segmentation, and design a lightweight yet effective transformer model, called
SSformer. In this model, considering the inherent hierarchical design of Swin
Transformer, we propose a decoder to aggregate information from different
layers, thus obtaining both local and global attentions. Experimental results
show the proposed SSformer yields comparable mIoU performance with
state-of-the-art models, while maintaining a smaller model size and lower
compute.",https://github.com/shiwt03/SSformer,4373
How to Fine-Tune Vision Models with SGD,0.414509,"SGD and AdamW are the two most used optimizers for fine-tuning large neural
networks in computer vision. When the two methods perform the same, SGD is
preferable because it uses less memory (12 bytes/parameter with momentum and 8
bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite
of downstream tasks, especially those with distribution shifts, we find that
fine-tuning with AdamW performs substantially better than SGD on modern Vision
Transformer and ConvNeXt models. We find that large gaps in performance between
SGD and AdamW occur when the fine-tuning gradients in the first ""embedding""
layer are much larger than in the rest of the model. Our analysis suggests an
easy fix that works consistently across datasets and models: freezing the
embedding layer (less than 1% of the parameters) leads to SGD with or without
momentum performing slightly better than AdamW while using less memory (e.g.,
on ViT-L, SGD uses 33% less GPU memory). Our insights result in
state-of-the-art accuracies on five popular distribution shift benchmarks:
WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet.",None,4318
Learning Visual Explanations for DCNN-Based Image Classifiers Using an Attention Mechanism,0.438454,"In this paper two new learning-based eXplainable AI (XAI) methods for deep
convolutional neural network (DCNN) image classifiers, called L-CAM-Fm and
L-CAM-Img, are proposed. Both methods use an attention mechanism that is
inserted in the original (frozen) DCNN and is trained to derive class
activation maps (CAMs) from the last convolutional layer's feature maps. During
training, CAMs are applied to the feature maps (L-CAM-Fm) or the input image
(L-CAM-Img) forcing the attention mechanism to learn the image regions
explaining the DCNN's outcome. Experimental evaluation on ImageNet shows that
the proposed methods achieve competitive results while requiring a single
forward pass at the inference stage. Moreover, based on the derived
explanations a comprehensive qualitative analysis is performed providing
valuable insight for understanding the reasons behind classification errors,
including possible dataset biases affecting the trained classifier.",https://github.com/eclique/RISE,6033
VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification,0.398475,"Multimodal learning from document data has achieved great success lately as
it allows to pre-train semantically meaningful features as a prior into a
learnable downstream task. In this paper, we approach the document
classification problem by learning cross-modal representations through language
and vision cues, considering intra- and inter-modality relationships. Instead
of merging features from different modalities into a joint representation
space, the proposed method exploits high-level interactions and learns relevant
semantic information from effective attention flows within and across
modalities. The proposed learning objective is devised between intra- and
inter-modality alignment tasks, where the similarity distribution per task is
computed by contracting positive sample pairs while simultaneously contrasting
negative ones in the joint representation space}. Extensive experiments on
public document classification datasets demonstrate the effectiveness and the
generality of our model on low-scale and large-scale datasets.",https://github.com/tesseract-ocr/tesseract,-1
InstaFormer: Instance-Aware Image-to-Image Translation with Transformer,0.885141,"We present a novel Transformer-based network architecture for instance-aware
image-to-image translation, dubbed InstaFormer, to effectively integrate
global- and instance-level information. By considering extracted content
features from an image as tokens, our networks discover global consensus of
content features by considering context information through a self-attention
module in Transformers. By augmenting such tokens with an instance-level
feature extracted from the content feature with respect to bounding box
information, our framework is capable of learning an interaction between object
instances and the global image, thus boosting the instance-awareness. We
replace layer normalization (LayerNorm) in standard Transformers with adaptive
instance normalization (AdaIN) to enable a multi-modal translation with style
codes. In addition, to improve the instance-awareness and translation quality
at object regions, we present an instance-level content contrastive loss
defined between input and translated image. We conduct experiments to
demonstrate the effectiveness of our InstaFormer over the latest methods and
provide extensive ablation studies.",None,-1
Efficient Adversarial Training with Robust Early-Bird Tickets,0.292621,"Adversarial training is one of the most powerful methods to improve the
robustness of pre-trained language models (PLMs). However, this approach is
typically more expensive than traditional fine-tuning because of the necessity
to generate adversarial examples via gradient descent. Delving into the
optimization process of adversarial training, we find that robust connectivity
patterns emerge in the early training phase (typically $0.15\sim0.3$ epochs),
far before parameters converge. Inspired by this finding, we dig out robust
early-bird tickets (i.e., subnetworks) to develop an efficient adversarial
training method: (1) searching for robust tickets with structured sparsity in
the early stage; (2) fine-tuning robust tickets in the remaining time. To
extract the robust tickets as early as possible, we design a ticket convergence
metric to automatically terminate the searching process. Experiments show that
the proposed efficient adversarial training method can achieve up to $7\times
\sim 13 \times$ training speedups while maintaining comparable or even better
robustness compared to the most competitive state-of-the-art adversarial
training methods.",https://github.com/WooooDyy/EarlyRobust,-1
Self-Distribution Distillation: Efficient Uncertainty Estimation,0.430207,"Deep learning is increasingly being applied in safety-critical domains. For
these scenarios it is important to know the level of uncertainty in a model's
prediction to ensure appropriate decisions are made by the system. Deep
ensembles are the de-facto standard approach to obtaining various measures of
uncertainty. However, ensembles often significantly increase the resources
required in the training and/or deployment phases. Approaches have been
developed that typically address the costs in one of these phases. In this work
we propose a novel training approach, self-distribution distillation (S2D),
which is able to efficiently train a single model that can estimate
uncertainties. Furthermore it is possible to build ensembles of these models
and apply hierarchical ensemble distillation approaches. Experiments on
CIFAR-100 showed that S2D models outperformed standard models and Monte-Carlo
dropout. Additional out-of-distribution detection experiments on LSUN, Tiny
ImageNet, SVHN showed that even a standard deep ensemble can be outperformed
using S2D based ensembles and novel distilled models.",None,-1
RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs,0.855099,"Blind face restoration is to recover a high-quality face image from unknown
degradations. As face image contains abundant contextual information, we
propose a method, RestoreFormer, which explores fully-spatial attentions to
model contextual information and surpasses existing works that use local
operators. RestoreFormer has several benefits compared to prior arts. First,
unlike the conventional multi-head self-attention in previous Vision
Transformers (ViTs), RestoreFormer incorporates a multi-head cross-attention
layer to learn fully-spatial interactions between corrupted queries and
high-quality key-value pairs. Second, the key-value pairs in ResotreFormer are
sampled from a reconstruction-oriented high-quality dictionary, whose elements
are rich in high-quality facial features specifically aimed for face
reconstruction, leading to superior restoration results. Third, RestoreFormer
outperforms advanced state-of-the-art methods on one synthetic dataset and
three real-world datasets, as well as produces images with better visual
quality.",https://github.com/wzhouxiff/RestoreFormer.git,53489
Data Contamination: From Memorization to Exploitation,0.955182,"Pretrained language models are typically trained on massive web-based
datasets, which are often ""contaminated"" with downstream test sets. It is not
clear to what extent models exploit the contaminated data for downstream tasks.
We present a principled method to study this question. We pretrain BERT models
on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune
them on the relevant task. Comparing performance between samples seen and
unseen during pretraining enables us to define and quantify levels of
memorization and exploitation. Experiments with two models and three downstream
tasks show that exploitation exists in some cases, but in others the models
memorize the contaminated data, but do not exploit it. We show that these two
measures are affected by different factors such as the number of duplications
of the contaminated data and the model size. Our results highlight the
importance of analyzing massive web-scale datasets to verify that progress in
NLP is obtained by better language understanding and not better data
exploitation.",https://github.com/schwartz-lab-NLP/data_contamination,-1
Learning to Efficiently Plan Robust Frictional Multi-Object Grasps,0.279986,"We consider a decluttering problem where multiple rigid convex polygonal
objects rest in randomly placed positions and orientations on a planar surface
and must be efficiently transported to a packing box using both single and
multi-object grasps. Prior work considered frictionless multi-object grasping.
In this paper, we introduce friction to increase the number of potential grasps
for a given group of objects, and thus increase picks per hour. We train a
neural network using real examples to plan robust multi-object grasps. In
physical experiments, we find a 13.7% increase in success rate, a 1.6x increase
in picks per hour, and a 6.3x decrease in grasp planning time compared to prior
work on multi-object grasping. Compared to single-object grasping, we find a
3.1x increase in picks per hour.",None,-1
ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking,0.985847,"We introduce ReFinED, an efficient end-to-end entity linking model which uses
fine-grained entity types and entity descriptions to perform linking. The model
performs mention detection, fine-grained entity typing, and entity
disambiguation for all mentions within a document in a single forward pass,
making it more than 60 times faster than competitive existing approaches.
ReFinED also surpasses state-of-the-art performance on standard entity linking
datasets by an average of 3.7 F1. The model is capable of generalising to
large-scale knowledge bases such as Wikidata (which has 15 times more entities
than Wikipedia) and of zero-shot entity linking. The combination of speed,
accuracy and scale makes ReFinED an effective and cost-efficient system for
extracting entities from web-scale datasets, for which the model has been
successfully deployed. Our code and pre-trained models are available at
https://github.com/alexa/ReFinED",https://github.com/alexa/ReFinED,-1
Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks,0.311146,"The field of Natural Language Processing has experienced a dramatic leap in
capabilities with the recent introduction of huge Language Models. Despite this
success, natural language problems that involve several compounded steps are
still practically unlearnable, even by the largest LMs. This complies with
experimental failures for end-to-end learning of composite problems that were
demonstrated in a variety of domains. An effective mitigation is to introduce
intermediate supervision for solving sub-tasks of the compounded problem.
Recently, several works have demonstrated high gains by taking a
straightforward approach for incorporating intermediate supervision in
compounded natural language problems: the sequence-to-sequence LM is fed with
an augmented input, in which the decomposed tasks' labels are simply
concatenated to the original input. In this paper, we prove a positive learning
result that motivates these recent efforts. We show that when concatenating
intermediate supervision to the input and training a sequence-to-sequence model
on this modified input, unlearnable composite problems can become learnable. We
show that this is true for any family of tasks which on the one hand, are
unlearnable, and on the other hand, can be decomposed into a polynomial number
of simple sub-tasks, each of which depends only on O(1) previous sub-task
results. Beyond motivating contemporary empirical efforts for incorporating
intermediate supervision in sequence-to-sequence language models, our positive
theoretical result is the first of its kind in the landscape of results on the
benefits of intermediate supervision for neural-network learning: Until now,
all theoretical results on the subject are negative, i.e., show cases where
learning is impossible without intermediate supervision, while our result is
positive, showing that learning is facilitated in the presence of intermediate
supervision.",None,-1
Data-Driven Mitigation of Adversarial Text Perturbation,0.418859,"Social networks have become an indispensable part of our lives, with billions
of people producing ever-increasing amounts of text. At such scales, content
policies and their enforcement become paramount. To automate moderation,
questionable content is detected by Natural Language Processing (NLP)
classifiers. However, high-performance classifiers are hampered by misspellings
and adversarial text perturbations. In this paper, we classify intentional and
unintentional adversarial text perturbation into ten types and propose a
deobfuscation pipeline to make NLP models robust to such perturbations. We
propose Continuous Word2Vec (CW2V), our data-driven method to learn word
embeddings that ensures that perturbations of words have embeddings similar to
those of the original words. We show that CW2V embeddings are generally more
robust to text perturbations than embeddings based on character ngrams. Our
robust classification pipeline combines deobfuscation and classification, using
proposed defense methods and word embeddings to classify whether Facebook posts
are requesting engagement such as likes. Our pipeline results in engagement
bait classification that goes from 0.70 to 0.67 AUC with adversarial text
perturbation, while character ngram-based word embedding methods result in
downstream classification that goes from 0.76 to 0.64.",https://github.com/pytorch/captum,-1
Uncertainty estimation for Cross-dataset performance in Trajectory prediction,0.633854,"While a lot of work has been carried on developing trajectory prediction
methods, and various datasets have been proposed for benchmarking this task,
little study has been done so far on the generalizability and the
transferability of these methods across dataset. In this paper, we observe the
performance of two of the latest state-of-the-art trajectory prediction methods
across four different datasets (Argoverse, NuScenes, Interaction, Shifts). This
analysis allows to gain some insights on the generalizability proprieties of
most recent trajectory prediction models and to analyze which dataset is more
representative of real driving scenes and therefore enables better
transferability. Furthermore we present a novel method to estimate prediction
uncertainty and show how it could be used to achieve better performance across
datasets.",None,-1
Spatial Transformer Network with Transfer Learning for Small-scale Fine-grained Skeleton-based Tai Chi Action Recognition,0.23342,"Human action recognition is a quite hugely investigated area where most
remarkable action recognition networks usually use large-scale coarse-grained
action datasets of daily human actions as inputs to state the superiority of
their networks. We intend to recognize our small-scale fine-grained Tai Chi
action dataset using neural networks and propose a transfer-learning method
using NTU RGB+D dataset to pre-train our network. More specifically, the
proposed method first uses a large-scale NTU RGB+D dataset to pre-train the
Transformer-based network for action recognition to extract common features
among human motion. Then we freeze the network weights except for the fully
connected (FC) layer and take our Tai Chi actions as inputs only to train the
initialized FC weights. Experimental results show that our general model
pipeline can reach a high accuracy of small-scale fine-grained Tai Chi action
recognition with even few inputs and demonstrate that our method achieves the
state-of-the-art performance compared with previous Tai Chi action recognition
methods.",None,-1
Exploring Effective Information Utilization in Multi-Turn Topic-Driven Conversations,0.416355,"Conversations are always related to certain topics. However, it is
challenging to fuse dialogue history and topic information from various sources
at the same time in current dialogue generation models because of the input
length limit of pre-trained language models (PLMs). In order to expand the
information that PLMs can utilize, we encode topic and dialogue history
information using certain prompts with multiple channels of Fusion-in-Decoder
(FiD) and explore the influence of three different channel settings. In this
paper, our experiments focus on a specific Chinese dataset named NaturalConv,
where the conversation revolves around a piece of recent news. We thoroughly
compared different dialogue models and different FiD channel settings.
Empirical results show that by combining our proposed whole passage channel
with additional history channel, our methods can achieve competitive
performance on NaturalConv, making it possible to encode various information
from excessively long texts.",https://github.com/ZhuiyiTechnology/t5-pegasus,-1
LERT: A Linguistically-motivated Pre-trained Language Model,0.919326,"Pre-trained Language Model (PLM) has become a representative foundation model
in the natural language processing field. Most PLMs are trained with
linguistic-agnostic pre-training tasks on the surface form of the text, such as
the masked language model (MLM). To further empower the PLMs with richer
linguistic features, in this paper, we aim to propose a simple but effective
way to learn linguistic features for pre-trained language models. We propose
LERT, a pre-trained language model that is trained on three types of linguistic
features along with the original MLM pre-training task, using a
linguistically-informed pre-training (LIP) strategy. We carried out extensive
experiments on ten Chinese NLU tasks, and the experimental results show that
LERT could bring significant improvements over various comparable baselines.
Furthermore, we also conduct analytical experiments in various linguistic
aspects, and the results prove that the design of LERT is valid and effective.
Resources are available at https://github.com/ymcui/LERT",https://github.com/ymcui/LERT,-1
VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction,0.804549,"The success of the Neural Radiance Fields (NeRF) in novel view synthesis has
inspired researchers to propose neural implicit scene reconstruction. However,
most existing neural implicit reconstruction methods optimize per-scene
parameters and therefore lack generalizability to new scenes. We introduce
VolRecon, a novel generalizable implicit reconstruction method with Signed Ray
Distance Function (SRDF). To reconstruct the scene with fine details and little
noise, VolRecon combines projection features aggregated from multi-view
features, and volume features interpolated from a coarse global feature volume.
Using a ray transformer, we compute SRDF values of sampled points on a ray and
then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by
about 30% in sparse view reconstruction and achieves comparable accuracy as
MVSNet in full view reconstruction. Furthermore, our approach exhibits good
generalization performance on the large-scale ETH3D benchmark.",https://github.com/IVRL/VolRecon/,-1
A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank,0.527381,"We show that the choice of pretraining languages affects downstream
cross-lingual transfer for BERT-based models. We inspect zero-shot performance
in balanced data conditions to mitigate data size confounds, classifying
pretraining languages that improve downstream performance as donors, and
languages that are improved in zero-shot performance as recipients. We develop
a method of quadratic time complexity in the number of languages to estimate
these relations, instead of an exponential exhaustive computation of all
possible combinations. We find that our method is effective on a diverse set of
languages spanning different linguistic features and two downstream tasks. Our
findings can inform developers of large-scale multilingual language models in
choosing better pretraining configurations.",https://github.com/SLAB-NLP/linguistic-blood-bank,-1
MAViL: Masked Audio-Video Learners,0.904404,"We present Masked Audio-Video Learners (MAViL) to train audio-visual
representations. Our approach learns with three complementary forms of
self-supervision: (1) reconstruction of masked audio and video input data, (2)
intra- and inter-modal contrastive learning with masking, and (3) self-training
by reconstructing joint audio-video contextualized features learned from the
first two objectives. Pre-training with MAViL not only enables the model to
perform well in audio-visual classification and retrieval tasks but also
improves representations of each modality in isolation, without using
information from the other modality for fine-tuning or inference. Empirically,
MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%
accuracy). For the first time, a self-supervised audio-visual model outperforms
ones that use external supervision on these benchmarks.",https://github.com/facebookresearch/MAViL,-1
Forming Predictive Features of Tweets for Decision-Making Support,0.0957745,"The article describes the approaches for forming different predictive
features of tweet data sets and using them in the predictive analysis for
decision-making support. The graph theory as well as frequent itemsets and
association rules theory is used for forming and retrieving different features
from these datasests. The use of these approaches makes it possible to reveal a
semantic structure in tweets related to a specified entity. It is shown that
quantitative characteristics of semantic frequent itemsets can be used in
predictive regression models with specified target variables.",None,-1
Mono-surrogate vs Multi-surrogate in Multi-objective Bayesian Optimisation,0.115205,"Bayesian optimisation (BO) has been widely used to solve problems with
expensive function evaluations. In multi-objective optimisation problems, BO
aims to find a set of approximated Pareto optimal solutions. There are
typically two ways to build surrogates in multi-objective BO: One surrogate by
aggregating objective functions (by using a scalarising function, also called
mono-surrogate approach) and multiple surrogates (for each objective function,
also called multi-surrogate approach). In both approaches, an acquisition
function (AF) is used to guide the search process. Mono-surrogate has the
advantage that only one model is used, however, the approach has two major
limitations. Firstly, the fitness landscape of the scalarising function and the
objective functions may not be similar. Secondly, the approach assumes that the
scalarising function distribution is Gaussian, and thus a closed-form
expression of the AF can be used. In this work, we overcome these limitations
by building a surrogate model for each objective function and show that the
scalarising function distribution is not Gaussian. We approximate the
distribution using Generalised extreme value distribution. The results and
comparison with existing approaches on standard benchmark and real-world
optimisation problems show the potential of the multi-surrogate approach.",None,1960
Text-Only Training for Image Captioning using Noise-Injected CLIP,0.59811,"We consider the task of image-captioning using only the CLIP model and
additional text data at training time, and no additional captioned images. Our
approach relies on the fact that CLIP is trained to make visual and textual
embeddings similar. Therefore, we only need to learn how to translate CLIP
textual embeddings back into text, and we can learn how to do this by learning
a decoder for the frozen CLIP text encoder using only text. We argue that this
intuition is ""almost correct"" because of a gap between the embedding spaces,
and propose to rectify this via noise injection during training. We demonstrate
the effectiveness of our approach by showing SOTA zero-shot image captioning
across four benchmarks, including style transfer. Code, data, and models are
available on GitHub.",https://github.com/DavidHuji/CapDec,-1
A Second Wave of UD Hebrew Treebanking and Cross-Domain Parsing,0.882681,"Foundational Hebrew NLP tasks such as segmentation, tagging and parsing, have
relied to date on various versions of the Hebrew Treebank (HTB, Sima'an et al.
2001). However, the data in HTB, a single-source newswire corpus, is now over
30 years old, and does not cover many aspects of contemporary Hebrew on the
web. This paper presents a new, freely available UD treebank of Hebrew
stratified from a range of topics selected from Hebrew Wikipedia. In addition
to introducing the corpus and evaluating the quality of its annotations, we
deploy automatic validation tools based on grew (Guillaume, 2021), and conduct
the first cross domain parsing experiments in Hebrew. We obtain new
state-of-the-art (SOTA) results on UD NLP tasks, using a combination of the
latest language modelling and some incremental improvements to existing
transformer based approaches. We also release a new version of the UD HTB
matching annotation scheme updates from our new corpus.",https://github.com/amir-zeldes/HebPipe/,-1
Dual-Geometric Space Embedding Model for Two-View Knowledge Graphs,0.692489,"Two-view knowledge graphs (KGs) jointly represent two components: an ontology
view for abstract and commonsense concepts, and an instance view for specific
entities that are instantiated from ontological concepts. As such, these KGs
contain heterogeneous structures that are hierarchical, from the ontology-view,
and cyclical, from the instance-view. Despite these various structures in KGs,
most recent works on embedding KGs assume that the entire KG belongs to only
one of the two views but not both simultaneously. For works that seek to put
both views of the KG together, the instance and ontology views are assumed to
belong to the same geometric space, such as all nodes embedded in the same
Euclidean space or non-Euclidean product space, an assumption no longer
reasonable for two-view KGs where different portions of the graph exhibit
different structures. To address this issue, we define and construct a
dual-geometric space embedding model (DGS) that models two-view KGs using a
complex non-Euclidean geometric space, by embedding different portions of the
KG in different geometric spaces. DGS utilizes the spherical space, hyperbolic
space, and their intersecting space in a unified framework for learning
embeddings. Furthermore, for the spherical space, we propose novel closed
spherical space operators that directly operate in the spherical space without
the need for mapping to an approximate tangent space. Experiments on public
datasets show that DGS significantly outperforms previous state-of-the-art
baseline models on KG completion tasks, demonstrating its ability to better
model heterogeneous structures in KGs.",https://github.com/roshnigiyer/dgs,-1
Controllable Dynamic Multi-Task Architectures,0.549198,"Multi-task learning commonly encounters competition for resources among
tasks, specifically when model capacity is limited. This challenge motivates
models which allow control over the relative importance of tasks and total
compute cost during inference time. In this work, we propose such a
controllable multi-task network that dynamically adjusts its architecture and
weights to match the desired task preference as well as the resource
constraints. In contrast to the existing dynamic multi-task approaches that
adjust only the weights within a fixed architecture, our approach affords the
flexibility to dynamically control the total computational cost and match the
user-preferred task importance better. We propose a disentangled training of
two hypernetworks, by exploiting task affinity and a novel branching
regularized loss, to take input preferences and accordingly predict
tree-structured models with adapted weights. Experiments on three multi-task
benchmarks, namely PASCAL-Context, NYU-v2, and CIFAR-100, show the efficacy of
our approach. Project page is available at https://www.nec-labs.com/~mas/DYMU.",https://www.nec-labs.com/˜mas/DYMU,-1
Mining Error Templates for Grammatical Error Correction,0.222005,"Some grammatical error correction (GEC) systems incorporate hand-crafted
rules and achieve positive results. However, manually defining rules is
time-consuming and laborious. In view of this, we propose a method to mine
error templates for GEC automatically. An error template is a regular
expression aiming at identifying text errors. We use the web crawler to acquire
such error templates from the Internet. For each template, we further select
the corresponding corrective action by using the language model perplexity as a
criterion. We have accumulated 1,119 error templates for Chinese GEC based on
this method. Experimental results on the newly proposed CTC-2021 Chinese GEC
benchmark show that combing our error templates can effectively improve the
performance of a strong GEC system, especially on two error types with very
little training data. Our error templates are available at
\url{https://github.com/HillZhang1999/gec_error_template}.",https://github.com/HillZhang1999/gec_error_template,-1
Neural Knowledge Bank for Pretrained Transformers,0.483295,"The ability of pretrained Transformers to remember factual knowledge is
essential but still limited for existing models. Inspired by existing work that
regards Feed-Forward Networks (FFNs) in Transformers as key-value memories, we
design a Neural Knowledge Bank (NKB) and a knowledge injection strategy to
introduce extra factual knowledge for pretrained Transformers. The NKB is in
the form of additional knowledgeable memory slots to the FFN and the
memory-like architecture makes it highly interpretable and flexible. When
injecting extra knowledge with the Salient Span Masking (SSM) pretraining
objective, we fix the original pretrained model and train only the NKB. This
training strategy makes sure the general language modeling ability of the
original pretrained model is not influenced. By mounting the NKB onto the T5
model, we verify its strong ability to store extra factual knowledge based on
three closed-book question answering datasets. Also, we prove that mounting the
NKB will not degrade the general language modeling ability of T5 through two
representative tasks, summarization and machine translation. Further, we
thoroughly analyze the interpretability of the NKB and reveal the meaning of
its keys and values in a human-readable way. Finally, we show the flexibility
of the NKB by directly modifying its value vectors to update the factual
knowledge stored in it.",https://github.com/huggingface/transformers,-1
Explainability as statistical inference,0.0428235,"A wide variety of model explanation approaches have been proposed in recent
years, all guided by very different rationales and heuristics. In this paper,
we take a new route and cast interpretability as a statistical inference
problem. We propose a general deep probabilistic model designed to produce
interpretable predictions. The model parameters can be learned via maximum
likelihood, and the method can be adapted to any predictor network architecture
and any type of prediction problem. Our method is a case of amortized
interpretability models, where a neural network is used as a selector to allow
for fast interpretation at inference time. Several popular interpretability
methods are shown to be particular cases of regularised maximum likelihood for
our general model. We propose new datasets with ground truth selection which
allow for the evaluation of the features importance map. Using these datasets,
we show experimentally that using multiple imputation provides more reasonable
interpretations.",None,1641
COMET-QE and Active Learning for Low-Resource Machine Translation,0.275136,"Active learning aims to deliver maximum benefit when resources are scarce. We
use COMET-QE, a reference-free evaluation metric, to select sentences for
low-resource neural machine translation. Using Swahili, Kinyarwanda and Spanish
for our experiments, we show that COMET-QE significantly outperforms two
variants of Round Trip Translation Likelihood (RTTL) and random sentence
selection by up to 5 BLEU points for 20k sentences selected by Active Learning
on a 30k baseline. This suggests that COMET-QE is a powerful tool for sentence
selection in the very low-resource limit.",None,-1
Deep Probabilistic Graph Matching,0.189323,"Most previous learning-based graph matching algorithms solve the
\textit{quadratic assignment problem} (QAP) by dropping one or more of the
matching constraints and adopting a relaxed assignment solver to obtain
sub-optimal correspondences. Such relaxation may actually weaken the original
graph matching problem, and in turn hurt the matching performance. In this
paper we propose a deep learning-based graph matching framework that works for
the original QAP without compromising on the matching constraints. In
particular, we design an affinity-assignment prediction network to jointly
learn the pairwise affinity and estimate the node assignments, and we then
develop a differentiable solver inspired by the probabilistic perspective of
the pairwise affinities. Aiming to obtain better matching results, the
probabilistic solver refines the estimated assignments in an iterative manner
to impose both discrete and one-to-one matching constraints. The proposed
method is evaluated on three popularly tested benchmarks (Pascal VOC, Willow
Object and SPair-71k), and it outperforms all previous state-of-the-arts on all
benchmarks.",https://github.com/Thinklab-SJTU/ThinkMatch,-1
Deep Surrogate Assisted Generation of Environments,0.795734,"Recent progress in reinforcement learning (RL) has started producing
generally capable agents that can solve a distribution of complex environments.
These agents are typically tested on fixed, human-authored environments. On the
other hand, quality diversity (QD) optimization has been proven to be an
effective component of environment generation algorithms, which can generate
collections of high-quality environments that are diverse in the resulting
agent behaviors. However, these algorithms require potentially expensive
simulations of agents on newly generated environments. We propose Deep
Surrogate Assisted Generation of Environments (DSAGE), a sample-efficient QD
environment generation algorithm that maintains a deep surrogate model for
predicting agent behaviors in new environments. Results in two benchmark
domains show that DSAGE significantly outperforms existing QD environment
generation algorithms in discovering collections of environments that elicit
diverse behaviors of a state-of-the-art RL agent and a planning agent. Our
source code and videos are available at https://dsagepaper.github.io/.",https://dsagepaper.github.io/,3297
Preserving Semantics in Textual Adversarial Attacks,0.250709,"The growth of hateful online content, or hate speech, has been associated
with a global increase in violent crimes against minorities [23]. Harmful
online content can be produced easily, automatically and anonymously. Even
though, some form of auto-detection is already achieved through text
classifiers in NLP, they can be fooled by adversarial attacks. To strengthen
existing systems and stay ahead of attackers, we need better adversarial
attacks. In this paper, we show that up to 70% of adversarial examples
generated by adversarial attacks should be discarded because they do not
preserve semantics. We address this core weakness and propose a new, fully
supervised sentence embedding technique called Semantics-Preserving-Encoder
(SPE). Our method outperforms existing sentence encoders used in adversarial
attacks by achieving 1.2x - 5.1x better real attack success rate. We release
our code as a plugin that can be used in any existing adversarial attack to
improve its quality and speed up its execution.",https://github.com/DavidHerel/semantics-preserving-encoder,-1
SS-VAERR: Self-Supervised Apparent Emotional Reaction Recognition from Video,0.135303,"This work focuses on the apparent emotional reaction recognition (AERR) from
the video-only input, conducted in a self-supervised fashion. The network is
first pre-trained on different self-supervised pretext tasks and later
fine-tuned on the downstream target task. Self-supervised learning facilitates
the use of pre-trained architectures and larger datasets that might be deemed
unfit for the target task and yet might be useful to learn informative
representations and hence provide useful initializations for further
fine-tuning on smaller more suitable data. Our presented contribution is
two-fold: (1) an analysis of different state-of-the-art (SOTA) pretext tasks
for the video-only apparent emotional reaction recognition architecture, and
(2) an analysis of various combinations of the regression and classification
losses that are likely to improve the performance further. Together these two
contributions result in the current state-of-the-art performance for the
video-only spontaneous apparent emotional reaction recognition with continuous
annotations.",None,-1
Learning to Answer Questions in Dynamic Audio-Visual Scenarios,0.970529,"In this paper, we focus on the Audio-Visual Question Answering (AVQA) task,
which aims to answer questions regarding different visual objects, sounds, and
their associations in videos. The problem requires comprehensive multimodal
understanding and spatio-temporal reasoning over audio-visual scenes. To
benchmark this task and facilitate our study, we introduce a large-scale
MUSIC-AVQA dataset, which contains more than 45K question-answer pairs covering
33 different question templates spanning over different modalities and question
types. We develop several baselines and introduce a spatio-temporal grounded
audio-visual network for the AVQA problem. Our results demonstrate that AVQA
benefits from multisensory perception and our model outperforms recent A-, V-,
and AVQA approaches. We believe that our built dataset has the potential to
serve as testbed for evaluating and promoting progress in audio-visual scene
understanding and spatio-temporal reasoning. Code and dataset:
http://gewu-lab.github.io/MUSIC-AVQA/",http://gewu-lab.github.io/MUSIC-AVQA/,-1
Natural Language Inference Prompts for Zero-shot Emotion Classification in Text across Corpora,0.316883,"Within textual emotion classification, the set of relevant labels depends on
the domain and application scenario and might not be known at the time of model
development. This conflicts with the classical paradigm of supervised learning
in which the labels need to be predefined. A solution to obtain a model with a
flexible set of labels is to use the paradigm of zero-shot learning as a
natural language inference task, which in addition adds the advantage of not
needing any labeled training data. This raises the question how to prompt a
natural language inference model for zero-shot learning emotion classification.
Options for prompt formulations include the emotion name anger alone or the
statement ""This text expresses anger"". With this paper, we analyze how
sensitive a natural language inference-based zero-shot-learning classifier is
to such changes to the prompt under consideration of the corpus: How carefully
does the prompt need to be selected? We perform experiments on an established
set of emotion datasets presenting different language registers according to
different sources (tweets, events, blogs) with three natural language inference
models and show that indeed the choice of a particular prompt formulation needs
to fit to the corpus. We show that this challenge can be tackled with
combinations of multiple prompts. Such ensemble is more robust across corpora
than individual prompts and shows nearly the same performance as the individual
best prompt for a particular corpus.",https://github.com/fmplaza/zsl_nli_emotion_prompts,-1
Contrast-Phys: Unsupervised Video-based Remote Physiological Measurement via Spatiotemporal Contrast,0.999793,"Video-based remote physiological measurement utilizes face videos to measure
the blood volume change signal, which is also called remote
photoplethysmography (rPPG). Supervised methods for rPPG measurements achieve
state-of-the-art performance. However, supervised rPPG methods require face
videos and ground truth physiological signals for model training. In this
paper, we propose an unsupervised rPPG measurement method that does not require
ground truth signals for training. We use a 3DCNN model to generate multiple
rPPG signals from each video in different spatiotemporal locations and train
the model with a contrastive loss where rPPG signals from the same video are
pulled together while those from different videos are pushed away. We test on
five public datasets, including RGB videos and NIR videos. The results show
that our method outperforms the previous unsupervised baseline and achieves
accuracies very close to the current best supervised rPPG methods on all five
datasets. Furthermore, we also demonstrate that our approach can run at a much
faster speed and is more robust to noises than the previous unsupervised
baseline. Our code is available at
https://github.com/zhaodongsun/contrast-phys.",https://github.com/zhaodongsun/contrast-phys,-1
A Multimodal Corpus for Emotion Recognition in Sarcasm,0.757536,"While sentiment and emotion analysis have been studied extensively, the
relationship between sarcasm and emotion has largely remained unexplored. A
sarcastic expression may have a variety of underlying emotions. For example, ""I
love being ignored"" belies sadness, while ""my mobile is fabulous with a battery
backup of only 15 minutes!"" expresses frustration. Detecting the emotion behind
a sarcastic expression is non-trivial yet an important task. We undertake the
task of detecting the emotion in a sarcastic statement, which to the best of
our knowledge, is hitherto unexplored. We start with the recently released
multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions.
We identify and correct 343 incorrect emotion labels (out of 690). We double
the size of the dataset, label it with emotions along with valence and arousal
which are important indicators of emotional intensity. Finally, we label each
sarcastic utterance with one of the four sarcasm types-Propositional, Embedded,
Likeprefixed and Illocutionary, with the goal of advancing sarcasm detection
research. Exhaustive experimentation with multimodal (text, audio, and video)
fusion models establishes a benchmark for exact emotion recognition in sarcasm
and outperforms the state-of-art sarcasm detection. We release the dataset
enriched with various annotations and the code for research purposes:
https://github.com/apoorva-nunna/MUStARD_Plus_Plus",https://github.com/apoorva-nunna/MUStARD_Plus_Plus,-1
ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks,0.0419964,"Causal chain reasoning (CCR) is an essential ability for many decision-making
AI systems, which requires the model to build reliable causal chains by
connecting causal pairs. However, CCR suffers from two main transitive
problems: threshold effect and scene drift. In other words, the causal pairs to
be spliced may have a conflicting threshold boundary or scenario. To address
these issues, we propose a novel Reliable Causal chain reasoning
framework~(ReCo), which introduces exogenous variables to represent the
threshold and scene factors of each causal pair within the causal chain, and
estimates the threshold and scene contradictions across exogenous variables via
structural causal recurrent neural networks~(SRNN). Experiments show that ReCo
outperforms a series of strong baselines on both Chinese and English CCR
datasets. Moreover, by injecting reliable causal chain knowledge distilled by
ReCo, BERT can achieve better performances on four downstream causal-related
tasks than BERT models enhanced by other kinds of knowledge.",https://github.com/Waste-Wood/ReCo,-1
Synthetic Disinformation Attacks on Automated Fact Verification Systems,0.908585,"Automated fact-checking is a needed technology to curtail the spread of
online misinformation. One current framework for such solutions proposes to
verify claims by retrieving supporting or refuting evidence from related
textual sources. However, the realistic use cases for fact-checkers will
require verifying claims against evidence sources that could be affected by the
same misinformation. Furthermore, the development of modern NLP tools that can
produce coherent, fabricated content would allow malicious actors to
systematically generate adversarial disinformation for fact-checkers.
  In this work, we explore the sensitivity of automated fact-checkers to
synthetic adversarial evidence in two simulated settings: AdversarialAddition,
where we fabricate documents and add them to the evidence repository available
to the fact-checking system, and AdversarialModification, where existing
evidence source documents in the repository are automatically altered. Our
study across multiple models on three benchmarks demonstrates that these
systems suffer significant performance drops against these attacks. Finally, we
discuss the growing threat of modern NLG systems as generators of
disinformation in the context of the challenges they pose to automated
fact-checkers.",https://github.com/Yibing-Du/adversarial-factcheck,-1
Multi-Target Active Object Tracking with Monte Carlo Tree Search and Target Motion Modeling,0.207798,"In this work, we are dedicated to multi-target active object tracking (AOT),
where there are multiple targets as well as multiple cameras in the
environment. The goal is maximize the overall target coverage of all cameras.
Previous work makes a strong assumption that each camera is fixed in a location
and only allowed to rotate, which limits its application. In this work, we
relax the setting by allowing all cameras to both move along the boundary lines
and rotate. In our setting, the action space becomes much larger, which leads
to much higher computational complexity to identify the optimal action. To this
end, we propose to leverage the action selection from multi-agent reinforcement
learning (MARL) network to prune the search tree of Monte Carlo Tree Search
(MCTS) method, so as to find the optimal action more efficiently. Besides, we
model the motion of the targets to predict the future position of the targets,
which makes a better estimation of the future environment state in the MCTS
process. We establish a multi-target 2D environment to simulate the sports
games, and experimental results demonstrate that our method can effectively
improve the target coverage.",None,-1
Efficient Speech Translation with Pre-trained Models,0.131497,"When building state-of-the-art speech translation models, the need for large
computational resources is a significant obstacle due to the large training
data size and complex models. The availability of pre-trained models is a
promising opportunity to build strong speech translation systems efficiently.
In a first step, we investigate efficient strategies to build cascaded and
end-to-end speech translation systems based on pre-trained models. Using this
strategy, we can train and apply the models on a single GPU. While the
end-to-end models show superior translation performance to cascaded ones, the
application of this technology has a limitation on the need for additional
end-to-end training data. In a second step, we proposed an additional
similarity loss to encourage the model to generate similar hidden
representations for speech and transcript. Using this technique, we can
increase the data efficiency and improve the translation quality by 6 BLEU
points in scenarios with limited end-to-end training data.",None,-1
BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model,0.668824,"Pretrained language models have served as important backbones for natural
language processing. Recently, in-domain pretraining has been shown to benefit
various domain-specific downstream tasks. In the biomedical domain, natural
language generation (NLG) tasks are of critical importance, while understudied.
Approaching natural language understanding (NLU) tasks as NLG achieves
satisfying performance in the general domain through constrained language
generation or language prompting. We emphasize the lack of in-domain generative
language models and the unsystematic generative downstream benchmarks in the
biomedical domain, hindering the development of the research community. In this
work, we introduce the generative language model BioBART that adapts BART to
the biomedical domain. We collate various biomedical language generation tasks
including dialogue, summarization, entity linking, and named entity
recognition. BioBART pretrained on PubMed abstracts has enhanced performance
compared to BART and set strong baselines on several tasks. Furthermore, we
conduct ablation studies on the pretraining tasks for BioBART and find that
sentence permutation has negative effects on downstream tasks.",https://github.com/GanjinZero/BioBART,-1
AdvDO: Realistic Adversarial Attacks for Trajectory Prediction,0.972395,"Trajectory prediction is essential for autonomous vehicles (AVs) to plan
correct and safe driving behaviors. While many prior works aim to achieve
higher prediction accuracy, few study the adversarial robustness of their
methods. To bridge this gap, we propose to study the adversarial robustness of
data-driven trajectory prediction systems. We devise an optimization-based
adversarial attack framework that leverages a carefully-designed differentiable
dynamic model to generate realistic adversarial trajectories. Empirically, we
benchmark the adversarial robustness of state-of-the-art prediction models and
show that our attack increases the prediction error for both general metrics
and planning-aware metrics by more than 50% and 37%. We also show that our
attack can lead an AV to drive off road or collide into other vehicles in
simulation. Finally, we demonstrate how to mitigate the adversarial attacks
using an adversarial training scheme.",https://robustav.github.io/RobustPred,-1
Graph Neural Network Policies and Imitation Learning for Multi-Domain Task-Oriented Dialogues,0.0553779,"Task-oriented dialogue systems are designed to achieve specific goals while
conversing with humans. In practice, they may have to handle simultaneously
several domains and tasks. The dialogue manager must therefore be able to take
into account domain changes and plan over different domains/tasks in order to
deal with multidomain dialogues. However, learning with reinforcement in such
context becomes difficult because the state-action dimension is larger while
the reward signal remains scarce. Our experimental results suggest that
structured policies based on graph neural networks combined with different
degrees of imitation learning can effectively handle multi-domain dialogues.
The reported experiments underline the benefit of structured policies over
standard policies.",None,3446
Enriching Relation Extraction with OpenIE,0.0893831,"Relation extraction (RE) is a sub-discipline of information extraction (IE)
which focuses on the prediction of a relational predicate from a
natural-language input unit (such as a sentence, a clause, or even a short
paragraph consisting of multiple sentences and/or clauses). Together with
named-entity recognition (NER) and disambiguation (NED), RE forms the basis for
many advanced IE tasks such as knowledge-base (KB) population and verification.
In this work, we explore how recent approaches for open information extraction
(OpenIE) may help to improve the task of RE by encoding structured information
about the sentences' principal units, such as subjects, objects, verbal
phrases, and adverbials, into various forms of vectorized (and hence
unstructured) representations of the sentences. Our main conjecture is that the
decomposition of long and possibly convoluted sentences into multiple smaller
clauses via OpenIE even helps to fine-tune context-sensitive language models
such as BERT (and its plethora of variants) for RE. Our experiments over two
annotated corpora, KnowledgeNet and FewRel, demonstrate the improved accuracy
of our enriched models compared to existing RE approaches. Our best results
reach 92% and 71% of F1 score for KnowledgeNet and FewRel, respectively,
proving the effectiveness of our approach on competitive benchmarks.",https://github.com/dair-iitd/OpenIE-standalone,-1
Considerations for meaningful sign language machine translation based on glosses,0.678136,"Automatic sign language processing is gaining popularity in Natural Language
Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in
particular, sign language translation based on glosses is a prominent approach.
In this paper, we review recent works on neural gloss translation. We find that
limitations of glosses in general and limitations of specific datasets are not
discussed in a transparent manner and that there is no common standard for
evaluation.
  To address these issues, we put forward concrete recommendations for future
research on gloss translation. Our suggestions advocate awareness of the
inherent limitations of gloss-based approaches, realistic datasets, stronger
baselines and convincing evaluation.",None,-1
Scalable Joint Learning of Wireless Multiple-Access Policies and their Signaling,0.448471,"In this paper, we apply an multi-agent reinforcement learning (MARL)
framework allowing the base station (BS) and the user equipments (UEs) to
jointly learn a channel access policy and its signaling in a wireless multiple
access scenario. In this framework, the BS and UEs are reinforcement learning
(RL) agents that need to cooperate in order to deliver data. The comparison
with a contention-free and a contention-based baselines shows that our
framework achieves a superior performance in terms of goodput even in high
traffic situations while maintaining a low collision rate. The scalability of
the proposed method is studied, since it is a major problem in MARL and this
paper provides the first results in order to address it.",None,-1
Federated Non-negative Matrix Factorization for Short Texts Topic Modeling with Mutual Information,0.124915,"Non-negative matrix factorization (NMF) based topic modeling is widely used
in natural language processing (NLP) to uncover hidden topics of short text
documents. Usually, training a high-quality topic model requires large amount
of textual data. In many real-world scenarios, customer textual data should be
private and sensitive, precluding uploading to data centers. This paper
proposes a Federated NMF (FedNMF) framework, which allows multiple clients to
collaboratively train a high-quality NMF based topic model with locally stored
data. However, standard federated learning will significantly undermine the
performance of topic models in downstream tasks (e.g., text classification)
when the data distribution over clients is heterogeneous. To alleviate this
issue, we further propose FedNMF+MI, which simultaneously maximizes the mutual
information (MI) between the count features of local texts and their topic
weight vectors to mitigate the performance degradation. Experimental results
show that our FedNMF+MI methods outperform Federated Latent Dirichlet
Allocation (FedLDA) and the FedNMF without MI methods for short texts by a
significant margin on both coherence score and classification F1 score.",None,-1
Maximum Likelihood Uncertainty Estimation: Robustness to Outliers,0.18043,"We benchmark the robustness of maximum likelihood based uncertainty
estimation methods to outliers in training data for regression tasks. Outliers
or noisy labels in training data results in degraded performances as well as
incorrect estimation of uncertainty. We propose the use of a heavy-tailed
distribution (Laplace distribution) to improve the robustness to outliers. This
property is evaluated using standard regression benchmarks and on a
high-dimensional regression task of monocular depth estimation, both containing
outliers. In particular, heavy-tailed distribution based maximum likelihood
provides better uncertainty estimates, better separation in uncertainty for
out-of-distribution data, as well as better detection of adversarial attacks in
the presence of outliers.",https://github.com/deebuls/uncertainty-robustness-deep-regression-benchmark,-1
Reasoning Circuits: Few-shot Multihop Question Generation with Structured Rationales,0.0661085,"Multi-hop Question Generation is the task of generating questions which
require the reader to reason over and combine information spread across
multiple passages using several reasoning steps. Chain-of-thought rationale
generation has been shown to improve performance on multi-step reasoning tasks
and make model predictions more interpretable. However, few-shot performance
gains from including rationales have been largely observed only in +100B
language models, and otherwise require large scale manual rationale annotation.
In this work, we introduce a new framework for applying chain-of-thought
inspired structured rationale generation to multi-hop question generation under
a very low supervision regime (8- to 128-shot). We propose to annotate a small
number of examples following our proposed multi-step rationale schema, treating
each reasoning step as a separate task to be performed by a generative language
model. We show that our framework leads to improved control over the difficulty
of the generated questions and better performance compared to baselines trained
without rationales, both on automatic evaluation metrics and in human
evaluation. Importantly, we show that this is achievable with a modest model
size.",https://github.com/google-research/text-to-text-transfer-transformer,-1
"GO-Surf: Neural Feature Grid Optimization for Fast, High-Fidelity RGB-D Surface Reconstruction",0.977338,"We present GO-Surf, a direct feature grid optimization method for accurate
and fast surface reconstruction from RGB-D sequences. We model the underlying
scene with a learned hierarchical feature voxel grid that encapsulates
multi-level geometric and appearance local information. Feature vectors are
directly optimized such that after being tri-linearly interpolated, decoded by
two shallow MLPs into signed distance and radiance values, and rendered via
surface volume rendering, the discrepancy between synthesized and observed
RGB/depth values is minimized. Our supervision signals -- RGB, depth and
approximate SDF -- can be obtained directly from input images without any need
for fusion or post-processing. We formulate a novel SDF gradient regularization
term that encourages surface smoothness and hole filling while maintaining high
frequency details. GO-Surf can optimize sequences of $1$-$2$K frames in
$15$-$45$ minutes, a speedup of $\times60$ over NeuralRGB-D, the most related
approach based on an MLP representation, while maintaining on par performance
on standard benchmarks. Project page: https://jingwenwang95.github.io/go_surf/",https://jingwenwang95.github.io/go_surf,6103
BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog,0.897849,"A typical end-to-end task-oriented dialog system transfers context into
dialog state, and upon which generates a response, which usually faces the
problem of error propagation from both previously generated inaccurate dialog
states and responses, especially in low-resource scenarios. To alleviate these
issues, we propose BORT, a back and denoising reconstruction approach for
end-to-end task-oriented dialog system. Squarely, to improve the accuracy of
dialog states, back reconstruction is used to reconstruct the original input
context from the generated dialog states since inaccurate dialog states cannot
recover the corresponding input context. To enhance the denoising capability of
the model to reduce the impact of error propagation, denoising reconstruction
is used to reconstruct the corrupted dialog state and response. Extensive
experiments conducted on MultiWOZ 2.0 and CamRest676 show the effectiveness of
BORT. Furthermore, BORT demonstrates its advanced capabilities in the zero-shot
domain and low-resource scenarios.",https://github.com/,-1
Cross-Lingual Text Classification with Multilingual Distillation and Zero-Shot-Aware Training,0.106865,"Multilingual pre-trained language models (MPLMs) not only can handle tasks in
different languages but also exhibit surprising zero-shot cross-lingual
transferability. However, MPLMs usually are not able to achieve comparable
supervised performance on rich-resource languages compared to the
state-of-the-art monolingual pre-trained models. In this paper, we aim to
improve the multilingual model's supervised and zero-shot performance
simultaneously only with the resources from supervised languages. Our approach
is based on transferring knowledge from high-performance monolingual models
with a teacher-student framework. We let the multilingual model learn from
multiple monolingual models simultaneously. To exploit the model's
cross-lingual transferability, we propose MBLM (multi-branch multilingual
language model), a model built on the MPLMs with multiple language branches.
Each branch is a stack of transformers. MBLM is trained with the
zero-shot-aware training strategy that encourages the model to learn from the
mixture of zero-shot representations from all the branches. The results on two
cross-lingual classification tasks show that, with only the task's supervised
data used, our method improves both the supervised and zero-shot performance of
MPLMs.",None,-1
Federated Learning: Balancing the Thin Line Between Data Intelligence and Privacy,0.0987622,"Federated learning holds great promise in learning from fragmented sensitive
data and has revolutionized how machine learning models are trained. This
article provides a systematic overview and detailed taxonomy of federated
learning. We investigate the existing security challenges in federated learning
and provide a comprehensive overview of established defense techniques for data
poisoning, inference attacks, and model poisoning attacks. The work also
presents an overview of current training challenges for federated learning,
focusing on handling non-i.i.d. data, high dimensionality issues, and
heterogeneous architecture, and discusses several solutions for the associated
challenges. Finally, we discuss the remaining challenges in managing federated
learning training and suggest focused research directions to address the open
questions. Potential candidate areas for federated learning, including IoT
ecosystem, healthcare applications, are discussed with a particular focus on
banking and financial domains.",None,-1
AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach,0.534173,"Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).
It is a well-studied topic in several resource-rich languages. However, the
development of computational linguistic resources is still in its infancy
despite the existence of numerous languages that are historically and literary
rich. Assamese, an Indian scheduled language, spoken by more than 25 million
people, falls under this category. In this paper, we present a Deep Learning
(DL)-based POS tagger for Assamese. The development process is divided into two
stages. In the first phase, several pre-trained word embeddings are employed to
train several tagging models. This allows us to evaluate the performance of the
word embeddings in the POS tagging task. The top-performing model from the
first phase is employed to annotate another set of new sentences. In the second
phase, the model is trained further using the fresh dataset. Finally, we attain
a tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for
further study on DL-based Assamese POS tagging.",None,-1
DIVA-DAF: A Deep Learning Framework for Historical Document Image Analysis,0.170643,"Deep learning methods have shown strong performance in solving tasks for
historical document image analysis. However, despite current libraries and
frameworks, programming an experiment or a set of experiments and executing
them can be time-consuming. This is why we propose an open-source deep learning
framework, DIVA-DAF, which is based on PyTorch Lightning and specifically
designed for historical document analysis. Pre-implemented tasks such as
segmentation and classification can be easily used or customized. It is also
easy to create one's own tasks with the benefit of powerful modules for loading
data, even large data sets, and different forms of ground truth. The
applications conducted have demonstrated time savings for the programming of a
document analysis task, as well as for different scenarios such as pre-training
or changing the architecture. Thanks to its data module, the framework also
allows to reduce the time of model training significantly.",https://github.com/DIVA-DIA/DIVA-DAF,-1
SentBS: Sentence-level Beam Search for Controllable Summarization,0.146521,"A wide range of control perspectives have been explored in controllable text
generation. Structure-controlled summarization is recently proposed as a useful
and interesting research direction. However, current structure-controlling
methods have limited effectiveness in enforcing the desired structure. To
address this limitation, we propose a sentence-level beam search generation
method (SentBS), where evaluation is conducted throughout the generation
process to select suitable sentences for subsequent generations. We experiment
with different combinations of decoding methods to be used as subcomponents by
SentBS and evaluate results on the structure-controlled dataset MReD.
Experiments show that all explored combinations for SentBS can improve the
agreement between the generated text and the desired structure, with the best
method significantly reducing the structural discrepancies suffered by the
existing model, by approximately 68%.",https://github.com/Shen-Chenhui/SentBS,-1
Context Sensing Attention Network for Video-based Person Re-identification,0.31867,"Video-based person re-identification (ReID) is challenging due to the
presence of various interferences in video frames. Recent approaches handle
this problem using temporal aggregation strategies. In this work, we propose a
novel Context Sensing Attention Network (CSA-Net), which improves both the
frame feature extraction and temporal aggregation steps. First, we introduce
the Context Sensing Channel Attention (CSCA) module, which emphasizes responses
from informative channels for each frame. These informative channels are
identified with reference not only to each individual frame, but also to the
content of the entire sequence. Therefore, CSCA explores both the individuality
of each frame and the global context of the sequence. Second, we propose the
Contrastive Feature Aggregation (CFA) module, which predicts frame weights for
temporal aggregation. Here, the weight for each frame is determined in a
contrastive manner: i.e., not only by the quality of each individual frame, but
also by the average quality of the other frames in a sequence. Therefore, it
effectively promotes the contribution of relatively good frames. Extensive
experimental results on four datasets show that CSA-Net consistently achieves
state-of-the-art performance.",None,-1
Fine-Grained Visual Classification using Self Assessment Classifier,0.39428,"Extracting discriminative features plays a crucial role in the fine-grained
visual classification task. Most of the existing methods focus on developing
attention or augmentation mechanisms to achieve this goal. However, addressing
the ambiguity in the top-k prediction classes is not fully investigated. In
this paper, we introduce a Self Assessment Classifier, which simultaneously
leverages the representation of the image and top-k prediction classes to
reassess the classification results. Our method is inspired by continual
learning with coarse-grained and fine-grained classifiers to increase the
discrimination of features in the backbone and produce attention maps of
informative areas on the image. In practice, our method works as an auxiliary
branch and can be easily integrated into different architectures. We show that
by effectively addressing the ambiguity in the top-k prediction classes, our
method achieves new state-of-the-art results on CUB200-2011, Stanford Dog, and
FGVC Aircraft datasets. Furthermore, our method also consistently improves the
accuracy of different existing fine-grained classifiers with a unified setup.",https://github.com/aioz-ai/SAC,-1
Accelerating Shapley Explanation via Contributive Cooperator Selection,0.619467,"Even though Shapley value provides an effective explanation for a DNN model
prediction, the computation relies on the enumeration of all possible input
feature coalitions, which leads to the exponentially growing complexity. To
address this problem, we propose a novel method SHEAR to significantly
accelerate the Shapley explanation for DNN models, where only a few coalitions
of input features are involved in the computation. The selection of the feature
coalitions follows our proposed Shapley chain rule to minimize the absolute
error from the ground-truth Shapley values, such that the computation can be
both efficient and accurate. To demonstrate the effectiveness, we
comprehensively evaluate SHEAR across multiple metrics including the absolute
error from the ground-truth Shapley value, the faithfulness of the
explanations, and running speed. The experimental results indicate SHEAR
consistently outperforms state-of-the-art baseline methods across different
evaluation metrics, which demonstrates its potentials in real-world
applications where the computational resource is limited.",https://github.com/guanchuwang/SHEAR,-1
Auction-Based Ex-Post-Payment Incentive Mechanism Design for Horizontal Federated Learning with Reputation and Contribution Measurement,0.337991,"Federated learning trains models across devices with distributed data, while
protecting the privacy and obtaining a model similar to that of centralized ML.
A large number of workers with data and computing power are the foundation of
federal learning. However, the inevitable costs prevent self-interested workers
from serving for free. Moreover, due to data isolation, task publishers lack
effective methods to select, evaluate and pay reliable workers with
high-quality data. Therefore, we design an auction-based incentive mechanism
for horizontal federated learning with reputation and contribution measurement.
By designing a reasonable method of measuring contribution, we establish the
reputation of workers, which is easy to decline and difficult to improve.
Through reverse auctions, workers bid for tasks, and the task publisher selects
workers combining reputation and bid price. With the budget constraint, winning
workers are paid based on performance. We proved that our mechanism satisfies
the individual rationality of the honest worker, budget feasibility,
truthfulness, and computational efficiency.",None,-1
Understanding Interpersonal Conflict Types and their Impact on Perception Classification,0.63351,"Studies on interpersonal conflict have a long history and contain many
suggestions for conflict typology. We use this as the basis of a novel
annotation scheme and release a new dataset of situations and conflict aspect
annotations. We then build a classifier to predict whether someone will
perceive the actions of one individual as right or wrong in a given situation.
Our analyses include conflict aspects, but also generated clusters, which are
human validated, and show differences in conflict content based on the
relationship of participants to the author. Our findings have important
implications for understanding conflict and social norms.",https://github.com/caisa-lab/interpersonal-conflict-types,-1
RoPGen: Towards Robust Code Authorship Attribution via Automatic Coding Style Transformation,0.9463,"Source code authorship attribution is an important problem often encountered
in applications such as software forensics, bug fixing, and software quality
analysis. Recent studies show that current source code authorship attribution
methods can be compromised by attackers exploiting adversarial examples and
coding style manipulation. This calls for robust solutions to the problem of
code authorship attribution. In this paper, we initiate the study on making
Deep Learning (DL)-based code authorship attribution robust. We propose an
innovative framework called Robust coding style Patterns Generation (RoPGen),
which essentially learns authors' unique coding style patterns that are hard
for attackers to manipulate or imitate. The key idea is to combine data
augmentation and gradient augmentation at the adversarial training phase. This
effectively increases the diversity of training examples, generates meaningful
perturbations to gradients of deep neural networks, and learns diversified
representations of coding styles. We evaluate the effectiveness of RoPGen using
four datasets of programs written in C, C++, and Java. Experimental results
show that RoPGen can significantly improve the robustness of DL-based code
authorship attribution, by respectively reducing 22.8% and 41.0% of the success
rate of targeted and untargeted attacks on average.",https://github.com/RoPGen/RoPGen,-1
Simple Open-Vocabulary Object Detection with Vision Transformers,0.999984,"Combining simple architectures with large-scale pre-training has led to
massive improvements in image classification. For object detection,
pre-training and scaling approaches are less well established, especially in
the long-tailed and open-vocabulary setting, where training data is relatively
scarce. In this paper, we propose a strong recipe for transferring image-text
models to open-vocabulary object detection. We use a standard Vision
Transformer architecture with minimal modifications, contrastive image-text
pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling
properties of this setup shows that increasing image-level pre-training and
model size yield consistent improvements on the downstream detection task. We
provide the adaptation strategies and regularizations needed to attain very
strong performance on zero-shot text-conditioned and one-shot image-conditioned
object detection. Code and models are available on GitHub.",https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit,82737
Learning Appearance-motion Normality for Video Anomaly Detection,0.849839,"Video anomaly detection is a challenging task in the computer vision
community. Most single task-based methods do not consider the independence of
unique spatial and temporal patterns, while two-stream structures lack the
exploration of the correlations. In this paper, we propose spatial-temporal
memories augmented two-stream auto-encoder framework, which learns the
appearance normality and motion normality independently and explores the
correlations via adversarial learning. Specifically, we first design two proxy
tasks to train the two-stream structure to extract appearance and motion
features in isolation. Then, the prototypical features are recorded in the
corresponding spatial and temporal memory pools. Finally, the encoding-decoding
network performs adversarial learning with the discriminator to explore the
correlations between spatial and temporal patterns. Experimental results show
that our framework outperforms the state-of-the-art methods, achieving AUCs of
98.1% and 89.8% on UCSD Ped2 and CUHK Avenue datasets.",None,-1
Multi-Focus Image Fusion based on Gradient Transform,0.103278,"Multi-focus image fusion is a challenging field of study that aims to provide
a completely focused image by integrating focused and un-focused pixels. Most
existing methods suffer from shift variance, misregistered images, and
data-dependent. In this study, we introduce a novel gradient information-based
multi-focus image fusion method that is robust for the aforementioned problems.
The proposed method first generates gradient images from original images by
using Halftoning-Inverse Halftoning (H-IH) transform. Then, Energy of Gradient
(EOG) and Standard Deviation functions are used as the focus measurement on the
gradient images to form a fused image. Finally, in order to enhance the fused
image a decision fusion approach is applied with the majority voting method.
The proposed method is compared with 17 different novel and conventional
techniques both visually and objectively. For objective evaluation, 6 different
quantitative metrics are used. It is observed that the proposed method is
promising according to visual evaluation and 83.3% success is achieved by being
first in five out of six metrics according to objective evaluation.",None,-1
DETR++: Taming Your Multi-Scale Detection Transformer,0.252993,"Convolutional Neural Networks (CNN) have dominated the field of detection
ever since the success of AlexNet in ImageNet classification [12]. With the
sweeping reform of Transformers [27] in natural language processing, Carion et
al. [2] introduce the Transformer-based detection method, i.e., DETR. However,
due to the quadratic complexity in the self-attention mechanism in the
Transformer, DETR is never able to incorporate multi-scale features as
performed in existing CNN-based detectors, leading to inferior results in small
object detection. To mitigate this issue and further improve performance of
DETR, in this work, we investigate different methods to incorporate multi-scale
features and find that a Bi-directional Feature Pyramid (BiFPN) works best with
DETR in further raising the detection precision. With this discovery, we
propose DETR++, a new architecture that improves detection results by 1.9% AP
on MS COCO 2017, 11.5% AP on RICO icon detection, and 9.1% AP on RICO layout
extraction over existing baselines.",None,7581
Optical Flow Based Motion Detection for Autonomous Driving,0.0924037,"Motion detection is a fundamental but challenging task for autonomous
driving. In particular scenes like highway, remote objects have to be paid
extra attention for better controlling decision. Aiming at distant vehicles, we
train a neural network model to classify the motion status using optical flow
field information as the input. The experiments result in high accuracy,
showing that our idea is viable and promising. The trained model also achieves
an acceptable performance for nearby vehicles. Our work is implemented in
PyTorch. Open tools including nuScenes, FastFlowNet and RAFT are used.
Visualization videos are available at
https://www.youtube.com/playlist?list=PLVVrWgq4OrlBnRebmkGZO1iDHEksMHKGk .",https://github.com/kamanphoebe/MotionDetection.git,-1
OTExtSum: Extractive Text Summarisation with Optimal Transport,0.231678,"Extractive text summarisation aims to select salient sentences from a
document to form a short yet informative summary. While learning-based methods
have achieved promising results, they have several limitations, such as
dependence on expensive training and lack of interpretability. Therefore, in
this paper, we propose a novel non-learning-based method by for the first time
formulating text summarisation as an Optimal Transport (OT) problem, namely
Optimal Transport Extractive Summariser (OTExtSum). Optimal sentence extraction
is conceptualised as obtaining an optimal summary that minimises the
transportation cost to a given document regarding their semantic distributions.
Such a cost is defined by the Wasserstein distance and used to measure the
summary's semantic coverage of the original document. Comprehensive experiments
on four challenging and widely used datasets - MultiNews, PubMed, BillSum, and
CNN/DM demonstrate that our proposed method outperforms the state-of-the-art
non-learning-based methods and several recent learning-based methods in terms
of the ROUGE metric.",https://github.com/peggypytang/OTExtSum/,-1
QAScore -- An Unsupervised Unreferenced Metric for the Question Generation Evaluation,0.20757,"Question Generation (QG) aims to automate the task of composing questions for
a passage with a set of chosen answers found within the passage. In recent
years, the introduction of neural generation models has resulted in substantial
improvements of automatically generated questions in terms of quality,
especially compared to traditional approaches that employ manually crafted
heuristics. However, the metrics commonly applied in QG evaluations have been
criticized for their low agreement with human judgement. We therefore propose a
new reference-free evaluation metric that has the potential to provide a better
mechanism for evaluating QG systems, called QAScore. Instead of fine-tuning a
language model to maximize its correlation with human judgements, QAScore
evaluates a question by computing the cross entropy according to the
probability that the language model can correctly generate the masked words in
the answer to that question. Furthermore, we conduct a new crowd-sourcing human
evaluation experiment for the QG evaluation to investigate how QAScore and
other metrics can correlate with human judgements. Experiments show that
QAScore obtains a stronger correlation with the results of our proposed human
evaluation method compared to existing traditional word-overlap-based metrics
such as BLEU and ROUGE, as well as the existing pretrained-model-based metric
BERTScore.",None,-1
Efficient Visual Tracking via Hierarchical Cross-Attention Transformer,0.832323,"In recent years, target tracking has made great progress in accuracy. This
development is mainly attributed to powerful networks (such as transformers)
and additional modules (such as online update and refinement modules). However,
less attention has been paid to tracking speed. Most state-of-the-art trackers
are satisfied with the real-time speed on powerful GPUs. However, practical
applications necessitate higher requirements for tracking speed, especially
when edge platforms with limited resources are used. In this work, we present
an efficient tracking method via a hierarchical cross-attention transformer
named HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on
the edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our
HCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100,
UAV123, and VOT2020. Code and models are available at
https://github.com/chenxin-dlut/HCAT.",https://github.com/chenxin-dlut/HCAT,48744
Neural Topic Modeling of Psychotherapy Sessions,0.488786,"In this work, we compare different neural topic modeling methods in learning
the topical propensities of different psychiatric conditions from the
psychotherapy session transcripts parsed from speech recordings. We also
incorporate temporal modeling to put this additional interpretability to action
by parsing out topic similarities as a time series in a turn-level resolution.
We believe this topic modeling framework can offer interpretable insights for
the therapist to optimally decide his or her strategy and improve psychotherapy
effectiveness.",https://github.com/zll17/Neural Topic Models,-1
On Label Granularity and Object Localization,0.395277,"Weakly supervised object localization (WSOL) aims to learn representations
that encode object location using only image-level category labels. However,
many objects can be labeled at different levels of granularity. Is it an
animal, a bird, or a great horned owl? Which image-level labels should we use?
In this paper we study the role of label granularity in WSOL. To facilitate
this investigation we introduce iNatLoc500, a new large-scale fine-grained
benchmark dataset for WSOL. Surprisingly, we find that choosing the right
training label granularity provides a much larger performance boost than
choosing the best WSOL algorithm. We also show that changing the label
granularity can significantly improve data efficiency.",https://github.com/tensorflow/models/blob/65407126c5adc216d606d360429fe12ed3c3f187/research/object_detection/configs/tf2/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.config,-1
MorDeephy: Face Morphing Detection Via Fused Classification,0.435882,"Face morphing attack detection (MAD) is one of the most challenging tasks in
the field of face recognition nowadays. In this work, we introduce a novel deep
learning strategy for a single image face morphing detection, which implies the
discrimination of morphed face images along with a sophisticated face
recognition task in a complex classification scheme. It is directed onto
learning the deep facial features, which carry information about the
authenticity of these features. Our work also introduces several additional
contributions: the public and easy-to-use face morphing detection benchmark and
the results of our wild datasets filtering strategy. Our method, which we call
MorDeephy, achieved the state of the art performance and demonstrated a
prominent ability for generalising the task of morphing detection to unseen
scenarios.",None,-1
A Distributional Lens for Multi-Aspect Controllable Text Generation,0.738625,"Multi-aspect controllable text generation is a more challenging and practical
task than single-aspect control. Existing methods achieve complex multi-aspect
control by fusing multiple controllers learned from single-aspect, but suffer
from attribute degeneration caused by the mutual interference of these
controllers. To address this, we provide observations on attribute fusion from
a distributional perspective and propose to directly search for the
intersection areas of multiple attribute distributions as their combination for
generation. Our method first estimates the attribute space with an autoencoder
structure. Afterward, we iteratively approach the intersections by jointly
minimizing distances to points representing different attributes. Finally, we
map them to attribute-relevant sentences with a prefix-tuning-based decoder.
Experiments on the three-aspect control task, including sentiment, topic, and
detoxification aspects, reveal that our method outperforms several strong
baselines on attribute relevance and text quality and achieves the SOTA.
Further analysis also supplies some explanatory support for the effectiveness
of our approach.",https://github.com/HappyGu0524/MultiControl,-1
Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging,0.956756,"Training vision or language models on large datasets can take days, if not
weeks. We show that averaging the weights of the k latest checkpoints, each
collected at the end of an epoch, can speed up the training progression in
terms of loss and accuracy by dozens of epochs, corresponding to time savings
up to ~68 and ~30 GPU hours when training a ResNet50 on ImageNet and
RoBERTa-Base model on WikiText-103, respectively. We also provide the code and
model checkpoint trajectory to reproduce the results and facilitate research on
reusing historical weights for faster convergence.",https://github.com/jeankaddour/lawa,632
Structural Bias for Aspect Sentiment Triplet Extraction,0.8039,"Structural bias has recently been exploited for aspect sentiment triplet
extraction (ASTE) and led to improved performance. On the other hand, it is
recognized that explicitly incorporating structural bias would have a negative
impact on efficiency, whereas pretrained language models (PLMs) can already
capture implicit structures. Thus, a natural question arises: Is structural
bias still a necessity in the context of PLMs? To answer the question, we
propose to address the efficiency issues by using an adapter to integrate
structural bias in the PLM and using a cheap-to-compute relative position
structure in place of the syntactic dependency structure. Benchmarking
evaluation is conducted on the SemEval datasets. The results show that our
proposed structural adapter is beneficial to PLMs and achieves state-of-the-art
performance over a range of strong baselines, yet with a light parameter demand
and low latency. Meanwhile, we give rise to the concern that the current
evaluation default with data of small scale is under-confident. Consequently,
we release a large-scale dataset for ASTE. The results on the new dataset hint
that the structural adapter is confidently effective and efficient to a large
scale. Overall, we draw the conclusion that structural bias shall still be a
necessity even with PLMs.",https://github.com/GeneZC/StructBias,-1
CMNEROne at SemEval-2022 Task 11: Code-Mixed Named Entity Recognition by leveraging multilingual data,0.361296,"Identifying named entities is, in general, a practical and challenging task
in the field of Natural Language Processing. Named Entity Recognition on the
code-mixed text is further challenging due to the linguistic complexity
resulting from the nature of the mixing. This paper addresses the submission of
team CMNEROne to the SEMEVAL 2022 shared task 11 MultiCoNER. The Code-mixed NER
task aimed to identify named entities on the code-mixed dataset. Our work
consists of Named Entity Recognition (NER) on the code-mixed dataset by
leveraging the multilingual data. We achieved a weighted average F1 score of
0.7044, i.e., 6% greater than the baseline.",https://github.com/scrapinghub/python-crfsuite,-1
Learning to Adapt Domain Shifts of Moral Values via Instance Weighting,0.809831,"Classifying moral values in user-generated text from social media is critical
in understanding community cultures and interpreting user behaviors of social
movements. Moral values and language usage can change across the social
movements; however, text classifiers are usually trained in source domains of
existing social movements and tested in target domains of new social issues
without considering the variations. In this study, we examine domain shifts of
moral values and language usage, quantify the effects of domain shifts on the
morality classification task, and propose a neural adaptation framework via
instance weighting to improve cross-domain classification tasks. The
quantification analysis suggests a strong correlation between morality shifts,
language usage, and classification performance. We evaluate the neural
adaptation framework on a public Twitter data across 7 social movements and
gain classification improvements up to 12.1\%. Finally, we release a new data
of the COVID-19 vaccine labeled with moral values and evaluate our approach on
the new target domain. For the case study of the COVID-19 vaccine, our
adaptation framework achieves up to 5.26\% improvements over neural baselines.",None,12040
$\mathcal{X}$-Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing,0.976658,"This paper presents a generic probabilistic framework for estimating the
statistical dependency and finding the anatomical correspondences among an
arbitrary number of medical images. The method builds on a novel formulation of
the $N$-dimensional joint intensity distribution by representing the common
anatomy as latent variables and estimating the appearance model with
nonparametric estimators. Through connection to maximum likelihood and the
expectation-maximization algorithm, an information\hyp{}theoretic metric called
$\mathcal{X}$-metric and a co-registration algorithm named $\mathcal{X}$-CoReg
are induced, allowing groupwise registration of the $N$ observed images with
computational complexity of $\mathcal{O}(N)$. Moreover, the method naturally
extends for a weakly-supervised scenario where anatomical labels of certain
images are provided. This leads to a combined\hyp{}computing framework
implemented with deep learning, which performs registration and segmentation
simultaneously and collaboratively in an end-to-end fashion. Extensive
experiments were conducted to demonstrate the versatility and applicability of
our model, including multimodal groupwise registration, motion correction for
dynamic contrast enhanced magnetic resonance images, and deep combined
computing for multimodal medical images. Results show the superiority of our
method in various applications in terms of both accuracy and efficiency,
highlighting the advantage of the proposed representation of the imaging
process.",https://zmiclab.github.io/projects.html,5501
Do LSTMs See Gender? Probing the Ability of LSTMs to Learn Abstract Syntactic Rules,0.292948,"LSTMs trained on next-word prediction can accurately perform linguistic tasks
that require tracking long-distance syntactic dependencies. Notably, model
accuracy approaches human performance on number agreement tasks (Gulordava et
al., 2018). However, we do not have a mechanistic understanding of how LSTMs
perform such linguistic tasks. Do LSTMs learn abstract grammatical rules, or do
they rely on simple heuristics? Here, we test gender agreement in French which
requires tracking both hierarchical syntactic structures and the inherent
gender of lexical units. Our model is able to reliably predict long-distance
gender agreement in two subject-predicate contexts: noun-adjective and
noun-passive-verb agreement. The model showed more inaccuracies on plural noun
phrases with gender attractors compared to singular cases, suggesting a
reliance on clues from gendered articles for agreement. Overall, our study
highlights key ways in which LSTMs deviate from human behaviour and questions
whether LSTMs genuinely learn abstract syntactic rules and categories. We
propose using gender agreement as a useful probe to investigate the underlying
mechanisms, internal representations, and linguistic capabilities of LSTM
language models.",None,-1
MMRotate: A Rotated Object Detection Benchmark using PyTorch,0.951203,"We present an open-source toolbox, named MMRotate, which provides a coherent
algorithm framework of training, inferring, and evaluation for the popular
rotated object detection algorithm based on deep learning. MMRotate implements
18 state-of-the-art algorithms and supports the three most frequently used
angle definition methods. To facilitate future research and industrial
applications of rotated object detection-related problems, we also provide a
large number of trained models and detailed benchmarks to give insights into
the performance of rotated object detection. MMRotate is publicly released at
https://github.com/open-mmlab/mmrotate.",https://github.com/open-mmlab/mmrotate,-1
Ladder Siamese Network: a Method and Insights for Multi-level Self-Supervised Learning,0.08368,"Siamese-network-based self-supervised learning (SSL) suffers from slow
convergence and instability in training. To alleviate this, we propose a
framework to exploit intermediate self-supervisions in each stage of deep nets,
called the Ladder Siamese Network. Our self-supervised losses encourage the
intermediate layers to be consistent with different data augmentations to
single samples, which facilitates training progress and enhances the
discriminative ability of the intermediate layers themselves. While some
existing work has already utilized multi-level self supervisions in SSL, ours
is different in that 1) we reveal its usefulness with non-contrastive Siamese
frameworks in both theoretical and empirical viewpoints, and 2) ours improves
image-level classification, instance-level detection, and pixel-level
segmentation simultaneously. Experiments show that the proposed framework can
improve BYOL baselines by 1.0% points in ImageNet linear classification, 1.2%
points in COCO detection, and 3.1% points in PASCAL VOC segmentation. In
comparison with the state-of-the-art methods, our Ladder-based model achieves
competitive and balanced performances in all tested benchmarks without causing
large degradation in one.",https://github.com/open-mmlab/mmsegmentation,-1
A Low-Shot Object Counting Network With Iterative Prototype Adaptation,0.336799,"We consider low-shot counting of arbitrary semantic categories in the image
using only few annotated exemplars (few-shot) or no exemplars (no-shot). The
standard few-shot pipeline follows extraction of appearance queries from
exemplars and matching them with image features to infer the object counts.
Existing methods extract queries by feature pooling which neglects the shape
information (e.g., size and aspect) and leads to a reduced object localization
accuracy and count estimates. We propose a Low-shot Object Counting network
with iterative prototype Adaptation (LOCA). Our main contribution is the new
object prototype extraction module, which iteratively fuses the exemplar shape
and appearance information with image features. The module is easily adapted to
zero-shot scenarios, enabling LOCA to cover the entire spectrum of low-shot
counting problems. LOCA outperforms all recent state-of-the-art methods on
FSC147 benchmark by 20-30% in RMSE on one-shot and few-shot and achieves
state-of-the-art on zero-shot scenarios, while demonstrating better
generalization capabilities.",https://github.com/djukicn/loca,11587
Unsupervised Domain Adaptation Based on the Predictive Uncertainty of Models,0.454371,"Unsupervised domain adaptation (UDA) aims to improve the prediction
performance in the target domain under distribution shifts from the source
domain. The key principle of UDA is to minimize the divergence between the
source and the target domains. To follow this principle, many methods employ a
domain discriminator to match the feature distributions. Some recent methods
evaluate the discrepancy between two predictions on target samples to detect
those that deviate from the source distribution. However, their performance is
limited because they either match the marginal distributions or measure the
divergence conservatively. In this paper, we present a novel UDA method that
learns domain-invariant features that minimize the domain divergence. We
propose model uncertainty as a measure of the domain divergence. Our UDA method
based on model uncertainty (MUDA) adopts a Bayesian framework and provides an
efficient way to evaluate model uncertainty by means of Monte Carlo dropout
sampling. Empirical results on image recognition tasks show that our method is
superior to existing state-of-the-art methods. We also extend MUDA to
multi-source domain adaptation problems.",https://github.com/joonholee-research/MUDA,-1
Zero-shot Cross-lingual Conversational Semantic Role Labeling,0.257708,"While conversational semantic role labeling (CSRL) has shown its usefulness
on Chinese conversational tasks, it is still under-explored in non-Chinese
languages due to the lack of multilingual CSRL annotations for the parser
training. To avoid expensive data collection and error-propagation of
translation-based methods, we present a simple but effective approach to
perform zero-shot cross-lingual CSRL. Our model implicitly learns
language-agnostic, conversational structure-aware and semantically rich
representations with the hierarchical encoders and elaborately designed
pre-training objectives. Experimental results show that our model outperforms
all baselines by large margins on two newly collected English CSRL test sets.
More importantly, we confirm the usefulness of CSRL to non-Chinese
conversational tasks such as the question-in-context rewriting task in English
and the multi-turn dialogue response generation tasks in English, German and
Japanese by incorporating the CSRL information into the downstream
conversation-based models. We believe this finding is significant and will
facilitate the research of non-Chinese dialogue tasks which suffer the problems
of ellipsis and anaphora.",https://github.com/hahahawu/Zero-Shot-XCSRL,-1
Joint Discriminative and Metric Embedding Learning for Person Re-Identification,0.35718,"Person re-identification is a challenging task because of the high
intra-class variance induced by the unrestricted nuisance factors of variations
such as pose, illumination, viewpoint, background, and sensor noise. Recent
approaches postulate that powerful architectures have the capacity to learn
feature representations invariant to nuisance factors, by training them with
losses that minimize intra-class variance and maximize inter-class separation,
without modeling nuisance factors explicitly. The dominant approaches use
either a discriminative loss with margin, like the softmax loss with the
additive angular margin, or a metric learning loss, like the triplet loss with
batch hard mining of triplets. Since the softmax imposes feature normalization,
it limits the gradient flow supervising the feature embedding. We address this
by joining the losses and leveraging the triplet loss as a proxy for the
missing gradients. We further improve invariance to nuisance factors by adding
the discriminative task of predicting attributes. Our extensive evaluation
highlights that when only a holistic representation is learned, we consistently
outperform the state-of-the-art on the three most challenging datasets. Such
representations are easier to deploy in practical systems. Finally, we found
that joining the losses removes the requirement for having a margin in the
softmax loss while increasing performance.",None,-1
Construction and Evaluation of a Self-Attention Model for Semantic Understanding of Sentence-Final Particles,0.0440456,"Sentence-final particles serve an essential role in spoken Japanese because
they express the speaker's mental attitudes toward a proposition and/or an
interlocutor. They are acquired at early ages and occur very frequently in
everyday conversation. However, there has been little proposal for a
computational model of acquiring sentence-final particles. This paper proposes
Subjective BERT, a self-attention model that takes various subjective senses in
addition to language and images as input and learns the relationship between
words and subjective senses. An evaluation experiment revealed that the model
understands the usage of ""yo"", which expresses the speaker's intention to
communicate new information, and that of ""ne"", which denotes the speaker's
desire to confirm that some information is shared.",None,-1
VITA: A Multi-Source Vicinal Transfer Augmentation Method for Out-of-Distribution Generalization,0.0402562,"Invariance to diverse types of image corruption, such as noise, blurring, or
colour shifts, is essential to establish robust models in computer vision. Data
augmentation has been the major approach in improving the robustness against
common corruptions. However, the samples produced by popular augmentation
strategies deviate significantly from the underlying data manifold. As a
result, performance is skewed toward certain types of corruption. To address
this issue, we propose a multi-source vicinal transfer augmentation (VITA)
method for generating diverse on-manifold samples. The proposed VITA consists
of two complementary parts: tangent transfer and integration of multi-source
vicinal samples. The tangent transfer creates initial augmented samples for
improving corruption robustness. The integration employs a generative model to
characterize the underlying manifold built by vicinal samples, facilitating the
generation of on-manifold samples. Our proposed VITA significantly outperforms
the current state-of-the-art augmentation methods, demonstrated in extensive
experiments on corruption benchmarks.",None,-1
BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision,0.999951,"We present a novel bird's-eye-view (BEV) detector with perspective
supervision, which converges faster and better suits modern image backbones.
Existing state-of-the-art BEV detectors are often tied to certain depth
pre-trained backbones like VoVNet, hindering the synergy between booming image
backbones and BEV detectors. To address this limitation, we prioritize easing
the optimization of BEV detectors by introducing perspective space supervision.
To this end, we propose a two-stage BEV detector, where proposals from the
perspective head are fed into the bird's-eye-view head for final predictions.
To evaluate the effectiveness of our model, we conduct extensive ablation
studies focusing on the form of supervision and the generality of the proposed
detector. The proposed method is verified with a wide spectrum of traditional
and modern image backbones and achieves new SoTA results on the large-scale
nuScenes dataset. The code shall be released soon.",None,-1
CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation,0.411054,"The full power of human language-based communication cannot be realized
without negation. All human languages have some form of negation. Despite this,
negation remains a challenging phenomenon for current natural language
understanding systems. To facilitate the future development of models that can
process negation effectively, we present CONDAQA, the first English reading
comprehension dataset which requires reasoning about the implications of
negated statements in paragraphs. We collect paragraphs with diverse negation
cues, then have crowdworkers ask questions about the implications of the
negated statement in the passage. We also have workers make three kinds of
edits to the passage -- paraphrasing the negated statement, changing the scope
of the negation, and reversing the negation -- resulting in clusters of
question-answer pairs that are difficult for models to answer with spurious
shortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique
negation cues and is challenging for current state-of-the-art models. The best
performing model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our
consistency metric, well below human performance which is 81%. We release our
dataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to
facilitate the development of future NLP methods that work on negated language.",https://github.com/AbhilashaRavichander/CondaQA,-1
Uncertainty-aware Panoptic Segmentation,0.689514,"Reliable scene understanding is indispensable for modern autonomous systems.
Current learning-based methods typically try to maximize their performance
based on segmentation metrics that only consider the quality of the
segmentation. However, for the safe operation of a system in the real world it
is crucial to consider the uncertainty in the prediction as well. In this work,
we introduce the novel task of uncertainty-aware panoptic segmentation, which
aims to predict per-pixel semantic and instance segmentations, together with
per-pixel uncertainty estimates. We define two novel metrics to facilitate its
quantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the
panoptic Expected Calibration Error (pECE). We further propose the novel
top-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task.
Our architecture employs a simple yet effective panoptic fusion module that
leverages the predicted uncertainties. Furthermore, we provide several strong
baselines combining state-of-the-art panoptic segmentation networks with
sampling-free uncertainty estimation techniques. Extensive evaluations show
that our EvPSNet achieves the new state-of-the-art for the standard Panoptic
Quality (PQ), as well as for our uncertainty-aware panoptic metrics. We make
the code available at: \url{https://github.com/kshitij3112/EvPSNet}",https://github.com/kshitij3112/EvPSNet,115008
GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator,0.652066,"Pre-trained models have achieved remarkable success in natural language
processing (NLP). However, existing pre-training methods underutilize the
benefits of language understanding for generation. Inspired by the idea of
Generative Adversarial Networks (GANs), we propose a GAN-style model for
encoder-decoder pre-training by introducing an auxiliary discriminator,
unifying the ability of language understanding and generation in a single
model. Our model, named as GanLM, is trained with two pre-training objectives:
replaced token detection and replaced token denoising. Specifically, given
masked source sentences, the generator outputs the target distribution and the
discriminator predicts whether the target sampled tokens from distribution are
incorrect. The target sentence is replaced with misclassified tokens to
construct noisy previous context, which is used to generate the gold sentence.
In general, both tasks improve the ability of language understanding and
generation by selectively using the denoising data. Extensive experiments in
language generation benchmarks show that GanLM with the powerful language
understanding capability outperforms various strong pre-trained language models
(PLMs) and achieves state-of-the-art performance.",https://github.com/CSJianYang/GanLM,-1
A integrating critic-waspas group decision making method under interval-valued q-rung orthogonal fuzzy enviroment,0.0755598,"This paper provides a new tool for multi-attribute multi-objective group
decision-making with unknown weights and attributes' weights. An
interval-valued generalized orthogonal fuzzy group decision-making method is
proposed based on the Yager operator and CRITIC-WASPAS method with unknown
weights. The method integrates Yager operator, CRITIC, WASPAS, and interval
value generalized orthogonal fuzzy group. Its merits lie in allowing
decision-makers greater freedom, avoiding bias due to decision-makers' weight,
and yielding accurate evaluation. The research includes: expanding the interval
value generalized distance measurement method for comparison and application of
similarity measurement and decision-making methods; developing a new scoring
function for comparing the size of interval value generalized orthogonal fuzzy
numbers,and further existing researches. The proposed interval-valued Yager
weighted average operator (IVq-ROFYWA) and Yager weighted geometric average
operator (IVq-ROFYWG) are used for information aggregation. The CRITIC-WASPAS
combines the advantages of CRITIC and WASPAS, which not only work in the single
decision but also serve as the basis of the group decision. The in-depth study
of the decision-maker's weight matrix overcomes the shortcomings of taking the
decision as a whole, and weighs the decision-maker's information aggregation.
Finally, the group decision algorithm is used for hypertension risk management.
The results are consistent with decision-makers' opinions. Practice and case
analysis have proved the effectiveness of the method proposed in this paper. At
the same time, it is compared with other operators and decision-making methods,
which proves the method effective and feasible.",None,-1
End-to-end Multilingual Coreference Resolution with Mention Head Prediction,0.650769,"This paper describes our approach to the CRAC 2022 Shared Task on
Multilingual Coreference Resolution. Our model is based on a state-of-the-art
end-to-end coreference resolution system. Apart from joined multilingual
training, we improved our results with mention head prediction. We also tried
to integrate dependency information into our model. Our system ended up in
$3^{rd}$ place. Moreover, we reached the best performance on two datasets out
of 13.",https://github.com/ufal/corefud-scorer,-1
SWFormer: Sparse Window Transformer for 3D Object Detection in Point Clouds,0.990203,"3D object detection in point clouds is a core component for modern robotics
and autonomous driving systems. A key challenge in 3D object detection comes
from the inherent sparse nature of point occupancy within the 3D scene. In this
paper, we propose Sparse Window Transformer (SWFormer ), a scalable and
accurate model for 3D object detection, which can take full advantage of the
sparsity of point clouds. Built upon the idea of window-based Transformers,
SWFormer converts 3D points into sparse voxels and windows, and then processes
these variable-length sparse windows efficiently using a bucketing scheme. In
addition to self-attention within each spatial window, our SWFormer also
captures cross-window correlation with multi-scale feature fusion and window
shifting operations. To further address the unique challenge of detecting 3D
objects accurately from sparse features, we propose a new voxel diffusion
technique. Experimental results on the Waymo Open Dataset show our SWFormer
achieves state-of-the-art 73.36 L2 mAPH on vehicle and pedestrian for 3D object
detection on the official test set, outperforming all previous single-stage and
two-stage models, while being much more efficient.",None,-1
Original or Translated? On the Use of Parallel Data for Translation Quality Estimation,0.20497,"Machine Translation Quality Estimation (QE) is the task of evaluating
translation output in the absence of human-written references. Due to the
scarcity of human-labeled QE data, previous works attempted to utilize the
abundant unlabeled parallel corpora to produce additional training data with
pseudo labels. In this paper, we demonstrate a significant gap between parallel
data and real QE data: for QE data, it is strictly guaranteed that the source
side is original texts and the target side is translated (namely
translationese). However, for parallel data, it is indiscriminate and the
translationese may occur on either source or target side. We compare the impact
of parallel data with different translation directions in QE data augmentation,
and find that using the source-original part of parallel corpus consistently
outperforms its target-original counterpart. Moreover, since the WMT corpus
lacks direction information for each parallel sentence, we train a classifier
to distinguish source- and target-original bitext, and carry out an analysis of
their difference in both style and domain. Together, these findings suggest
using source-original parallel data for QE data augmentation, which brings a
relative improvement of up to 4.0% and 6.4% compared to undifferentiated data
on sentence- and word-level QE tasks respectively.",None,-1
Multi-Class Segmentation from Aerial Views using Recursive Noise Diffusion,0.540464,"Semantic segmentation from aerial views is a crucial task for autonomous
drones, as they rely on precise and accurate segmentation to navigate safely
and efficiently. However, aerial images present unique challenges such as
diverse viewpoints, extreme scale variations, and high scene complexity. In
this paper, we propose an end-to-end multi-class semantic segmentation
diffusion model that addresses these challenges. We introduce recursive
denoising to allow information to propagate through the denoising process, as
well as a hierarchical multi-scale approach that complements the diffusion
process. Our method achieves competitive results on the UAVid dataset and
state-of-the-art performance on the Vaihingen Building segmentation benchmark.
Being the first iteration of this method, it shows great promise for future
improvements.",https://github.com/benediktkol/recursive-noise-diffusion,-1
Testing predictive automated driving systems: lessons learned and future recommendations,0.638137,"Conventional vehicles are certified through classical approaches, where
different physical certification tests are set up on test tracks to assess
required safety levels. These approaches are well suited for vehicles with
limited complexity and limited interactions with other entities as last-second
resources. However, these approaches do not allow to evaluate safety with real
behaviors for critical and edge cases, nor to evaluate the ability to
anticipate them in the mid or long term. This is particularly relevant for
automated and autonomous driving functions that make use of advanced predictive
systems to anticipate future actions and motions to be considered in the path
planning layer. In this paper, we present and analyze the results of physical
tests on proving grounds of several predictive systems in automated driving
functions developed within the framework of the BRAVE project. Based on our
experience in testing predictive automated driving functions, we identify the
main limitations of current physical testing approaches when dealing with
predictive systems, analyze the main challenges ahead, and provide a set of
practical actions and recommendations to consider in future physical testing
procedures for automated and autonomous driving functions.",None,-1
Geometry-Aware Reference Synthesis for Multi-View Image Super-Resolution,0.130869,"Recent multi-view multimedia applications struggle between high-resolution
(HR) visual experience and storage or bandwidth constraints. Therefore, this
paper proposes a Multi-View Image Super-Resolution (MVISR) task. It aims to
increase the resolution of multi-view images captured from the same scene. One
solution is to apply image or video super-resolution (SR) methods to
reconstruct HR results from the low-resolution (LR) input view. However, these
methods cannot handle large-angle transformations between views and leverage
information in all multi-view images. To address these problems, we propose the
MVSRnet, which uses geometry information to extract sharp details from all LR
multi-view to support the SR of the LR input view. Specifically, the proposed
Geometry-Aware Reference Synthesis module in MVSRnet uses geometry information
and all multi-view LR images to synthesize pixel-aligned HR reference images.
Then, the proposed Dynamic High-Frequency Search network fully exploits the
high-frequency textural details in reference images for SR. Extensive
experiments on several benchmarks show that our method significantly improves
over the state-of-the-art approaches.",None,-1
Transcending Scaling Laws with 0.1% Extra Compute,0.508321,"Scaling language models improves performance but comes with significant
computational costs. This paper proposes UL2R, a method that substantially
improves existing language models and their scaling curves with a relatively
tiny amount of extra compute. The key idea is to continue training a
state-of-the-art large language model (e.g., PaLM) on a few more steps with
UL2's mixture-of-denoiser objective. We show that, with almost negligible extra
computational costs and no new sources of data, we are able to substantially
improve the scaling properties of large language models on downstream metrics.
In this paper, we continue training PaLM with UL2R, introducing a new set of
models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B
scale, we show an approximately 2x computational savings rate where U-PaLM
achieves the same performance as the final PaLM 540B model at around half its
computational budget (i.e., saving $\sim$4.4 million TPUv4 hours). We further
show that this improved scaling curve leads to 'emergent abilities' on
challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM
on some tasks or demonstrates better quality at much smaller scale (62B as
opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many
few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question
answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual
tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide
qualitative examples showing the new capabilities of U-PaLM for single and
multi-span infilling.",None,-1
ULDGNN: A Fragmented UI Layer Detector Based on Graph Neural Networks,0.197344,"While some work attempt to generate front-end code intelligently from UI
screenshots, it may be more convenient to utilize UI design drafts in Sketch
which is a popular UI design software, because we can access multimodal UI
information directly such as layers type, position, size, and visual images.
However, fragmented layers could degrade the code quality without being merged
into a whole part if all of them are involved in the code generation. In this
paper, we propose a pipeline to merge fragmented layers automatically. We first
construct a graph representation for the layer tree of a UI draft and detect
all fragmented layers based on the visual features and graph neural networks.
Then a rule-based algorithm is designed to merge fragmented layers. Through
experiments on a newly constructed dataset, our approach can retrieve most
fragmented layers in UI design drafts, and achieve 87% accuracy in the
detection task, and the post-processing algorithm is developed to cluster
associative layers under simple and general circumstances.",None,-1
"Intelligent Computing: The Latest Advances, Challenges and Future",0.923387,"Computing is a critical driving force in the development of human
civilization. In recent years, we have witnessed the emergence of intelligent
computing, a new computing paradigm that is reshaping traditional computing and
promoting digital revolution in the era of big data, artificial intelligence
and internet-of-things with new computing theories, architectures, methods,
systems, and applications. Intelligent computing has greatly broadened the
scope of computing, extending it from traditional computing on data to
increasingly diverse computing paradigms such as perceptual intelligence,
cognitive intelligence, autonomous intelligence, and human-computer fusion
intelligence. Intelligence and computing have undergone paths of different
evolution and development for a long time but have become increasingly
intertwined in recent years: intelligent computing is not only
intelligence-oriented but also intelligence-driven. Such cross-fertilization
has prompted the emergence and rapid advancement of intelligent computing.
Intelligent computing is still in its infancy and an abundance of innovations
in the theories, systems, and applications of intelligent computing are
expected to occur soon. We present the first comprehensive survey of literature
on intelligent computing, covering its theory fundamentals, the technological
fusion of intelligence and computing, important applications, challenges, and
future perspectives. We believe that this survey is highly timely and will
provide a comprehensive reference and cast valuable insights into intelligent
computing for academic and industrial researchers and practitioners.",None,50644
AutoField: Automating Feature Selection in Deep Recommender Systems,0.835652,"Feature quality has an impactful effect on recommendation performance.
Thereby, feature selection is a critical process in developing deep
learning-based recommender systems. Most existing deep recommender systems,
however, focus on designing sophisticated neural networks, while neglecting the
feature selection process. Typically, they just feed all possible features into
their proposed deep architectures, or select important features manually by
human experts. The former leads to non-trivial embedding parameters and extra
inference time, while the latter requires plenty of expert knowledge and human
labor effort. In this work, we propose an AutoML framework that can adaptively
select the essential feature fields in an automatic manner. Specifically, we
first design a differentiable controller network, which is capable of
automatically adjusting the probability of selecting a particular feature
field; then, only selected feature fields are utilized to retrain the deep
recommendation model. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our framework. We conduct further experiments
to investigate its properties, including the transferability, key components,
and parameter sensitivity.",https://github.com/rixwew/pytorch-fm,-1
Box-supervised Instance Segmentation with Level Set Evolution,0.900773,"In contrast to the fully supervised methods using pixel-wise mask labels,
box-supervised instance segmentation takes advantage of the simple box
annotations, which has recently attracted a lot of research attentions. In this
paper, we propose a novel single-shot box-supervised instance segmentation
approach, which integrates the classical level set model with deep neural
network delicately. Specifically, our proposed method iteratively learns a
series of level sets through a continuous Chan-Vese energy-based function in an
end-to-end fashion. A simple mask supervised SOLOv2 model is adapted to predict
the instance-aware mask map as the level set for each instance. Both the input
image and its deep features are employed as the input data to evolve the level
set curves, where a box projection function is employed to obtain the initial
boundary. By minimizing the fully differentiable energy function, the level set
for each instance is iteratively optimized within its corresponding bounding
box annotation. The experimental results on four challenging benchmarks
demonstrate the leading performance of our proposed approach to robust instance
segmentation in various scenarios. The code is available at:
https://github.com/LiWentomng/boxlevelset.",https://github.com/LiWentomng/boxlevelset,-1
Interactive Language: Talking to Robots in Real Time,0.994635,"We present a framework for building interactive, real-time, natural
language-instructable robots in the real world, and we open source related
assets (dataset, environment, benchmark, and policies). Trained with behavioral
cloning on a dataset of hundreds of thousands of language-annotated
trajectories, a produced policy can proficiently execute an order of magnitude
more commands than previous works: specifically we estimate a 93.5% success
rate on a set of 87,000 unique natural language strings specifying raw
end-to-end visuo-linguo-motor skills in the real world. We find that the same
policy is capable of being guided by a human via real-time language to address
a wide range of precise long-horizon rearrangement goals, e.g. ""make a smiley
face out of blocks"". The dataset we release comprises nearly 600,000
language-labeled trajectories, an order of magnitude larger than prior
available datasets. We hope the demonstrated results and associated assets
enable further advancement of helpful, capable, natural-language-interactable
robots. See videos at https://interactive-language.github.io.",None,-1
AI agents for facilitating social interactions and wellbeing,0.0362305,"Wellbeing AI has been becoming a new trend in individuals' mental health,
organizational health, and flourishing our societies. Various applications of
wellbeing AI have been introduced to our daily lives. While social
relationships within groups are a critical factor for wellbeing, the
development of wellbeing AI for social interactions remains relatively scarce.
In this paper, we provide an overview of the mediative role of AI-augmented
agents for social interactions. First, we discuss the two-dimensional framework
for classifying wellbeing AI: individual/group and analysis/intervention.
Furthermore, wellbeing AI touches on intervening social relationships between
human-human interactions since positive social relationships are key to human
wellbeing. This intervention may raise technical and ethical challenges. We
discuss opportunities and challenges of the relational approach with wellbeing
AI to promote wellbeing in our societies.",None,-1
Learning Keypoints from Synthetic Data for Robotic Cloth Folding,0.166726,"Robotic cloth manipulation is challenging due to its deformability, which
makes determining its full state infeasible. However, for cloth folding, it
suffices to know the position of a few semantic keypoints. Convolutional neural
networks (CNN) can be used to detect these keypoints, but require large amounts
of annotated data, which is expensive to collect. To overcome this, we propose
to learn these keypoint detectors purely from synthetic data, enabling low-cost
data collection. In this paper, we procedurally generate images of towels and
use them to train a CNN. We evaluate the performance of this detector for
folding towels on a unimanual robot setup and find that the grasp and fold
success rates are 77% and 53%, respectively. We conclude that learning keypoint
detectors from synthetic data for cloth folding and related tasks is a
promising research direction, discuss some failures and relate them to future
work. A video of the system, as well as the codebase, more details on the CNN
architecture and the training setup can be found at
https://github.com/tlpss/workshop-icra-2022-cloth-keypoints.git.",https://github.com/tlpss/workshop-icra-2022-cloth-keypoints.git,-1
An Outlier Exposure Approach to Improve Visual Anomaly Detection Performance for Mobile Robots,0.509035,"We consider the problem of building visual anomaly detection systems for
mobile robots. Standard anomaly detection models are trained using large
datasets composed only of non-anomalous data. However, in robotics
applications, it is often the case that (potentially very few) examples of
anomalies are available. We tackle the problem of exploiting these data to
improve the performance of a Real-NVP anomaly detection model, by minimizing,
jointly with the Real-NVP loss, an auxiliary outlier exposure margin loss. We
perform quantitative experiments on a novel dataset (which we publish as
supplementary material) designed for anomaly detection in an indoor patrolling
scenario. On a disjoint test set, our approach outperforms alternatives and
shows that exposing even a small number of anomalous frames yields significant
performance improvements.",https://github.com/idsia-robotics/hazard-detection,-1
Enhanced Vehicle Re-identification for ITS: A Feature Fusion approach using Deep Learning,0.12148,"In recent years, the development of robust Intelligent transportation systems
(ITS) is tackled across the globe to provide better traffic efficiency by
reducing frequent traffic problems. As an application of ITS, vehicle
re-identification has gained ample interest in the domain of computer vision
and robotics. Convolutional neural network (CNN) based methods are developed to
perform vehicle re-identification to address key challenges such as occlusion,
illumination change, scale, etc. The advancement of transformers in computer
vision has opened an opportunity to explore the re-identification process
further to enhance performance. In this paper, a framework is developed to
perform the re-identification of vehicles across CCTV cameras. To perform
re-identification, the proposed framework fuses the vehicle representation
learned using a CNN and a transformer model. The framework is tested on a
dataset that contains 81 unique vehicle identities observed across 20 CCTV
cameras. From the experiments, the fused vehicle re-identification framework
yields an mAP of 61.73% which is significantly better when compared with the
standalone CNN or transformer model.",None,2569
The four-fifths rule is not disparate impact: a woeful tale of epistemic trespassing in algorithmic fairness,0.732937,"Computer scientists are trained to create abstractions that simplify and
generalize. However, a premature abstraction that omits crucial contextual
details creates the risk of epistemic trespassing, by falsely asserting its
relevance into other contexts. We study how the field of responsible AI has
created an imperfect synecdoche by abstracting the four-fifths rule (a.k.a. the
4/5 rule or 80% rule), a single part of disparate impact discrimination law,
into the disparate impact metric. This metric incorrectly introduces a new
deontic nuance and new potentials for ethical harms that were absent in the
original 4/5 rule. We also survey how the field has amplified the potential for
harm in codifying the 4/5 rule into popular AI fairness software toolkits. The
harmful erasure of legal nuances is a wake-up call for computer scientists to
self-critically re-evaluate the abstractions they create and use, particularly
in the interdisciplinary field of AI ethics.",None,-1
Exploration of Machine Learning Classification Models Used for Behavioral Biometrics Authentication,0.707976,"Mobile devices have been manufactured and enhanced at growing rates in the
past decades. While this growth has significantly evolved the capability of
these devices, their security has been falling behind. This contrast in
development between capability and security of mobile devices is a significant
problem with the sensitive information of the public at risk. Continuing the
previous work in this field, this study identifies key Machine Learning
algorithms currently being used for behavioral biometric mobile authentication
schemes and aims to provide a comprehensive review of these algorithms when
used with touch dynamics and phone movement. Throughout this paper the
benefits, limitations, and recommendations for future work will be discussed.",None,664
A context-aware knowledge transferring strategy for CTC-based ASR,0.747647,"Non-autoregressive automatic speech recognition (ASR) modeling has received
increasing attention recently because of its fast decoding speed and superior
performance. Among representatives, methods based on the connectionist temporal
classification (CTC) are still a dominating stream. However, the theoretically
inherent flaw, the assumption of independence between tokens, creates a
performance barrier for the school of works. To mitigate the challenge, we
propose a context-aware knowledge transferring strategy, consisting of a
knowledge transferring module and a context-aware training strategy, for
CTC-based ASR. The former is designed to distill linguistic information from a
pre-trained language model, and the latter is framed to modulate the
limitations caused by the conditional independence assumption. As a result, a
knowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is
presented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2
datasets demonstrate the effectiveness of the proposed method.",None,810
Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning,0.485203,"Controlled table-to-text generation seeks to generate natural language
descriptions for highlighted subparts of a table. Previous SOTA systems still
employ a sequence-to-sequence generation method, which merely captures the
table as a linear structure and is brittle when table layouts change. We seek
to go beyond this paradigm by (1) effectively expressing the relations of
content pieces in the table, and (2) making our model robust to
content-invariant structural transformations. Accordingly, we propose an
equivariance learning framework, which encodes tables with a structure-aware
self-attention mechanism. This prunes the full self-attention structure into an
order-invariant graph attention that captures the connected graph structure of
cells belonging to the same row or column, and it differentiates between
relevant cells and irrelevant cells from the structural perspective. Our
framework also modifies the positional encoding mechanism to preserve the
relative position of tokens in the same cell but enforce position invariance
among different cells. Our technology is free to be plugged into existing
table-to-text generation models, and has improved T5-based models to offer
better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo,
we preserve promising performance, while previous SOTA systems, even with
transformation-based data augmentation, have seen significant performance
drops. Our code is available at https://github.com/luka-group/Lattice.",https://github.com/luka-group/Lattice,-1
Efficient Remote Photoplethysmography with Temporal Derivative Modules and Time-Shift Invariant Loss,0.66878,"We present a lightweight neural model for remote heart rate estimation
focused on the efficient spatio-temporal learning of facial
photoplethysmography (PPG) based on i) modelling of PPG dynamics by
combinations of multiple convolutional derivatives, and ii) increased
flexibility of the model to learn possible offsets between the facial video PPG
and the ground truth. PPG dynamics are modelled by a Temporal Derivative Module
(TDM) constructed by the incremental aggregation of multiple convolutional
derivatives, emulating a Taylor series expansion up to the desired order.
Robustness to ground truth offsets is handled by the introduction of TALOS
(Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based
models. We verify the effectiveness of our model by reporting accuracy and
efficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to
existing models, our approach shows competitive heart rate estimation accuracy
with a much lower number of parameters and lower computational cost.",None,-1
Normalized Feature Distillation for Semantic Segmentation,0.0361114,"As a promising approach in model compression, knowledge distillation improves
the performance of a compact model by transferring the knowledge from a
cumbersome one. The kind of knowledge used to guide the training of the student
is important. Previous distillation methods in semantic segmentation strive to
extract various forms of knowledge from the features, which involve elaborate
manual design relying on prior information and have limited performance gains.
In this paper, we propose a simple yet effective feature distillation method
called normalized feature distillation (NFD), aiming to enable effective
distillation with the original features without the need to manually design new
forms of knowledge. The key idea is to prevent the student from focusing on
imitating the magnitude of the teacher's feature response by normalization. Our
method achieves state-of-the-art distillation results for semantic segmentation
on Cityscapes, VOC 2012, and ADE20K datasets. Code will be available.",None,-1
Crowd Source Scene Change Detection and Local Map Update,0.0444982,"As scene changes with time map descriptors become outdated, affecting VPS
localization accuracy. In this work, we propose an approach to detect
structural and texture scene changes to be followed by map update. In our
method - map includes 3D points with descriptors generated either via LiDAR or
SFM. Common approaches suffer from shortcomings: 1) Direct comparison of the
two point-clouds for change detection is slow due to the need to build new
point-cloud every time we want to compare; 2) Image based comparison requires
to keep the map images adding substantial storage overhead. To circumvent this
problems, we propose an approach based on point-clouds descriptors comparison:
1) Based on VPS poses select close query and map images pairs, 2) Registration
of query images to map image descriptors, 3) Use segmentation to filter out
dynamic or short term temporal changes, 4) Compare the descriptors between
corresponding segments.",None,-1
Comparative layer-wise analysis of self-supervised speech models,0.837552,"Many self-supervised speech models, varying in their pre-training objective,
input modality, and pre-training data, have been proposed in the last few
years. Despite impressive successes on downstream tasks, we still have a
limited understanding of the properties encoded by the models and the
differences across models. In this work, we examine the intermediate
representations for a variety of recent models. Specifically, we measure
acoustic, phonetic, and word-level properties encoded in individual layers,
using a lightweight analysis tool based on canonical correlation analysis
(CCA). We find that these properties evolve across layers differently depending
on the model, and the variations relate to the choice of pre-training
objective. We further investigate the utility of our analyses for downstream
tasks by comparing the property trends with performance on speech recognition
and spoken language understanding tasks. We discover that CCA trends provide
reliable guidance to choose layers of interest for downstream tasks and that
single-layer performance often matches or improves upon using all layers,
suggesting implications for more efficient use of pre-trained models.",https://github.com/ankitapasad/layerwise-analysis/,-1
"What's Different between Visual Question Answering for Machine ""Understanding"" Versus for Accessibility?",0.106056,"In visual question answering (VQA), a machine must answer a question given an
associated image. Recently, accessibility researchers have explored whether VQA
can be deployed in a real-world setting where users with visual impairments
learn about their environment by capturing their visual surroundings and asking
questions. However, most of the existing benchmarking datasets for VQA focus on
machine ""understanding"" and it remains unclear how progress on those datasets
corresponds to improvements in this real-world use case. We aim to answer this
question by evaluating discrepancies between machine ""understanding"" datasets
(VQA-v2) and accessibility datasets (VizWiz) by evaluating a variety of VQA
models. Based on our findings, we discuss opportunities and challenges in VQA
for accessibility and suggest directions for future work.",https://github.com/kyleseelman/vqa_accessibility,-1
Boolean Decision Rules for Reinforcement Learning Policy Summarisation,0.142171,"Explainability of Reinforcement Learning (RL) policies remains a challenging
research problem, particularly when considering RL in a safety context.
Understanding the decisions and intentions of an RL policy offer avenues to
incorporate safety into the policy by limiting undesirable actions. We propose
the use of a Boolean Decision Rules model to create a post-hoc rule-based
summary of an agent's policy. We evaluate our proposed approach using a DQN
agent trained on an implementation of a lava gridworld and show that, given a
hand-crafted feature representation of this gridworld, simple generalised rules
can be created, giving a post-hoc explainable summary of the agent's policy. We
discuss possible avenues to introduce safety into a RL agent's policy by using
rules generated by this rule-based model as constraints imposed on the agent's
policy, as well as discuss how creating simple rule summaries of an agent's
policy may help in the debugging process of RL agents.",https://github.com/mila-iqia/Conscious-Planning,-1
Anticipation-Free Training for Simultaneous Machine Translation,0.246442,"Simultaneous machine translation (SimulMT) speeds up the translation process
by starting to translate before the source sentence is completely available. It
is difficult due to limited context and word order difference between
languages. Existing methods increase latency or introduce adaptive read-write
policies for SimulMT models to handle local reordering and improve translation
quality. However, the long-distance reordering would make the SimulMT models
learn translation mistakenly. Specifically, the model may be forced to predict
target tokens when the corresponding source tokens have not been read. This
leads to aggressive anticipation during inference, resulting in the
hallucination phenomenon. To mitigate this problem, we propose a new framework
that decompose the translation process into the monotonic translation step and
the reordering step, and we model the latter by the auxiliary sorting network
(ASN). The ASN rearranges the hidden states to match the order in the target
language, so that the SimulMT model could learn to translate more reasonably.
The entire model is optimized end-to-end and does not rely on external aligners
or data. During inference, ASN is removed to achieve streaming. Experiments
show the proposed framework could outperform previous methods with less
latency.",https://github.com/George0828Zhang/sinkhorn-simultrans,-1
Interpretable Hidden Markov Model-Based Deep Reinforcement Learning Hierarchical Framework for Predictive Maintenance of Turbofan Engines,0.676536,"An open research question in deep reinforcement learning is how to focus the
policy learning of key decisions within a sparse domain. This paper emphasizes
combining the advantages of inputoutput hidden Markov models and reinforcement
learning towards interpretable maintenance decisions. We propose a novel
hierarchical-modeling methodology that, at a high level, detects and interprets
the root cause of a failure as well as the health degradation of the turbofan
engine, while, at a low level, it provides the optimal replacement policy. It
outperforms the baseline performance of deep reinforcement learning methods
applied directly to the raw data or when using a hidden Markov model without
such a specialized hierarchy. It also provides comparable performance to prior
work, however, with the additional benefit of interpretability.",None,-1
MISeval: a Metric Library for Medical Image Segmentation Evaluation,0.474804,"Correct performance assessment is crucial for evaluating modern artificial
intelligence algorithms in medicine like deep-learning based medical image
segmentation models. However, there is no universal metric library in Python
for standardized and reproducible evaluation. Thus, we propose our open-source
publicly available Python package MISeval: a metric library for Medical Image
Segmentation Evaluation. The implemented metrics can be intuitively used and
easily integrated into any performance assessment pipeline. The package
utilizes modern CI/CD strategies to ensure functionality and stability. MISeval
is available from PyPI (miseval) and GitHub:
https://github.com/frankkramer-lab/miseval.",https://github.com/frankkramer-lab/miseval,-1
CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding,0.61224,"Story generation and understanding -- as with all NLG/NLU tasks -- has seen a
surge in neurosymbolic work. Researchers have recognized that, while large
language models (LLMs) have tremendous utility, they can be augmented with
symbolic means to be even better and to make up for any flaws that the neural
networks might have. However, symbolic methods are extremely costly in terms of
the amount of time and expertise needed to create them. In this work, we
capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use
of symbolic methods for tracking the state of stories and aiding in story
understanding. We show that our CoRRPUS system and abstracted prompting
procedures can beat current state-of-the-art structured LLM techniques on
pre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand
engineering. We hope that this work can help highlight the importance of
symbolic representations and specialized prompting for LLMs as these models
require some guidance for performing reasoning tasks properly.",https://github.com/dong-river/CoRRPUS,-1
Probabilistic Representations for Video Contrastive Learning,0.716789,"This paper presents Probabilistic Video Contrastive Learning, a
self-supervised representation learning method that bridges contrastive
learning with probabilistic representation. We hypothesize that the clips
composing the video have different distributions in short-term duration, but
can represent the complicated and sophisticated video distribution through
combination in a common embedding space. Thus, the proposed method represents
video clips as normal distributions and combines them into a Mixture of
Gaussians to model the whole video distribution. By sampling embeddings from
the whole video distribution, we can circumvent the careful sampling strategy
or transformations to generate augmented views of the clips, unlike previous
deterministic methods that have mainly focused on such sample generation
strategies for contrastive learning. We further propose a stochastic
contrastive loss to learn proper video distributions and handle the inherent
uncertainty from the nature of the raw video. Experimental results verify that
our probabilistic embedding stands as a state-of-the-art video representation
learning for action recognition and video retrieval on the most popular
benchmarks, including UCF101 and HMDB51.",None,-1
Panoptic-PHNet: Towards Real-Time and High-Precision LiDAR Panoptic Segmentation via Clustering Pseudo Heatmap,0.897188,"As a rising task, panoptic segmentation is faced with challenges in both
semantic segmentation and instance segmentation. However, in terms of speed and
accuracy, existing LiDAR methods in the field are still limited. In this paper,
we propose a fast and high-performance LiDAR-based framework, referred to as
Panoptic-PHNet, with three attractive aspects: 1) We introduce a clustering
pseudo heatmap as a new paradigm, which, followed by a center grouping module,
yields instance centers for efficient clustering without object-level learning
tasks. 2) A knn-transformer module is proposed to model the interaction among
foreground points for accurate offset regression. 3) For backbone design, we
fuse the fine-grained voxel features and the 2D Bird's Eye View (BEV) features
with different receptive fields to utilize both detailed and global
information. Extensive experiments on both SemanticKITTI dataset and nuScenes
dataset show that our Panoptic-PHNet surpasses state-of-the-art methods by
remarkable margins with a real-time speed. We achieve the 1st place on the
public leaderboard of SemanticKITTI and leading performance on the recently
released leaderboard of nuScenes.",None,-1
From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer,0.939238,"Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC.",https://github.com/zjunlp/PromptKG/tree/main/research/GenKGC,8507
A Robust Document Image Watermarking Scheme using Deep Neural Network,0.610994,"Watermarking is an important copyright protection technology which generally
embeds the identity information into the carrier imperceptibly. Then the
identity can be extracted to prove the copyright from the watermarked carrier
even after suffering various attacks. Most of the existing watermarking
technologies take the nature images as carriers. Different from the natural
images, document images are not so rich in color and texture, and thus have
less redundant information to carry watermarks. This paper proposes an
end-to-end document image watermarking scheme using the deep neural network.
Specifically, an encoder and a decoder are designed to embed and extract the
watermark. A noise layer is added to simulate the various attacks that could be
encountered in reality, such as the Cropout, Dropout, Gaussian blur, Gaussian
noise, Resize, and JPEG Compression. A text-sensitive loss function is designed
to limit the embedding modification on characters. An embedding strength
adjustment strategy is proposed to improve the quality of watermarked image
with little loss of extraction accuracy. Experimental results show that the
proposed document image watermarking technology outperforms three
state-of-the-arts in terms of the robustness and image quality.",https://github.com/gslxr/Document-image-watermarking,-1
Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law,0.237421,"NLP in the legal domain has seen increasing success with the emergence of
Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text.
PLMs trained over European and US legal text are available publicly; however,
legal text from other domains (countries), such as India, have a lot of
distinguishing characteristics. With the rapidly increasing volume of Legal NLP
applications in various countries, it has become necessary to pre-train such
LMs over legal text of other countries as well. In this work, we attempt to
investigate pre-training in the Indian legal domain. We re-train (continue
pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian
legal data, as well as train a model from scratch with a vocabulary based on
Indian legal text. We apply these PLMs over three benchmark legal NLP tasks --
Legal Statute Identification from facts, Semantic Segmentation of Court
Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian
and non-Indian (EU, UK) datasets. We observe that our approach not only
enhances performance on the new domain (Indian texts) but also over the
original domain (European and UK texts). We also conduct explainability
experiments for a qualitative comparison of all these different PLMs.",https://github.com/Law-AI/pretraining-bert,-1
SAT: Improving Semi-Supervised Text Classification with Simple Instance-Adaptive Self-Training,0.331893,"Self-training methods have been explored in recent years and have exhibited
great performance in improving semi-supervised learning. This work presents a
Simple instance-Adaptive self-Training method (SAT) for semi-supervised text
classification. SAT first generates two augmented views for each unlabeled data
and then trains a meta-learner to automatically identify the relative strength
of augmentations based on the similarity between the original view and the
augmented views. The weakly-augmented view is fed to the model to produce a
pseudo-label and the strongly-augmented view is used to train the model to
predict the same pseudo-label. We conducted extensive experiments and analyses
on three text classification datasets and found that with varying sizes of
labeled training data, SAT consistently shows competitive performance compared
to existing semi-supervised learning methods. Our code can be found at
\url{https://github.com/declare-lab/SAT.git}.",https://github.com/declare-lab/SAT.git,-1
Massively Digitized Power Grid: Opportunities and Challenges of Use-inspired AI,0.721697,"This article presents a use-inspired perspective of the opportunities and
challenges in a massively digitized power grid. It argues that the intricate
interplay of data availability, computing capability, and artificial
intelligence (AI) algorithm development are the three key factors driving the
adoption of digitized solutions in the power grid. The impact of these three
factors on critical functions of power system operation and planning practices
are reviewed and illustrated with industrial practice case studies. Open
challenges and research opportunities for data, computing, and AI algorithms
are articulated within the context of the power industry's tremendous
decarbonization efforts.",None,-1
ViTOL: Vision Transformer for Weakly Supervised Object Localization,0.735191,"Weakly supervised object localization (WSOL) aims at predicting object
locations in an image using only image-level category labels. Common challenges
that image classification models encounter when localizing objects are, (a)
they tend to look at the most discriminative features in an image that confines
the localization map to a very small region, (b) the localization maps are
class agnostic, and the models highlight objects of multiple classes in the
same image and, (c) the localization performance is affected by background
noise. To alleviate the above challenges we introduce the following simple
changes through our proposed method ViTOL. We leverage the vision-based
transformer for self-attention and introduce a patch-based attention dropout
layer (p-ADL) to increase the coverage of the localization map and a gradient
attention rollout mechanism to generate class-dependent attention maps. We
conduct extensive quantitative, qualitative and ablation experiments on the
ImageNet-1K and CUB datasets. We achieve state-of-the-art MaxBoxAcc-V2
localization scores of 70.47% and 73.17% on the two datasets respectively. Code
is available on https://github.com/Saurav-31/ViTOL",https://github.com/Saurav-31/ViTOL,-1
KPI-BERT: A Joint Named Entity Recognition and Relation Extraction Model for Financial Reports,0.985052,"We present KPI-BERT, a system which employs novel methods of named entity
recognition (NER) and relation extraction (RE) to extract and link key
performance indicators (KPIs), e.g. ""revenue"" or ""interest expenses"", of
companies from real-world German financial documents. Specifically, we
introduce an end-to-end trainable architecture that is based on Bidirectional
Encoder Representations from Transformers (BERT) combining a recurrent neural
network (RNN) with conditional label masking to sequentially tag entities
before it classifies their relations. Our model also introduces a learnable
RNN-based pooling mechanism and incorporates domain expert knowledge by
explicitly filtering impossible relations. We achieve a substantially higher
prediction performance on a new practical dataset of German financial reports,
outperforming several strong baselines including a competing state-of-the-art
span-based entity tagging approach.",None,-1
Texture image analysis based on joint of multi directions GLCM and local ternary patterns,0.120399,"Human visual brain use three main component such as color, texture and shape
to detect or identify environment and objects. Hence, texture analysis has been
paid much attention by scientific researchers in last two decades. Texture
features can be used in many different applications in commuter vision or
machine learning problems. Since now, many different approaches have been
proposed to classify textures. Most of them consider the classification
accuracy as the main challenge that should be improved. In this article, a new
approach is proposed based on combination of two efficient texture descriptor,
co-occurrence matrix and local ternary patterns (LTP). First of all, basic
local binary pattern and LTP are performed to extract local textural
information. Next, a subset of statistical features is extracted from
gray-level co-occurrence matrixes. Finally, concatenated features are used to
train classifiers. The performance is evaluated on Brodatz benchmark dataset in
terms of accuracy. Experimental results show that proposed approach provide
higher classification rate in comparison with some state-of-the-art approaches.",None,-1
KATSum: Knowledge-aware Abstractive Text Summarization,0.0702935,"Text Summarization is recognised as one of the NLP downstream tasks and it
has been extensively investigated in recent years. It can assist people with
perceiving the information rapidly from the Internet, including news articles,
social posts, videos, etc. Most existing research works attempt to develop
summarization models to produce a better output. However, advent limitations of
most existing models emerge, including unfaithfulness and factual errors. In
this paper, we propose a novel model, named as Knowledge-aware Abstractive Text
Summarization, which leverages the advantages offered by Knowledge Graph to
enhance the standard Seq2Seq model. On top of that, the Knowledge Graph
triplets are extracted from the source text and utilised to provide keywords
with relational information, producing coherent and factually errorless
summaries. We conduct extensive experiments by using real-world data sets. The
results reveal that the proposed framework can effectively utilise the
information from Knowledge Graph and significantly reduce the factual errors in
the summary.",None,-1
Alternate Intermediate Conditioning with Syllable-level and Character-level Targets for Japanese ASR,0.0926003,"End-to-end automatic speech recognition directly maps input speech to
characters. However, the mapping can be problematic when several different
pronunciations should be mapped into one character or when one pronunciation is
shared among many different characters. Japanese ASR suffers the most from such
many-to-one and one-to-many mapping problems due to Japanese kanji characters.
To alleviate the problems, we introduce explicit interaction between characters
and syllables using Self-conditioned connectionist temporal classification
(CTC), in which the upper layers are ``self-conditioned'' on the intermediate
predictions from the lower layers. The proposed method utilizes character-level
and syllable-level intermediate predictions as conditioning features to deal
with mutual dependency between characters and syllables. Experimental results
on Corpus of Spontaneous Japanese show that the proposed method outperformed
the conventional multi-task and Self-conditioned CTC methods.",https://github.com/mozillazg/python-pinyin,-1
Refining neural network predictions using background knowledge,0.284347,"Recent work has shown logical background knowledge can be used in learning
systems to compensate for a lack of labeled training data. Many methods work by
creating a loss function that encodes this knowledge. However, often the logic
is discarded after training, even if it is still useful at test time. Instead,
we ensure neural network predictions satisfy the knowledge by refining the
predictions with an extra computation step. We introduce differentiable
refinement functions that find a corrected prediction close to the original
prediction. We study how to effectively and efficiently compute these
refinement functions. Using a new algorithm called Iterative Local Refinement
(ILR), we combine refinement functions to find refined predictions for logical
formulas of any complexity. ILR finds refinements on complex SAT formulas in
significantly fewer iterations and frequently finds solutions where gradient
descent can not. Finally, ILR produces competitive results in the MNIST
addition task.",https://github.com/DanieleAlessandro/IterativeLocalRefinement,-1
Zero-Shot Retrieval with Search Agents and Hybrid Environments,0.667085,"Learning to search is the task of building artificial agents that learn to
autonomously use a search box to find information. So far, it has been shown
that current language models can learn symbolic query reformulation policies,
in combination with traditional term-based retrieval, but fall short of
outperforming neural retrievers. We extend the previous learning to search
setup to a hybrid environment, which accepts discrete query refinement
operations, after a first-pass retrieval step via a dual encoder. Experiments
on the BEIR task show that search agents, trained via behavioral cloning,
outperform the underlying search system based on a combined dual encoder
retriever and cross encoder reranker. Furthermore, we find that simple
heuristic Hybrid Retrieval Environments (HRE) can improve baseline performance
by several nDCG points. The search agent based on HRE (HARE) matches
state-of-the-art performance, balanced in both zero-shot and in-domain
evaluations, via interpretable actions, and at twice the speed.",None,2397
Towards Linguistically Informed Multi-Objective Pre-Training for Natural Language Inference,0.00152806,"We introduce a linguistically enhanced combination of pre-training methods
for transformers. The pre-training objectives include POS-tagging, synset
prediction based on semantic knowledge graphs, and parent prediction based on
dependency parse trees. Our approach achieves competitive results on the
Natural Language Inference task, compared to the state of the art. Specifically
for smaller models, the method results in a significant performance boost,
emphasizing the fact that intelligent pre-training can make up for fewer
parameters and help building more efficient models. Combining POS-tagging and
synset prediction yields the overall best results.",None,-1
TENET: Transformer Encoding Network for Effective Temporal Flow on Motion Prediction,0.701998,"This technical report presents an effective method for motion prediction in
autonomous driving. We develop a Transformer-based method for input encoding
and trajectory prediction. Besides, we propose the Temporal Flow Header to
enhance the trajectory encoding. In the end, an efficient K-means ensemble
method is used. Using our Transformer network and ensemble method, we win the
first place of Argoverse 2 Motion Forecasting Challenge with the
state-of-the-art brier-minFDE score of 1.90.",None,-1
Domain-General Crowd Counting in Unseen Scenarios,0.61098,"Domain shift across crowd data severely hinders crowd counting models to
generalize to unseen scenarios. Although domain adaptive crowd counting
approaches close this gap to a certain extent, they are still dependent on the
target domain data to adapt (e.g. finetune) their models to the specific
domain. In this paper, we aim to train a model based on a single source domain
which can generalize well on any unseen domain. This falls into the realm of
domain generalization that remains unexplored in crowd counting. We first
introduce a dynamic sub-domain division scheme which divides the source domain
into multiple sub-domains such that we can initiate a meta-learning framework
for domain generalization. The sub-domain division is dynamically refined
during the meta-learning. Next, in order to disentangle domain-invariant
information from domain-specific information in image features, we design the
domain-invariant and -specific crowd memory modules to re-encode image
features. Two types of losses, i.e. feature reconstruction and orthogonal
losses, are devised to enable this disentanglement. Extensive experiments on
several standard crowd counting benchmarks i.e. SHA, SHB, QNRF, and NWPU, show
the strong generalizability of our method.",https://github.com/ZPDu/Domain-general-Crowd-Counting-in-Unseen-Scenarios,2009
A Lower Bound of Hash Codes' Performance,0.0295495,"As a crucial approach for compact representation learning, hashing has
achieved great success in effectiveness and efficiency. Numerous heuristic
Hamming space metric learning objectives are designed to obtain high-quality
hash codes. Nevertheless, a theoretical analysis of criteria for learning good
hash codes remains largely unexploited. In this paper, we prove that
inter-class distinctiveness and intra-class compactness among hash codes
determine the lower bound of hash codes' performance. Promoting these two
characteristics could lift the bound and improve hash learning. We then propose
a surrogate model to fully exploit the above objective by estimating the
posterior of hash codes and controlling it, which results in a low-bias
optimization. Extensive experiments reveal the effectiveness of the proposed
method. By testing on a series of hash-models, we obtain performance
improvements among all of them, with an up to $26.5\%$ increase in mean Average
Precision and an up to $20.5\%$ increase in accuracy. Our code is publicly
available at https://github.com/VL-Group/LBHash.",https://github.com/VL-Group/LBHash,-1
Improving Crowded Object Detection via Copy-Paste,0.156188,"Crowdedness caused by overlapping among similar objects is a ubiquitous
challenge in the field of 2D visual object detection. In this paper, we first
underline two main effects of the crowdedness issue: 1) IoU-confidence
correlation disturbances (ICD) and 2) confused de-duplication (CDD). Then we
explore a pathway of cracking these nuts from the perspective of data
augmentation. Primarily, a particular copy-paste scheme is proposed towards
making crowded scenes. Based on this operation, we first design a ""consensus
learning"" method to further resist the ICD problem and then find out the
pasting process naturally reveals a pseudo ""depth"" of object in the scene,
which can be potentially used for alleviating CDD dilemma. Both methods are
derived from magical using of the copy-pasting without extra cost for
hand-labeling. Experiments show that our approach can easily improve the
state-of-the-art detector in typical crowded detection task by more than 2%
without any bells and whistles. Moreover, this work can outperform existing
data augmentation strategies in crowded scenario.",None,-1
Adversarial Robustness through the Lens of Convolutional Filters,0.54567,"Deep learning models are intrinsically sensitive to distribution shifts in
the input data. In particular, small, barely perceivable perturbations to the
input data can force models to make wrong predictions with high confidence. An
common defense mechanism is regularization through adversarial training which
injects worst-case perturbations back into training to strengthen the decision
boundaries, and to reduce overfitting. In this context, we perform an
investigation of 3x3 convolution filters that form in adversarially-trained
models. Filters are extracted from 71 public models of the linf-RobustBench
CIFAR-10/100 and ImageNet1k leaderboard and compared to filters extracted from
models built on the same architectures but trained without robust
regularization. We observe that adversarially-robust models appear to form more
diverse, less sparse, and more orthogonal convolution filters than their normal
counterparts. The largest differences between robust and normal models are
found in the deepest layers, and the very first convolution layer, which
consistently and predominantly forms filters that can partially eliminate
perturbations, irrespective of the architecture. Data & Project website:
https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens",https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens,-1
"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",0.728393,"The past decade has witnessed dramatic gains in natural language processing
and an unprecedented scaling of large language models. These developments have
been accelerated by the advent of few-shot techniques such as chain of thought
(CoT) prompting. Specifically, CoT pushes the performance of large language
models in a few-shot setup by augmenting the prompts with intermediate steps.
Despite impressive results across various tasks, the reasons behind their
success have not been explored. This work uses counterfactual prompting to
develop a deeper understanding of CoT-based few-shot prompting mechanisms in
large language models. We first systematically identify and define the key
components of a prompt: symbols, patterns, and text. Then, we devise and
conduct an exhaustive set of experiments across four different tasks, by
querying the model with counterfactual prompts where only one of these
components is altered. Our experiments across three models (PaLM, GPT-3, and
CODEX) reveal several surprising findings and brings into question the
conventional wisdom around few-shot prompting. First, the presence of factual
patterns in a prompt is practically immaterial to the success of CoT. Second,
our results conclude that the primary role of intermediate steps may not be to
facilitate learning how to solve a task. The intermediate steps are rather a
beacon for the model to realize what symbols to replicate in the output to form
a factual answer. Further, text imbues patterns with commonsense knowledge and
meaning. Our empirical and qualitative analysis reveals that a symbiotic
relationship between text and patterns explains the success of few-shot
prompting: text helps extract commonsense from the question to help patterns,
and patterns enforce task understanding and direct text generation.",https://github.com/google-research/google-research/tree/master/l2da/learned2design,-1
CL2R: Compatible Lifelong Learning Representations,0.0744011,"In this paper, we propose a method to partially mimic natural intelligence
for the problem of lifelong learning representations that are compatible. We
take the perspective of a learning agent that is interested in recognizing
object instances in an open dynamic universe in a way in which any update to
its internal feature representation does not render the features in the gallery
unusable for visual search. We refer to this learning problem as Compatible
Lifelong Learning Representations (CL2R) as it considers compatible
representation learning within the lifelong learning paradigm. We identify
stationarity as the property that the feature representation is required to
hold to achieve compatibility and propose a novel training procedure that
encourages local and global stationarity on the learned representation. Due to
stationarity, the statistical properties of the learned features do not change
over time, making them interoperable with previously learned features.
Extensive experiments on standard benchmark datasets show that our CL2R
training procedure outperforms alternative baselines and state-of-the-art
methods. We also provide novel metrics to specifically evaluate compatible
representation learning under catastrophic forgetting in various sequential
learning tasks. Code at
https://github.com/NiccoBiondi/CompatibleLifelongRepresentation.",https://github.com/NiccoBiondi/CompatibleLifelongRepresentation,-1
Learning Control Admissibility Models with Graph Neural Networks for Multi-Agent Navigation,0.862066,"Deep reinforcement learning in continuous domains focuses on learning control
policies that map states to distributions over actions that ideally concentrate
on the optimal choices in each step. In multi-agent navigation problems, the
optimal actions depend heavily on the agents' density. Their interaction
patterns grow exponentially with respect to such density, making it hard for
learning-based methods to generalize. We propose to switch the learning
objectives from predicting the optimal actions to predicting sets of admissible
actions, which we call control admissibility models (CAMs), such that they can
be easily composed and used for online inference for an arbitrary number of
agents. We design CAMs using graph neural networks and develop training methods
that optimize the CAMs in the standard model-free setting, with the additional
benefit of eliminating the need for reward engineering typically required to
balance collision avoidance and goal-reaching requirements. We evaluate the
proposed approach in multi-agent navigation environments. We show that the CAM
models can be trained in environments with only a few agents and be easily
composed for deployment in dense environments with hundreds of agents,
achieving better performance than state-of-the-art methods.",None,-1
Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries,0.854259,"Learning under a continuously changing data distribution with incorrect
labels is a desirable real-world problem yet challenging. A large body of
continual learning (CL) methods, however, assumes data streams with clean
labels, and online learning scenarios under noisy data streams are yet
underexplored. We consider a more practical CL task setup of an online learning
from blurry data stream with corrupted labels, where existing CL methods
struggle. To address the task, we first argue the importance of both diversity
and purity of examples in the episodic memory of continual learning models. To
balance diversity and purity in the episodic memory, we propose a novel
strategy to manage and use the memory by a unified approach of label noise
aware diverse sampling and robust learning with semi-supervised learning. Our
empirical validations on four real-world or synthetic noise datasets (CIFAR10
and 100, mini-WebVision, and Food-101N) exhibit that our method significantly
outperforms prior arts in this realistic and challenging continual learning
scenario. Code and data splits are available in
https://github.com/clovaai/puridiver.",https://github.com/clovaai/puridiver,-1
Relational Message Passing for Fully Inductive Knowledge Graph Completion,0.93954,"In knowledge graph completion (KGC), predicting triples involving emerging
entities and/or relations, which are unseen when the KG embeddings are learned,
has become a critical challenge. Subgraph reasoning with message passing is a
promising and popular solution. Some recent methods have achieved good
performance, but they (i) usually can only predict triples involving unseen
entities alone, failing to address more realistic fully inductive situations
with both unseen entities and unseen relations, and (ii) often conduct message
passing over the entities with the relation patterns not fully utilized. In
this study, we propose a new method named RMPI which uses a novel Relational
Message Passing network for fully Inductive KGC. It passes messages directly
between relations to make full use of the relation patterns for subgraph
reasoning with new techniques on graph transformation, graph pruning,
relation-aware neighborhood attention, addressing empty subgraphs, etc., and
can utilize the relation semantics defined in the ontological schema of KG.
Extensive evaluation on multiple benchmarks has shown the effectiveness of
techniques involved in RMPI and its better performance compared with the
existing methods that support fully inductive KGC. RMPI is also comparable to
the state-of-the-art partially inductive KGC methods with very promising
results achieved. Our codes and data are available at
https://github.com/zjukg/RMPI.",https://github.com/zjukg/RMPI,-1
A Self-Guided Framework for Radiology Report Generation,0.715289,"Automatic radiology report generation is essential to computer-aided
diagnosis. Through the success of image captioning, medical report generation
has been achievable. However, the lack of annotated disease labels is still the
bottleneck of this area. In addition, the image-text data bias problem and
complex sentences make it more difficult to generate accurate reports. To
address these gaps, we pre-sent a self-guided framework (SGF), a suite of
unsupervised and supervised deep learning methods to mimic the process of human
learning and writing. In detail, our framework obtains the domain knowledge
from medical reports with-out extra disease labels and guides itself to extract
fined-grain visual features as-sociated with the text. Moreover, SGF
successfully improves the accuracy and length of medical report generation by
incorporating a similarity comparison mechanism that imitates the process of
human self-improvement through compar-ative practice. Extensive experiments
demonstrate the utility of our SGF in the majority of cases, showing its
superior performance over state-of-the-art meth-ods. Our results highlight the
capacity of the proposed framework to distinguish fined-grained visual details
between words and verify its advantage in generating medical reports.",None,2284
On Improving Summarization Factual Consistency from Natural Language Feedback,0.804394,"Despite the recent progress in language generation models, their outputs may
not always meet user expectations. In this work, we study whether informational
feedback in natural language can be leveraged to improve generation quality and
user preference alignment. To this end, we consider factual consistency in
summarization, the quality that the summary should only contain information
supported by the input documents, as the user-expected preference. We collect a
high-quality dataset, DeFacto, containing human demonstrations and
informational natural language feedback consisting of corrective instructions,
edited summaries, and explanations with respect to the factual consistency of
the summary. Using our dataset, we study three natural language generation
tasks: (1) editing a summary by following the human feedback, (2) generating
human feedback for editing the original summary, and (3) revising the initial
summary to correct factual errors by generating both the human feedback and
edited summary. We show that DeFacto can provide factually consistent
human-edited summaries and further insights into summarization factual
consistency thanks to its informational natural language feedback. We further
demonstrate that fine-tuned language models can leverage our dataset to improve
the summary factual consistency, while large language models lack the zero-shot
learning ability in our proposed tasks that require controllable text
generation.",https://github.com/microsoft/DeFacto,-1
Can pre-trained Transformers be used in detecting complex sensitive sentences? -- A Monsanto case study,0.419001,"Each and every organisation releases information in a variety of forms
ranging from annual reports to legal proceedings. Such documents may contain
sensitive information and releasing them openly may lead to the leakage of
confidential information. Detection of sentences that contain sensitive
information in documents can help organisations prevent the leakage of valuable
confidential information. This is especially challenging when such sentences
contain a substantial amount of information or are paraphrased versions of
known sensitive content. Current approaches to sensitive information detection
in such complex settings are based on keyword-based approaches or standard
machine learning models. In this paper, we wish to explore whether pre-trained
transformer models are well suited to detect complex sensitive information.
Pre-trained transformers are typically trained on an enormous amount of text
and therefore readily learn grammar, structure and other linguistic features,
making them particularly attractive for this task. Through our experiments on
the Monsanto trial data set, we observe that the fine-tuned Bidirectional
Encoder Representations from Transformers (BERT) transformer model performs
better than traditional models. We experimented with four different categories
of documents in the Monsanto dataset and observed that BERT achieves better F2
scores by 24.13\% to 65.79\% for GHOST, 30.14\% to 54.88\% for TOXIC, 39.22\%
for CHEMI, 53.57\% for REGUL compared to existing sensitive information
detection models.",None,-1
Meta-Learning Priors for Safe Bayesian Optimization,0.910318,"In robotics, optimizing controller parameters under safety constraints is an
important challenge. Safe Bayesian optimization (BO) quantifies uncertainty in
the objective and constraints to safely guide exploration in such settings.
Hand-designing a suitable probabilistic model can be challenging, however. In
the presence of unknown safety constraints, it is crucial to choose reliable
model hyper-parameters to avoid safety violations. Here, we propose a
data-driven approach to this problem by meta-learning priors for safe BO from
offline data. We build on a meta-learning algorithm, F-PACOH, capable of
providing reliable uncertainty quantification in settings of data scarcity. As
core contribution, we develop a novel framework for choosing safety-compliant
priors in a data-riven manner via empirical uncertainty metrics and a frontier
search algorithm. On benchmark functions and a high-precision motion system, we
demonstrate that our meta-learned priors accelerate the convergence of safe BO
approaches while maintaining safety.",https://tinyurl.com/safe-meta-bo,-1
Modeling Dual Read/Write Paths for Simultaneous Machine Translation,0.689124,"Simultaneous machine translation (SiMT) outputs translation while reading
source sentence and hence requires a policy to decide whether to wait for the
next source word (READ) or generate a target word (WRITE), the actions of which
form a read/write path. Although the read/write path is essential to SiMT
performance, no direct supervision is given to the path in the existing
methods. In this paper, we propose a method of dual-path SiMT which introduces
duality constraints to direct the read/write path. According to duality
constraints, the read/write path in source-to-target and target-to-source SiMT
models can be mapped to each other. As a result, the two SiMT models can be
optimized jointly by forcing their read/write paths to satisfy the mapping.
Experiments on En-Vi and De-En tasks show that our method can outperform strong
baselines under all latency.",https://github.com/ictnlp/,-1
Towards a Secure and Reliable Federated Learning using Blockchain,0.431615,"Federated learning (FL) is a distributed machine learning (ML) technique that
enables collaborative training in which devices perform learning using a local
dataset while preserving their privacy. This technique ensures privacy,
communication efficiency, and resource conservation. Despite these advantages,
FL still suffers from several challenges related to reliability (i.e.,
unreliable participating devices in training), tractability (i.e., a large
number of trained models), and anonymity. To address these issues, we propose a
secure and trustworthy blockchain framework (SRB-FL) tailored to FL, which uses
blockchain features to enable collaborative model training in a fully
distributed and trustworthy manner. In particular, we design a secure FL based
on the blockchain sharding that ensures data reliability, scalability, and
trustworthiness. In addition, we introduce an incentive mechanism to improve
the reliability of FL devices using subjective multi-weight logic. The results
show that our proposed SRB-FL framework is efficient and scalable, making it a
promising and suitable solution for federated learning.",None,-1
Surrogate Gradient Spiking Neural Networks as Encoders for Large Vocabulary Continuous Speech Recognition,0.422037,"Compared to conventional artificial neurons that produce dense and
real-valued responses, biologically-inspired spiking neurons transmit sparse
and binary information, which can also lead to energy-efficient
implementations. Recent research has shown that spiking neural networks can be
trained like standard recurrent neural networks using the surrogate gradient
method. They have shown promising results on speech command recognition tasks.
Using the same technique, we show that they are scalable to large vocabulary
continuous speech recognition, where they are capable of replacing LSTMs in the
encoder with only minor loss of performance. This suggests that they may be
applicable to more involved sequence-to-sequence tasks. Moreover, in contrast
to their recurrent non-spiking counterparts, they show robustness to exploding
gradient problems without the need to use gates.",None,-1
CarFi: Rider Localization Using Wi-Fi CSI,0.629042,"With the rise of hailing services, people are increasingly relying on shared
mobility (e.g., Uber, Lyft) drivers to pick up for transportation. However,
such drivers and riders have difficulties finding each other in urban areas as
GPS signals get blocked by skyscrapers, in crowded environments (e.g., in
stadiums, airports, and bars), at night, and in bad weather. It wastes their
time, creates a bad user experience, and causes more CO2 emissions due to idle
driving. In this work, we explore the potential of Wi-Fi to help drivers to
determine the street side of the riders. Our proposed system is called CarFi
that uses Wi-Fi CSI from two antennas placed inside a moving vehicle and a
data-driven technique to determine the street side of the rider. By collecting
real-world data in realistic and challenging settings by blocking the signal
with other people and other parked cars, we see that CarFi is 95.44% accurate
in rider-side determination in both line of sight (LoS) and non-line of sight
(nLoS) conditions, and can be run on an embedded GPU in real-time.",None,2064
A Secure Clustering Protocol with Fuzzy Trust Evaluation and Outlier Detection for Industrial Wireless Sensor Networks,0.881122,"Security is one of the major concerns in Industrial Wireless Sensor Networks
(IWSNs). To assure the security in clustered IWSNs, this paper presents a
secure clustering protocol with fuzzy trust evaluation and outlier detection
(SCFTO). Firstly, to deal with the transmission uncertainty in an open wireless
medium, an interval type-2 fuzzy logic controller is adopted to estimate the
trusts. And then a density based outlier detection mechanism is introduced to
acquire an adaptive trust threshold used to isolate the malicious nodes from
being cluster heads. Finally, a fuzzy based cluster heads election method is
proposed to achieve a balance between energy saving and security assurance, so
that a normal sensor node with more residual energy or less confidence on other
nodes has higher probability to be the cluster head. Extensive experiments
verify that our secure clustering protocol can effectively defend the network
against attacks from internal malicious or compromised nodes.",None,-1
Symbolic Physics Learner: Discovering governing equations via Monte Carlo tree search,0.596257,"Nonlinear dynamics is ubiquitous in nature and commonly seen in various
science and engineering disciplines. Distilling analytical expressions that
govern nonlinear dynamics from limited data remains vital but challenging. To
tackle this fundamental issue, we propose a novel Symbolic Physics Learner
(SPL) machine to discover the mathematical structure of nonlinear dynamics. The
key concept is to interpret mathematical operations and system state variables
by computational rules and symbols, establish symbolic reasoning of
mathematical formulas via expression trees, and employ a Monte Carlo tree
search (MCTS) agent to explore optimal expression trees based on measurement
data. The MCTS agent obtains an optimistic selection policy through the
traversal of expression trees, featuring the one that maps to the arithmetic
expression of underlying physics. Salient features of the proposed framework
include search flexibility and enforcement of parsimony for discovered
equations. The efficacy and superiority of the SPL machine are demonstrated by
numerical examples, compared with state-of-the-art baselines.",https://github.com/brendenpetersen/deep-symbolic-optimization/tree/master/dso/dso,-1
Fast Object Placement Assessment,0.815331,"Object placement assessment (OPA) aims to predict the rationality score of a
composite image in terms of the placement (e.g., scale, location) of inserted
foreground object. However, given a pair of scaled foreground and background,
to enumerate all the reasonable locations, existing OPA model needs to place
the foreground at each location on the background and pass the obtained
composite image through the model one at a time, which is very time-consuming.
In this work, we investigate a new task named as fast OPA. Specifically,
provided with a scaled foreground and a background, we only pass them through
the model once and predict the rationality scores for all locations. To
accomplish this task, we propose a pioneering fast OPA model with several
innovations (i.e., foreground dynamic filter, background prior transfer, and
composite feature mimicking) to bridge the performance gap between slow OPA
model and fast OPA model. Extensive experiments on OPA dataset show that our
proposed fast OPA model performs on par with slow OPA model but runs
significantly faster.",None,-1
Contrastive Boundary Learning for Point Cloud Segmentation,0.970563,"Point cloud segmentation is fundamental in understanding 3D environments.
However, current 3D point cloud segmentation methods usually perform poorly on
scene boundaries, which degenerates the overall segmentation performance. In
this paper, we focus on the segmentation of scene boundaries. Accordingly, we
first explore metrics to evaluate the segmentation performance on scene
boundaries. To address the unsatisfactory performance on boundaries, we then
propose a novel contrastive boundary learning (CBL) framework for point cloud
segmentation. Specifically, the proposed CBL enhances feature discrimination
between points across boundaries by contrasting their representations with the
assistance of scene contexts at multiple scales. By applying CBL on three
different baseline methods, we experimentally show that CBL consistently
improves different baselines and assists them to achieve compelling performance
on boundaries, as well as the overall performance, eg in mIoU. The experimental
results demonstrate the effectiveness of our method and the importance of
boundaries for 3D point cloud segmentation. Code and model will be made
publicly available at https://github.com/LiyaoTang/contrastBoundary.",https://github.com/LiyaoTang/contrastBoundary,-1
Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data,0.995833,"Retrieval-based methods have been shown to be effective in NLP tasks via
introducing external knowledge. However, the indexing and retrieving of
large-scale corpora bring considerable computational cost. Surprisingly, we
found that REtrieving from the traINing datA (REINA) only can lead to
significant gains on multiple NLG and NLU tasks. We retrieve the labeled
training instances most similar to the input text and then concatenate them
with the input to feed into the model to generate the output. Experimental
results show that this simple method can achieve significantly better
performance on a variety of NLU and NLG tasks, including summarization, machine
translation, language modeling, and question answering tasks. For instance, our
proposed method achieved state-of-the-art results on XSum, BigPatent, and
CommonsenseQA. Our code is released, https://github.com/microsoft/REINA .",https://github.com/microsoft/REINA,-1
Changing the Representation: Examining Language Representation for Neural Sign Language Production,0.636322,"Neural Sign Language Production (SLP) aims to automatically translate from
spoken language sentences to sign language videos. Historically the SLP task
has been broken into two steps; Firstly, translating from a spoken language
sentence to a gloss sequence and secondly, producing a sign language video
given a sequence of glosses. In this paper we apply Natural Language Processing
techniques to the first step of the SLP pipeline. We use language models such
as BERT and Word2Vec to create better sentence level embeddings, and apply
several tokenization techniques, demonstrating how these improve performance on
the low resource translation task of Text to Gloss. We introduce Text to
HamNoSys (T2H) translation, and show the advantages of using a phonetic
representation for sign language translation rather than a sign level gloss
representation. Furthermore, we use HamNoSys to extract the hand shape of a
sign and use this as additional supervision during training, further increasing
the performance on T2H. Assembling best practise, we achieve a BLEU-4 score of
26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art
baselines.",None,-1
Multimodal data matters: language model pre-training over structured and unstructured electronic health records,0.473879,"As two important textual modalities in electronic health records (EHR), both
structured data (clinical codes) and unstructured data (clinical narratives)
have recently been increasingly applied to the healthcare domain. Most existing
EHR-oriented studies, however, either focus on a particular modality or
integrate data from different modalities in a straightforward manner, which
usually treats structured and unstructured data as two independent sources of
information about patient admission and ignore the intrinsic interactions
between them. In fact, the two modalities are documented during the same
encounter where structured data inform the documentation of unstructured data
and vice versa. In this paper, we proposed a Medical Multimodal Pre-trained
Language Model, named MedM-PLM, to learn enhanced EHR representations over
structured and unstructured data and explore the interaction of two modalities.
In MedM-PLM, two Transformer-based neural network components are firstly
adopted to learn representative characteristics from each modality. A
cross-modal module is then introduced to model their interactions. We
pre-trained MedM-PLM on the MIMIC-III dataset and verified the effectiveness of
the model on three downstream clinical tasks, i.e., medication recommendation,
30-day readmission prediction and ICD coding. Extensive experiments demonstrate
the power of MedM-PLM compared with state-of-the-art methods. Further analyses
and visualizations show the robustness of our model, which could potentially
provide more comprehensive interpretations for clinical decision-making.",https://git.openi.org.cn/liusc/3-6-li-usicen-multi-modal-pretrain,-1
Revisiting Self-Supervised Contrastive Learning for Facial Expression Recognition,0.645136,"The success of most advanced facial expression recognition works relies
heavily on large-scale annotated datasets. However, it poses great challenges
in acquiring clean and consistent annotations for facial expression datasets.
On the other hand, self-supervised contrastive learning has gained great
popularity due to its simple yet effective instance discrimination training
strategy, which can potentially circumvent the annotation issue. Nevertheless,
there remain inherent disadvantages of instance-level discrimination, which are
even more challenging when faced with complicated facial representations. In
this paper, we revisit the use of self-supervised contrastive learning and
explore three core strategies to enforce expression-specific representations
and to minimize the interference from other facial attributes, such as identity
and face styling. Experimental results show that our proposed method
outperforms the current state-of-the-art self-supervised learning methods, in
terms of both categorical and dimensional facial expression recognition tasks.",None,-1
Tencent's Multilingual Machine Translation System for WMT22 Large-Scale African Languages,0.590783,"This paper describes Tencent's multilingual machine translation systems for
the WMT22 shared task on Large-Scale Machine Translation Evaluation for African
Languages. We participated in the $\mathbf{constrained}$ translation track in
which only the data and pretrained models provided by the organizer are
allowed. The task is challenging due to three problems, including the absence
of training data for some to-be-evaluated language pairs, the uneven
optimization of language pairs caused by data imbalance, and the curse of
multilinguality. To address these problems, we adopt data augmentation,
distributionally robust optimization, and language family grouping,
respectively, to develop our multilingual neural machine translation (MNMT)
models. Our submissions won the $\mathbf{1st\ place}$ on the blind test sets in
terms of the automatic evaluation metrics. Codes, models, and detailed
competition results are available at
https://github.com/wxjiao/WMT2022-Large-Scale-African.",https://github.com/wxjiao/,-1
Sample-Efficient Reinforcement Learning of Partially Observable Markov Games,0.754062,"This paper considers the challenging tasks of Multi-Agent Reinforcement
Learning (MARL) under partial observability, where each agent only sees her own
individual observations and actions that reveal incomplete information about
the underlying state of system. This paper studies these tasks under the
general model of multiplayer general-sum Partially Observable Markov Games
(POMGs), which is significantly larger than the standard model of Imperfect
Information Extensive-Form Games (IIEFGs). We identify a rich subclass of POMGs
-- weakly revealing POMGs -- in which sample-efficient learning is tractable.
In the self-play setting, we prove that a simple algorithm combining optimism
and Maximum Likelihood Estimation (MLE) is sufficient to find approximate Nash
equilibria, correlated equilibria, as well as coarse correlated equilibria of
weakly revealing POMGs, in a polynomial number of samples when the number of
agents is small. In the setting of playing against adversarial opponents, we
show that a variant of our optimistic MLE algorithm is capable of achieving
sublinear regret when being compared against the optimal maximin policies. To
our best knowledge, this work provides the first line of sample-efficient
results for learning POMGs.",None,-1
StereoKG: Data-Driven Knowledge Graph Construction for Cultural Knowledge and Stereotypes,0.274136,"Analyzing ethnic or religious bias is important for improving fairness,
accountability, and transparency of natural language processing models.
However, many techniques rely on human-compiled lists of bias terms, which are
expensive to create and are limited in coverage. In this study, we present a
fully data-driven pipeline for generating a knowledge graph (KG) of cultural
knowledge and stereotypes. Our resulting KG covers 5 religious groups and 5
nationalities and can easily be extended to include more entities. Our human
evaluation shows that the majority (59.2%) of non-singleton entries are
coherent and complete stereotypes. We further show that performing intermediate
masked language model training on the verbalized KG leads to a higher level of
cultural awareness in the model and has the potential to increase
classification performance on knowledge-crucial samples on a related task,
i.e., hate speech detection.",https://github.com/uds-lsv/StereoKG,4373
UCEpic: Unifying Aspect Planning and Lexical Constraints for Generating Explanations in Recommendation,0.0271996,"Personalized natural language generation for explainable recommendations
plays a key role in justifying why a recommendation might match a user's
interests. Existing models usually control the generation process by aspect
planning. While promising, these aspect-planning methods struggle to generate
specific information correctly, which prevents generated explanations from
being convincing. In this paper, we claim that introducing lexical constraints
can alleviate the above issues. We propose a model, UCEpic, that generates
high-quality personalized explanations for recommendation results by unifying
aspect planning and lexical constraints in an insertion-based generation
manner.
  Methodologically, to ensure text generation quality and robustness to various
lexical constraints, we pre-train a non-personalized text generator via our
proposed robust insertion process. Then, to obtain personalized explanations
under this framework of insertion-based generation, we design a method of
incorporating aspect planning and personalized references into the insertion
process. Hence, UCEpic unifies aspect planning and lexical constraints into one
framework and generates explanations for recommendations under different
settings. Compared to previous recommendation explanation generators controlled
by only aspects, UCEpic incorporates specific information from keyphrases and
then largely improves the diversity and informativeness of generated
explanations for recommendations on datasets such as RateBeer and Yelp.",https://github.com/JiachengLi1995/UCEpic,-1
PolyHope: Two-Level Hope Speech Detection from Tweets,0.848905,"Hope is characterized as openness of spirit toward the future, a desire,
expectation, and wish for something to happen or to be true that remarkably
affects human's state of mind, emotions, behaviors, and decisions. Hope is
usually associated with concepts of desired expectations and
possibility/probability concerning the future. Despite its importance, hope has
rarely been studied as a social media analysis task. This paper presents a hope
speech dataset that classifies each tweet first into ""Hope"" and ""Not Hope"",
then into three fine-grained hope categories: ""Generalized Hope"", ""Realistic
Hope"", and ""Unrealistic Hope"" (along with ""Not Hope""). English tweets in the
first half of 2022 were collected to build this dataset. Furthermore, we
describe our annotation process and guidelines in detail and discuss the
challenges of classifying hope and the limitations of the existing hope speech
detection corpora. In addition, we reported several baselines based on
different learning approaches, such as traditional machine learning, deep
learning, and transformers, to benchmark our dataset. We evaluated our
baselines using weighted-averaged and macro-averaged F1-scores. Observations
show that a strict process for annotator selection and detailed annotation
guidelines enhanced the dataset's quality. This strict annotation process
resulted in promising performance for simple machine learning classifiers with
only bi-grams; however, binary and multiclass hope speech detection results
reveal that contextual embedding models have higher performance in this
dataset.",None,-1
A device-interaction model for users with special needs,0.0283666,"Interaction is a fundamental part of using any computer system but it is
still an issue for people with special needs. In order to improve this
situation, this paper describes a new device-interaction model based on
adaptation rules for user models. The aim is the adaptation at the interaction
level, taking into account the interaction device features in order to improve
the usability through the user experience in the education sector. In the
evaluation process, several students from a special education center have
participated. These students have either a physical or sensory disability or
autism. The results are promising enough to consider that this model will be
able to help students with disabilities to interact with a computer system
which will inevitably provide tremendous benefits to their academic and
personal development.",None,-1
Attend to the Right Context: A Plug-and-Play Module for Content-Controllable Summarization,0.128067,"Content-Controllable Summarization generates summaries focused on the given
controlling signals. Due to the lack of large-scale training corpora for the
task, we propose a plug-and-play module RelAttn to adapt any general
summarizers to the content-controllable summarization task. RelAttn first
identifies the relevant content in the source documents, and then makes the
model attend to the right context by directly steering the attention weight. We
further apply an unsupervised online adaptive parameter searching algorithm to
determine the degree of control in the zero-shot setting, while such parameters
are learned in the few-shot setting. By applying the module to three backbone
summarization models, experiments show that our method effectively improves all
the summarizers, and outperforms the prefix-based method and a widely used
plug-and-play model in both zero- and few-shot settings. Tellingly, more
benefit is observed in the scenarios when more control is needed.",https://github.com/Wendy-Xiao/relattn_controllable_summ,-1
Scaling Laws Beyond Backpropagation,0.155408,"Alternatives to backpropagation have long been studied to better understand
how biological brains may learn. Recently, they have also garnered interest as
a way to train neural networks more efficiently. By relaxing constraints
inherent to backpropagation (e.g., symmetric feedforward and feedback weights,
sequential updates), these methods enable promising prospects, such as local
learning. However, the tradeoffs between different methods in terms of final
task performance, convergence speed, and ultimately compute and data
requirements are rarely outlined. In this work, we use scaling laws to study
the ability of Direct Feedback Alignment~(DFA) to train causal decoder-only
Transformers efficiently. Scaling laws provide an overview of the tradeoffs
implied by a modeling decision, up to extrapolating how it might transfer to
increasingly large models. We find that DFA fails to offer more efficient
scaling than backpropagation: there is never a regime for which the degradation
in loss incurred by using DFA is worth the potential reduction in compute
budget. Our finding comes at variance with previous beliefs in the alternative
training methods community, and highlights the need for holistic empirical
approaches to better understand modeling decisions.",None,-1
The Neural Race Reduction: Dynamics of Abstraction in Gated Networks,0.761019,"Our theoretical understanding of deep learning has not kept pace with its
empirical success. While network architecture is known to be critical, we do
not yet understand its effect on learned representations and network behavior,
or how this architecture should reflect task structure.In this work, we begin
to address this gap by introducing the Gated Deep Linear Network framework that
schematizes how pathways of information flow impact learning dynamics within an
architecture. Crucially, because of the gating, these networks can compute
nonlinear functions of their input. We derive an exact reduction and, for
certain cases, exact solutions to the dynamics of learning. Our analysis
demonstrates that the learning dynamics in structured networks can be
conceptualized as a neural race with an implicit bias towards shared
representations, which then govern the model's ability to systematically
generalize, multi-task, and transfer. We validate our key insights on
naturalistic datasets and with relaxed assumptions. Taken together, our work
gives rise to general hypotheses relating neural architecture to learning and
provides a mathematical approach towards understanding the design of more
complex architectures and the role of modularity and compositionality in
solving real-world problems. The code and results are available at
https://www.saxelab.org/gated-dln .",None,1844
HRPlanes: High Resolution Airplane Dataset for Deep Learning,0.013068,"Airplane detection from satellite imagery is a challenging task due to the
complex backgrounds in the images and differences in data acquisition
conditions caused by the sensor geometry and atmospheric effects. Deep learning
methods provide reliable and accurate solutions for automatic detection of
airplanes; however, huge amount of training data is required to obtain
promising results. In this study, we create a novel airplane detection dataset
called High Resolution Planes (HRPlanes) by using images from Google Earth (GE)
and labeling the bounding box of each plane on the images. HRPlanes include GE
images of several different airports across the world to represent a variety of
landscape, seasonal and satellite geometry conditions obtained from different
satellites. We evaluated our dataset with two widely used object detection
methods namely YOLOv4 and Faster R-CNN. Our preliminary results show that the
proposed dataset can be a valuable data source and benchmark data set for
future applications. Moreover, proposed architectures and results of this study
could be used for transfer learning of different datasets and models for
airplane detection.",None,-1
Topic Modeling on Clinical Social Work Notes for Exploring Social Determinants of Health Factors,0.0424149,"Most research studying social determinants of health (SDoH) has focused on
physician notes or structured elements of the electronic medical record (EMR).
We hypothesize that clinical notes from social workers, whose role is to
ameliorate social and economic factors, might provide a richer source of data
on SDoH. We sought to perform topic modeling to identify robust topics of
discussion within a large cohort of social work notes. We retrieved a diverse,
deidentified corpus of 0.95 million clinical social work notes from 181,644
patients at the University of California, San Francisco. We used word frequency
analysis and Latent Dirichlet Allocation (LDA) topic modeling analysis to
characterize this corpus and identify potential topics of discussion. Word
frequency analysis identified both medical and non-medical terms associated
with specific ICD10 chapters. The LDA topic modeling analysis extracted 11
topics related to social determinants of health risk factors including
financial status, abuse history, social support, risk of death, and mental
health. In addition, the topic modeling approach captured the variation between
different types of social work notes and across patients with different types
of diseases or conditions. We demonstrated that social work notes contain rich,
unique, and otherwise unobtainable information on an individual's SDoH.",https://github.com/ShenghuanSun/LDA_TM,-1
Natural Language Sentence Generation from API Specifications,0.0565936,"APIs are everywhere; they provide access to automation solutions that could
help businesses automate some of their tasks. Unfortunately, they may not be
accessible to the business users who need them but are not equipped with the
necessary technical skills to leverage them. Wrapping these APIs with chatbot
capabilities is one solution to make these automation solutions interactive. In
this work, we propose a system to generate sentences to train intent
recognition models, a crucial component within chatbots to understand natural
language utterances from users. Evaluation of our approach based on deep
learning models showed promising and inspiring results, and the
human-in-the-loop interaction will provide further improvement on the system.",None,-1
Learnable Graph Convolutional Network and Feature Fusion for Multi-view Learning,0.983776,"In practical applications, multi-view data depicting objectives from assorted
perspectives can facilitate the accuracy increase of learning algorithms.
However, given multi-view data, there is limited work for learning
discriminative node relationships and graph information simultaneously via
graph convolutional network that has drawn the attention from considerable
researchers in recent years. Most of existing methods only consider the
weighted sum of adjacency matrices, yet a joint neural network of both feature
and graph fusion is still under-explored. To cope with these issues, this paper
proposes a joint deep learning framework called Learnable Graph Convolutional
Network and Feature Fusion (LGCN-FF), consisting of two stages: feature fusion
network and learnable graph convolutional network. The former aims to learn an
underlying feature representation from heterogeneous views, while the latter
explores a more discriminative graph fusion via learnable weights and a
parametric activation function dubbed Differentiable Shrinkage Activation (DSA)
function. The proposed LGCN-FF is validated to be superior to various
state-of-the-art methods in multi-view semi-supervised classification.",None,-1
TabLLM: Few-shot Classification of Tabular Data with Large Language Models,1.0,"We study the application of large language models to zero-shot and few-shot
classification of tabular data. We prompt the large language model with a
serialization of the tabular data to a natural-language string, together with a
short description of the classification problem. In the few-shot setting, we
fine-tune the large language model using some labeled examples. We evaluate
several serialization methods including templates, table-to-text models, and
large language models. Despite its simplicity, we find that this technique
outperforms prior deep-learning-based tabular classification methods on several
benchmark datasets. In most cases, even zero-shot classification obtains
non-trivial performance, illustrating the method's ability to exploit prior
knowledge encoded in large language models. Unlike many deep learning methods
for tabular datasets, this approach is also competitive with strong traditional
baselines like gradient-boosted trees, especially in the very-few-shot setting.",https://github.com/clinicalml/TabLLM,-1
A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection,0.243719,"We propose a stealthy clean-label video backdoor attack against Deep Learning
(DL)-based models aiming at detecting a particular class of spoofing attacks,
namely video rebroadcast attacks. The injected backdoor does not affect
spoofing detection in normal conditions, but induces a misclassification in the
presence of a specific triggering signal. The proposed backdoor relies on a
temporal trigger altering the average chrominance of the video sequence. The
backdoor signal is designed by taking into account the peculiarities of the
Human Visual System (HVS) to reduce the visibility of the trigger, thus
increasing the stealthiness of the backdoor. To force the network to look at
the presence of the trigger in the challenging clean-label scenario, we choose
the poisoned samples used for the injection of the backdoor following a
so-called Outlier Poisoning Strategy (OPS). According to OPS, the triggering
signal is inserted in the training samples that the network finds more
difficult to classify. The effectiveness of the proposed backdoor attack and
its generality are validated experimentally on different datasets and
anti-spoofing rebroadcast detection architectures.",None,-1
Help Me Explore: Minimal Social Interventions for Graph-Based Autotelic Agents,0.572333,"In the quest for autonomous agents learning open-ended repertoires of skills,
most works take a Piagetian perspective: learning trajectories are the results
of interactions between developmental agents and their physical environment.
The Vygotskian perspective, on the other hand, emphasizes the centrality of the
socio-cultural environment: higher cognitive functions emerge from
transmissions of socio-cultural processes internalized by the agent. This paper
argues that both perspectives could be coupled within the learning of autotelic
agents to foster their skill acquisition. To this end, we make two
contributions: 1) a novel social interaction protocol called Help Me Explore
(HME), where autotelic agents can benefit from both individual and socially
guided exploration. In social episodes, a social partner suggests goals at the
frontier of the learning agent knowledge. In autotelic episodes, agents can
either learn to master their own discovered goals or autonomously rehearse
failed social goals; 2) GANGSTR, a graph-based autotelic agent for manipulation
domains capable of decomposing goals into sequences of intermediate sub-goals.
We show that when learning within HME, GANGSTR overcomes its individual
learning limits by mastering the most complex configurations (e.g. stacks of 5
blocks) with only few social interventions.",https://github.com/akakzia/gangstr,-1
ViT2Hash: Unsupervised Information-Preserving Hashing,0.120351,"Unsupervised image hashing, which maps images into binary codes without
supervision, is a compressor with a high compression rate. Hence, how to
preserving meaningful information of the original data is a critical problem.
Inspired by the large-scale vision pre-training model, known as ViT, which has
shown significant progress for learning visual representations, in this paper,
we propose a simple information-preserving compressor to finetune the ViT model
for the target unsupervised hashing task. Specifically, from pixels to
continuous features, we first propose a feature-preserving module, using the
corrupted image as input to reconstruct the original feature from the
pre-trained ViT model and the complete image, so that the feature extractor can
focus on preserving the meaningful information of original data. Secondly, from
continuous features to hash codes, we propose a hashing-preserving module,
which aims to keep the semantic information from the pre-trained ViT model by
using the proposed Kullback-Leibler divergence loss. Besides, the quantization
loss and the similarity loss are added to minimize the quantization error. Our
method is very simple and achieves a significantly higher degree of MAP on
three benchmark image datasets.",None,-1
Realistic Blur Synthesis for Learning Image Deblurring,0.737305,"Training learning-based deblurring methods demands a tremendous amount of
blurred and sharp image pairs. Unfortunately, existing synthetic datasets are
not realistic enough, and deblurring models trained on them cannot handle real
blurred images effectively. While real datasets have recently been proposed,
they provide limited diversity of scenes and camera settings, and capturing
real datasets for diverse settings is still challenging. To resolve this, this
paper analyzes various factors that introduce differences between real and
synthetic blurred images. To this end, we present RSBlur, a novel dataset with
real blurred images and the corresponding sharp image sequences to enable a
detailed analysis of the difference between real and synthetic blur. With the
dataset, we reveal the effects of different factors in the blur generation
process. Based on the analysis, we also present a novel blur synthesis pipeline
to synthesize more realistic blur. We show that our synthesis pipeline can
improve the deblurring performance on real blurred images.",None,-1
Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement,0.895265,"Low-Light Image Enhancement is a computer vision task which intensifies the
dark images to appropriate brightness. It can also be seen as an ill-posed
problem in image restoration domain. With the success of deep neural networks,
the convolutional neural networks surpass the traditional algorithm-based
methods and become the mainstream in the computer vision area. To advance the
performance of enhancement algorithms, we propose an image enhancement network
(HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use
a half wavelet attention block on M-Net+ to enrich the features from wavelet
domain. Furthermore, our HWMNet has competitive performance results on two
image enhancement datasets in terms of quantitative metrics and visual quality.
The source code and pretrained model are available at
https://github.com/FanChiMao/HWMNet.",https://github.com/FanChiMao/HWMNet,-1
Dynamic Local Aggregation Network with Adaptive Clusterer for Anomaly Detection,0.612392,"Existing methods for anomaly detection based on memory-augmented autoencoder
(AE) have the following drawbacks: (1) Establishing a memory bank requires
additional memory space. (2) The fixed number of prototypes from subjective
assumptions ignores the data feature differences and diversity. To overcome
these drawbacks, we introduce DLAN-AC, a Dynamic Local Aggregation Network with
Adaptive Clusterer, for anomaly detection. First, The proposed DLAN can
automatically learn and aggregate high-level features from the AE to obtain
more representative prototypes, while freeing up extra memory space. Second,
The proposed AC can adaptively cluster video data to derive initial prototypes
with prior information. In addition, we also propose a dynamic redundant
clustering strategy (DRCS) to enable DLAN for automatically eliminating feature
clusters that do not contribute to the construction of prototypes. Extensive
experiments on benchmarks demonstrate that DLAN-AC outperforms most existing
methods, validating the effectiveness of our method. Our code is publicly
available at https://github.com/Beyond-Zw/DLAN-AC.",https://github.com/Beyond-Zw/DLAN-AC,-1
AARGH! End-to-end Retrieval-Generation for Task-Oriented Dialog,0.473217,"We introduce AARGH, an end-to-end task-oriented dialog system combining
retrieval and generative approaches in a single model, aiming at improving
dialog management and lexical diversity of outputs. The model features a new
response selection method based on an action-aware training objective and a
simplified single-encoder retrieval architecture which allow us to build an
end-to-end retrieval-enhanced generation model where retrieval and generation
share most of the parameters. On the MultiWOZ dataset, we show that our
approach produces more diverse outputs while maintaining or improving state
tracking and context-to-response generation performance, compared to
state-of-the-art baselines.",https://github.com/Tomiinek/Aargh,-1
LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension,0.614527,"The application of Natural Language Processing (NLP) to specialized domains,
such as the law, has recently received a surge of interest. As many legal
services rely on processing and analyzing large collections of documents,
automating such tasks with NLP tools emerges as a key challenge. Many popular
language models, such as BERT or RoBERTa, are general-purpose models, which
have limitations on processing specialized legal terminology and syntax. In
addition, legal documents may contain specialized vocabulary from other
domains, such as medical terminology in personal injury text. Here, we propose
LegalRelectra, a legal-domain language model that is trained on mixed-domain
legal and medical corpora. We show that our model improves over general-domain
and single-domain medical and legal language models when processing
mixed-domain (personal injury) text. Our training architecture implements the
Electra framework, but utilizes Reformer instead of BERT for its generator and
discriminator. We show that this improves the model's performance on processing
long passages and results in better long-range text comprehension.",None,-1
Anomaly Detection in Emails using Machine Learning and Header Information,0.397112,"Anomalies in emails such as phishing and spam present major security risks
such as the loss of privacy, money, and brand reputation to both individuals
and organizations. Previous studies on email anomaly detection relied on a
single type of anomaly and the analysis of the email body and subject content.
A drawback of this approach is that it takes into account the written language
of the email content. To overcome this deficit, this study conducted feature
extraction and selection on email header datasets and leveraged both multi and
one-class anomaly detection approaches. Experimental analysis results obtained
demonstrate that email header information only is enough to reliably detect
spam and phishing emails. Supervised learning algorithms such as Random Forest,
SVM, MLP, KNN, and their stacked ensembles were found to be very successful,
achieving high accuracy scores of 97% for phishing and 99% for spam emails.
One-class classification with One-Class SVM achieved accuracy scores of 87% and
89% with spam and phishing emails, respectively. Real-world email filtering
applications will benefit from the use of only the header information in terms
of resources utilization and efficiency.",https://github.com/kregg34/EmailHeaderAnomalyDetection,-1
Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement,0.982031,"Diabetic retinopathy (DR) and diabetic macular edema (DME) are leading causes
of permanent blindness worldwide. Designing an automatic grading system with
good generalization ability for DR and DME is vital in clinical practice.
However, prior works either grade DR or DME independently, without considering
internal correlations between them, or grade them jointly by shared feature
representation, yet ignoring potential generalization issues caused by
difficult samples and data bias. Aiming to address these problems, we propose a
framework for joint grading with the dynamic difficulty-aware weighted loss
(DAW) and the dual-stream disentangled learning architecture (DETACH). Inspired
by curriculum learning, DAW learns from simple samples to difficult samples
dynamically via measuring difficulty adaptively. DETACH separates features of
grading tasks to avoid potential emphasis on the bias. With the addition of DAW
and DETACH, the model learns robust disentangled feature representations to
explore internal correlations between DR and DME and achieve better grading
performance. Experiments on three benchmarks show the effectiveness and
robustness of our framework under both the intra-dataset and cross-dataset
tests.",None,-1
Intake Monitoring in Free-Living Conditions: Overview and Lessons we Have Learned,0.0774131,"The progress in artificial intelligence and machine learning algorithms over
the past decade has enabled the development of new methods for the objective
measurement of eating, including both the measurement of eating episodes as
well as the measurement of in-meal eating behavior. These allow the study of
eating behavior outside the laboratory in free-living conditions, without the
need for video recordings and laborious manual annotations. In this paper, we
present a high-level overview of our recent work on intake monitoring using a
smartwatch, as well as methods using an in-ear microphone. We also present
evaluation results of these methods in challenging, real-world datasets.
Furthermore, we discuss use-cases of such intake monitoring tools for advancing
research in eating behavior, for improving dietary monitoring, as well as for
developing evidence-based health policies. Our goal is to inform researchers
and users of intake monitoring methods regarding (i) the development of new
methods based on commercially available devices, (ii) what to expect in terms
of effectiveness, and (iii) how these methods can be used in research as well
as in practical applications.",None,-1
On the Paradox of Learning to Reason from Data,0.787685,"Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be
trained end-to-end to solve logical reasoning problems presented in natural
language? We attempt to answer this question in a confined problem space where
there exists a set of parameters that perfectly simulates logical reasoning. We
make observations that seem to contradict each other: BERT attains near-perfect
accuracy on in-distribution test examples while failing to generalize to other
data distributions over the exact same problem space. Our study provides an
explanation for this paradox: instead of learning to emulate the correct
reasoning function, BERT has in fact learned statistical features that
inherently exist in logical reasoning problems. We also show that it is
infeasible to jointly remove statistical features from data, illustrating the
difficulty of learning to reason in general. Our result naturally extends to
other neural models and unveils the fundamental difference between learning to
reason and learning to achieve high performance on NLP benchmarks using
statistical features.",https://github.com/joshuacnf/paradox-learning2reason,34559
A Diversity-Aware Domain Development Methodology,0.403651,"The development of domain ontological models, though being a mature research
arena backed by well-established methodologies, still suffer from two key
shortcomings. Firstly, the issues concerning the semantic persistency of
ontology concepts and their flexible reuse in domain development employing
existing approaches. Secondly, due to the difficulty in understanding and
reusing top-level concepts in existing foundational ontologies, the obfuscation
regarding the semantic nature of domain representations. The paper grounds the
aforementioned shortcomings in representation diversity and proposes a
three-fold solution - (i) a pipeline for rendering concepts reuse-ready, (ii) a
first characterization of a minimalistic foundational knowledge model, named
foundational teleology, semantically explicating foundational distinctions
enforcing the static as well as dynamic nature of domain representations, and
(iii) a flexible, reuse-native methodology for diversity-aware domain
development exploiting solutions (i) and (ii). The preliminary work reported
validates the potentiality of the solution components.",None,-1
METGEN: A Module-Based Entailment Tree Generation Framework for Answer Explanation,0.955353,"Knowing the reasoning chains from knowledge to the predicted answers can help
construct an explainable question answering (QA) system. Advances on QA
explanation propose to explain the answers with entailment trees composed of
multiple entailment steps. While current work proposes to generate entailment
trees with end-to-end generative models, the steps in the generated trees are
not constrained and could be unreliable. In this paper, we propose METGEN, a
Module-based Entailment Tree GENeration framework that has multiple modules and
a reasoning controller. Given a question and several supporting knowledge,
METGEN can iteratively generate the entailment tree by conducting single-step
entailment with separate modules and selecting the reasoning flow with the
controller. As each module is guided to perform a specific type of entailment
reasoning, the steps generated by METGEN are more reliable and valid.
Experiment results on the standard benchmark show that METGEN can outperform
previous state-of-the-art models with only 9% of the parameters.",https://github.com/huggingface/transformers,-1
Evolutionary Game-Theoretical Analysis for General Multiplayer Asymmetric Games,0.590861,"Evolutionary game theory has been a successful tool to combine classical game
theory with learning-dynamical descriptions in multiagent systems. Provided
some symmetric structures of interacting players, many studies have been
focused on using a simplified heuristic payoff table as input to analyse the
dynamics of interactions. Nevertheless, even for the state-of-the-art method,
there are two limits. First, there is inaccuracy when analysing the simplified
payoff table. Second, no existing work is able to deal with 2-population
multiplayer asymmetric games. In this paper, we fill the gap between heuristic
payoff table and dynamic analysis without any inaccuracy. In addition, we
propose a general framework for $m$ versus $n$ 2-population multiplayer
asymmetric games. Then, we compare our method with the state-of-the-art in some
classic games. Finally, to illustrate our method, we perform empirical
game-theoretical analysis on Wolfpack as well as StarCraft II, both of which
involve complex multiagent interactions.",None,-1
Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets,0.493145,"Natural language processing models often exploit spurious correlations
between task-independent features and labels in datasets to perform well only
within the distributions they are trained on, while not generalising to
different task distributions. We propose to tackle this problem by generating a
debiased version of a dataset, which can then be used to train a debiased,
off-the-shelf model, by simply replacing its training data. Our approach
consists of 1) a method for training data generators to generate high-quality,
label-consistent data samples; and 2) a filtering mechanism for removing data
points that contribute to spurious correlations, measured in terms of
z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and
we evaluate on a large suite of debiased, out-of-distribution, and adversarial
test sets. Results show that models trained on our debiased datasets generalise
better than those trained on the original datasets in all settings. On the
majority of the datasets, our method outperforms or performs comparably to
previous state-of-the-art debiasing strategies, and when combined with an
orthogonal technique, product-of-experts, it improves further and outperforms
previous best results of SNLI-hard and MNLI-hard.",https://github.com/jimmycode/,-1
Semantic Enhanced Text-to-SQL Parsing via Iteratively Learning Schema Linking Graph,0.763441,"The generalizability to new databases is of vital importance to Text-to-SQL
systems which aim to parse human utterances into SQL statements. Existing works
achieve this goal by leveraging the exact matching method to identify the
lexical matching between the question words and the schema items. However,
these methods fail in other challenging scenarios, such as the synonym
substitution in which the surface form differs between the corresponding
question words and schema items. In this paper, we propose a framework named
ISESL-SQL to iteratively build a semantic enhanced schema-linking graph between
question tokens and database schemas. First, we extract a schema linking graph
from PLMs through a probing procedure in an unsupervised manner. Then the
schema linking graph is further optimized during the training process through a
deep graph learning method. Meanwhile, we also design an auxiliary task called
graph regularization to improve the schema information mentioned in the
schema-linking graph. Extensive experiments on three benchmarks demonstrate
that ISESL-SQL could consistently outperform the baselines and further
investigations show its generalizability and robustness.",https://github.com/THU-BPM/ISESL-SQL,7471
Is Conditional Generative Modeling all you need for Decision-Making?,0.988665,"Recent improvements in conditional generative modeling have made it possible
to generate high-quality images from language descriptions alone. We
investigate whether these methods can directly address the problem of
sequential decision-making. We view decision-making not through the lens of
reinforcement learning (RL), but rather through conditional generative
modeling. To our surprise, we find that our formulation leads to policies that
can outperform existing offline RL approaches across standard benchmarks. By
modeling a policy as a return-conditional diffusion model, we illustrate how we
may circumvent the need for dynamic programming and subsequently eliminate many
of the complexities that come with traditional offline RL. We further
demonstrate the advantages of modeling policies as conditional diffusion models
by considering two other conditioning variables: constraints and skills.
Conditioning on a single constraint or skill during training leads to behaviors
at test-time that can satisfy several constraints together or demonstrate a
composition of skills. Our results illustrate that conditional generative
modeling is a powerful tool for decision-making.",https://github.com/jannerm/diffuser,-1
4DenoiseNet: Adverse Weather Denoising from Adjacent Point Clouds,0.902202,"Reliable point cloud data is essential for perception tasks \textit{e.g.} in
robotics and autonomous driving applications. Adverse weather causes a specific
type of noise to light detection and ranging (LiDAR) sensor data, which
degrades the quality of the point clouds significantly. To address this issue,
this letter presents a novel point cloud adverse weather denoising deep
learning algorithm (4DenoiseNet). Our algorithm takes advantage of the time
dimension unlike deep learning adverse weather denoising methods in the
literature. It performs about 10\% better in terms of intersection over union
metric compared to the previous work and is more computationally efficient.
These results are achieved on our novel SnowyKITTI dataset, which has over
40000 adverse weather annotated point clouds. Moreover, strong qualitative
results on the Canadian Adverse Driving Conditions dataset indicate good
generalizability to domain shifts and to different sensor intrinsics.",https://github.com/alvariseppanen/4DenoiseNet,-1
MPA: MultiPath++ Based Architecture for Motion Prediction,0.668082,"Autonomous driving technology is developing rapidly and nowadays first
autonomous rides are being provided in city areas. This requires the highest
standards for the safety and reliability of the technology. Motion prediction
part of the general self-driving pipeline plays a crucial role in providing
these qualities. In this work we present one of the solutions for Waymo Motion
Prediction Challenge 2022 based on MultiPath++ ranked the 3rd as of May, 26
2022. Our source code is publicly available on GitHub.",https://github.com/stepankonev/waymo-motion-prediction-challenge-2022-multipath-plus-plus,-1
NaturalAdversaries: Can Naturalistic Adversaries Be as Effective as Artificial Adversaries?,0.0668951,"While a substantial body of prior work has explored adversarial example
generation for natural language understanding tasks, these examples are often
unrealistic and diverge from the real-world data distributions. In this work,
we introduce a two-stage adversarial example generation framework
(NaturalAdversaries), for designing adversaries that are effective at fooling a
given classifier and demonstrate natural-looking failure cases that could
plausibly occur during in-the-wild deployment of the models.
  At the first stage a token attribution method is used to summarize a given
classifier's behaviour as a function of the key tokens in the input. In the
second stage a generative model is conditioned on the key tokens from the first
stage. NaturalAdversaries is adaptable to both black-box and white-box
adversarial attacks based on the level of access to the model parameters. Our
results indicate these adversaries generalize across domains, and offer
insights for future research on improving robustness of neural text
classification models.",https://github.com/skgabriel/NaturalAdversaries,-1
ROAD-R: The Autonomous Driving Dataset with Logical Requirements,0.807808,"Neural networks have proven to be very powerful at computer vision tasks.
However, they often exhibit unexpected behaviours, violating known requirements
expressing background knowledge. This calls for models (i) able to learn from
the requirements, and (ii) guaranteed to be compliant with the requirements
themselves. Unfortunately, the development of such models is hampered by the
lack of datasets equipped with formally specified requirements. In this paper,
we introduce the ROad event Awareness Dataset with logical Requirements
(ROAD-R), the first publicly available dataset for autonomous driving with
requirements expressed as logical constraints. Given ROAD-R, we show that
current state-of-the-art models often violate its logical constraints, and that
it is possible to exploit them to create models that (i) have a better
performance, and (ii) are guaranteed to be compliant with the requirements
themselves.",https://github.com/gurkirt/road-dataset,-1
On the Importance of Asymmetry for Siamese Representation Learning,0.642189,"Many recent self-supervised frameworks for visual representation learning are
based on certain forms of Siamese networks. Such networks are conceptually
symmetric with two parallel encoders, but often practically asymmetric as
numerous mechanisms are devised to break the symmetry. In this work, we conduct
a formal study on the importance of asymmetry by explicitly distinguishing the
two encoders within the network -- one produces source encodings and the other
targets. Our key insight is keeping a relatively lower variance in target than
source generally benefits learning. This is empirically justified by our
results from five case studies covering different variance-oriented designs,
and is aligned with our preliminary theoretical analysis on the baseline.
Moreover, we find the improvements from asymmetric designs generalize well to
longer training schedules, multiple other frameworks and newer backbones.
Finally, the combined effect of several asymmetric designs achieves a
state-of-the-art accuracy on ImageNet linear probing and competitive results on
downstream transfer. We hope our exploration will inspire more research in
exploiting asymmetry for Siamese representation learning.",https://github.com/facebookresearch/asym-siam,-1
Memory Efficient Patch-based Training for INR-based GANs,0.056817,"Recent studies have shown remarkable progress in GANs based on implicit
neural representation (INR) - an MLP that produces an RGB value given its (x,
y) coordinate. They represent an image as a continuous version of the
underlying 2D signal instead of a 2D array of pixels, which opens new horizons
for GAN applications (e.g., zero-shot super-resolution, image outpainting).
However, training existing approaches require a heavy computational cost
proportional to the image resolution, since they compute an MLP operation for
every (x, y) coordinate. To alleviate this issue, we propose a multi-stage
patch-based training, a novel and scalable approach that can train INR-based
GANs with a flexible computational cost regardless of the image resolution.
Specifically, our method allows to generate and discriminate by patch to learn
the local details of the image and learn global structural information by a
novel reconstruction loss to enable efficient GAN training. We conduct
experiments on several benchmark datasets to demonstrate that our approach
enhances baseline models in GPU memory while maintaining FIDs at a reasonable
level.",None,-1
LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection,0.580291,"Visual question answering (VQA) often requires an understanding of visual
concepts and language semantics, which relies on external knowledge. Most
existing methods exploit pre-trained language models or/and unstructured text,
but the knowledge in these resources are often incomplete and noisy. Some other
methods prefer to use knowledge graphs (KGs) which often have intensive
structured knowledge, but the research is still quite preliminary. In this
paper, we propose LaKo, a knowledge-driven VQA method via Late
Knowledge-to-text Injection. To effectively incorporate an external KG, we
transfer triples into textual format and propose a late injection mechanism for
knowledge fusion. Finally we address VQA as a text generation task with an
effective encoder-decoder paradigm, which achieves state-of-the-art results on
OKVQA dataset.",https://github.com/hackerchenzhuo/LaKo,-1
"""I think this is the most disruptive technology"": Exploring Sentiments of ChatGPT Early Adopters using Twitter Data",0.999993,"Large language models have recently attracted significant attention due to
their impressive performance on a variety of tasks. ChatGPT developed by OpenAI
is one such implementation of a large, pre-trained language model that has
gained immense popularity among early adopters, where certain users go to the
extent of characterizing it as a disruptive technology in many domains.
Understanding such early adopters' sentiments is important because it can
provide insights into the potential success or failure of the technology, as
well as its strengths and weaknesses. In this paper, we conduct a mixed-method
study using 10,732 tweets from early ChatGPT users. We first use topic
modelling to identify the main topics and then perform an in-depth qualitative
sentiment analysis of each topic. Our results show that the majority of the
early adopters have expressed overwhelmingly positive sentiments related to
topics such as Disruptions to software development, Entertainment and
exercising creativity. Only a limited percentage of users expressed concerns
about issues such as the potential for misuse of ChatGPT, especially regarding
topics such as Impact on educational aspects. We discuss these findings by
providing specific examples for each topic and then detail implications related
to addressing these concerns for both researchers and users.",None,440
Geoclidean: Few-Shot Generalization in Euclidean Geometry,0.275196,"Euclidean geometry is among the earliest forms of mathematical thinking.
While the geometric primitives underlying its constructions, such as perfect
lines and circles, do not often occur in the natural world, humans rarely
struggle to perceive and reason with them. Will computer vision models trained
on natural images show the same sensitivity to Euclidean geometry? Here we
explore these questions by studying few-shot generalization in the universe of
Euclidean geometry constructions. We introduce Geoclidean, a domain-specific
language for Euclidean geometry, and use it to generate two datasets of
geometric concept learning tasks for benchmarking generalization judgements of
humans and machines. We find that humans are indeed sensitive to Euclidean
geometry and generalize strongly from a few visual examples of a geometric
concept. In contrast, low-level and high-level visual features from standard
computer vision models pretrained on natural images do not support correct
generalization. Thus Geoclidean represents a novel few-shot generalization
benchmark for geometric concept learning, where the performance of humans and
of AI models diverge. The Geoclidean framework and dataset are publicly
available for download.",https://github.com/joyhsu0504/geoclidean_framework,-1
A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation,0.610134,"Early exiting allows instances to exit at different layers according to the
estimation of difficulty. Previous works usually adopt heuristic metrics such
as the entropy of internal outputs to measure instance difficulty, which
suffers from generalization and threshold-tuning. In contrast, learning to
exit, or learning to predict instance difficulty is a more appealing way.
Though some effort has been devoted to employing such ""learn-to-exit"" modules,
it is still unknown whether and how well the instance difficulty can be
learned. As a response, we first conduct experiments on the learnability of
instance difficulty, which demonstrates that modern neural models perform
poorly on predicting instance difficulty. Based on this observation, we propose
a simple-yet-effective Hash-based Early Exiting approach (HashEE) that replaces
the learn-to-exit modules with hash functions to assign each token to a fixed
exiting layer. Different from previous methods, HashEE requires no internal
classifiers nor extra parameters, and therefore is more efficient. Experimental
results on classification, regression, and generation tasks demonstrate that
HashEE can achieve higher performance with fewer FLOPs and inference time
compared with previous state-of-the-art early exiting methods.",https://github.com/txsun1997/HashEE,21857
ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver Distraction Detection,0.828128,"Ensuring traffic safety and mitigating accidents in modern driving is of
paramount importance, and computer vision technologies have the potential to
significantly contribute to this goal. This paper presents a multi-modal Vision
Transformer for Driver Distraction Detection (termed ViT-DD), which
incorporates inductive information from training signals related to both
distraction detection and driver emotion recognition. Additionally, a
self-learning algorithm is developed, allowing for the seamless integration of
driver data without emotion labels into the multi-task training process of
ViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existing
state-of-the-art methods for driver distraction detection by 6.5% and 0.9% on
the SFDDD and AUCDD datasets, respectively.",None,-1
PersDet: Monocular 3D Detection in Perspective Bird's-Eye-View,0.184808,"Currently, detecting 3D objects in Bird's-Eye-View (BEV) is superior to other
3D detectors for autonomous driving and robotics. However, transforming image
features into BEV necessitates special operators to conduct feature sampling.
These operators are not supported on many edge devices, bringing extra
obstacles when deploying detectors. To address this problem, we revisit the
generation of BEV representation and propose detecting objects in perspective
BEV -- a new BEV representation that does not require feature sampling. We
demonstrate that perspective BEV features can likewise enjoy the benefits of
the BEV paradigm. Moreover, the perspective BEV improves detection performance
by addressing issues caused by feature sampling. We propose PersDet for
high-performance object detection in perspective BEV space based on this
discovery. While implementing a simple and memory-efficient structure, PersDet
outperforms existing state-of-the-art monocular methods on the nuScenes
benchmark, reaching 34.6% mAP and 40.8% NDS when using ResNet-50 as the
backbone.",None,-1
Context-Consistent Semantic Image Editing with Style-Preserved Modulation,0.222757,"Semantic image editing utilizes local semantic label maps to generate the
desired content in the edited region. A recent work borrows SPADE block to
achieve semantic image editing. However, it cannot produce pleasing results due
to style discrepancy between the edited region and surrounding pixels. We
attribute this to the fact that SPADE only uses an image-independent local
semantic layout but ignores the image-specific styles included in the known
pixels. To address this issue, we propose a style-preserved modulation (SPM)
comprising two modulations processes: The first modulation incorporates the
contextual style and semantic layout, and then generates two fused modulation
parameters. The second modulation employs the fused parameters to modulate
feature maps. By using such two modulations, SPM can inject the given semantic
layout while preserving the image-specific context style. Moreover, we design a
progressive architecture for generating the edited content in a coarse-to-fine
manner. The proposed method can obtain context-consistent results and
significantly alleviate the unpleasant boundary between the generated regions
and the known pixels.",https://github.com/WuyangLuo/SPMPGAN,-1
MarkovGNN: Graph Neural Networks on Markov Diffusion,0.177966,"Most real-world networks contain well-defined community structures where
nodes are densely connected internally within communities. To learn from these
networks, we develop MarkovGNN that captures the formation and evolution of
communities directly in different convolutional layers. Unlike most Graph
Neural Networks (GNNs) that consider a static graph at every layer, MarkovGNN
generates different stochastic matrices using a Markov process and then uses
these community-capturing matrices in different layers. MarkovGNN is a general
approach that could be used with most existing GNNs. We experimentally show
that MarkovGNN outperforms other GNNs for clustering, node classification, and
visualization tasks. The source code of MarkovGNN is publicly available at
\url{https://github.com/HipGraph/MarkovGNN}.",https://github.com/HipGraph/MarkovGNN,-1
A Novel Underwater Image Enhancement and Improved Underwater Biological Detection Pipeline,0.323108,"For aquaculture resource evaluation and ecological environment monitoring,
automatic detection and identification of marine organisms is critical.
However, due to the low quality of underwater images and the characteristics of
underwater biological, a lack of abundant features may impede traditional
hand-designed feature extraction approaches or CNN-based object detection
algorithms, particularly in complex underwater environment. Therefore, the goal
of this paper is to perform object detection in the underwater environment.
This paper proposed a novel method for capturing feature information, which
adds the convolutional block attention module (CBAM) to the YOLOv5 backbone.
The interference of underwater creature characteristics on object
characteristics is decreased, and the output of the backbone network to object
information is enhanced. In addition, the self-adaptive global histogram
stretching algorithm (SAGHS) is designed to eliminate the degradation problems
such as low contrast and color loss caused by underwater environmental
information to better restore image quality. Extensive experiments and
comprehensive evaluation on the URPC2021 benchmark dataset demonstrate the
effectiveness and adaptivity of our methods. Beyond that, this paper conducts
an exhaustive analysis of the role of training data on performance.",None,-1
"When classifying grammatical role, BERT doesn't care about word order... except when it matters",0.742317,"Because meaning can often be inferred from lexical semantics alone, word
order is often a redundant cue in natural language. For example, the words
chopped, chef, and onion are more likely used to convey ""The chef chopped the
onion,"" not ""The onion chopped the chef."" Recent work has shown large language
models to be surprisingly word order invariant, but crucially has largely
considered natural prototypical inputs, where compositional meaning mostly
matches lexical expectations. To overcome this confound, we probe grammatical
role representation in English BERT and GPT-2, on instances where lexical
expectations are not sufficient, and word order knowledge is necessary for
correct classification. Such non-prototypical instances are naturally occurring
English sentences with inanimate subjects or animate objects, or sentences
where we systematically swap the arguments to make sentences like ""The onion
chopped the chef"". We find that, while early layer embeddings are largely
lexical, word order is in fact crucial in defining the later-layer
representations of words in semantically non-prototypical positions. Our
experiments isolate the effect of word order on the contextualization process,
and highlight how models use context in the uncommon, but critical, instances
where it matters.",https://github.com/toizzy/except-when-it-matters,4825
Non-rigid Point Cloud Registration with Neural Deformation Pyramid,0.839812,"Non-rigid point cloud registration is a key component in many computer vision
and computer graphics applications. The high complexity of the unknown
non-rigid motion make this task a challenging problem. In this paper, we break
down this problem via hierarchical motion decomposition. Our method called
Neural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid
architecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP),
takes as input a sinusoidally encoded 3D point and outputs its motion
increments from the previous level. The sinusoidal function starts with a low
input frequency and gradually increases when the pyramid level goes down. This
allows a multi-level rigid to nonrigid motion decomposition and also speeds up
the solving by 50 times compared to the existing MLP-based approach. Our method
achieves advanced partialto-partial non-rigid point cloud registration results
on the 4DMatch/4DLoMatch benchmark under both no-learned and supervised
settings.",https://github.com/rabbityl/DeformationPyramid,-1
Xplique: A Deep Learning Explainability Toolbox,0.418994,"Today's most advanced machine-learning models are hardly scrutable. The key
challenge for explainability methods is to help assisting researchers in
opening up these black boxes, by revealing the strategy that led to a given
decision, by characterizing their internal states or by studying the underlying
data representation. To address this challenge, we have developed Xplique: a
software library for explainability which includes representative
explainability methods as well as associated evaluation metrics. It interfaces
with one of the most popular learning libraries: Tensorflow as well as other
libraries including PyTorch, scikit-learn and Theano. The code is licensed
under the MIT license and is freely available at github.com/deel-ai/xplique.",https://github.com/deel-ai/xplique,-1
MM-Claims: A Dataset for Multimodal Claim Detection in Social Media,0.44479,"In recent years, the problem of misinformation on the web has become
widespread across languages, countries, and various social media platforms.
Although there has been much work on automated fake news detection, the role of
images and their variety are not well explored. In this paper, we investigate
the roles of image and text at an earlier stage of the fake news detection
pipeline, called claim detection. For this purpose, we introduce a novel
dataset, MM-Claims, which consists of tweets and corresponding images over
three topics: COVID-19, Climate Change and broadly Technology. The dataset
contains roughly 86000 tweets, out of which 3400 are labeled manually by
multiple annotators for the training and evaluation of multimodal models. We
describe the dataset in detail, evaluate strong unimodal and multimodal
baselines, and analyze the potential and drawbacks of current models.",https://github.com/TIBHannover/MM_Claims,-1
Continuous Scene Representations for Embodied AI,0.861507,"We propose Continuous Scene Representations (CSR), a scene representation
constructed by an embodied agent navigating within a space, where objects and
their relationships are modeled by continuous valued embeddings. Our method
captures feature relationships between objects, composes them into a graph
structure on-the-fly, and situates an embodied agent within the representation.
Our key insight is to embed pair-wise relationships between objects in a latent
space. This allows for a richer representation compared to discrete relations
(e.g., [support], [next-to]) commonly used for building scene representations.
CSR can track objects as the agent moves in a scene, update the representation
accordingly, and detect changes in room configurations. Using CSR, we
outperform state-of-the-art approaches for the challenging downstream task of
visual room rearrangement, without any task specific training. Moreover, we
show the learned embeddings capture salient spatial details of the scene and
show applicability to real world data. A summery video and code is available at
https://prior.allenai.org/projects/csr.",https://github.com/facebookresearch/detectron2,-1
"Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data",0.414772,"Advances in deep learning for human activity recognition have been relatively
limited due to the lack of large labelled datasets. In this study, we leverage
self-supervised learning techniques on the UK-Biobank activity tracker
dataset--the largest of its kind to date--containing more than 700,000
person-days of unlabelled wearable sensor data. Our resulting activity
recognition model consistently outperformed strong baselines across seven
benchmark datasets, with an F1 relative improvement of 2.5%-100% (median
18.4%), the largest improvements occurring in the smaller datasets. In contrast
to previous studies, our results generalise across external datasets, devices,
and environments. Our open-source model will help researchers and developers to
build customisable and generalisable activity classifiers with high
performance.",https://github.com/OxWearables/ssl-wearables,-1
3D Equivariant Graph Implicit Functions,0.393469,"In recent years, neural implicit representations have made remarkable
progress in modeling of 3D shapes with arbitrary topology. In this work, we
address two key limitations of such representations, in failing to capture
local 3D geometric fine details, and to learn from and generalize to shapes
with unseen 3D transformations. To this end, we introduce a novel family of
graph implicit functions with equivariant layers that facilitates modeling fine
local details and guaranteed robustness to various groups of geometric
transformations, through local $k$-NN graph embeddings with sparse point set
observations at multiple resolutions. Our method improves over the existing
rotation-equivariant implicit function from 0.69 to 0.89 (IoU) on the ShapeNet
reconstruction task. We also show that our equivariant implicit function can be
extended to other types of similarity transformations and generalizes to unseen
translations and scaling.",None,-1
Acquiring and Modelling Abstract Commonsense Knowledge via Conceptualization,0.766864,"Conceptualization, or viewing entities and situations as instances of
abstract concepts in mind and making inferences based on that, is a vital
component in human intelligence for commonsense reasoning. Although recent
artificial intelligence has made progress in acquiring and modelling
commonsense, attributed to large neural language models and commonsense
knowledge graphs (CKGs), conceptualization is yet to thoroughly be introduced,
making current approaches ineffective to cover knowledge about countless
diverse entities and situations in the real world. To address the problem, we
thoroughly study the possible role of conceptualization in commonsense
reasoning, and formulate a framework to replicate human conceptual induction
from acquiring abstract knowledge about abstract concepts. Aided by the
taxonomy Probase, we develop tools for contextualized conceptualization on
ATOMIC, a large-scale human annotated CKG. We annotate a dataset for the
validity of conceptualizations for ATOMIC on both event and triple level,
develop a series of heuristic rules based on linguistic features, and train a
set of neural models, so as to generate and verify abstract knowledge. Based on
these components, a pipeline to acquire abstract knowledge is built. A large
abstract CKG upon ATOMIC is then induced, ready to be instantiated to infer
about unseen entities or situations. Furthermore, experiments find directly
augmenting data with abstract triples to be helpful in commonsense modelling.",https://github.com/HKUST-KnowComp/atomic-conceptualization,-1
Falsesum: Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization,0.856856,"Neural abstractive summarization models are prone to generate summaries which
are factually inconsistent with their source documents. Previous work has
introduced the task of recognizing such factual inconsistency as a downstream
application of natural language inference (NLI). However, state-of-the-art NLI
models perform poorly in this context due to their inability to generalize to
the target task. In this work, we show that NLI models can be effective for
this task when the training data is augmented with high-quality task-oriented
examples. We introduce Falsesum, a data generation pipeline leveraging a
controllable text generation model to perturb human-annotated summaries,
introducing varying types of factual inconsistencies. Unlike previously
introduced document-level NLI datasets, our generated dataset contains examples
that are diverse and inconsistent yet plausible. We show that models trained on
a Falsesum-augmented NLI dataset improve the state-of-the-art performance
across four benchmarks for detecting factual inconsistency in summarization.
  The code to obtain the dataset is available online at
https://github.com/joshbambrick/Falsesum",https://github.com/joshbambrick/Falsesum,-1
Deep object detection for waterbird monitoring using aerial imagery,0.608394,"Monitoring of colonial waterbird nesting islands is essential to tracking
waterbird population trends, which are used for evaluating ecosystem health and
informing conservation management decisions. Recently, unmanned aerial
vehicles, or drones, have emerged as a viable technology to precisely monitor
waterbird colonies. However, manually counting waterbirds from hundreds, or
potentially thousands, of aerial images is both difficult and time-consuming.
In this work, we present a deep learning pipeline that can be used to precisely
detect, count, and monitor waterbirds using aerial imagery collected by a
commercial drone. By utilizing convolutional neural network-based object
detectors, we show that we can detect 16 classes of waterbird species that are
commonly found in colonial nesting islands along the Texas coast. Our
experiments using Faster R-CNN and RetinaNet object detectors give mean
interpolated average precision scores of 67.9% and 63.1% respectively.",https://github.com/RiceD2KLab/Audubon F21,-1
nerf2nerf: Pairwise Registration of Neural Radiance Fields,0.60803,"We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io",None,9133
OPERA:Operation-Pivoted Discrete Reasoning over Text,0.201351,"Machine reading comprehension (MRC) that requires discrete reasoning
involving symbolic operations, e.g., addition, sorting, and counting, is a
challenging task. According to this nature, semantic parsing-based methods
predict interpretable but complex logical forms. However, logical form
generation is nontrivial and even a little perturbation in a logical form will
lead to wrong answers. To alleviate this issue, multi-predictor -based methods
are proposed to directly predict different types of answers and achieve
improvements. However, they ignore the utilization of symbolic operations and
encounter a lack of reasoning ability and interpretability. To inherit the
advantages of these two types of methods, we propose OPERA, an
operation-pivoted discrete reasoning framework, where lightweight symbolic
operations (compared with logical forms) as neural modules are utilized to
facilitate the reasoning ability and interpretability. Specifically, operations
are first selected and then softly executed to simulate the answer reasoning
procedure. Extensive experiments on both DROP and RACENum datasets show the
reasoning ability of OPERA. Moreover, further analysis verifies its
interpretability.",https://github.com/JD-AI-Research-NLP/OPERA,-1
textless-lib: a Library for Textless Spoken Language Processing,0.976723,"Textless spoken language processing research aims to extend the applicability
of standard NLP toolset onto spoken language and languages with few or no
textual resources. In this paper, we introduce textless-lib, a PyTorch-based
library aimed to facilitate research in this research area. We describe the
building blocks that the library provides and demonstrate its usability by
discuss three different use-case examples: (i) speaker probing, (ii) speech
resynthesis and compression, and (iii) speech continuation. We believe that
textless-lib substantially simplifies research the textless setting and will be
handful not only for speech researchers but also for the NLP community at
large. The code, documentation, and pre-trained models are available at
https://github.com/facebookresearch/textlesslib/ .",https://github.com/facebookresearch/textlesslib/,-1
Deep Learning Hyperparameter Optimization for Breast Mass Detection in Mammograms,0.0996438,"Accurate breast cancer diagnosis through mammography has the potential to
save millions of lives around the world. Deep learning (DL) methods have shown
to be very effective for mass detection in mammograms. Additional improvements
of current DL models will further improve the effectiveness of these methods. A
critical issue in this context is how to pick the right hyperparameters for DL
models. In this paper, we present GA-E2E, a new approach for tuning the
hyperparameters of DL models for brest cancer detection using Genetic
Algorithms (GAs). Our findings reveal that differences in parameter values can
considerably alter the area under the curve (AUC), which is used to determine a
classifier's performance.",None,-1
Continual Inference: A Library for Efficient Online Inference with Deep Neural Networks in PyTorch,0.347091,"We present Continual Inference, a Python library for implementing Continual
Inference Networks (CINs) in PyTorch, a class of Neural Networks designed
specifically for efficient inference in both online and batch processing
scenarios. We offer a comprehensive introduction and guide to CINs and their
implementation in practice, and provide best-practices and code examples for
composing complex modules for modern Deep Learning. Continual Inference is
readily downloadable via the Python Package Index and at
\url{www.github.com/lukashedegaard/continual-inference}.",https://github.com/lukashedegaard/continual-inference,-1
Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos,0.870179,"Understanding dynamic hand motions and actions from egocentric RGB videos is
a fundamental yet challenging task due to self-occlusion and ambiguity. To
address occlusion and ambiguity, we develop a transformer-based framework to
exploit temporal information for robust estimation. Noticing the different
temporal granularity of and the semantic correlation between hand pose
estimation and action recognition, we build a network hierarchy with two
cascaded transformer encoders, where the first one exploits the short-term
temporal cue for hand pose estimation, and the latter aggregates per-frame pose
and object information over a longer time span to recognize the action. Our
approach achieves competitive results on two first-person hand action
benchmarks, namely FPHA and H2O. Extensive ablation studies verify our design
choices.",https://github.com/fylwen/HTT,-1
Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again,0.999943,"The strong few-shot in-context learning capability of large pre-trained
language models (PLMs) such as GPT-3 is highly appealing for application
domains such as biomedicine, which feature high and diverse demands of language
technologies but also high data annotation costs. In this paper, we present the
first systematic and comprehensive study to compare the few-shot performance of
GPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on
two highly representative biomedical information extraction tasks, named entity
recognition and relation extraction. We follow the true few-shot setting to
avoid overestimating models' few-shot performance by model selection over a
large validation set. We also optimize GPT-3's performance with known
techniques such as contextual calibration and dynamic in-context example
retrieval. However, our results show that GPT-3 still significantly
underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3
in-context learning also yields smaller gains in accuracy when more training
data becomes available. Our in-depth analyses further reveal issues of the
in-context learning setting that may be detrimental to information extraction
tasks in general. Given the high cost of experimenting with GPT-3, we hope our
study provides guidance for biomedical researchers and practitioners towards
more promising directions such as fine-tuning small PLMs.",https://github.com/dki-lab/few-shot-bioIE,-1
Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation,0.994078,"The performance of nighttime semantic segmentation is restricted by the poor
illumination and a lack of pixel-wise annotation, which severely limit its
application in autonomous driving. Existing works, e.g., using the twilight as
the intermediate target domain to perform the adaptation from daytime to
nighttime, may fail to cope with the inherent difference between datasets
caused by the camera equipment and the urban style. Faced with these two types
of domain shifts, i.e., the illumination and the inherent difference of the
datasets, we propose a novel domain adaptation framework via cross-domain
correlation distillation, called CCDistill. The invariance of illumination or
inherent difference between two images is fully explored so as to make up for
the lack of labels for nighttime images. Specifically, we extract the content
and style knowledge contained in features, calculate the degree of inherent or
illumination difference between two images. The domain adaptation is achieved
using the invariance of the same kind of difference. Extensive experiments on
Dark Zurich and ACDC demonstrate that CCDistill achieves the state-of-the-art
performance for nighttime semantic segmentation. Notably, our method is a
one-stage domain adaptation network which can avoid affecting the inference
time. Our implementation is available at https://github.com/ghuan99/CCDistill.",https://github.com/ghuan99/CCDistill,-1
RECALL: Rehearsal-free Continual Learning for Object Classification,0.0352349,"Convolutional neural networks show remarkable results in classification but
struggle with learning new things on the fly. We present a novel rehearsal-free
approach, where a deep neural network is continually learning new unseen object
categories without saving any data of prior sequences. Our approach is called
RECALL, as the network recalls categories by calculating logits for old
categories before training new ones. These are then used during training to
avoid changing the old categories. For each new sequence, a new head is added
to accommodate the new categories. To mitigate forgetting, we present a
regularization strategy where we replace the classification with a regression.
Moreover, for the known categories, we propose a Mahalanobis loss that includes
the variances to account for the changing densities between known and unknown
categories. Finally, we present a novel dataset for continual learning,
especially suited for object recognition on a mobile robot (HOWS-CL-25),
including 150,795 synthetic images of 25 household object categories. Our
approach RECALL outperforms the current state of the art on CORe50 and
iCIFAR-100 and reaches the best performance on HOWS-CL-25.",https://github.com/DLR-RM/RECALL,-1
Vulnerability Prioritization: An Offensive Security Approach,0.133851,"Organizations struggle to handle sheer number of vulnerabilities in their
cloud environments. The de facto methodology used for prioritizing
vulnerabilities is to use Common Vulnerability Scoring System (CVSS). However,
CVSS has inherent limitations that makes it not ideal for prioritization. In
this work, we propose a new way of prioritizing vulnerabilities. Our approach
is inspired by how offensive security practitioners perform penetration
testing. We evaluate our approach with a real world case study for a large
client, and the accuracy of machine learning to automate the process end to
end.",None,112
Cyberbullying detection across social media platforms via platform-aware adversarial encoding,0.808821,"Despite the increasing interest in cyberbullying detection, existing efforts
have largely been limited to experiments on a single platform and their
generalisability across different social media platforms have received less
attention. We propose XP-CB, a novel cross-platform framework based on
Transformers and adversarial learning. XP-CB can enhance a Transformer
leveraging unlabelled data from the source and target platforms to come up with
a common representation while preventing platform-specific training. To
validate our proposed framework, we experiment on cyberbullying datasets from
three different platforms through six cross-platform configurations, showing
its effectiveness with both BERT and RoBERTa as the underlying Transformer
models.",None,-1
"Statistical Dataset Evaluation: Reliability, Difficulty, and Validity",0.0361859,"Datasets serve as crucial training resources and model performance trackers.
However, existing datasets have exposed a plethora of problems, inducing biased
models and unreliable evaluation results. In this paper, we propose a
model-agnostic dataset evaluation framework for automatic dataset quality
evaluation. We seek the statistical properties of the datasets and address
three fundamental dimensions: reliability, difficulty, and validity, following
a classical testing theory. Taking the Named Entity Recognition (NER) datasets
as a case study, we introduce $9$ statistical metrics for a statistical dataset
evaluation framework. Experimental results and human evaluation validate that
our evaluation framework effectively assesses various aspects of the dataset
quality. Furthermore, we study how the dataset scores on our statistical
metrics affect the model performance, and appeal for dataset quality evaluation
or targeted dataset improvement before training or testing models.",https://github.com/dqxiu/DataEval,-1
Graph Augmentation Learning,0.365285,"Graph Augmentation Learning (GAL) provides outstanding solutions for graph
learning in handling incomplete data, noise data, etc. Numerous GAL methods
have been proposed for graph-based applications such as social network analysis
and traffic flow forecasting. However, the underlying reasons for the
effectiveness of these GAL methods are still unclear. As a consequence, how to
choose optimal graph augmentation strategy for a certain application scenario
is still in black box. There is a lack of systematic, comprehensive, and
experimentally validated guideline of GAL for scholars. Therefore, in this
survey, we in-depth review GAL techniques from macro (graph), meso (subgraph),
and micro (node/edge) levels. We further detailedly illustrate how GAL enhance
the data quality and the model performance. The aggregation mechanism of
augmentation strategies and graph learning models are also discussed by
different application scenarios, i.e., data-specific, model-specific, and
hybrid scenarios. To better show the outperformance of GAL, we experimentally
validate the effectiveness and adaptability of different GAL strategies in
different downstream tasks. Finally, we share our insights on several open
issues of GAL, including heterogeneity, spatio-temporal dynamics, scalability,
and generalization.",https://github.com/yushuowiki/awesome-GAL,-1
Biometric Signature Verification Using Recurrent Neural Networks,0.737732,"Architectures based on Recurrent Neural Networks (RNNs) have been
successfully applied to many different tasks such as speech or handwriting
recognition with state-of-the-art results. The main contribution of this work
is to analyse the feasibility of RNNs for on-line signature verification in
real practical scenarios. We have considered a system based on Long Short-Term
Memory (LSTM) with a Siamese architecture whose goal is to learn a similarity
metric from pairs of signatures. For the experimental work, the BiosecurID
database comprised of 400 users and 4 separated acquisition sessions are
considered. Our proposed LSTM RNN system has outperformed the results of recent
published works on the BiosecurID benchmark in figures ranging from 17.76% to
28.00% relative verification performance improvement for skilled forgeries.",None,-1
CNSNet: A Cleanness-Navigated-Shadow Network for Shadow Removal,0.520938,"The key to shadow removal is recovering the contents of the shadow regions
with the guidance of the non-shadow regions. Due to the inadequate long-range
modeling, the CNN-based approaches cannot thoroughly investigate the
information from the non-shadow regions. To solve this problem, we propose a
novel cleanness-navigated-shadow network (CNSNet), with a shadow-oriented
adaptive normalization (SOAN) module and a shadow-aware aggregation with
transformer (SAAT) module based on the shadow mask. Under the guidance of the
shadow mask, the SOAN module formulates the statistics from the non-shadow
region and adaptively applies them to the shadow region for region-wise
restoration. The SAAT module utilizes the shadow mask to precisely guide the
restoration of each shadowed pixel by considering the highly relevant pixels
from the shadow-free regions for global pixel-wise restoration. Extensive
experiments on three benchmark datasets (ISTD, ISTD+, and SRD) show that our
method achieves superior de-shadowing performance.",None,6653
Unsupervised Scene Sketch to Photo Synthesis,0.798104,"Sketches make an intuitive and powerful visual expression as they are fast
executed freehand drawings. We present a method for synthesizing realistic
photos from scene sketches. Without the need for sketch and photo pairs, our
framework directly learns from readily available large-scale photo datasets in
an unsupervised manner. To this end, we introduce a standardization module that
provides pseudo sketch-photo pairs during training by converting photos and
sketches to a standardized domain, i.e. the edge map. The reduced domain gap
between sketch and photo also allows us to disentangle them into two
components: holistic scene structures and low-level visual styles such as color
and texture. Taking this advantage, we synthesize a photo-realistic image by
combining the structure of a sketch and the visual style of a reference photo.
Extensive experimental results on perceptual similarity metrics and human
perceptual studies show the proposed method could generate realistic photos
with high fidelity from scene sketches and outperform state-of-the-art photo
synthesis baselines. We also demonstrate that our framework facilitates a
controllable manipulation of photo synthesis by editing strokes of
corresponding sketches, delivering more fine-grained details than previous
approaches that rely on region-level editing.",None,-1
Ask Me Anything: A simple strategy for prompting language models,0.798629,"Large language models (LLMs) transfer well to new tasks out-of-the-box simply
given a natural language prompt that demonstrates how to perform the task and
no additional training. Prompting is a brittle process wherein small
modifications to the prompt can cause large variations in the model
predictions, and therefore significant effort is dedicated towards designing a
painstakingly ""perfect prompt"" for a task. To mitigate the high degree of
effort involved in prompt-design, we instead ask whether producing multiple
effective, yet imperfect, prompts and aggregating them can lead to a high
quality prompting strategy. Our observations motivate our proposed prompting
method, ASK ME ANYTHING (AMA). We first develop an understanding of the
effective prompt formats, finding that question-answering (QA) prompts, which
encourage open-ended generation (""Who went to the park?"") tend to outperform
those that restrict the model outputs (""John went to the park. Output True or
False.""). Our approach recursively uses the LLM itself to transform task inputs
to the effective QA format. We apply the collected prompts to obtain several
noisy votes for the input's true label. We find that the prompts can have very
different accuracies and complex dependencies and thus propose to use weak
supervision, a procedure for combining the noisy predictions, to produce the
final predictions for the inputs. We evaluate AMA across open-source model
families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B
parameters), demonstrating an average performance lift of 10.2% over the
few-shot baseline. This simple strategy enables the open-source GPT-J-6B model
to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular
benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms
few-shot GPT3-175B. We release our code here:
https://github.com/HazyResearch/ama_prompting",None,-1
Route Planning for Last-Mile Deliveries Using Mobile Parcel Lockers: A Hybrid Q-Learning Network Approach,0.482134,"Mobile parcel lockers have been recently proposed by logistics operators as a
technology that could help reduce traffic congestion and operational costs in
urban freight distribution. Given their ability to relocate throughout their
area of deployment, they hold the potential to improve customer accessibility
and convenience. In this study, we formulate the Mobile Parcel Locker Problem
(MPLP) , a special case of the Location-Routing Problem (LRP) which determines
the optimal stopover location for MPLs throughout the day and plans
corresponding delivery routes. A Hybrid Q Learning Network based Method (HQM)
is developed to resolve the computational complexity of the resulting large
problem instances while escaping local optima. In addition, the HQM is
integrated with global and local search mechanisms to resolve the dilemma of
exploration and exploitation faced by classic reinforcement learning methods.
We examine the performance of HQM under different problem sizes (up to 200
nodes) and benchmarked it against the exact approach and Genetic Algorithm
(GA). Our results indicate that HQM achieves better optimisation performance
with shorter computation time than the exact approach solved by the Gurobi
solver in large problem instances. Additionally, the average reward obtained by
HQM is 1.96 times greater than GA, which demonstrates that HQM has a better
optimisation ability. Further, we identify critical factors that contribute to
fleet size requirements, travel distances, and service delays. Our findings
outline that the efficiency of MPLs is mainly contingent on the length of time
windows and the deployment of MPL stopovers. Finally, we highlight managerial
implications based on parametric analysis to provide guidance for logistics
operators in the context of efficient last-mile distribution operations.",None,-1
Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs,0.918276,"Most existing deep neural networks (DNNs) are easily disturbed by slight
noise. However, there are few researches on physical attacks by deploying
lighting equipment. The light-based physical attacks has excellent covertness,
which brings great security risks to many vision-based applications (such as
self-driving). Therefore, we propose a light-based physical attack, called
adversarial laser spot (AdvLS), which optimizes the physical parameters of
laser spots through genetic algorithm to perform physical attacks. It realizes
robust and covert physical attack by using low-cost laser equipment. As far as
we know, AdvLS is the first light-based physical attack that perform physical
attacks in the daytime. A large number of experiments in the digital and
physical environments show that AdvLS has excellent robustness and covertness.
In addition, through in-depth analysis of the experimental data, we find that
the adversarial perturbations generated by AdvLS have superior adversarial
attack migration. The experimental results show that AdvLS impose serious
interference to advanced DNNs, we call for the attention of the proposed AdvLS.
The code of AdvLS is available at: https://github.com/ChengYinHu/AdvLS",https://github.com/ChengYinHu/AdvLS,9817
FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales,0.578565,"Following how humans communicate, free-text rationales aim to use natural
language to explain neural language model (LM) behavior. However, free-text
rationales' unconstrained nature makes them prone to hallucination, so it is
important to have metrics for free-text rationale quality. Existing free-text
rationale metrics measure how consistent the rationale is with the LM's
predicted label, but there is no protocol for assessing such metrics'
reliability. Thus, we propose FRAME, a framework for evaluating rationale-label
consistency (RLC) metrics for free-text rationales. FRAME is based on three
axioms: (1) good metrics should yield highest scores for reference rationales,
which maximize RLC by construction; (2) good metrics should be appropriately
sensitive to semantic perturbation of rationales; and (3) good metrics should
be robust to variation in the LM's task performance. Across three text
classification datasets, we show that existing RLC metrics cannot satisfy all
three FRAME axioms, since they are implemented via model pretraining which
muddles the metric's signal. Then, we introduce a non-pretraining RLC metric
that greatly outperforms baselines on (1) and (3), while performing
competitively on (2). Finally, we discuss the limitations of using RLC to
evaluate free-text rationales.",None,-1
Domain Generalization Strategy to Train Classifiers Robust to Spatial-Temporal Shift,0.637755,"Deep learning-based weather prediction models have advanced significantly in
recent years. However, data-driven models based on deep learning are difficult
to apply to real-world applications because they are vulnerable to
spatial-temporal shifts. A weather prediction task is especially susceptible to
spatial-temporal shifts when the model is overfitted to locality and
seasonality. In this paper, we propose a training strategy to make the weather
prediction model robust to spatial-temporal shifts. We first analyze the effect
of hyperparameters and augmentations of the existing training strategy on the
spatial-temporal shift robustness of the model. Next, we propose an optimal
combination of hyperparameters and augmentation based on the analysis results
and a test-time augmentation. We performed all experiments on the W4C22
Transfer dataset and achieved the 1st performance.",https://github.com/seominseok0429/W4C22-Simple-Baseline-for-Weather-Forecasting-Using-Spatiotemporal-Context-Aggregation-Network,-1
Consistency Regularization for Domain Adaptation,0.0477416,"Collection of real world annotations for training semantic segmentation
models is an expensive process. Unsupervised domain adaptation (UDA) tries to
solve this problem by studying how more accessible data such as synthetic data
can be used to train and adapt models to real world images without requiring
their annotations. Recent UDA methods applies self-learning by training on
pixel-wise classification loss using a student and teacher network. In this
paper, we propose the addition of a consistency regularization term to
semi-supervised UDA by modelling the inter-pixel relationship between elements
in networks' output. We demonstrate the effectiveness of the proposed
consistency regularization term by applying it to the state-of-the-art DAFormer
framework and improving mIoU19 performance on the GTA5 to Cityscapes benchmark
by 0.8 and mIou16 performance on the SYNTHIA to Cityscapes benchmark by 1.2.",https://github.com/kw01sg/CRDA,-1
Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Conservative Natural Policy Gradient Primal-Dual Algorithm,0.521509,"We consider the problem of constrained Markov decision process (CMDP) in
continuous state-actions spaces where the goal is to maximize the expected
cumulative reward subject to some constraints. We propose a novel Conservative
Natural Policy Gradient Primal-Dual Algorithm (C-NPG-PD) to achieve zero
constraint violation while achieving state of the art convergence results for
the objective value function. For general policy parametrization, we prove
convergence of value function to global optimal upto an approximation error due
to restricted policy class. We even improve the sample complexity of existing
constrained NPG-PD algorithm \cite{Ding2020} from $\mathcal{O}(1/\epsilon^6)$
to $\mathcal{O}(1/\epsilon^4)$. To the best of our knowledge, this is the first
work to establish zero constraint violation with Natural policy gradient style
algorithms for infinite horizon discounted CMDPs. We demonstrate the merits of
proposed algorithm via experimental evaluations.",None,-1
TAMFormer: Multi-Modal Transformer with Learned Attention Mask for Early Intent Prediction,0.135673,"Human intention prediction is a growing area of research where an activity in
a video has to be anticipated by a vision-based system. To this end, the model
creates a representation of the past, and subsequently, it produces future
hypotheses about upcoming scenarios. In this work, we focus on pedestrians'
early intention prediction in which, from a current observation of an urban
scene, the model predicts the future activity of pedestrians that approach the
street. Our method is based on a multi-modal transformer that encodes past
observations and produces multiple predictions at different anticipation times.
Moreover, we propose to learn the attention masks of our transformer-based
model (Temporal Adaptive Mask Transformer) in order to weigh differently
present and past temporal dependencies. We investigate our method on several
public benchmarks for early intention prediction, improving the prediction
performances at different anticipation times compared to the previous works.",None,-1
Reward Learning with Trees: Methods and Evaluation,0.0557825,"Recent efforts to learn reward functions from human feedback have tended to
use deep neural networks, whose lack of transparency hampers our ability to
explain agent behaviour or verify alignment. We explore the merits of learning
intrinsically interpretable tree models instead. We develop a recently proposed
method for learning reward trees from preference labels, and show it to be
broadly competitive with neural networks on challenging high-dimensional tasks,
with good robustness to limited or corrupted data. Having found that reward
tree learning can be done effectively in complex settings, we then consider why
it should be used, demonstrating that the interpretable reward structure gives
significant scope for traceability, verification and explanation.",None,-1
Leveraging Trajectory Prediction for Pedestrian Video Anomaly Detection,0.222062,"Video anomaly detection is a core problem in vision. Correctly detecting and
identifying anomalous behaviors in pedestrians from video data will enable
safety-critical applications such as surveillance, activity monitoring, and
human-robot interaction. In this paper, we propose to leverage trajectory
localization and prediction for unsupervised pedestrian anomaly event
detection. Different than previous reconstruction-based approaches, our
proposed framework rely on the prediction errors of normal and abnormal
pedestrian trajectories to detect anomalies spatially and temporally. We
present experimental results on real-world benchmark datasets on varying
timescales and show that our proposed trajectory-predictor-based anomaly
detection pipeline is effective and efficient at identifying anomalous
activities of pedestrians in videos. Code will be made available at
https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection.",https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection,604
Type-aware Embeddings for Multi-Hop Reasoning over Knowledge Graphs,0.695082,"Multi-hop reasoning over real-life knowledge graphs (KGs) is a highly
challenging problem as traditional subgraph matching methods are not capable to
deal with noise and missing information. To address this problem, it has been
recently introduced a promising approach based on jointly embedding logical
queries and KGs into a low-dimensional space to identify answer entities.
However, existing proposals ignore critical semantic knowledge inherently
available in KGs, such as type information. To leverage type information, we
propose a novel TypE-aware Message Passing (TEMP) model, which enhances the
entity and relation representations in queries, and simultaneously improves
generalization, deductive and inductive reasoning. Remarkably, TEMP is a
plug-and-play model that can be easily incorporated into existing
embedding-based models to improve their performance. Extensive experiments on
three real-world datasets demonstrate TEMP's effectiveness.",https://github.com/zhiweihu1103/QE-TEMP,-1
FullSubNet+: Channel Attention FullSubNet with Complex Spectrograms for Speech Enhancement,0.992897,"Previously proposed FullSubNet has achieved outstanding performance in Deep
Noise Suppression (DNS) Challenge and attracted much attention. However, it
still encounters issues such as input-output mismatch and coarse processing for
frequency bands. In this paper, we propose an extended single-channel real-time
speech enhancement framework called FullSubNet+ with following significant
improvements. First, we design a lightweight multi-scale time sensitive channel
attention (MulCA) module which adopts multi-scale convolution and channel
attention mechanism to help the network focus on more discriminative frequency
bands for noise reduction. Then, to make full use of the phase information in
noisy speech, our model takes all the magnitude, real and imaginary
spectrograms as inputs. Moreover, by replacing the long short-term memory
(LSTM) layers in original full-band model with stacked temporal convolutional
network (TCN) blocks, we design a more efficient full-band module called
full-band extractor. The experimental results in DNS Challenge dataset show the
superior performance of our FullSubNet+, which reaches the state-of-the-art
(SOTA) performance and outperforms other existing speech enhancement
approaches.",https://github.com/thuhcsi/FullSubNet-plus,-1
ILSGAN: Independent Layer Synthesis for Unsupervised Foreground-Background Segmentation,0.319388,"Unsupervised foreground-background segmentation aims at extracting salient
objects from cluttered backgrounds, where Generative Adversarial Network (GAN)
approaches, especially layered GANs, show great promise. However, without human
annotations, they are typically prone to produce foreground and background
layers with non-negligible semantic and visual confusion, dubbed ""information
leakage"", resulting in notable degeneration of the generated segmentation mask.
To alleviate this issue, we propose a simple-yet-effective explicit layer
independence modeling approach, termed Independent Layer Synthesis GAN
(ILSGAN), pursuing independent foreground-background layer generation by
encouraging their discrepancy. Specifically, it targets minimizing the mutual
information between visible and invisible regions of the foreground and
background to spur interlayer independence. Through in-depth theoretical and
experimental analyses, we justify that explicit layer independence modeling is
critical to suppressing information leakage and contributes to impressive
segmentation performance gains. Also, our ILSGAN achieves strong
state-of-the-art generation quality and segmentation performance on complex
real-world data. Code is available at: https://github.com/qrzou/ILSGAN",https://github.com/qrzou/ILSGAN,6360
Long-Tail Prediction Uncertainty Aware Trajectory Planning for Self-driving Vehicles,0.666931,"A typical trajectory planner of autonomous driving commonly relies on
predicting the future behavior of surrounding obstacles. Recently, deep
learning technology has been widely adopted to design prediction models due to
their impressive performance. However, such models may fail in the ""long-tail""
driving cases where the training data is sparse or unavailable, leading to
planner failures. To this end, this work proposes a trajectory planner to
consider the prediction model uncertainty arising from insufficient data for
safer performance. Firstly, an ensemble network structure estimates the
prediction model's uncertainty due to insufficient training data. Then a
trajectory planner is designed to consider the worst-case arising from
prediction uncertainty. The results show that the proposed method can improve
the safety of trajectory planning under the prediction uncertainty caused by
insufficient data. At the same time, with sufficient data, the framework will
not lead to overly conservative results. This technology helps to improve the
safety and reliability of autonomous vehicles under the long-tail data
distribution of the real world.",None,-1
QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity,0.512302,"The mechanism of existing style transfer algorithms is by minimizing a hybrid
loss function to push the generated image toward high similarities in both
content and style. However, this type of approach cannot guarantee visual
fidelity, i.e., the generated artworks should be indistinguishable from real
ones. In this paper, we devise a new style transfer framework called QuantArt
for high visual-fidelity stylization. QuantArt pushes the latent representation
of the generated artwork toward the centroids of the real artwork distribution
with vector quantization. By fusing the quantized and continuous latent
representations, QuantArt allows flexible control over the generated artworks
in terms of content preservation, style similarity, and visual fidelity.
Experiments on various style transfer settings show that our QuantArt framework
achieves significantly higher visual fidelity compared with the existing style
transfer methods.",https://github.com/siyuhuang/QuantArt,-1
VPN: Verification of Poisoning in Neural Networks,0.0831377,"Neural networks are successfully used in a variety of applications, many of
them having safety and security concerns. As a result researchers have proposed
formal verification techniques for verifying neural network properties. While
previous efforts have mainly focused on checking local robustness in neural
networks, we instead study another neural network security issue, namely data
poisoning. In this case an attacker inserts a trigger into a subset of the
training data, in such a way that at test time, this trigger in an input causes
the trained model to misclassify to some target class. We show how to formulate
the check for data poisoning as a property that can be checked with
off-the-shelf verification tools, such as Marabou and nneum, where
counterexamples of failed checks constitute the triggers. We further show that
the discovered triggers are `transferable' from a small model to a larger,
better-trained model, allowing us to analyze state-of-the art performant models
trained for image classification tasks.",https://github.com/theyoucheng/vpn,-1
"RARR: Researching and Revising What Language Models Say, Using Language Models",0.986403,"Language models (LMs) now excel at many tasks such as few-shot learning,
question answering, reasoning, and dialog. However, they sometimes generate
unsupported or misleading content. A user cannot easily determine whether their
outputs are trustworthy or not, because most LMs do not have any built-in
mechanism for attribution to external evidence. To enable attribution while
still preserving all the powerful advantages of recent generation models, we
propose RARR (Retrofit Attribution using Research and Revision), a system that
1) automatically finds attribution for the output of any text generation model
and 2) post-edits the output to fix unsupported content while preserving the
original output as much as possible. When applied to the output of several
state-of-the-art LMs on a diverse set of generation tasks, we find that RARR
significantly improves attribution while otherwise preserving the original
input to a much greater degree than previously explored edit models.
Furthermore, the implementation of RARR requires only a handful of training
examples, a large language model, and standard web search.",https://github.com/anthonywchen/RARR,-1
Learning Audio-Visual embedding for Person Verification in the Wild,0.450167,"It has already been observed that audio-visual embedding is more robust than
uni-modality embedding for person verification. Here, we proposed a novel
audio-visual strategy that considers aggregators from a fusion perspective.
First, we introduced weight-enhanced attentive statistics pooling for the first
time in face verification. We find that a strong correlation exists between
modalities during pooling, so joint attentive pooling is proposed which
contains cycle consistency to learn the implicit inter-frame weight. Finally,
each modality is fused with a gated attention mechanism to gain robust
audio-visual embedding. All the proposed models are trained on the VoxCeleb2
dev dataset and the best system obtains 0.18%, 0.27%, and 0.49% EER on three
official trial lists of VoxCeleb1 respectively, which is to our knowledge the
best-published results for person verification.",None,-1
High-resolution Face Swapping via Latent Semantics Disentanglement,0.865615,"We present a novel high-resolution face swapping method using the inherent
prior knowledge of a pre-trained GAN model. Although previous research can
leverage generative priors to produce high-resolution results, their quality
can suffer from the entangled semantics of the latent space. We explicitly
disentangle the latent semantics by utilizing the progressive nature of the
generator, deriving structure attributes from the shallow layers and appearance
attributes from the deeper ones. Identity and pose information within the
structure attributes are further separated by introducing a landmark-driven
structure transfer latent direction. The disentangled latent code produces rich
generative features that incorporate feature blending to produce a plausible
swapping result. We further extend our method to video face swapping by
enforcing two spatio-temporal constraints on the latent space and the image
space. Extensive experiments demonstrate that the proposed method outperforms
state-of-the-art image/video face swapping methods in terms of hallucination
quality and consistency. Code can be found at:
https://github.com/cnnlstm/FSLSD_HiRes.",https://github.com/cnnlstm/FSLSD_HiRes,-1
A high-precision underwater object detection based on joint self-supervised deblurring and improved spatial transformer network,0.0375598,"Deep learning-based underwater object detection (UOD) remains a major
challenge due to the degraded visibility and difficulty to obtain sufficient
underwater object images captured from various perspectives for training. To
address these issues, this paper presents a high-precision UOD based on joint
self-supervised deblurring and improved spatial transformer network. A
self-supervised deblurring subnetwork is introduced into the designed
multi-task learning aided object detection architecture to force the shared
feature extraction module to output clean features for detection subnetwork.
Aiming at alleviating the limitation of insufficient photos from different
perspectives, an improved spatial transformer network is designed based on
perspective transformation, adaptively enriching image features within the
network. The experimental results show that the proposed UOD approach achieved
47.9 mAP in URPC2017 and 70.3 mAP in URPC2018, outperforming many
state-of-the-art UOD methods and indicating the designed method is more
suitable for UOD.",None,-1
Generative Biomedical Entity Linking via Knowledge Base-Guided Pre-training and Synonyms-Aware Fine-tuning,0.984574,"Entities lie in the heart of biomedical natural language understanding, and
the biomedical entity linking (EL) task remains challenging due to the
fine-grained and diversiform concept names. Generative methods achieve
remarkable performances in general domain EL with less memory usage while
requiring expensive pre-training. Previous biomedical EL methods leverage
synonyms from knowledge bases (KB) which is not trivial to inject into a
generative method. In this work, we use a generative approach to model
biomedical EL and propose to inject synonyms knowledge in it. We propose
KB-guided pre-training by constructing synthetic samples with synonyms and
definitions from KB and require the model to recover concept names. We also
propose synonyms-aware fine-tuning to select concept names for training, and
propose decoder prompt and multi-synonyms constrained prefix tree for
inference. Our method achieves state-of-the-art results on several biomedical
EL tasks without candidate selection which displays the effectiveness of
proposed pre-training and fine-tuning strategies.",https://github.com/Yuanhy1997/GenBioEL,-1
Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective,0.303055,"Unsupervised video domain adaptation is a practical yet challenging task. In
this work, for the first time, we tackle it from a disentanglement view. Our
key idea is to handle the spatial and temporal domain divergence separately
through disentanglement. Specifically, we consider the generation of
cross-domain videos from two sets of latent factors, one encoding the static
information and another encoding the dynamic information. A Transfer Sequential
VAE (TranSVAE) framework is then developed to model such generation. To better
serve for adaptation, we propose several objectives to constrain the latent
factors. With these constraints, the spatial divergence can be readily removed
by disentangling the static domain-specific information out, and the temporal
divergence is further reduced from both frame- and video-levels through
adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and
Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE
compared with several state-of-the-art approaches. Code is publicly available.",https://github.com/ldkong1205/TranSVAE,-1
TSM: Measuring the Enticement of Honeyfiles with Natural Language Processing,0.406512,"Honeyfile deployment is a useful breach detection method in cyber deception
that can also inform defenders about the intent and interests of intruders and
malicious insiders. A key property of a honeyfile, enticement, is the extent to
which the file can attract an intruder to interact with it. We introduce a
novel metric, Topic Semantic Matching (TSM), which uses topic modelling to
represent files in the repository and semantic matching in an embedding vector
space to compare honeyfile text and topic words robustly. We also present a
honeyfile corpus created with different Natural Language Processing (NLP)
methods. Experiments show that TSM is effective in inter-corpus comparisons and
is a promising tool to measure the enticement of honeyfiles. TSM is the first
measure to use NLP techniques to quantify the enticement of honeyfile content
that compares the essential topical content of local contexts to honeyfiles and
is robust to paraphrasing.",https://github.com/RoelTim/tsm-honeyfile-nlp-enticement,-1
Retrieval-based Disentangled Representation Learning with Natural Language Supervision,0.0695313,"Disentangled representation learning remains challenging as the underlying
factors of variation in the data do not naturally exist. The inherent
complexity of real-world data makes it unfeasible to exhaustively enumerate and
encapsulate all its variations within a finite set of factors. However, it is
worth noting that most real-world data have linguistic equivalents, typically
in the form of textual descriptions. These linguistic counterparts can
represent the data and effortlessly decomposed into distinct tokens. In light
of this, we present Vocabulary Disentangled Retrieval (VDR), a retrieval-based
framework that harnesses natural language as proxies of the underlying data
variation to drive disentangled representation learning. Our approach employ a
bi-encoder model to represent both data and natural language in a vocabulary
space, enabling the model to distinguish dimensions that capture intrinsic
characteristics within data through its natural language counterpart, thus
facilitating disentanglement. We extensively assess the performance of VDR
across 15 retrieval benchmark datasets, covering text-to-text and cross-modal
retrieval scenarios, as well as human evaluation. Our experimental results
compellingly demonstrate the superiority of VDR over previous bi-encoder
retrievers with comparable model size and training costs, achieving an
impressive 8.7% improvement in NDCG@10 on the BEIR benchmark, a 5.3% increase
on MS COCO, and a 6.0% increase on Flickr30k in terms of mean recall in the
zero-shot setting. Moreover, The results from human evaluation indicate that
interpretability of our method is on par with SOTA captioning models.",None,-1
Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts,0.455361,"Previous work has shown that there exists a scaling law between the size of
Language Models (LMs) and their zero-shot performance on different downstream
NLP tasks. In this work, we show that this phenomenon does not hold when
evaluating large LMs on tasks with negated prompts, but instead shows an
inverse scaling law. We evaluate 9 different tasks with negated prompts on (1)
pretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further
pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with
few-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all
LM types perform worse on negated prompts as they scale and show a huge
performance gap between the human performance when comparing the average score
on both original and negated prompts. By highlighting a critical limitation of
existing LMs and methods, we urge the community to develop new approaches of
developing LMs that actually follow the given instructions. We provide the code
and the datasets to explore negated prompts at
https://github.com/joeljang/negated-prompts-for-llms",None,6400
Label-dependent and event-guided interpretable disease risk prediction using EHRs,0.0840032,"Electronic health records (EHRs) contain patients' heterogeneous data that
are collected from medical providers involved in the patient's care, including
medical notes, clinical events, laboratory test results, symptoms, and
diagnoses. In the field of modern healthcare, predicting whether patients would
experience any risks based on their EHRs has emerged as a promising research
area, in which artificial intelligence (AI) plays a key role. To make AI models
practically applicable, it is required that the prediction results should be
both accurate and interpretable. To achieve this goal, this paper proposed a
label-dependent and event-guided risk prediction model (LERP) to predict the
presence of multiple disease risks by mainly extracting information from
unstructured medical notes. Our model is featured in the following aspects.
First, we adopt a label-dependent mechanism that gives greater attention to
words from medical notes that are semantically similar to the names of risk
labels. Secondly, as the clinical events (e.g., treatments and drugs) can also
indicate the health status of patients, our model utilizes the information from
events and uses them to generate an event-guided representation of medical
notes. Thirdly, both label-dependent and event-guided representations are
integrated to make a robust prediction, in which the interpretability is
enabled by the attention weights over words from medical notes. To demonstrate
the applicability of the proposed method, we apply it to the MIMIC-III dataset,
which contains real-world EHRs collected from hospitals. Our method is
evaluated in both quantitative and qualitative ways.",https://github.com/guoyinwang/LEAM,-1
Relational Attention: Generalizing Transformers for Graph-Structured Tasks,0.412379,"Transformers flexibly operate over sets of real-valued vectors representing
task-specific entities and their attributes, where each vector might encode one
word-piece token and its position in a sequence, or some piece of information
that carries no position at all. But as set processors, transformers are at a
disadvantage in reasoning over more general graph-structured data where nodes
represent entities and edges represent relations between entities. To address
this shortcoming, we generalize transformer attention to consider and update
edge vectors in each transformer layer. We evaluate this relational transformer
on a diverse array of graph-structured tasks, including the large and
challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically
outperforms state-of-the-art graph neural networks expressly designed to reason
over graph-structured data. Our analysis demonstrates that these gains are
attributable to relational attention's inherent ability to leverage the greater
expressivity of graphs over sets.",None,-1
Yet Another Format of Universal Dependencies for Korean,0.567008,"In this study, we propose a morpheme-based scheme for Korean dependency
parsing and adopt the proposed scheme to Universal Dependencies. We present the
linguistic rationale that illustrates the motivation and the necessity of
adopting the morpheme-based format, and develop scripts that convert between
the original format used by Universal Dependencies and the proposed
morpheme-based format automatically. The effectiveness of the proposed format
for Korean dependency parsing is then testified by both statistical and neural
models, including UDPipe and Stanza, with our carefully constructed
morpheme-based word embedding for Korean. morphUD outperforms parsing results
for all Korean UD treebanks, and we also present detailed error analyses.",https://github.com/jungyeul/morphUD-korean,-1
On the State of the Art in Authorship Attribution and Authorship Verification,0.834999,"Despite decades of research on authorship attribution (AA) and authorship
verification (AV), inconsistent dataset splits/filtering and mismatched
evaluation methods make it difficult to assess the state of the art. In this
paper, we present a survey of the fields, resolve points of confusion,
introduce Valla that standardizes and benchmarks AA/AV datasets and metrics,
provide a large-scale empirical evaluation, and provide apples-to-apples
comparisons between existing methods. We evaluate eight promising methods on
fifteen datasets (including distribution-shifted challenge sets) and introduce
a new large-scale dataset based on texts archived by Project Gutenberg.
Surprisingly, we find that a traditional Ngram-based model performs best on 5
(of 7) AA tasks, achieving an average macro-accuracy of $76.50\%$ (compared to
$66.71\%$ for a BERT-based model). However, on the two AA datasets with the
greatest number of words per author, as well as on the AV datasets, BERT-based
models perform best. While AV methods are easily applied to AA, they are seldom
included as baselines in AA papers. We show that through the application of
hard-negative mining, AV methods are competitive alternatives to AA methods.
Valla and all experiment code can be found here:
https://github.com/JacobTyo/Valla",https://github.com/JacobTyo/Valla,-1
MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations,0.865856,"Poor sample efficiency continues to be the primary challenge for deployment
of deep Reinforcement Learning (RL) algorithms for real-world applications, and
in particular for visuo-motor control. Model-based RL has the potential to be
highly sample efficient by concurrently learning a world model and using
synthetic rollouts for planning and policy improvement. However, in practice,
sample-efficient learning with model-based RL is bottlenecked by the
exploration challenge. In this work, we find that leveraging just a handful of
demonstrations can dramatically improve the sample-efficiency of model-based
RL. Simply appending demonstrations to the interaction dataset, however, does
not suffice. We identify key ingredients for leveraging demonstrations in model
learning -- policy pretraining, targeted exploration, and oversampling of
demonstration data -- which forms the three phases of our model-based RL
framework. We empirically study three complex visuo-motor control domains and
find that our method is 150%-250% more successful in completing sparse reward
tasks compared to prior approaches in the low data regime (100K interaction
steps, 5 demonstrations). Code and videos are available at:
https://nicklashansen.github.io/modemrl",https://github.com/facebookresearch/modem,-1
Automated Isovist Computation for Minecraft,0.811124,"Procedural content generation for games is a growing trend in both research
and industry, even though there is no consensus of how good content looks, nor
how to automatically evaluate it. A number of metrics have been developed in
the past, usually focused on the artifact as a whole, and mostly lacking
grounding in human experience. In this study we develop a new set of automated
metrics, motivated by ideas from architecture, namely isovists and space
syntax, which have a track record of capturing human experience of space. These
metrics can be computed for a specific game state, from the player's
perspective, and take into account their embodiment in the game world. We show
how to apply those metrics to the 3d blockworld of Minecraft. We use a dataset
of generated settlements from the GDMC Settlement Generation Challenge in
Minecraft and establish several rank-based correlations between the isovist
properties and the rating human judges gave those settelements. We also produce
a range of heat maps that demonstrate the location based applicability of the
approach, which allows for development of those metrics as measures for a game
experience at a specific time and space.",None,-1
Large Language Models are Zero-Shot Reasoners,1.0,"Pretrained large language models (LLMs) are widely used in many sub-fields of
natural language processing (NLP) and generally known as excellent few-shot
learners with task-specific exemplars. Notably, chain of thought (CoT)
prompting, a recent technique for eliciting complex multi-step reasoning
through step-by-step answer examples, achieved the state-of-the-art
performances in arithmetics and symbolic reasoning, difficult system-2 tasks
that do not follow the standard scaling laws for LLMs. While these successes
are often attributed to LLMs' ability for few-shot learning, we show that LLMs
are decent zero-shot reasoners by simply adding ""Let's think step by step""
before each answer. Experimental results demonstrate that our Zero-shot-CoT,
using the same single prompt template, significantly outperforms zero-shot LLM
performances on diverse benchmark reasoning tasks including arithmetics
(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin
Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled
Objects), without any hand-crafted few-shot examples, e.g. increasing the
accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with
large InstructGPT model (text-davinci-002), as well as similar magnitudes of
improvements with another off-the-shelf large model, 540B parameter PaLM. The
versatility of this single prompt across very diverse reasoning tasks hints at
untapped and understudied fundamental zero-shot capabilities of LLMs,
suggesting high-level, multi-task broad cognitive capabilities may be extracted
by simple prompting. We hope our work not only serves as the minimal strongest
zero-shot baseline for the challenging reasoning benchmarks, but also
highlights the importance of carefully exploring and analyzing the enormous
zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or
few-shot exemplars.",None,-1
DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection,0.814973,"Graph Anomaly Detection (GAD) has recently become a hot research spot due to
its practicability and theoretical value. Since GAD emphasizes the application
and the rarity of anomalous samples, enriching the varieties of its datasets is
fundamental work. Thus, this paper present DGraph, a real-world dynamic graph
in the finance domain. DGraph overcomes many limitations of current GAD
datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth
nodes. We provide a comprehensive observation of DGraph, revealing that
anomalous nodes and normal nodes generally have different structures, neighbor
distribution, and temporal dynamics. Moreover, it suggests that unlabeled nodes
are also essential for detecting fraudsters. Furthermore, we conduct extensive
experiments on DGraph. Observation and experiments demonstrate that DGraph is
propulsive to advance GAD research and enable in-depth exploration of anomalous
nodes.",https://github.com/hxttkl/DGraph_Experiments,-1
Exploiting Transformation Invariance and Equivariance for Self-supervised Sound Localisation,0.804774,"We present a simple yet effective self-supervised framework for audio-visual
representation learning, to localize the sound source in videos. To understand
what enables to learn useful representations, we systematically investigate the
effects of data augmentations, and reveal that (1) composition of data
augmentations plays a critical role, i.e. explicitly encouraging the
audio-visual representations to be invariant to various transformations~({\em
transformation invariance}); (2) enforcing geometric consistency substantially
improves the quality of learned representations, i.e. the detected sound source
should follow the same transformation applied on input video frames~({\em
transformation equivariance}). Extensive experiments demonstrate that our model
significantly outperforms previous methods on two sound localization
benchmarks, namely, Flickr-SoundNet and VGG-Sound. Additionally, we also
evaluate audio retrieval and cross-modal retrieval tasks. In both cases, our
self-supervised models demonstrate superior retrieval performances, even
competitive with the supervised approach in audio retrieval. This reveals the
proposed framework learns strong multi-modal representations that are
beneficial to sound localisation and generalization to further applications.
\textit{All codes will be available}.",https://jinxiang-liu.github.io/SSL-TIE/,-1
Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers,0.425663,"Demographic factors (e.g., gender or age) shape our language. Previous work
showed that incorporating demographic factors can consistently improve
performance for various NLP tasks with traditional NLP models. In this work, we
investigate whether these previous findings still hold with state-of-the-art
pretrained Transformer-based language models (PLMs). We use three common
specialization methods proven effective for incorporating external knowledge
into pretrained Transformers (e.g., domain-specific or geographic knowledge).
We adapt the language representations for the demographic dimensions of gender
and age, using continuous language modeling and dynamic multi-task learning for
adaptation, where we couple language modeling objectives with the prediction of
demographic classes. Our results, when employing a multilingual PLM, show
substantial gains in task performance across four languages (English, German,
French, and Danish), which is consistent with the results of previous work.
However, controlling for confounding factors - primarily domain and language
proficiency of Transformer-based PLMs - shows that downstream performance gains
from our demographic adaptation do not actually stem from demographic
knowledge. Our results indicate that demographic specialization of PLMs, while
holding promise for positive societal impact, still represents an unsolved
problem for (modern) NLP.",https://github.com/umanlp/SocioAdapt,-1
Distribution-based Emotion Recognition in Conversation,0.20811,"Automatic emotion recognition in conversation (ERC) is crucial for
emotion-aware conversational artificial intelligence. This paper proposes a
distribution-based framework that formulates ERC as a sequence-to-sequence
problem for emotion distribution estimation. The inherent ambiguity of emotions
and the subjectivity of human perception lead to disagreements in emotion
labels, which is handled naturally in our framework from the perspective of
uncertainty estimation in emotion distributions. A Bayesian training loss is
introduced to improve the uncertainty estimation by conditioning each emotional
state on an utterance-specific Dirichlet prior distribution. Experimental
results on the IEMOCAP dataset show that ERC outperformed the
single-utterance-based system, and the proposed distribution-based ERC methods
have not only better classification accuracy, but also show improved
uncertainty estimation.",None,3441
Neural Retriever and Go Beyond: A Thesis Proposal,0.0399132,"Information Retriever (IR) aims to find the relevant documents (e.g.
snippets, passages, and articles) to a given query at large scale. IR plays an
important role in many tasks such as open domain question answering and
dialogue systems, where external knowledge is needed. In the past, searching
algorithms based on term matching have been widely used. Recently, neural-based
algorithms (termed as neural retrievers) have gained more attention which can
mitigate the limitations of traditional methods. Regardless of the success
achieved by neural retrievers, they still face many challenges, e.g. suffering
from a small amount of training data and failing to answer simple
entity-centric questions. Furthermore, most of the existing neural retrievers
are developed for pure-text query. This prevents them from handling
multi-modality queries (i.e. the query is composed of textual description and
images). This proposal has two goals. First, we introduce methods to address
the abovementioned issues of neural retrievers from three angles, new model
architectures, IR-oriented pretraining tasks, and generating large scale
training data. Second, we identify the future research direction and propose
potential corresponding solution.",None,-1
Few-Max: Few-Shot Domain Adaptation for Unsupervised Contrastive Representation Learning,0.068973,"Contrastive self-supervised learning methods learn to map data points such as
images into non-parametric representation space without requiring labels. While
highly successful, current methods require a large amount of data in the
training phase. In situations where the target training set is limited in size,
generalization is known to be poor. Pretraining on a large source data set and
fine-tuning on the target samples is prone to overfitting in the few-shot
regime, where only a small number of target samples are available. Motivated by
this, we propose a domain adaption method for self-supervised contrastive
learning, termed Few-Max, to address the issue of adaptation to a target
distribution under few-shot learning. To quantify the representation quality,
we evaluate Few-Max on a range of source and target datasets, including
ImageNet, VisDA, and fastMRI, on which Few-Max consistently outperforms other
approaches.",https://github.com/utcsilab/fewmax,-1
Event Tables for Efficient Experience Replay,0.142922,"Experience replay (ER) is a crucial component of many deep reinforcement
learning (RL) systems. However, uniform sampling from an ER buffer can lead to
slow convergence and unstable asymptotic behaviors. This paper introduces
Stratified Sampling from Event Tables (SSET), which partitions an ER buffer
into Event Tables, each capturing important subsequences of optimal behavior.
We prove a theoretical advantage over the traditional monolithic buffer
approach and combine SSET with an existing prioritized sampling strategy to
further improve learning speed and stability. Empirical results in challenging
MiniGrid domains, benchmark RL environments, and a high-fidelity car racing
simulator demonstrate the advantages and versatility of SSET over existing ER
buffer sampling approaches.",None,-1
Learning Classifier Systems for Self-Explaining Socio-Technical-Systems,0.388717,"In socio-technical settings, operators are increasingly assisted by decision
support systems. By employing these, important properties of socio-technical
systems such as self-adaptation and self-optimization are expected to improve
further. To be accepted by and engage efficiently with operators, decision
support systems need to be able to provide explanations regarding the reasoning
behind specific decisions. In this paper, we propose the usage of Learning
Classifier Systems, a family of rule-based machine learning methods, to
facilitate transparent decision making and highlight some techniques to improve
that. We then present a template of seven questions to assess
application-specific explainability needs and demonstrate their usage in an
interview-based case study for a manufacturing scenario. We find that the
answers received did yield useful insights for a well-designed LCS model and
requirements to have stakeholders actively engage with an intelligent agent.",None,-1
Finding Counterfactual Explanations through Constraint Relaxations,0.139441,"Interactive constraint systems often suffer from infeasibility (no solution)
due to conflicting user constraints. A common approach to recover infeasibility
is to eliminate the constraints that cause the conflicts in the system. This
approach allows the system to provide an explanation as: ""if the user is
willing to drop out some of their constraints, there exists a solution"".
However, one can criticise this form of explanation as not being very
informative. A counterfactual explanation is a type of explanation that can
provide a basis for the user to recover feasibility by helping them understand
which changes can be applied to their existing constraints rather than removing
them. This approach has been extensively studied in the machine learning field,
but requires a more thorough investigation in the context of constraint
satisfaction. We propose an iterative method based on conflict detection and
maximal relaxations in over-constrained constraint satisfaction problems to
help compute a counterfactual explanation.",None,-1
Cross-Domain Aspect Extraction using Transformers Augmented with Knowledge Graphs,0.443671,"The extraction of aspect terms is a critical step in fine-grained sentiment
analysis of text. Existing approaches for this task have yielded impressive
results when the training and testing data are from the same domain. However,
these methods show a drastic decrease in performance when applied to
cross-domain settings where the domain of the testing data differs from that of
the training data. To address this lack of extensibility and robustness, we
propose a novel approach for automatically constructing domain-specific
knowledge graphs that contain information relevant to the identification of
aspect terms. We introduce a methodology for injecting information from these
knowledge graphs into Transformer models, including two alternative mechanisms
for knowledge insertion: via query enrichment and via manipulation of attention
patterns. We demonstrate state-of-the-art performance on benchmark datasets for
cross-domain aspect term extraction using our approach and investigate how the
amount of external knowledge available to the Transformer impacts model
performance.",None,-1
"ESGBERT: Language Model to Help with Classification Tasks Related to Companies Environmental, Social, and Governance Practices",0.334218,"Environmental, Social, and Governance (ESG) are non-financial factors that
are garnering attention from investors as they increasingly look to apply these
as part of their analysis to identify material risks and growth opportunities.
Some of this attention is also driven by clients who, now more aware than ever,
are demanding for their money to be managed and invested responsibly. As the
interest in ESG grows, so does the need for investors to have access to
consumable ESG information. Since most of it is in text form in reports,
disclosures, press releases, and 10-Q filings, we see a need for sophisticated
NLP techniques for classification tasks for ESG text. We hypothesize that an
ESG domain-specific pre-trained model will help with such and study building of
the same in this paper. We explored doing this by fine-tuning BERTs pre-trained
weights using ESG specific text and then further fine-tuning the model for a
classification task. We were able to achieve accuracy better than the original
BERT and baseline models in environment-specific classification tasks.",None,-1
GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost,0.56207,"Large pre-trained models have revolutionized natural language processing
(NLP) research and applications, but high training costs and limited data
resources have prevented their benefits from being shared equally amongst
speakers of all the world's languages. To address issues of cross-linguistic
access to such models and reduce energy consumption for sustainability during
large-scale model training, this study proposes an effective and
energy-efficient framework called GreenPLM that uses bilingual lexicons to
directly ""translate"" pre-trained language models of one language into another
at almost no additional cost. We validate this approach in 18 languages' BERT
models and show that this framework is comparable to, if not better than, other
heuristics with high training costs. In addition, given lightweight continued
pre-training on limited data where available, this framework outperforms the
original monolingual language models in six out of seven tested languages with
up to 200x less pre-training efforts. Aiming at the Leave No One Behind
Principle (LNOB), our approach manages to reduce inequalities between languages
and energy consumption greatly. We make our codes and models publicly available
here: \url{https://github.com/qcznlp/GreenPLMs}",https://github.com/qcznlp/GreenPLMs,-1
PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition,0.717369,"3D Point cloud is becoming a critical data representation in many real-world
applications like autonomous driving, robotics, and medical imaging. Although
the success of deep learning further accelerates the adoption of 3D point
clouds in the physical world, deep learning is notorious for its vulnerability
to adversarial attacks. In this work, we first identify that the
state-of-the-art empirical defense, adversarial training, has a major
limitation in applying to 3D point cloud models due to gradient obfuscation. We
further propose PointDP, a purification strategy that leverages diffusion
models to defend against 3D adversarial attacks. We extensively evaluate
PointDP on six representative 3D point cloud architectures, and leverage 10+
strong and adaptive attacks to demonstrate its lower-bound robustness. Our
evaluation shows that PointDP achieves significantly better robustness than
state-of-the-art purification methods under strong attacks. Results of
certified defenses on randomized smoothing combined with PointDP will be
included in the near future.",None,-1
Positive Pair Distillation Considered Harmful: Continual Meta Metric Learning for Lifelong Object Re-Identification,0.175464,"Lifelong object re-identification incrementally learns from a stream of
re-identification tasks. The objective is to learn a representation that can be
applied to all tasks and that generalizes to previously unseen
re-identification tasks. The main challenge is that at inference time the
representation must generalize to previously unseen identities. To address this
problem, we apply continual meta metric learning to lifelong object
re-identification. To prevent forgetting of previous tasks, we use knowledge
distillation and explore the roles of positive and negative pairs. Based on our
observation that the distillation and metric losses are antagonistic, we
propose to remove positive pairs from distillation to robustify model updates.
Our method, called Distillation without Positive Pairs (DwoPP), is evaluated on
extensive intra-domain experiments on person and vehicle re-identification
datasets, as well as inter-domain experiments on the LReID benchmark. Our
experiments demonstrate that DwoPP significantly outperforms the
state-of-the-art. The code is here: https://github.com/wangkai930418/DwoPP_code",None,-1
Improving End-to-End Contextual Speech Recognition with Fine-Grained Contextual Knowledge Selection,0.815495,"Nowadays, most methods in end-to-end contextual speech recognition bias the
recognition process towards contextual knowledge. Since all-neural contextual
biasing methods rely on phrase-level contextual modeling and attention-based
relevance modeling, they may encounter confusion between similar
context-specific phrases, which hurts predictions at the token level. In this
work, we focus on mitigating confusion problems with fine-grained contextual
knowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge
to reduce the uncertainty of token predictions. Specifically, we first apply
phrase selection to narrow the range of phrase candidates, and then conduct
token attention on the tokens in the selected phrase candidates. Moreover, we
re-normalize the attention weights of most relevant phrases in inference to
obtain more focused phrase-level contextual representations, and inject
position information to better discriminate phrases or tokens. On LibriSpeech
and an in-house 160,000-hour dataset, we explore the proposed methods based on
a controllable all-neural biasing method, collaborative decoding (ColDec). The
proposed methods provide at most 6.1% relative word error rate reduction on
LibriSpeech and 16.4% relative character error rate reduction on the in-house
dataset over ColDec.",https://github.com/MingLunHan/CIF-ColDec,-1
Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles,0.916779,"Video Anomaly Detection (VAD) is an important topic in computer vision.
Motivated by the recent advances in self-supervised learning, this paper
addresses VAD by solving an intuitive yet challenging pretext task, i.e.,
spatio-temporal jigsaw puzzles, which is cast as a multi-label fine-grained
classification problem. Our method exhibits several advantages over existing
works: 1) the spatio-temporal jigsaw puzzles are decoupled in terms of spatial
and temporal dimensions, responsible for capturing highly discriminative
appearance and motion features, respectively; 2) full permutations are used to
provide abundant jigsaw puzzles covering various difficulty levels, allowing
the network to distinguish subtle spatio-temporal differences between normal
and abnormal events; and 3) the pretext task is tackled in an end-to-end manner
without relying on any pre-trained models. Our method outperforms
state-of-the-art counterparts on three public benchmarks. Especially on
ShanghaiTech Campus, the result is superior to reconstruction and
prediction-based methods by a large margin.",https://github.com/wizyoung/YOLOv3,29064
Compositional Semantic Parsing with Large Language Models,0.999867,"Humans can reason compositionally when presented with new tasks. Previous
research shows that appropriate prompting techniques enable large language
models (LLMs) to solve artificial compositional generalization tasks such as
SCAN. In this work, we identify additional challenges in more realistic
semantic parsing tasks with larger vocabulary and refine these prompting
techniques to address them. Our best method is based on least-to-most
prompting: it decomposes the problem using prompting-based syntactic parsing,
then uses this decomposition to select appropriate exemplars and to
sequentially generate the semantic parse. This method allows us to set a new
state of the art for CFQ while requiring only 1% of the training data used by
traditional approaches. Due to the general nature of our approach, we expect
similar efforts will lead to new results in other tasks and domains, especially
for knowledge-intensive applications.",None,-1
A Two-Stage Efficient 3-D CNN Framework for EEG Based Emotion Recognition,0.345428,"This paper proposes a novel two-stage framework for emotion recognition using
EEG data that outperforms state-of-the-art models while keeping the model size
small and computationally efficient. The framework consists of two stages; the
first stage involves constructing efficient models named EEGNet, which is
inspired by the state-of-the-art efficient architecture and employs
inverted-residual blocks that contain depthwise separable convolutional layers.
The EEGNet models on both valence and arousal labels achieve the average
classification accuracy of 90%, 96.6%, and 99.5% with only 6.4k, 14k, and 25k
parameters, respectively. In terms of accuracy and storage cost, these models
outperform the previous state-of-the-art result by up to 9%. In the second
stage, we binarize these models to further compress them and deploy them easily
on edge devices. Binary Neural Networks (BNNs) typically degrade model
accuracy. We improve the EEGNet binarized models in this paper by introducing
three novel methods and achieving a 20\% improvement over the baseline binary
models. The proposed binarized EEGNet models achieve accuracies of 81%, 95%,
and 99% with storage costs of 0.11Mbits, 0.28Mbits, and 0.46Mbits,
respectively. Those models help deploy a precise human emotion recognition
system on the edge environment.",None,-1
Towards Enabling Dynamic Convolution Neural Network Inference for Edge Intelligence,0.234058,"Deep learning applications have achieved great success in numerous real-world
applications. Deep learning models, especially Convolution Neural Networks
(CNN) are often prototyped using FPGA because it offers high power efficiency
and reconfigurability. The deployment of CNNs on FPGAs follows a design cycle
that requires saving of model parameters in the on-chip memory during
High-level synthesis (HLS). Recent advances in edge intelligence require CNN
inference on edge network to increase throughput and reduce latency. To provide
flexibility, dynamic parameter allocation to different mobile devices is
required to implement either a predefined or defined on-the-fly CNN
architecture. In this study, we present novel methodologies for dynamically
streaming the model parameters at run-time to implement a traditional CNN
architecture. We further propose a library-based approach to design scalable
and dynamic distributed CNN inference on the fly leveraging
partial-reconfiguration techniques, which is particularly suitable for
resource-constrained edge devices. The proposed techniques are implemented on
the Xilinx PYNQ-Z2 board to prove the concept by utilizing the LeNet-5 CNN
model. The results show that the proposed methodologies are effective, with
classification accuracy rates of 92%, 86%, and 94% respectively",None,-1
Provable Defense against Backdoor Policies in Reinforcement Learning,0.820057,"We propose a provable defense mechanism against backdoor policies in
reinforcement learning under subspace trigger assumption. A backdoor policy is
a security threat where an adversary publishes a seemingly well-behaved policy
which in fact allows hidden triggers. During deployment, the adversary can
modify observed states in a particular way to trigger unexpected actions and
harm the agent. We assume the agent does not have the resources to re-train a
good policy. Instead, our defense mechanism sanitizes the backdoor policy by
projecting observed states to a 'safe subspace', estimated from a small number
of interactions with a clean (non-triggered) environment. Our sanitized policy
achieves $\epsilon$ approximate optimality in the presence of triggers,
provided the number of clean interactions is $O\left(\frac{D}{(1-\gamma)^4
\epsilon^2}\right)$ where $\gamma$ is the discounting factor and $D$ is the
dimension of state space. Empirically, we show that our sanitization defense
performs well on two Atari game environments.",https://github.com/skbharti/Provable-Defense-in-RL,-1
Semi-Supervised Single-View 3D Reconstruction via Prototype Shape Priors,0.722299,"The performance of existing single-view 3D reconstruction methods heavily
relies on large-scale 3D annotations. However, such annotations are tedious and
expensive to collect. Semi-supervised learning serves as an alternative way to
mitigate the need for manual labels, but remains unexplored in 3D
reconstruction. Inspired by the recent success of semi-supervised image
classification tasks, we propose SSP3D, a semi-supervised framework for 3D
reconstruction. In particular, we introduce an attention-guided prototype shape
prior module for guiding realistic object reconstruction. We further introduce
a discriminator-guided module to incentivize better shape generation, as well
as a regularizer to tolerate noisy training samples. On the ShapeNet benchmark,
the proposed approach outperforms previous supervised methods by clear margins
under various labeling ratios, (i.e., 1%, 5% , 10% and 20%). Moreover, our
approach also performs well when transferring to real-world Pix3D datasets
under labeling ratios of 10%. We also demonstrate our method could transfer to
novel categories with few novel supervised data. Experiments on the popular
ShapeNet dataset show that our method outperforms the zero-shot baseline by
over 12% and we also perform rigorous ablations and analysis to validate our
approach.",https://github.com/ChenHsing/SSP3D,-1
UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue,0.809392,"The goal-oriented document-grounded dialogue aims at responding to the user
query based on the dialogue context and supporting document. Existing studies
tackle this problem by decomposing it into two sub-tasks: knowledge
identification and response generation. However, such pipeline methods would
unavoidably suffer from the error propagation issue. This paper proposes to
unify these two sub-tasks via sequentially generating the grounding knowledge
and the response. We further develop a prompt-connected multi-task learning
strategy to model the characteristics and connections of different tasks and
introduce linear temperature scheduling to reduce the negative effect of
irrelevant document information. Experimental results demonstrate the
effectiveness of our framework.",https://github.com/doc2dial/sharedtask-dialdoc2021,-1
RWT-SLAM: Robust Visual SLAM for Highly Weak-textured Environments,0.390983,"As a fundamental task for intelligent robots, visual SLAM has made great
progress over the past decades. However, robust SLAM under highly weak-textured
environments still remains very challenging. In this paper, we propose a novel
visual SLAM system named RWT-SLAM to tackle this problem. We modify LoFTR
network which is able to produce dense point matching under low-textured scenes
to generate feature descriptors. To integrate the new features into the popular
ORB-SLAM framework, we develop feature masks to filter out the unreliable
features and employ KNN strategy to strengthen the matching robustness. We also
retrained visual vocabulary upon new descriptors for efficient loop closing.
The resulting RWT-SLAM is tested in various public datasets such as TUM and
OpenLORIS, as well as our own data. The results shows very promising
performance under highly weak-textured environments.",None,1155
Streaming Adaptive Submodular Maximization,0.767535,"Many sequential decision making problems can be formulated as an adaptive
submodular maximization problem. However, most of existing studies in this
field focus on pool-based setting, where one can pick items in any order, and
there have been few studies for the stream-based setting where items arrive in
an arbitrary order and one must immediately decide whether to select an item or
not upon its arrival. In this paper, we introduce a new class of utility
functions, semi-policywise submodular functions. We develop a series of
effective algorithms to maximize a semi-policywise submodular function under
the stream-based setting.",None,-1
Computational linguistics and Natural Language Processing,0.0619643,"This chapter provides an introduction to computational linguistics methods,
with focus on their applications to the practice and study of translation. It
covers computational models, methods and tools for collection, storage,
indexing and analysis of linguistic data in the context of translation, and
discusses the main methodological issues and challenges in this field. While an
exhaustive review of existing computational linguistics methods and tools is
beyond the scope of this chapter, we describe the most representative
approaches, and illustrate them with descriptions of typical applications.",None,-1
Smart Speech Segmentation using Acousto-Linguistic Features with look-ahead,0.519278,"Segmentation for continuous Automatic Speech Recognition (ASR) has
traditionally used silence timeouts or voice activity detectors (VADs), which
are both limited to acoustic features. This segmentation is often overly
aggressive, given that people naturally pause to think as they speak.
Consequently, segmentation happens mid-sentence, hindering both punctuation and
downstream tasks like machine translation for which high-quality segmentation
is critical. Model-based segmentation methods that leverage acoustic features
are powerful, but without an understanding of the language itself, these
approaches are limited. We present a hybrid approach that leverages both
acoustic and language information to improve segmentation. Furthermore, we show
that including one word as a look-ahead boosts segmentation quality. On
average, our models improve segmentation-F0.5 score by 9.8% over baseline. We
show that this approach works for multiple languages. For the downstream task
of machine translation, it improves the translation BLEU score by an average of
1.05 points.",None,753
Fine-Grained Object Classification via Self-Supervised Pose Alignment,0.823612,"Semantic patterns of fine-grained objects are determined by subtle appearance
difference of local parts, which thus inspires a number of part-based methods.
However, due to uncontrollable object poses in images, distinctive details
carried by local regions can be spatially distributed or even self-occluded,
leading to a large variation on object representation. For discounting pose
variations, this paper proposes to learn a novel graph based object
representation to reveal a global configuration of local parts for
self-supervised pose alignment across classes, which is employed as an
auxiliary feature regularization on a deep representation learning
network.Moreover, a coarse-to-fine supervision together with the proposed
pose-insensitive constraint on shallow-to-deep sub-networks encourages
discriminative features in a curriculum learning manner. We evaluate our method
on three popular fine-grained object classification benchmarks, consistently
achieving the state-of-the-art performance. Source codes are available at
https://github.com/yangxh11/P2P-Net.",https://github.com/yangxh11/P2P-Net,15319
Improving Few-Shot Part Segmentation using Coarse Supervision,0.335946,"A significant bottleneck in training deep networks for part segmentation is
the cost of obtaining detailed annotations. We propose a framework to exploit
coarse labels such as figure-ground masks and keypoint locations that are
readily available for some categories to improve part segmentation models. A
key challenge is that these annotations were collected for different tasks and
with different labeling styles and cannot be readily mapped to the part labels.
To this end, we propose to jointly learn the dependencies between labeling
styles and the part segmentation model, allowing us to utilize supervision from
diverse labels. To evaluate our approach we develop a benchmark on the
Caltech-UCSD birds and OID Aircraft dataset. Our approach outperforms baselines
based on multi-task learning, semi-supervised learning, and competitive methods
relying on loss functions manually designed to exploit sparse-supervision.",None,-1
FinBERT-MRC: financial named entity recognition using BERT under the machine reading comprehension paradigm,0.872557,"Financial named entity recognition (FinNER) from literature is a challenging
task in the field of financial text information extraction, which aims to
extract a large amount of financial knowledge from unstructured texts. It is
widely accepted to use sequence tagging frameworks to implement FinNER tasks.
However, such sequence tagging models cannot fully take advantage of the
semantic information in the texts. Instead, we formulate the FinNER task as a
machine reading comprehension (MRC) problem and propose a new model termed
FinBERT-MRC. This formulation introduces significant prior information by
utilizing well-designed queries, and extracts start index and end index of
target entities without decoding modules such as conditional random fields
(CRF). We conduct experiments on a publicly available Chinese financial dataset
ChFinAnn and a real-word bussiness dataset AdminPunish. FinBERT-MRC model
achieves average F1 scores of 92.78% and 96.80% on the two datasets,
respectively, with average F1 gains +3.94% and +0.89% over some sequence
tagging models including BiLSTM-CRF, BERT-Tagger, and BERT-CRF. The source code
is available at https://github.com/zyz0000/FinBERT-MRC.",https://github.com/zyz0000/FinBERT-MRC,-1
Do Large Language Models know what humans know?,0.271297,"Humans can attribute beliefs to others. However, it is unknown to what extent
this ability results from an innate biological endowment or from experience
accrued through child development, particularly exposure to language describing
others' mental states. We test the viability of the language exposure
hypothesis by assessing whether models exposed to large quantities of human
language display sensitivity to the implied knowledge states of characters in
written passages. In pre-registered analyses, we present a linguistic version
of the False Belief Task to both human participants and a Large Language Model,
GPT-3. Both are sensitive to others' beliefs, but while the language model
significantly exceeds chance behavior, it does not perform as well as the
humans, nor does it explain the full extent of their behavior -- despite being
exposed to more language than a human would in a lifetime. This suggests that
while statistical learning from language exposure may in part explain how
humans develop the ability to reason about the mental states of others, other
mechanisms are also responsible.",None,556
iSimLoc: Visual Global Localization for Previously Unseen Environments with Simulated Images,0.219413,"The visual camera is an attractive device in beyond visual line of sight
(B-VLOS) drone operation, since they are low in size, weight, power, and cost,
and can provide redundant modality to GPS failures. However, state-of-the-art
visual localization algorithms are unable to match visual data that have a
significantly different appearance due to illuminations or viewpoints. This
paper presents iSimLoc, a condition/viewpoint consistent hierarchical global
re-localization approach. The place features of iSimLoc can be utilized to
search target images under changing appearances and viewpoints. Additionally,
our hierarchical global re-localization module refines in a coarse-to-fine
manner, allowing iSimLoc to perform a fast and accurate estimation. We evaluate
our method on one dataset with appearance variations and one dataset that
focuses on demonstrating large-scale matching over a long flight in complicated
environments. On our two datasets, iSimLoc achieves 88.7\% and 83.8\%
successful retrieval rates with 1.5s inferencing time, compared to 45.8% and
39.7% using the next best method. These results demonstrate robust localization
in a range of environments.",None,-1
Uncertainty-aware deep learning methods for robust diabetic retinopathy classification,0.766631,"Automatic classification of diabetic retinopathy from retinal images has been
widely studied using deep neural networks with impressive results. However,
there is a clinical need for estimation of the uncertainty in the
classifications, a shortcoming of modern neural networks. Recently, approximate
Bayesian deep learning methods have been proposed for the task but the studies
have only considered the binary referable/non-referable diabetic retinopathy
classification applied to benchmark datasets. We present novel results by
systematically investigating a clinical dataset and a clinically relevant
5-class classification scheme, in addition to benchmark datasets and the binary
classification scheme. Moreover, we derive a connection between uncertainty
measures and classifier risk, from which we develop a new uncertainty measure.
We observe that the previously proposed entropy-based uncertainty measure
generalizes to the clinical dataset on the binary classification scheme but not
on the 5-class scheme, whereas our new uncertainty measure generalizes to the
latter case.",None,-1
Multi-Spectral Image Classification with Ultra-Lean Complex-Valued Models,0.147378,"Multi-spectral imagery is invaluable for remote sensing due to different
spectral signatures exhibited by materials that often appear identical in
greyscale and RGB imagery. Paired with modern deep learning methods, this
modality has great potential utility in a variety of remote sensing
applications, such as humanitarian assistance and disaster recovery efforts.
State-of-the-art deep learning methods have greatly benefited from large-scale
annotations like in ImageNet, but existing MSI image datasets lack annotations
at a similar scale. As an alternative to transfer learning on such data with
few annotations, we apply complex-valued co-domain symmetric models to classify
real-valued MSI images. Our experiments on 8-band xView data show that our
ultra-lean model trained on xView from scratch without data augmentations can
outperform ResNet with data augmentation and modified transfer learning on
xView. Our work is the first to demonstrate the value of complex-valued deep
learning on real-valued MSI data.",None,12734
Near-Optimal Multi-Agent Learning for Safe Coverage Control,0.836703,"In multi-agent coverage control problems, agents navigate their environment
to reach locations that maximize the coverage of some density. In practice, the
density is rarely known $\textit{a priori}$, further complicating the original
NP-hard problem. Moreover, in many applications, agents cannot visit arbitrary
locations due to $\textit{a priori}$ unknown safety constraints. In this paper,
we aim to efficiently learn the density to approximately solve the coverage
problem while preserving the agents' safety. We first propose a conditionally
linear submodular coverage function that facilitates theoretical analysis.
Utilizing this structure, we develop MacOpt, a novel algorithm that efficiently
trades off the exploration-exploitation dilemma due to partial observability,
and show that it achieves sublinear regret. Next, we extend results on
single-agent safe exploration to our multi-agent setting and propose SafeMac
for safe coverage and exploration. We analyze SafeMac and give first of its
kind results: near optimal coverage in finite time while provably guaranteeing
safety. We extensively evaluate our algorithms on synthetic and real problems,
including a bio-diversity monitoring task under safety constraints, where
SafeMac outperforms competing methods.",https://github.com/manish-pra/SafeMaC,-1
Sustainable AI Processing at the Edge,0.187169,"Edge computing is a popular target for accelerating machine learning
algorithms supporting mobile devices without requiring the communication
latencies to handle them in the cloud. Edge deployments of machine learning
primarily consider traditional concerns such as SWaP constraints (Size, Weight,
and Power) for their installations. However, such metrics are not entirely
sufficient to consider environmental impacts from computing given the
significant contributions from embodied energy and carbon. In this paper we
explore the tradeoffs of convolutional neural network acceleration engines for
both inference and on-line training. In particular, we explore the use of
processing-in-memory (PIM) approaches, mobile GPU accelerators, and recently
released FPGAs, and compare them with novel Racetrack memory PIM. Replacing
PIM-enabled DDR3 with Racetrack memory PIM can recover its embodied energy as
quickly as 1 year. For high activity ratios, mobile GPUs can be more
sustainable but have higher embodied energy to overcome compared to PIM-enabled
Racetrack memory.",None,-1
Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models,0.425338,"In the era of digital healthcare, the huge volumes of textual information
generated every day in hospitals constitute an essential but underused asset
that could be exploited with task-specific, fine-tuned biomedical language
representation models, improving patient care and management. For such
specialized domains, previous research has shown that fine-tuning models
stemming from broad-coverage checkpoints can largely benefit additional
training rounds over large-scale in-domain resources. However, these resources
are often unreachable for less-resourced languages like Italian, preventing
local medical institutions to employ in-domain adaptation. In order to reduce
this gap, our work investigates two accessible approaches to derive biomedical
language models in languages other than English, taking Italian as a concrete
use-case: one based on neural machine translation of English resources,
favoring quantity over quality; the other based on a high-grade, narrow-scoped
corpus natively written in Italian, thus preferring quality over quantity. Our
study shows that data quantity is a harder constraint than data quality for
biomedical adaptation, but the concatenation of high-quality data can improve
model performance even when dealing with relatively size-limited corpora. The
models published from our investigations have the potential to unlock important
research opportunities for Italian hospitals and academia. Finally, the set of
lessons learned from the study constitutes valuable insights towards a solution
to build biomedical language models that are generalizable to other
less-resourced languages and different domain settings.",https://github.com/IVN-RIN/bio-med-BIT,-1
Measuring Geographic Performance Disparities of Offensive Language Classifiers,0.516392,"Text classifiers are applied at scale in the form of one-size-fits-all
solutions. Nevertheless, many studies show that classifiers are biased
regarding different languages and dialects. When measuring and discovering
these biases, some gaps present themselves and should be addressed. First,
``Does language, dialect, and topical content vary across geographical
regions?'' and secondly ``If there are differences across the regions, do they
impact model performance?''. We introduce a novel dataset called GeoOLID with
more than 14 thousand examples across 15 geographically and demographically
diverse cities to address these questions. We perform a comprehensive analysis
of geographical-related content and their impact on performance disparities of
offensive language detection models. Overall, we find that current models do
not generalize across locations. Likewise, we show that while offensive
language models produce false positives on African American English, model
performance is not correlated with each city's minority population proportions.
Warning: This paper contains offensive language.",None,-1
On the Effectiveness of Parameter-Efficient Fine-Tuning,0.898387,"Fine-tuning pre-trained models has been ubiquitously proven to be effective
in a wide range of NLP tasks. However, fine-tuning the whole model is parameter
inefficient as it always yields an entirely new model for each task. Currently,
many research works propose to only fine-tune a small portion of the parameters
while keeping most of the parameters shared across different tasks. These
methods achieve surprisingly good performance and are shown to be more stable
than their corresponding fully fine-tuned counterparts. However, such kind of
methods is still not well understood. Some natural questions arise: How does
the parameter sparsity lead to promising performance? Why is the model more
stable than the fully fine-tuned models? How to choose the tunable parameters?
In this paper, we first categorize the existing methods into random approaches,
rule-based approaches, and projection-based approaches based on how they choose
which parameters to tune. Then, we show that all of the methods are actually
sparse fine-tuned models and conduct a novel theoretical analysis of them. We
indicate that the sparsity is actually imposing a regularization on the
original model by controlling the upper bound of the stability. Such stability
leads to better generalization capability which has been empirically observed
in a lot of recent research works. Despite the effectiveness of sparsity
grounded by our theory, it still remains an open problem of how to choose the
tunable parameters. To better choose the tunable parameters, we propose a novel
Second-order Approximation Method (SAM) which approximates the original problem
with an analytically solvable optimization function. The tunable parameters are
determined by directly optimizing the approximation function. The experimental
results show that our proposed SAM model outperforms many strong baseline
models and it also verifies our theoretical analysis.",https://github.com/fuzihaofzh/AnalyzeParameterEfficientFinetune,-1
Better plain ViT baselines for ImageNet-1k,0.668979,"It is commonly accepted that the Vision Transformer model requires
sophisticated regularization techniques to excel at ImageNet-1k scale data.
Surprisingly, we find this is not the case and standard data augmentation is
sufficient. This note presents a few minor modifications to the original Vision
Transformer (ViT) vanilla training setting that dramatically improve the
performance of plain ViT models. Notably, 90 epochs of training surpass 76%
top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic
ResNet50 baseline, and 300 epochs of training reach 80% in less than one day.",https://github.com/google-research/big_vision,-1
Estimating and Explaining Model Performance When Both Covariates and Labels Shift,0.579429,"Deployed machine learning (ML) models often encounter new user data that
differs from their training data. Therefore, estimating how well a given model
might perform on the new data is an important step toward reliable ML
applications. This is very challenging, however, as the data distribution can
change in flexible ways, and we may not have any labels on the new data, which
is often the case in monitoring settings. In this paper, we propose a new
distribution shift model, Sparse Joint Shift (SJS), which considers the joint
shift of both labels and a few features. This unifies and generalizes several
existing shift models including label shift and sparse covariate shift, where
only marginal feature or label distribution shifts are considered. We describe
mathematical conditions under which SJS is identifiable. We further propose
SEES, an algorithmic framework to characterize the distribution shift under SJS
and to estimate a model's performance on new data without any labels. We
conduct extensive experiments on several real-world datasets with various ML
models. Across different datasets and distribution shifts, SEES achieves
significant (up to an order of magnitude) shift estimation error improvements
over existing approaches.",None,-1
A Classical-Quantum Convolutional Neural Network for Detecting Pneumonia from Chest Radiographs,0.221785,"While many quantum computing techniques for machine learning have been
proposed, their performance on real-world datasets remains to be studied. In
this paper, we explore how a variational quantum circuit could be integrated
into a classical neural network for the problem of detecting pneumonia from
chest radiographs. We substitute one layer of a classical convolutional neural
network with a variational quantum circuit to create a hybrid neural network.
We train both networks on an image dataset containing chest radiographs and
benchmark their performance. To mitigate the influence of different sources of
randomness in network training, we sample the results over multiple rounds. We
show that the hybrid network outperforms the classical network on different
performance measures, and that these improvements are statistically
significant. Our work serves as an experimental demonstration of the potential
of quantum computing to significantly improve neural network performance for
real-world, non-trivial problems relevant to society and industry.",None,-1
Fast Event-based Optical Flow Estimation by Triplet Matching,0.887678,"Event cameras are novel bio-inspired sensors that offer advantages over
traditional cameras (low latency, high dynamic range, low power, etc.). Optical
flow estimation methods that work on packets of events trade off speed for
accuracy, while event-by-event (incremental) methods have strong assumptions
and have not been tested on common benchmarks that quantify progress in the
field. Towards applications on resource-constrained devices, it is important to
develop optical flow algorithms that are fast, light-weight and accurate. This
work leverages insights from neuroscience, and proposes a novel optical flow
estimation scheme based on triplet matching. The experiments on publicly
available benchmarks demonstrate its capability to handle complex scenes with
comparable results as prior packet-based algorithms. In addition, the proposed
method achieves the fastest execution time (> 10 kHz) on standard CPUs as it
requires only three events in estimation. We hope that our research opens the
door to real-time, incremental motion estimation methods and applications in
real-world scenarios.",None,-1
HyperShot: Few-Shot Learning by Kernel HyperNetworks,0.563876,"Few-shot models aim at making predictions using a minimal number of labeled
examples from a given task. The main challenge in this area is the one-shot
setting where only one element represents each class. We propose HyperShot -
the fusion of kernels and hypernetwork paradigm. Compared to reference
approaches that apply a gradient-based adjustment of the parameters, our model
aims to switch the classification module parameters depending on the task's
embedding. In practice, we utilize a hypernetwork, which takes the aggregated
information from support data and returns the classifier's parameters
handcrafted for the considered problem. Moreover, we introduce the kernel-based
representation of the support examples delivered to hypernetwork to create the
parameters of the classification module. Consequently, we rely on relations
between embeddings of the support examples instead of direct feature values
provided by the backbone models. Thanks to this approach, our model can adapt
to highly different tasks.",None,-1
GaitMM: Multi-Granularity Motion Sequence Learning for Gait Recognition,0.0370478,"Gait recognition aims to identify individual-specific walking patterns by
observing the different periodic movements of each body part. However, most
existing methods treat each part equally and fail to account for the data
redundancy caused by the different step frequencies and sampling rates of gait
sequences. In this study, we propose a multi-granularity motion representation
network (GaitMM) for gait sequence learning. In GaitMM, we design a combined
full-body and fine-grained sequence learning module (FFSL) to explore
part-independent spatio-temporal representations. Moreover, we utilize a
frame-wise compression strategy, referred to as multi-scale motion aggregation
(MSMA), to capture discriminative information in the gait sequence. Experiments
on two public datasets, CASIA-B and OUMVLP, show that our approach reaches
state-of-the-art performances.",None,-1
Heatmap Distribution Matching for Human Pose Estimation,0.448634,"For tackling the task of 2D human pose estimation, the great majority of the
recent methods regard this task as a heatmap estimation problem, and optimize
the heatmap prediction using the Gaussian-smoothed heatmap as the optimization
objective and using the pixel-wise loss (e.g. MSE) as the loss function. In
this paper, we show that optimizing the heatmap prediction in such a way, the
model performance of body joint localization, which is the intrinsic objective
of this task, may not be consistently improved during the optimization process
of the heatmap prediction. To address this problem, from a novel perspective,
we propose to formulate the optimization of the heatmap prediction as a
distribution matching problem between the predicted heatmap and the dot
annotation of the body joint directly. By doing so, our proposed method does
not need to construct the Gaussian-smoothed heatmap and can achieve a more
consistent model performance improvement during the optimization of the heatmap
prediction. We show the effectiveness of our proposed method through extensive
experiments on the COCO dataset and the MPII dataset.",None,-1
Event Collapse in Contrast Maximization Frameworks,0.560296,"Contrast maximization (CMax) is a framework that provides state-of-the-art
results on several event-based computer vision tasks, such as ego-motion or
optical flow estimation. However, it may suffer from a problem called event
collapse, which is an undesired solution where events are warped into too few
pixels. As prior works have largely ignored the issue or proposed workarounds,
it is imperative to analyze this phenomenon in detail. Our work demonstrates
event collapse in its simplest form and proposes collapse metrics by using
first principles of space-time deformation based on differential geometry and
physics. We experimentally show on publicly available datasets that the
proposed metrics mitigate event collapse and do not harm well-posed warps. To
the best of our knowledge, regularizers based on the proposed metrics are the
only effective solution against event collapse in the experimental settings
considered, compared with other methods. We hope that this work inspires
further research to tackle more complex warp models.",None,7090
WPPNets and WPPFlows: The Power of Wasserstein Patch Priors for Superresolution,0.55241,"Exploiting image patches instead of whole images have proved to be a powerful
approach to tackle various problems in image processing. Recently, Wasserstein
patch priors (WPP), which are based on the comparison of the patch
distributions of the unknown image and a reference image, were successfully
used as data-driven regularizers in the variational formulation of
superresolution. However, for each input image, this approach requires the
solution of a non-convex minimization problem which is computationally costly.
In this paper, we propose to learn two kind of neural networks in an
unsupervised way based on WPP loss functions. First, we show how convolutional
neural networks (CNNs) can be incorporated. Once the network, called WPPNet, is
learned, it can be very efficiently applied to any input image. Second, we
incorporate conditional normalizing flows to provide a tool for uncertainty
quantification. Numerical examples demonstrate the very good performance of
WPPNets for superresolution in various image classes even if the forward
operator is known only approximately.",https://github.com/FabianAltekrueger/WPPNets,-1
Keys to Better Image Inpainting: Structure and Texture Go Hand in Hand,0.709273,"Deep image inpainting has made impressive progress with recent advances in
image generation and processing algorithms. We claim that the performance of
inpainting algorithms can be better judged by the generated structures and
textures. Structures refer to the generated object boundary or novel geometric
structures within the hole, while texture refers to high-frequency details,
especially man-made repeating patterns filled inside the structural regions. We
believe that better structures are usually obtained from a coarse-to-fine
GAN-based generator network while repeating patterns nowadays can be better
modeled using state-of-the-art high-frequency fast fourier convolutional
layers. In this paper, we propose a novel inpainting network combining the
advantages of the two designs. Therefore, our model achieves a remarkable
visual quality to match state-of-the-art performance in both structure
generation and repeating texture synthesis using a single network. Extensive
experiments demonstrate the effectiveness of the method, and our conclusions
further highlight the two critical factors of image inpainting quality,
structures, and textures, as the future design directions of inpainting
networks.",https://github.com/SHI-Labs/FcF-Inpainting/,-1
First Competitive Ant Colony Scheme for the CARP,0.458344,"This paper addresses the Capacitated Arc Routing Problem (CARP) using an Ant
Colony Optimization scheme. Ant Colony schemes can compute solutions for medium
scale instances of VRP. The proposed Ant Colony is dedicated to large-scale
instances of CARP with more than 140 nodes and 190 arcs to service. The Ant
Colony scheme is coupled with a local search procedure and provides high
quality solutions. The benchmarks we carried out prove possible to obtain
solutions as profitable as CARPET ones can be obtained using such scheme when a
sufficient number of iterations is devoted to the ants. It competes with the
Genetic Algorithm of Lacomme et al. regarding solution quality but it is more
time consuming on large scale instances. The method has been intensively
benchmarked on the well-known instances of Eglese, DeArmon and the last ones of
Belenguer and Benavent. This research report is a step forward CARP resolution
by Ant Colony proving ant schemes can compete with Taboo search methods and
Genetic Algorithms",None,-1
A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference,0.489696,"We study the problem of combining neural networks with symbolic reasoning.
Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL),
such as DeepProbLog, perform exponential-time exact inference, limiting the
scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference
(A-NeSI): a new framework for PNL that uses neural networks for scalable
approximate inference. A-NeSI 1) performs approximate inference in polynomial
time without changing the semantics of probabilistic logics; 2) is trained
using data generated by the background knowledge; 3) can generate symbolic
explanations of predictions; and 4) can guarantee the satisfaction of logical
constraints at test time, which is vital in safety-critical applications. Our
experiments show that A-NeSI is the first end-to-end method to solve three
neurosymbolic tasks with exponential combinatorial scaling. Finally, our
experiments show that A-NeSI achieves explainability and safety without a
penalty in performance.",https://github.com/HEmile/a-nesi,40025
Entity Linking in Tabular Data Needs the Right Attention,0.151271,"Understanding the semantic meaning of tabular data requires Entity Linking
(EL), in order to associate each cell value to a real-world entity in a
Knowledge Base (KB). In this work, we focus on end-to-end solutions for EL on
tabular data that do not rely on fact lookup in the target KB. Tabular data
contains heterogeneous and sparse context, including column headers, cell
values and table captions. We experiment with various models to generate a
vector representation for each cell value to be linked. Our results show that
it is critical to apply an attention mechanism as well as an attention mask, so
that the model can only attend to the most relevant context and avoid
information dilution. The most relevant context includes: same-row cells,
same-column cells, headers and caption. Computational complexity, however,
grows quadratically with the size of tabular data for such a complex model. We
achieve constant memory usage by introducing a Tabular Entity Linking Lite
model (TELL ) that generates vector representation for a cell based only on its
value, the table headers and the table caption. TELL achieves 80.8% accuracy on
Wikipedia tables, which is only 0.1% lower than the state-of-the-art model with
quadratic memory usage.",https://github.com/jcklie/wikimapper,-1
Extension: Adaptive Sampling with Implicit Radiance Field,0.142682,"This manuscript discusses the extension of adaptive light field sampling with
implicit radiance fields.",None,-1
Learning Physical Dynamics with Subequivariant Graph Neural Networks,0.575253,"Graph Neural Networks (GNNs) have become a prevailing tool for learning
physical dynamics. However, they still encounter several challenges: 1)
Physical laws abide by symmetry, which is a vital inductive bias accounting for
model generalization and should be incorporated into the model design. Existing
simulators either consider insufficient symmetry, or enforce excessive
equivariance in practice when symmetry is partially broken by gravity. 2)
Objects in the physical world possess diverse shapes, sizes, and properties,
which should be appropriately processed by the model. To tackle these
difficulties, we propose a novel backbone, Subequivariant Graph Neural Network,
which 1) relaxes equivariance to subequivariance by considering external fields
like gravity, where the universal approximation ability holds theoretically; 2)
introduces a new subequivariant object-aware message passing for learning
physical interactions between multiple objects of various shapes in the
particle-based representation; 3) operates in a hierarchical fashion, allowing
for modeling long-range and complex interactions. Our model achieves on average
over 3% enhancement in contact prediction accuracy across 8 scenarios on
Physion and 2X lower rollout MSE on RigidFall compared with state-of-the-art
GNN simulators, while exhibiting strong generalization and data efficiency.",https://hanjq17.github.io/SGNN/,-1
"A Machine with Short-Term, Episodic, and Semantic Memory Systems",0.131704,"Inspired by the cognitive science theory of the explicit human memory
systems, we have modeled an agent with short-term, episodic, and semantic
memory systems, each of which is modeled with a knowledge graph. To evaluate
this system and analyze the behavior of this agent, we designed and released
our own reinforcement learning agent environment, ""the Room"", where an agent
has to learn how to encode, store, and retrieve memories to maximize its return
by answering questions. We show that our deep Q-learning based agent
successfully learns whether a short-term memory should be forgotten, or rather
be stored in the episodic or semantic memory systems. Our experiments indicate
that an agent with human-like memory systems can outperform an agent without
this memory structure in the environment.",https://github.com/tae898/room-env,-1
LISA: Learning Implicit Shape and Appearance of Hands,0.727271,"This paper proposes a do-it-all neural model of human hands, named LISA. The
model can capture accurate hand shape and appearance, generalize to arbitrary
hand subjects, provide dense surface correspondences, be reconstructed from
images in the wild and easily animated. We train LISA by minimizing the shape
and appearance losses on a large set of multi-view RGB image sequences
annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand
local coordinate, our model predicts the color and the signed distance with
respect to each hand bone independently, and then combines the per-bone
predictions using predicted skinning weights. The shape, color and pose
representations are disentangled by design, allowing to estimate or animate
only selected parameters. We experimentally demonstrate that LISA can
accurately reconstruct a dynamic hand from monocular or multi-view sequences,
achieving a noticeably higher quality of reconstructed hand shapes compared to
baseline approaches. Project page:
https://www.iri.upc.edu/people/ecorona/lisa/.",https://www.iri.upc.edu/people/ecorona/lisa/,-1
Multielement polynomial chaos Kriging-based metamodelling for Bayesian inference of non-smooth systems,0.695532,"This paper presents a surrogate modelling technique based on domain
partitioning for Bayesian parameter inference of highly nonlinear engineering
models. In order to alleviate the computational burden typically involved in
Bayesian inference applications, a multielement Polynomial Chaos Expansion
based Kriging metamodel is proposed. The developed surrogate model combines in
a piecewise function an array of local Polynomial Chaos based Kriging
metamodels constructed on a finite set of non-overlapping subdomains of the
stochastic input space. Therewith, the presence of non-smoothness in the
response of the forward model (e.g.~ nonlinearities and sparseness) can be
reproduced by the proposed metamodel with minimum computational costs owing to
its local adaptation capabilities. The model parameter inference is conducted
through a Markov chain Monte Carlo approach comprising adaptive exploration and
delayed rejection. The efficiency and accuracy of the proposed approach are
validated through two case studies, including an analytical benchmark and a
numerical case study. The latter relates the partial differential equation
governing the hydrogen diffusion phenomenon of metallic materials in Thermal
Desorption Spectroscopy tests.",None,-1
MDFEND: Multi-domain Fake News Detection,0.998453,"Fake news spread widely on social media in various domains, which lead to
real-world threats in many aspects like politics, disasters, and finance. Most
existing approaches focus on single-domain fake news detection (SFND), which
leads to unsatisfying performance when these methods are applied to
multi-domain fake news detection. As an emerging field, multi-domain fake news
detection (MFND) is increasingly attracting attention. However, data
distributions, such as word frequency and propagation patterns, vary from
domain to domain, namely domain shift. Facing the challenge of serious domain
shift, existing fake news detection techniques perform poorly for multi-domain
scenarios. Therefore, it is demanding to design a specialized model for MFND.
In this paper, we first design a benchmark of fake news dataset for MFND with
domain label annotated, namely Weibo21, which consists of 4,488 fake news and
4,640 real news from 9 different domains. We further propose an effective
Multi-domain Fake News Detection Model (MDFEND) by utilizing a domain gate to
aggregate multiple representations extracted by a mixture of experts. The
experiments show that MDFEND can significantly improve the performance of
multi-domain fake news detection. Our dataset and code are available at
https://github.com/kennqiang/MDFEND-Weibo21.",None,-1
Learning First-Order Symbolic Planning Representations That Are Grounded,0.456069,"Two main approaches have been developed for learning first-order planning
(action) models from unstructured data: combinatorial approaches that yield
crisp action schemas from the structure of the state space, and deep learning
approaches that produce action schemas from states represented by images. A
benefit of the former approach is that the learned action schemas are similar
to those that can be written by hand; a benefit of the latter is that the
learned representations (predicates) are grounded on the images, and as a
result, new instances can be given in terms of images. In this work, we develop
a new formulation for learning crisp first-order planning models that are
grounded on parsed images, a step to combine the benefits of the two
approaches. Parsed images are assumed to be given in a simple O2D language
(objects in 2D) that involves a small number of unary and binary predicates
like ""left"", ""above"", ""shape"", etc. After learning, new planning instances can
be given in terms of pairs of parsed images, one for the initial situation and
the other for the goal. Learning and planning experiments are reported for
several domains including Blocks, Sokoban, IPC Grid, and Hanoi.",None,12924
Optimizing Partial Area Under the Top-k Curve: Theory and Practice,0.356966,"Top-k error has become a popular metric for large-scale classification
benchmarks due to the inevitable semantic ambiguity among classes. Existing
literature on top-k optimization generally focuses on the optimization method
of the top-k objective, while ignoring the limitations of the metric itself. In
this paper, we point out that the top-k objective lacks enough discrimination
such that the induced predictions may give a totally irrelevant label a top
rank. To fix this issue, we develop a novel metric named partial Area Under the
top-k Curve (AUTKC). Theoretical analysis shows that AUTKC has a better
discrimination ability, and its Bayes optimal score function could give a
correct top-K ranking with respect to the conditional probability. This shows
that AUTKC does not allow irrelevant labels to appear in the top list.
Furthermore, we present an empirical surrogate risk minimization framework to
optimize the proposed metric. Theoretically, we present (1) a sufficient
condition for Fisher consistency of the Bayes optimal score function; (2) a
generalization upper bound which is insensitive to the number of classes under
a simple hyperparameter setting. Finally, the experimental results on four
benchmark datasets validate the effectiveness of our proposed framework.",https://github.com/CSAILVision/places365,-1
Information Extraction from Scanned Invoice Images using Text Analysis and Layout Features,0.414656,"While storing invoice content as metadata to avoid paper document processing
may be the future trend, almost all of daily issued invoices are still printed
on paper or generated in digital formats such as PDFs. In this paper, we
introduce the OCRMiner system for information extraction from scanned document
images which is based on text analysis techniques in combination with layout
features to extract indexing metadata of (semi-)structured documents. The
system is designed to process the document in a similar way a human reader
uses, i.e. to employ different layout and text attributes in a coordinated
decision. The system consists of a set of interconnected modules that start
with (possibly erroneous) character-based output from a standard OCR system and
allow to apply different techniques and to expand the extracted knowledge at
each step. Using an open source OCR, the system is able to recover the invoice
data in 90% for English and in 88% for the Czech set.",https://github.com/naiveHobo/InvoiceNet,-1
ASOCEM: Automatic Segmentation Of Contaminations in cryo-EM,0.131905,"Particle picking is currently a critical step in the cryo-electron microscopy
single particle reconstruction pipeline. Contaminations in the acquired
micrographs severely degrade the performance of particle pickers, resulting is
many ``non-particles'' in the collected stack of particles. In this paper, we
present ASOCEM (Automatic Segmentation Of Contaminations in cryo-EM), an
automatic method to detect and segment contaminations, which requires as an
input only the approximated particle size. In particular, it does not require
any parameter tuning nor manual intervention. Our method is based on the
observation that the statistical distribution of contaminated regions is
different from that of the rest of the micrograph. This nonrestrictive
assumption allows to automatically detect various types of contaminations, from
the carbon edges of the supporting grid to high contrast blobs of different
sizes. We demonstrate the efficiency of our algorithm using various
experimental data sets containing various types of contaminations. ASOCEM is
integrated as part of the KLT picker \cite{ELDAR2020107473} and is available at
\url{https://github.com/ShkolniskyLab/kltpicker2}.",https://github.com/ShkolniskyLab/kltpicker2,-1
OpenScene: 3D Scene Understanding with Open Vocabularies,0.999086,"Traditional 3D scene understanding approaches rely on labeled 3D datasets to
train a model for a single task with supervision. We propose OpenScene, an
alternative approach where a model predicts dense features for 3D scene points
that are co-embedded with text and image pixels in CLIP feature space. This
zero-shot approach enables task-agnostic training and open-vocabulary queries.
For example, to perform SOTA zero-shot 3D semantic segmentation it first infers
CLIP features for every 3D point and later classifies them based on
similarities to embeddings of arbitrary class labels. More interestingly, it
enables a suite of open-vocabulary scene understanding applications that have
never been done before. For example, it allows a user to enter an arbitrary
text query and then see a heat map indicating which parts of a scene match. Our
approach is effective at identifying objects, materials, affordances,
activities, and room types in complex 3D scenes, all using a single model
trained without any labeled 3D data.",https://github.com/mseg-dataset/mseg-semantic,-1
Deep Learning-based Facial Appearance Simulation Driven by Surgically Planned Craniomaxillofacial Bony Movement,0.308497,"Simulating facial appearance change following bony movement is a critical
step in orthognathic surgical planning for patients with jaw deformities.
Conventional biomechanics-based methods such as the finite-element method (FEM)
are labor intensive and computationally inefficient. Deep learning-based
approaches can be promising alternatives due to their high computational
efficiency and strong modeling capability. However, the existing deep
learning-based method ignores the physical correspondence between facial soft
tissue and bony segments and thus is significantly less accurate compared to
FEM. In this work, we propose an Attentive Correspondence assisted Movement
Transformation network (ACMT-Net) to estimate the facial appearance by
transforming the bony movement to facial soft tissue through a point-to-point
attentive correspondence matrix. Experimental results on patients with jaw
deformity show that our proposed method can achieve comparable facial change
prediction accuracy compared with the state-of-the-art FEM-based approach with
significantly improved computational efficiency.",None,-1
Decoupling Makes Weakly Supervised Local Feature Better,0.90894,"Weakly supervised learning can help local feature methods to overcome the
obstacle of acquiring a large-scale dataset with densely labeled
correspondences. However, since weak supervision cannot distinguish the losses
caused by the detection and description steps, directly conducting weakly
supervised learning within a joint describe-then-detect pipeline suffers
limited performance. In this paper, we propose a decoupled describe-then-detect
pipeline tailored for weakly supervised local feature learning. Within our
pipeline, the detection step is decoupled from the description step and
postponed until discriminative and robust descriptors are learned. In addition,
we introduce a line-to-window search strategy to explicitly use the camera pose
information for better descriptor learning. Extensive experiments show that our
method, namely PoSFeat (Camera Pose Supervised Feature), outperforms previous
fully and weakly supervised methods and achieves state-of-the-art performance
on a wide range of downstream tasks.",https://github.com/The-Learning-And-Vision-Atelier-LAVA/PoSFeat,-1
CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,0.444112,"Knowledge-intensive language tasks (KILT) usually require a large body of
information to provide correct answers. A popular paradigm to solve this
problem is to combine a search system with a machine reader, where the former
retrieves supporting evidences and the latter examines them to produce answers.
Recently, the reader component has witnessed significant advances with the help
of large-scale pre-trained generative models. Meanwhile most existing solutions
in the search component rely on the traditional ``index-retrieve-then-rank''
pipeline, which suffers from large memory footprint and difficulty in
end-to-end optimization. Inspired by recent efforts in constructing model-based
IR models, we propose to replace the traditional multi-step search pipeline
with a novel single-step generative model, which can dramatically simplify the
search process and be optimized in an end-to-end manner. We show that a strong
generative retrieval model can be learned with a set of adequately designed
pre-training tasks, and be adopted to improve a variety of downstream KILT
tasks with further fine-tuning. We name the pre-trained generative retrieval
model as CorpusBrain as all information about the corpus is encoded in its
parameters without the need of constructing additional index. Empirical results
show that CorpusBrain can significantly outperform strong baselines for the
retrieval task on the KILT benchmark and establish new state-of-the-art
downstream performances. We also show that CorpusBrain works well under zero-
and low-resource settings.",https://github.com/ict-bigdatalab/CorpusBrain,-1
Evaluating Machine Common Sense via Cloze Testing,0.11547,"Language models (LMs) show state of the art performance for common sense (CS)
question answering, but whether this ability implies a human-level mastery of
CS remains an open question. Understanding the limitations and strengths of LMs
can help researchers improve these models, potentially by developing novel ways
of integrating external CS knowledge. We devise a series of tests and
measurements to systematically quantify their performance on different aspects
of CS. We propose the use of cloze testing combined with word embeddings to
measure the LM's robustness and confidence. Our results show than although
language models tend to achieve human-like accuracy, their confidence is
subpar. Future work can leverage this information to build more complex
systems, such as an ensemble of symbolic and distributed knowledge.",None,-1
"Automated Clinical Coding: What, Why, and Where We Are?",0.95329,"Clinical coding is the task of transforming medical information in a
patient's health records into structured codes so that they can be used for
statistical analysis. This is a cognitive and time-consuming task that follows
a standard process in order to achieve a high level of consistency. Clinical
coding could potentially be supported by an automated system to improve the
efficiency and accuracy of the process. We introduce the idea of automated
clinical coding and summarise its challenges from the perspective of Artificial
Intelligence (AI) and Natural Language Processing (NLP), based on the
literature, our project experience over the past two and half years (late 2019
- early 2022), and discussions with clinical coding experts in Scotland and the
UK. Our research reveals the gaps between the current deep learning-based
approach applied to clinical coding and the need for explainability and
consistency in real-world practice. Knowledge-based methods that represent and
reason the standard, explainable process of a task may need to be incorporated
into deep learning-based methods for clinical coding. Automated clinical coding
is a promising task for AI, despite the technical and organisational
challenges. Coders are needed to be involved in the development process. There
is much to achieve to develop and deploy an AI-based automated system to
support coding in the next five years and beyond.",None,-1
Is Lip Region-of-Interest Sufficient for Lipreading?,0.32461,"Lip region-of-interest (ROI) is conventionally used for visual input in the
lipreading task. Few works have adopted the entire face as visual input because
lip-excluded parts of the face are usually considered to be redundant and
irrelevant to visual speech recognition. However, faces contain much more
detailed information than lips, such as speakers' head pose, emotion, identity
etc. We argue that such information might benefit visual speech recognition if
a powerful feature extractor employing the entire face is trained. In this
work, we propose to adopt the entire face for lipreading with self-supervised
learning. AV-HuBERT, an audio-visual multi-modal self-supervised learning
framework, was adopted in our experiments. Our experimental results showed that
adopting the entire face achieved 16% relative word error rate (WER) reduction
on the lipreading task, compared with the baseline method using lip as visual
input. Without self-supervised pretraining, the model with face input achieved
a higher WER than that using lip input in the case of limited training data (30
hours), while a slightly lower WER when using large amount of training data
(433 hours).",None,-1
Differentiable Inference of Temporal Logic Formulas,0.610211,"We demonstrate the first Recurrent Neural Network architecture for learning
Signal Temporal Logic formulas, and present the first systematic comparison of
formula inference methods. Legacy systems embed much expert knowledge which is
not explicitly formalized. There is great interest in learning formal
specifications that characterize the ideal behavior of such systems -- that is,
formulas in temporal logic that are satisfied by the system's output signals.
Such specifications can be used to better understand the system's behavior and
improve design of its next iteration. Previous inference methods either assumed
certain formula templates, or did a heuristic enumeration of all possible
templates. This work proposes a neural network architecture that infers the
formula structure via gradient descent, eliminating the need for imposing any
specific templates. It combines learning of formula structure and parameters in
one optimization. Through systematic comparison, we demonstrate that this
method achieves similar or better mis-classification rates (MCR) than
enumerative and lattice methods. We also observe that different formulas can
achieve similar MCR, empirically demonstrating the under-determinism of the
problem of temporal logic inference.",https://github.com/nicaless/fernn,51
Weakly-supervised segmentation of referring expressions,0.463169,"Visual grounding localizes regions (boxes or segments) in the image
corresponding to given referring expressions. In this work we address image
segmentation from referring expressions, a problem that has so far only been
addressed in a fully-supervised setting. A fully-supervised setup, however,
requires pixel-wise supervision and is hard to scale given the expense of
manual annotation. We therefore introduce a new task of weakly-supervised image
segmentation from referring expressions and propose Text grounded semantic
SEGgmentation (TSEG) that learns segmentation masks directly from image-level
referring expressions without pixel-level annotations. Our transformer-based
method computes patch-text similarities and guides the classification objective
during training with a new multi-label patch assignment mechanism. The
resulting visual grounding model segments image regions corresponding to given
natural language expressions. Our approach TSEG demonstrates promising results
for weakly-supervised referring expression segmentation on the challenging
PhraseCut and RefCOCO datasets. TSEG also shows competitive performance when
evaluated in a zero-shot setting for semantic segmentation on Pascal VOC.",None,-1
Video + CLIP Baseline for Ego4D Long-term Action Anticipation,0.328206,"In this report, we introduce our adaptation of image-text models for
long-term action anticipation. Our Video + CLIP framework makes use of a
large-scale pre-trained paired image-text model: CLIP and a video encoder
Slowfast network. The CLIP embedding provides fine-grained understanding of
objects relevant for an action whereas the slowfast network is responsible for
modeling temporal information within a video clip of few frames. We show that
the features obtained from both encoders are complementary to each other, thus
outperforming the baseline on Ego4D for the task of long-term action
anticipation. Our code is available at
github.com/srijandas07/clip_baseline_LTA_Ego4d.",https://github.com/srijandas07/clip_baseline_LTA_Ego4d,-1
DFNet: Enhance Absolute Pose Regression with Direct Feature Matching,0.807341,"We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. By incorporating
exposure-adaptive novel view synthesis, our method successfully addresses
photometric distortions in outdoor environments that existing photometric-based
methods fail to handle. With domain-invariant feature matching, our solution
improves pose regression accuracy using semi-supervised learning on unlabeled
data. In particular, the pipeline consists of two components: Novel View
Synthesizer and DFNet. The former synthesizes novel views compensating for
changes in exposure and the latter regresses camera poses and extracts robust
features that close the domain gap between real images and synthetic ones.
Furthermore, we introduce an online synthetic data generation scheme. We show
that these approaches effectively enhance camera pose estimation both in indoor
and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by
outperforming existing single-image APR methods by as much as 56%, comparable
to 3D structure-based methods.",None,-1
A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning,0.513942,"Large datasets as required for deep learning of lip reading do not exist in
many languages. In this paper we present the dataset GLips (German Lips)
consisting of 250,000 publicly available videos of the faces of speakers of the
Hessian Parliament, which was processed for word-level lip reading using an
automatic pipeline. The format is similar to that of the English language LRW
(Lip Reading in the Wild) dataset, with each video encoding one word of
interest in a context of 1.16 seconds duration, which yields compatibility for
studying transfer learning between both datasets. By training a deep neural
network, we investigate whether lip reading has language-independent features,
so that datasets of different languages can be used to improve lip reading
models. We demonstrate learning from scratch and show that transfer learning
from LRW to GLips and vice versa improves learning speed and performance, in
particular for the validation set.",None,-1
Cross-Spectral Neural Radiance Fields,0.689794,"We propose X-NeRF, a novel method to learn a Cross-Spectral scene
representation given images captured from cameras with different light spectrum
sensitivity, based on the Neural Radiance Fields formulation. X-NeRF optimizes
camera poses across spectra during training and exploits Normalized
Cross-Device Coordinates (NXDC) to render images of different modalities from
arbitrary viewpoints, which are aligned and at the same resolution. Experiments
on 16 forward-facing scenes, featuring color, multi-spectral and infrared
images, confirm the effectiveness of X-NeRF at modeling Cross-Spectral scene
representations.",None,12441
Unsupervised Sentence Textual Similarity with Compositional Phrase Semantics,0.507148,"Measuring Sentence Textual Similarity (STS) is a classic task that can be
applied to many downstream NLP applications such as text generation and
retrieval. In this paper, we focus on unsupervised STS that works on various
domains but only requires minimal data and computational resources.
Theoretically, we propose a light-weighted Expectation-Correction (EC)
formulation for STS computation. EC formulation unifies unsupervised STS
approaches including the cosine similarity of Additively Composed (AC) sentence
embeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose
the Recursive Optimal Transport Similarity (ROTS) algorithm to capture the
compositional phrase semantics by composing multiple recursive EC formulations.
ROTS finishes in linear time and is faster than its predecessors. ROTS is
empirically more effective and scalable than previous approaches. Extensive
experiments on 29 STS tasks under various settings show the clear advantage of
ROTS over existing approaches. Detailed ablation studies demonstrate the
effectiveness of our approaches.",https://github.com/zihao-wang/rots,-1
A Graph Convolution for Signed Directed Graphs,0.0678065,"A signed directed graph is a graph with sign and direction information on the
edges. Even though signed directed graphs are more informative than unsigned or
undirected graphs, they are more complicated to analyze and have received less
research attention. This paper investigates a spectral graph convolution model
to fully utilize the information embedded in signed directed edges. We propose
a novel complex Hermitian adjacency matrix that encodes graph information via
complex numbers. Compared to a simple connection-based adjacency matrix, the
complex Hermitian can represent edge direction, sign, and connectivity via its
phases and magnitudes. Then, we define a magnetic Laplacian of the proposed
adjacency matrix and prove that it is positive semi-definite (PSD) for the
analyses using spectral graph convolution. We perform extensive experiments on
four real-world datasets. Our experiments show that the proposed scheme
outperforms several state-of-the-art techniques.",https://github.com/huangjunjie-cs/SiGAT,-1
Socio-cognitive Optimization of Time-delay Control Problems using Evolutionary Metaheuristics,0.261135,"Metaheuristics are universal optimization algorithms which should be used for
solving difficult problems, unsolvable by classic approaches. In this paper we
aim at constructing novel socio-cognitive metaheuristic based on castes, and
apply several versions of this algorithm to optimization of time-delay system
model. Besides giving the background and the details of the proposed algorithms
we apply them to optimization of selected variants of the problem and discuss
the results.",None,-1
DSI++: Updating Transformer Memory with New Documents,0.641119,"Differentiable Search Indices (DSIs) encode a corpus of documents in model
parameters and use the same model to answer user queries directly. Despite the
strong performance of DSI models, deploying them in situations where the corpus
changes over time is computationally expensive because reindexing the corpus
requires re-training the model. In this work, we introduce DSI++, a continual
learning challenge for DSI to incrementally index new documents while being
able to answer queries related to both previously and newly indexed documents.
Across different model scales and document identifier representations, we show
that continual indexing of new documents leads to considerable forgetting of
previously indexed documents. We also hypothesize and verify that the model
experiences forgetting events during training, leading to unstable learning. To
mitigate these issues, we investigate two approaches. The first focuses on
modifying the training dynamics. Flatter minima implicitly alleviate
forgetting, so we optimize for flatter loss basins and show that the model
stably memorizes more documents ($+12\%$). Next, we introduce a generative
memory to sample pseudo-queries for documents and supplement them during
continual indexing to prevent forgetting for the retrieval task. Extensive
experiments on novel continual indexing benchmarks based on Natural Questions
(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting
significantly. Concretely, it improves the average Hits@10 by $+21.1\%$ over
competitive baselines for NQ and requires $6$ times fewer model updates
compared to re-training the DSI model for incrementally indexing five corpora
in a sequence.",None,-1
GBSVM: Granular-ball Support Vector Machine,0.890357,"GBSVM (Granular-ball Support Vector Machine) is a significant attempt to
construct a classifier using the coarse-to-fine granularity of a granular-ball
as input, rather than a single data point. It is the first classifier whose
input contains no points. However, the existing model has some errors, and its
dual model has not been derived. As a result, the current algorithm cannot be
implemented or applied. To address these problems, this paper has fixed the
errors of the original model of the existing GBSVM, and derived its dual model.
Furthermore, a particle swarm optimization algorithm is designed to solve the
dual model. The sequential minimal optimization algorithm is also carefully
designed to solve the dual model. The solution is faster and more stable than
the particle swarm optimization based version. The experimental results on the
UCI benchmark datasets demonstrate that GBSVM has good robustness and
efficiency. All codes have been released in the open source library at
http://www.cquptshuyinxia.com/GBSVM.html or https://github.com/syxiaa/GBSVM.",https://github.com/syxiaa/GBSVM,-1
NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers,0.63117,"The complicated architecture and high training cost of vision transformers
urge the exploration of post-training quantization. However, the heavy-tailed
distribution of vision transformer activations hinders the effectiveness of
previous post-training quantization methods, even with advanced quantizer
designs. Instead of tuning the quantizer to better fit the complicated
activation distribution, this paper proposes NoisyQuant, a quantizer-agnostic
enhancement for the post-training activation quantization performance of vision
transformers. We make a surprising theoretical discovery that for a given
quantizer, adding a fixed Uniform noisy bias to the values being quantized can
significantly reduce the quantization error under provable conditions. Building
on the theoretical insight, NoisyQuant achieves the first success on actively
altering the heavy-tailed activation distribution with additive noisy bias to
fit a given quantizer. Extensive experiments show NoisyQuant largely improves
the post-training quantization performance of vision transformer with minimal
computation overhead. For instance, on linear uniform 6-bit activation
quantization, NoisyQuant improves SOTA top-1 accuracy on ImageNet by up to
1.7%, 1.1% and 0.5% for ViT, DeiT, and Swin Transformer respectively, achieving
on-par or even higher performance than previous nonlinear, mixed-precision
quantization.",https://github.com/rwightman/pytorch-image-models,-1
GCDT: A Chinese RST Treebank for Multigenre and Multilingual Discourse Parsing,0.46885,"A lack of large-scale human-annotated data has hampered the hierarchical
discourse parsing of Chinese. In this paper, we present GCDT, the largest
hierarchical discourse treebank for Mandarin Chinese in the framework of
Rhetorical Structure Theory (RST). GCDT covers over 60K tokens across five
genres of freely available text, using the same relation inventory as
contemporary RST treebanks for English. We also report on this dataset's
parsing experiments, including state-of-the-art (SOTA) scores for Chinese RST
parsing and RST parsing on the English GUM dataset, using cross-lingual
training in Chinese and English with multilingual embeddings.",https://github.com/logan-siyao-peng/GCDT,-1
NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles,0.304544,"In this paper, we present the fifth installment of the NELA-GT datasets,
NELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between
January 1st, 2022 and December 31st, 2022. Just as in past releases of the
dataset, NELA-GT-2022 includes outlet-level veracity labels from Media
Bias/Fact Check and tweets embedded in collected news articles. The
NELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H",https://github.com/MELALab/nela-gt,-1
Augmented cross-selling through explainable AI -- a case from energy retailing,0.205263,"The advance of Machine Learning (ML) has led to a strong interest in this
technology to support decision making. While complex ML models provide
predictions that are often more accurate than those of traditional tools, such
models often hide the reasoning behind the prediction from their users, which
can lead to lower adoption and lack of insight. Motivated by this tension,
research has put forth Explainable Artificial Intelligence (XAI) techniques
that uncover patterns discovered by ML. Despite the high hopes in both ML and
XAI, there is little empirical evidence of the benefits to traditional
businesses. To this end, we analyze data on 220,185 customers of an energy
retailer, predict cross-purchases with up to 86% correctness (AUC), and show
that the XAI method SHAP provides explanations that hold for actual buyers. We
further outline implications for research in information systems, XAI, and
relationship marketing.",None,-1
Evaluating Parameter Efficient Learning for Generation,0.100147,"Parameter efficient learning methods (PERMs) have recently gained significant
attention as they provide an efficient way for pre-trained language models
(PLMs) to adapt to a downstream task. However, these conclusions are mostly
drawn from in-domain evaluations over the full training set. In this paper, we
present comparisons between PERMs and finetuning from three new perspectives:
(1) the effect of sample and model size to in-domain evaluations, (2)
generalization to unseen domains and new datasets, and (3) the faithfulness of
generations. Our results show that for in-domain settings (a) there is a cross
point of sample size for which PERMs will perform better than finetuning when
training with fewer samples, and (b) larger PLMs have larger cross points. For
cross-domain and cross-dataset cases, we show that (a) Adapter (Houlsby et al.,
2019) performs the best amongst all the PERMs studied here, and (b) it
outperforms finetuning if the task dataset is below a certain size. We also
compare the faithfulness of generations and show that PERMs can achieve better
faithfulness score than finetuning, especially for small training set, by as
much as 6%. Finally, we apply Adapter to MT-NLG 530b (Smith et al., 2022) and
achieve new state-of-the-art results on Xsum (Narayan et al., 2018) for all
ROUGE scores (ROUGE-1 49.17, ROUGE-2 27.20, ROUGE-L 40.98).",None,-1
Iterative Patch Selection for High-Resolution Image Recognition,0.666843,"High-resolution images are prevalent in various applications, such as
autonomous driving and computer-aided diagnosis. However, training neural
networks on such images is computationally challenging and easily leads to
out-of-memory errors even on modern GPUs. We propose a simple method, Iterative
Patch Selection (IPS), which decouples the memory usage from the input size and
thus enables the processing of arbitrarily large images under tight hardware
constraints. IPS achieves this by selecting only the most salient patches,
which are then aggregated into a global representation for image recognition.
For both patch selection and aggregation, a cross-attention based transformer
is introduced, which exhibits a close connection to Multiple Instance Learning.
Our method demonstrates strong performance and has wide applicability across
different domains, training regimes and image sizes while using minimal
accelerator memory. For example, we are able to finetune our model on
whole-slide images consisting of up to 250k patches (>16 gigapixels) with only
5 GB of GPU VRAM at a batch size of 16.",https://github.com/benbergner/ips,-1
Boosting 3D Adversarial Attacks with Attacking On Frequency,0.648193,"Deep neural networks (DNNs) have been shown to be vulnerable to adversarial
attacks. Recently, 3D adversarial attacks, especially adversarial attacks on
point clouds, have elicited mounting interest. However, adversarial point
clouds obtained by previous methods show weak transferability and are easy to
defend. To address these problems, in this paper we propose a novel point cloud
attack (dubbed AOF) that pays more attention on the low-frequency component of
point clouds. We combine the losses from point cloud and its low-frequency
component to craft adversarial samples. Extensive experiments validate that AOF
can improve the transferability significantly compared to state-of-the-art
(SOTA) attacks, and is more robust to SOTA 3D defense methods. Otherwise,
compared to clean point clouds, adversarial point clouds obtained by AOF
contain more deformation than outlier.",None,-1
Hierarchical Multi-Label Classification of Scientific Documents,0.673935,"Automatic topic classification has been studied extensively to assist
managing and indexing scientific documents in a digital collection. With the
large number of topics being available in recent years, it has become necessary
to arrange them in a hierarchy. Therefore, the automatic classification systems
need to be able to classify the documents hierarchically. In addition, each
paper is often assigned to more than one relevant topic. For example, a paper
can be assigned to several topics in a hierarchy tree. In this paper, we
introduce a new dataset for hierarchical multi-label text classification
(HMLTC) of scientific papers called SciHTC, which contains 186,160 papers and
1,233 categories from the ACM CCS tree. We establish strong baselines for HMLTC
and propose a multi-task learning approach for topic classification with
keyword labeling as an auxiliary task. Our best model achieves a Macro-F1 score
of 34.57% which shows that this dataset provides significant research
opportunities on hierarchical scientific topic classification. We make our
dataset and code available on Github.",https://github.com/msadat3/SciHTC,-1
SCAMPS: Synthetics for Camera Measurement of Physiological Signals,0.932711,"The use of cameras and computational algorithms for noninvasive, low-cost and
scalable measurement of physiological (e.g., cardiac and pulmonary) vital signs
is very attractive. However, diverse data representing a range of environments,
body motions, illumination conditions and physiological states is laborious,
time consuming and expensive to obtain. Synthetic data have proven a valuable
tool in several areas of machine learning, yet are not widely available for
camera measurement of physiological states. Synthetic data offer ""perfect""
labels (e.g., without noise and with precise synchronization), labels that may
not be possible to obtain otherwise (e.g., precise pixel level segmentation
maps) and provide a high degree of control over variation and diversity in the
dataset. We present SCAMPS, a dataset of synthetics containing 2,800 videos
(1.68M frames) with aligned cardiac and respiratory signals and facial action
intensities. The RGB frames are provided alongside segmentation maps. We
provide precise descriptive statistics about the underlying waveforms,
including inter-beat interval, heart rate variability, and pulse arrival time.
Finally, we present baseline results training on these synthetic data and
testing on real-world datasets to illustrate generalizability.",https://github.com/danmcduff/scampsdataset,-1
Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics,0.635434,"Recent work incorporates pre-trained word embeddings such as BERT embeddings
into Neural Topic Models (NTMs), generating highly coherent topics. However,
with high-quality contextualized document representations, do we really need
sophisticated neural models to obtain coherent and interpretable topics? In
this paper, we conduct thorough experiments showing that directly clustering
high-quality sentence embeddings with an appropriate word selecting method can
generate more coherent and diverse topics than NTMs, achieving also higher
efficiency and simplicity.",https://github.com/hyintell/topicx,-1
Goal-Space Planning with Subgoal Models,0.137357,"This paper investigates a new approach to model-based reinforcement learning
using background planning: mixing (approximate) dynamic programming updates and
model-free updates, similar to the Dyna architecture. Background planning with
learned models is often worse than model-free alternatives, such as Double DQN,
even though the former uses significantly more memory and computation. The
fundamental problem is that learned models can be inaccurate and often generate
invalid states, especially when iterated many steps. In this paper, we avoid
this limitation by constraining background planning to a set of (abstract)
subgoals and learning only local, subgoal-conditioned models. This goal-space
planning (GSP) approach is more computationally efficient, naturally
incorporates temporal abstraction for faster long-horizon planning and avoids
learning the transition dynamics entirely. We show that our GSP algorithm can
propagate value from an abstract space in a manner that helps a variety of base
learners learn significantly faster in different domains.",None,-1
DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation,0.691153,"Task-oriented dialogue generation is challenging since the underlying
knowledge is often dynamic and effectively incorporating knowledge into the
learning process is hard. It is particularly challenging to generate both
human-like and informative responses in this setting. Recent research primarily
focused on various knowledge distillation methods where the underlying
relationship between the facts in a knowledge base is not effectively captured.
In this paper, we go one step further and demonstrate how the structural
information of a knowledge graph can improve the system's inference
capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue
system that effectively incorporates knowledge into a language model. Our
proposed system views relational knowledge as a knowledge graph and introduces
(1) a structure-aware knowledge embedding technique, and (2) a knowledge
graph-weighted attention masking strategy to facilitate the system selecting
relevant information during the dialogue generation. An empirical evaluation
demonstrates the effectiveness of DialoKG over state-of-the-art methods on
several standard benchmark datasets.",https://github.com/rashad101/DialoKG,-1
The Arabic Ontology -- An Arabic Wordnet with Ontologically Clean Content,0.771269,"We present a formal Arabic wordnet built on the basis of a carefully designed
ontology hereby referred to as the Arabic Ontology. The ontology provides a
formal representation of the concepts that the Arabic terms convey, and its
content was built with ontological analysis in mind, and benchmarked to
scientific advances and rigorous knowledge sources as much as this is possible,
rather than to only speakers' beliefs as lexicons typically are. A
comprehensive evaluation was conducted thereby demonstrating that the current
version of the top-levels of the ontology can top the majority of the Arabic
meanings. The ontology consists currently of about 1,300 well-investigated
concepts in addition to 11,000 concepts that are partially validated. The
ontology is accessible and searchable through a lexicographic search engine
(https://ontology.birzeit.edu) that also includes about 150 Arabic-multilingual
lexicons, and which are being mapped and enriched using the ontology. The
ontology is fully mapped with Princeton WordNet, Wikidata, and other resources.",None,-1
Findings of the The RuATD Shared Task 2022 on Artificial Text Detection in Russian,0.505969,"We present the shared task on artificial text detection in Russian, which is
organized as a part of the Dialogue Evaluation initiative, held in 2022. The
shared task dataset includes texts from 14 text generators, i.e., one human
writer and 13 text generative models fine-tuned for one or more of the
following generation tasks: machine translation, paraphrase generation, text
summarization, text simplification. We also consider back-translation and
zero-shot generation approaches. The human-written texts are collected from
publicly available resources across multiple domains. The shared task consists
of two sub-tasks: (i) to determine if a given text is automatically generated
or written by a human; (ii) to identify the author of a given text. The first
task is framed as a binary classification problem. The second task is a
multi-class classification problem. We provide count-based and BERT-based
baselines, along with the human evaluation on the first sub-task. A total of 30
and 8 systems have been submitted to the binary and multi-class sub-tasks,
correspondingly. Most teams outperform the baselines by a wide margin. We
publicly release our codebase, human evaluation results, and other materials in
our GitHub repository (https://github.com/dialogue-evaluation/RuATD).",https://github.com/dialogue-evaluation/RuATD,2091
Design and Development of Rule-based open-domain Question-Answering System on SQuAD v2.0 Dataset,0.16923,"Human mind is the palace of curious questions that seek answers.
Computational resolution of this challenge is possible through Natural Language
Processing techniques. Statistical techniques like machine learning and deep
learning require a lot of data to train and despite that they fail to tap into
the nuances of language. Such systems usually perform best on close-domain
datasets. We have proposed development of a rule-based open-domain
question-answering system which is capable of answering questions of any domain
from a corresponding context passage. We have used 1000 questions from SQuAD
2.0 dataset for testing the developed system and it gives satisfactory results.
In this paper, we have described the structure of the developed system and have
analyzed the performance.",None,1659
N-Best Hypotheses Reranking for Text-To-SQL Systems,0.861995,"Text-to-SQL task maps natural language utterances to structured queries that
can be issued to a database. State-of-the-art (SOTA) systems rely on finetuning
large, pre-trained language models in conjunction with constrained decoding
applying a SQL parser. On the well established Spider dataset, we begin with
Oracle studies: specifically, choosing an Oracle hypothesis from a SOTA model's
10-best list, yields a $7.7\%$ absolute improvement in both exact match (EM)
and execution (EX) accuracy, showing significant potential improvements with
reranking. Identifying coherence and correctness as reranking approaches, we
design a model generating a query plan and propose a heuristic schema linking
algorithm. Combining both approaches, with T5-Large, we obtain a consistent
$1\% $ improvement in EM accuracy, and a $~2.5\%$ improvement in EX,
establishing a new SOTA for this task. Our comprehensive error studies on DEV
data show the underlying difficulty in making progress on this task.",https://github.com/microsoft/DeepSpeed,-1
Local Feature Swapping for Generalization in Reinforcement Learning,0.200777,"Over the past few years, the acceleration of computing resources and research
in deep learning has led to significant practical successes in a range of
tasks, including in particular in computer vision. Building on these advances,
reinforcement learning has also seen a leap forward with the emergence of
agents capable of making decisions directly from visual observations. Despite
these successes, the over-parametrization of neural architectures leads to
memorization of the data used during training and thus to a lack of
generalization. Reinforcement learning agents based on visual inputs also
suffer from this phenomenon by erroneously correlating rewards with unrelated
visual features such as background elements. To alleviate this problem, we
introduce a new regularization technique consisting of channel-consistent local
permutations (CLOP) of the feature maps. The proposed permutations induce
robustness to spatial correlations and help prevent overfitting behaviors in
RL. We demonstrate, on the OpenAI Procgen Benchmark, that RL agents trained
with the CLOP method exhibit robustness to visual changes and better
generalization properties than agents trained using other state-of-the-art
regularization techniques. We also demonstrate the effectiveness of CLOP as a
general regularization technique in supervised learning.",https://github.com/rraileanu/idaac,-1
Neural Contourlet Network for Monocular 360 Depth Estimation,0.550373,"For a monocular 360 image, depth estimation is a challenging because the
distortion increases along the latitude. To perceive the distortion, existing
methods devote to designing a deep and complex network architecture. In this
paper, we provide a new perspective that constructs an interpretable and sparse
representation for a 360 image. Considering the importance of the geometric
structure in depth estimation, we utilize the contourlet transform to capture
an explicit geometric cue in the spectral domain and integrate it with an
implicit cue in the spatial domain. Specifically, we propose a neural
contourlet network consisting of a convolutional neural network and a
contourlet transform branch. In the encoder stage, we design a spatial-spectral
fusion module to effectively fuse two types of cues. Contrary to the encoder,
we employ the inverse contourlet transform with learned low-pass subbands and
band-pass directional subbands to compose the depth in the decoder. Experiments
on the three popular panoramic image datasets demonstrate that the proposed
approach outperforms the state-of-the-art schemes with faster convergence. Code
is available at
https://github.com/zhijieshen-bjtu/Neural-Contourlet-Network-for-MODE.",None,-1
Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue,0.457297,"Complex conversation settings such as persuasion involve communicating
changes in attitude or behavior, so users' perspectives need to be addressed,
even when not directly related to the topic. In this work, we contribute a
novel modular dialogue system framework that seamlessly integrates factual
information and social content into persuasive dialogue. Our framework is
generalizable to any dialogue tasks that have mixed social and task contents.
We conducted a study that compared user evaluations of our framework versus a
baseline end-to-end generation model. We found our framework was evaluated more
favorably in all dimensions including competence and friendliness, compared to
the end-to-end model which does not explicitly handle social content or factual
questions.",None,-1
3D Dense Face Alignment with Fused Features by Aggregating CNNs and GCNs,0.166233,"In this paper, we propose a novel multi-level aggregation network to regress
the coordinates of the vertices of a 3D face from a single 2D image in an
end-to-end manner. This is achieved by seamlessly combining standard
convolutional neural networks (CNNs) with Graph Convolution Networks (GCNs). By
iteratively and hierarchically fusing the features across different layers and
stages of the CNNs and GCNs, our approach can provide a dense face alignment
and 3D face reconstruction simultaneously for the benefit of direct feature
learning of 3D face mesh. Experiments on several challenging datasets
demonstrate that our method outperforms state-of-the-art approaches on both 2D
and 3D face alignment tasks.",None,-1
Improving the Cross-Lingual Generalisation in Visual Question Answering,0.0445715,"While several benefits were realized for multilingual vision-language
pretrained models, recent benchmarks across various tasks and languages showed
poor cross-lingual generalisation when multilingually pre-trained
vision-language models are applied to non-English data, with a large gap
between (supervised) English performance and (zero-shot) cross-lingual
transfer. In this work, we explore the poor performance of these models on a
zero-shot cross-lingual visual question answering (VQA) task, where models are
fine-tuned on English visual-question data and evaluated on 7 typologically
diverse languages. We improve cross-lingual transfer with three strategies: (1)
we introduce a linguistic prior objective to augment the cross-entropy loss
with a similarity-based loss to guide the model during training, (2) we learn a
task-specific subnetwork that improves cross-lingual generalisation and reduces
variance without model modification, (3) we augment training examples using
synthetic code-mixing to promote alignment of embeddings between source and
target languages. Our experiments on xGQA using the pretrained multilingual
multimodal transformers UC2 and M3P demonstrate the consistent effectiveness of
the proposed fine-tuning strategy for 7 languages, outperforming existing
transfer methods with sparse models. Code and data to reproduce our findings
are publicly available.",https://github.com/nooralahzadeh/CLG-VQA,21498
Learning English with Peppa Pig,0.287925,"Recent computational models of the acquisition of spoken language via
grounding in perception exploit associations between the spoken and visual
modalities and learn to represent speech and visual data in a joint vector
space. A major unresolved issue from the point of ecological validity is the
training data, typically consisting of images or videos paired with spoken
descriptions of what is depicted. Such a setup guarantees an unrealistically
strong correlation between speech and the visual data. In the real world the
coupling between the linguistic and the visual modality is loose, and often
confounded by correlations with non-semantic aspects of the speech signal. Here
we address this shortcoming by using a dataset based on the children's cartoon
Peppa Pig. We train a simple bi-modal architecture on the portion of the data
consisting of dialog between characters, and evaluate on segments containing
descriptive narrations. Despite the weak and confounded signal in this training
data our model succeeds at learning aspects of the visual semantics of spoken
language.",https://github.com/gchrupala/peppa,-1
Fairly Accurate: Learning Optimal Accuracy vs. Fairness Tradeoffs for Hate Speech Detection,0.0911082,"Recent work has emphasized the importance of balancing competing objectives
in model training (e.g., accuracy vs. fairness, or competing measures of
fairness). Such trade-offs reflect a broader class of multi-objective
optimization (MOO) problems in which optimization methods seek Pareto optimal
trade-offs between competing goals. In this work, we first introduce a
differentiable measure that enables direct optimization of group fairness
(specifically, balancing accuracy across groups) in model training. Next, we
demonstrate two model-agnostic MOO frameworks for learning Pareto optimal
parameterizations over different groups of neural classification models. We
evaluate our methods on the specific task of hate speech detection, in which
prior work has shown lack of group fairness across speakers of different
English dialects. Empirical results across convolutional, sequential, and
transformer-based neural architectures show superior empirical accuracy vs.
fairness trade-offs over prior work. More significantly, our measure enables
the Pareto machinery to ensure that each architecture achieves the best
possible trade-off between fairness and accuracy w.r.t. the dataset, given
user-prescribed error tolerance bounds.",None,-1
C-VTON: Context-Driven Image-Based Virtual Try-On Network,0.999691,"Image-based virtual try-on techniques have shown great promise for enhancing
the user-experience and improving customer satisfaction on fashion-oriented
e-commerce platforms. However, existing techniques are currently still limited
in the quality of the try-on results they are able to produce from input images
of diverse characteristics. In this work, we propose a Context-Driven Virtual
Try-On Network (C-VTON) that addresses these limitations and convincingly
transfers selected clothing items to the target subjects even under challenging
pose configurations and in the presence of self-occlusions. At the core of the
C-VTON pipeline are: (i) a geometric matching procedure that efficiently aligns
the target clothing with the pose of the person in the input images, and (ii) a
powerful image generator that utilizes various types of contextual information
when synthesizing the final try-on result. C-VTON is evaluated in rigorous
experiments on the VITON and MPV datasets and in comparison to state-of-the-art
techniques from the literature. Experimental results show that the proposed
approach is able to produce photo-realistic and visually convincing results and
significantly improves on the existing state-of-the-art.",https://github.com/benquick123/C-VTON,3927
Conformal Risk Control,0.999878,"We extend conformal prediction to control the expected value of any monotone
loss function. The algorithm generalizes split conformal prediction together
with its coverage guarantee. Like conformal prediction, the conformal risk
control procedure is tight up to an $\mathcal{O}(1/n)$ factor. We also
introduce extensions of the idea to distribution shift, quantile risk control,
multiple and adversarial risk control, and expectations of U-statistics. Worked
examples from computer vision and natural language processing demonstrate the
usage of our algorithm to bound the false negative rate, graph distance, and
token-level F1-score.",None,-1
CounTR: Transformer-based Generalised Visual Counting,0.701063,"In this paper, we consider the problem of generalised visual object counting,
with the goal of developing a computational model for counting the number of
objects from arbitrary semantic categories, using arbitrary number of
""exemplars"", i.e. zero-shot or few-shot counting. To this end, we make the
following four contributions: (1) We introduce a novel transformer-based
architecture for generalised visual object counting, termed as Counting
Transformer (CounTR), which explicitly capture the similarity between image
patches or with given ""exemplars"" with the attention mechanism;(2) We adopt a
two-stage training regime, that first pre-trains the model with self-supervised
learning, and followed by supervised fine-tuning;(3) We propose a simple,
scalable pipeline for synthesizing training images with a large number of
instances or that from different semantic categories, explicitly forcing the
model to make use of the given ""exemplars"";(4) We conduct thorough ablation
studies on the large-scale counting benchmark, e.g. FSC-147, and demonstrate
state-of-the-art performance on both zero and few-shot settings.",https://verg-avesta.github.io/CounTR_Webpage/,-1
Bridging POMDPs and Bayesian decision making for robust maintenance planning under model uncertainty: An application to railway systems,0.734882,"Structural Health Monitoring (SHM) describes a process for inferring
quantifiable metrics of structural condition, which can serve as input to
support decisions on the operation and maintenance of infrastructure assets.
Given the long lifespan of critical structures, this problem can be cast as a
sequential decision making problem over prescribed horizons. Partially
Observable Markov Decision Processes (POMDPs) offer a formal framework to solve
the underlying optimal planning task. However, two issues can undermine the
POMDP solutions. Firstly, the need for a model that can adequately describe the
evolution of the structural condition under deterioration or corrective actions
and, secondly, the non-trivial task of recovery of the observation process
parameters from available monitoring data. Despite these potential challenges,
the adopted POMDP models do not typically account for uncertainty on model
parameters, leading to solutions which can be unrealistically confident. In
this work, we address both key issues. We present a framework to estimate POMDP
transition and observation model parameters directly from available data, via
Markov Chain Monte Carlo (MCMC) sampling of a Hidden Markov Model (HMM)
conditioned on actions. The MCMC inference estimates distributions of the
involved model parameters. We then form and solve the POMDP problem by
exploiting the inferred distributions, to derive solutions that are robust to
model uncertainty. We successfully apply our approach on maintenance planning
for railway track assets on the basis of a ""fractal value"" indicator, which is
computed from actual railway monitoring data.",http://github.com/google/jax,9608
Development of Automatic Endotracheal Tube and Carina Detection on Portable Supine Chest Radiographs using Artificial Intelligence,0.2809,"The image quality of portable supine chest radiographs is inherently poor due
to low contrast and high noise. The endotracheal intubation detection requires
the locations of the endotracheal tube (ETT) tip and carina. The goal is to
find the distance between the ETT tip and the carina in chest radiography. To
overcome such a problem, we propose a feature extraction method with Mask
R-CNN. The Mask R-CNN predicts a tube and a tracheal bifurcation in an image.
Then, the feature extraction method is used to find the feature point of the
ETT tip and that of the carina. Therefore, the ETT-carina distance can be
obtained. In our experiments, our results can exceed 96\% in terms of recall
and precision. Moreover, the object error is less than $4.7751\pm 5.3420$ mm,
and the ETT-carina distance errors are less than $5.5432\pm 6.3100$ mm. The
external validation shows that the proposed method is a high-robustness system.
According to the Pearson correlation coefficient, we have a strong correlation
between the board-certified intensivists and our result in terms of ETT-carina
distance.",None,857
Robust Category-Level 6D Pose Estimation with Coarse-to-Fine Rendering of Neural Features,0.660481,"We consider the problem of category-level 6D pose estimation from a single
RGB image. Our approach represents an object category as a cuboid mesh and
learns a generative model of the neural feature activations at each mesh vertex
to perform pose estimation through differentiable rendering. A common problem
of rendering-based approaches is that they rely on bounding box proposals,
which do not convey information about the 3D rotation of the object and are not
reliable when objects are partially occluded. Instead, we introduce a
coarse-to-fine optimization strategy that utilizes the rendering process to
estimate a sparse set of 6D object proposals, which are subsequently refined
with gradient-based optimization. The key to enabling the convergence of our
approach is a neural feature representation that is trained to be scale- and
rotation-invariant using contrastive learning. Our experiments demonstrate an
enhanced category-level 6D pose estimation performance compared to prior work,
particularly under strong partial occlusion.",None,-1
Don't Say What You Don't Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search,0.684194,"Abstractive summarization systems today produce fluent and relevant output,
but often ""hallucinate"" statements not supported by the source text. We analyze
the connection between hallucinations and training data, and find evidence that
models hallucinate because they train on target summaries that are unsupported
by the source. Based on our findings, we present PINOCCHIO, a new decoding
method that improves the consistency of a transformer-based abstractive
summarizer by constraining beam search to avoid hallucinations. Given the model
states and outputs at a given step, PINOCCHIO detects likely model
hallucinations based on various measures of attribution to the source text.
PINOCCHIO backtracks to find more consistent output, and can opt to produce no
summary at all when no consistent generation can be found. In experiments, we
find that PINOCCHIO improves the consistency of generation (in terms of F1) by
an average of~67% on two abstractive summarization datasets.",https://github.com/allenai/pinocchio,14304
Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives,0.953495,"This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a
new member in the family of InfoNCE losses that preserves a ranked ordering of
positive samples. In contrast to the standard InfoNCE loss, which requires a
strict binary separation of the training pairs into similar and dissimilar
samples, RINCE can exploit information about a similarity ranking for learning
a corresponding embedding space. We show that the proposed loss function learns
favorable embeddings compared to the standard InfoNCE whenever at least noisy
ranking information can be obtained or when the definition of positives and
negatives is blurry. We demonstrate this for a supervised classification task
with additional superclass labels and noisy similarity scores. Furthermore, we
show that RINCE can also be applied to unsupervised training with experiments
on unsupervised representation learning from videos. In particular, the
embedding yields higher classification accuracy, retrieval rates and performs
better in out-of-distribution detection than the standard InfoNCE loss.",https://github.com/boschresearch/rince,-1
Fourier Document Restoration for Robust Document Dewarping and Recognition,0.538112,"State-of-the-art document dewarping techniques learn to predict 3-dimensional
information of documents which are prone to errors while dealing with documents
with irregular distortions or large variations in depth. This paper presents
FDRNet, a Fourier Document Restoration Network that can restore documents with
different distortions and improve document recognition in a reliable and
simpler manner. FDRNet focuses on high-frequency components in the Fourier
space that capture most structural information but are largely free of
degradation in appearance. It dewarps documents by a flexible Thin-Plate Spline
transformation which can handle various deformations effectively without
requiring deformation annotations in training. These features allow FDRNet to
learn from a small amount of simply labeled training images, and the learned
model can dewarp documents with complex geometric distortion and recognize the
restored texts accurately. To facilitate document restoration research, we
create a benchmark dataset consisting of over one thousand camera documents
with different types of geometric and photometric distortion. Extensive
experiments show that FDRNet outperforms the state-of-the-art by large margins
on both dewarping and text recognition tasks. In addition, FDRNet requires a
small amount of simply labeled training data and is easy to deploy.",None,-1
"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",0.698401,"We introduce RoMQA, the first benchmark for robust, multi-evidence,
multi-answer question answering (QA). RoMQA contains clusters of questions that
are derived from related constraints mined from the Wikidata knowledge graph.
RoMQA evaluates robustness of QA models to varying constraints by measuring
worst-case performance within each question cluster. Compared to prior QA
datasets, RoMQA has more human-written questions that require reasoning over
more evidence text and have, on average, many more correct answers. In
addition, human annotators rate RoMQA questions as more natural or likely to be
asked by people. We evaluate state-of-the-art large language models in
zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is
challenging: zero-shot and few-shot models perform similarly to naive
baselines, while supervised retrieval methods perform well below gold evidence
upper bounds. Moreover, existing models are not robust to variations in
question constraints, but can be made more robust by tuning on clusters of
related questions. Our results show that RoMQA is a challenging benchmark for
large language models, and provides a quantifiable test to build more robust QA
methods.",https://github.com/facebookresearch/romqa,-1
Scaling Knowledge Graphs for Automating AI of Digital Twins,0.136803,"Digital Twins are digital representations of systems in the Internet of
Things (IoT) that are often based on AI models that are trained on data from
those systems. Semantic models are used increasingly to link these datasets
from different stages of the IoT systems life-cycle together and to
automatically configure the AI modelling pipelines. This combination of
semantic models with AI pipelines running on external datasets raises unique
challenges particular if rolled out at scale. Within this paper we will discuss
the unique requirements of applying semantic graphs to automate Digital Twins
in different practical use cases. We will introduce the benchmark dataset DTBM
that reflects these characteristics and look into the scaling challenges of
different knowledge graph technologies. Based on these insights we will propose
a reference architecture that is in-use in multiple products in IBM and derive
lessons learned for scaling knowledge graphs for configuring AI models for
Digital Twins.",https://github.com/IBM/digital-twin-benchmark-model,-1
Interacting Hand-Object Pose Estimation via Dense Mutual Attention,0.64656,"3D hand-object pose estimation is the key to the success of many computer
vision applications. The main focus of this task is to effectively model the
interaction between the hand and an object. To this end, existing works either
rely on interaction constraints in a computationally-expensive iterative
optimization, or consider only a sparse correlation between sampled hand and
object keypoints. In contrast, we propose a novel dense mutual attention
mechanism that is able to model fine-grained dependencies between the hand and
the object. Specifically, we first construct the hand and object graphs
according to their mesh structures. For each hand node, we aggregate features
from every object node by the learned attention and vice versa for each object
node. Thanks to such dense mutual attention, our method is able to produce
physically plausible poses with high quality and real-time inference speed.
Extensive quantitative and qualitative experiments on large benchmark datasets
show that our method outperforms state-of-the-art methods. The code is
available at https://github.com/rongakowang/DenseMutualAttention.git.",https://github.com/rongakowang/DenseMutualAttention.git,24
Attention-based Feature Compression for CNN Inference Offloading in Edge Computing,0.205165,"This paper studies the computational offloading of CNN inference in
device-edge co-inference systems. Inspired by the emerging paradigm semantic
communication, we propose a novel autoencoder-based CNN architecture (AECNN),
for effective feature extraction at end-device. We design a feature compression
module based on the channel attention method in CNN, to compress the
intermediate data by selecting the most important features. To further reduce
communication overhead, we can use entropy encoding to remove the statistical
redundancy in the compressed data. At the receiver, we design a lightweight
decoder to reconstruct the intermediate data through learning from the received
compressed data to improve accuracy. To fasten the convergence, we use a
step-by-step approach to train the neural networks obtained based on ResNet-50
architecture. Experimental results show that AECNN can compress the
intermediate data by more than 256x with only about 4% accuracy loss, which
outperforms the state-of-the-art work, BottleNet++. Compared to offloading
inference task directly to edge server, AECNN can complete inference task
earlier, in particular, under poor wireless channel condition, which highlights
the effectiveness of AECNN in guaranteeing higher accuracy within time
constraint.",None,-1
Multi-Faceted Distillation of Base-Novel Commonality for Few-shot Object Detection,0.393716,"Most of existing methods for few-shot object detection follow the fine-tuning
paradigm, which potentially assumes that the class-agnostic generalizable
knowledge can be learned and transferred implicitly from base classes with
abundant samples to novel classes with limited samples via such a two-stage
training strategy. However, it is not necessarily true since the object
detector can hardly distinguish between class-agnostic knowledge and
class-specific knowledge automatically without explicit modeling. In this work
we propose to learn three types of class-agnostic commonalities between base
and novel classes explicitly: recognition-related semantic commonalities,
localization-related semantic commonalities and distribution commonalities. We
design a unified distillation framework based on a memory bank, which is able
to perform distillation of all three types of commonalities jointly and
efficiently. Extensive experiments demonstrate that our method can be readily
integrated into most of existing fine-tuning based methods and consistently
improve the performance by a large margin.",https://github.com/WuShuang1998/MFDC,-1
Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks,0.0229479,"Pre-trained language models have shown excellent results in few-shot learning
scenarios using in-context learning. Although it is impressive, the size of
language models can be prohibitive to make them usable in on-device
applications, such as sensors or smartphones. With smaller language models,
task-specific data annotation is needed to fine-tune the language model for a
specific purpose. However, data annotation can have a substantial financial and
time burden for small research groups, startups, and even companies. In this
paper, we analyze different prompt-based fine-tuning techniques to improve
results on both language and multimodal causal transformer models. To evaluate
our results, we use a dataset focusing on visual commonsense reasoning in time.
Our results show that by simple model-agnostic prompt-based fine-tuning,
comparable results can be reached by only using 35%-40% of the fine-tuning
training dataset. The proposed approaches result in significant time and
financial savings. As the proposed methods make minimal architectural
assumptions, other researchers can use the results in their transformer models
with minimal adaptations. We plan to release the source code freely to make it
easier for the community to use and contribute to our work.",None,-1
Submodularity In Machine Learning and Artificial Intelligence,0.999539,"In this manuscript, we offer a gentle review of submodularity and
supermodularity and their properties. We offer a plethora of submodular
definitions; a full description of a number of example submodular functions and
their generalizations; example discrete constraints; a discussion of basic
algorithms for maximization, minimization, and other operations; a brief
overview of continuous submodular extensions; and some historical applications.
We then turn to how submodularity is useful in machine learning and artificial
intelligence. This includes summarization, and we offer a complete account of
the differences between and commonalities amongst sketching, coresets,
extractive and abstractive summarization in NLP, data distillation and
condensation, and data subset selection and feature selection. We discuss a
variety of ways to produce a submodular function useful for machine learning,
including heuristic hand-crafting, learning or approximately learning a
submodular function or aspects thereof, and some advantages of the use of a
submodular function as a coreset producer. We discuss submodular combinatorial
information functions, and how submodularity is useful for clustering, data
partitioning, parallel machine learning, active and semi-supervised learning,
probabilistic modeling, and structured norms and loss functions.",None,-1
DICE: Data-Efficient Clinical Event Extraction with Generative Models,0.784135,"Event extraction for the clinical domain is an under-explored research area.
The lack of training data along with the high volume of domain-specific
terminologies with vague entity boundaries makes the task especially
challenging. In this paper, we introduce DICE, a robust and data-efficient
generative model for clinical event extraction. DICE frames event extraction as
a conditional generation problem and introduces a contrastive learning
objective to accurately decide the boundaries of biomedical mentions. DICE also
trains an auxiliary mention identification task jointly with event extraction
tasks to better identify entity mention boundaries, and further introduces
special markers to incorporate identified entity mentions as trigger and
argument candidates for their respective tasks. To benchmark clinical event
extraction, we compose MACCROBAT-EE, the first clinical event extraction
dataset with argument annotation, based on an existing clinical information
extraction dataset MACCROBAT. Our experiments demonstrate state-of-the-art
performances of DICE for clinical and news domain event extraction, especially
under low data settings.",None,-1
Out-Of-Distribution Detection In Unsupervised Continual Learning,0.426876,"Unsupervised continual learning aims to learn new tasks incrementally without
requiring human annotations. However, most existing methods, especially those
targeted on image classification, only work in a simplified scenario by
assuming all new data belong to new tasks, which is not realistic if the class
labels are not provided. Therefore, to perform unsupervised continual learning
in real life applications, an out-of-distribution detector is required at
beginning to identify whether each new data corresponds to a new task or
already learned tasks, which still remains under-explored yet. In this work, we
formulate the problem for Out-of-distribution Detection in Unsupervised
Continual Learning (OOD-UCL) with the corresponding evaluation protocol. In
addition, we propose a novel OOD detection method by correcting the output bias
at first and then enhancing the output confidence for in-distribution data
based on task discriminativeness, which can be applied directly without
modifying the learning procedures and objectives of continual learning. Our
method is evaluated on CIFAR-100 dataset by following the proposed evaluation
protocol and we show improved performance compared with existing OOD detection
methods under the unsupervised continual learning scenario.",None,-1
"Paraphrasing, textual entailment, and semantic similarity above word level",0.108878,"This dissertation explores the linguistic and computational aspects of the
meaning relations that can hold between two or more complex linguistic
expressions (phrases, clauses, sentences, paragraphs). In particular, it
focuses on Paraphrasing, Textual Entailment, Contradiction, and Semantic
Similarity.
  In Part I: ""Similarity at the Level of Words and Phrases"", I study the
Distributional Hypothesis (DH) and explore several different methodologies for
quantifying semantic similarity at the levels of words and short phrases.
  In Part II: ""Paraphrase Typology and Paraphrase Identification"", I focus on
the meaning relation of paraphrasing and the empirical task of automated
Paraphrase Identification (PI).
  In Part III: ""Paraphrasing, Textual Entailment, and Semantic Similarity"", I
present a novel direction in the research on textual meaning relations,
resulting from joint research carried out on on paraphrasing, textual
entailment, contradiction, and semantic similarity.",None,12
SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient object detection,0.940169,"Convolutional neural networks (CNNs) are good at extracting contexture
features within certain receptive fields, while transformers can model the
global long-range dependency features. By absorbing the advantage of
transformer and the merit of CNN, Swin Transformer shows strong feature
representation ability. Based on it, we propose a cross-modality fusion model
SwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin
Transformer to extract the hierarchical features, boosted by attention
mechanism to bridge the gap between two modalities, and guided by edge
information to sharp the contour of salient object. To be specific, two-stream
Swin Transformer encoder first extracts multi-modality features, and then
spatial alignment and channel re-calibration module is presented to optimize
intra-level cross-modality features. To clarify the fuzzy boundary, edge-guided
decoder achieves inter-level cross-modality fusion under the guidance of edge
features. The proposed model outperforms the state-of-the-art models on RGB-D
and RGB-T datasets, showing that it provides more insight into the
cross-modality complementarity task.",https://github.com/liuzywen/SwinNet,5263
Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning,0.720484,"Recent research shows synthetic data as a source of supervision helps
pretrained language models (PLM) transfer learning to new target tasks/domains.
However, this idea is less explored for spatial language. We provide two new
data resources on multiple spatial language processing tasks. The first dataset
is synthesized for transfer learning on spatial question answering (SQA) and
spatial role labeling (SpRL). Compared to previous SQA datasets, we include a
larger variety of spatial relation types and spatial expressions. Our data
generation process is easily extendable with new spatial expression lexicons.
The second one is a real-world SQA dataset with human-generated questions built
on an existing corpus with SPRL annotations. This dataset can be used to
evaluate spatial language processing models in realistic situations. We show
pretraining with automatically generated data significantly improves the SOTA
results on several SQA and SPRL benchmarks, particularly when the training data
in the target domain is small.",https://github.com/HLR/SpaRTUN,-1
Region-Aware Face Swapping,0.731105,"This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to
achieve identity-consistent harmonious high-resolution face generation in a
local-global manner: \textbf{1)} Local Facial Region-Aware (FRA) branch
augments local identity-relevant features by introducing the Transformer to
effectively model misaligned cross-scale semantic interaction. \textbf{2)}
Global Source Feature-Adaptive (SFA) branch further complements global
identity-relevant cues for generating identity-consistent swapped faces.
Besides, we propose a \textit{Face Mask Predictor} (FMP) module incorporated
with StyleGAN2 to predict identity-relevant soft facial masks in an
unsupervised manner that is more practical for generating harmonious
high-resolution faces. Abundant experiments qualitatively and quantitatively
demonstrate the superiority of our method for generating more
identity-consistent high-resolution swapped faces over SOTA methods, \eg,
obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\uparrow$.",None,6531
ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework,0.86921,"In this paper, a computation efficient regression framework is presented for
estimating the 6D pose of rigid objects from a single RGB-D image, which is
applicable to handling symmetric objects. This framework is designed in a
simple architecture that efficiently extracts point-wise features from RGB-D
data using a fully convolutional network, called XYZNet, and directly regresses
the 6D pose without any post refinement. In the case of symmetric object, one
object has multiple ground-truth poses, and this one-to-many relationship may
lead to estimation ambiguity. In order to solve this ambiguity problem, we
design a symmetry-invariant pose distance metric, called average (maximum)
grouped primitives distance or A(M)GPD. The proposed A(M)GPD loss can make the
regression network converge to the correct state, i.e., all minima in the
A(M)GPD loss surface are mapped to the correct poses. Extensive experiments on
YCB-Video and T-LESS datasets demonstrate the proposed framework's
substantially superior performance in top accuracy and low computational cost.",https://github.com/GANWANSHUI/ES6D.git,12636
Extreme Masking for Learning Instance and Distributed Visual Representations,0.524127,"The paper presents a scalable approach for learning spatially distributed
visual representations over individual tokens and a holistic instance
representation simultaneously. We use self-attention blocks to represent
spatially distributed tokens, followed by cross-attention blocks to aggregate
the holistic image instance. The core of the approach is the use of extremely
large token masking (75\%-90\%) as the data augmentation for supervision. Our
model, named ExtreMA, follows the plain BYOL approach where the instance
representation from the unmasked subset is trained to predict that from the
intact input. Instead of encouraging invariance across inputs, the model is
required to capture informative variations in an image. The paper makes three
contributions: 1) It presents random masking as a strong and computationally
efficient data augmentation for siamese representation learning. 2) With
multiple sampling per instance, extreme masking greatly speeds up learning and
improves performance with more data. 3) ExtreMA obtains stronger linear probing
performance than masked modeling methods, and better transfer performance than
prior contrastive models.",None,-1
To Answer or Not to Answer? Improving Machine Reading Comprehension Model with Span-based Contrastive Learning,0.716549,"Machine Reading Comprehension with Unanswerable Questions is a difficult NLP
task, challenged by the questions which can not be answered from passages. It
is observed that subtle literal changes often make an answerable question
unanswerable, however, most MRC models fail to recognize such changes. To
address this problem, in this paper, we propose a span-based method of
Contrastive Learning (spanCL) which explicitly contrast answerable questions
with their answerable and unanswerable counterparts at the answer span level.
With spanCL, MRC models are forced to perceive crucial semantic changes from
slight literal differences. Experiments on SQuAD 2.0 dataset show that spanCL
can improve baselines significantly, yielding 0.86-2.14 absolute EM
improvements. Additional experiments also show that spanCL is an effective way
to utilize generated questions.",https://github.com/explosion/spaCy,-1
A two-step approach to leverage contextual data: speech recognition in air-traffic communications,0.469055,"Automatic Speech Recognition (ASR), as the assistance of speech communication
between pilots and air-traffic controllers, can significantly reduce the
complexity of the task and increase the reliability of transmitted information.
ASR application can lead to a lower number of incidents caused by
misunderstanding and improve air traffic management (ATM) efficiency.
Evidently, high accuracy predictions, especially, of key information, i.e.,
callsigns and commands, are required to minimize the risk of errors. We prove
that combining the benefits of ASR and Natural Language Processing (NLP)
methods to make use of surveillance data (i.e. additional modality) helps to
considerably improve the recognition of callsigns (named entity). In this
paper, we investigate a two-step callsign boosting approach: (1) at the 1 step
(ASR), weights of probable callsign n-grams are reduced in G.fst and/or in the
decoding FST (lattices), (2) at the 2 step (NLP), callsigns extracted from the
improved recognition outputs with Named Entity Recognition (NER) are correlated
with the surveillance data to select the most suitable one. Boosting callsign
n-grams with the combination of ASR and NLP methods eventually leads up to
53.7% of an absolute, or 60.4% of a relative, improvement in callsign
recognition.",None,10700
Video Prediction by Efficient Transformers,0.830643,"Video prediction is a challenging computer vision task that has a wide range
of applications. In this work, we present a new family of Transformer-based
models for video prediction. Firstly, an efficient local spatial-temporal
separation attention mechanism is proposed to reduce the complexity of standard
Transformers. Then, a full autoregressive model, a partial autoregressive model
and a non-autoregressive model are developed based on the new efficient
Transformer. The partial autoregressive model has a similar performance with
the full autoregressive model but a faster inference speed. The
non-autoregressive model not only achieves a faster inference speed but also
mitigates the quality degradation problem of the autoregressive counterparts,
but it requires additional parameters and loss function for learning. Given the
same attention mechanism, we conducted a comprehensive study to compare the
proposed three video prediction variants. Experiments show that the proposed
video prediction models are competitive with more complex state-of-the-art
convolutional-LSTM based models. The source code is available at
https://github.com/XiYe20/VPTR.",https://github.com/XiYe20/VPTR,-1
Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge,0.39421,"We propose a novel open-domain question answering (ODQA) framework for
answering single/multi-hop questions across heterogeneous knowledge sources.
The key novelty of our method is the introduction of the intermediary modules
into the current retriever-reader pipeline. Unlike previous methods that solely
rely on the retriever for gathering all evidence in isolation, our intermediary
performs a chain of reasoning over the retrieved set. Specifically, our method
links the retrieved evidence with its related global context into graphs and
organizes them into a candidate list of evidence chains. Built upon pretrained
language models, our system achieves competitive performance on two ODQA
datasets, OTT-QA and NQ, against tables and passages from Wikipedia. In
particular, our model substantially outperforms the previous state-of-the-art
on OTT-QA with an exact match score of 47.3 (45 % relative gain).",https://github.com/Mayer123/UDT-QA,-1
Exploring Hybrid and Ensemble Models for Multiclass Prediction of Mental Health Status on Social Media,0.206983,"In recent years, there has been a surge of interest in research on automatic
mental health detection (MHD) from social media data leveraging advances in
natural language processing and machine learning techniques. While significant
progress has been achieved in this interdisciplinary research area, the vast
majority of work has treated MHD as a binary classification task. The
multiclass classification setup is, however, essential if we are to uncover the
subtle differences among the statistical patterns of language use associated
with particular mental health conditions. Here, we report on experiments aimed
at predicting six conditions (anxiety, attention deficit hyperactivity
disorder, bipolar disorder, post-traumatic stress disorder, depression, and
psychological stress) from Reddit social media posts. We explore and compare
the performance of hybrid and ensemble models leveraging transformer-based
architectures (BERT and RoBERTa) and BiLSTM neural networks trained on
within-text distributions of a diverse set of linguistic features. This set
encompasses measures of syntactic complexity, lexical sophistication and
diversity, readability, and register-specific ngram frequencies, as well as
sentiment and emotion lexicons. In addition, we conduct feature ablation
experiments to investigate which types of features are most indicative of
particular mental health conditions.",None,-1
Neural Activation Patterns (NAPs): Visual Explainability of Learned Concepts,0.38801,"A key to deciphering the inner workings of neural networks is understanding
what a model has learned. Promising methods for discovering learned features
are based on analyzing activation values, whereby current techniques focus on
analyzing high activation values to reveal interesting features on a neuron
level. However, analyzing high activation values limits layer-level concept
discovery. We present a method that instead takes into account the entire
activation distribution. By extracting similar activation profiles within the
high-dimensional activation space of a neural network layer, we find groups of
inputs that are treated similarly. These input groups represent neural
activation patterns (NAPs) and can be used to visualize and interpret learned
layer concepts. We release a framework with which NAPs can be extracted from
pre-trained models and provide a visual introspection tool that can be used to
analyze NAPs. We tested our method with a variety of networks and show how it
complements existing methods for analyzing neural network activation values.",None,-1
EAutoDet: Efficient Architecture Search for Object Detection,0.58587,"Training CNN for detection is time-consuming due to the large dataset and
complex network modules, making it hard to search architectures on detection
datasets directly, which usually requires vast search costs (usually tens and
even hundreds of GPU-days). In contrast, this paper introduces an efficient
framework, named EAutoDet, that can discover practical backbone and FPN
architectures for object detection in 1.4 GPU-days. Specifically, we construct
a supernet for both backbone and FPN modules and adopt the differentiable
method. To reduce the GPU memory requirement and computational cost, we propose
a kernel reusing technique by sharing the weights of candidate operations on
one edge and consolidating them into one convolution. A dynamic channel
refinement strategy is also introduced to search channel numbers. Extensive
experiments show significant efficacy and efficiency of our method. In
particular, the discovered architectures surpass state-of-the-art object
detection NAS methods and achieve 40.1 mAP with 120 FPS and 49.2 mAP with 41.3
FPS on COCO test-dev set. We also transfer the discovered architectures to
rotation detection task, which achieve 77.05 mAP$_{\text{50}}$ on DOTA-v1.0
test set with 21.1M parameters.",None,-1
Adversarially-Aware Robust Object Detector,0.641784,"Object detection, as a fundamental computer vision task, has achieved a
remarkable progress with the emergence of deep neural networks. Nevertheless,
few works explore the adversarial robustness of object detectors to resist
adversarial attacks for practical applications in various real-world scenarios.
Detectors have been greatly challenged by unnoticeable perturbation, with sharp
performance drop on clean images and extremely poor performance on adversarial
images. In this work, we empirically explore the model training for adversarial
robustness in object detection, which greatly attributes to the conflict
between learning clean images and adversarial images. To mitigate this issue,
we propose a Robust Detector (RobustDet) based on adversarially-aware
convolution to disentangle gradients for model learning on clean and
adversarial images. RobustDet also employs the Adversarial Image Discriminator
(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable
robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that
our model effectively disentangles gradients and significantly enhances the
detection robustness with maintaining the detection ability on clean images.",https://github.com/7eu7d7/RobustDet,-1
MMES: Mixture Model based Evolution Strategy for Large-Scale Optimization,0.371766,"This work provides an efficient sampling method for the covariance matrix
adaptation evolution strategy (CMA-ES) in large-scale settings. In contract to
the Gaussian sampling in CMA-ES, the proposed method generates mutation vectors
from a mixture model, which facilitates exploiting the rich variable
correlations of the problem landscape within a limited time budget. We analyze
the probability distribution of this mixture model and show that it
approximates the Gaussian distribution of CMA-ES with a controllable accuracy.
We use this sampling method, coupled with a novel method for mutation strength
adaptation, to formulate the mixture model based evolution strategy (MMES) -- a
CMA-ES variant for large-scale optimization. The numerical simulations show
that, while significantly reducing the time complexity of CMA-ES, MMES
preserves the rotational invariance, is scalable to high dimensional problems,
and is competitive against the state-of-the-arts in performing global
optimization.",https://github.com/hxyokokok/MMES,-1
ITTR: Unpaired Image-to-Image Translation with Transformers,0.569248,"Unpaired image-to-image translation is to translate an image from a source
domain to a target domain without paired training data. By utilizing CNN in
extracting local semantics, various techniques have been developed to improve
the translation performance. However, CNN-based generators lack the ability to
capture long-range dependency to well exploit global semantics. Recently,
Vision Transformers have been widely investigated for recognition tasks. Though
appealing, it is inappropriate to simply transfer a recognition-based vision
transformer to image-to-image translation due to the generation difficulty and
the computation limitation. In this paper, we propose an effective and
efficient architecture for unpaired Image-to-Image Translation with
Transformers (ITTR). It has two main designs: 1) hybrid perception block (HPB)
for token mixing from different receptive fields to utilize global semantics;
2) dual pruned self-attention (DPSA) to sharply reduce the computational
complexity. Our ITTR outperforms the state-of-the-arts for unpaired
image-to-image translation on six benchmark datasets.",None,-1
Norm of Word Embedding Encodes Information Gain,0.250673,"Distributed representations of words encode lexical semantic information, but
what type of information is encoded and how? Focusing on the skip-gram with
negative-sampling method, we found that the squared norm of static word
embedding encodes the information gain conveyed by the word; the information
gain is defined by the Kullback-Leibler divergence of the co-occurrence
distribution of the word to the unigram distribution. Our findings are
explained by the theoretical framework of the exponential family of probability
distributions and confirmed through precise experiments that remove spurious
correlations arising from word frequency. This theory also extends to
contextualized word embeddings in language models or any neural networks with
the softmax output layer. We also demonstrate that both the KL divergence and
the squared norm of embedding provide a useful metric of the informativeness of
a word in tasks such as keyword extraction, proper-noun discrimination, and
hypernym discrimination.",None,-1
"Deep Insights of Learning based Micro Expression Recognition: A Perspective on Promises, Challenges and Research Needs",0.547467,"Micro expression recognition (MER) is a very challenging area of research due
to its intrinsic nature and fine-grained changes. In the literature, the
problem of MER has been solved through handcrafted/descriptor-based techniques.
However, in recent times, deep learning (DL) based techniques have been adopted
to gain higher performance for MER. Also, rich survey articles on MER are
available by summarizing the datasets, experimental settings, conventional and
deep learning methods. In contrast, these studies lack the ability to convey
the impact of network design paradigms and experimental setting strategies for
DL-based MER. Therefore, this paper aims to provide a deep insight into the
DL-based MER frameworks with a perspective on promises in network model
designing, experimental strategies, challenges, and research needs. Also, the
detailed categorization of available MER frameworks is presented in various
aspects of model design and technical characteristics. Moreover, an empirical
analysis of the experimental and validation protocols adopted by MER methods is
presented. The challenges mentioned earlier and network design strategies may
assist the affective computing research community in forging ahead in MER
research. Finally, we point out the future directions, research needs, and draw
our conclusions.",None,-1
Spatiotemporal Costmap Inference for MPC via Deep Inverse Reinforcement Learning,0.777634,"It can be difficult to autonomously produce driver behavior so that it
appears natural to other traffic participants. Through Inverse Reinforcement
Learning (IRL), we can automate this process by learning the underlying reward
function from human demonstrations. We propose a new IRL algorithm that learns
a goal-conditioned spatiotemporal reward function. The resulting costmap is
used by Model Predictive Controllers (MPCs) to perform a task without any
hand-designing or hand-tuning of the cost function. We evaluate our proposed
Goal-conditioned SpatioTemporal Zeroing Maximum Entropy Deep IRL (GSTZ)-MEDIRL
framework together with MPC in the CARLA simulator for autonomous driving, lane
keeping, and lane changing tasks in a challenging dense traffic highway
scenario. Our proposed methods show higher success rates compared to other
baseline methods including behavior cloning, state-of-the-art RL policies, and
MPC with a learning-based behavior prediction model.",None,-1
On the Super-exponential Quantum Speedup of Equivariant Quantum Machine Learning Algorithms with SU($d$) Symmetry,0.822274,"We introduce a framework of the equivariant convolutional algorithms which is
tailored for a number of machine-learning tasks on physical systems with
arbitrary SU($d$) symmetries. It allows us to enhance a natural model of
quantum computation--permutational quantum computing (PQC) [Quantum Inf.
Comput., 10, 470-497 (2010)] --and defines a more powerful model: PQC+. While
PQC was shown to be effectively classically simulatable, we exhibit a problem
which can be efficiently solved on PQC+ machine, whereas the best known
classical algorithms runs in $O(n!n^2)$ time, thus providing strong evidence
against PQC+ being classically simulatable. We further discuss practical
quantum machine learning algorithms which can be carried out in the paradigm of
PQC+.",None,13616
A Mixed Integer Programming Approach for Verifying Properties of Binarized Neural Networks,0.1501,"Many approaches for verifying input-output properties of neural networks have
been proposed recently. However, existing algorithms do not scale well to large
networks. Recent work in the field of model compression studied binarized
neural networks (BNNs), whose parameters and activations are binary. BNNs tend
to exhibit a slight decrease in performance compared to their full-precision
counterparts, but they can be easier to verify. This paper proposes a simple
mixed integer programming formulation for BNN verification that leverages
network structure. We demonstrate our approach by verifying properties of BNNs
trained on the MNIST dataset and an aircraft collision avoidance controller. We
compare the runtime of our approach against state-of-the-art verification
algorithms for full-precision neural networks. The results suggest that the
difficulty of training BNNs might be worth the reduction in runtime achieved by
our verification algorithm.",None,-1
Learning Local Displacements for Point Cloud Completion,0.684097,"We propose a novel approach aimed at object and semantic scene completion
from a partial scan represented as a 3D point cloud. Our architecture relies on
three novel layers that are used successively within an encoder-decoder
structure and specifically developed for the task at hand. The first one
carries out feature extraction by matching the point features to a set of
pre-trained local descriptors. Then, to avoid losing individual descriptors as
part of standard operations such as max-pooling, we propose an alternative
neighbor-pooling operation that relies on adopting the feature vectors with the
highest activations. Finally, up-sampling in the decoder modifies our feature
extraction in order to increase the output dimension. While this model is
already able to achieve competitive results with the state of the art, we
further propose a way to increase the versatility of our approach to process
point clouds. To this aim, we introduce a second model that assembles our
layers within a transformer architecture. We evaluate both architectures on
object and indoor scene completion tasks, achieving state-of-the-art
performance.",None,-1
Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion,0.420609,"Automatic discourse processing is bottlenecked by data: current discourse
formalisms pose highly demanding annotation tasks involving large taxonomies of
discourse relations, making them inaccessible to lay annotators. This work
instead adopts the linguistic framework of Questions Under Discussion (QUD) for
discourse analysis and seeks to derive QUD structures automatically. QUD views
each sentence as an answer to a question triggered in prior context; thus, we
characterize relationships between sentences as free-form questions, in
contrast to exhaustive fine-grained taxonomies. We develop the
first-of-its-kind QUD parser that derives a dependency structure of questions
over full documents, trained using a large, crowdsourced question-answering
dataset DCQA (Ko et al., 2022). Human evaluation results show that QUD
dependency parsing is possible for language models trained with this
crowdsourced, generalizable annotation scheme. We illustrate how our QUD
structure is distinct from RST trees, and demonstrate the utility of QUD
analysis in the context of document simplification. Our findings show that QUD
parsing is an appealing alternative for automatic discourse processing.",https://github.com/wjko/discourse-qa,-1
Robust Speech Recognition via Large-Scale Weak Supervision,1.0,"We study the capabilities of speech processing systems trained simply to
predict large amounts of transcripts of audio on the internet. When scaled to
680,000 hours of multilingual and multitask supervision, the resulting models
generalize well to standard benchmarks and are often competitive with prior
fully supervised results but in a zero-shot transfer setting without the need
for any fine-tuning. When compared to humans, the models approach their
accuracy and robustness. We are releasing models and inference code to serve as
a foundation for further work on robust speech processing.",https://github.com/openai/whisper,-1
Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models,0.739619,"Self-supervised learning (SSL) is seen as a very promising approach with high
performance for several speech downstream tasks. Since the parameters of SSL
models are generally so large that training and inference require a lot of
memory and computational cost, it is desirable to produce compact SSL models
without a significant performance degradation by applying compression methods
such as knowledge distillation (KD). Although the KD approach is able to shrink
the depth and/or width of SSL model structures, there has been little research
on how varying the depth and width impacts the internal representation of the
small-footprint model. This paper provides an empirical study that addresses
the question. We investigate the performance on SUPERB while varying the
structure and KD methods so as to keep the number of parameters constant; this
allows us to analyze the contribution of the representation introduced by
varying the model architecture. Experiments demonstrate that a certain depth is
essential for solving content-oriented tasks (e.g. automatic speech
recognition) accurately, whereas a certain width is necessary for achieving
high performance on several speaker-oriented tasks (e.g. speaker
identification). Based on these observations, we identify, for SUPERB, a more
compressed model with better performance than previous studies.",https://github.com/s3prl/s3prl,-1
Some Stylometric Remarks on Ovid's Heroides and the Epistula Sapphus,0.152375,"This article aims to contribute to two well-worn areas of debate in classical
Latin philology, relating to Ovid's Heroides. The first is the question of the
authenticity (and, to a lesser extent the correct position) of the letter
placed fifteenth by almost every editor -- the so-called Epistula Sapphus
(henceforth ES). The secondary question, although perhaps now less fervently
debated, is the authenticity of the 'Double Heroides', placed by those who
accept them as letters 16-21. I employ a variety of methods drawn from the
domain of computational stylometry to consider the poetics and the
lexico-grammatical features of these elegiac poems in the broader context of a
corpus of 'shorter' (from 20 to 546 lines) elegiac works from five authors (266
poems in all) comprising more or less all of the non-fragmentary classical
corpus. Based on a variety of techniques, every measure gives clear indication
that the poetic style of the Heroides is Ovidian, but distinctive; they can be
accurately isolated from Ovid more broadly. The Single and Double Heroides
split into two clear groups, with the ES grouped consistently with the single
letters. Furthermore, by comparing the style of the letters with the 'early'
(although there are complications in this label) works of the Amores and the
late works of the Ex Ponto, the evidence supports sequential composition --
meaning that the ES is correctly placed -- and, further, supports the growing
consensus that the double letters were composed significantly later, in exile.",https://github.com/bnagy/heroides-paper,-1
Multi-sensor large-scale dataset for multi-view 3D reconstruction,0.293736,"We present a new multi-sensor dataset for multi-view 3D surface
reconstruction. It includes registered RGB and depth data from sensors of
different resolutions and modalities: smartphones, Intel RealSense, Microsoft
Kinect, industrial cameras, and structured-light scanner. The scenes are
selected to emphasize a diverse set of material properties challenging for
existing algorithms. We provide around 1.4 million images of 107 different
scenes acquired from 100 viewing directions under 14 lighting conditions. We
expect our dataset will be useful for evaluation and training of 3D
reconstruction algorithms and for related tasks. The dataset is available at
skoltech3d.appliedai.tech.",None,15328
Interactive Sketching of Mannequin Poses,0.108368,"It can be easy and even fun to sketch humans in different poses. In contrast,
creating those same poses on a 3D graphics ""mannequin"" is comparatively
tedious. Yet 3D body poses are necessary for various downstream applications.
We seek to preserve the convenience of 2D sketching while giving users of
different skill levels the flexibility to accurately and more quickly
pose\slash refine a 3D mannequin.
  At the core of the interactive system, we propose a machine-learning model
for inferring the 3D pose of a CG mannequin from sketches of humans drawn in a
cylinder-person style. Training such a model is challenging because of artist
variability, a lack of sketch training data with corresponding ground truth 3D
poses, and the high dimensionality of human pose-space. Our unique approach to
synthesizing vector graphics training data underpins our integrated
ML-and-kinematics system. We validate the system by tightly coupling it with a
user interface, and by performing a user study, in addition to quantitative
comparisons.",None,-1
Localized Vision-Language Matching for Open-vocabulary Object Detection,0.614434,"In this work, we propose an open-vocabulary object detection method that,
based on image-caption pairs, learns to detect novel object classes along with
a given set of known classes. It is a two-stage training approach that first
uses a location-guided image-caption matching technique to learn class labels
for both novel and known classes in a weakly-supervised manner and second
specializes the model for the object detection task using known class
annotations. We show that a simple language model fits better than a large
contextualized language model for detecting novel objects. Moreover, we
introduce a consistency-regularization technique to better exploit
image-caption pair information. Our method compares favorably to existing
open-vocabulary detection approaches while being data-efficient. Source code is
available at https://github.com/lmb-freiburg/locov .",https://github.com/lmb-freiburg/locov,-1
Coupled Iterative Refinement for 6D Multi-Object Pose Estimation,0.961303,"We address the task of 6D multi-object pose: given a set of known 3D objects
and an RGB or RGB-D input image, we detect and estimate the 6D pose of each
object. We propose a new approach to 6D object pose estimation which consists
of an end-to-end differentiable architecture that makes use of geometric
knowledge. Our approach iteratively refines both pose and correspondence in a
tightly coupled manner, allowing us to dynamically remove outliers to improve
accuracy. We use a novel differentiable layer to perform pose refinement by
solving an optimization problem we refer to as Bidirectional Depth-Augmented
Perspective-N-Point (BD-PnP). Our method achieves state-of-the-art accuracy on
standard 6D Object Pose benchmarks. Code is available at
https://github.com/princeton-vl/Coupled-Iterative-Refinement.",None,-1
Sparse Conditional Hidden Markov Model for Weakly Supervised Named Entity Recognition,0.650643,"Weakly supervised named entity recognition methods train label models to
aggregate the token annotations of multiple noisy labeling functions (LFs)
without seeing any manually annotated labels. To work well, the label model
needs to contextually identify and emphasize well-performed LFs while
down-weighting the under-performers. However, evaluating the LFs is challenging
due to the lack of ground truths. To address this issue, we propose the sparse
conditional hidden Markov model (Sparse-CHMM). Instead of predicting the entire
emission matrix as other HMM-based methods, Sparse-CHMM focuses on estimating
its diagonal elements, which are considered as the reliability scores of the
LFs. The sparse scores are then expanded to the full-fledged emission matrix
with pre-defined expansion functions. We also augment the emission with
weighted XOR scores, which track the probabilities of an LF observing incorrect
entities. Sparse-CHMM is optimized through unsupervised learning with a
three-stage training pipeline that reduces the training difficulty and prevents
the model from falling into local optima. Compared with the baselines in the
Wrench benchmark, Sparse-CHMM achieves a 3.01 average F1 score improvement on
five comprehensive datasets. Experiments show that each component of
Sparse-CHMM is effective, and the estimated LF reliabilities strongly correlate
with true LF F1 scores.",https://github.com/Yinghao-Li/Sparse-CHMM,-1
D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning,0.0952773,"While combining imitation learning (IL) and reinforcement learning (RL) is a
promising way to address poor sample efficiency in autonomous behavior
acquisition, methods that do so typically assume that the requisite behavior
demonstrations are provided by an expert that behaves optimally with respect to
a task reward. If, however, suboptimal demonstrations are provided, a
fundamental challenge appears in that the demonstration-matching objective of
IL conflicts with the return-maximization objective of RL. This paper
introduces D-Shape, a new method for combining IL and RL that uses ideas from
reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape
allows learning from suboptimal demonstrations while retaining the ability to
find the optimal policy with respect to the task reward. We experimentally
validate D-Shape in sparse-reward gridworld domains, showing that it both
improves over RL in terms of sample efficiency and converges consistently to
the optimal policy in the presence of suboptimal demonstrations.",None,-1
A Deep Neural Network for Multiclass Bridge Element Parsing in Inspection Image Analysis,0.4307,"Aerial robots such as drones have been leveraged to perform bridge
inspections. Inspection images with both recognizable structural elements and
apparent surface defects can be collected by onboard cameras to provide
valuable information for the condition assessment. This article aims to
determine a suitable deep neural network (DNN) for parsing multiclass bridge
elements in inspection images. An extensive set of quantitative evaluations
along with qualitative examples show that High-Resolution Net (HRNet) possesses
the desired ability. With data augmentation and a training sample of 130
images, a pre-trained HRNet is efficiently transferred to the task of
structural element parsing and has achieved a 92.67% mean F1-score and 86.33%
mean IoU.",None,-1
VRKitchen2.0-IndoorKit: A Tutorial for Augmented Indoor Scene Building in Omniverse,0.206621,"With the recent progress of simulations by 3D modeling software and game
engines, many researchers have focused on Embodied AI tasks in the virtual
environment. However, the research community lacks a platform that can easily
serve both indoor scene synthesis and model benchmarking with various
algorithms. Meanwhile, computer graphics-related tasks need a toolkit for
implementing advanced synthesizing techniques. To facilitate the study of
indoor scene building methods and their potential robotics applications, we
introduce INDOORKIT: a built-in toolkit for NVIDIA OMNIVERSE that provides
flexible pipelines for indoor scene building, scene randomizing, and animation
controls. Besides, combining Python coding in the animation software INDOORKIT
assists researchers in creating real-time training and controlling avatars and
robotics. The source code for this toolkit is available at
https://github.com/realvcla/VRKitchen2.0-Tutorial, and the tutorial along with
the toolkit is available at https://vrkitchen20-tutorial.readthedocs.io/en/",https://github.com/realvcla/VRKitchen2.0-Tutorial,-1
Enhancing Document-level Relation Extraction by Entity Knowledge Injection,0.490073,"Document-level relation extraction (RE) aims to identify the relations
between entities throughout an entire document. It needs complex reasoning
skills to synthesize various knowledge such as coreferences and commonsense.
Large-scale knowledge graphs (KGs) contain a wealth of real-world facts, and
can provide valuable knowledge to document-level RE. In this paper, we propose
an entity knowledge injection framework to enhance current document-level RE
models. Specifically, we introduce coreference distillation to inject
coreference knowledge, endowing an RE model with the more general capability of
coreference reasoning. We also employ representation reconciliation to inject
factual knowledge and aggregate KG representations and document representations
into a unified space. The experiments on two benchmark datasets validate the
generalization of our entity knowledge injection framework and the consistent
improvement to several document-level RE models.",https://github.com/nju-websoft/KIRE,-1
TAPE: Task-Agnostic Prior Embedding for Image Restoration,0.746813,"Learning a generalized prior for natural image restoration is an important
yet challenging task. Early methods mostly involved handcrafted priors
including normalized sparsity, l_0 gradients, dark channel priors, etc.
Recently, deep neural networks have been used to learn various image priors but
do not guarantee to generalize. In this paper, we propose a novel approach that
embeds a task-agnostic prior into a transformer. Our approach, named
Task-Agnostic Prior Embedding (TAPE), consists of two stages, namely,
task-agnostic pre-training and task-specific fine-tuning, where the first stage
embeds prior knowledge about natural images into the transformer and the second
stage extracts the knowledge to assist downstream image restoration.
Experiments on various types of degradation validate the effectiveness of TAPE.
The image restoration performance in terms of PSNR is improved by as much as
1.45dB and even outperforms task-specific algorithms. More importantly, TAPE
shows the ability of disentangling generalized image priors from degraded
images, which enjoys favorable transfer ability to unknown downstream tasks.",None,-1
Optimizing Relevance Maps of Vision Transformers Improves Robustness,0.5879,"It has been observed that visual classification models often rely mostly on
the image background, neglecting the foreground, which hurts their robustness
to distribution changes. To alleviate this shortcoming, we propose to monitor
the model's relevancy signal and manipulate it such that the model is focused
on the foreground object. This is done as a finetuning step, involving
relatively few samples consisting of pairs of images and their associated
foreground masks. Specifically, we encourage the model's relevancy map (i) to
assign lower relevance to background regions, (ii) to consider as much
information as possible from the foreground, and (iii) we encourage the
decisions to have high confidence. When applied to Vision Transformer (ViT)
models, a marked improvement in robustness to domain shifts is observed.
Moreover, the foreground masks can be obtained automatically, from a
self-supervised variant of the ViT model itself; therefore no additional
supervision is required.",https://github.com/hila-chefer/RobustViT,-1
Designing Decision Support Systems for Emergency Response: Challenges and Opportunities,0.205838,"Designing effective emergency response management (ERM) systems to respond to
incidents such as road accidents is a major problem faced by communities. In
addition to responding to frequent incidents each day (about 240 million
emergency medical services calls and over 5 million road accidents in the US
each year), these systems also support response during natural hazards.
Recently, there has been a consistent interest in building decision support and
optimization tools that can help emergency responders provide more efficient
and effective response. This includes a number of principled subsystems that
implement early incident detection, incident likelihood forecasting and
strategic resource allocation and dispatch policies. In this paper, we
highlight the key challenges and provide an overview of the approach developed
by our team in collaboration with our community partners.",None,-1
Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention for Social-Text Classification,0.582176,"Social media has become the fulcrum of all forms of communication.
Classifying social texts such as fake news, rumour, sarcasm, etc. has gained
significant attention. The surface-level signals expressed by a social-text
itself may not be adequate for such tasks; therefore, recent methods attempted
to incorporate other intrinsic signals such as user behavior and the underlying
graph structure. Oftentimes, the `public wisdom' expressed through the
comments/replies to a social-text acts as a surrogate of crowd-sourced view and
may provide us with complementary signals. State-of-the-art methods on
social-text classification tend to ignore such a rich hierarchical signal.
Here, we propose Hyphen, a discourse-aware hyperbolic spectral co-attention
network. Hyphen is a fusion of hyperbolic graph representation learning with a
novel Fourier co-attention mechanism in an attempt to generalise the
social-text classification tasks by incorporating public discourse. We parse
public discourse as an Abstract Meaning Representation (AMR) graph and use the
powerful hyperbolic geometric representation to model graphs with hierarchical
structure. Finally, we equip it with a novel Fourier co-attention mechanism to
capture the correlation between the source post and public discourse. Extensive
experiments on four different social-text classification tasks, namely
detecting fake news, hate speech, rumour, and sarcasm, show that Hyphen
generalises well, and achieves state-of-the-art results on ten benchmark
datasets. We also employ a sentence-level fact-checked and annotated dataset to
evaluate how Hyphen is capable of producing explanations as analogous evidence
to the final prediction.",https://github.com/LCS2-IIITD/Hyphen,-1
SAFER: Safe Collision Avoidance using Focused and Efficient Trajectory Search with Reinforcement Learning,0.446559,"Collision avoidance is key for mobile robots and agents to operate safely in
the real world. In this work we present SAFER, an efficient and effective
collision avoidance system that is able to improve safety by correcting the
control commands sent by an operator. It combines real-world reinforcement
learning (RL), search-based online trajectory planning, and automatic emergency
intervention, e.g. automatic emergency braking (AEB). The goal of the RL is to
learn an effective corrective control action that is used in a focused search
for collision-free trajectories, and to reduce the frequency of triggering
automatic emergency braking. This novel setup enables the RL policy to learn
safely and directly on mobile robots in a real-world indoor environment,
minimizing actual crashes even during training. Our real-world experiments show
that, when compared with several baselines, our approach enjoys a higher
average speed, lower crash rate, less emergency intervention, smaller
computation overhead, and smoother overall control.",None,-1
Co-evolving morphology and control of soft robots using a single genome,0.793808,"When simulating soft robots, both their morphology and their controllers play
important roles in task performance. This paper introduces a new method to
co-evolve these two components in the same process. We do that by using the
hyperNEAT algorithm to generate two separate neural networks in one pass, one
responsible for the design of the robot body structure and the other for the
control of the robot.
  The key difference between our method and most existing approaches is that it
does not treat the development of the morphology and the controller as separate
processes. Similar to nature, our method derives both the ""brain"" and the
""body"" of an agent from a single genome and develops them together. While our
approach is more realistic and doesn't require an arbitrary separation of
processes during evolution, it also makes the problem more complex because the
search space for this single genome becomes larger and any mutation to the
genome affects ""brain"" and the ""body"" at the same time.
  Additionally, we present a new speciation function that takes into
consideration both the genotypic distance, as is the standard for NEAT, and the
similarity between robot bodies. By using this function, agents with very
different bodies are more likely to be in different species, this allows robots
with different morphologies to have more specialized controllers since they
won't crossover with other robots that are too different from them.
  We evaluate the presented methods on four tasks and observe that even if the
search space was larger, having a single genome makes the evolution process
converge faster when compared to having separated genomes for body and control.
The agents in our population also show morphologies with a high degree of
regularity and controllers capable of coordinating the voxels to produce the
necessary movements.",https://github.com/fhtanaka/SGR,-1
Just Another Method to Compute MTTF from Continuous Time Markov Chain,0.155793,"The Meantime to Failure is a statistic used to determine how much time a
system spends to enter one of its absorption states. This statistic can be used
in most areas of knowledge. In engineering, for example, can be used as a
measure of equipment reliability, and in business, as a measure of processes
performance. This work presents a method to obtain the Meantime to Failure from
a Continuous Time Markov Chain models. The method is intuitive and is simpler
to be implemented, since, it consists of solving a system of linear equations.",None,-1
Full-Text Argumentation Mining on Scientific Publications,0.476315,"Scholarly Argumentation Mining (SAM) has recently gained attention due to its
potential to help scholars with the rapid growth of published scientific
literature. It comprises two subtasks: argumentative discourse unit recognition
(ADUR) and argumentative relation extraction (ARE), both of which are
challenging since they require e.g. the integration of domain knowledge, the
detection of implicit statements, and the disambiguation of argument structure.
While previous work focused on dataset construction and baseline methods for
specific document sections, such as abstract or results, full-text scholarly
argumentation mining has seen little progress. In this work, we introduce a
sequential pipeline model combining ADUR and ARE for full-text SAM, and provide
a first analysis of the performance of pretrained language models (PLMs) on
both subtasks. We establish a new SotA for ADUR on the Sci-Arg corpus,
outperforming the previous best reported result by a large margin (+7% F1). We
also present the first results for ARE, and thus for the full AM pipeline, on
this benchmark dataset. Our detailed error analysis reveals that non-contiguous
ADUs as well as the interpretation of discourse connectors pose major
challenges and that data annotation needs to be more consistent.",https://github.com/DFKI-NLP/sam,-1
Segmentation Enhanced Lameness Detection in Dairy Cows from RGB and Depth Video,0.54291,"Cow lameness is a severe condition that affects the life cycle and life
quality of dairy cows and results in considerable economic losses. Early
lameness detection helps farmers address illnesses early and avoid negative
effects caused by the degeneration of cows' condition. We collected a dataset
of short clips of cows passing through a hallway exiting a milking station and
annotated the degree of lameness of the cows. This paper explores the resulting
dataset and provides a detailed description of the data collection process.
Additionally, we proposed a lameness detection method that leverages
pre-trained neural networks to extract discriminative features from videos and
assign a binary score to each cow indicating its condition: ""healthy"" or
""lame."" We improve this approach by forcing the model to focus on the structure
of the cow, which we achieve by substituting the RGB videos with binary
segmentation masks predicted with a trained segmentation model. This work aims
to encourage research and provide insights into the applicability of computer
vision models for cow lameness detection on farms.",None,-1
Exploring Transformer Backbones for Image Diffusion Models,0.115876,"We present an end-to-end Transformer based Latent Diffusion model for image
synthesis. On the ImageNet class conditioned generation task we show that a
Transformer based Latent Diffusion model achieves a 14.1FID which is comparable
to the 13.1FID score of a UNet based architecture. In addition to showing the
application of Transformer models for Diffusion based image synthesis this
simplification in architecture allows easy fusion and modeling of text and
image data. The multi-head attention mechanism of Transformers enables
simplified interaction between the image and text features which removes the
requirement for crossattention mechanism in UNet based Diffusion models.",None,-1
Self-Supervised Losses for One-Class Textual Anomaly Detection,0.273566,"Current deep learning methods for anomaly detection in text rely on
supervisory signals in inliers that may be unobtainable or bespoke
architectures that are difficult to tune. We study a simpler alternative:
fine-tuning Transformers on the inlier data with self-supervised objectives and
using the losses as an anomaly score. Overall, the self-supervision approach
outperforms other methods under various anomaly detection scenarios, improving
the AUROC score on semantic anomalies by 11.6% and on syntactic anomalies by
22.8% on average. Additionally, the optimal objective and resultant learnt
representation depend on the type of downstream anomaly. The separability of
anomalies and inliers signals that a representation is more effective for
detecting semantic anomalies, whilst the presence of narrow feature directions
signals a representation that is effective for detecting syntactic anomalies.",None,-1
Data Augmentation with Paraphrase Generation and Entity Extraction for Multimodal Dialogue System,0.412649,"Contextually aware intelligent agents are often required to understand the
users and their surroundings in real-time. Our goal is to build Artificial
Intelligence (AI) systems that can assist children in their learning process.
Within such complex frameworks, Spoken Dialogue Systems (SDS) are crucial
building blocks to handle efficient task-oriented communication with children
in game-based learning settings. We are working towards a multimodal dialogue
system for younger kids learning basic math concepts. Our focus is on improving
the Natural Language Understanding (NLU) module of the task-oriented SDS
pipeline with limited datasets. This work explores the potential benefits of
data augmentation with paraphrase generation for the NLU models trained on
small task-specific datasets. We also investigate the effects of extracting
entities for conceivably further data expansion. We have shown that
paraphrasing with model-in-the-loop (MITL) strategies using small seed data is
a promising approach yielding improved performance results for the Intent
Recognition task.",None,-1
NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,0.849796,"Given the ubiquitous nature of numbers in text, reasoning with numbers to
perform simple calculations is an important skill of AI systems. While many
datasets and models have been developed to this end, state-of-the-art AI
systems are brittle; failing to perform the underlying mathematical reasoning
when they appear in a slightly different scenario. Drawing inspiration from
GLUE that was proposed in the context of natural language understanding, we
propose NumGLUE, a multi-task benchmark that evaluates the performance of AI
systems on eight different tasks, that at their core require simple arithmetic
understanding. We show that this benchmark is far from being solved with neural
models including state-of-the-art large-scale language models performing
significantly worse than humans (lower by 46.4%). Further, NumGLUE promotes
sharing knowledge across tasks, especially those with limited training data as
evidenced by the superior performance (average gain of 3.4% on each task) when
a model is jointly trained on all the tasks as opposed to task-specific
modeling. Finally, we hope that NumGLUE will encourage systems that perform
robust and general arithmetic reasoning within language, a first step towards
being able to perform more complex mathematical reasoning.",None,-1
VSVC: Backdoor attack against Keyword Spotting based on Voiceprint Selection and Voice Conversion,0.17985,"Keyword spotting (KWS) based on deep neural networks (DNNs) has achieved
massive success in voice control scenarios. However, training of such DNN-based
KWS systems often requires significant data and hardware resources.
Manufacturers often entrust this process to a third-party platform. This makes
the training process uncontrollable, where attackers can implant backdoors in
the model by manipulating third-party training data. An effective backdoor
attack can force the model to make specified judgments under certain
conditions, i.e., triggers. In this paper, we design a backdoor attack scheme
based on Voiceprint Selection and Voice Conversion, abbreviated as VSVC.
Experimental results demonstrated that VSVC is feasible to achieve an average
attack success rate close to 97% in four victim models when poisoning less than
1% of the training data.",None,-1
"The $(1+(λ,λ))$ Global SEMO Algorithm",0.455936,"The $(1+(\lambda,\lambda))$ genetic algorithm is a recently proposed
single-objective evolutionary algorithm with several interesting properties. We
show that its main working principle, mutation with a high rate and crossover
as repair mechanism, can be transported also to multi-objective evolutionary
computation. We define the $(1+(\lambda,\lambda))$ global SEMO algorithm, a
variant of the classic global SEMO algorithm, and prove that it optimizes the
OneMinMax benchmark asymptotically faster than the global SEMO. Following the
single-objective example, we design a one-fifth rule inspired dynamic parameter
setting (to the best of our knowledge for the first time in discrete
multi-objective optimization) and prove that it further improves the runtime to
$O(n^2)$, whereas the best runtime guarantee for the global SEMO is only $O(n^2
\log n)$.",None,-1
An Effective Iterated Two-stage Heuristic Algorithm for the Multiple Traveling Salesmen Problem,0.515148,"The multiple Traveling Salesmen Problem (mTSP) is a general extension of the
famous NP-hard Traveling Salesmen Problem (TSP), that there are m (m > 1)
salesmen to visit the cities. In this paper, we address the mTSP with both the
minsum objective and minmax objective, which aims at minimizing the total
length of the $m$ tours and the length of the longest tour among all the m
tours, respectively. We propose an iterated two-stage heuristic algorithm
called ITSHA for the mTSP. Each iteration of ITSHA consists of an
initialization stage and an improvement stage. The initialization stage aims to
generate high-quality and diverse initial solutions. The improvement stage
mainly applies the variable neighborhood search (VNS) approach based on our
proposed effective local search neighborhoods to optimize the initial solution.
Moreover, some local optima escaping approaches are employed to enhance the
search ability of the algorithm. Extensive experimental results on a wide range
of public benchmark instances show that ITSHA significantly outperforms the
state-of-the-art heuristic algorithms in solving the mTSP on both the
objectives.",None,-1
Learning Smooth Neural Functions via Lipschitz Regularization,0.879769,"Neural implicit fields have recently emerged as a useful representation for
3D shapes. These fields are commonly represented as neural networks which map
latent descriptors and 3D coordinates to implicit function values. The latent
descriptor of a neural field acts as a deformation handle for the 3D shape it
represents. Thus, smoothness with respect to this descriptor is paramount for
performing shape-editing operations. In this work, we introduce a novel
regularization designed to encourage smooth latent spaces in neural fields by
penalizing the upper bound on the field's Lipschitz constant. Compared with
prior Lipschitz regularized networks, ours is computationally fast, can be
implemented in four lines of code, and requires minimal hyperparameter tuning
for geometric applications. We demonstrate the effectiveness of our approach on
shape interpolation and extrapolation as well as partial shape reconstruction
from 3D point clouds, showing both qualitative and quantitative improvements
over existing state-of-the-art and non-regularized baselines.",http://github.com/google/jax,-1
Learning to Reuse Distractors to support Multiple Choice Question Generation in Education,0.48607,"Multiple choice questions (MCQs) are widely used in digital learning systems,
as they allow for automating the assessment process. However, due to the
increased digital literacy of students and the advent of social media
platforms, MCQ tests are widely shared online, and teachers are continuously
challenged to create new questions, which is an expensive and time-consuming
task. A particularly sensitive aspect of MCQ creation is to devise relevant
distractors, i.e., wrong answers that are not easily identifiable as being
wrong. This paper studies how a large existing set of manually created answers
and distractors for questions over a variety of domains, subjects, and
languages can be leveraged to help teachers in creating new MCQs, by the smart
reuse of existing distractors. We built several data-driven models based on
context-aware question and distractor representations, and compared them with
static feature-based models. The proposed models are evaluated with automated
metrics and in a realistic user test with teachers. Both automatic and human
evaluations indicate that context-aware models consistently outperform a static
feature-based approach. For our best-performing context-aware model, on average
3 distractors out of the 10 shown to teachers were rated as high-quality
distractors. We create a performance benchmark, and make it public, to enable
comparison between different approaches and to introduce a more standardized
evaluation of the task. The benchmark contains a test of 298 educational
questions covering multiple subjects & languages and a 77k multilingual pool of
distractor vocabulary for future research.",https://github.com/semerekiros/dist-retrieval,-1
Abstract Interpretation on E-Graphs,0.156808,"Recent e-graph applications have typically considered concrete semantics of
expressions, where the notion of equivalence stems from concrete interpretation
of expressions. However, equivalences that hold over one interpretation may not
hold in an alternative interpretation. Such an observation can be exploited. We
consider the application of abstract interpretation to e-graphs, and show that
within an e-graph, the lattice meet operation associated with the abstract
domain has a natural interpretation for an e-class, leading to improved
precision in over-approximation. In this extended abstract, we use Interval
Arithmetic (IA) to illustrate this point.",None,-1
DKM: Dense Kernelized Feature Matching for Geometry Estimation,0.655523,"Feature matching is a challenging computer vision task that involves finding
correspondences between two images of a 3D scene. In this paper we consider the
dense approach instead of the more common sparse paradigm, thus striving to
find all correspondences. Perhaps counter-intuitively, dense methods have
previously shown inferior performance to their sparse and semi-sparse
counterparts for estimation of two-view geometry. This changes with our novel
dense method, which outperforms both dense and sparse methods on geometry
estimation. The novelty is threefold: First, we propose a kernel regression
global matcher. Secondly, we propose warp refinement through stacked feature
maps and depthwise convolution kernels. Thirdly, we propose learning dense
confidence through consistent depth and a balanced sampling approach for dense
confidence maps. Through extensive experiments we confirm that our proposed
dense method, \textbf{D}ense \textbf{K}ernelized Feature \textbf{M}atching,
sets a new state-of-the-art on multiple geometry estimation benchmarks. In
particular, we achieve an improvement on MegaDepth-1500 of +4.9 and +8.9
AUC$@5^{\circ}$ compared to the best previous sparse method and dense method
respectively. Our code is provided at https://github.com/Parskatt/dkm",https://github.com/Parskatt/dkm,29085
"Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2",0.884732,"The field of image synthesis has made great strides in the last couple of
years. Recent models are capable of generating images with astonishing quality.
Fine-grained evaluation of these models on some interesting categories such as
faces is still missing. Here, we conduct a quantitative comparison of three
popular systems including Stable Diffusion, Midjourney, and DALL-E 2 in their
ability to generate photorealistic faces in the wild. We find that Stable
Diffusion generates better faces than the other systems, according to the FID
score. We also introduce a dataset of generated faces in the wild dubbed GFW,
including a total of 15,076 faces. Furthermore, we hope that our study spurs
follow-up research in assessing the generative models and improving them. Data
and code are available at data and code, respectively.",None,-1
Automatic Evaluation and Analysis of Idioms in Neural Machine Translation,0.192132,"A major open problem in neural machine translation (NMT) is the translation
of idiomatic expressions, such as ""under the weather"". The meaning of these
expressions is not composed by the meaning of their constituent words, and NMT
models tend to translate them literally (i.e., word-by-word), which leads to
confusing and nonsensical translations. Research on idioms in NMT is limited
and obstructed by the absence of automatic methods for quantifying these
errors. In this work, first, we propose a novel metric for automatically
measuring the frequency of literal translation errors without human
involvement. Equipped with this metric, we present controlled translation
experiments with models trained in different conditions (with/without the
test-set idioms) and across a wide range of (global and targeted) metrics and
test sets. We explore the role of monolingual pretraining and find that it
yields substantial targeted improvements, even without observing any
translation examples of the test-set idioms. In our analysis, we probe the role
of idiom context. We find that the randomly initialized models are more local
or ""myopic"" as they are relatively unaffected by variations of the idiom
context, unlike the pretrained ones.",https://github.com/amazon-research/idiom-mt,1321
An Empirical Study on Cross-X Transfer for Legal Judgment Prediction,0.749591,"Cross-lingual transfer learning has proven useful in a variety of Natural
Language Processing (NLP) tasks, but it is understudied in the context of legal
NLP, and not at all in Legal Judgment Prediction (LJP). We explore transfer
learning techniques on LJP using the trilingual Swiss-Judgment-Prediction
dataset, including cases written in three languages. We find that cross-lingual
transfer improves the overall results across languages, especially when we use
adapter-based fine-tuning. Finally, we further improve the model's performance
by augmenting the training dataset with machine-translated versions of the
original documents, using a 3x larger training corpus. Further on, we perform
an analysis exploring the effect of cross-domain and cross-regional transfer,
i.e., train a model across domains (legal areas), or regions. We find that in
both settings (legal areas, origin regions), models trained across all groups
perform overall better, while they also have improved results in the worst-case
scenarios. Finally, we report improved results when we ambitiously apply
cross-jurisdiction transfer, where we further augment our dataset with Indian
legal cases.",https://github.com/UKPLab/EasyNMT,-1
Semantic Image Synthesis via Diffusion Models,0.905988,"Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable
success in various image generation tasks compared with Generative Adversarial
Nets (GANs). Recent work on semantic image synthesis mainly follows the
\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality
or diversity of generated images. In this paper, we propose a novel framework
based on DDPM for semantic image synthesis. Unlike previous conditional
diffusion model directly feeds the semantic layout and noisy image as input to
a U-Net structure, which may not fully leverage the information in the input
semantic mask, our framework processes semantic layout and noisy image
differently. It feeds noisy image to the encoder of the U-Net structure while
the semantic layout to the decoder by multi-layer spatially-adaptive
normalization operators. To further improve the generation quality and semantic
interpretability in semantic image synthesis, we introduce the classifier-free
guidance sampling strategy, which acknowledge the scores of an unconditional
model for sampling process. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our proposed method, achieving
state-of-the-art performance in terms of fidelity (FID) and diversity (LPIPS).",https://github.com/WeilunWang/semantic-diffusion-model,-1
Super forecasting the technological singularity risks from artificial intelligence,0.631679,"The article forecasts emerging cyber-risks from the integration of AI in
cybersecurity.",None,-1
Mixed Differential Privacy in Computer Vision,0.966163,"We introduce AdaMix, an adaptive differentially private algorithm for
training deep neural network classifiers using both private and public image
data. While pre-training language models on large public datasets has enabled
strong differential privacy (DP) guarantees with minor loss of accuracy, a
similar practice yields punishing trade-offs in vision tasks. A few-shot or
even zero-shot learning baseline that ignores private data can outperform
fine-tuning on a large private dataset. AdaMix incorporates few-shot training,
or cross-modal zero-shot learning, on public data prior to private fine-tuning,
to improve the trade-off. AdaMix reduces the error increase from the
non-private upper bound from the 167-311\% of the baseline, on average across 6
datasets, to 68-92\% depending on the desired privacy level selected by the
user. AdaMix tackles the trade-off arising in visual classification, whereby
the most privacy sensitive data, corresponding to isolated points in
representation space, are also critical for high classification accuracy. In
addition, AdaMix comes with strong theoretical privacy guarantees and
convergence analysis.",https://github.com/yuxiangw/autodp,-1
Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval,0.105521,"Contrastive learning (CL) has become a ubiquitous approach for several
natural language processing (NLP) downstream tasks, especially for question
answering (QA). However, the major challenge, how to efficiently train the
knowledge retrieval model in an unsupervised manner, is still unresolved.
Recently the commonly used methods are composed of CL and masked language model
(MLM). Unexpectedly, MLM ignores the sentence-level training, and CL also
neglects extraction of the internal info from the query. To optimize the CL
hardly obtain internal information from the original query, we introduce a
joint training method by combining CL and Auto-MLM for self-supervised
multi-lingual knowledge retrieval. First, we acquire the fixed dimensional
sentence vector. Then, mask some words among the original sentences with random
strategy. Finally, we generate a new token representation for predicting the
masked tokens. Experimental results show that our proposed approach
consistently outperforms all the previous SOTA methods on both AliExpress $\&$
LAZADA service corpus and openly available corpora in 8 languages.",https://github.com/taolei87/askubuntu,-1
Applying Automatic Text Summarization for Fake News Detection,0.562473,"The distribution of fake news is not a new but a rapidly growing problem. The
shift to news consumption via social media has been one of the drivers for the
spread of misleading and deliberately wrong information, as in addition to it
of easy use there is rarely any veracity monitoring. Due to the harmful effects
of such fake news on society, the detection of these has become increasingly
important. We present an approach to the problem that combines the power of
transformer-based language models while simultaneously addressing one of their
inherent problems. Our framework, CMTR-BERT, combines multiple text
representations, with the goal of circumventing sequential limits and related
loss of information the underlying transformer architecture typically suffers
from. Additionally, it enables the incorporation of contextual information.
Extensive experiments on two very different, publicly available datasets
demonstrates that our approach is able to set new state-of-the-art performance
benchmarks. Apart from the benefit of using automatic text summarization
techniques we also find that the incorporation of contextual information
contributes to performance gains.",https://github.com/phHartl/lrec_2022,-1
Contextually Enhanced ES-dRNN with Dynamic Attention for Short-Term Load Forecasting,0.414567,"In this paper, we propose a new short-term load forecasting (STLF) model
based on contextually enhanced hybrid and hierarchical architecture combining
exponential smoothing (ES) and a recurrent neural network (RNN). The model is
composed of two simultaneously trained tracks: the context track and the main
track. The context track introduces additional information to the main track.
It is extracted from representative series and dynamically modulated to adjust
to the individual series forecasted by the main track. The RNN architecture
consists of multiple recurrent layers stacked with hierarchical dilations and
equipped with recently proposed attentive dilated recurrent cells. These cells
enable the model to capture short-term, long-term and seasonal dependencies
across time series as well as to weight dynamically the input information. The
model produces both point forecasts and predictive intervals. The experimental
part of the work performed on 35 forecasting problems shows that the proposed
model outperforms in terms of accuracy its predecessor as well as standard
statistical models and state-of-the-art machine learning models.",https://github.com/slaweks17/ES-adRNN-with-Context,-1
Fine-grained Semantic Alignment Network for Weakly Supervised Temporal Language Grounding,0.573411,"Temporal language grounding (TLG) aims to localize a video segment in an
untrimmed video based on a natural language description. To alleviate the
expensive cost of manual annotations for temporal boundary labels, we are
dedicated to the weakly supervised setting, where only video-level descriptions
are provided for training. Most of the existing weakly supervised methods
generate a candidate segment set and learn cross-modal alignment through a
MIL-based framework. However, the temporal structure of the video as well as
the complicated semantics in the sentence are lost during the learning. In this
work, we propose a novel candidate-free framework: Fine-grained Semantic
Alignment Network (FSAN), for weakly supervised TLG. Instead of view the
sentence and candidate moments as a whole, FSAN learns token-by-clip
cross-modal semantic alignment by an iterative cross-modal interaction module,
generates a fine-grained cross-modal semantic alignment map, and performs
grounding directly on top of the map. Extensive experiments are conducted on
two widely-used benchmarks: ActivityNet-Captions, and DiDeMo, where our FSAN
achieves state-of-the-art performance.",None,-1
Multi-Vector Retrieval as Sparse Alignment,0.242062,"Multi-vector retrieval models improve over single-vector dual encoders on
many information retrieval tasks. In this paper, we cast the multi-vector
retrieval problem as sparse alignment between query and document tokens. We
propose AligneR, a novel multi-vector retrieval model that learns sparsified
pairwise alignments between query and document tokens (e.g. `dog' vs. `puppy')
and per-token unary saliences reflecting their relative importance for
retrieval. We show that controlling the sparsity of pairwise token alignments
often brings significant performance gains. While most factoid questions
focusing on a specific part of a document require a smaller number of
alignments, others requiring a broader understanding of a document favor a
larger number of alignments. Unary saliences, on the other hand, decide whether
a token ever needs to be aligned with others for retrieval (e.g. `kind' from
`kind of currency is used in new zealand}'). With sparsified unary saliences,
we are able to prune a large number of query and document token vectors and
improve the efficiency of multi-vector retrieval. We learn the sparse unary
saliences with entropy-regularized linear programming, which outperforms other
methods to achieve sparsity. In a zero-shot setting, AligneR scores 51.1 points
nDCG@10, achieving a new retriever-only state-of-the-art on 13 tasks in the
BEIR benchmark. In addition, adapting pairwise alignments with a few examples
(<= 8) further improves the performance up to 15.7 points nDCG@10 for argument
retrieval tasks. The unary saliences of AligneR helps us to keep only 20% of
the document token representations with minimal performance loss. We further
show that our model often produces interpretable alignments and significantly
improves its performance when initialized from larger language models.",None,-1
Asynchronous Actor-Critic for Multi-Agent Reinforcement Learning,0.692917,"Synchronizing decisions across multiple agents in realistic settings is
problematic since it requires agents to wait for other agents to terminate and
communicate about termination reliably. Ideally, agents should learn and
execute asynchronously instead. Such asynchronous methods also allow temporally
extended actions that can take different amounts of time based on the situation
and action executed. Unfortunately, current policy gradient methods are not
applicable in asynchronous settings, as they assume that agents synchronously
reason about action selection at every time step. To allow asynchronous
learning and decision-making, we formulate a set of asynchronous multi-agent
actor-critic methods that allow agents to directly optimize asynchronous
policies in three standard training paradigms: decentralized learning,
centralized learning, and centralized training for decentralized execution.
Empirical results (in simulation and hardware) in a variety of realistic
domains demonstrate the superiority of our approaches in large multi-agent
problems and validate the effectiveness of our algorithms for learning
high-quality and asynchronous solutions.",https://github.com/mgualti/PointCloudsPython,-1
Better Smatch = Better Parser? AMR evaluation is not so simple anymore,0.665558,"Recently, astonishing advances have been observed in AMR parsing, as measured
by the structural Smatch metric. In fact, today's systems achieve performance
levels that seem to surpass estimates of human inter annotator agreement (IAA).
Therefore, it is unclear how well Smatch (still) relates to human estimates of
parse quality, as in this situation potentially fine-grained errors of similar
weight may impact the AMR's meaning to different degrees.
  We conduct an analysis of two popular and strong AMR parsers that --
according to Smatch -- reach quality levels on par with human IAA, and assess
how human quality ratings relate to Smatch and other AMR metrics. Our main
findings are: i) While high Smatch scores indicate otherwise, we find that AMR
parsing is far from being solved: we frequently find structurally small, but
semantically unacceptable errors that substantially distort sentence meaning.
ii) Considering high-performance parsers, better Smatch scores may not
necessarily indicate consistently better parsing quality. To obtain a
meaningful and comprehensive assessment of quality differences of parse(r)s, we
recommend augmenting evaluations with macro statistics, use of additional
metrics, and more human analysis.",https://github.com/Heidelberg-nlp/AMRParseEval,-1
Characterizing the Action-Generalization Gap in Deep Q-Learning,0.0276883,"We study the action generalization ability of deep Q-learning in discrete
action spaces. Generalization is crucial for efficient reinforcement learning
(RL) because it allows agents to use knowledge learned from past experiences on
new tasks. But while function approximation provides deep RL agents with a
natural way to generalize over state inputs, the same generalization mechanism
does not apply to discrete action outputs. And yet, surprisingly, our
experiments indicate that Deep Q-Networks (DQN), which use exactly this type of
function approximator, are still able to achieve modest action generalization.
Our main contribution is twofold: first, we propose a method of evaluating
action generalization using expert knowledge of action similarity, and
empirically confirm that action generalization leads to faster learning;
second, we characterize the action-generalization gap (the difference in
learning performance between DQN and the expert) in different domains. We find
that DQN can indeed generalize over actions in several simple domains, but that
its ability to do so decreases as the action space grows larger.",None,-1
Multimodal Hate Speech Detection from Bengali Memes and Texts,0.888433,"Numerous machine learning (ML) and deep learning (DL)-based approaches have
been proposed to utilize textual data from social media for anti-social
behavior analysis like cyberbullying, fake news detection, and identification
of hate speech mainly for highly-resourced languages such as English. However,
despite having a lot of diversity and millions of native speakers, some
languages like Bengali are under-resourced, which is due to a lack of
computational resources for natural language processing (NLP). Similar to other
languages, Bengali social media contents also include images along with texts
(e.g., multimodal memes are posted by embedding short texts into images on
Facebook). Therefore, only the textual data is not enough to judge them since
images might give extra context to make a proper judgement. This paper is about
hate speech detection from multimodal Bengali memes and texts. We prepared the
only multimodal hate speech dataset for-a-kind of problem for Bengali, which we
use to train state-of-the-art neural architectures (e.g., Bi-LSTM/Conv-LSTM
with word embeddings, ConvNets + pre-trained language models, e.g., monolingual
Bangla BERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) to jointly
analyze textual and visual information for hate speech detection. Conv-LSTM and
XLM-RoBERTa models performed best for texts, yielding F1 scores of 0.78 and
0.82, respectively. As of memes, ResNet-152 and DenseNet-161 models yield F1
scores of 0.78 and 0.79, respectively. As for multimodal fusion, XLM-RoBERTa +
DenseNet-161 performed the best, yielding an F1 score of 0.83. Our study
suggests that text modality is most useful for hate speech detection, while
memes are moderately useful.",https://github.com/rezacsedu/Multimodal-Hate-Speech-Bengali,-1
"The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",0.940257,"Construction Grammar (CxG) is a paradigm from cognitive linguistics
emphasising the connection between syntax and semantics. Rather than rules that
operate on lexical items, it posits constructions as the central building
blocks of language, i.e., linguistic units of different granularity that
combine syntax and semantics. As a first step towards assessing the
compatibility of CxG with the syntactic and semantic knowledge demonstrated by
state-of-the-art pretrained language models (PLMs), we present an investigation
of their capability to classify and understand one of the most commonly studied
constructions, the English comparative correlative (CC). We conduct experiments
examining the classification accuracy of a syntactic probe on the one hand and
the models' behaviour in a semantic application task on the other, with BERT,
RoBERTa, and DeBERTa as the example PLMs. Our results show that all three
investigated PLMs are able to recognise the structure of the CC but fail to use
its meaning. While human-like performance of PLMs on many NLP tasks has been
alleged, this indicates that PLMs still suffer from substantial shortcomings in
central domains of linguistic knowledge.",https://github.com/LeonieWeissweiler/ComparativeCorrelative,76971
Backbone is All Your Need: A Simplified Architecture for Visual Object Tracking,0.962422,"Exploiting a general-purpose neural architecture to replace hand-wired
designs or inductive biases has recently drawn extensive interest. However,
existing tracking approaches rely on customized sub-modules and need prior
knowledge for architecture selection, hindering the tracking development in a
more general system. This paper presents a Simplified Tracking architecture
(SimTrack) by leveraging a transformer backbone for joint feature extraction
and interaction. Unlike existing Siamese trackers, we serialize the input
images and concatenate them directly before the one-branch backbone. Feature
interaction in the backbone helps to remove well-designed interaction modules
and produce a more efficient and effective framework. To reduce the information
loss from down-sampling in vision transformers, we further propose a foveal
window strategy, providing more diverse input patches with acceptable
computational costs. Our SimTrack improves the baseline with 2.5%/2.6% AUC
gains on LaSOT/TNL2K and gets results competitive with other specialized
tracking algorithms without bells and whistles.",https://github.com/LPXTT/SimTrack,-1
Sequential Manipulation Planning on Scene Graph,0.839497,"We devise a 3D scene graph representation, contact graph+ (cg+), for
efficient sequential task planning. Augmented with predicate-like attributes,
this contact graph-based representation abstracts scene layouts with succinct
geometric information and valid robot-scene interactions. Goal configurations,
naturally specified on contact graphs, can be produced by a genetic algorithm
with a stochastic optimization method. A task plan is then initialized by
computing the Graph Editing Distance (GED) between the initial contact graphs
and the goal configurations, which generates graph edit operations
corresponding to possible robot actions. We finalize the task plan by imposing
constraints to regulate the temporal feasibility of graph edit operations,
ensuring valid task and motion correspondences. In a series of simulations and
experiments, robots successfully complete complex sequential object
rearrangement tasks that are difficult to specify using conventional planning
language like Planning Domain Definition Language (PDDL), demonstrating the
high feasibility and potential of robot sequential task planning on contact
graph.",https://sites.google.com/view/planning-on-graph,-1
Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions,0.752118,"Role-oriented dialogue summarization is to generate summaries for different
roles in the dialogue, e.g., merchants and consumers. Existing methods handle
this task by summarizing each role's content separately and thus are prone to
ignore the information from other roles. However, we believe that other roles'
content could benefit the quality of summaries, such as the omitted information
mentioned by other roles. Therefore, we propose a novel role interaction
enhanced method for role-oriented dialogue summarization. It adopts cross
attention and decoder self-attention interactions to interactively acquire
other roles' critical information. The cross attention interaction aims to
select other roles' critical dialogue utterances, while the decoder
self-attention interaction aims to obtain key information from other roles'
summaries. Experimental results have shown that our proposed method
significantly outperforms strong baselines on two public role-oriented dialogue
summarization datasets. Extensive analyses have demonstrated that other roles'
content could help generate summaries with more complete semantics and correct
topic structures.",https://github.com/xiaolinAndy/RODS,15943
What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis,0.669387,"Market sentiment analysis on social media content requires knowledge of both
financial markets and social media jargon, which makes it a challenging task
for human raters. The resulting lack of high-quality labeled data stands in the
way of conventional supervised learning methods. Instead, we approach this
problem using semi-supervised learning with a large language model (LLM). Our
pipeline generates weak financial sentiment labels for Reddit posts with an LLM
and then uses that data to train a small model that can be served in
production. We find that prompting the LLM to produce Chain-of-Thought
summaries and forcing it through several reasoning paths helps generate more
stable and accurate labels, while using a regression loss further improves
distillation quality. With only a handful of prompts, the final model performs
on par with existing supervised models. Though production applications of our
model are limited by ethical considerations, the model's competitive
performance points to the great potential of using LLMs for tasks that
otherwise require skill-intensive annotation.",None,-1
The ReturnZero System for VoxCeleb Speaker Recognition Challenge 2022,0.158419,"In this paper, we describe the top-scoring submissions for team RTZR VoxCeleb
Speaker Recognition Challenge 2022 (VoxSRC-22) in the closed dataset, speaker
verification Track 1. The top performed system is a fusion of 7 models, which
contains 3 different types of model architectures. We focus on training models
to learn extra-temporal information. Therefore, all models were trained with
4-6 second frames for each utterance. Also, we apply the Large Margin
Fine-tuning strategy which has shown good performance on the previous
challenges for some of our fusion models. While the evaluation process, we
apply the scoring methods with adaptive symmetric normalization (AS-Norm) and
matrix score average (MSA). Finally, we mix up models with logistic regression
to fuse all the trained models. The final submission achieves 0.165 DCF and
2.912% EER on the VoxSRC22 test set.",None,-1
Contribution of the Temperature of the Objects to the Problem of Thermal Imaging Focusing,0.00948534,"When focusing an image, depth of field, aperture and distance from the camera
to the object, must be taking into account, both, in visible and in infrared
spectrum. Our experiments reveal that in addition, the focusing problem in
thermal spectrum is also hardly dependent of the temperature of the object
itself (and/or the scene).",None,-1
Variational Reasoning over Incomplete Knowledge Graphs for Conversational Recommendation,0.629163,"Conversational recommender systems (CRSs) often utilize external knowledge
graphs (KGs) to introduce rich semantic information and recommend relevant
items through natural language dialogues. However, original KGs employed in
existing CRSs are often incomplete and sparse, which limits the reasoning
capability in recommendation. Moreover, only few of existing studies exploit
the dialogue context to dynamically refine knowledge from KGs for better
recommendation. To address the above issues, we propose the Variational
Reasoning over Incomplete KGs Conversational Recommender (VRICR). Our key idea
is to incorporate the large dialogue corpus naturally accompanied with CRSs to
enhance the incomplete KGs; and perform dynamic knowledge reasoning conditioned
on the dialogue context. Specifically, we denote the dialogue-specific
subgraphs of KGs as latent variables with categorical priors for adaptive
knowledge graphs refactor. We propose a variational Bayesian method to
approximate posterior distributions over dialogue-specific subgraphs, which not
only leverages the dialogue corpus for restructuring missing entity relations
but also dynamically selects knowledge based on the dialogue context. Finally,
we infuse the dialogue-specific subgraphs to decode the recommendation and
responses. We conduct experiments on two benchmark CRSs datasets. Experimental
results confirm the effectiveness of our proposed method.",https://github.com/zxd-octopus/VRICR,-1
When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture,0.809908,"Vision Transformers (ViTs) have recently achieved competitive performance in
broad vision tasks. Unfortunately, on popular threat models, naturally trained
ViTs are shown to provide no more adversarial robustness than convolutional
neural networks (CNNs). Adversarial training is still required for ViTs to
defend against such adversarial attacks. In this paper, we provide the first
and comprehensive study on the adversarial training recipe of ViTs via
extensive evaluation of various training techniques across benchmark datasets.
We find that pre-training and SGD optimizer are necessary for ViTs' adversarial
training. Further considering ViT as a new type of model architecture, we
investigate its adversarial robustness from the perspective of its unique
architectural components. We find, when randomly masking gradients from some
attention blocks or masking perturbations on some patches during adversarial
training, the adversarial robustness of ViTs can be remarkably improved, which
may potentially open up a line of work to explore the architectural information
inside the newly designed models like ViTs. Our code is available at
https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers.",https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers,-1
A logical theory for conditional weak ontic necessity based on context update,0.251691,"Weak ontic necessity is the ontic necessity expressed by ``should'' or
``ought to'' in English. An example of it is ``I should be dead by now''. A
feature of this necessity is whether it holds does not have anything to do with
whether its prejacent holds. In this paper, we present a logical theory for
conditional weak ontic necessity based on context update. A context is a set of
ordered defaults, determining expected possible states of the present world.
Sentences are evaluated with respect to contexts. When evaluating the
conditional weak ontic necessity with respect to a context, we first update the
context with the antecedent, then check whether the consequent holds with
respect to the updated context. The logic is complete. Our theory combines
premise semantics and update semantics for conditionals.",None,-1
Deep Counterfactual Estimation with Categorical Background Variables,0.251617,"Referred to as the third rung of the causal inference ladder, counterfactual
queries typically ask the ""What if ?"" question retrospectively. The standard
approach to estimate counterfactuals resides in using a structural equation
model that accurately reflects the underlying data generating process. However,
such models are seldom available in practice and one usually wishes to infer
them from observational data alone. Unfortunately, the correct structural
equation model is in general not identifiable from the observed factual
distribution. Nevertheless, in this work, we show that under the assumption
that the main latent contributors to the treatment responses are categorical,
the counterfactuals can be still reliably predicted. Building upon this
assumption, we introduce CounterFactual Query Prediction (CFQP), a novel method
to infer counterfactuals from continuous observations when the background
variables are categorical. We show that our method significantly outperforms
previously available deep-learning-based counterfactual methods, both
theoretically and empirically on time series and image data. Our code is
available at https://github.com/edebrouwer/cfqp.",https://anonymous.4open.science/r/cfqp,-1
Backdoor Attack is a Devil in Federated GAN-based Medical Image Synthesis,0.564873,"Deep Learning-based image synthesis techniques have been applied in
healthcare research for generating medical images to support open research.
Training generative adversarial neural networks (GAN) usually requires large
amounts of training data. Federated learning (FL) provides a way of training a
central model using distributed data from different medical institutions while
keeping raw data locally. However, FL is vulnerable to backdoor attack, an
adversarial by poisoning training data, given the central server cannot access
the original data directly. Most backdoor attack strategies focus on
classification models and centralized domains. In this study, we propose a way
of attacking federated GAN (FedGAN) by treating the discriminator with a
commonly used data poisoning strategy in backdoor attack classification models.
We demonstrate that adding a small trigger with size less than 0.5 percent of
the original image size can corrupt the FL-GAN model. Based on the proposed
attack, we provide two effective defense strategies: global malicious detection
and local training regularization. We show that combining the two defense
strategies yields a robust medical image generation.",None,-1
Arabic Word-level Readability Visualization for Assisted Text Simplification,0.457826,"This demo paper presents a Google Docs add-on for automatic Arabic word-level
readability visualization. The add-on includes a lemmatization component that
is connected to a five-level readability lexicon and Arabic WordNet-based
substitution suggestions. The add-on can be used for assessing the reading
difficulty of a text and identifying difficult words as part of the task of
manual text simplification. We make our add-on and its code publicly available.",None,16583
Poseur: Direct Human Pose Regression with Transformers,0.914351,"We propose a direct, regression-based approach to 2D human pose estimation
from single images. We formulate the problem as a sequence prediction task,
which we solve using a Transformer network. This network directly learns a
regression mapping from images to the keypoint coordinates, without resorting
to intermediate representations such as heatmaps. This approach avoids much of
the complexity associated with heatmap-based approaches. To overcome the
feature misalignment issues of previous regression-based methods, we propose an
attention mechanism that adaptively attends to the features that are most
relevant to the target keypoints, considerably improving the accuracy.
Importantly, our framework is end-to-end differentiable, and naturally learns
to exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,
two predominant pose-estimation datasets, demonstrate that our method
significantly improves upon the state-of-the-art in regression-based pose
estimation. More notably, ours is the first regression-based approach to
perform favorably compared to the best heatmap-based pose estimation methods.",https://github.com/aim-uofa/Poseur,-1
Pruning Pre-trained Language Models Without Fine-Tuning,0.196214,"To overcome the overparameterized problem in Pre-trained Language Models
(PLMs), pruning is widely used as a simple and straightforward compression
method by directly removing unimportant weights. Previous first-order methods
successfully compress PLMs to extremely high sparsity with little performance
drop. These methods, such as movement pruning, use first-order information to
prune PLMs while fine-tuning the remaining weights. In this work, we argue
fine-tuning is redundant for first-order pruning, since first-order pruning is
sufficient to converge PLMs to downstream tasks without fine-tuning. Under this
motivation, we propose Static Model Pruning (SMP), which only uses first-order
pruning to adapt PLMs to downstream tasks while achieving the target sparsity
level. In addition, we also design a new masking function and training
objective to further improve SMP. Extensive experiments at various sparsity
levels show SMP has significant improvements over first-order and zero-order
methods. Unlike previous first-order methods, SMP is also applicable to low
sparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter
efficient than other methods due to it does not require fine-tuning.",https://github.com/kongds/SMP,-1
A Fast Post-Training Pruning Framework for Transformers,0.615454,"Pruning is an effective way to reduce the huge inference cost of Transformer
models. However, prior work on pruning Transformers requires retraining the
models. This can add high training cost and high complexity to model
deployment, making it difficult to use in many practical situations. To address
this, we propose a fast post-training pruning framework for Transformers that
does not require any retraining. Given a resource constraint and a sample
dataset, our framework automatically prunes the Transformer model using
structured sparsity methods. To retain high accuracy without retraining, we
introduce three novel techniques: (i) a lightweight mask search algorithm that
finds which heads and filters to prune based on the Fisher information; (ii)
mask rearrangement that complements the search algorithm; and (iii) mask tuning
that reconstructs the output activations for each layer. We apply our method to
BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD
benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x
speedup in inference latency, while maintaining < 1% loss in accuracy.
Importantly, our framework prunes Transformers in less than 3 minutes on a
single GPU, which is over two orders of magnitude faster than existing pruning
approaches that retrain the models.",https://github.com/WoosukKwon/retraining-free-pruning,-1
CoSP: Co-supervised pretraining of pocket and ligand,0.570203,"Can we inject the pocket-ligand interaction knowledge into the pre-trained
model and jointly learn their chemical space? Pretraining molecules and
proteins has attracted considerable attention in recent years, while most of
these approaches focus on learning one of the chemical spaces and lack the
injection of biological knowledge. We propose a co-supervised pretraining
(CoSP) framework to simultaneously learn 3D pocket and ligand representations.
We use a gated geometric message passing layer to model both 3D pockets and
ligands, where each node's chemical features, geometric position and
orientation are considered. To learn biological meaningful embeddings, we
inject the pocket-ligand interaction knowledge into the pretraining model via
contrastive loss. Considering the specificity of molecules, we further propose
a chemical similarity-enhanced negative sampling strategy to improve the
contrastive learning performance. Through extensive experiments, we conclude
that CoSP can achieve competitive results in pocket matching, molecule property
predictions, and virtual screening.",None,-1
Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization,0.599108,"The most advanced abstractive dialogue summarizers lack generalization
ability on new domains and the existing researches for domain adaptation in
summarization generally rely on large-scale pre-trainings. To explore the
lightweight fine-tuning methods for domain adaptation of dialogue
summarization, in this paper, we propose an efficient and generalizable
Domain-Oriented Prefix-tuning model, which utilizes a domain word initialized
prefix module to alleviate domain entanglement and adopts discrete prompts to
guide the model to focus on key contents of dialogues and enhance model
generalization. We conduct zero-shot experiments and build domain adaptation
benchmarks on two multi-domain dialogue summarization datasets, TODSum and
QMSum. Adequate experiments and qualitative analysis prove the effectiveness of
our methods.",https://github.com/Zeng-WH/DOP-Tuning,-1
A Question-Answer Driven Approach to Reveal Affirmative Interpretations from Verbal Negations,0.0801253,"This paper explores a question-answer driven approach to reveal affirmative
interpretations from verbal negations (i.e., when a negation cue grammatically
modifies a verb). We create a new corpus consisting of 4,472 verbal negations
and discover that 67.1% of them convey that an event actually occurred.
Annotators generate and answer 7,277 questions for the 3,001 negations that
convey an affirmative interpretation. We first cast the problem of revealing
affirmative interpretations from negations as a natural language inference
(NLI) classification task. Experimental results show that state-of-the-art
transformers trained with existing NLI corpora are insufficient to reveal
affirmative interpretations. We also observe, however, that fine-tuning brings
small improvements. In addition to NLI classification, we also explore the more
realistic task of generating affirmative interpretations directly from
negations with the T5 transformer. We conclude that the generation task remains
a challenge as T5 substantially underperforms humans.",https://github.com/mosharafhossain/AFIN,-1
SAT: Self-adaptive training for fashion compatibility prediction,0.405306,"This paper presents a self-adaptive training (SAT) model for fashion
compatibility prediction. It focuses on the learning of some hard items, such
as those that share similar color, texture, and pattern features but are
considered incompatible due to the aesthetics or temporal shifts. Specifically,
we first design a method to define hard outfits and a difficulty score (DS) is
defined and assigned to each outfit based on the difficulty in recommending an
item for it. Then, we propose a self-adaptive triplet loss (SATL), where the DS
of the outfit is considered. Finally, we propose a very simple conditional
similarity network combining the proposed SATL to achieve the learning of hard
items in the fashion compatibility prediction. Experiments on the publicly
available Polyvore Outfits and Polyvore Outfits-D datasets demonstrate our
SAT's effectiveness in fashion compatibility prediction. Besides, our SATL can
be easily extended to other conditional similarity networks to improve their
performance.",None,7956
On Vision Features in Multimodal Machine Translation,0.882987,"Previous work on multimodal machine translation (MMT) has focused on the way
of incorporating vision features into translation but little attention is on
the quality of vision models. In this work, we investigate the impact of vision
models on MMT. Given the fact that Transformer is becoming popular in computer
vision, we experiment with various strong models (such as Vision Transformer)
and enhanced features (such as object-detection and image captioning). We
develop a selective attention model to study the patch-level contribution of an
image in MMT. On detailed probing tasks, we find that stronger vision models
are helpful for learning translation from the visual modality. Our results also
suggest the need of carefully examining MMT models, especially when current
benchmarks are small-scale and biased. Our code could be found at
\url{https://github.com/libeineu/fairseq_mmt}.",https://github.com/libeineu/fairseq_mmt,-1
Training Mixed-Domain Translation Models via Federated Learning,0.359718,"Training mixed-domain translation models is a complex task that demands
tailored architectures and costly data preparation techniques. In this work, we
leverage federated learning (FL) in order to tackle the problem. Our
investigation demonstrates that with slight modifications in the training
process, neural machine translation (NMT) engines can be easily adapted when an
FL-based aggregation is applied to fuse different domains. Experimental results
also show that engines built via FL are able to perform on par with
state-of-the-art baselines that rely on centralized training techniques. We
evaluate our hypothesis in the presence of five datasets with different sizes,
from different domains, to translate from German into English and discuss how
FL and NMT can mutually benefit from each other. In addition to providing
benchmarking results on the union of FL and NMT, we also propose a novel
technique to dynamically control the communication bandwidth by selecting
impactful parameters during FL updates. This is a significant achievement
considering the large size of NMT engines that need to be exchanged between FL
parties.",https://github.com/moses-smt/,-1
How to Backpropagate through Hungarian in Your DETR?,0.154271,"The DEtection TRansformer (DETR) approach, which uses a transformer
encoder-decoder architecture and a set-based global loss, has become a building
block in many transformer based applications. However, as originally presented,
the assignment cost and the global loss are not aligned, i.e., reducing the
former is likely but not guaranteed to reduce the latter. And the issue of
gradient is ignored when a combinatorial solver such as Hungarian is used. In
this paper we show that the global loss can be expressed as the sum of an
assignment-independent term, and an assignment-dependent term which can be used
to define the assignment cost matrix. Recent results on generalized gradients
of optimal assignment cost with respect to parameters of an assignment problem
are then used to define generalized gradients of the loss with respect to
network parameters, and backpropagation is carried out properly. Our
experiments using the same loss weights show interesting convergence properties
and a potential for further performance improvements.",None,-1
Biometric identification by means of hand geometry and a neural net classifier,0.108669,"This Paper describes a hand geometry biometric identification system. We have
acquired a database of 22 people using a conventional document scanner. The
experimental section consists of a study about the discrimination capability of
different extracted features, and the identification rate using different
classifiers based on neural networks.",None,-1
"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",0.5956,"Understanding longer narratives or participating in conversations requires
tracking of discourse entities that have been mentioned. Indefinite noun
phrases (NPs), such as 'a dog', frequently introduce discourse entities but
this behavior is modulated by sentential operators such as negation. For
example, 'a dog' in 'Arthur doesn't own a dog' does not introduce a discourse
entity due to the presence of negation. In this work, we adapt the
psycholinguistic assessment of language models paradigm to higher-level
linguistic phenomena and introduce an English evaluation suite that targets the
knowledge of the interactions between sentential operators and indefinite NPs.
We use this evaluation suite for a fine-grained investigation of the entity
tracking abilities of the Transformer-based models GPT-2 and GPT-3. We find
that while the models are to a certain extent sensitive to the interactions we
investigate, they are all challenged by the presence of multiple NPs and their
behavior is not systematic, which suggests that even models at the scale of
GPT-3 do not fully acquire basic entity tracking abilities.",https://github.com/sebschu/discourse-entity-lm,-1
Learning Semantic Segmentation from Multiple Datasets with Label Shifts,0.235065,"With increasing applications of semantic segmentation, numerous datasets have
been proposed in the past few years. Yet labeling remains expensive, thus, it
is desirable to jointly train models across aggregations of datasets to enhance
data volume and diversity. However, label spaces differ across datasets and may
even be in conflict with one another. This paper proposes UniSeg, an effective
approach to automatically train models across multiple datasets with differing
label spaces, without any manual relabeling efforts. Specifically, we propose
two losses that account for conflicting and co-occurring labels to achieve
better generalization performance in unseen domains. First, a gradient conflict
in training due to mismatched label spaces is identified and a
class-independent binary cross-entropy loss is proposed to alleviate such label
conflicts. Second, a loss function that considers class-relationships across
datasets is proposed for a better multi-dataset training scheme. Extensive
quantitative and qualitative analyses on road-scene datasets show that UniSeg
improves over multi-dataset baselines, especially on unseen datasets, e.g.,
achieving more than 8% gain in IoU on KITTI averaged over all the settings.",None,-1
Graph Modeling in Computer Assisted Automotive Development,0.0674454,"We consider graph modeling for a knowledge graph for vehicle development,
with a focus on crash safety. An organized schema that incorporates information
from various structured and unstructured data sources is provided, which
includes relevant concepts within the domain. In particular, we propose
semantics for crash computer aided engineering (CAE) data, which enables
searchability, filtering, recommendation, and prediction for crash CAE data
during the development process. This graph modeling considers the CAE data in
the context of the R\&D development process and vehicle safety. Consequently,
we connect CAE data to the protocols that are used to assess vehicle safety
performances. The R\&D process includes CAD engineering and safety attributes,
with a focus on multidisciplinary problem-solving. We describe previous efforts
in graph modeling in comparison to our proposal, discuss its strengths and
limitations, and identify areas for future work.",https://github.com/Fraunhofer-SCAI/GAE-vehicle-safety,-1
Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection,0.270472,"Hate speech classifiers exhibit substantial performance degradation when
evaluated on datasets different from the source. This is due to learning
spurious correlations between words that are not necessarily relevant to
hateful language, and hate speech labels from the training corpus. Previous
work has attempted to mitigate this problem by regularizing specific terms from
pre-defined static dictionaries. While this has been demonstrated to improve
the generalizability of classifiers, the coverage of such methods is limited
and the dictionaries require regular manual updates from human experts. In this
paper, we propose to automatically identify and reduce spurious correlations
using attribution methods with dynamic refinement of the list of terms that
need to be regularized during training. Our approach is flexible and improves
the cross-corpora performance over previous work independently and in
combination with pre-defined dictionaries.",https://github.com/tbose20/D-Ref,-1
In-Hand 3D Object Scanning from an RGB Sequence,0.968619,"We propose a method for in-hand 3D scanning of an unknown object with a
monocular camera. Our method relies on a neural implicit surface representation
that captures both the geometry and the appearance of the object, however, by
contrast with most NeRF-based methods, we do not assume that the camera-object
relative poses are known. Instead, we simultaneously optimize both the object
shape and the pose trajectory. As direct optimization over all shape and pose
parameters is prone to fail without coarse-level initialization, we propose an
incremental approach that starts by splitting the sequence into carefully
selected overlapping segments within which the optimization is likely to
succeed. We reconstruct the object shape and track its poses independently
within each segment, then merge all the segments before performing a global
optimization. We show that our method is able to reconstruct the shape and
color of both textured and challenging texture-less objects, outperforms
classical methods that rely only on appearance features, and that its
performance is close to recent methods that assume known camera poses.",None,-1
Action Languages Based Actual Causality for Computational Ethics: a Sound and Complete Implementation in ASP,0.0488551,"Although moral responsibility is not circumscribed by causality, they are
both closely intermixed. Furthermore, rationally understanding the evolution of
the physical world is inherently linked with the idea of causality. Thus, the
decision-making applications based on automated planning inevitably have to
deal with causality, especially if they consider imputability aspects or
integrate references to ethical norms. The many debates around causation in the
last decades have shown how complex this notion is and thus, how difficult is
its integration with planning. As a result, much of the work in computational
ethics relegates causality to the background, despite the considerations stated
above. This paper's contribution is to provide a complete and sound translation
into logic programming from an actual causation definition suitable for action
languages, this definition is a formalisation of Wright's NESS test. The
obtained logic program allows to deal with complex causal relations. In
addition to enabling agents to reason about causality, this contribution
specifically enables the computational ethics domain to handle situations that
were previously out of reach. In a context where ethical considerations in
decision-making are increasingly important, advances in computational ethics
can greatly benefit the entire AI community.",None,-1
Explaining Causal Models with Argumentation: the Case of Bi-variate Reinforcement,0.0662542,"Causal models are playing an increasingly important role in machine learning,
particularly in the realm of explainable AI. We introduce a conceptualisation
for generating argumentation frameworks (AFs) from causal models for the
purpose of forging explanations for the models' outputs. The conceptualisation
is based on reinterpreting desirable properties of semantics of AFs as
explanation moulds, which are means for characterising the relations in the
causal model argumentatively. We demonstrate our methodology by reinterpreting
the property of bi-variate reinforcement as an explanation mould to forge
bipolar AFs as explanations for the outputs of causal models. We perform a
theoretical evaluation of these argumentative explanations, examining whether
they satisfy a range of desirable explanatory and argumentative properties.",None,12536
The Internet of Senses: Building on Semantic Communications and Edge Intelligence,0.853393,"The Internet of Senses (IoS) holds the promise of flawless telepresence-style
communication for all human `receptors' and therefore blurs the difference of
virtual and real environments. We commence by highlighting the compelling use
cases empowered by the IoS and also the key network requirements. We then
elaborate on how the emerging semantic communications and Artificial
Intelligence (AI)/Machine Learning (ML) paradigms along with 6G technologies
may satisfy the requirements of IoS use cases. On one hand, semantic
communications can be applied for extracting meaningful and significant
information and hence efficiently exploit the resources and for harnessing a
priori information at the receiver to satisfy IoS requirements. On the other
hand, AI/ML facilitates frugal network resource management by making use of the
enormous amount of data generated in IoS edge nodes and devices, as well as by
optimizing the IoS performance via intelligent agents. However, the intelligent
agents deployed at the edge are not completely aware of each others' decisions
and the environments of each other, hence they operate in a partially rather
than fully observable environment. Therefore, we present a case study of
Partially Observable Markov Decision Processes (POMDP) for improving the User
Equipment (UE) throughput and energy consumption, as they are imperative for
IoS use cases, using Reinforcement Learning for astutely activating and
deactivating the component carriers in carrier aggregation. Finally, we outline
the challenges and open issues of IoS implementations and employing semantic
communications, edge intelligence as well as learning under partial
observability in the IoS context.",None,-1
Warmup and Transfer Knowledge-Based Federated Learning Approach for IoT Continuous Authentication,0.15333,"Continuous behavioural authentication methods add a unique layer of security
by allowing individuals to verify their unique identity when accessing a
device. Maintaining session authenticity is now feasible by monitoring users'
behaviour while interacting with a mobile or Internet of Things (IoT) device,
making credential theft and session hijacking ineffective. Such a technique is
made possible by integrating the power of artificial intelligence and Machine
Learning (ML). Most of the literature focuses on training machine learning for
the user by transmitting their data to an external server, subject to private
user data exposure to threats. In this paper, we propose a novel Federated
Learning (FL) approach that protects the anonymity of user data and maintains
the security of his data. We present a warmup approach that provides a
significant accuracy increase. In addition, we leverage the transfer learning
technique based on feature extraction to boost the models' performance. Our
extensive experiments based on four datasets: MNIST, FEMNIST, CIFAR-10 and
UMDAA-02-FD, show a significant increase in user authentication accuracy while
maintaining user privacy and data security.",None,-1
Exemplar Free Class Agnostic Counting,0.819298,"We tackle the task of Class Agnostic Counting, which aims to count objects in
a novel object category at test time without any access to labeled training
data for that category. All previous class agnostic counting methods cannot
work in a fully automated setting, and require computationally expensive test
time adaptation. To address these challenges, we propose a visual counter which
operates in a fully automated setting and does not require any test time
adaptation. Our proposed approach first identifies exemplars from repeating
objects in an image, and then counts the repeating objects. We propose a novel
region proposal network for identifying the exemplars. After identifying the
exemplars, we obtain the corresponding count by using a density estimation
based Visual Counter. We evaluate our proposed approach on FSC-147 dataset, and
show that it achieves superior performance compared to the existing approaches.",None,-1
imitation: Clean Imitation Learning Implementations,0.396492,"imitation provides open-source implementations of imitation and reward
learning algorithms in PyTorch. We include three inverse reinforcement learning
(IRL) algorithms, three imitation learning algorithms and a preference
comparison algorithm. The implementations have been benchmarked against
previous results, and automated tests cover 98% of the code. Moreover, the
algorithms are implemented in a modular fashion, making it simple to develop
novel algorithms in the framework. Our source code, including documentation and
examples, is available at https://github.com/HumanCompatibleAI/imitation",https://github.com/HumanCompatibleAI/imitation,-1
Are High-Resolution Event Cameras Really Needed?,0.51533,"Due to their outstanding properties in challenging conditions, event cameras
have become indispensable in a wide range of applications, ranging from
automotive, computational photography, and SLAM. However, as further
improvements are made to the sensor design, modern event cameras are trending
toward higher and higher sensor resolutions, which result in higher bandwidth
and computational requirements on downstream tasks. Despite this trend, the
benefits of using high-resolution event cameras to solve standard computer
vision tasks are still not clear. In this work, we report the surprising
discovery that, in low-illumination conditions and at high speeds,
low-resolution cameras can outperform high-resolution ones, while requiring a
significantly lower bandwidth. We provide both empirical and theoretical
evidence for this claim, which indicates that high-resolution event cameras
exhibit higher per-pixel event rates, leading to higher temporal noise in
low-illumination conditions and at high speeds. As a result, in most cases,
high-resolution event cameras show a lower task performance, compared to lower
resolution sensors in these conditions. We empirically validate our findings
across several tasks, namely image reconstruction, optical flow estimation, and
camera pose tracking, both on synthetic and real data. We believe that these
findings will provide important guidelines for future trends in event camera
development.",None,-1
Sample-efficient Iterative Lower Bound Optimization of Deep Reactive Policies for Planning in Continuous MDPs,0.242156,"Recent advances in deep learning have enabled optimization of deep reactive
policies (DRPs) for continuous MDP planning by encoding a parametric policy as
a deep neural network and exploiting automatic differentiation in an end-to-end
model-based gradient descent framework. This approach has proven effective for
optimizing DRPs in nonlinear continuous MDPs, but it requires a large number of
sampled trajectories to learn effectively and can suffer from high variance in
solution quality. In this work, we revisit the overall model-based DRP
objective and instead take a minorization-maximization perspective to
iteratively optimize the DRP w.r.t. a locally tight lower-bounded objective.
This novel formulation of DRP learning as iterative lower bound optimization
(ILBO) is particularly appealing because (i) each step is structurally easier
to optimize than the overall objective, (ii) it guarantees a monotonically
improving objective under certain theoretical conditions, and (iii) it reuses
samples between iterations thus lowering sample complexity. Empirical
evaluation confirms that ILBO is significantly more sample-efficient than the
state-of-the-art DRP planner and consistently produces better solution quality
with lower variance. We additionally demonstrate that ILBO generalizes well to
new problem instances (i.e., different initial states) without requiring
retraining.",https://github.com/thiagopbueno/rddlgym,-1
Hardware-aware mobile building block evaluation for computer vision,0.0646389,"In this work we propose a methodology to accurately evaluate and compare the
performance of efficient neural network building blocks for computer vision in
a hardware-aware manner. Our comparison uses pareto fronts based on randomly
sampled networks from a design space to capture the underlying
accuracy/complexity trade-offs. We show that our approach allows to match the
information obtained by previous comparison paradigms, but provides more
insights in the relationship between hardware cost and accuracy. We use our
methodology to analyze different building blocks and evaluate their performance
on a range of embedded hardware platforms. This highlights the importance of
benchmarking building blocks as a preselection step in the design process of a
neural network. We show that choosing the right building block can speed up
inference by up to a factor of 2x on specific hardware ML accelerators.",None,-1
Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers,0.588761,"Existing Math Word Problem (MWP) solvers have achieved high accuracy on
benchmark datasets. However, prior works have shown that such solvers do not
generalize well and rely on superficial cues to achieve high performance. In
this paper, we first conduct experiments to showcase that this behaviour is
mainly associated with the limited size and diversity present in existing MWP
datasets. Next, we propose several data augmentation techniques broadly
categorized into Substitution and Paraphrasing based methods. By deploying
these methods we increase the size of existing datasets by five folds.
Extensive experiments on two benchmark datasets across three state-of-the-art
MWP solvers show that proposed methods increase the generalization and
robustness of existing solvers. On average, proposed methods significantly
increase the state-of-the-art results by over five percentage points on
benchmark datasets. Further, the solvers trained on the augmented dataset
perform comparatively better on the challenge test set. We also show the
effectiveness of proposed techniques through ablation studies and verify the
quality of augmented samples through human evaluation.",https://github.com/kevivk/MWP-Augmentation,-1
Knowledge Graph Augmented Network Towards Multiview Representation Learning for Aspect-based Sentiment Analysis,0.972621,"Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment
analysis. To better comprehend long complicated sentences and obtain accurate
aspect-specific information, linguistic and commonsense knowledge are generally
required in this task. However, most current methods employ complicated and
inefficient approaches to incorporate external knowledge, e.g., directly
searching the graph nodes. Additionally, the complementarity between external
knowledge and linguistic information has not been thoroughly studied. To this
end, we propose a knowledge graph augmented network KGAN, which aims to
effectively incorporate external knowledge with explicitly syntactic and
contextual information. In particular, KGAN captures the sentiment feature
representations from multiple different perspectives, i.e., context-, syntax-
and knowledge-based. First, KGAN learns the contextual and syntactic
representations in parallel to fully extract the semantic features. Then, KGAN
integrates the knowledge graphs into the embedding space, based on which the
aspect-specific knowledge representations are further obtained via an attention
mechanism. Last, we propose a hierarchical fusion module to complement these
multi-view representations in a local-to-global manner. Extensive experiments
on five popular ABSA benchmarks demonstrate the effectiveness and robustness of
our KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN
achieves a new record of state-of-the-art performance among all datasets.",https://github.com/WHU-ZQH/KGAN,-1
Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning,0.858513,"We introduce Patch Aligned Contrastive Learning (PACL), a modified
compatibility function for CLIP's contrastive loss, intending to train an
alignment between the patch tokens of the vision encoder and the CLS token of
the text encoder. With such an alignment, a model can identify regions of an
image corresponding to a given text input, and therefore transfer seamlessly to
the task of open vocabulary semantic segmentation without requiring any
segmentation annotations during training. Using pre-trained CLIP encoders with
PACL, we are able to set the state-of-the-art on the task of open vocabulary
zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC,
Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also
applicable to image-level predictions and when used with a CLIP backbone,
provides a general improvement in zero-shot classification accuracy compared to
CLIP, across a suite of 12 image classification datasets.",None,-1
Membership-Mappings for Practical Secure Distributed Deep Learning,0.327728,"This study leverages the data representation capability of fuzzy based
membership-mappings for practical secure distributed deep learning using fully
homomorphic encryption. The impracticality issue of secure machine (deep)
learning with fully homomorphic encrypted data, arising from large
computational overhead, is addressed via applying fuzzy attributes. Fuzzy
attributes are induced by globally convergent and robust variational
membership-mappings based local deep models. Fuzzy attributes combine the local
deep models in a robust and flexible manner such that the global model can be
evaluated homomorphically in an efficient manner using a boolean circuit
composed of bootstrapped binary gates. The proposed method, while preserving
privacy in a distributed learning scenario, remains accurate, practical, and
scalable. The method is evaluated through numerous experiments including
demonstrations through MNIST dataset and Freiburg Groceries Dataset. Further, a
biomedical application related to mental stress detection on individuals is
considered.",None,-1
KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,0.467197,"Relative positional embeddings (RPE) have received considerable attention
since RPEs effectively model the relative distance among tokens and enable
length extrapolation. We propose KERPLE, a framework that generalizes relative
position embedding for extrapolation by kernelizing positional differences. We
achieve this goal using conditionally positive definite (CPD) kernels, a class
of functions known for generalizing distance metrics. To maintain the inner
product interpretation of self-attention, we show that a CPD kernel can be
transformed into a PD kernel by adding a constant offset. This offset is
implicitly absorbed in the Softmax normalization during self-attention. The
diversity of CPD kernels allows us to derive various RPEs that enable length
extrapolation in a principled way. Experiments demonstrate that the logarithmic
variant achieves excellent extrapolation performance on three large language
modeling datasets. Our implementation and pretrained checkpoints are released
at https://github.com/chijames/KERPLE.git.",https://github.com/chijames/KERPLE.git,-1
Image Segmentation-based Unsupervised Multiple Objects Discovery,0.543427,"Unsupervised object discovery aims to localize objects in images, while
removing the dependence on annotations required by most deep learning-based
methods. To address this problem, we propose a fully unsupervised, bottom-up
approach, for multiple objects discovery. The proposed approach is a two-stage
framework. First, instances of object parts are segmented by using the
intra-image similarity between self-supervised local features. The second step
merges and filters the object parts to form complete object instances. The
latter is performed by two CNN models that capture semantic information on
objects from the entire dataset. We demonstrate that the pseudo-labels
generated by our method provide a better precision-recall trade-off than
existing single and multiple objects discovery methods. In particular, we
provide state-of-the-art results for both unsupervised class-agnostic object
detection and unsupervised image segmentation.",None,-1
FitCLIP: Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks,0.455948,"Large-scale pretrained image-text models have shown incredible zero-shot
performance in a handful of tasks, including video ones such as action
recognition and text-to-video retrieval. However, these models have not been
adapted to video, mainly because they do not account for the time dimension but
also because video frames are different from the typical images (e.g.,
containing motion blur, and less sharpness). In this paper, we present a
fine-tuning strategy to refine these large-scale pretrained image-text models
for zero-shot video understanding tasks. We show that by carefully adapting
these models we obtain considerable improvements on two zero-shot Action
Recognition tasks and three zero-shot Text-to-video Retrieval tasks. The code
is available at https://github.com/bryant1410/fitclip",None,-1
3D Pose Based Feedback for Physical Exercises,0.901406,"Unsupervised self-rehabilitation exercises and physical training can cause
serious injuries if performed incorrectly. We introduce a learning-based
framework that identifies the mistakes made by a user and proposes corrective
measures for easier and safer individual training. Our framework does not rely
on hard-coded, heuristic rules. Instead, it learns them from data, which
facilitates its adaptation to specific user needs. To this end, we use a Graph
Convolutional Network (GCN) architecture acting on the user's pose sequence to
model the relationship between the body joints trajectories. To evaluate our
approach, we introduce a dataset with 3 different physical exercises. Our
approach yields 90.9% mistake identification accuracy and successfully corrects
94.2% of the mistakes.",None,-1
Prompt Distribution Learning,0.943454,"We present prompt distribution learning for effectively adapting a
pre-trained vision-language model to address downstream recognition tasks. Our
method not only learns low-bias prompts from a few samples but also captures
the distribution of diverse prompts to handle the varying visual
representations. In this way, we provide high-quality task-related content for
facilitating recognition. This prompt distribution learning is realized by an
efficient approach that learns the output embeddings of prompts instead of the
input embeddings. Thus, we can employ a Gaussian distribution to model them
effectively and derive a surrogate loss for efficient training. Extensive
experiments on 12 datasets demonstrate that our method consistently and
significantly outperforms existing methods. For example, with 1 sample per
category, it relatively improves the average result by 9.1% compared to
human-crafted prompts.",None,-1
Continual Spatio-Temporal Graph Convolutional Networks,0.472025,"Graph-based reasoning over skeleton data has emerged as a promising approach
for human action recognition. However, the application of prior graph-based
methods, which predominantly employ whole temporal sequences as their input, to
the setting of online inference entails considerable computational redundancy.
In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph
Convolutional Neural Network as a Continual Inference Network, which can
perform step-by-step predictions in time without repeat frame processing. To
evaluate our method, we create a continual version of ST-GCN, CoST-GCN,
alongside two derived methods with different self-attention mechanisms, CoAGCN
and CoS-TR. We investigate weight transfer strategies and architectural
modifications for inference acceleration, and perform experiments on the NTU
RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar
predictive accuracy, we observe up to 109x reduction in time complexity,
on-hardware accelerations of 26x, and reductions in maximum allocated memory of
52% during online inference.",https://github.com/lukashedegaard/continual-skeletons,-1
Feature transforms for image data augmentation,0.414006,"A problem with Convolutional Neural Networks (CNNs) is that they require
large datasets to obtain adequate robustness; on small datasets, they are prone
to overfitting. Many methods have been proposed to overcome this shortcoming
with CNNs. In cases where additional samples cannot easily be collected, a
common approach is to generate more data points from existing data using an
augmentation technique. In image classification, many augmentation approaches
utilize simple image manipulation algorithms. In this work, we build ensembles
on the data level by adding images generated by combining fourteen augmentation
approaches, three of which are proposed here for the first time. These novel
methods are based on the Fourier Transform (FT), the Radon Transform (RT) and
the Discrete Cosine Transform (DCT). Pretrained ResNet50 networks are finetuned
on training sets that include images derived from each augmentation method.
These networks and several fusions are evaluated and compared across eleven
benchmarks. Results show that building ensembles on the data level by combining
different data augmentation methods produce classifiers that not only compete
competitively against the state-of-the-art but often surpass the best
approaches reported in the literature.",None,-1
Non-Isometric Shape Matching via Functional Maps on Landmark-Adapted Bases,0.397394,"We propose a principled approach for non-isometric landmark-preserving
non-rigid shape matching. Our method is based on the functional maps framework,
but rather than promoting isometries we focus instead on near-conformal maps
that preserve landmarks exactly. We achieve this, first, by introducing a novel
landmark-adapted basis using an intrinsic Dirichlet-Steklov eigenproblem.
Second, we establish the functional decomposition of conformal maps expressed
in this basis. Finally, we formulate a conformally-invariant energy that
promotes high-quality landmark-preserving maps, and show how it can be solved
via a variant of the recently proposed ZoomOut method that we extend to our
setting. Our method is descriptor-free, efficient and robust to significant
mesh variability. We evaluate our approach on a range of benchmark datasets and
demonstrate state-of-the-art performance on non-isometric benchmarks and near
state-of-the-art performance on isometric ones.",https://github.com/mpanine/DirichletSteklovLandmarkMatching,-1
HarmoF0: Logarithmic Scale Dilated Convolution For Pitch Estimation,0.328853,"Sounds, especially music, contain various harmonic components scattered in
the frequency dimension. It is difficult for normal convolutional neural
networks to observe these overtones. This paper introduces a multiple rates
dilated causal convolution (MRDC-Conv) method to capture the harmonic structure
in logarithmic scale spectrograms efficiently. The harmonic is helpful for
pitch estimation, which is important for many sound processing applications. We
propose HarmoF0, a fully convolutional network, to evaluate the MRDC-Conv and
other dilated convolutions in pitch estimation. The results show that this
model outperforms the DeepF0, yields state-of-the-art performance in three
datasets, and simultaneously reduces more than 90% parameters. We also find
that it has stronger noise resistance and fewer octave errors. The code and
pre-trained model are available at https://github.com/WX-Wei/HarmoF0.",https://github.com/WX-Wei/HarmoF0,-1
Design of High-Throughput Mixed-Precision CNN Accelerators on FPGA,0.52301,"Convolutional Neural Networks (CNNs) reach high accuracies in various
application domains, but require large amounts of computation and incur costly
data movements. One method to decrease these costs while trading accuracy is
weight and/or activation word-length reduction. Thereby, layer-wise
mixed-precision quantization allows for more efficient results while inflating
the design space. In this work, we present an in-depth quantitative methodology
to efficiently explore the design space considering the limited hardware
resources of a given FPGA. Our holistic exploration approach vertically
traverses the various design entry levels from the architectural down to the
logic level, and laterally covers optimization from processing elements to
dataflow for an efficient mixed-precision CNN accelerator. Our resulting
hardware accelerators implement truly mixed-precision operations that enable
efficient execution of layer-wise and channel-wise quantized CNNs. Mapping
feed-forward and identity-shortcut-connection mixed-precision CNNs result in
competitive accuracy-throughout trade-offs: 245 frames/s with 87.48% Top-5
accuracy for ResNet-18 and 92.9% Top-5 accuracy with 1.13 TOps/s for
ResNet-152, respectively. Thereby, the required memory footprint for parameters
is reduced by 4.9x and 9.4x compared to the respective floating-point baseline.",None,699
Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition,0.930699,"Subject-invariant facial action unit (AU) recognition remains challenging for
the reason that the data distribution varies among subjects. In this paper, we
propose a causal inference framework for subject-invariant facial action unit
recognition. To illustrate the causal effect existing in AU recognition task,
we formulate the causalities among facial images, subjects, latent AU semantic
relations, and estimated AU occurrence probabilities via a structural causal
model. By constructing such a causal diagram, we clarify the causal effect
among variables and propose a plug-in causal intervention module, CIS, to
deconfound the confounder \emph{Subject} in the causal diagram. Extensive
experiments conducted on two commonly used AU benchmark datasets, BP4D and
DISFA, show the effectiveness of our CIS, and the model with CIS inserted,
CISNet, has achieved state-of-the-art performance.",None,250
Transformer-Based Self-Supervised Learning for Emotion Recognition,0.63353,"In order to exploit representations of time-series signals, such as
physiological signals, it is essential that these representations capture
relevant information from the whole signal. In this work, we propose to use a
Transformer-based model to process electrocardiograms (ECG) for emotion
recognition. Attention mechanisms of the Transformer can be used to build
contextualized representations for a signal, giving more importance to relevant
parts. These representations may then be processed with a fully-connected
network to predict emotions. To overcome the relatively small size of datasets
with emotional labels, we employ self-supervised learning. We gathered several
ECG datasets with no labels of emotion to pre-train our model, which we then
fine-tuned for emotion recognition on the AMIGOS dataset. We show that our
approach reaches state-of-the-art performances for emotion recognition using
ECG signals on AMIGOS. More generally, our experiments show that transformers
and pre-training are promising strategies for emotion recognition with
physiological signals.",https://code.engineering.queensu.ca/pritam/SSL-ECG,-1
Bringing Old Films Back to Life,0.91539,"We present a learning-based framework, recurrent transformer network (RTN),
to restore heavily degraded old films. Instead of performing frame-wise
restoration, our method is based on the hidden knowledge learned from adjacent
frames that contain abundant information about the occlusion, which is
beneficial to restore challenging artifacts of each frame while ensuring
temporal coherency. Moreover, contrasting the representation of the current
frame and the hidden knowledge makes it possible to infer the scratch position
in an unsupervised manner, and such defect localization generalizes well to
real-world degradations. To better resolve mixed degradation and compensate for
the flow estimation error during frame alignment, we propose to leverage more
expressive transformer blocks for spatial restoration. Experiments on both
synthetic dataset and real-world old films demonstrate the significant
superiority of the proposed RTN over existing solutions. In addition, the same
framework can effectively propagate the color from keyframes to the whole
video, ultimately yielding compelling restored films. The implementation and
model will be released at
https://github.com/raywzy/Bringing-Old-Films-Back-to-Life.",https://github.com/raywzy/Bringing-Old-Films-Back-to-Life,-1
Mitigating Attacks on Artificial Intelligence-based Spectrum Sensing for Cellular Network Signals,0.303256,"Cellular networks (LTE, 5G, and beyond) are dramatically growing with high
demand from consumers and more promising than the other wireless networks with
advanced telecommunication technologies. The main goal of these networks is to
connect billions of devices, systems, and users with high-speed data
transmission, high cell capacity, and low latency, as well as to support a wide
range of new applications, such as virtual reality, metaverse, telehealth,
online education, autonomous and flying vehicles, advanced manufacturing, and
many more. To achieve these goals, spectrum sensing has been paid more
attention, along with new approaches using artificial intelligence (AI) methods
for spectrum management in cellular networks. This paper provides a
vulnerability analysis of spectrum sensing approaches using AI-based semantic
segmentation models for identifying cellular network signals under adversarial
attacks with and without defensive distillation methods. The results showed
that mitigation methods can significantly reduce the vulnerabilities of
AI-based spectrum sensing models against adversarial attacks.",None,-1
OCFormer: One-Class Transformer Network for Image Classification,0.0778467,"We propose a novel deep learning framework based on Vision Transformers (ViT)
for one-class classification. The core idea is to use zero-centered Gaussian
noise as a pseudo-negative class for latent space representation and then train
the network using the optimal loss function. In prior works, there have been
tremendous efforts to learn a good representation using varieties of loss
functions, which ensures both discriminative and compact properties. The
proposed one-class Vision Transformer (OCFormer) is exhaustively experimented
on CIFAR-10, CIFAR-100, Fashion-MNIST and CelebA eyeglasses datasets. Our
method has shown significant improvements over competing CNN based one-class
classifier approaches.",None,3158
Bounding Counterfactuals under Selection Bias,0.586299,"Causal analysis may be affected by selection bias, which is defined as the
systematic exclusion of data from a certain subpopulation. Previous work in
this area focused on the derivation of identifiability conditions. We propose
instead a first algorithm to address both identifiable and unidentifiable
queries. We prove that, in spite of the missingness induced by the selection
bias, the likelihood of the available data is unimodal. This enables us to use
the causal expectation-maximisation scheme to obtain the values of causal
queries in the identifiable case, and to compute bounds otherwise. Experiments
demonstrate the approach to be practically viable. Theoretical convergence
characterisations are provided.",https://github.com/IDSIA-papers/2022-PGM-selection,-1
Interstellar Object Accessibility and Mission Design,0.434551,"Interstellar objects (ISOs) are fascinating and under-explored celestial
objects, providing physical laboratories to understand the formation of our
solar system and probe the composition and properties of material formed in
exoplanetary systems. This paper will discuss the accessibility of and mission
design to ISOs with varying characteristics, including a discussion of state
covariance estimation over the course of a cruise, handoffs from traditional
navigation approaches to novel autonomous navigation for fast flyby regimes,
and overall recommendations about preparing for the future in situ exploration
of these targets. The lessons learned also apply to the fast flyby of other
small bodies including long-period comets and potentially hazardous asteroids,
which also require a tactical response with similar characteristics",None,-1
Soft-Labeled Contrastive Pre-training for Function-level Code Representation,0.315772,"Code contrastive pre-training has recently achieved significant progress on
code-related tasks. In this paper, we present \textbf{SCodeR}, a
\textbf{S}oft-labeled contrastive pre-training framework with two positive
sample construction methods to learn functional-level \textbf{Code}
\textbf{R}epresentation. Considering the relevance between codes in a
large-scale code corpus, the soft-labeled contrastive pre-training can obtain
fine-grained soft-labels through an iterative adversarial manner and use them
to learn better code representation. The positive sample construction is
another key for contrastive pre-training. Previous works use
transformation-based methods like variable renaming to generate semantically
equal positive codes. However, they usually result in the generated code with a
highly similar surface form, and thus mislead the model to focus on superficial
code structure instead of code semantics. To encourage SCodeR to capture
semantic information from the code, we utilize code comments and abstract
syntax sub-trees of the code to build positive samples. We conduct experiments
on four code-related tasks over seven datasets. Extensive experimental results
show that SCodeR achieves new state-of-the-art performance on all of them,
which illustrates the effectiveness of the proposed pre-training method.",https://github.com/microsoft/AR2/tree/main/SCodeR,-1
Generalized Inter-class Loss for Gait Recognition,0.435356,"Gait recognition is a unique biometric technique that can be performed at a
long distance non-cooperatively and has broad applications in public safety and
intelligent traffic systems. Previous gait works focus more on minimizing the
intra-class variance while ignoring the significance in constraining
inter-class variance. To this end, we propose a generalized inter-class loss
which resolves the inter-class variance from both sample-level feature
distribution and class-level feature distribution. Instead of equal penalty
strength on pair scores, the proposed loss optimizes sample-level inter-class
feature distribution by dynamically adjusting the pairwise weight. Further, in
class-level distribution, generalized inter-class loss adds a constraint on the
uniformity of inter-class feature distribution, which forces the feature
representations to approximate a hypersphere and keep maximal inter-class
variance. In addition, the proposed method automatically adjusts the margin
between classes which enables the inter-class feature distribution to be more
flexible. The proposed method can be generalized to different gait recognition
networks and achieves significant improvements. We conduct a series of
experiments on CASIA-B and OUMVLP, and the experimental results show that the
proposed loss can significantly improve the performance and achieves the
state-of-the-art performances.",https://github.com/ShiqiYu/OpenGait.git,-1
Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries,0.805202,"Knowledge graph (KG) embeddings have been a mainstream approach for reasoning
over incomplete KGs. However, limited by their inherently shallow and static
architectures, they can hardly deal with the rising focus on complex logical
queries, which comprise logical operators, imputed edges, multiple source
entities, and unknown intermediate entities. In this work, we present the
Knowledge Graph Transformer (kgTransformer) with masked pre-training and
fine-tuning strategies. We design a KG triple transformation method to enable
Transformer to handle KGs, which is further strengthened by the
Mixture-of-Experts (MoE) sparse activation. We then formulate the complex
logical queries as masked prediction and introduce a two-stage masked
pre-training strategy to improve transferability and generalizability.
Extensive experiments on two benchmarks demonstrate that kgTransformer can
consistently outperform both KG embedding-based baselines and advanced encoders
on nine in-domain and out-of-domain reasoning tasks. Additionally,
kgTransformer can reason with explainability via providing the full reasoning
paths to interpret given answers.",https://github.com/THUDM/kgTransformer,-1
Unsupervised Domain Adaptive Salient Object Detection Through Uncertainty-Aware Pseudo-Label Learning,0.707335,"Recent advances in deep learning significantly boost the performance of
salient object detection (SOD) at the expense of labeling larger-scale
per-pixel annotations. To relieve the burden of labor-intensive labeling, deep
unsupervised SOD methods have been proposed to exploit noisy labels generated
by handcrafted saliency methods. However, it is still difficult to learn
accurate saliency details from rough noisy labels. In this paper, we propose to
learn saliency from synthetic but clean labels, which naturally has higher
pixel-labeling quality without the effort of manual annotations. Specifically,
we first construct a novel synthetic SOD dataset by a simple copy-paste
strategy. Considering the large appearance differences between the synthetic
and real-world scenarios, directly training with synthetic data will lead to
performance degradation on real-world scenarios. To mitigate this problem, we
propose a novel unsupervised domain adaptive SOD method to adapt between these
two domains by uncertainty-aware self-training. Experimental results show that
our proposed method outperforms the existing state-of-the-art deep unsupervised
SOD methods on several benchmark datasets, and is even comparable to
fully-supervised ones.",None,-1
"""This is Fake! Shared it by Mistake"": Assessing the Intent of Fake News Spreaders",0.887701,"Individuals can be misled by fake news and spread it unintentionally without
knowing it is false. This phenomenon has been frequently observed but has not
been investigated. Our aim in this work is to assess the intent of fake news
spreaders. To distinguish between intentional versus unintentional spreading,
we study the psychological explanations of unintentional spreading. With this
foundation, we then propose an influence graph, using which we assess the
intent of fake news spreaders. Our extensive experiments show that the assessed
intent can help significantly differentiate between intentional and
unintentional fake news spreaders. Furthermore, the estimated intent can
significantly improve the current techniques that detect fake news. To our best
knowledge, this is the first work to model individuals' intent in fake news
spreading.",https://github.com/flairNLP/flair,-1
Mix and Localize: Localizing Sound Sources in Mixtures,0.753976,"We present a method for simultaneously localizing multiple sound sources
within a visual scene. This task requires a model to both group a sound mixture
into individual sources, and to associate them with a visual signal. Our method
jointly solves both tasks at once, using a formulation inspired by the
contrastive random walk of Jabri et al. We create a graph in which images and
separated sounds correspond to nodes, and train a random walker to transition
between nodes from different modalities with high return probability. The
transition probabilities for this walk are determined by an audio-visual
similarity metric that is learned by our model. We show through experiments
with musical instruments and human speech that our model can successfully
localize multiple sounds, outperforming other self-supervised methods. Project
site: https://hxixixh.github.io/mix-and-localize",https://hxixixh.github.io/mix-and-localize,-1
A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering,0.458414,"Outside-knowledge visual question answering (OK-VQA) requires the agent to
comprehend the image, make use of relevant knowledge from the entire web, and
digest all the information to answer the question. Most previous works address
the problem by first fusing the image and question in the multi-modal space,
which is inflexible for further fusion with a vast amount of external
knowledge. In this paper, we call for a paradigm shift for the OK-VQA task,
which transforms the image into plain text, so that we can enable knowledge
passage retrieval, and generative question-answering in the natural language
space. This paradigm takes advantage of the sheer volume of gigantic knowledge
bases and the richness of pre-trained language models. A
Transform-Retrieve-Generate framework (TRiG) framework is proposed, which can
be plug-and-played with alternative image-to-text models and textual knowledge
bases. Experimental results show that our TRiG framework outperforms all
state-of-the-art supervised methods by at least 11.1% absolute margin.",https://github.com/JaidedAI/EasyOCR,-1
Identifying Spurious Correlations and Correcting them with an Explanation-based Learning,0.504485,"Identifying spurious correlations learned by a trained model is at the core
of refining a trained model and building a trustworthy model. We present a
simple method to identify spurious correlations that have been learned by a
model trained for image classification problems. We apply image-level
perturbations and monitor changes in certainties of predictions made using the
trained model. We demonstrate this approach using an image classification
dataset that contains images with synthetically generated spurious regions and
show that the trained model was overdependent on spurious regions. Moreover, we
remove the learned spurious correlations with an explanation based learning
approach.",None,-1
A Perspective on K-12 AI Education,0.384484,"Artificial intelligence (AI), which enables machines to learn to perform a
task by training on diverse datasets, is one of the most revolutionary
developments in scientific history. Although AI and especially deep learning is
relatively new, it has already had transformative impact on medicine, biology,
transportation, entertainment, and beyond. As AI changes our daily lives at an
increasingly fast pace, we are challenged with preparing our society for an
AI-driven future. To this end, a critical step is to ensure an AI-ready
workforce through education. Advocates of beginning instruction of AI basics at
the K-12 level typically note benefits to the workforce, economy, and national
security. In this complementary perspective, we discuss why learning AI is
beneficial for motivating students and promoting creative thinking, and how to
develop a module-based approach that optimizes learning outcomes. We hope to
excite and engage more members of the education community to join the effort to
advance K-12 AI education in the USA and worldwide.",None,-1
Feature Selective Transformer for Semantic Image Segmentation,0.108345,"Recently, it has attracted more and more attentions to fuse multi-scale
features for semantic image segmentation. Various works were proposed to employ
progressive local or global fusion, but the feature fusions are not rich enough
for modeling multi-scale context features. In this work, we focus on fusing
multi-scale features from Transformer-based backbones for semantic
segmentation, and propose a Feature Selective Transformer (FeSeFormer), which
aggregates features from all scales (or levels) for each query feature.
Specifically, we first propose a Scale-level Feature Selection (SFS) module,
which can choose an informative subset from the whole multi-scale feature set
for each scale, where those features that are important for the current scale
(or level) are selected and the redundant are discarded. Furthermore, we
propose a Full-scale Feature Fusion (FFF) module, which can adaptively fuse
features of all scales for queries. Based on the proposed SFS and FFF modules,
we develop a Feature Selective Transformer (FeSeFormer), and evaluate our
FeSeFormer on four challenging semantic segmentation benchmarks, including
PASCAL Context, ADE20K, COCO-Stuff 10K, and Cityscapes, outperforming the
state-of-the-art.",None,-1
DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing,0.327003,"In the field of representation learning on knowledge graphs (KGs), a
hyper-relational fact consists of a main triple and several auxiliary
attribute-value descriptions, which is considered more comprehensive and
specific than a triple-based fact. However, currently available
hyper-relational KG embedding methods in a single view are limited in
application because they weaken the hierarchical structure that represents the
affiliation between entities. To overcome this limitation, we propose a
dual-view hyper-relational KG structure (DH-KG) that contains a
hyper-relational instance view for entities and a hyper-relational ontology
view for concepts that are abstracted hierarchically from the entities. This
paper defines link prediction and entity typing tasks on DH-KG for the first
time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and
HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding
model based on GRAN encoders, HGNNs, and joint learning. DHGE outperforms
baseline models on DH-KG, according to experimental results. Finally, we
provide an example of how this technology can be used to treat hypertension.
Our model and new datasets are publicly available.",https://github.com/LHRLAB/DHGE,1107
Safe and Robust Experience Sharing for Deterministic Policy Gradient Algorithms,0.242476,"Learning in high dimensional continuous tasks is challenging, mainly when the
experience replay memory is very limited. We introduce a simple yet effective
experience sharing mechanism for deterministic policies in continuous action
domains for the future off-policy deep reinforcement learning applications in
which the allocated memory for the experience replay buffer is limited. To
overcome the extrapolation error induced by learning from other agents'
experiences, we facilitate our algorithm with a novel off-policy correction
technique without any action probability estimates. We test the effectiveness
of our method in challenging OpenAI Gym continuous control tasks and conclude
that it can achieve a safe experience sharing across multiple agents and
exhibits a robust performance when the replay memory is strictly limited.",https://github.com/baturaysaglam/DASE,644
"Towards Trustworthy AutoGrading of Short, Multi-lingual, Multi-type Answers",0.997875,"Autograding short textual answers has become much more feasible due to the
rise of NLP and the increased availability of question-answer pairs brought
about by a shift to online education. Autograding performance is still inferior
to human grading. The statistical and black-box nature of state-of-the-art
machine learning models makes them untrustworthy, raising ethical concerns and
limiting their practical utility. Furthermore, the evaluation of autograding is
typically confined to small, monolingual datasets for a specific question type.
This study uses a large dataset consisting of about 10 million question-answer
pairs from multiple languages covering diverse fields such as math and
language, and strong variation in question and answer syntax. We demonstrate
the effectiveness of fine-tuning transformer models for autograding for such
complex datasets. Our best hyperparameter-tuned model yields an accuracy of
about 86.5\%, comparable to the state-of-the-art models that are less general
and more tuned to a specific type of question, subject, and language. More
importantly, we address trust and ethical concerns. By involving humans in the
autograding process, we show how to improve the accuracy of automatically
graded answers, achieving accuracy equivalent to that of teaching assistants.
We also show how teachers can effectively control the type of errors made by
the system and how they can validate efficiently that the autograder's
performance on individual exams is close to the expected performance.",None,-1
MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering,0.783259,"Visual language data such as plots, charts, and infographics are ubiquitous
in the human world. However, state-of-the-art vision-language models do not
perform well on these data. We propose MatCha (Math reasoning and Chart
derendering pretraining) to enhance visual language models' capabilities in
jointly modeling charts/plots and language data. Specifically, we propose
several pretraining tasks that cover plot deconstruction and numerical
reasoning which are the key capabilities in visual language modeling.
  We perform the MatCha pretraining starting from Pix2Struct, a recently
proposed image-to-text visual language model. On standard benchmarks such as
PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as
much as nearly 20%. We also examine how well MatCha pretraining transfers to
domains such as screenshots, textbook diagrams, and document figures and
observe overall improvement, verifying the usefulness of MatCha pretraining on
broader visual language tasks.",https://github.com/google-research/google-research/tree/master/deplot,-1
PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,0.414396,"While contextualized word embeddings have been a de-facto standard, learning
contextualized phrase embeddings is less explored and being hindered by the
lack of a human-annotated benchmark that tests machine understanding of phrase
semantics given a context sentence or paragraph (instead of phrases alone). To
fill this gap, we propose PiC -- a dataset of ~28K of noun phrases accompanied
by their contextual Wikipedia pages and a suite of three tasks for training and
evaluating phrase embeddings. Training on PiC improves ranking models' accuracy
and remarkably pushes span-selection (SS) models (i.e., predicting the start
and end index of the target phrase) near-human accuracy, which is 95% Exact
Match (EM) on semantic search given a query phrase and a passage.
Interestingly, we find evidence that such impressive performance is because the
SS models learn to better capture the common meaning of a phrase regardless of
its actual context. SotA models perform poorly in distinguishing two senses of
the same phrase in two contexts (~60% EM) and in estimating the similarity
between two different phrases in the same context (~70% EM).",https://phrase-in-context.github.io,-1
Perceptual Quality Assessment of Omnidirectional Images,0.993223,"Omnidirectional images and videos can provide immersive experience of
real-world scenes in Virtual Reality (VR) environment. We present a perceptual
omnidirectional image quality assessment (IQA) study in this paper since it is
extremely important to provide a good quality of experience under the VR
environment. We first establish an omnidirectional IQA (OIQA) database, which
includes 16 source images and 320 distorted images degraded by 4 commonly
encountered distortion types, namely JPEG compression, JPEG2000 compression,
Gaussian blur and Gaussian noise. Then a subjective quality evaluation study is
conducted on the OIQA database in the VR environment. Considering that humans
can only see a part of the scene at one movement in the VR environment, visual
attention becomes extremely important. Thus we also track head and eye movement
data during the quality rating experiments. The original and distorted
omnidirectional images, subjective quality ratings, and the head and eye
movement data together constitute the OIQA database. State-of-the-art
full-reference (FR) IQA measures are tested on the OIQA database, and some new
observations different from traditional IQA are made.",None,-1
Block-NeRF: Scalable Large Scene Neural View Synthesis,0.999458,"We present Block-NeRF, a variant of Neural Radiance Fields that can represent
large-scale environments. Specifically, we demonstrate that when scaling NeRF
to render city-scale scenes spanning multiple blocks, it is vital to decompose
the scene into individually trained NeRFs. This decomposition decouples
rendering time from scene size, enables rendering to scale to arbitrarily large
environments, and allows per-block updates of the environment. We adopt several
architectural changes to make NeRF robust to data captured over months under
different environmental conditions. We add appearance embeddings, learned pose
refinement, and controllable exposure to each individual NeRF, and introduce a
procedure for aligning appearance between adjacent NeRFs so that they can be
seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to
create the largest neural scene representation to date, capable of rendering an
entire neighborhood of San Francisco.",None,-1
Improving the Adversarial Robustness of NLP Models by Information Bottleneck,0.679301,"Existing studies have demonstrated that adversarial examples can be directly
attributed to the presence of non-robust features, which are highly predictive,
but can be easily manipulated by adversaries to fool NLP models. In this study,
we explore the feasibility of capturing task-specific robust features, while
eliminating the non-robust ones by using the information bottleneck theory.
Through extensive experiments, we show that the models trained with our
information bottleneck-based method are able to achieve a significant
improvement in robust accuracy, exceeding performances of all the previously
reported defense methods while suffering almost no performance drop in clean
accuracy on SST-2, AGNEWS and IMDB datasets.",https://github.com/zhangcen456/IB,-1
DisPositioNet: Disentangled Pose and Identity in Semantic Image Manipulation,0.0735788,"Graph representation of objects and their relations in a scene, known as a
scene graph, provides a precise and discernible interface to manipulate a scene
by modifying the nodes or the edges in the graph. Although existing works have
shown promising results in modifying the placement and pose of objects, scene
manipulation often leads to losing some visual characteristics like the
appearance or identity of objects. In this work, we propose DisPositioNet, a
model that learns a disentangled representation for each object for the task of
image manipulation using scene graphs in a self-supervised manner. Our
framework enables the disentanglement of the variational latent embeddings as
well as the feature representation in the graph. In addition to producing more
realistic images due to the decomposition of features like pose and identity,
our method takes advantage of the probabilistic sampling in the intermediate
features to generate more diverse images in object replacement or addition
tasks. The results of our experiments show that disentangling the feature
representations in the latent manifold of the model outperforms the previous
works qualitatively and quantitatively on two public benchmarks. Project Page:
https://scenegenie.github.io/DispositioNet/",https://scenegenie.github.io/DispositioNet/,-1
Combining Static and Contextualised Multilingual Embeddings,0.162167,"Static and contextual multilingual embeddings have complementary strengths.
Static embeddings, while less expressive than contextual language models, can
be more straightforwardly aligned across multiple languages. We combine the
strengths of static and contextual models to improve multilingual
representations. We extract static embeddings for 40 languages from XLM-R,
validate those embeddings with cross-lingual word retrieval, and then align
them using VecMap. This results in high-quality, highly multilingual static
embeddings. Then we apply a novel continued pre-training approach to XLM-R,
leveraging the high quality alignment of our static embeddings to better align
the representation space of XLM-R. We show positive results for multiple
complex semantic tasks. We release the static embeddings and the continued
pre-training code. Unlike most previous work, our continued pre-training
approach does not require parallel text.",https://github.com/KathyHaem/combining-static-contextual,-1
Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,0.763926,"Language models have been shown to perform better with an increase in scale
on a wide variety of tasks via the in-context learning paradigm. In this paper,
we investigate the hypothesis that the ability of a large language model to
in-context learn-perform a task is not uniformly spread across all of its
underlying components. Using a 66 billion parameter language model (OPT-66B)
across a diverse set of 14 downstream tasks, we find this is indeed the case:
$\sim$70% of attention heads and $\sim$20% of feed forward networks can be
removed with minimal decline in task performance. We find substantial overlap
in the set of attention heads (un)important for in-context learning across
tasks and number of in-context examples. We also address our hypothesis through
a task-agnostic lens, finding that a small set of attention heads in OPT-66B
score highly on their ability to perform primitive induction operations
associated with in-context learning, namely, prefix matching and copying. These
induction heads overlap with task-specific important heads, reinforcing
arguments by Olsson et al. (arXiv:2209.11895) regarding induction head
generality to more sophisticated behaviors associated with in-context learning.
Overall, our study provides several insights that indicate large language
models may be under-trained for in-context learning and opens up questions on
how to pre-train language models to more effectively perform in-context
learning.",github.com/amazon-science/llm-interpret,42346
Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection,0.820799,"Language models increasingly rely on massive web dumps for diverse text data.
However, these sources are rife with undesirable content. As such, resources
like Wikipedia, books, and newswire often serve as anchors for automatically
selecting web text most suitable for language modeling, a process typically
referred to as quality filtering. Using a new dataset of U.S. high school
newspaper articles -- written by students from across the country -- we
investigate whose language is preferred by the quality filter used for GPT-3.
We find that newspapers from larger schools, located in wealthier, educated,
and urban ZIP codes are more likely to be classified as high quality. We then
demonstrate that the filter's measurement of quality is unaligned with other
sensible metrics, such as factuality or literary acclaim. We argue that
privileging any corpus as high quality entails a language ideology, and more
care is needed to construct training corpora for language models, with better
transparency and justification for the inclusion or exclusion of various texts.",https://github.com/kernelmachine/quality-filter,-1
FisheyeHDK: Hyperbolic Deformable Kernel Learning for Ultra-Wide Field-of-View Image Recognition,0.793149,"Conventional convolution neural networks (CNNs) trained on narrow
Field-of-View (FoV) images are the state-of-the-art approaches for object
recognition tasks. Some methods proposed the adaptation of CNNs to ultra-wide
FoV images by learning deformable kernels. However, they are limited by the
Euclidean geometry and their accuracy degrades under strong distortions caused
by fisheye projections. In this work, we demonstrate that learning the shape of
convolution kernels in non-Euclidean spaces is better than existing deformable
kernel methods. In particular, we propose a new approach that learns deformable
kernel parameters (positions) in hyperbolic space. FisheyeHDK is a hybrid CNN
architecture combining hyperbolic and Euclidean convolution layers for
positions and features learning. First, we provide an intuition of hyperbolic
space for wide FoV images. Using synthetic distortion profiles, we demonstrate
the effectiveness of our approach. We select two datasets - Cityscapes and
BDD100K 2020 - of perspective images which we transform to fisheye equivalents
at different scaling factors (analog to focal lengths). Finally, we provide an
experiment on data collected by a real fisheye camera. Validations and
experiments show that our approach improves existing deformable kernel methods
for CNN adaptation on fisheye images.",None,-1
Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting,0.987607,"In document classification for, e.g., legal and biomedical text, we often
deal with hundreds of classes, including very infrequent ones, as well as
temporal concept drift caused by the influence of real world events, e.g.,
policy changes, conflicts, or pandemics. Class imbalance and drift can
sometimes be mitigated by resampling the training data to simulate (or
compensate for) a known target distribution, but what if the target
distribution is determined by unknown future events? Instead of simply
resampling uniformly to hedge our bets, we focus on the underlying optimization
algorithms used to train such document classifiers and evaluate several
group-robust optimization algorithms, initially proposed to mitigate
group-level disparities. Reframing group-robust algorithms as adaptation
algorithms under concept drift, we find that Invariant Risk Minimization and
Spectral Decoupling outperform sampling-based approaches to class imbalance and
concept drift, and lead to much better performance on minority classes. The
effect is more pronounced the larger the label set.",https://github.com/coastalcph/lw-robust,-1
Proper Reuse of Image Classification Features Improves Object Detection,0.391092,"A common practice in transfer learning is to initialize the downstream model
weights by pre-training on a data-abundant upstream task. In object detection
specifically, the feature backbone is typically initialized with Imagenet
classifier weights and fine-tuned on the object detection task. Recent works
show this is not strictly necessary under longer training regimes and provide
recipes for training the backbone from scratch. We investigate the opposite
direction of this end-to-end training trend: we show that an extreme form of
knowledge preservation -- freezing the classifier-initialized backbone --
consistently improves many different detection models, and leads to
considerable resource savings. We hypothesize and corroborate experimentally
that the remaining detector components capacity and structure is a crucial
factor in leveraging the frozen backbone. Immediate applications of our
findings include performance improvements on hard cases like detection of
long-tail object classes and computational and memory resource savings that
contribute to making the field more accessible to researchers with access to
fewer computational resources.",https://github.com/tensorflow/models/blob/master/official/projects/backbone_reuse/README.md,-1
Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology,0.694489,"Tissue phenotyping is a fundamental task in learning objective
characterizations of histopathologic biomarkers within the tumor-immune
microenvironment in cancer pathology. However, whole-slide imaging (WSI) is a
complex computer vision in which: 1) WSIs have enormous image resolutions with
precludes large-scale pixel-level efforts in data curation, and 2) diversity of
morphological phenotypes results in inter- and intra-observer variability in
tissue labeling. To address these limitations, current efforts have proposed
using pretrained image encoders (transfer learning from ImageNet,
self-supervised pretraining) in extracting morphological features from
pathology, but have not been extensively validated. In this work, we conduct a
search for good representations in pathology by training a variety of
self-supervised models with validation on a variety of weakly-supervised and
patch-level tasks. Our key finding is in discovering that Vision Transformers
using DINO-based knowledge distillation are able to learn data-efficient and
interpretable features in histology images wherein the different attention
heads learn distinct morphological phenotypes. We make evaluation code and
pretrained weights publicly-available at:
https://github.com/Richarizardd/Self-Supervised-ViT-Path.",https://github.com/Richarizardd/Self-Supervised-ViT-Path,-1
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations,0.102238,"Due to the huge amount of parameters, fine-tuning of pretrained language
models (PLMs) is prone to overfitting in the low resource scenarios. In this
work, we present a novel method that operates on the hidden representations of
a PLM to reduce overfitting. During fine-tuning, our method inserts random
autoencoders between the hidden layers of a PLM, which transform activations
from the previous layers into multi-view compressed representations before
feeding them into the upper layers. The autoencoders are plugged out after
fine-tuning, so our method does not add extra parameters or increase
computation cost during inference. Our method demonstrates promising
performance improvement across a wide range of sequence- and token-level
low-resource NLP tasks.",https://github.com/DAMO-NLP-SG/MVCR,11730
Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning,0.940425,"In this paper, we study the named entity recognition (NER) problem under
distant supervision. Due to the incompleteness of the external dictionaries
and/or knowledge bases, such distantly annotated training data usually suffer
from a high false negative rate. To this end, we formulate the Distantly
Supervised NER (DS-NER) problem via Multi-class Positive and Unlabeled (MPU)
learning and propose a theoretically and practically novel CONFidence-based MPU
(Conf-MPU) approach. To handle the incomplete annotations, Conf-MPU consists of
two steps. First, a confidence score is estimated for each token of being an
entity token. Then, the proposed Conf-MPU risk estimation is applied to train a
multi-class classifier for the NER task. Thorough experiments on two benchmark
datasets labeled by various external knowledge demonstrate the superiority of
the proposed Conf-MPU over existing DS-NER methods.",https://github.com/shangjingbo1226/AutoNER,3269
A Real-Time Fusion Framework for Long-term Visual Localization,0.233637,"Visual localization is a fundamental task that regresses the 6 Degree Of
Freedom (6DoF) poses with image features in order to serve the high precision
localization requests in many robotics applications. Degenerate conditions like
motion blur, illumination changes and environment variations place great
challenges in this task. Fusion with additional information, such as sequential
information and Inertial Measurement Unit (IMU) inputs, would greatly assist
such problems. In this paper, we present an efficient client-server visual
localization architecture that fuses global and local pose estimations to
realize promising precision and efficiency. We include additional geometry
hints in mapping and global pose regressing modules to improve the measurement
quality. A loosely coupled fusion policy is adopted to leverage the computation
complexity and accuracy. We conduct the evaluations on two typical open-source
benchmarks, 4Seasons and OpenLORIS. Quantitative results prove that our
framework has competitive performance with respect to other state-of-the-art
visual localization solutions.",None,5803
Understanding Translationese in Cross-Lingual Summarization,0.864011,"Given a document in a source language, cross-lingual summarization (CLS) aims
at generating a concise summary in a different target language. Unlike
monolingual summarization (MS), naturally occurring source-language documents
paired with target-language summaries are rare. To collect large-scale CLS
data, existing datasets typically involve translation in their creation.
However, the translated text is distinguished from the text originally written
in that language, i.e., translationese. In this paper, we first confirm that
different approaches of constructing CLS datasets will lead to different
degrees of translationese. Then we systematically investigate how
translationese affects CLS model evaluation and performance when it appears in
source documents or target summaries. In detail, we find that (1) the
translationese in documents or summaries of test sets might lead to the
discrepancy between human judgment and automatic evaluation; (2) the
translationese in training sets would harm model performance in real-world
applications; (3) though machine-translated documents involve translationese,
they are very useful for building CLS systems on low-resource languages under
specific training strategies. Lastly, we give suggestions for future CLS
research including dataset and model developments. We hope that our work could
let researchers notice the phenomenon of translationese in CLS and take it into
account in the future.",https://github.com/xcfcode/MSAMSum,-1
Can Visual Context Improve Automatic Speech Recognition for an Embodied Agent?,0.282516,"The usage of automatic speech recognition (ASR) systems are becoming
omnipresent ranging from personal assistant to chatbots, home, and industrial
automation systems, etc. Modern robots are also equipped with ASR capabilities
for interacting with humans as speech is the most natural interaction modality.
However, ASR in robots faces additional challenges as compared to a personal
assistant. Being an embodied agent, a robot must recognize the physical
entities around it and therefore reliably recognize the speech containing the
description of such entities. However, current ASR systems are often unable to
do so due to limitations in ASR training, such as generic datasets and
open-vocabulary modeling. Also, adverse conditions during inference, such as
noise, accented, and far-field speech makes the transcription inaccurate. In
this work, we present a method to incorporate a robot's visual information into
an ASR system and improve the recognition of a spoken utterance containing a
visible entity. Specifically, we propose a new decoder biasing technique to
incorporate the visual context while ensuring the ASR output does not degrade
for incorrect context. We achieve a 59% relative reduction in WER from an
unmodified ASR system.",https://github.com/mozilla/TTS,-1
Semi-Supervised Learning of Optical Flow by Flow Supervisor,0.552894,"A training pipeline for optical flow CNNs consists of a pretraining stage on
a synthetic dataset followed by a fine tuning stage on a target dataset.
However, obtaining ground truth flows from a target video requires a tremendous
effort. This paper proposes a practical fine tuning method to adapt a
pretrained model to a target dataset without ground truth flows, which has not
been explored extensively. Specifically, we propose a flow supervisor for
self-supervision, which consists of parameter separation and a student output
connection. This design is aimed at stable convergence and better accuracy over
conventional self-supervision methods which are unstable on the fine tuning
task. Experimental results show the effectiveness of our method compared to
different self-supervision methods for semi-supervised learning. In addition,
we achieve meaningful improvements over state-of-the-art optical flow models on
Sintel and KITTI benchmarks by exploiting additional unlabeled datasets. Code
is available at https://github.com/iwbn/flow-supervisor.",https://github.com/iwbn/flow-supervisor,-1
Monte Carlo Tree Search based Variable Selection for High Dimensional Bayesian Optimization,0.806896,"Bayesian optimization (BO) is a class of popular methods for expensive
black-box optimization, and has been widely applied to many scenarios. However,
BO suffers from the curse of dimensionality, and scaling it to high-dimensional
problems is still a challenge. In this paper, we propose a variable selection
method MCTS-VS based on Monte Carlo tree search (MCTS), to iteratively select
and optimize a subset of variables. That is, MCTS-VS constructs a
low-dimensional subspace via MCTS and optimizes in the subspace with any BO
algorithm. We give a theoretical analysis of the general variable selection
method to reveal how it can work. Experiments on high-dimensional synthetic
functions and real-world problems (i.e., NAS-bench problems and MuJoCo
locomotion tasks) show that MCTS-VS equipped with a proper BO optimizer can
achieve state-of-the-art performance.",https://github.com/lamda-bbo/MCTS-VS,-1
A Novel Neural Network Training Method for Autonomous Driving Using Semi-Pseudo-Labels and 3D Data Augmentations,0.0548285,"Training neural networks to perform 3D object detection for autonomous
driving requires a large amount of diverse annotated data. However, obtaining
training data with sufficient quality and quantity is expensive and sometimes
impossible due to human and sensor constraints. Therefore, a novel solution is
needed for extending current training methods to overcome this limitation and
enable accurate 3D object detection. Our solution for the above-mentioned
problem combines semi-pseudo-labeling and novel 3D augmentations. For
demonstrating the applicability of the proposed method, we have designed a
convolutional neural network for 3D object detection which can significantly
increase the detection range in comparison with the training data distribution.",None,-1
Shapley Head Pruning: Identifying and Removing Interference in Multilingual Transformers,0.0542663,"Multilingual transformer-based models demonstrate remarkable zero and
few-shot transfer across languages by learning and reusing language-agnostic
features. However, as a fixed-size model acquires more languages, its
performance across all languages degrades, a phenomenon termed interference.
Often attributed to limited model capacity, interference is commonly addressed
by adding additional parameters despite evidence that transformer-based models
are overparameterized. In this work, we show that it is possible to reduce
interference by instead identifying and pruning language-specific parameters.
First, we use Shapley Values, a credit allocation metric from coalitional game
theory, to identify attention heads that introduce interference. Then, we show
that removing identified attention heads from a fixed model improves
performance for a target language on both sentence classification and
structural prediction, seeing gains as large as 24.7\%. Finally, we provide
insights on language-agnostic and language-specific attention heads using
attention visualization.",None,-1
Clinical Dialogue Transcription Error Correction using Seq2Seq Models,0.193411,"Good communication is critical to good healthcare. Clinical dialogue is a
conversation between health practitioners and their patients, with the explicit
goal of obtaining and sharing medical information. This information contributes
to medical decision-making regarding the patient and plays a crucial role in
their healthcare journey. The reliance on note taking and manual scribing
processes are extremely inefficient and leads to manual transcription errors
when digitizing notes. Automatic Speech Recognition (ASR) plays a significant
role in speech-to-text applications, and can be directly used as a text
generator in conversational applications. However, recording clinical dialogue
presents a number of general and domain-specific challenges. In this paper, we
present a seq2seq learning approach for ASR transcription error correction of
clinical dialogues. We introduce a new Gastrointestinal Clinical Dialogue (GCD)
Dataset which was gathered by healthcare professionals from a NHS Inflammatory
Bowel Disease clinic and use this in a comparative study with four commercial
ASR systems. Using self-supervision strategies, we fine-tune a seq2seq model on
a mask-filling task using a domain-specific PubMed dataset which we have shared
publicly for future research. The BART model fine-tuned for mask-filling was
able to correct transcription errors and achieve lower word error rates for
three out of four commercial ASR outputs.",None,-1
CorefDiffs: Co-referential and Differential Knowledge Flow in Document Grounded Conversations,0.46327,"Knowledge-grounded dialog systems need to incorporate smooth transitions
among knowledge selected for generating responses, to ensure that dialog flows
naturally. For document-grounded dialog systems, the inter- and intra-document
knowledge relations can be used to model such conversational flows. We develop
a novel Multi-Document Co-Referential Graph (Coref-MDG) to effectively capture
the inter-document relationships based on commonsense and similarity and the
intra-document co-referential structures of knowledge segments within the
grounding documents. We propose CorefDiffs, a Co-referential and Differential
flow management method, to linearize the static Coref-MDG into conversational
sequence logic. CorefDiffs performs knowledge selection by accounting for
contextual graph structures and the knowledge difference sequences. CorefDiffs
significantly outperforms the state-of-the-art by 9.5\%, 7.4\%, and 8.2\% on
three public benchmarks. This demonstrates that the effective modeling of
co-reference and knowledge difference for dialog flows are critical for
transitions in document-grounded conversation",https://github.com/cathyxl/coref-diffs,-1
ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations,0.70853,"Precise representations of 3D faces are beneficial to various computer vision
and graphics applications. Due to the data discretization and model linearity,
however, it remains challenging to capture accurate identity and expression
clues in current studies. This paper presents a novel 3D morphable face model,
namely ImFace, to learn a nonlinear and continuous space with implicit neural
representations. It builds two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
and designs an improved learning strategy to extend embeddings of expressions
to allow more diverse changes. We further introduce a Neural Blend-Field to
learn sophisticated details by adaptively blending a series of local fields. In
addition to ImFace, an effective preprocessing pipeline is proposed to address
the issue of watertight input requirement in implicit representations, enabling
them to work with common facial surfaces for the first time. Extensive
experiments are performed to demonstrate the superiority of ImFace.",https://github.com/MingwuZheng/ImFace,11135
A Few-shot Approach to Resume Information Extraction via Prompts,0.116009,"Prompt learning's fine-tune performance on text classification tasks has
attracted the NLP community. This paper applies it to resume information
extraction, improving existing methods for this task. We created manual
templates and verbalizers tailored to resume texts and compared the performance
of Masked Language Model (MLM) and Seq2Seq PLMs. Also, we enhanced the
verbalizer design for Knowledgeable Prompt-tuning, contributing to prompt
template design across NLP tasks. We present the Manual Knowledgeable
Verbalizer (MKV), a rule for constructing verbalizers for specific
applications. Our tests show that MKV rules yield more effective, robust
templates and verbalizers than existing methods. Our MKV approach resolved
sample imbalance, surpassing current automatic prompt methods. This study
underscores the value of tailored prompt learning for resume extraction,
stressing the importance of custom-designed templates and verbalizers.",https://github.com/thunlp/OpenPrompt,-1
Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection,0.887628,"In some scenarios, classifier requires detecting out-of-distribution samples
far from its training data. With desirable characteristics, reconstruction
autoencoder-based methods deal with this problem by using input reconstruction
error as a metric of novelty vs. normality. We formulate the essence of such
approach as a quadruplet domain translation with an intrinsic bias to only
query for a proxy of conditional data uncertainty. Accordingly, an improvement
direction is formalized as maximumly compressing the autoencoder's latent space
while ensuring its reconstructive power for acting as a described domain
translator. From it, strategies are introduced including semantic
reconstruction, data certainty decomposition and normalized L2 distance to
substantially improve original methods, which together establish
state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of
CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method
works without any additional data, hard-to-implement structure, time-consuming
pipeline, and even harming the classification accuracy of known classes.",https://github.com/xxx,-1
C2F-TCN: A Framework for Semi and Fully Supervised Temporal Action Segmentation,0.634983,"Temporal action segmentation tags action labels for every frame in an input
untrimmed video containing multiple actions in a sequence. For the task of
temporal action segmentation, we propose an encoder-decoder-style architecture
named C2F-TCN featuring a ""coarse-to-fine"" ensemble of decoder outputs. The
C2F-TCN framework is enhanced with a novel model agnostic temporal feature
augmentation strategy formed by the computationally inexpensive strategy of the
stochastic max-pooling of segments. It produces more accurate and
well-calibrated supervised results on three benchmark action segmentation
datasets. We show that the architecture is flexible for both supervised and
representation learning. In line with this, we present a novel unsupervised way
to learn frame-wise representation from C2F-TCN. Our unsupervised learning
approach hinges on the clustering capabilities of the input features and the
formation of multi-resolution features from the decoder's implicit structure.
Further, we provide the first semi-supervised temporal action segmentation
results by merging representation learning with conventional supervised
learning. Our semi-supervised learning scheme, called
``Iterative-Contrastive-Classify (ICC)'', progressively improves in performance
with more labeled data. The ICC semi-supervised learning in C2F-TCN, with 40%
labeled videos, performs similar to fully supervised counterparts.",None,-1
Probing Speech Emotion Recognition Transformers for Linguistic Knowledge,0.820586,"Large, pre-trained neural networks consisting of self-attention layers
(transformers) have recently achieved state-of-the-art results on several
speech emotion recognition (SER) datasets. These models are typically
pre-trained in self-supervised manner with the goal to improve automatic speech
recognition performance -- and thus, to understand linguistic information. In
this work, we investigate the extent in which this information is exploited
during SER fine-tuning. Using a reproducible methodology based on open-source
tools, we synthesise prosodically neutral speech utterances while varying the
sentiment of the text. Valence predictions of the transformer model are very
reactive to positive and negative sentiment content, as well as negations, but
not to intensifiers or reducers, while none of those linguistic features impact
arousal or dominance. These findings show that transformers can successfully
leverage linguistic information to improve their valence predictions, and that
linguistic analysis should be included in their testing.",https://github.com/espnet/espnet,-1
Subword Segmental Language Modelling for Nguni Languages,0.366644,"Subwords have become the standard units of text in NLP, enabling efficient
open-vocabulary models. With algorithms like byte-pair encoding (BPE), subword
segmentation is viewed as a preprocessing step applied to the corpus before
training. This can lead to sub-optimal segmentations for low-resource languages
with complex morphologies. We propose a subword segmental language model (SSLM)
that learns how to segment words while being trained for autoregressive
language modelling. By unifying subword segmentation and language modelling,
our model learns subwords that optimise LM performance. We train our model on
the 4 Nguni languages of South Africa. These are low-resource agglutinative
languages, so subword information is critical. As an LM, SSLM outperforms
existing approaches such as BPE-based models on average across the 4 languages.
Furthermore, it outperforms standard subword segmenters on unsupervised
morphological segmentation. We also train our model as a word-level sequence
model, resulting in an unsupervised morphological segmenter that outperforms
existing methods by a large margin for all 4 languages. Our results show that
learning subword segmentation is an effective alternative to existing subword
segmenters, enabling the model to discover morpheme-like subwords that improve
its LM capabilities.",https://github.com/francois-meyer/subword-segmental-lm,3337
Confidence estimation of classification based on the distribution of the neural network output layer,0.365625,"One of the most common problems preventing the application of prediction
models in the real world is lack of generalization: The accuracy of models,
measured in the benchmark does repeat itself on future data, e.g. in the
settings of real business. There is relatively little methods exist that
estimate the confidence of prediction models. In this paper, we propose novel
methods that, given a neural network classification model, estimate uncertainty
of particular predictions generated by this model. Furthermore, we propose a
method that, given a model and a confidence level, calculates a threshold that
separates prediction generated by this model into two subsets, one of them
meets the given confidence level. In contrast to other methods, the proposed
methods do not require any changes on existing neural networks, because they
simply build on the output logit layer of a common neural network. In
particular, the methods infer the confidence of a particular prediction based
on the distribution of the logit values corresponding to this prediction. The
proposed methods constitute a tool that is recommended for filtering
predictions in the process of knowledge extraction, e.g. based on web
scrapping, where predictions subsets are identified that maximize the precision
on cost of the recall, which is less important due to the availability of data.
The method has been tested on different tasks including relation extraction,
named entity recognition and image classification to show the significant
increase of accuracy achieved.",None,-1
Compressing Transformer-based self-supervised models for speech processing,0.0569188,"Despite the success of Transformers in self- supervised learning with
applications to various downstream tasks, the computational cost of training
and inference remains a major challenge for applying these models to a wide
spectrum of devices. Several isolated attempts have been made to compress
Transformers, but the settings and metrics are different across studies.
Trade-off at various compression rates are also largely missing in prior work,
making it difficult to compare compression techniques. In this work, we aim to
provide context for the isolated results, studying several commonly used
compression techniques, including weight pruning, head pruning, low-rank
approximation, and knowledge distillation. We report trade- off at various
compression rate, including wall-clock time, the number of parameters, and the
number of multiply-accumulate operations. Our results show that compared to
recent approaches, basic compression techniques are strong baselines. We
further present several applications of our results, revealing properties of
Transformers, such as the significance of diagonal attention heads. In
addition, our results lead to a simple combination of compression techniques
that improves trade-off over recent approaches. We hope the results would
promote more diverse comparisons among model compression techniques and promote
the use of model compression as a tool for analyzing models. Our code of
compressing speech self-supervised model is available at
https://github.com/nervjack2/Speech-SSL-Compression/.",https://github.com/nervjack2/Speech-SSL-Compression/,-1
FL-Tuning: Layer Tuning for Feed-Forward Network in Transformer,0.0770613,"Prompt tuning is an emerging way of adapting pre-trained language models to
downstream tasks. However, the existing studies are mainly to add prompts to
the input sequence. This way would not work as expected due to the intermediate
multi-head self-attention and feed-forward network computation, making model
optimization not very smooth. Hence, we propose a novel tuning way called layer
tuning, aiming to add learnable parameters in Transformer layers. Specifically,
we focus on layer tuning for feed-forward network in the Transformer, namely
FL-tuning. It introduces additional units into the hidden layer of each
feed-forward network. We conduct extensive experiments on the public CLUE
benchmark. The results show that: 1) Our FL-tuning outperforms prompt tuning
methods under both full-data and few-shot settings in almost all cases. In
particular, it improves accuracy by 17.93% (full-data setting) on WSC 1.0 and
F1 by 16.142% (few-shot setting) on CLUENER over P-tuning v2. 2) Our FL-tuning
is more stable and converges about 1.17 times faster than P-tuning v2. 3) With
only about 3% of Transformer's parameters to be trained, FL-tuning is
comparable with fine-tuning on most datasets, and significantly outperforms
fine-tuning (e.g., accuracy improved by 12.9% on WSC 1.1) on several datasets.
The source codes are available at https://github.com/genggui001/FL-Tuning.",https://github.com/genggui001/FL-Tuning,-1
What are the best systems? New perspectives on NLP Benchmarking,0.262474,"In Machine Learning, a benchmark refers to an ensemble of datasets associated
with one or multiple metrics together with a way to aggregate different systems
performances. They are instrumental in (i) assessing the progress of new
methods along different axes and (ii) selecting the best systems for practical
use. This is particularly the case for NLP with the development of large
pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a
variety of tasks. While the community mainly focused on developing new datasets
and metrics, there has been little interest in the aggregation procedure, which
is often reduced to a simple average over various performance measures.
However, this procedure can be problematic when the metrics are on a different
scale, which may lead to spurious conclusions. This paper proposes a new
procedure to rank systems based on their performance across different tasks.
Motivated by the social choice theory, the final system ordering is obtained
through aggregating the rankings induced by each task and is theoretically
grounded. We conduct extensive numerical experiments (on over 270k scores) to
assess the soundness of our approach both on synthetic and real scores (e.g.
GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method
yields different conclusions on state-of-the-art systems than the
mean-aggregation procedure while being both more reliable and robust.",None,-1
Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation,0.551782,"Back-translation is a critical component of Unsupervised Neural Machine
Translation (UNMT), which generates pseudo parallel data from target
monolingual data. A UNMT model is trained on the pseudo parallel data with
translated source, and translates natural source sentences in inference. The
source discrepancy between training and inference hinders the translation
performance of UNMT models. By carefully designing experiments, we identify two
representative characteristics of the data gap in source: (1) style gap (i.e.,
translated vs. natural text style) that leads to poor generalization
capability; (2) content gap that induces the model to produce hallucination
content biased towards the target language. To narrow the data gap, we propose
an online self-training approach, which simultaneously uses the pseudo parallel
data {natural source, translated target} to mimic the inference scenario.
Experimental results on several widely-used language pairs show that our
approach outperforms two strong baselines (XLM and MASS) by remedying the style
and content gaps.",https://github.com/zwhe99/SelfTraining4UNMT,-1
Dead or Murdered? Predicting Responsibility Perception in Femicide News Reports,0.429658,"Different linguistic expressions can conceptualize the same event from
different viewpoints by emphasizing certain participants over others. Here, we
investigate a case where this has social consequences: how do linguistic
expressions of gender-based violence (GBV) influence who we perceive as
responsible? We build on previous psycholinguistic research in this area and
conduct a large-scale perception survey of GBV descriptions automatically
extracted from a corpus of Italian newspapers. We then train regression models
that predict the salience of GBV participants with respect to different
dimensions of perceived responsibility. Our best model (fine-tuned BERT) shows
solid overall performance, with large differences between dimensions and
participants: salient _focus_ is more predictable than salient _blame_, and
perpetrators' salience is more predictable than victims' salience. Experiments
with ridge regression models using different representations show that features
based on linguistic theory similarly to word-based features. Overall, we show
that different linguistic choices do trigger different perceptions of
responsibility, and that such perceptions can be modelled automatically. This
work can be a core instrument to raise awareness of the consequences of
different perspectivizations in the general public and in news producers alike.",https://gitlab.com/sociofillmore/perceived-perspective-prediction,92
On the Power of Foundation Models,0.396804,"With infinitely many high-quality data points, infinite computational power,
an infinitely large foundation model with a perfect training algorithm and
guaranteed zero generalization error on the pretext task, can the model be used
for everything? This question cannot be answered by the existing theory of
representation, optimization or generalization, because the issues they mainly
investigate are assumed to be nonexistent here. In this paper, we show that
category theory provides powerful machinery to answer this question. We have
proved three results. The first one limits the power of prompt-based learning,
saying that the model can solve a downstream task with prompts if and only if
the task is representable. The second one says fine tuning does not have this
limit, as a foundation model with the minimum required power (up to symmetry)
can theoretically solve downstream tasks for the category defined by pretext
task, with fine tuning and enough resources. Our final result can be seen as a
new type of generalization theorem, showing that the foundation model can
generate unseen objects from the target category (e.g., images) using the
structural information from the source category (e.g., texts). Along the way,
we provide a categorical framework for supervised and self-supervised learning,
which might be of independent interest.",None,-1
Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation,0.464593,"3D Human body pose and shape estimation within a temporal sequence can be
quite critical for understanding human behavior. Despite the significant
progress in human pose estimation in the recent years, which are often based on
single images or videos, human motion estimation on live stream videos is still
a rarely-touched area considering its special requirements for real-time output
and temporal consistency. To address this problem, we present a temporally
embedded 3D human body pose and shape estimation (TePose) method to improve the
accuracy and temporal consistency of pose estimation in live stream videos.
TePose uses previous predictions as a bridge to feedback the error for better
estimation in the current frame and to learn the correspondence between data
frames and predictions in the history. A multi-scale spatio-temporal graph
convolutional network is presented as the motion discriminator for adversarial
training using datasets without any 3D labeling. We propose a sequential data
loading strategy to meet the special start-to-end data processing requirement
of live stream. We demonstrate the importance of each proposed module with
extensive experiments. The results show the effectiveness of TePose on
widely-used human pose benchmarks with state-of-the-art performance.",https://github.com/ostadabbas/TePose,2464
Ultrahyperbolic Knowledge Graph Embeddings,0.727421,"Recent knowledge graph (KG) embeddings have been advanced by hyperbolic
geometry due to its superior capability for representing hierarchies. The
topological structures of real-world KGs, however, are rather heterogeneous,
i.e., a KG is composed of multiple distinct hierarchies and non-hierarchical
graph structures. Therefore, a homogeneous (either Euclidean or hyperbolic)
geometry is not sufficient for fairly representing such heterogeneous
structures. To capture the topological heterogeneity of KGs, we present an
ultrahyperbolic KG embedding (UltraE) in an ultrahyperbolic (or
pseudo-Riemannian) manifold that seamlessly interleaves hyperbolic and
spherical manifolds. In particular, we model each relation as a
pseudo-orthogonal transformation that preserves the pseudo-Riemannian bilinear
form. The pseudo-orthogonal transformation is decomposed into various operators
(i.e., circular rotations, reflections and hyperbolic rotations), allowing for
simultaneously modeling heterogeneous structures as well as complex relational
patterns. Experimental results on three standard KGs show that UltraE
outperforms previous Euclidean- and hyperbolic-based approaches.",None,-1
VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding,0.328646,"Pre-trained language models have achieved promising performance on general
benchmarks, but underperform when migrated to a specific domain. Recent works
perform pre-training from scratch or continual pre-training on domain corpora.
However, in many specific domains, the limited corpus can hardly support
obtaining precise representations. To address this issue, we propose a novel
Transformer-based language model named VarMAE for domain-adaptive language
understanding. Under the masked autoencoding objective, we design a context
uncertainty learning module to encode the token's context into a smooth latent
distribution. The module can produce diverse and well-formed contextual
representations. Experiments on science- and finance-domain NLU tasks
demonstrate that VarMAE can be efficiently adapted to new domains with limited
resources.",None,-1
AGQA 2.0: An Updated Benchmark for Compositional Spatio-Temporal Reasoning,0.562196,"Prior benchmarks have analyzed models' answers to questions about videos in
order to measure visual compositional reasoning. Action Genome Question
Answering (AGQA) is one such benchmark. AGQA provides a training/test split
with balanced answer distributions to reduce the effect of linguistic biases.
However, some biases remain in several AGQA categories. We introduce AGQA 2.0,
a version of this benchmark with several improvements, most namely a stricter
balancing procedure. We then report results on the updated benchmark for all
experiments.",None,-1
Accurate and Reliable Methods for 5G UAV Jamming Identification With Calibrated Uncertainty,0.234421,"Only increasing accuracy without considering uncertainty may negatively
impact Deep Neural Network (DNN) decision-making and decrease its reliability.
This paper proposes five combined preprocessing and post-processing methods for
time-series binary classification problems that simultaneously increase the
accuracy and reliability of DNN outputs applied in a 5G UAV security dataset.
These techniques use DNN outputs as input parameters and process them in
different ways. Two methods use a well-known Machine Learning (ML) algorithm as
a complement, and the other three use only confidence values that the DNN
estimates. We compare seven different metrics, such as the Expected Calibration
Error (ECE), Maximum Calibration Error (MCE), Mean Confidence (MC), Mean
Accuracy (MA), Normalized Negative Log Likelihood (NLL), Brier Score Loss
(BSL), and Reliability Score (RS) and the tradeoffs between them to evaluate
the proposed hybrid algorithms. First, we show that the eXtreme Gradient
Boosting (XGB) classifier might not be reliable for binary classification under
the conditions this work presents. Second, we demonstrate that at least one of
the potential methods can achieve better results than the classification in the
DNN softmax layer. Finally, we show that the prospective methods may improve
accuracy and reliability with better uncertainty calibration based on the
assumption that the RS determines the difference between MC and MA metrics, and
this difference should be zero to increase reliability. For example, Method 3
presents the best RS of 0.65 even when compared to the XGB classifier, which
achieves RS of 7.22.",None,453
OptG: Optimizing Gradient-driven Criteria in Network Sparsity,0.145053,"Network sparsity receives popularity mostly due to its capability to reduce
the network complexity. Extensive studies excavate gradient-driven sparsity.
Typically, these methods are constructed upon premise of weight independence,
which however, is contrary to the fact that weights are mutually influenced.
Thus, their performance remains to be improved. In this paper, we propose to
optimize gradient-driven sparsity (OptG) by solving this independence paradox.
Our motive comes from the recent advances in supermask training which shows
that high-performing sparse subnetworks can be located by simply updating mask
values without modifying any weight. We prove that supermask training is to
accumulate the criteria of gradient-driven sparsity for both removed and
preserved weights, and it can partly solve the independence paradox.
Consequently, OptG integrates supermask training into gradient-driven sparsity,
and a novel supermask optimizer is further proposed to comprehensively mitigate
the independence paradox. Experiments show that OptG can well surpass many
existing state-of-the-art competitors, especially at ultra-high sparsity
levels. Our code is available at \url{https://github.com/zyxxmu/OptG}.",https://github.com/zyxxmu/OptG,-1
An Emotion-guided Approach to Domain Adaptive Fake News Detection using Adversarial Learning,0.243591,"Recent works on fake news detection have shown the efficacy of using emotions
as a feature for improved performance. However, the cross-domain impact of
emotion-guided features for fake news detection still remains an open problem.
In this work, we propose an emotion-guided, domain-adaptive, multi-task
approach for cross-domain fake news detection, proving the efficacy of
emotion-guided models in cross-domain settings for various datasets.",None,6855
Real-time Detection of 2D Tool Landmarks with Synthetic Training Data,0.329588,"In this paper a deep learning architecture is presented that can, in real
time, detect the 2D locations of certain landmarks of physical tools, such as a
hammer or screwdriver. To avoid the labor of manual labeling, the network is
trained on synthetically generated data. Training computer vision models on
computer generated images, while still achieving good accuracy on real images,
is a challenge due to the difference in domain. The proposed method uses an
advanced rendering method in combination with transfer learning and an
intermediate supervision architecture to address this problem. It is shown that
the model presented in this paper, named Intermediate Heatmap Model (IHM),
generalizes to real images when trained on synthetic data. To avoid the need
for an exact textured 3D model of the tool in question, it is shown that the
model will generalize to an unseen tool when trained on a set of different 3D
models of the same type of tool. IHM is compared to two existing approaches to
keypoint detection and it is shown that it outperforms those at detecting tool
landmarks, trained on synthetic data.",https://github.com/yuanyuanli85/Stacked Hourglass Network Keras,-1
CXTrack: Improving 3D Point Cloud Tracking with Contextual Information,0.532579,"3D single object tracking plays an essential role in many applications, such
as autonomous driving. It remains a challenging problem due to the large
appearance variation and the sparsity of points caused by occlusion and limited
sensor capabilities. Therefore, contextual information across two consecutive
frames is crucial for effective object tracking. However, points containing
such useful information are often overlooked and cropped out in existing
methods, leading to insufficient use of important contextual knowledge. To
address this issue, we propose CXTrack, a novel transformer-based network for
3D object tracking, which exploits ConteXtual information to improve the
tracking results. Specifically, we design a target-centric transformer network
that directly takes point features from two consecutive frames and the previous
bounding box as input to explore contextual information and implicitly
propagate target cues. To achieve accurate localization for objects of all
sizes, we propose a transformer-based localization head with a novel center
embedding module to distinguish the target from distractors. Extensive
experiments on three large-scale datasets, KITTI, nuScenes and Waymo Open
Dataset, show that CXTrack achieves state-of-the-art tracking performance while
running at 34 FPS.",None,-1
Towards Trustworthy Multi-label Sewer Defect Classification via Evidential Deep Learning,0.656177,"An automatic vision-based sewer inspection plays a key role of sewage system
in a modern city. Recent advances focus on utilizing deep learning model to
realize the sewer inspection system, benefiting from the capability of
data-driven feature representation. However, the inherent uncertainty of sewer
defects is ignored, resulting in the missed detection of serious unknown sewer
defect categories. In this paper, we propose a trustworthy multi-label sewer
defect classification (TMSDC) method, which can quantify the uncertainty of
sewer defect prediction via evidential deep learning. Meanwhile, a novel expert
base rate assignment (EBRA) is proposed to introduce the expert knowledge for
describing reliable evidences in practical situations. Experimental results
demonstrate the effectiveness of TMSDC and the superior capability of
uncertainty estimation is achieved on the latest public benchmark.",None,-1
Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes,0.487329,"We study offline reinforcement learning (RL) in partially observable Markov
decision processes. In particular, we aim to learn an optimal policy from a
dataset collected by a behavior policy which possibly depends on the latent
state. Such a dataset is confounded in the sense that the latent state
simultaneously affects the action and the observation, which is prohibitive for
existing offline RL algorithms. To this end, we propose the \underline{P}roxy
variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization
(\texttt{P3O}) algorithm, which addresses the confounding bias and the
distributional shift between the optimal and behavior policies in the context
of general function approximation. At the core of \texttt{P3O} is a coupled
sequence of pessimistic confidence regions constructed via proximal causal
inference, which is formulated as minimax estimation. Under a partial coverage
assumption on the confounded dataset, we prove that \texttt{P3O} achieves a
$n^{-1/2}$-suboptimality, where $n$ is the number of trajectories in the
dataset. To our best knowledge, \texttt{P3O} is the first provably efficient
offline RL algorithm for POMDPs with a confounded dataset.",None,-1
Analytical Solutions for the Inverse Problem within Gradual Semantics,0.141499,"Gradual semantics within abstract argumentation associate a numeric score
with every argument in a system, which represents the level of acceptability of
this argument, and from which a preference ordering over arguments can be
derived. While some semantics operate over standard argumentation frameworks,
many utilise a weighted framework, where a numeric initial weight is associated
with each argument. Recent work has examined the inverse problem within gradual
semantics. Rather than determining a preference ordering given an argumentation
framework and a semantics, the inverse problem takes an argumentation
framework, a gradual semantics, and a preference ordering as inputs, and
identifies what weights are needed to over arguments in the framework to obtain
the desired preference ordering. Existing work has attacked the inverse problem
numerically, using a root finding algorithm (the bisection method) to identify
appropriate initial weights. In this paper we demonstrate that for a class of
gradual semantics, an analytical approach can be used to solve the inverse
problem. Unlike the current state-of-the-art, such an analytic approach can
rapidly find a solution, and is guaranteed to do so. In obtaining this result,
we are able to prove several important properties which previous work had posed
as conjectures.",None,3518
Missingness Bias in Model Debugging,0.522385,"Missingness, or the absence of features from an input, is a concept
fundamental to many model debugging tools. However, in computer vision, pixels
cannot simply be removed from an image. One thus tends to resort to heuristics
such as blacking out pixels, which may in turn introduce bias into the
debugging process. We study such biases and, in particular, show how
transformer-based architectures can enable a more natural implementation of
missingness, which side-steps these issues and improves the reliability of
model debugging in practice. Our code is available at
https://github.com/madrylab/missingness",https://github.com/madrylab/missingness,-1
Memory Efficient Continual Learning with Transformers,0.422233,"In many real-world scenarios, data to train machine learning models becomes
available over time. Unfortunately, these models struggle to continually learn
new concepts without forgetting what has been learnt in the past. This
phenomenon is known as catastrophic forgetting and it is difficult to prevent
due to practical constraints. For instance, the amount of data that can be
stored or the computational resources that can be used might be limited.
Moreover, applications increasingly rely on large pre-trained neural networks,
such as pre-trained Transformers, since the resources or data might not be
available in sufficiently large quantities to practitioners to train the model
from scratch. In this paper, we devise a method to incrementally train a model
on a sequence of tasks using pre-trained Transformers and extending them with
Adapters. Different than the existing approaches, our method is able to scale
to a large number of tasks without significant overhead and allows sharing
information across tasks. On both image and text classification tasks, we
empirically demonstrate that our method maintains a good predictive performance
without retraining the model or increasing the number of model parameters over
time. The resulting model is also significantly faster at inference time
compared to Adapter-based state-of-the-art methods.",https://github.com/Adapter-Hub/adapter-transformers,-1
Beyond RGB: Scene-Property Synthesis with Neural Radiance Fields,0.144263,"Comprehensive 3D scene understanding, both geometrically and semantically, is
important for real-world applications such as robot perception. Most of the
existing work has focused on developing data-driven discriminative models for
scene understanding. This paper provides a new approach to scene understanding,
from a synthesis model perspective, by leveraging the recent progress on
implicit 3D representation and neural rendering. Building upon the great
success of Neural Radiance Fields (NeRFs), we introduce Scene-Property
Synthesis with NeRF (SS-NeRF) that is able to not only render photo-realistic
RGB images from novel viewpoints, but also render various accurate scene
properties (e.g., appearance, geometry, and semantics). By doing so, we
facilitate addressing a variety of scene understanding tasks under a unified
framework, including semantic segmentation, surface normal estimation,
reshading, keypoint detection, and edge detection. Our SS-NeRF framework can be
a powerful tool for bridging generative learning and discriminative learning,
and thus be beneficial to the investigation of a wide range of interesting
problems, such as studying task relationships within a synthesis paradigm,
transferring knowledge to novel tasks, facilitating downstream discriminative
tasks as ways of data augmentation, and serving as auto-labeller for data
creation.",None,-1
Using Multi-Encoder Fusion Strategies to Improve Personalized Response Selection,0.44463,"Personalized response selection systems are generally grounded on persona.
However, there exists a co-relation between persona and empathy, which is not
explored well in these systems. Also, faithfulness to the conversation context
plunges when a contradictory or an off-topic response is selected. This paper
attempts to address these issues by proposing a suite of fusion strategies that
capture the interaction between persona, emotion, and entailment information of
the utterances. Ablation studies on the Persona-Chat dataset show that
incorporating emotion and entailment improves the accuracy of response
selection. We combine our fusion strategies and concept-flow encoding to train
a BERT-based model which outperforms the previous methods by margins larger
than 2.3 % on original personas and 1.9 % on revised personas in terms of
hits@1 (top-1 accuracy), achieving a new state-of-the-art performance on the
Persona-Chat dataset.",https://github.com/allenai/allennlp-models,-1
Multi-Layer Modeling of Dense Vegetation from Aerial LiDAR Scans,0.245636,"The analysis of the multi-layer structure of wild forests is an important
challenge of automated large-scale forestry. While modern aerial LiDARs offer
geometric information across all vegetation layers, most datasets and methods
focus only on the segmentation and reconstruction of the top of canopy. We
release WildForest3D, which consists of 29 study plots and over 2000 individual
trees across 47 000m2 with dense 3D annotation, along with occupancy and height
maps for 3 vegetation layers: ground vegetation, understory, and overstory. We
propose a 3D deep network architecture predicting for the first time both 3D
point-wise labels and high-resolution layer occupancy rasters simultaneously.
This allows us to produce a precise estimation of the thickness of each
vegetation layer as well as the corresponding watertight meshes, therefore
meeting most forestry purposes. Both the dataset and the model are released in
open access: https://github.com/ekalinicheva/multi_layer_vegetation.",https://github.com/ekalinicheva/multi_layer_vegetation,-1
ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding,1.0,"Recently, learned image compression techniques have achieved remarkable
performance, even surpassing the best manually designed lossy image coders.
They are promising to be large-scale adopted. For the sake of practicality, a
thorough investigation of the architecture design of learned image compression,
regarding both compression performance and running speed, is essential. In this
paper, we first propose uneven channel-conditional adaptive coding, motivated
by the observation of energy compaction in learned image compression. Combining
the proposed uneven grouping model with existing context models, we obtain a
spatial-channel contextual adaptive model to improve the coding performance
without damage to running speed. Then we study the structure of the main
transform and propose an efficient model, ELIC, to achieve state-of-the-art
speed and compression ability. With superior performance, the proposed model
also supports extremely fast preview decoding and progressive decoding, which
makes the coming application of learning-based image compression more
promising.",https://github.com/InterDigitalInc/CompressAI/blob/v1.1.8/results/kodak/vtm.json,-1
"Layout-Aware Information Extraction for Document-Grounded Dialogue: Dataset, Method and Demonstration",0.562066,"Building document-grounded dialogue systems have received growing interest as
documents convey a wealth of human knowledge and commonly exist in enterprises.
Wherein, how to comprehend and retrieve information from documents is a
challenging research problem. Previous work ignores the visual property of
documents and treats them as plain text, resulting in incomplete modality. In
this paper, we propose a Layout-aware document-level Information Extraction
dataset, LIE, to facilitate the study of extracting both structural and
semantic knowledge from visually rich documents (VRDs), so as to generate
accurate responses in dialogue systems. LIE contains 62k annotations of three
extraction tasks from 4,061 pages in product and official documents, becoming
the largest VRD-based information extraction dataset to the best of our
knowledge. We also develop benchmark methods that extend the token-based
language model to consider layout features like humans. Empirical results show
that layout is critical for VRD-based extraction, and system demonstration also
verifies that the extracted knowledge can help locate the answers that users
care about.",https://github.com/jsvine/pdfplumber,-1
Self-Configurable Stabilized Real-Time Detection Learning for Autonomous Driving Applications,0.0828189,"Guaranteeing real-time and accurate object detection simultaneously is
paramount in autonomous driving environments. However, the existing object
detection neural network systems are characterized by a tradeoff between
computation time and accuracy, making it essential to optimize such a tradeoff.
Fortunately, in many autonomous driving environments, images come in a
continuous form, providing an opportunity to use optical flow. In this paper,
we improve the performance of an object detection neural network utilizing
optical flow estimation. In addition, we propose a Lyapunov optimization
framework for time-average performance maximization subject to stability. It
adaptively determines whether to use optical flow to suit the dynamic vehicle
environment, thereby ensuring the vehicle's queue stability and the
time-average maximum performance simultaneously. To verify the key ideas, we
conduct numerical experiments with various object detection neural networks and
optical flow estimation networks. In addition, we demonstrate the
self-configurable stabilized detection with YOLOv3-tiny and FlowNet2-S, which
are the real-time object detection network and an optical flow estimation
network, respectively. In the demonstration, our proposed framework improves
the accuracy by 3.02%, the number of detected objects by 59.6%, and the queue
stability for computing capabilities.",None,-1
Detecting Arbitrary Keypoints on Limbs and Skis with Sparse Partly Correct Segmentation Masks,0.559356,"Analyses based on the body posture are crucial for top-class athletes in many
sports disciplines. If at all, coaches label only the most important keypoints,
since manual annotations are very costly. This paper proposes a method to
detect arbitrary keypoints on the limbs and skis of professional ski jumpers
that requires a few, only partly correct segmentation masks during training.
Our model is based on the Vision Transformer architecture with a special design
for the input tokens to query for the desired keypoints. Since we use
segmentation masks only to generate ground truth labels for the freely
selectable keypoints, partly correct segmentation masks are sufficient for our
training procedure. Hence, there is no need for costly hand-annotated
segmentation masks. We analyze different training techniques for freely
selected and standard keypoints, including pseudo labels, and show in our
experiments that only a few partly correct segmentation masks are sufficient
for learning to detect arbitrary keypoints on limbs and skis.",https://github.com/kaulquappe23/arbitrary-keypoints-skijump,-1
Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models,0.682942,"End-to-end spoken language understanding (SLU) systems are gaining popularity
over cascaded approaches due to their simplicity and ability to avoid error
propagation. However, these systems model sequence labeling as a sequence
prediction task causing a divergence from its well-established token-level
tagging formulation. We build compositional end-to-end SLU systems that
explicitly separate the added complexity of recognizing spoken mentions in SLU
from the NLU task of sequence labeling. By relying on intermediate decoders
trained for ASR, our end-to-end systems transform the input modality from
speech to token-level representations that can be used in the traditional
sequence labeling framework. This composition of ASR and NLU formulations in
our end-to-end SLU system offers direct compatibility with pre-trained ASR and
NLU systems, allows performance monitoring of individual components and enables
the use of globally normalized losses like CRF, making them attractive in
practical scenarios. Our models outperform both cascaded and direct end-to-end
models on a labeling task of named entity recognition across SLU benchmarks.",https://github.com/espnet/espnet,-1
FLAG: Flow-based 3D Avatar Generation from Sparse Observations,0.894228,"To represent people in mixed reality applications for collaboration and
communication, we need to generate realistic and faithful avatar poses.
However, the signal streams that can be applied for this task from head-mounted
devices (HMDs) are typically limited to head pose and hand pose estimates.
While these signals are valuable, they are an incomplete representation of the
human body, making it challenging to generate a faithful full-body avatar. We
address this challenge by developing a flow-based generative model of the 3D
human body from sparse observations, wherein we learn not only a conditional
distribution of 3D human pose, but also a probabilistic mapping from
observations to the latent space from which we can generate a plausible pose
along with uncertainty estimates for the joints. We show that our approach is
not only a strong predictive model, but can also act as an efficient pose prior
in different optimization settings where a good initial latent code plays a
major role.",https://microsoft.github.io/flag,-1
Facial Action Unit Recognition Based on Transfer Learning,0.480554,"Facial action unit recognition is an important task for facial analysis.
Owing to the complex collection environment, facial action unit recognition in
the wild is still challenging. The 3rd competition on affective behavior
analysis in-the-wild (ABAW) has provided large amount of facial images with
facial action unit annotations. In this paper, we introduce a facial action
unit recognition method based on transfer learning. We first use available
facial images with expression labels to train the feature extraction network.
Then we fine-tune the network for facial action unit recognition.",None,-1
Proximal PanNet: A Model-Based Deep Network for Pansharpening,0.594003,"Recently, deep learning techniques have been extensively studied for
pansharpening, which aims to generate a high resolution multispectral (HRMS)
image by fusing a low resolution multispectral (LRMS) image with a high
resolution panchromatic (PAN) image. However, existing deep learning-based
pansharpening methods directly learn the mapping from LRMS and PAN to HRMS.
These network architectures always lack sufficient interpretability, which
limits further performance improvements. To alleviate this issue, we propose a
novel deep network for pansharpening by combining the model-based methodology
with the deep learning method. Firstly, we build an observation model for
pansharpening using the convolutional sparse coding (CSC) technique and design
a proximal gradient algorithm to solve this model. Secondly, we unfold the
iterative algorithm into a deep network, dubbed as Proximal PanNet, by learning
the proximal operators using convolutional neural networks. Finally, all the
learnable modules can be automatically learned in an end-to-end manner.
Experimental results on some benchmark datasets show that our network performs
better than other advanced methods both quantitatively and qualitatively.",None,-1
3DMM-RF: Convolutional Radiance Fields for 3D Face Modeling,0.183131,"Facial 3D Morphable Models are a main computer vision subject with countless
applications and have been highly optimized in the last two decades. The
tremendous improvements of deep generative networks have created various
possibilities for improving such models and have attracted wide interest.
Moreover, the recent advances in neural radiance fields, are revolutionising
novel-view synthesis of known scenes. In this work, we present a facial 3D
Morphable Model, which exploits both of the above, and can accurately model a
subject's identity, pose and expression and render it in arbitrary
illumination. This is achieved by utilizing a powerful deep style-based
generator to overcome two main weaknesses of neural radiance fields, their
rigidity and rendering speed. We introduce a style-based generative network
that synthesizes in one pass all and only the required rendering samples of a
neural radiance field. We create a vast labelled synthetic dataset of facial
renders, and train the network on these data, so that it can accurately model
and generalize on facial identity, pose and appearance. Finally, we show that
this model can accurately be fit to ""in-the-wild"" facial images of arbitrary
pose and illumination, extract the facial characteristics, and be used to
re-render the face in controllable conditions.",None,-1
Towards Better Chinese-centric Neural Machine Translation for Low-resource Languages,0.818251,"The last decade has witnessed enormous improvements in science and
technology, stimulating the growing demand for economic and cultural exchanges
in various countries. Building a neural machine translation (NMT) system has
become an urgent trend, especially in the low-resource setting. However, recent
work tends to study NMT systems for low-resource languages centered on English,
while few works focus on low-resource NMT systems centered on other languages
such as Chinese. To achieve this, the low-resource multilingual translation
challenge of the 2021 iFLYTEK AI Developer Competition provides the
Chinese-centric multilingual low-resource NMT tasks, where participants are
required to build NMT systems based on the provided low-resource samples. In
this paper, we present the winner competition system that leverages monolingual
word embeddings data enhancement, bilingual curriculum learning, and
contrastive re-ranking. In addition, a new Incomplete-Trust (In-trust) loss
function is proposed to replace the traditional cross-entropy loss when
training. The experimental results demonstrate that the implementation of these
ideas leads better performance than other state-of-the-art methods. All the
experimental codes are released at:
https://github.com/WENGSYX/Low-resource-text-translation.",https://github.com/WENGSYX/Low-resource-text-translation,-1
Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity,0.981761,"Large Language Models (LLMs) have demonstrated impressive capabilities in
generating fluent text, as well as tendencies to reproduce undesirable social
biases. This study investigates whether LLMs reproduce the moral biases
associated with political groups in the United States, an instance of a broader
capability herein termed moral mimicry. This hypothesis is explored in the
GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral
Foundations Theory, it is shown that these LLMs are indeed moral mimics. When
prompted with a liberal or conservative political identity, the models generate
text reflecting corresponding moral biases. This study also explores the
relationship between moral mimicry and model size, and similarity between human
and LLM moral word use.",https://github.com/mbforbes/social-chemistry-101,-1
Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction,0.758732,"Recently, prompt-tuning has attracted growing interests in event argument
extraction (EAE). However, the existing prompt-tuning methods have not achieved
satisfactory performance due to the lack of consideration of entity
information. In this paper, we propose a bi-directional iterative prompt-tuning
method for EAE, where the EAE task is treated as a cloze-style task to take
full advantage of entity information and pre-trained language models (PLMs).
Furthermore, our method explores event argument interactions by introducing the
argument roles of contextual entities into prompt construction. Since template
and verbalizer are two crucial components in a cloze-style prompt, we propose
to utilize the role label semantic knowledge to construct a semantic verbalizer
and design three kinds of templates for the EAE task. Experiments on the ACE
2005 English dataset with standard and low-resource settings show that the
proposed method significantly outperforms the peer state-of-the-art methods.
Our code is available at https://github.com/HustMinsLab/BIP.",https://github.com/HustMinsLab/BIP,-1
"A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding",0.21396,"Collecting sufficient labeled data for spoken language understanding (SLU) is
expensive and time-consuming. Recent studies achieved promising results by
using pre-trained models in low-resource scenarios. Inspired by this, we aim to
ask: which (if any) pre-training strategies can improve performance across SLU
benchmarks? To answer this question, we employ four types of pre-trained models
and their combinations for SLU. We leverage self-supervised speech and language
models (LM) pre-trained on large quantities of unpaired data to extract strong
speech and text representations. We also explore using supervised models
pre-trained on larger external automatic speech recognition (ASR) or SLU
corpora. We conduct extensive experiments on the SLU Evaluation (SLUE)
benchmark and observe self-supervised pre-trained models to be more powerful,
with pre-trained LM and speech models being most beneficial for the Sentiment
Analysis and Named Entity Recognition task, respectively.",None,-1
A Baseline for Detecting Out-of-Distribution Examples in Image Captioning,0.12448,"Image captioning research achieved breakthroughs in recent years by
developing neural models that can generate diverse and high-quality
descriptions for images drawn from the same distribution as training images.
However, when facing out-of-distribution (OOD) images, such as corrupted
images, or images containing unknown objects, the models fail in generating
relevant captions.
  In this paper, we consider the problem of OOD detection in image captioning.
We formulate the problem and suggest an evaluation setup for assessing the
model's performance on the task. Then, we analyze and show the effectiveness of
the caption's likelihood score at detecting and rejecting OOD images, which
implies that the relatedness between the input image and the generated caption
is encapsulated within the score.",None,-1
A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels,0.047551,"Label noise is common in large real-world datasets, and its presence harms
the training process of deep neural networks. Although several works have
focused on the training strategies to address this problem, there are few
studies that evaluate the impact of data augmentation as a design choice for
training deep neural networks. In this work, we analyse the model robustness
when using different data augmentations and their improvement on the training
with the presence of noisy labels. We evaluate state-of-the-art and classical
data augmentation strategies with different levels of synthetic noise for the
datasets MNist, CIFAR-10, CIFAR-100, and the real-world dataset Clothing1M. We
evaluate the methods using the accuracy metric. Results show that the
appropriate selection of data augmentation can drastically improve the model
robustness to label noise, increasing up to 177.84% of relative best test
accuracy compared to the baseline with no augmentation, and an increase of up
to 6% in absolute value with the state-of-the-art DivideMix training strategy.",None,-1
TextHacker: Learning based Hybrid Local Search Algorithm for Text Hard-label Adversarial Attack,0.285864,"Existing textual adversarial attacks usually utilize the gradient or
prediction confidence to generate adversarial examples, making it hard to be
deployed in real-world applications. To this end, we consider a rarely
investigated but more rigorous setting, namely hard-label attack, in which the
attacker can only access the prediction label. In particular, we find we can
learn the importance of different words via the change on prediction label
caused by word substitutions on the adversarial examples. Based on this
observation, we propose a novel adversarial attack, termed Text Hard-label
attacker (TextHacker). TextHacker randomly perturbs lots of words to craft an
adversarial example. Then, TextHacker adopts a hybrid local search algorithm
with the estimation of word importance from the attack history to minimize the
adversarial perturbation. Extensive evaluations for text classification and
textual entailment show that TextHacker significantly outperforms existing
hard-label attacks regarding the attack performance as well as adversary
quality.",https://github.com/JHL-HUST/TextHacker,-1
FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,0.54836,"Transformers have been shown to be able to perform deductive reasoning on a
logical rulebase containing rules and statements written in natural language.
Recent works show that such models can also produce the reasoning steps (i.e.,
the proof graph) that emulate the model's logical reasoning process. Currently,
these black-box models generate both the proof graph and intermediate
inferences within the same model and thus may be unfaithful. In this work, we
frame the deductive logical reasoning task by defining three modular
components: rule selection, fact selection, and knowledge composition. The rule
and fact selection steps select the candidate rule and facts to be used and
then the knowledge composition combines them to generate new inferences. This
ensures model faithfulness by assured causal relation from the proof step to
the inference reasoning. To test our framework, we propose FaiRR (Faithful and
Robust Reasoner) where the above three components are independently modeled by
transformers. We observe that FaiRR is robust to novel language perturbations,
and is faster at inference than previous works on existing reasoning datasets.
Additionally, in contrast to black-box generative models, the errors made by
FaiRR are more interpretable due to the modular approach.",https://github.com/INK-USC/FaiRR,-1
"A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes",0.574235,"While datasets with single-label supervision have propelled rapid advances in
image classification, additional annotations are necessary in order to
quantitatively assess how models make predictions. To this end, for a subset of
ImageNet samples, we collect segmentation masks for the entire object and $18$
informative attributes. We call this dataset RIVAL10 (RIch Visual Attributes
with Localization), consisting of roughly $26k$ instances over $10$ classes.
Using RIVAL10, we evaluate the sensitivity of a broad set of models to noise
corruptions in foregrounds, backgrounds and attributes. In our analysis, we
consider diverse state-of-the-art architectures (ResNets, Transformers) and
training procedures (CLIP, SimCLR, DeiT, Adversarial Training). We find that,
somewhat surprisingly, in ResNets, adversarial training makes models more
sensitive to the background compared to foreground than standard training.
Similarly, contrastively-trained models also have lower relative foreground
sensitivity in both transformers and ResNets. Lastly, we observe intriguing
adaptive abilities of transformers to increase relative foreground sensitivity
as corruption level increases. Using saliency methods, we automatically
discover spurious features that drive the background sensitivity of models and
assess alignment of saliency maps with foregrounds. Finally, we quantitatively
study the attribution problem for neural features by comparing feature saliency
with ground-truth localization of semantic attributes.",None,-1
pymdp: A Python library for active inference in discrete state spaces,0.480415,"Active inference is an account of cognition and behavior in complex systems
which brings together action, perception, and learning under the theoretical
mantle of Bayesian inference. Active inference has seen growing applications in
academic research, especially in fields that seek to model human or animal
behavior. While in recent years, some of the code arising from the active
inference literature has been written in open source languages like Python and
Julia, to-date, the most popular software for simulating active inference
agents is the DEM toolbox of SPM, a MATLAB library originally developed for the
statistical analysis and modelling of neuroimaging data. Increasing interest in
active inference, manifested both in terms of sheer number as well as
diversifying applications across scientific disciplines, has thus created a
need for generic, widely-available, and user-friendly code for simulating
active inference in open-source scientific computing languages like Python. The
Python package we present here, pymdp (see
https://github.com/infer-actively/pymdp), represents a significant step in this
direction: namely, we provide the first open-source package for simulating
active inference with partially-observable Markov Decision Processes or POMDPs.
We review the package's structure and explain its advantages like modular
design and customizability, while providing in-text code blocks along the way
to demonstrate how it can be used to build and run active inference processes
with ease. We developed pymdp to increase the accessibility and exposure of the
active inference framework to researchers, engineers, and developers with
diverse disciplinary backgrounds. In the spirit of open-source software, we
also hope that it spurs new innovation, development, and collaboration in the
growing active inference community.",https://pymdp-rtd.readthedocs.io/,-1
Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation,0.999812,"Document-level Relation Extraction (DocRE) is a more challenging task
compared to its sentence-level counterpart. It aims to extract relations from
multiple sentences at once. In this paper, we propose a semi-supervised
framework for DocRE with three novel components. Firstly, we use an axial
attention module for learning the interdependency among entity-pairs, which
improves the performance on two-hop relations. Secondly, we propose an adaptive
focal loss to tackle the class imbalance problem of DocRE. Lastly, we use
knowledge distillation to overcome the differences between human annotated data
and distantly supervised data. We conducted experiments on two DocRE datasets.
Our model consistently outperforms strong baselines and its performance exceeds
the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.
Our code and data will be released at https://github.com/tonytan48/KD-DocRE.",https://github.com/tonytan48/KD-DocRE,18583
Transfer Learning based Search Space Design for Hyperparameter Tuning,0.694246,"The tuning of hyperparameters becomes increasingly important as machine
learning (ML) models have been extensively applied in data mining applications.
Among various approaches, Bayesian optimization (BO) is a successful
methodology to tune hyper-parameters automatically. While traditional methods
optimize each tuning task in isolation, there has been recent interest in
speeding up BO by transferring knowledge across previous tasks. In this work,
we introduce an automatic method to design the BO search space with the aid of
tuning history from past tasks. This simple yet effective approach can be used
to endow many existing BO methods with transfer learning capabilities. In
addition, it enjoys the three advantages: universality, generality, and
safeness. The extensive experiments show that our approach considerably boosts
BO by designing a promising and compact search space instead of using the
entire space, and outperforms the state-of-the-arts on a wide range of
benchmarks, including machine learning and deep learning tuning tasks, and
neural architecture search.",None,10542
"Entities, Dates, and Languages: Zero-Shot on Historical Texts with T0",0.623256,"In this work, we explore whether the recently demonstrated zero-shot
abilities of the T0 model extend to Named Entity Recognition for
out-of-distribution languages and time periods. Using a historical newspaper
corpus in 3 languages as test-bed, we use prompts to extract possible named
entities. Our results show that a naive approach for prompt-based zero-shot
multilingual Named Entity Recognition is error-prone, but highlights the
potential of such an approach for historical languages lacking labeled
datasets. Moreover, we also find that T0-like models can be probed to predict
the publication date and language of a document, which could be very relevant
for the study of historical texts.",https://github.com/bigscience-workshop/historical_texts,-1
Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech,0.745477,"We introduce a generic, language-independent method to collect a large
percentage of offensive and hate tweets regardless of their topics or genres.
We harness the extralinguistic information embedded in the emojis to collect a
large number of offensive tweets. We apply the proposed method on Arabic tweets
and compare it with English tweets - analysing key cultural differences. We
observed a constant usage of these emojis to represent offensiveness throughout
different timespans on Twitter. We manually annotate and publicly release the
largest Arabic dataset for offensive, fine-grained hate speech, vulgar and
violence content. Furthermore, we benchmark the dataset for detecting
offensiveness and hate speech using different transformer architectures and
perform in-depth linguistic analysis. We evaluate our models on external
datasets - a Twitter dataset collected using a completely different method, and
a multi-platform dataset containing comments from Twitter, YouTube and
Facebook, for assessing generalization capability. Competitive results on these
datasets suggest that the data collected using our method captures universal
characteristics of offensive language. Our findings also highlight the common
words used in offensive communications, common targets for hate speech,
specific patterns in violence tweets; and pinpoint common classification errors
that can be attributed to limitations of NLP models. We observe that even
state-of-the-art transformer models may fail to take into account culture,
background and context or understand nuances present in real-world data such as
sarcasm.",None,-1
More Interpretable Graph Similarity Computation via Maximum Common Subgraph Inference,0.675987,"Graph similarity measurement, which computes the distance/similarity between
two graphs, arises in various graph-related tasks. Recent learning-based
methods lack interpretability, as they directly transform interaction
information between two graphs into one hidden vector and then map it to
similarity. To cope with this problem, this study proposes a more interpretable
end-to-end paradigm for graph similarity learning, named Similarity Computation
via Maximum Common Subgraph Inference (INFMCS). Our critical insight into
INFMCS is the strong correlation between similarity score and Maximum Common
Subgraph (MCS). We implicitly infer MCS to obtain the normalized MCS size, with
the supervision information being only the similarity score during training. To
capture more global information, we also stack some vanilla transformer encoder
layers with graph convolution layers and propose a novel permutation-invariant
node Positional Encoding. The entire model is quite simple yet effective.
Comprehensive experiments demonstrate that INFMCS consistently outperforms
state-of-the-art baselines for graph-graph classification and regression tasks.
Ablation experiments verify the effectiveness of the proposed computation
paradigm and other components. Also, visualization and statistics of results
reveal the interpretability of INFMCS.",https://github.com/cszhangzhen/H2MN,-1
A Holistic Framework for Analyzing the COVID-19 Vaccine Debate,0.821673,"The Covid-19 pandemic has led to infodemic of low quality information leading
to poor health decisions. Combating the outcomes of this infodemic is not only
a question of identifying false claims, but also reasoning about the decisions
individuals make. In this work we propose a holistic analysis framework
connecting stance and reason analysis, and fine-grained entity level moral
sentiment analysis. We study how to model the dependencies between the
different level of analysis and incorporate human insights into the learning
process. Experiments show that our framework provides reliable predictions even
in the low-supervision settings.",https://gitlab.com/mlpacheco/covid-moral-foundations,-1
Thermodynamics-informed neural networks for physically realistic mixed reality,0.712728,"The imminent impact of immersive technologies in society urges for active
research in real-time and interactive physics simulation for virtual worlds to
be realistic. In this context, realistic means to be compliant to the laws of
physics. In this paper we present a method for computing the dynamic response
of (possibly non-linear and dissipative) deformable objects induced by
real-time user interactions in mixed reality using deep learning. The
graph-based architecture of the method ensures the thermodynamic consistency of
the predictions, whereas the visualization pipeline allows a natural and
realistic user experience. Two examples of virtual solids interacting with
virtual or physical solids in mixed reality scenarios are provided to prove the
performance of the method.",https://github.com/quercushernandez,-1
Improve Ranking Correlation of Super-net through Training Scheme from One-shot NAS to Few-shot NAS,0.0876605,"The algorithms of one-shot neural architecture search(NAS) have been widely
used to reduce computation consumption. However, because of the interference
among the subnets in which weights are shared, the subnets inherited from these
super-net trained by those algorithms have poor consistency in precision
ranking. To address this problem, we propose a step-by-step training super-net
scheme from one-shot NAS to few-shot NAS. In the training scheme, we firstly
train super-net in a one-shot way, and then we disentangle the weights of
super-net by splitting them into multi-subnets and training them gradually.
Finally, our method ranks 4th place in the CVPR2022 3rd Lightweight NAS
Challenge Track1. Our code is available at
https://github.com/liujiawei2333/CVPR2022-NAS-competition-Track-1-4th-solution.",https://github.com/liujiawei2333/CVPR2022-NAS-competition-Track-1-4th-solution,2394
PanDepth: Joint Panoptic Segmentation and Depth Completion,0.0330909,"Understanding 3D environments semantically is pivotal in autonomous driving
applications where multiple computer vision tasks are involved. Multi-task
models provide different types of outputs for a given scene, yielding a more
holistic representation while keeping the computational cost low. We propose a
multi-task model for panoptic segmentation and depth completion using RGB
images and sparse depth maps. Our model successfully predicts fully dense depth
maps and performs semantic segmentation, instance segmentation, and panoptic
segmentation for every input frame. Extensive experiments were done on the
Virtual KITTI 2 dataset and we demonstrate that our model solves multiple
tasks, without a significant increase in computational cost, while keeping high
accuracy performance. Code is available at
https://github.com/juanb09111/PanDepth.git",https://github.com/juanb09111/PanDepth.git,-1
Zero-shot Aspect-level Sentiment Classification via Explicit Utilization of Aspect-to-Document Sentiment Composition,0.0812469,"As aspect-level sentiment labels are expensive and labor-intensive to
acquire, zero-shot aspect-level sentiment classification is proposed to learn
classifiers applicable to new domains without using any annotated aspect-level
data. In contrast, document-level sentiment data with ratings are more easily
accessible. In this work, we achieve zero-shot aspect-level sentiment
classification by only using document-level reviews. Our key intuition is that
the sentiment representation of a document is composed of the sentiment
representations of all the aspects of that document. Based on this, we propose
the AF-DSC method to explicitly model such sentiment composition in reviews.
AF-DSC first learns sentiment representations for all potential aspects and
then aggregates aspect-level sentiments into a document-level one to perform
document-level sentiment classification. In this way, we obtain the
aspect-level sentiment classifier as the by-product of the document-level
sentiment classifier. Experimental results on aspect-level sentiment
classification benchmarks demonstrate the effectiveness of explicit utilization
of sentiment composition in document-level sentiment classification. Our model
with only 30k training data outperforms previous work utilizing millions of
data.",https://github.com/explosion/,-1
NeReF: Neural Refractive Field for Fluid Surface Reconstruction and Implicit Representation,0.45294,"Existing neural reconstruction schemes such as Neural Radiance Field (NeRF)
are largely focused on modeling opaque objects. We present a novel neural
refractive field(NeReF) to recover wavefront of transparent fluids by
simultaneously estimating the surface position and normal of the fluid front.
Unlike prior arts that treat the reconstruction target as a single layer of the
surface, NeReF is specifically formulated to recover a volumetric normal field
with its corresponding density field. A query ray will be refracted by NeReF
according to its accumulated refractive point and normal, and we employ the
correspondences and uniqueness of refracted ray for NeReF optimization. We show
NeReF, as a global optimization scheme, can more robustly tackle refraction
distortions detrimental to traditional methods for correspondence matching.
Furthermore, the continuous NeReF representation of wavefront enables view
synthesis as well as normal integration. We validate our approach on both
synthetic and real data and show it is particularly suitable for sparse
multi-view acquisition. We hence build a small light field array and experiment
on various surface shapes to demonstrate high fidelity NeReF reconstruction.",None,-1
Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs,0.980411,"Recent graph-based models for joint multiple intent detection and slot
filling have obtained promising results through modeling the guidance from the
prediction of intents to the decoding of slot filling. However, existing
methods (1) only model the \textit{unidirectional guidance} from intent to
slot; (2) adopt \textit{homogeneous graphs} to model the interactions between
the slot semantics nodes and intent label nodes, which limit the performance.
In this paper, we propose a novel model termed Co-guiding Net, which implements
a two-stage framework achieving the \textit{mutual guidances} between the two
tasks. In the first stage, the initial estimated labels of both tasks are
produced, and then they are leveraged in the second stage to model the mutual
guidances. Specifically, we propose two \textit{heterogeneous graph attention
networks} working on the proposed two \textit{heterogeneous semantics-label
graphs}, which effectively represent the relations among the semantics nodes
and label nodes. Experiment results show that our model outperforms existing
models by a large margin, obtaining a relative improvement of 19.3\% over the
previous best model on MixATIS dataset in overall accuracy.",https://github.com/XingBowen714/Co-guiding,-1
Toward Understanding Bias Correlations for Mitigation in NLP,0.270836,"Natural Language Processing (NLP) models have been found discriminative
against groups of different social identities such as gender and race. With the
negative consequences of these undesired biases, researchers have responded
with unprecedented effort and proposed promising approaches for bias
mitigation. In spite of considerable practical importance, current algorithmic
fairness literature lacks an in-depth understanding of the relations between
different forms of biases. Social bias is complex by nature. Numerous studies
in social psychology identify the ""generalized prejudice"", i.e., generalized
devaluing sentiments across different groups. For example, people who devalue
ethnic minorities are also likely to devalue women and gays. Therefore, this
work aims to provide a first systematic study toward understanding bias
correlations in mitigation. In particular, we examine bias mitigation in two
common NLP tasks -- toxicity detection and word embeddings -- on three social
identities, i.e., race, gender, and religion. Our findings suggest that biases
are correlated and present scenarios in which independent debiasing approaches
dominant in current literature may be insufficient. We further investigate
whether jointly mitigating correlated biases is more desired than independent
and individual debiasing. Lastly, we shed light on the inherent issue of
debiasing-accuracy trade-off in bias mitigation. This study serves to motivate
future research on joint bias mitigation that accounts for correlated biases.",https://github.com/ogencoglu/fair_cyberbullying_detection,-1
Conflict-Aware Pseudo Labeling via Optimal Transport for Entity Alignment,0.149327,"Entity alignment aims to discover unique equivalent entity pairs with the
same meaning across different knowledge graphs (KGs). Existing models have
focused on projecting KGs into a latent embedding space so that inherent
semantics between entities can be captured for entity alignment. However, the
adverse impacts of alignment conflicts have been largely overlooked during
training, thereby limiting the entity alignment performance. To address this
issue, we propose a novel Conflict-aware Pseudo Labeling via Optimal Transport
model (CPL-OT) for entity alignment. The key idea is to iteratively
pseudo-label alignment pairs empowered with conflict-aware optimal transport
(OT) modeling to boost the precision of entity alignment. CPL-OT is composed of
two key components -- entity embedding learning with global-local aggregation
and iterative conflict-aware pseudo labeling -- that mutually reinforce each
other. To mitigate alignment conflicts during pseudo labeling, we propose to
use optimal transport as an effective means to warrant one-to-one entity
alignment between two KGs with the minimal overall transport cost. Extensive
experiments on benchmark datasets validate the superiority of CPL-OT over
state-of-the-art baselines under both settings with and without prior alignment
seeds.",https://github.com/qdin4048/CPL-OT,-1
Active Learning Framework to Automate NetworkTraffic Classification,0.203338,"Recent network traffic classification methods benefitfrom machine learning
(ML) technology. However, there aremany challenges due to use of ML, such as:
lack of high-qualityannotated datasets, data-drifts and other effects causing
aging ofdatasets and ML models, high volumes of network traffic etc. Thispaper
argues that it is necessary to augment traditional workflowsof ML
training&deployment and adapt Active Learning concepton network traffic
analysis. The paper presents a novel ActiveLearning Framework (ALF) to address
this topic. ALF providesprepared software components that can be used to deploy
an activelearning loop and maintain an ALF instance that continuouslyevolves a
dataset and ML model automatically. The resultingsolution is deployable for IP
flow-based analysis of high-speed(100 Gb/s) networks, and also supports
research experiments ondifferent strategies and methods for annotation,
evaluation, datasetoptimization, etc. Finally, the paper lists some research
challengesthat emerge from the first experiments with ALF in practice.",https://github.com/CESNET/ALF,-1
Active Learning by Feature Mixing,0.913428,"The promise of active learning (AL) is to reduce labelling costs by selecting
the most valuable examples to annotate from a pool of unlabelled data.
Identifying these examples is especially challenging with high-dimensional data
(e.g. images, videos) and in low-data regimes. In this paper, we propose a
novel method for batch AL called ALFA-Mix. We identify unlabelled instances
with sufficiently-distinct features by seeking inconsistencies in predictions
resulting from interventions on their representations. We construct
interpolations between representations of labelled and unlabelled instances
then examine the predicted labels. We show that inconsistencies in these
predictions help discovering features that the model is unable to recognise in
the unlabelled instances. We derive an efficient implementation based on a
closed-form solution to the optimal interpolation causing changes in
predictions. Our method outperforms all recent AL approaches in 30 different
settings on 12 benchmarks of images, videos, and non-visual data. The
improvements are especially significant in low-data regimes and on self-trained
vision transformers, where ALFA-Mix outperforms the state-of-the-art in 59% and
43% of the experiments respectively.",https://github.com/aminparvaneh/alpha_mix_active_learning,-1
Streaming Radiance Fields for 3D Video Synthesis,0.592729,"We present an explicit-grid based method for efficiently reconstructing
streaming radiance fields for novel view synthesis of real world dynamic
scenes. Instead of training a single model that combines all the frames, we
formulate the dynamic modeling problem with an incremental learning paradigm in
which per-frame model difference is trained to complement the adaption of a
base model on the current frame. By exploiting the simple yet effective tuning
strategy with narrow bands, the proposed method realizes a feasible framework
for handling video sequences on-the-fly with high training efficiency. The
storage overhead induced by using explicit grid representations can be
significantly reduced through the use of model difference based compression. We
also introduce an efficient strategy to further accelerate model optimization
for each frame. Experiments on challenging video sequences demonstrate that our
approach is capable of achieving a training speed of 15 seconds per-frame with
competitive rendering quality, which attains $1000 \times$ speedup over the
state-of-the-art implicit methods. Code is available at
https://github.com/AlgoHunt/StreamRF.",https://github.com/AlgoHunt/StreamRF,-1
Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion,0.80512,"We present a framework for modeling interactional communication in dyadic
conversations: given multimodal inputs of a speaker, we autoregressively output
multiple possibilities of corresponding listener motion. We combine the motion
and speech audio of the speaker using a motion-audio cross attention
transformer. Furthermore, we enable non-deterministic prediction by learning a
discrete latent representation of realistic listener motion with a novel
motion-encoding VQ-VAE. Our method organically captures the multimodal and
non-deterministic nature of nonverbal dyadic interactions. Moreover, it
produces realistic 3D listener facial motion synchronous with the speaker (see
video). We demonstrate that our method outperforms baselines qualitatively and
quantitatively via a rich suite of experiments. To facilitate this line of
research, we introduce a novel and large in-the-wild dataset of dyadic
conversations. Code, data, and videos available at
https://evonneng.github.io/learning2listen/.",None,-1
Maze Learning using a Hyperdimensional Predictive Processing Cognitive Architecture,0.399824,"We present the COGnitive Neural GENerative system (CogNGen), a cognitive
architecture that combines two neurobiologically-plausible, computational
models: predictive processing and hyperdimensional/vector-symbolic models. We
draw inspiration from architectures such as ACT-R and Spaun/Nengo. CogNGen is
in broad agreement with these, providing a level of detail between ACT-R's
high-level symbolic description of human cognition and Spaun's low-level
neurobiological description, furthermore creating the groundwork for designing
agents that learn continually from diverse tasks and model human performance at
larger scales than what is possible with current systems. We test CogNGen on
four maze-learning tasks, including those that test memory and planning, and
find that CogNGen matches performance of deep reinforcement learning models and
exceeds on a task designed to test memory.",None,-1
Comparison of biomedical relationship extraction methods and models for knowledge graph creation,0.868319,"Biomedical research is growing at such an exponential pace that scientists,
researchers, and practitioners are no more able to cope with the amount of
published literature in the domain. The knowledge presented in the literature
needs to be systematized in such a way that claims and hypotheses can be easily
found, accessed, and validated. Knowledge graphs can provide such a framework
for semantic knowledge representation from literature. However, in order to
build a knowledge graph, it is necessary to extract knowledge as relationships
between biomedical entities and normalize both entities and relationship types.
In this paper, we present and compare few rule-based and machine learning-based
(Naive Bayes, Random Forests as examples of traditional machine learning
methods and DistilBERT, PubMedBERT, T5 and SciFive-based models as examples of
modern deep learning transformers) methods for scalable relationship extraction
from biomedical literature, and for the integration into the knowledge graphs.
We examine how resilient are these various methods to unbalanced and fairly
small datasets. Our experiments show that transformer-based models handle well
both small (due to pre-training on a large dataset) and unbalanced datasets.
The best performing model was the PubMedBERT-based model fine-tuned on balanced
data, with a reported F1-score of 0.92. DistilBERT-based model followed with
F1-score of 0.89, performing faster and with lower resource requirements.
BERT-based models performed better then T5-based generative models.",None,-1
AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models,0.849964,"The objective of pre-trained language models is to learn contextual
representations of textual data. Pre-trained language models have become
mainstream in natural language processing and code modeling. Using probes, a
technique to study the linguistic properties of hidden vector spaces, previous
works have shown that these pre-trained language models encode simple
linguistic properties in their hidden representations. However, none of the
previous work assessed whether these models encode the whole grammatical
structure of a programming language. In this paper, we prove the existence of a
syntactic subspace, lying in the hidden representations of pre-trained language
models, which contain the syntactic information of the programming language. We
show that this subspace can be extracted from the models' representations and
define a novel probing method, the AST-Probe, that enables recovering the whole
abstract syntax tree (AST) of an input code snippet. In our experimentations,
we show that this syntactic subspace exists in five state-of-the-art
pre-trained language models. In addition, we highlight that the middle layers
of the models are the ones that encode most of the AST information. Finally, we
estimate the optimal size of this syntactic subspace and show that its
dimension is substantially lower than those of the models' representation
spaces. This suggests that pre-trained language models use a small part of
their representation spaces to encode syntactic information of the programming
languages.",https://doi.org/10.5281/zenodo.7032076,-1
Generalizing to New Physical Systems via Context-Informed Dynamics Model,0.904573,"Data-driven approaches to modeling physical systems fail to generalize to
unseen systems that share the same general dynamics with the learning domain,
but correspond to different physical contexts. We propose a new framework for
this key problem, context-informed dynamics adaptation (CoDA), which takes into
account the distributional shift across systems for fast and efficient
adaptation to new dynamics. CoDA leverages multiple environments, each
associated to a different dynamic, and learns to condition the dynamics model
on contextual parameters, specific to each environment. The conditioning is
performed via a hypernetwork, learned jointly with a context vector from
observed data. The proposed formulation constrains the search hypothesis space
to foster fast adaptation and better generalization across environments. We
theoretically motivate our approach and show state-of-the-art generalization
results on a set of nonlinear dynamics, representative of a variety of
application domains. We also show, on these systems, that new system parameters
can be inferred from context vectors with minimal supervision. Code is
available at https://github.com/yuan-yin/CoDA .",None,-1
EfficientNeRF: Efficient Neural Radiance Fields,0.894623,"Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications.",https://github.com/dvlab-research/EfﬁcientNeRF,-1
FastRE: Towards Fast Relation Extraction with Convolutional Encoder and Improved Cascade Binary Tagging Framework,0.297536,"Recent work for extracting relations from texts has achieved excellent
performance. However, most existing methods pay less attention to the
efficiency, making it still challenging to quickly extract relations from
massive or streaming text data in realistic scenarios. The main efficiency
bottleneck is that these methods use a Transformer-based pre-trained language
model for encoding, which heavily affects the training speed and inference
speed. To address this issue, we propose a fast relation extraction model
(FastRE) based on convolutional encoder and improved cascade binary tagging
framework. Compared to previous work, FastRE employs several innovations to
improve efficiency while also keeping promising performance. Concretely, FastRE
adopts a novel convolutional encoder architecture combined with dilated
convolution, gated unit and residual connection, which significantly reduces
the computation cost of training and inference, while maintaining the
satisfactory performance. Moreover, to improve the cascade binary tagging
framework, FastRE first introduces a type-relation mapping mechanism to
accelerate tagging efficiency and alleviate relation redundancy, and then
utilizes a position-dependent adaptive thresholding strategy to obtain higher
tagging accuracy and better model generalization. Experimental results
demonstrate that FastRE is well balanced between efficiency and performance,
and achieves 3-10x training speed, 7-15x inference speed faster, and 1/100
parameters compared to the state-of-the-art models, while the performance is
still competitive.",https://github.com/seukgcode/FastRE,-1
Turning Stocks into Memes: A Dataset for Understanding How Social Communities Can Drive Wall Street,0.148272,"Who actually expresses an intent to buy GameStop shares on Reddit? What
convinces people to buy stocks? Are people convinced to support a coordinated
plan to adversely impact Wall Street investors? Existing literature on
understanding intent has mainly relied on surveys and self reporting; however
there are limitations to these methodologies. Hence, in this paper, we develop
an annotated dataset of communications centered on the GameStop phenomenon to
analyze the subscriber intentions behaviors within the r/WallStreetBets
community to buy (or not buy) stocks. Likewise, we curate a dataset to better
understand how intent interacts with a user's general support towards the
coordinated actions of the community for GameStop. Overall, our dataset can
provide insight to social scientists on the persuasive power to buy into social
movements online by adopting common language and narrative. WARNING: This paper
contains offensive language that commonly appears on Reddit's r/WallStreetBets
subreddit.",None,-1
Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping,0.656316,"In machine learning, an agent needs to estimate uncertainty to efficiently
explore and adapt and to make effective decisions. A common approach to
uncertainty estimation maintains an ensemble of models. In recent years,
several approaches have been proposed for training ensembles, and conflicting
views prevail with regards to the importance of various ingredients of these
approaches. In this paper, we aim to address the benefits of two ingredients --
prior functions and bootstrapping -- which have come into question. We show
that prior functions can significantly improve an ensemble agent's joint
predictions across inputs and that bootstrapping affords additional benefits if
the signal-to-noise ratio varies across inputs. Our claims are justified by
both theoretical and experimental results.",https://github.com/deepmind/enn,-1
Hybrid Multimodal Fusion for Humor Detection,0.675339,"In this paper, we present our solution to the MuSe-Humor sub-challenge of the
Multimodal Emotional Challenge (MuSe) 2022. The goal of the MuSe-Humor
sub-challenge is to detect humor and calculate AUC from audiovisual recordings
of German football Bundesliga press conferences. It is annotated for humor
displayed by the coaches. For this sub-challenge, we first build a discriminant
model using the transformer module and BiLSTM module, and then propose a hybrid
fusion strategy to use the prediction results of each modality to improve the
performance of the model. Our experiments demonstrate the effectiveness of our
proposed model and hybrid fusion strategy on multimodal fusion, and the AUC of
our proposed model on the test set is 0.8972.",None,-1
Federated learning for violence incident prediction in a simulated cross-institutional psychiatric setting,0.903522,"Inpatient violence is a common and severe problem within psychiatry. Knowing
who might become violent can influence staffing levels and mitigate severity.
Predictive machine learning models can assess each patient's likelihood of
becoming violent based on clinical notes. Yet, while machine learning models
benefit from having more data, data availability is limited as hospitals
typically do not share their data for privacy preservation. Federated Learning
(FL) can overcome the problem of data limitation by training models in a
decentralised manner, without disclosing data between collaborators. However,
although several FL approaches exist, none of these train Natural Language
Processing models on clinical notes. In this work, we investigate the
application of Federated Learning to clinical Natural Language Processing,
applied to the task of Violence Risk Assessment by simulating a
cross-institutional psychiatric setting. We train and compare four models: two
local models, a federated model and a data-centralised model. Our results
indicate that the federated model outperforms the local models and has similar
performance as the data-centralised model. These findings suggest that
Federated Learning can be used successfully in a cross-institutional setting
and is a step towards new applications of Federated Learning based on clinical
notes",None,-1
"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends",0.90895,"This paper surveys vision-language pre-training (VLP) methods for multimodal
intelligence that have been developed in the last few years. We group these
approaches into three categories: ($i$) VLP for image-text tasks, such as image
captioning, image-text retrieval, visual question answering, and visual
grounding; ($ii$) VLP for core computer vision tasks, such as (open-set) image
classification, object detection, and segmentation; and ($iii$) VLP for
video-text tasks, such as video captioning, video-text retrieval, and video
question answering. For each category, we present a comprehensive review of
state-of-the-art methods, and discuss the progress that has been made and
challenges still being faced, using specific systems and models as case
studies. In addition, for each category, we discuss advanced topics being
actively explored in the research community, such as big foundation models,
unified modeling, in-context few-shot learning, knowledge, robustness, and
computer vision in the wild, to name a few.",None,-1
HYPRO: A Hybridly Normalized Probabilistic Model for Long-Horizon Prediction of Event Sequences,0.559377,"In this paper, we tackle the important yet under-investigated problem of
making long-horizon prediction of event sequences. Existing state-of-the-art
models do not perform well at this task due to their autoregressive structure.
We propose HYPRO, a hybridly normalized probabilistic model that naturally fits
this task: its first part is an autoregressive base model that learns to
propose predictions; its second part is an energy function that learns to
reweight the proposals such that more realistic predictions end up with higher
probabilities. We also propose efficient training and inference algorithms for
this model. Experiments on multiple real-world datasets demonstrate that our
proposed HYPRO model can significantly outperform previous models at making
long-horizon predictions of future events. We also conduct a range of ablation
studies to investigate the effectiveness of each component of our proposed
methods.",https://github.com/iLampard/hypro_tpp,-1
A new Hyper-heuristic based on Adaptive Simulated Annealing and Reinforcement Learning for the Capacitated Electric Vehicle Routing Problem,0.724777,"Electric vehicles (EVs) have been adopted in urban areas to reduce
environmental pollution and global warming as a result of the increasing number
of freight vehicles. However, there are still deficiencies in routing the
trajectories of last-mile logistics that continue to impact social and economic
sustainability. For that reason, in this paper, a hyper-heuristic (HH) approach
called Hyper-heuristic Adaptive Simulated Annealing with Reinforcement Learning
(HHASA$_{RL}$) is proposed. It is composed of a multi-armed bandit method and
the self-adaptive Simulated Annealing (SA) metaheuristic algorithm for solving
the problem called Capacitated Electric Vehicle Routing Problem (CEVRP). Due to
the limited number of charging stations and the travel range of EVs, the EVs
must require battery recharging moments in advance and reduce travel times and
costs. The HH implemented improves multiple minimum best-known solutions and
obtains the best mean values for some high-dimensional instances for the
proposed benchmark for the IEEE WCCI2020 competition.",None,4806
Efficient Training of Neural Transducer for Speech Recognition,0.477627,"As one of the most popular sequence-to-sequence modeling approaches for
speech recognition, the RNN-Transducer has achieved evolving performance with
more and more sophisticated neural network models of growing size and
increasing training epochs. While strong computation resources seem to be the
prerequisite of training superior models, we try to overcome it by carefully
designing a more efficient training pipeline. In this work, we propose an
efficient 3-stage progressive training pipeline to build highly-performing
neural transducer models from scratch with very limited computation resources
in a reasonable short time period. The effectiveness of each stage is
experimentally verified on both Librispeech and Switchboard corpora. The
proposed pipeline is able to train transducer models approaching
state-of-the-art performance with a single GPU in just 2-3 weeks. Our best
conformer transducer achieves 4.1% WER on Librispeech test-other with only 35
epochs of training.",None,-1
EventGraph: Event Extraction as Semantic Graph Parsing,0.330405,"Event extraction involves the detection and extraction of both the event
triggers and corresponding event arguments. Existing systems often decompose
event extraction into multiple subtasks, without considering their possible
interactions. In this paper, we propose EventGraph, a joint framework for event
extraction, which encodes events as graphs. We represent event triggers and
arguments as nodes in a semantic graph. Event extraction therefore becomes a
graph parsing problem, which provides the following advantages: 1) performing
event detection and argument extraction jointly; 2) detecting and extracting
multiple events from a piece of text; and 3) capturing the complicated
interaction between event arguments and triggers. Experimental results on
ACE2005 show that our model is competitive to state-of-the-art systems and has
substantially improved the results on argument extraction. Additionally, we
create two new datasets from ACE2005 where we keep the entire text spans for
event arguments, instead of just the head word(s). Our code and models are
released as open-source.",https://github.com/huiling-y/EventGraph,-1
MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation,0.999827,"Applying existing methods to emotional support conversation -- which provides
valuable assistance to people who are in need -- has two major limitations: (a)
they generally employ a conversation-level emotion label, which is too
coarse-grained to capture user's instant mental state; (b) most of them focus
on expressing empathy in the response(s) rather than gradually reducing user's
distress. To address the problems, we propose a novel model \textbf{MISC},
which firstly infers the user's fine-grained emotional status, and then
responds skillfully using a mixture of strategy. Experimental results on the
benchmark dataset demonstrate the effectiveness of our method and reveal the
benefits of fine-grained emotion understanding as well as mixed-up strategy
modeling. Our code and data could be found in
\url{https://github.com/morecry/MISC}.",https://github.com/morecry/MISC,-1
Diffusion-LM Improves Controllable Text Generation,0.999017,"Controlling the behavior of language models (LMs) without re-training is a
major open problem in natural language generation. While recent works have
demonstrated successes on controlling simple sentence attributes (e.g.,
sentiment), there has been little progress on complex, fine-grained controls
(e.g., syntactic structure). To address this challenge, we develop a new
non-autoregressive language model based on continuous diffusions that we call
Diffusion-LM. Building upon the recent successes of diffusion models in
continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian
vectors into word vectors, yielding a sequence of intermediate latent
variables. The continuous, hierarchical nature of these intermediate variables
enables a simple gradient-based algorithm to perform complex, controllable
generation tasks. We demonstrate successful control of Diffusion-LM for six
challenging fine-grained control tasks, significantly outperforming prior work.",https://github.com/XiangLi1999/Diffusion-LM.git,-1
MentSum: A Resource for Exploring Summarization of Mental Health Online Posts,0.49556,"Mental health remains a significant challenge of public health worldwide.
With increasing popularity of online platforms, many use the platforms to share
their mental health conditions, express their feelings, and seek help from the
community and counselors. Some of these platforms, such as Reachout, are
dedicated forums where the users register to seek help. Others such as Reddit
provide subreddits where the users publicly but anonymously post their mental
health distress. Although posts are of varying length, it is beneficial to
provide a short, but informative summary for fast processing by the counselors.
To facilitate research in summarization of mental health online posts, we
introduce Mental Health Summarization dataset, MentSum, containing over 24k
carefully selected user posts from Reddit, along with their short user-written
summary (called TLDR) in English from 43 mental health subreddits. This
domain-specific dataset could be of interest not only for generating short
summaries on Reddit, but also for generating summaries of posts on the
dedicated mental health forums such as Reachout. We further evaluate both
extractive and abstractive state-of-the-art summarization baselines in terms of
Rouge scores, and finally conduct an in-depth human evaluation study of both
user-written and system-generated summaries, highlighting challenges in this
research.",https://github.com/miso-belica/sumy,-1
Consistency Learning via Decoding Path Augmentation for Transformers in Human Object Interaction Detection,0.643643,"Human-Object Interaction detection is a holistic visual recognition task that
entails object detection as well as interaction classification. Previous works
of HOI detection has been addressed by the various compositions of subset
predictions, e.g., Image -> HO -> I, Image -> HI -> O. Recently, transformer
based architecture for HOI has emerged, which directly predicts the HOI
triplets in an end-to-end fashion (Image -> HOI). Motivated by various
inference paths for HOI detection, we propose cross-path consistency learning
(CPC), which is a novel end-to-end learning strategy to improve HOI detection
for transformers by leveraging augmented decoding paths. CPC learning enforces
all the possible predictions from permuted inference sequences to be
consistent. This simple scheme makes the model learn consistent
representations, thereby improving generalization without increasing model
capacity. Our experiments demonstrate the effectiveness of our method, and we
achieved significant improvement on V-COCO and HICO-DET compared to the
baseline models. Our code is available at https://github.com/mlvlab/CPChoi.",https://github.com/mlvlab/CPChoi,-1
Transition-based Semantic Role Labeling with Pointer Networks,0.12636,"Semantic role labeling (SRL) focuses on recognizing the predicate-argument
structure of a sentence and plays a critical role in many natural language
processing tasks such as machine translation and question answering.
Practically all available methods do not perform full SRL, since they rely on
pre-identified predicates, and most of them follow a pipeline strategy, using
specific models for undertaking one or several SRL subtasks. In addition,
previous approaches have a strong dependence on syntactic information to
achieve state-of-the-art performance, despite being syntactic trees equally
hard to produce. These simplifications and requirements make the majority of
SRL systems impractical for real-world applications. In this article, we
propose the first transition-based SRL approach that is capable of completely
processing an input sentence in a single left-to-right pass, with neither
leveraging syntactic information nor resorting to additional modules. Thanks to
our implementation based on Pointer Networks, full SRL can be accurately and
efficiently done in $O(n^2)$, achieving the best performance to date on the
majority of languages from the CoNLL-2009 shared task.",None,-1
Utility Theory for Sequential Decision Making,0.193046,"The von Neumann-Morgenstern (VNM) utility theorem shows that under certain
axioms of rationality, decision-making is reduced to maximizing the expectation
of some utility function. We extend these axioms to increasingly structured
sequential decision making settings and identify the structure of the
corresponding utility functions. In particular, we show that memoryless
preferences lead to a utility in the form of a per transition reward and
multiplicative factor on the future return. This result motivates a
generalization of Markov Decision Processes (MDPs) with this structure on the
agent's returns, which we call Affine-Reward MDPs. A stronger constraint on
preferences is needed to recover the commonly used cumulative sum of scalar
rewards in MDPs. A yet stronger constraint simplifies the utility function for
goal-seeking agents in the form of a difference in some function of states that
we call potential functions. Our necessary and sufficient conditions demystify
the reward hypothesis that underlies the design of rational agents in
reinforcement learning by adding an axiom to the VNM rationality axioms and
motivates new directions for AI research involving sequential decision making.",None,-1
Prompting Large Pre-trained Vision-Language Models For Compositional Concept Learning,0.241998,"This work explores the zero-shot compositional learning ability of large
pre-trained vision-language models(VLMs) within the prompt-based learning
framework and propose a model (\textit{PromptCompVL}) to solve the compositonal
zero-shot learning (CZSL) problem. \textit{PromptCompVL} makes two design
choices: first, it uses a soft-prompting instead of hard-prompting to inject
learnable parameters to reprogram VLMs for compositional learning. Second, to
address the compositional challenge, it uses the soft-embedding layer to learn
primitive concepts in different combinations. By combining both soft-embedding
and soft-prompting, \textit{PromptCompVL} achieves state-of-the-art performance
on the MIT-States dataset. Furthermore, our proposed model achieves consistent
improvement compared to other CLIP-based methods which shows the effectiveness
of the proposed prompting strategies for CZSL.",None,1473
A Human-Centric Assessment Framework for AI,0.456418,"With the rise of AI systems in real-world applications comes the need for
reliable and trustworthy AI. An essential aspect of this are explainable AI
systems. However, there is no agreed standard on how explainable AI systems
should be assessed. Inspired by the Turing test, we introduce a human-centric
assessment framework where a leading domain expert accepts or rejects the
solutions of an AI system and another domain expert. By comparing the
acceptance rates of provided solutions, we can assess how the AI system
performs compared to the domain expert, and whether the AI system's
explanations (if provided) are human-understandable. This setup -- comparable
to the Turing test -- can serve as a framework for a wide range of
human-centric AI system assessments. We demonstrate this by presenting two
instantiations: (1) an assessment that measures the classification accuracy of
a system with the option to incorporate label uncertainties; (2) an assessment
where the usefulness of provided explanations is determined in a human-centric
manner.",None,1665
A Unified Framework for Attention-Based Few-Shot Object Detection,0.0906256,"Few-Shot Object Detection (FSOD) is a rapidly growing field in computer
vision. It consists in finding all occurrences of a given set of classes with
only a few annotated examples for each class. Numerous methods have been
proposed to address this challenge and most of them are based on attention
mechanisms. However, the great variety of classic object detection frameworks
and training strategies makes performance comparison between methods difficult.
In particular, for attention-based FSOD methods, it is laborious to compare the
impact of the different attention mechanisms on performance. This paper aims at
filling this shortcoming. To do so, a flexible framework is proposed to allow
the implementation of most of the attention techniques available in the
literature. To properly introduce such a framework, a detailed review of the
existing FSOD methods is firstly provided. Some different attention mechanisms
are then reimplemented within the framework and compared with all other
parameters fixed.",None,-1
Encoding Concepts in Graph Neural Networks,0.559215,"The opaque reasoning of Graph Neural Networks induces a lack of human trust.
Existing graph network explainers attempt to address this issue by providing
post-hoc explanations, however, they fail to make the model itself more
interpretable. To fill this gap, we introduce the Concept Encoder Module, the
first differentiable concept-discovery approach for graph networks. The
proposed approach makes graph networks explainable by design by first
discovering graph concepts and then using these to solve the task. Our results
demonstrate that this approach allows graph networks to: (i) attain model
accuracy comparable with their equivalent vanilla versions, (ii) discover
meaningful concepts that achieve high concept completeness and purity scores,
(iii) provide high-quality concept-based logic explanations for their
prediction, and (iv) support effective interventions at test time: these can
increase human trust as well as significantly improve model performance.",None,-1
AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages,0.512352,"In recent years, multilingual pre-trained language models have gained
prominence due to their remarkable performance on numerous downstream Natural
Language Processing tasks (NLP). However, pre-training these large multilingual
language models requires a lot of training data, which is not available for
African Languages. Active learning is a semi-supervised learning algorithm, in
which a model consistently and dynamically learns to identify the most
beneficial samples to train itself on, in order to achieve better optimization
and performance on downstream tasks. Furthermore, active learning effectively
and practically addresses real-world data scarcity. Despite all its benefits,
active learning, in the context of NLP and especially multilingual language
models pretraining, has received little consideration. In this paper, we
present AfroLM, a multilingual language model pretrained from scratch on 23
African languages (the largest effort to date) using our novel self-active
learning framework. Pretrained on a dataset significantly (14x) smaller than
existing baselines, AfroLM outperforms many multilingual pretrained language
models (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text
classification, and sentiment analysis). Additional out-of-domain sentiment
analysis experiments show that \textbf{AfroLM} is able to generalize well
across various domains. We release the code source, and our datasets used in
our framework at https://github.com/bonaventuredossou/MLM_AL.",https://github.com/bonaventuredossou/MLM_AL,-1
Masked Event Modeling: Self-Supervised Pretraining for Event Cameras,0.603911,"Event cameras asynchronously capture brightness changes with low latency,
high temporal resolution, and high dynamic range. However, annotation of event
data is a costly and laborious process, which limits the use of deep learning
methods for classification and other semantic tasks with the event modality. To
reduce the dependency on labeled event data, we introduce Masked Event Modeling
(MEM), a self-supervised framework for events. Our method pretrains a neural
network on unlabeled events, which can originate from any event camera
recording. Subsequently, the pretrained model is finetuned on a downstream
task, leading to a consistent improvement of the task accuracy. For example,
our method reaches state-of-the-art classification accuracy across three
datasets, N-ImageNet, N-Cars, and N-Caltech101, increasing the top-1 accuracy
of previous work by significant margins. When tested on real-world event data,
MEM is even superior to supervised RGB-based pretraining. The models pretrained
with MEM are also label-efficient and generalize well to the dense task of
semantic image segmentation.",https://github.com/tum-vision/mem,-1
TransLog: A Unified Transformer-based Framework for Log Anomaly Detection,0.483908,"Log anomaly detection is a key component in the field of artificial
intelligence for IT operations (AIOps). Considering log data of variant
domains, retraining the whole network for unknown domains is inefficient in
real industrial scenarios especially for low-resource domains. However,
previous deep models merely focused on extracting the semantics of log sequence
in the same domain, leading to poor generalization on multi-domain logs.
Therefore, we propose a unified Transformer-based framework for log anomaly
detection (\ourmethod{}), which is comprised of the pretraining and
adapter-based tuning stage. Our model is first pretrained on the source domain
to obtain shared semantic knowledge of log data. Then, we transfer the
pretrained model to the target domain via the adapter-based tuning. The
proposed method is evaluated on three public datasets including one source
domain and two target domains. The experimental results demonstrate that our
simple yet efficient approach, with fewer trainable parameters and lower
training costs in the target domain, achieves state-of-the-art performance on
three benchmarks.",None,-1
Transformer Based Multi-Grained Features for Unsupervised Person Re-Identification,0.612505,"Multi-grained features extracted from convolutional neural networks (CNNs)
have demonstrated their strong discrimination ability in supervised person
re-identification (Re-ID) tasks. Inspired by them, this work investigates the
way of extracting multi-grained features from a pure transformer network to
address the unsupervised Re-ID problem that is label-free but much more
challenging. To this end, we build a dual-branch network architecture based
upon a modified Vision Transformer (ViT). The local tokens output in each
branch are reshaped and then uniformly partitioned into multiple stripes to
generate part-level features, while the global tokens of two branches are
averaged to produce a global feature. Further, based upon offline-online
associated camera-aware proxies (O2CAP) that is a top-performing unsupervised
Re-ID method, we define offline and online contrastive learning losses with
respect to both global and part-level features to conduct unsupervised
learning. Extensive experiments on three person Re-ID datasets show that the
proposed method outperforms state-of-the-art unsupervised methods by a
considerable margin, greatly mitigating the gap to supervised counterparts.
Code will be available soon at https://github.com/RikoLi/WACV23-workshop-TMGF.",https://github.com/RikoLi/WACV23-workshop-TMGF,-1
XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence,0.953206,"Recent advances in machine learning have significantly improved the
understanding of source code data and achieved good performance on a number of
downstream tasks. Open source repositories like GitHub enable this process with
rich unlabeled code data. However, the lack of high quality labeled data has
largely hindered the progress of several code related tasks, such as program
translation, summarization, synthesis, and code search. This paper introduces
XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for
cross-lingual code intelligence. Our dataset contains fine-grained parallel
data from 8 languages (7 commonly used programming languages and English), and
supports 10 cross-lingual code tasks. To the best of our knowledge, it is the
largest parallel dataset for source code both in terms of size and the number
of languages. We also provide the performance of several state-of-the-art
baseline models for each task. We believe this new dataset can be a valuable
asset for the research community and facilitate the development and validation
of new methods for cross-lingual code intelligence.",https://github.com/reddy-lab-code-research/XLCoST,-1
Anytime-Lidar: Deadline-aware 3D Object Detection,0.135511,"In this work, we present a novel scheduling framework enabling anytime
perception for deep neural network (DNN) based 3D object detection pipelines.
We focus on computationally expensive region proposal network (RPN) and
per-category multi-head detector components, which are common in 3D object
detection pipelines, and make them deadline-aware. We propose a scheduling
algorithm, which intelligently selects the subset of the components to make
effective time and accuracy trade-off on the fly. We minimize accuracy loss of
skipping some of the neural network sub-components by projecting previously
detected objects onto the current scene through estimations. We apply our
approach to a state-of-art 3D object detection network, PointPillars, and
evaluate its performance on Jetson Xavier AGX using nuScenes dataset. Compared
to the baselines, our approach significantly improve the network's accuracy
under various deadline constraints.",https://github.com/open-mmlab/OpenPCDet,-1
Evaluating Long-Term Memory in 3D Mazes,0.536736,"Intelligent agents need to remember salient information to reason in
partially-observed environments. For example, agents with a first-person view
should remember the positions of relevant objects even if they go out of view.
Similarly, to effectively navigate through rooms agents need to remember the
floor plan of how rooms are connected. However, most benchmark tasks in
reinforcement learning do not test long-term memory in agents, slowing down
progress in this important research direction. In this paper, we introduce the
Memory Maze, a 3D domain of randomized mazes specifically designed for
evaluating long-term memory in agents. Unlike existing benchmarks, Memory Maze
measures long-term memory separate from confounding agent abilities and
requires the agent to localize itself by integrating information over time.
With Memory Maze, we propose an online reinforcement learning benchmark, a
diverse offline dataset, and an offline probing evaluation. Recording a human
player establishes a strong baseline and verifies the need to build up and
retain memories, which is reflected in their gradually increasing rewards
within each episode. We find that current algorithms benefit from training with
truncated backpropagation through time and succeed on small mazes, but fall
short of human performance on the large mazes, leaving room for future
algorithmic designs to be evaluated on the Memory Maze.",https://github.com/jurgisp/memory-maze,-1
Multi-View Dreaming: Multi-View World Model with Contrastive Learning,0.298509,"In this paper, we propose Multi-View Dreaming, a novel reinforcement learning
agent for integrated recognition and control from multi-view observations by
extending Dreaming. Most current reinforcement learning method assumes a
single-view observation space, and this imposes limitations on the observed
data, such as lack of spatial information and occlusions. This makes obtaining
ideal observational information from the environment difficult and is a
bottleneck for real-world robotics applications. In this paper, we use
contrastive learning to train a shared latent space between different
viewpoints, and show how the Products of Experts approach can be used to
integrate and control the probability distributions of latent states for
multiple viewpoints. We also propose Multi-View DreamingV2, a variant of
Multi-View Dreaming that uses a categorical distribution to model the latent
state instead of the Gaussian distribution. Experiments show that the proposed
method outperforms simple extensions of existing methods in a realistic robot
control task.",None,3802
A Unified View of Masked Image Modeling,0.728224,"Masked image modeling has demonstrated great potential to eliminate the
label-hungry problem of training large-scale vision Transformers, achieving
impressive performance on various downstream tasks. In this work, we propose a
unified view of masked image modeling after revisiting existing methods. Under
the unified view, we introduce a simple yet effective method, termed as
MaskDistill, which reconstructs normalized semantic features from teacher
models at the masked positions, conditioning on corrupted input images.
Experimental results on image classification and semantic segmentation show
that MaskDistill achieves comparable or superior performance than
state-of-the-art methods. When using the huge vision Transformer and
pretraining 300 epochs, MaskDistill obtains 88.3% fine-tuning top-1 accuracy on
ImageNet-1k (224 size) and 58.8% semantic segmentation mIoU metric on ADE20k
(512 size). The code and pretrained models will be available at
https://aka.ms/unimim.",https://aka.ms/unimim,-1
IDANI: Inference-time Domain Adaptation via Neuron-level Interventions,0.459322,"Large pre-trained models are usually fine-tuned on downstream task data, and
tested on unseen data. When the train and test data come from different
domains, the model is likely to struggle, as it is not adapted to the test
domain. We propose a new approach for domain adaptation (DA), using
neuron-level interventions: We modify the representation of each test example
in specific neurons, resulting in a counterfactual example from the source
domain, which the model is more familiar with. The modified example is then fed
back into the model. While most other DA methods are applied during training
time, ours is applied during inference only, making it more efficient and
applicable. Our experiments show that our method improves performance on unseen
domains.",https://github.com/technion-cs-nlp/idani,-1
Automatic Related Work Generation: A Meta Study,0.391008,"Academic research is an exploration activity to solve problems that have
never been resolved before. By this nature, each academic research work is
required to perform a literature review to distinguish its novelties that have
not been addressed by prior works. In natural language processing, this
literature review is usually conducted under the ""Related Work"" section. The
task of automatic related work generation aims to automatically generate the
""Related Work"" section given the rest of the research paper and a list of cited
papers. Although this task was proposed over 10 years ago, it received little
attention until very recently, when it was cast as a variant of the scientific
multi-document summarization problem. However, even today, the problems of
automatic related work and citation text generation are not yet standardized.
In this survey, we conduct a meta-study to compare the existing literature on
related work generation from the perspectives of problem formulation, dataset
collection, methodological approach, performance evaluation, and future
prospects to provide the reader insight into the progress of the
state-of-the-art studies, as well as and how future studies can be conducted.
We also survey relevant fields of study that we suggest future work to consider
integrating.",None,-1
Domain Adaptation for Time-Series Classification to Mitigate Covariate Shift,0.592017,"The performance of a machine learning model degrades when it is applied to
data from a similar but different domain than the data it has initially been
trained on. To mitigate this domain shift problem, domain adaptation (DA)
techniques search for an optimal transformation that converts the (current)
input data from a source domain to a target domain to learn a domain-invariant
representation that reduces domain discrepancy. This paper proposes a novel
supervised DA based on two steps. First, we search for an optimal
class-dependent transformation from the source to the target domain from a few
samples. We consider optimal transport methods such as the earth mover's
distance, Sinkhorn transport and correlation alignment. Second, we use
embedding similarity techniques to select the corresponding transformation at
inference. We use correlation metrics and higher-order moment matching
techniques. We conduct an extensive evaluation on time-series datasets with
domain shift including simulated and various online handwriting datasets to
demonstrate the performance.",None,-1
Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes,0.281679,"Analogy-making gives rise to reasoning, abstraction, flexible categorization
and counterfactual inference -- abilities lacking in even the best AI systems
today. Much research has suggested that analogies are key to non-brittle
systems that can adapt to new domains. Despite their importance, analogies
received little attention in the NLP community, with most research focusing on
simple word analogies. Work that tackled more complex analogies relied heavily
on manually constructed, hard-to-scale input representations. In this work, we
explore a more realistic, challenging setup: our input is a pair of natural
language procedural texts, describing a situation or a process (e.g., how the
heart works/how a pump works). Our goal is to automatically extract entities
and their relations from the text and find a mapping between the different
domains based on relational similarity (e.g., blood is mapped to water). We
develop an interpretable, scalable algorithm and demonstrate that it identifies
the correct mappings 87% of the time for procedural texts and 94% for stories
from cognitive-psychology literature. We show it can extract analogies from a
large dataset of procedural texts, achieving 79% precision (analogy prevalence
in data: 3%). Lastly, we demonstrate that our algorithm is robust to
paraphrasing the input texts.",https://github.com/orensul/analogies_mining,-1
Diverse Plausible 360-Degree Image Outpainting for Efficient 3DCG Background Creation,0.76693,"We address the problem of generating a 360-degree image from a single image
with a narrow field of view by estimating its surroundings. Previous methods
suffered from overfitting to the training resolution and deterministic
generation. This paper proposes a completion method using a transformer for
scene modeling and novel methods to improve the properties of a 360-degree
image on the output image. Specifically, we use CompletionNets with a
transformer to perform diverse completions and AdjustmentNet to match color,
stitching, and resolution with an input image, enabling inference at any
resolution. To improve the properties of a 360-degree image on an output image,
we also propose WS-perceptual loss and circular inference. Thorough experiments
show that our method outperforms state-of-the-art (SOTA) methods both
qualitatively and quantitatively. For example, compared to SOTA methods, our
method completes images 16 times larger in resolution and achieves 1.7 times
lower Frechet inception distance (FID). Furthermore, we propose a pipeline that
uses the completion results for lighting and background of 3DCG scenes. Our
plausible background completion enables perceptually natural results in the
application of inserting virtual objects with specular surfaces.",None,-1
Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss,0.626417,"In this paper, we study color image inpainting as a pure quaternion matrix
completion problem. In the literature, the theoretical guarantee for quaternion
matrix completion is not well-established. Our main aim is to propose a new
minimization problem with an objective combining nuclear norm and a quadratic
loss weighted among three channels. To fill the theoretical vacancy, we obtain
the error bound in both clean and corrupted regimes, which relies on some new
results of quaternion matrices. A general Gaussian noise is considered in
robust completion where all observations are corrupted. Motivated by the error
bound, we propose to handle unbalanced or correlated noise via a cross-channel
weight in the quadratic loss, with the main purpose of rebalancing noise level,
or removing noise correlation. Extensive experimental results on synthetic and
color image data are presented to confirm and demonstrate our theoretical
findings.",None,-1
Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation,0.501909,"Single-Image Super-Resolution can support robotic tasks in environments where
a reliable visual stream is required to monitor the mission, handle
teleoperation or study relevant visual details. In this work, we propose an
efficient Generative Adversarial Network model for real-time Super-Resolution,
called EdgeSRGAN (code available at https://github.com/PIC4SeR/EdgeSRGAN). We
adopt a tailored architecture of the original SRGAN and model quantization to
boost the execution on CPU and Edge TPU devices, achieving up to 200 fps
inference. We further optimize our model by distilling its knowledge to a
smaller version of the network and obtain remarkable improvements compared to
the standard training approach. Our experiments show that our fast and
lightweight model preserves considerably satisfying image quality compared to
heavier state-of-the-art models. Finally, we conduct experiments on image
transmission with bandwidth degradation to highlight the advantages of the
proposed system for mobile robotic applications.",https://github.com/PIC4SeR/EdgeSRGAN,-1
Face Super-Resolution with Progressive Embedding of Multi-scale Face Priors,0.0843075,"The face super-resolution (FSR) task is to reconstruct high-resolution face
images from low-resolution inputs. Recent works have achieved success on this
task by utilizing facial priors such as facial landmarks. Most existing methods
pay more attention to global shape and structure information, but less to local
texture information, which makes them cannot recover local details well. In
this paper, we propose a novel recurrent convolutional network based framework
for face super-resolution, which progressively introduces both global shape and
local texture information. We take full advantage of the intermediate outputs
of the recurrent network, and landmarks information and facial action units
(AUs) information are extracted in the output of the first and second steps
respectively, rather than low-resolution input. Moreover, we introduced AU
classification results as a novel quantitative metric for facial details
restoration. Extensive experiments show that our proposed method significantly
outperforms state-of-the-art FSR methods in terms of image quality and facial
details restoration.",None,-1
Interpretable Deep Learning Classifier by Detection of Prototypical Parts on Kidney Stones Images,0.180953,"Identifying the type of kidney stones can allow urologists to determine their
formation cause, improving the early prescription of appropriate treatments to
diminish future relapses. However, currently, the associated ex-vivo diagnosis
(known as morpho-constitutional analysis, MCA) is time-consuming, expensive,
and requires a great deal of experience, as it requires a visual analysis
component that is highly operator dependant. Recently, machine learning methods
have been developed for in-vivo endoscopic stone recognition. Shallow methods
have been demonstrated to be reliable and interpretable but exhibit low
accuracy, while deep learning-based methods yield high accuracy but are not
explainable. However, high stake decisions require understandable
computer-aided diagnosis (CAD) to suggest a course of action based on
reasonable evidence, rather than merely prescribe one. Herein, we investigate
means for learning part-prototypes (PPs) that enable interpretable models. Our
proposal suggests a classification for a kidney stone patch image and provides
explanations in a similar way as those used on the MCA method.",None,-1
Controlling Styles in Neural Machine Translation with Activation Prompt,0.400829,"Controlling styles in neural machine translation (NMT) has attracted wide
attention, as it is crucial for enhancing user experience. Earlier studies on
this topic typically concentrate on regulating the level of formality and
achieve some progress in this area. However, they still encounter two major
challenges. The first is the difficulty in style evaluation. The style
comprises various aspects such as lexis, syntax, and others that provide
abundant information. Nevertheless, only formality has been thoroughly
investigated. The second challenge involves excessive dependence on incremental
adjustments, particularly when new styles are necessary. To address both
challenges, this paper presents a new benchmark and approach. A multiway
stylized machine translation (MSMT) benchmark is introduced, incorporating
diverse categories of styles across four linguistic domains. Then, we propose a
method named style activation prompt (StyleAP) by retrieving prompts from
stylized monolingual corpus, which does not require extra fine-tuning.
Experiments show that StyleAP could effectively control the style of
translation and achieve remarkable performance.",https://github.com/bytedance/neurst,-1
High-Resource Methodological Bias in Low-Resource Investigations,0.160063,"The central bottleneck for low-resource NLP is typically regarded to be the
quantity of accessible data, overlooking the contribution of data quality. This
is particularly seen in the development and evaluation of low-resource systems
via down sampling of high-resource language data. In this work we investigate
the validity of this approach, and we specifically focus on two well-known NLP
tasks for our empirical investigations: POS-tagging and machine translation. We
show that down sampling from a high-resource language results in datasets with
different properties than the low-resource datasets, impacting the model
performance for both POS-tagging and machine translation. Based on these
results we conclude that naive down sampling of datasets results in a biased
view of how well these systems work in a low-resource scenario.",https://github.com/flairNLP/flair/,-1
Towards Understanding Mixture of Experts in Deep Learning,0.856862,"The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by
a router, has achieved great success in deep learning. However, the
understanding of such architecture remains elusive. In this paper, we formally
study how the MoE layer improves the performance of neural network learning and
why the mixture model will not collapse into a single model. Our empirical
results suggest that the cluster structure of the underlying problem and the
non-linearity of the expert are pivotal to the success of MoE. To further
understand this, we consider a challenging classification problem with
intrinsic cluster structures, which is hard to learn using a single expert. Yet
with the MoE layer, by choosing the experts as two-layer nonlinear
convolutional neural networks (CNNs), we show that the problem can be learned
successfully. Furthermore, our theory shows that the router can learn the
cluster-center features, which helps divide the input complex problem into
simpler linear classification sub-problems that individual experts can conquer.
To our knowledge, this is the first result towards formally understanding the
mechanism of the MoE layer for deep learning.",https://github.com/TheophileBlard/french-sentiment-analysis-with-bert,-1
"Massively Multilingual ASR on 70 Languages: Tokenization, Architecture, and Generalization Capabilities",0.190166,"End-to-end multilingual ASR has become more appealing because of several
reasons such as simplifying the training and deployment process and positive
performance transfer from high-resource to low-resource languages. However,
scaling up the number of languages, total hours, and number of unique tokens is
not a trivial task. This paper explores large-scale multilingual ASR models on
70 languages. We inspect two architectures: (1) Shared embedding and output and
(2) Multiple embedding and output model. In the shared model experiments, we
show the importance of tokenization strategy across different languages. Later,
we use our optimal tokenization strategy to train multiple embedding and output
model to further improve our result. Our multilingual ASR achieves 13.9%-15.6%
average WER relative improvement compared to monolingual models. We show that
our multilingual ASR generalizes well on an unseen dataset and domain,
achieving 9.5% and 7.5% WER on Multilingual Librispeech (MLS) with zero-shot
and finetuning, respectively.",https://github.com/facebookresearch/fairscale,-1
Human Interpretation of Saliency-based Explanation Over Text,0.796287,"While a lot of research in explainable AI focuses on producing effective
explanations, less work is devoted to the question of how people understand and
interpret the explanation. In this work, we focus on this question through a
study of saliency-based explanations over textual data. Feature-attribution
explanations of text models aim to communicate which parts of the input text
were more influential than others towards the model decision. Many current
explanation methods, such as gradient-based or Shapley value-based methods,
provide measures of importance which are well-understood mathematically. But
how does a person receiving the explanation (the explainee) comprehend it? And
does their understanding match what the explanation attempted to communicate?
We empirically investigate the effect of various factors of the input, the
feature-attribution explanation, and visualization procedure, on laypeople's
interpretation of the explanation. We query crowdworkers for their
interpretation on tasks in English and German, and fit a GAMM model to their
responses considering the factors of interest. We find that people often
mis-interpret the explanations: superficial and unrelated factors, such as word
length, influence the explainees' importance assignment despite the explanation
communicating importance directly. We then show that some of this distortion
can be attenuated: we propose a method to adjust saliencies based on model
estimates of over- and under-perception, and explore bar charts as an
alternative to heatmap saliency visualization. We find that both approaches can
attenuate the distorting effect of specific factors, leading to
better-calibrated understanding of the explanation.",None,-1
Improving Question Answering with Generation of NQ-like Questions,0.0810173,"Question Answering (QA) systems require a large amount of annotated data
which is costly and time-consuming to gather. Converting datasets of existing
QA benchmarks are challenging due to different formats and complexities. To
address these issues, we propose an algorithm to automatically generate shorter
questions resembling day-to-day human communication in the Natural Questions
(NQ) dataset from longer trivia questions in Quizbowl (QB) dataset by
leveraging conversion in style among the datasets. This provides an automated
way to generate more data for our QA systems. To ensure quality as well as
quantity of data, we detect and remove ill-formed questions using a neural
classifier. We demonstrate that in a low resource setting, using the generated
data improves the QA performance over the baseline system on both NQ and QB
data. Our algorithm improves the scalability of training data while maintaining
quality of data for QA systems.",None,-1
Generalized Differentiable RANSAC,0.871734,"We propose $\nabla$-RANSAC, a generalized differentiable RANSAC that allows
learning the entire randomized robust estimation pipeline. The proposed
approach enables the use of relaxation techniques for estimating the gradients
in the sampling distribution, which are then propagated through a
differentiable solver. The trainable quality function marginalizes over the
scores from all the models estimated within $\nabla$-RANSAC to guide the
network learning accurate and useful inlier probabilities or to train feature
detection and matching networks. Our method directly maximizes the probability
of drawing a good hypothesis, allowing us to learn better sampling
distributions. We test $\nabla$-RANSAC on various real-world scenarios on
fundamental and essential matrix estimation, and 3D point cloud registration,
outdoors and indoors, with handcrafted and learning-based features. It is
superior to the state-of-the-art in terms of accuracy while running at a
similar speed to its less accurate alternatives. The code and trained models
are available at https://github.com/weitong8591/differentiable_ransac.",https://github.com/weitong8591/differentiable_ransac,-1
ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field,0.293189,"Physical simulations produce excellent predictions of weather effects. Neural
radiance fields produce SOTA scene models. We describe a novel NeRF-editing
procedure that can fuse physical simulations with NeRF models of scenes,
producing realistic movies of physical phenomena in those scenes. Our
application -- Climate NeRF -- allows people to visualize what climate change
outcomes will do to them. ClimateNeRF allows us to render realistic weather
effects, including smog, snow, and flood. Results can be controlled with
physically meaningful variables like water level. Qualitative and quantitative
studies show that our simulated results are significantly more realistic than
those from SOTA 2D image editing and SOTA 3D NeRF stylization.",None,-1
UnCommonSense: Informative Negative Knowledge about Everyday Concepts,0.732947,"Commonsense knowledge about everyday concepts is an important asset for AI
applications, such as question answering and chatbots. Recently, we have seen
an increasing interest in the construction of structured commonsense knowledge
bases (CSKBs). An important part of human commonsense is about properties that
do not apply to concepts, yet existing CSKBs only store positive statements.
Moreover, since CSKBs operate under the open-world assumption, absent
statements are considered to have unknown truth rather than being invalid. This
paper presents the UNCOMMONSENSE framework for materializing informative
negative commonsense statements. Given a target concept, comparable concepts
are identified in the CSKB, for which a local closed-world assumption is
postulated. This way, positive statements about comparable concepts that are
absent for the target concept become seeds for negative statement candidates.
The large set of candidates is then scrutinized, pruned and ranked by
informativeness. Intrinsic and extrinsic evaluations show that our method
significantly outperforms the state-of-the-art. A large dataset of informative
negations is released as a resource for future research.",https://github.com/tsafavi/NegatER,-1
Can Requirements Engineering Support Explainable Artificial Intelligence? Towards a User-Centric Approach for Explainability Requirements,0.508468,"With the recent proliferation of artificial intelligence systems, there has
been a surge in the demand for explainability of these systems. Explanations
help to reduce system opacity, support transparency, and increase stakeholder
trust. In this position paper, we discuss synergies between requirements
engineering (RE) and Explainable AI (XAI). We highlight challenges in the field
of XAI, and propose a framework and research directions on how RE practices can
help to mitigate these challenges.",None,-1
A Study of Gender Impact in Self-supervised Models for Speech-to-Text Systems,0.187105,"Self-supervised models for speech processing emerged recently as popular
foundation blocks in speech processing pipelines. These models are pre-trained
on unlabeled audio data and then used in speech processing downstream tasks
such as automatic speech recognition (ASR) or speech translation (ST). Since
these models are now used in research and industrial systems alike, it becomes
necessary to understand the impact caused by some features such as gender
distribution within pre-training data. Using French as our investigation
language, we train and compare gender-specific wav2vec 2.0 models against
models containing different degrees of gender balance in their pre-training
data. The comparison is performed by applying these models to two
speech-to-text downstream tasks: ASR and ST. Results show the type of
downstream integration matters. We observe lower overall performance using
gender-specific pre-training before fine-tuning an end-to-end ASR system.
However, when self-supervised models are used as feature extractors, the
overall ASR and ST results follow more complex patterns in which the balanced
pre-trained model does not necessarily lead to the best results. Lastly, our
crude 'fairness' metric, the relative performance difference measured between
female and male test sets, does not display a strong variation from balanced to
gender-specific pre-trained wav2vec 2.0 models.",None,-1
Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation,0.52127,"Multi-modal Machine Translation (MMT) enables the use of visual information
to enhance the quality of translations. The visual information can serve as a
valuable piece of context information to decrease the ambiguity of input
sentences. Despite the increasing popularity of such a technique, good and
sizeable datasets are scarce, limiting the full extent of their potential.
Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It
is estimated that about 100 to 150 million people speak the language, with more
than 80 million indigenous speakers. This is more than any of the other Chadic
languages. Despite a large number of speakers, the Hausa language is considered
low-resource in natural language processing (NLP). This is due to the absence
of sufficient resources to implement most NLP tasks. While some datasets exist,
they are either scarce, machine-generated, or in the religious domain.
Therefore, there is a need to create training and evaluation data for
implementing machine learning tasks and bridging the research gap in the
language. This work presents the Hausa Visual Genome (HaVG), a dataset that
contains the description of an image or a section within the image in Hausa and
its equivalent in English. To prepare the dataset, we started by translating
the English description of the images in the Hindi Visual Genome (HVG) into
Hausa automatically. Afterward, the synthetic Hausa data was carefully
post-edited considering the respective images. The dataset comprises 32,923
images and their descriptions that are divided into training, development,
test, and challenge test set. The Hausa Visual Genome is the first dataset of
its kind and can be used for Hausa-English machine translation, multi-modal
research, and image description, among various other natural language
processing and generation tasks.",https://github.com/abumafrim/visual-genome-dataset-creation-tool,-1
Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes,0.727531,"We present a novel approach to efficiently compute tight non-convex
enclosures of the image through neural networks with ReLU, sigmoid, or
hyperbolic tangent activation functions. In particular, we abstract the
input-output relation of each neuron by a polynomial approximation, which is
evaluated in a set-based manner using polynomial zonotopes. While our approach
can also can be beneficial for open-loop neural network verification, our main
application is reachability analysis of neural network controlled systems,
where polynomial zonotopes are able to capture the non-convexity caused by the
neural network as well as the system dynamics. This results in a superior
performance compared to other methods, as we demonstrate on various benchmarks.",None,-1
ExPUNations: Augmenting Puns with Keywords and Explanations,0.283724,"The tasks of humor understanding and generation are challenging and
subjective even for humans, requiring commonsense and real-world knowledge to
master. Puns, in particular, add the challenge of fusing that knowledge with
the ability to interpret lexical-semantic ambiguity. In this paper, we present
the ExPUNations (ExPUN) dataset, in which we augment an existing dataset of
puns with detailed crowdsourced annotations of keywords denoting the most
distinctive words that make the text funny, pun explanations describing why the
text is funny, and fine-grained funniness ratings. This is the first humor
dataset with such extensive and fine-grained annotations specifically for puns.
Based on these annotations, we propose two tasks: explanation generation to aid
with pun classification and keyword-conditioned pun generation, to challenge
the current state-of-the-art natural language understanding and generation
models' ability to understand and generate humor. We showcase that the
annotated keywords we collect are helpful for generating better novel humorous
texts in human evaluation, and that our natural language explanations can be
leveraged to improve both the accuracy and robustness of humor classifiers.",https://github.com/JamesHujy/ELV/blob/main/EST_sa/train_restaurant.sh,-1
WALDO: Future Video Synthesis using Object Layer Decomposition and Parametric Flow Prediction,0.0897465,"This paper presents WALDO (WArping Layer-Decomposed Objects), a novel
approach to the prediction of future video frames from past ones. Individual
images are decomposed into multiple layers combining object masks and a small
set of control points. The layer structure is shared across all frames in each
video to build dense inter-frame connections. Complex scene motions are modeled
by combining parametric geometric transformations associated with individual
layers, and video synthesis is broken down into discovering the layers
associated with past frames, predicting the corresponding transformations for
upcoming ones and warping the associated object regions accordingly, and
filling in the remaining image parts. Extensive experiments on multiple
benchmarks including urban videos (Cityscapes and KITTI) and videos featuring
nonrigid motions (UCF-Sports and H3.6M), show that our method consistently
outperforms the state of the art by a significant margin in every case. Code,
pretrained models, and video samples synthesized by our approach can be found
in the project webpage https://16lemoing.github.io/waldo.",https://16lemoing.github.io/waldo,-1
EA$^2$E: Improving Consistency with Event Awareness for Document-Level Argument Extraction,0.771406,"Events are inter-related in documents. Motivated by the
one-sense-per-discourse theory, we hypothesize that a participant tends to play
consistent roles across multiple events in the same document. However recent
work on document-level event argument extraction models each individual event
in isolation and therefore causes inconsistency among extracted arguments
across events, which will further cause discrepancy for downstream applications
such as event knowledge base population, question answering, and hypothesis
generation. In this work, we formulate event argument consistency as the
constraints from event-event relations under the document-level setting. To
improve consistency we introduce the Event-Aware Argument Extraction (EA$^2$E)
model with augmented context for training and inference. Experiment results on
WIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA$^2$E
compared to baseline methods.",https://github.com/ZQS1943/DOCIE,-1
A study on cross-corpus speech emotion recognition and data augmentation,0.371757,"Models that can handle a wide range of speakers and acoustic conditions are
essential in speech emotion recognition (SER). Often, these models tend to show
mixed results when presented with speakers or acoustic conditions that were not
visible during training. This paper investigates the impact of cross-corpus
data complementation and data augmentation on the performance of SER models in
matched (test-set from same corpus) and mismatched (test-set from different
corpus) conditions. Investigations using six emotional speech corpora that
include single and multiple speakers as well as variations in emotion style
(acted, elicited, natural) and recording conditions are presented. Observations
show that, as expected, models trained on single corpora perform best in
matched conditions while performance decreases between 10-40% in mismatched
conditions, depending on corpus specific features. Models trained on mixed
corpora can be more stable in mismatched contexts, and the performance
reductions range from 1 to 8% when compared with single corpus models in
matched conditions. Data augmentation yields additional gains up to 4% and seem
to benefit mismatched conditions more than matched ones.",https://github.com/ludwig-ai/ludwig,-1
Fast Point Cloud Generation with Straight Flows,0.708762,"Diffusion models have emerged as a powerful tool for point cloud generation.
A key component that drives the impressive performance for generating
high-quality samples from noise is iteratively denoise for thousands of steps.
While beneficial, the complexity of learning steps has limited its applications
to many 3D real-world. To address this limitation, we propose Point Straight
Flow (PSF), a model that exhibits impressive performance using one step. Our
idea is based on the reformulation of the standard diffusion model, which
optimizes the curvy learning trajectory into a straight path. Further, we
develop a distillation strategy to shorten the straight path into one step
without a performance loss, enabling applications to 3D real-world with latency
constraints. We perform evaluations on multiple 3D tasks and find that our PSF
performs comparably to the standard diffusion model, outperforming other
efficient 3D point cloud generation methods. On real-world applications such as
point cloud completion and training-free text-guided generation in a
low-latency setup, PSF performs favorably.",None,-1
SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution,0.20924,"Dynamic convolution achieves better performance for efficient CNNs at the
cost of negligible FLOPs increase. However, the performance increase can not
match the significantly expanded number of parameters, which is the main
bottleneck in real-world applications. Contrastively, mask-based unstructured
pruning obtains a lightweight network by removing redundancy in the heavy
network. In this paper, we propose a new framework, \textbf{Sparse Dynamic
Convolution} (\textsc{SD-Conv}), to naturally integrate these two paths such
that it can inherit the advantage of dynamic mechanism and sparsity. We first
design a binary mask derived from a learnable threshold to prune static
kernels, significantly reducing the parameters and computational cost but
achieving higher performance in Imagenet-1K. We further transfer pretrained
models into a variety of downstream tasks, showing consistently better results
than baselines. We hope our SD-Conv could be an efficient alternative to
conventional dynamic convolutions.",https://github.com/weiaicunzai/pytorch-cifar100,-1
Decanus to Legatus: Synthetic training for 2D-3D human pose lifting,0.089479,"3D human pose estimation is a challenging task because of the difficulty to
acquire ground-truth data outside of controlled environments. A number of
further issues have been hindering progress in building a universal and robust
model for this task, including domain gaps between different datasets, unseen
actions between train and test datasets, various hardware settings and high
cost of annotation, etc. In this paper, we propose an algorithm to generate
infinite 3D synthetic human poses (Legatus) from a 3D pose distribution based
on 10 initial handcrafted 3D poses (Decanus) during the training of a 2D to 3D
human pose lifter neural network. Our results show that we can achieve 3D pose
estimation performance comparable to methods using real data from specialized
datasets but in a zero-shot setup, showing the generalization potential of our
framework.",None,2981
Moving Window Regression: A Novel Approach to Ordinal Regression,0.624344,"A novel ordinal regression algorithm, called moving window regression (MWR),
is proposed in this paper. First, we propose the notion of relative rank
($\rho$-rank), which is a new order representation scheme for input and
reference instances. Second, we develop global and local relative regressors
($\rho$-regressors) to predict $\rho$-ranks within entire and specific rank
ranges, respectively. Third, we refine an initial rank estimate iteratively by
selecting two reference instances to form a search window and then estimating
the $\rho$-rank within the window. Extensive experiments results show that the
proposed algorithm achieves the state-of-the-art performances on various
benchmark datasets for facial age estimation and historical color image
classification. The codes are available at https://github.com/nhshin-mcl/MWR.",None,-1
Cardinality-Regularized Hawkes-Granger Model,0.89356,"We propose a new sparse Granger-causal learning framework for temporal event
data. We focus on a specific class of point processes called the Hawkes
process. We begin by pointing out that most of the existing sparse causal
learning algorithms for the Hawkes process suffer from a singularity in maximum
likelihood estimation. As a result, their sparse solutions can appear only as
numerical artifacts. In this paper, we propose a mathematically well-defined
sparse causal learning framework based on a cardinality-regularized Hawkes
process, which remedies the pathological issues of existing approaches. We
leverage the proposed algorithm for the task of instance-wise causal event
analysis, where sparsity plays a critical role. We validate the proposed
framework with two real use-cases, one from the power grid and the other from
the cloud data center management domain.",https://github.com/iancovert/Neural-GC,-1
Multi-Level Modeling Units for End-to-End Mandarin Speech Recognition,0.0769842,"The choice of modeling units is crucial for automatic speech recognition
(ASR) tasks. In mandarin scenarios, the Chinese characters represent meaning
but are not directly related to the pronunciation. Thus only considering the
writing of Chinese characters as modeling units is insufficient to capture
speech features. In this paper, we present a novel method involves with
multi-level modeling units, which integrates multi-level information for
mandarin speech recognition. Specifically, the encoder block considers
syllables as modeling units and the decoder block deals with character-level
modeling units. To facilitate the incremental conversion from syllable features
to character features, we design an auxiliary task that applies cross-entropy
(CE) loss to intermediate decoder layers. During inference, the input feature
sequences are converted into syllable sequences by the encoder block and then
converted into Chinese characters by the decoder block. Experiments on the
widely used AISHELL-1 corpus demonstrate that our method achieves promising
results with CER of 4.1%/4.6% and 4.6%/5.2%, using the Conformer and the
Transformer backbones respectively.",https://github.com/mozillazg/python-pinyin,11
Medical Image Captioning via Generative Pretrained Transformers,0.751629,"The automatic clinical caption generation problem is referred to as proposed
model combining the analysis of frontal chest X-Ray scans with structured
patient information from the radiology records. We combine two language models,
the Show-Attend-Tell and the GPT-3, to generate comprehensive and descriptive
radiology records. The proposed combination of these models generates a textual
summary with the essential information about pathologies found, their location,
and the 2D heatmaps localizing each pathology on the original X-Ray scans. The
proposed model is tested on two medical datasets, the Open-I, MIMIC-CXR, and
the general-purpose MS-COCO. The results measured with the natural language
assessment metrics prove their efficient applicability to the chest X-Ray image
captioning.",None,-1
Transformer-based Entity Typing in Knowledge Graphs,0.741669,"We investigate the knowledge graph entity typing task which aims at inferring
plausible entity types. In this paper, we propose a novel Transformer-based
Entity Typing (TET) approach, effectively encoding the content of neighbors of
an entity. More precisely, TET is composed of three different mechanisms: a
local transformer allowing to infer missing types of an entity by independently
encoding the information provided by each of its neighbors; a global
transformer aggregating the information of all neighbors of an entity into a
single long sequence to reason about more complex entity types; and a context
transformer integrating neighbors content based on their contribution to the
type inference through information exchange between neighbor pairs.
Furthermore, TET uses information about class membership of types to
semantically strengthen the representation of an entity. Experiments on two
real-world datasets demonstrate the superior performance of TET compared to the
state-of-the-art.",https://github.com/zhiweihu1103/ET-TET,-1
Interactive Segmentation of Radiance Fields,0.837679,"Radiance Fields (RF) are popular to represent casually-captured scenes for
new view synthesis and several applications beyond it. Mixed reality on
personal spaces needs understanding and manipulating scenes represented as RFs,
with semantic segmentation of objects as an important step. Prior segmentation
efforts show promise but don't scale to complex objects with diverse
appearance. We present the ISRF method to interactively segment objects with
fine structure and appearance. Nearest neighbor feature matching using
distilled semantic features identifies high-confidence seed regions. Bilateral
search in a joint spatio-semantic space grows the region to recover accurate
segmentation. We show state-of-the-art results of segmenting objects from RFs
and compositing them to another scene, changing appearance, etc., and an
interactive segmentation tool that others can use.
  Project Page: https://rahul-goel.github.io/isrf/",https://rahul-goel.github.io/isrf/,-1
Using Ontologies for the Formalization and Recognition of Criticality for Automated Driving,0.792145,"Knowledge representation and reasoning has a long history of examining how
knowledge can be formalized, interpreted, and semantically analyzed by
machines. In the area of automated vehicles, recent advances suggest the
ability to formalize and leverage relevant knowledge as a key enabler in
handling the inherently open and complex context of the traffic world. This
paper demonstrates ontologies to be a powerful tool for a) modeling and
formalization of and b) reasoning about factors associated with criticality in
the environment of automated vehicles. For this, we leverage the well-known
6-Layer Model to create a formal representation of the environmental context.
Within this representation, an ontology models domain knowledge as logical
axioms, enabling deduction on the presence of critical factors within traffic
scenes and scenarios. For executing automated analyses, a joint description
logic and rule reasoner is used in combination with an a-priori predicate
augmentation. We elaborate on the modular approach, present a publicly
available implementation, and evaluate the method by means of a large-scale
drone data set of urban traffic scenarios.",None,-1
Leaf: Multiple-Choice Question Generation,0.256806,"Testing with quiz questions has proven to be an effective way to assess and
improve the educational process. However, manually creating quizzes is tedious
and time-consuming. To address this challenge, we present Leaf, a system for
generating multiple-choice questions from factual text. In addition to being
very well suited for the classroom, Leaf could also be used in an industrial
setting, e.g., to facilitate onboarding and knowledge sharing, or as a
component of chatbots, question answering systems, or Massive Open Online
Courses (MOOCs). The code and the demo are available on
https://github.com/KristiyanVachev/Leaf-Question-Generation.",https://github.com/KristiyanVachev/Leaf-Question-Generation,22966
Entailment Semantics Can Be Extracted from an Ideal Language Model,0.401227,"Language models are often trained on text alone, without additional
grounding. There is debate as to how much of natural language semantics can be
inferred from such a procedure. We prove that entailment judgments between
sentences can be extracted from an ideal language model that has perfectly
learned its target distribution, assuming the training sentences are generated
by Gricean agents, i.e., agents who follow fundamental principles of
communication from the linguistic theory of pragmatics. We also show entailment
judgments can be decoded from the predictions of a language model trained on
such Gricean data. Our results reveal a pathway for understanding the semantic
information encoded in unlabeled linguistic data and a potential framework for
extracting semantics from language models.",https://github.com/viking-sudo-rm/,-1
HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator,0.819026,"Video prediction is an important yet challenging problem; burdened with the
tasks of generating future frames and learning environment dynamics. Recently,
autoregressive latent video models have proved to be a powerful video
prediction tool, by separating the video prediction into two sub-problems:
pre-training an image generator model, followed by learning an autoregressive
prediction model in the latent space of the image generator. However,
successfully generating high-fidelity and high-resolution videos has yet to be
seen. In this work, we investigate how to train an autoregressive latent video
prediction model capable of predicting high-fidelity future frames with minimal
modification to existing models, and produce high-resolution (256x256) videos.
Specifically, we scale up prior models by employing a high-fidelity image
generator (VQ-GAN) with a causal transformer model, and introduce additional
techniques of top-k sampling and data augmentation to further improve video
prediction quality. Despite the simplicity, the proposed method achieves
competitive performance to state-of-the-art approaches on standard video
prediction benchmarks with fewer parameters, and enables high-resolution video
prediction on complex and large-scale datasets. Videos are available at
https://sites.google.com/view/harp-videos/home.",None,-1
Nested Named Entity Recognition as Latent Lexicalized Constituency Parsing,0.977522,"Nested named entity recognition (NER) has been receiving increasing
attention. Recently, (Fu et al, 2021) adapt a span-based constituency parser to
tackle nested NER. They treat nested entities as partially-observed
constituency trees and propose the masked inside algorithm for partial
marginalization. However, their method cannot leverage entity heads, which have
been shown useful in entity mention detection and entity typing. In this work,
we resort to more expressive structures, lexicalized constituency trees in
which constituents are annotated by headwords, to model nested entities. We
leverage the Eisner-Satta algorithm to perform partial marginalization and
inference efficiently. In addition, we propose to use (1) a two-stage strategy
(2) a head regularization loss and (3) a head-aware labeling loss in order to
enhance the performance. We make a thorough ablation study to investigate the
functionality of each component. Experimentally, our method achieves the
state-of-the-art performance on ACE2004, ACE2005 and NNE, and competitive
performance on GENIA, and meanwhile has a fast inference speed.",https://github.com/LouChao98/nner_as_parsing,-1
Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning,0.990294,"Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks for both high-resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is \textit{language
adaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual
texts of a language using the pre-training objective. However, adapting to a
target language individually takes a large disk space and limits the
cross-lingual transfer abilities of the resulting models because they have been
specialized for a single language. In this paper, we perform
\textit{multilingual adaptive fine-tuning} on 17 most-resourced African
languages and three other high-resource languages widely spoken on the African
continent to encourage cross-lingual transfer learning. To further specialize
the multilingual PLM, we removed vocabulary tokens from the embedding layer
that corresponds to non-African writing scripts before MAFT, thus reducing the
model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa
and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space.
Additionally, we show that our adapted PLM also improves the zero-shot
cross-lingual transfer abilities of parameter efficient fine-tuning methods.",None,-1
Linear programming word problems formulation using EnsembleCRF NER labeler and T5 text generator with data augmentations,0.306079,"We propose an ensemble approach to predict the labels in linear programming
word problems. The entity identification and the meaning representation are two
types of tasks to be solved in the NL4Opt competition. We propose the
ensembleCRF method to identify the named entities for the first task. We found
that single models didn't improve for the given task in our analysis. A set of
prediction models predict the entities. The generated results are combined to
form a consensus result in the ensembleCRF method. We present an ensemble text
generator to produce the representation sentences for the second task. We
thought of dividing the problem into multiple small tasks due to the overflow
in the output. A single model generates different representations based on the
prompt. All the generated text is combined to form an ensemble and produce a
mathematical meaning of a linear programming problem.",None,-1
MetaEMS: A Meta Reinforcement Learning-based Control Framework for Building Energy Management System,0.390883,"The building sector has been recognized as one of the primary sectors for
worldwide energy consumption. Improving the energy efficiency of the building
sector can help reduce the operation cost and reduce the greenhouse gas
emission. The energy management system (EMS) can monitor and control the
operations of built-in appliances in buildings, so an efficient EMS is of
crucial importance to improve the building operation efficiency and maintain
safe operations. With the growing penetration of renewable energy and
electrical appliances, increasing attention has been paid to the development of
intelligent building EMS. Recently, reinforcement learning (RL) has been
applied for building EMS and has shown promising potential. However, most of
the current RL-based EMS solutions would need a large amount of data to learn a
reliable control policy, which limits the applicability of these solutions in
the real world. In this work, we propose MetaEMS, which can help achieve better
energy management performance with the benefits of RL and meta-learning.
Experiment results showcase that our proposed MetaEMS can adapt faster to
environment changes and perform better in most situations compared with other
baselines.",None,-1
Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition,0.634918,"Due to the absence of explicit connectives, implicit discourse relation
recognition (IDRR) remains a challenging task in discourse analysis. The
critical step for IDRR is to learn high-quality discourse relation
representations between two arguments. Recent methods tend to integrate the
whole hierarchical information of senses into discourse relation
representations for multi-level sense recognition. Nevertheless, they
insufficiently incorporate the static hierarchical structure containing all
senses (defined as global hierarchy), and ignore the hierarchical sense label
sequence corresponding to each instance (defined as local hierarchy). For the
purpose of sufficiently exploiting global and local hierarchies of senses to
learn better discourse relation representations, we propose a novel GlObal and
Local Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of
hierarchies with the aid of multi-task learning and contrastive learning.
Experimental results on PDTB 2.0 and PDTB 3.0 datasets demonstrate that our
method remarkably outperforms current state-of-the-art models at all
hierarchical levels. Our code is publicly available at
https://github.com/YJiangcm/GOLF_for_IDRR",https://github.com/YJiangcm/GOLF_for_IDRR,-1
Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words,0.902884,"Cosine similarity of contextual embeddings is used in many NLP tasks (e.g.,
QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in
which word similarities estimated by cosine over BERT embeddings are
understated and trace this effect to training data frequency. We find that
relative to human judgements, cosine similarity underestimates the similarity
of frequent words with other instances of the same word or other words across
contexts, even after controlling for polysemy and other factors. We conjecture
that this underestimation of similarity for high frequency words is due to
differences in the representational geometry of high and low frequency words
and provide a formal argument for the two-dimensional case.",https://github.com/katezhou/cosine_and_frequency,-1
Ranking-Enhanced Unsupervised Sentence Representation Learning,0.708646,"Unsupervised sentence representation learning has progressed through
contrastive learning and data augmentation methods such as dropout masking.
Despite this progress, sentence encoders are still limited to using only an
input sentence when predicting its semantic vector. In this work, we show that
the semantic meaning of a sentence is also determined by nearest-neighbor
sentences that are similar to the input sentence. Based on this finding, we
propose a novel unsupervised sentence encoder, RankEncoder. RankEncoder
predicts the semantic vector of an input sentence by leveraging its
relationship with other sentences in an external corpus, as well as the input
sentence itself. We evaluate RankEncoder on semantic textual benchmark
datasets. From the experimental results, we verify that 1) RankEncoder achieves
80.07% Spearman's correlation, a 1.1% absolute improvement compared to the
previous state-of-the-art performance, 2) RankEncoder is universally applicable
to existing unsupervised sentence embedding methods, and 3) RankEncoder is
specifically effective for predicting the similarity scores of similar sentence
pairs.",https://github.com/yeonsw/RankEncoder.git,-1
Discrete Factorial Representations as an Abstraction for Goal Conditioned Reinforcement Learning,0.372448,"Goal-conditioned reinforcement learning (RL) is a promising direction for
training agents that are capable of solving multiple tasks and reach a diverse
set of objectives. How to \textit{specify} and \textit{ground} these goals in
such a way that we can both reliably reach goals during training as well as
generalize to new goals during evaluation remains an open area of research.
Defining goals in the space of noisy and high-dimensional sensory inputs poses
a challenge for training goal-conditioned agents, or even for generalization to
novel goals. We propose to address this by learning factorial representations
of goals and processing the resulting representation via a discretization
bottleneck, for coarser goal specification, through an approach we call DGRL.
We show that applying a discretizing bottleneck can improve performance in
goal-conditioned RL setups, by experimentally evaluating this method on tasks
ranging from maze environments to complex robotic navigation and manipulation.
Additionally, we prove a theorem lower-bounding the expected return on
out-of-distribution goals, while still allowing for specifying goals with
expressive combinatorial structure.",None,794065
Towards Climate Awareness in NLP Research,0.669303,"The climate impact of AI, and NLP research in particular, has become a
serious issue given the enormous amount of energy that is increasingly being
used for training and running computational models. Consequently, increasing
focus is placed on efficient NLP. However, this important initiative lacks
simple guidelines that would allow for systematic climate reporting of NLP
research. We argue that this deficiency is one of the reasons why very few
publications in NLP report key figures that would allow a more thorough
examination of environmental impact. As a remedy, we propose a climate
performance model card with the primary purpose of being practically usable
with only limited information about experiments and the underlying computer
hardware. We describe why this step is essential to increase awareness about
the environmental impact of NLP research and, thereby, paving the way for more
thorough discussions.",https://github.com/danielhers/climate-awareness-nlp,-1
Deep Convolutional Learning-Aided Detector for Generalized Frequency Division Multiplexing with Index Modulation,0.485178,"In this paper, a deep convolutional neural network-based symbol detection and
demodulation is proposed for generalized frequency division multiplexing with
index modulation (GFDM-IM) scheme in order to improve the error performance of
the system. The proposed method first pre-processes the received signal by
using a zero-forcing (ZF) detector and then uses a neural network consisting of
a convolutional neural network (CNN) followed by a fully-connected neural
network (FCNN). The FCNN part uses only two fully-connected layers, which can
be adapted to yield a trade-off between complexity and bit error rate (BER)
performance. This two-stage approach prevents the getting stuck of neural
network in a saddle point and enables IM blocks processing independently. It
has been demonstrated that the proposed deep convolutional neural network-based
detection and demodulation scheme provides better BER performance compared to
ZF detector with a reasonable complexity increase. We conclude that
non-orthogonal waveforms combined with IM schemes with the help of deep
learning is a promising physical layer (PHY) scheme for future wireless
networks",None,-1
Specializing Multi-domain NMT via Penalizing Low Mutual Information,0.113282,"Multi-domain Neural Machine Translation (NMT) trains a single model with
multiple domains. It is appealing because of its efficacy in handling multiple
domains within one model. An ideal multi-domain NMT should learn distinctive
domain characteristics simultaneously, however, grasping the domain peculiarity
is a non-trivial task. In this paper, we investigate domain-specific
information through the lens of mutual information (MI) and propose a new
objective that penalizes low MI to become higher. Our method achieved the
state-of-the-art performance among the current competitive multi-domain NMT
models. Also, we empirically show our objective promotes low MI to be higher
resulting in domain-specialized multi-domain NMT.",https://github.com/facebookresearch/fairseq,-1
Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification,0.547919,"Transformer has been widely used in histopathology whole slide image (WSI)
classification for the purpose of tumor grading, prognosis analysis, etc.
However, the design of token-wise self-attention and positional embedding
strategy in the common Transformer limits the effectiveness and efficiency in
the application to gigapixel histopathology images. In this paper, we propose a
kernel attention Transformer (KAT) for histopathology WSI classification. The
information transmission of the tokens is achieved by cross-attention between
the tokens and a set of kernels related to a set of positional anchors on the
WSI. Compared to the common Transformer structure, the proposed KAT can better
describe the hierarchical context information of the local regions of the WSI
and meanwhile maintains a lower computational complexity. The proposed method
was evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset
with 2560 WSIs, and was compared with 6 state-of-the-art methods. The
experimental results have demonstrated the proposed KAT is effective and
efficient in the task of histopathology WSI classification and is superior to
the state-of-the-art methods. The code is available at
https://github.com/zhengyushan/kat.",https://github.com/zhengyushan/kat,-1
"Reflectance-Guided, Contrast-Accumulated Histogram Equalization",0.0717411,"Existing image enhancement methods fall short of expectations because with
them it is difficult to improve global and local image contrast simultaneously.
To address this problem, we propose a histogram equalization-based method that
adapts to the data-dependent requirements of brightness enhancement and
improves the visibility of details without losing the global contrast. This
method incorporates the spatial information provided by image context in
density estimation for discriminative histogram equalization. To minimize the
adverse effect of non-uniform illumination, we propose defining spatial
information on the basis of image reflectance estimated with edge preserving
smoothing. Our method works particularly well for determining how the
background brightness should be adaptively adjusted and for revealing useful
image details hidden in the dark.",None,-1
Reinforced Question Rewriting for Conversational Question Answering,0.312827,"Conversational Question Answering (CQA) aims to answer questions contained
within dialogues, which are not easily interpretable without context.
Developing a model to rewrite conversational questions into self-contained ones
is an emerging solution in industry settings as it allows using existing
single-turn QA systems to avoid training a CQA model from scratch. Previous
work trains rewriting models using human rewrites as supervision. However, such
objectives are disconnected with QA models and therefore more human-like
rewrites do not guarantee better QA performance. In this paper we propose using
QA feedback to supervise the rewriting model with reinforcement learning.
Experiments show that our approach can effectively improve QA performance over
baselines for both extractive and retrieval QA. Furthermore, human evaluation
shows that our method can generate more accurate and detailed rewrites when
compared to human annotations.",None,-1
Designing Network Design Strategies Through Gradient Path Analysis,0.997823,"Designing a high-efficiency and high-quality expressive network architecture
has always been the most important research topic in the field of deep
learning. Most of today's network design strategies focus on how to integrate
features extracted from different layers, and how to design computing units to
effectively extract these features, thereby enhancing the expressiveness of the
network. This paper proposes a new network design strategy, i.e., to design the
network architecture based on gradient path analysis. On the whole, most of
today's mainstream network design strategies are based on feed forward path,
that is, the network architecture is designed based on the data path. In this
paper, we hope to enhance the expressive ability of the trained model by
improving the network learning ability. Due to the mechanism driving the
network parameter learning is the backward propagation algorithm, we design
network design strategies based on back propagation path. We propose the
gradient path design strategies for the layer-level, the stage-level, and the
network-level, and the design strategies are proved to be superior and feasible
from theoretical analysis and experiments.",https://github.com/ultralytics/yolov5/,-1
MagicPony: Learning Articulated 3D Animals in the Wild,0.975782,"We consider the problem of predicting the 3D shape, articulation, viewpoint,
texture, and lighting of an articulated animal like a horse given a single test
image as input. We present a new method, dubbed MagicPony, that learns this
predictor purely from in-the-wild single-view images of the object category,
with minimal assumptions about the topology of deformation. At its core is an
implicit-explicit representation of articulated shape and appearance, combining
the strengths of neural fields and meshes. In order to help the model
understand an object's shape and pose, we distil the knowledge captured by an
off-the-shelf self-supervised vision transformer and fuse it into the 3D model.
To overcome local optima in viewpoint estimation, we further introduce a new
viewpoint sampling scheme that comes at no additional training cost. MagicPony
outperforms prior work on this challenging task and demonstrates excellent
generalisation in reconstructing art, despite the fact that it is only trained
on real images.",https://3dmagicpony.github.io/,96096
Higher-order Clustering and Pooling for Graph Neural Networks,0.560582,"Graph Neural Networks achieve state-of-the-art performance on a plethora of
graph classification tasks, especially due to pooling operators, which
aggregate learned node embeddings hierarchically into a final graph
representation. However, they are not only questioned by recent work showing on
par performance with random pooling, but also ignore completely higher-order
connectivity patterns. To tackle this issue, we propose HoscPool, a
clustering-based graph pooling operator that captures higher-order information
hierarchically, leading to richer graph representations. In fact, we learn a
probabilistic cluster assignment matrix end-to-end by minimising relaxed
formulations of motif spectral clustering in our objective function, and we
then extend it to a pooling operator. We evaluate HoscPool on graph
classification tasks and its clustering component on graphs with ground-truth
community structure, achieving best performance. Lastly, we provide a deep
empirical analysis of pooling operators' inner functioning.",None,-1
RAILD: Towards Leveraging Relation Features for Inductive Link Prediction In Knowledge Graphs,0.689223,"Due to the open world assumption, Knowledge Graphs (KGs) are never complete.
In order to address this issue, various Link Prediction (LP) methods are
proposed so far. Some of these methods are inductive LP models which are
capable of learning representations for entities not seen during training.
However, to the best of our knowledge, none of the existing inductive LP models
focus on learning representations for unseen relations. In this work, a novel
Relation Aware Inductive Link preDiction (RAILD) is proposed for KG completion
which learns representations for both unseen entities and unseen relations. In
addition to leveraging textual literals associated with both entities and
relations by employing language models, RAILD also introduces a novel
graph-based approach to generate features for relations. Experiments are
conducted with different existing and newly created challenging benchmark
datasets and the results indicate that RAILD leads to performance improvement
over the state-of-the-art models. Moreover, since there are no existing
inductive LP models which learn representations for unseen relations, we have
created our own baselines and the results obtained with RAILD also outperform
these baselines.",https://github.com/GenetAsefa/RAILD,-1
"Heroes, Villains, and Victims, and GPT-3: Automated Extraction of Character Roles Without Training Data",0.94628,"This paper shows how to use large-scale pre-trained language models to
extract character roles from narrative texts without training data. Queried
with a zero-shot question-answering prompt, GPT-3 can identify the hero,
villain, and victim in diverse domains: newspaper articles, movie plot
summaries, and political speeches.",None,-1
Adaptive Model Predictive Control by Learning Classifiers,0.0541781,"Stochastic model predictive control has been a successful and robust control
framework for many robotics tasks where the system dynamics model is slightly
inaccurate or in the presence of environment disturbances. Despite the
successes, it is still unclear how to best adjust control parameters to the
current task in the presence of model parameter uncertainty and heteroscedastic
noise. In this paper, we propose an adaptive MPC variant that automatically
estimates control and model parameters by leveraging ideas from Bayesian
optimisation (BO) and the classical expected improvement acquisition function.
We leverage recent results showing that BO can be reformulated via density
ratio estimation, which can be efficiently approximated by simply learning a
classifier. This is then integrated into a model predictive path integral
control framework yielding robust controllers for a variety of challenging
robotics tasks. We demonstrate the approach on classical control problems under
model uncertainty and robotics manipulation tasks.",None,-1
Implicit Two-Tower Policies,0.307883,"We present a new class of structured reinforcement learning
policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are
chosen based on the attention scores of their learnable latent representations
with those of the input states. By explicitly disentangling action from state
processing in the policy stack, we achieve two main goals: substantial
computational gains and better performance. Our architectures are compatible
with both: discrete and continuous action spaces. By conducting tests on 15
environments from OpenAI Gym and DeepMind Control Suite, we show that
ITT-architectures are particularly suited for blackbox/evolutionary
optimization and the corresponding policy training algorithms outperform their
vanilla unstructured implicit counterparts as well as commonly used explicit
policies. We complement our analysis by showing how techniques such as hashing
and lazy tower updates, critically relying on the two-tower structure of ITTs,
can be applied to obtain additional computational improvements.",https://anonymous.4open.science/r/itt-9881/README.md,17269
Boosting 3D Object Detection by Simulating Multimodality on Point Clouds,0.673167,"This paper presents a new approach to boost a single-modality (LiDAR) 3D
object detector by teaching it to simulate features and responses that follow a
multi-modality (LiDAR-image) detector. The approach needs LiDAR-image data only
when training the single-modality detector, and once well-trained, it only
needs LiDAR data at inference. We design a novel framework to realize the
approach: response distillation to focus on the crucial response samples and
avoid the background samples; sparse-voxel distillation to learn voxel
semantics and relations from the estimated crucial voxels; a fine-grained
voxel-to-point distillation to better attend to features of small and distant
objects; and instance distillation to further enhance the deep-feature
consistency. Experimental results on the nuScenes dataset show that our
approach outperforms all SOTA LiDAR-only 3D detectors and even surpasses the
baseline LiDAR-image detector on the key NDS metric, filling 72% mAP gap
between the single- and multi-modality detectors.",None,-1
RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL,0.998734,"Relational structures such as schema linking and schema encoding have been
validated as a key component to qualitatively translating natural language into
SQL queries. However, introducing these structural relations comes with prices:
they often result in a specialized model structure, which largely prohibits
using large pretrained models in text-to-SQL. To address this problem, we
propose RASAT: a Transformer seq2seq architecture augmented with relation-aware
self-attention that could leverage a variety of relational structures while
inheriting the pretrained parameters from the T5 model effectively. Our model
can incorporate almost all types of existing relations in the literature, and
in addition, we propose introducing co-reference relations for the multi-turn
scenario. Experimental results on three widely used text-to-SQL datasets,
covering both single-turn and multi-turn scenarios, have shown that RASAT could
achieve state-of-the-art results across all three benchmarks (75.5% EX on
Spider, 52.6% IEX on SParC, and 37.4% IEX on CoSQL).",https://github.com/LUMIA-group/rasat,-1
Diffusion models for missing value imputation in tabular data,0.991869,"Missing value imputation in machine learning is the task of estimating the
missing values in the dataset accurately using available information. In this
task, several deep generative modeling methods have been proposed and
demonstrated their usefulness, e.g., generative adversarial imputation
networks. Recently, diffusion models have gained popularity because of their
effectiveness in the generative modeling task in images, texts, audio, etc. To
our knowledge, less attention has been paid to the investigation of the
effectiveness of diffusion models for missing value imputation in tabular data.
Based on recent development of diffusion models for time-series data
imputation, we propose a diffusion model approach called ""Conditional
Score-based Diffusion Models for Tabular data"" (TabCSDI). To effectively handle
categorical variables and numerical variables simultaneously, we investigate
three techniques: one-hot encoding, analog bits encoding, and feature
tokenization. Experimental results on benchmark datasets demonstrated the
effectiveness of TabCSDI compared with well-known existing methods, and also
emphasized the importance of the categorical embedding techniques.",https://github.com/vanderschaarlab/hyperimpute,-1
Summarizing Patients Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models,0.692584,"Automatically summarizing patients' main problems from daily progress notes
using natural language processing methods helps to battle against information
and cognitive overload in hospital settings and potentially assists providers
with computerized diagnostic decision support. Problem list summarization
requires a model to understand, abstract, and generate clinical documentation.
In this work, we propose a new NLP task that aims to generate a list of
problems in a patient's daily care plan using input from the provider's
progress notes during hospitalization. We investigate the performance of T5 and
BART, two state-of-the-art seq2seq transformer architectures, in solving this
problem. We provide a corpus built on top of progress notes from publicly
available electronic health record progress notes in the Medical Information
Mart for Intensive Care (MIMIC)-III. T5 and BART are trained on general domain
text, and we experiment with a data augmentation method and a domain adaptation
pre-training method to increase exposure to medical vocabulary and knowledge.
Evaluation methods include ROUGE, BERTScore, cosine similarity on sentence
embedding, and F-score on medical concepts. Results show that T5 with domain
adaptive pre-training achieves significant performance gains compared to a
rule-based system and general domain pre-trained language models, indicating a
promising direction for tackling the problem summarization task.",//git.doit.wisc.edu/smph/dom/UW-ICU-Data-Science-Lab/drbench,9607
Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control,0.385174,"We present Free-HeadGAN, a person-generic neural talking head synthesis
system. We show that modeling faces with sparse 3D facial landmarks are
sufficient for achieving state-of-the-art generative performance, without
relying on strong statistical priors of the face, such as 3D Morphable Models.
Apart from 3D pose and facial expressions, our method is capable of fully
transferring the eye gaze, from a driving actor to a source identity. Our
complete pipeline consists of three components: a canonical 3D key-point
estimator that regresses 3D pose and expression-related deformations, a gaze
estimation network and a generator that is built upon the architecture of
HeadGAN. We further experiment with an extension of our generator to
accommodate few-shot learning using an attention mechanism, in case more than
one source images are available. Compared to the latest models for reenactment
and motion transfer, our system achieves higher photo-realism combined with
superior identity preservation, while offering explicit gaze control.",None,34367
Dialogue Meaning Representation for Task-Oriented Dialogue Systems,0.404541,"Dialogue meaning representation formulates natural language utterance
semantics in their conversational context in an explicit and machine-readable
form. Previous work typically follows the intent-slot framework, which is easy
for annotation yet limited in scalability for complex linguistic expressions. A
line of works alleviates the representation issue by introducing hierarchical
structures but challenging to express complex compositional semantics, such as
negation and coreference. We propose Dialogue Meaning Representation (DMR), a
pliable and easily extendable representation for task-oriented dialogue. Our
representation contains a set of nodes and edges to represent rich
compositional semantics. Moreover, we propose an inheritance hierarchy
mechanism focusing on domain extensibility. Additionally, we annotated
DMR-FastFood, a multi-turn dialogue dataset with more than 70k utterances, with
DMR. We propose two evaluation tasks to evaluate different dialogue models and
a novel coreference resolution model GNNCoref for the graph-based coreference
resolution task. Experiments show that DMR can be parsed well with pre-trained
Seq2Seq models, and GNNCoref outperforms the baseline models by a large margin.",https://github.com/amazon-research/dialogue-meaning-representation,20692
Challenges and Opportunities of Edge AI for Next-Generation Implantable BMIs,0.570493,"Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.",None,-1
Evaluating Human-Language Model Interaction,0.951541,"Many real-world applications of language models (LMs), such as writing
assistance and code autocomplete, involve human-LM interaction. However, most
benchmarks are non-interactive in that a model produces output without human
involvement. To evaluate human-LM interaction, we develop a new framework,
Human-AI Language-based Interaction Evaluation (HALIE), that defines the
components of interactive systems and dimensions to consider when designing
evaluation metrics. Compared to standard, non-interactive evaluation, HALIE
captures (i) the interactive process, not only the final output; (ii) the
first-person subjective experience, not just a third-party assessment; and
(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We
then design five tasks to cover different forms of interaction: social
dialogue, question answering, crossword puzzles, summarization, and metaphor
generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3
and AI21 Labs' Jurassic-1), we find that better non-interactive performance
does not always translate to better human-LM interaction. In particular, we
highlight three cases where the results from non-interactive and interactive
metrics diverge and underscore the importance of human-LM interaction for LM
evaluation.",https://github.com/stanford-crfm/halie,-1
Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation,0.561553,"In reinforcement learning for visual navigation, it is common to develop a
model for each new task, and train that model from scratch with task-specific
interactions in 3D environments. However, this process is expensive; massive
amounts of interactions are needed for the model to generalize well. Moreover,
this process is repeated whenever there is a change in the task type or the
goal modality. We present a unified approach to visual navigation using a novel
modular transfer learning model. Our model can effectively leverage its
experience from one source task and apply it to multiple target tasks (e.g.,
ObjectNav, RoomNav, ViewNav) with various goal modalities (e.g., image, sketch,
audio, label). Furthermore, our model enables zero-shot experience learning,
whereby it can solve the target tasks without receiving any task-specific
interactive training. Our experiments on multiple photorealistic datasets and
challenging tasks show that our approach learns faster, generalizes better, and
outperforms SoTA models by a significant margin.",https://vision.cs.utexas.edu/projects/zsel/,-1
"Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings",0.470246,"In order to equip NLP systems with selective prediction capability, several
task-specific approaches have been proposed. However, which approaches work
best across tasks or even if they consistently outperform the simplest baseline
'MaxProb' remains to be explored. To this end, we systematically study
'selective prediction' in a large-scale setup of 17 datasets across several NLP
tasks. Through comprehensive experiments under in-domain (IID), out-of-domain
(OOD), and adversarial (ADV) settings, we show that despite leveraging
additional resources (held-out data/computation), none of the existing
approaches consistently and considerably outperforms MaxProb in all three
settings. Furthermore, their performance does not translate well across tasks.
For instance, Monte-Carlo Dropout outperforms all other approaches on Duplicate
Detection datasets but does not fare well on NLI datasets, especially in the
OOD setting. Thus, we recommend that future selective prediction approaches
should be evaluated across tasks and settings for reliable estimation of their
capabilities.",None,-1
CD-FSOD: A Benchmark for Cross-domain Few-shot Object Detection,0.349461,"In this paper, we propose a study of the cross-domain few-shot object
detection (CD-FSOD) benchmark, consisting of image data from a diverse data
domain. On the proposed benchmark, we evaluate state-of-art FSOD approaches,
including meta-learning FSOD approaches and fine-tuning FSOD approaches. The
results show that these methods tend to fall, and even underperform the naive
fine-tuning model. We analyze the reasons for their failure and introduce a
strong baseline that uses a mutually-beneficial manner to alleviate the
overfitting problem. Our approach is remarkably superior to existing approaches
by significant margins (2.0\% on average) on the proposed benchmark. Our code
is available at \url{https://github.com/FSOD/CD-FSOD}.",https://github.com/FSOD/CD-FSOD,30
DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation,0.10062,"Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies
on a pre-trained general NMT model which is adapted to the new domain on a
sample of in-domain parallel data. Without parallel data, there is no way to
estimate the potential benefit of DA, nor the amount of parallel samples it
would require. It is however a desirable functionality that could help MT
practitioners to make an informed decision before investing resources in
dataset creation. We propose a Domain adaptation Learning Curve prediction
(DaLC) model that predicts prospective DA performance based on in-domain
monolingual samples in the source language. Our model relies on the NMT encoder
representations combined with various instance and corpus-level features. We
demonstrate that instance-level is better able to distinguish between different
domains compared to corpus-level frameworks proposed in previous studies.
Finally, we perform in-depth analyses of the results highlighting the
limitations of our approach, and provide directions for future research.",https://github.com/dmlc/xgboost,-1
QC-StyleGAN -- Quality Controllable Image Generation and Manipulation,0.0334092,"The introduction of high-quality image generation models, particularly the
StyleGAN family, provides a powerful tool to synthesize and manipulate images.
However, existing models are built upon high-quality (HQ) data as desired
outputs, making them unfit for in-the-wild low-quality (LQ) images, which are
common inputs for manipulation. In this work, we bridge this gap by proposing a
novel GAN structure that allows for generating images with controllable
quality. The network can synthesize various image degradation and restore the
sharp image via a quality control code. Our proposed QC-StyleGAN can directly
edit LQ images without altering their quality by applying GAN inversion and
manipulation techniques. It also provides for free an image restoration
solution that can handle various degradations, including noise, blur,
compression artifacts, and their mixtures. Finally, we demonstrate numerous
other applications such as image degradation synthesis, transfer, and
interpolation. The code is available at
https://github.com/VinAIResearch/QC-StyleGAN.",https://github.com/VinAIResearch/QC-StyleGAN,-1
CellTranspose: Few-shot Domain Adaptation for Cellular Instance Segmentation,0.484522,"Automated cellular instance segmentation is a process utilized for
accelerating biological research for the past two decades, and recent
advancements have produced higher quality results with less effort from the
biologist. Most current endeavors focus on completely cutting the researcher
out of the picture by generating highly generalized models. However, these
models invariably fail when faced with novel data, distributed differently than
the ones used for training. Rather than approaching the problem with methods
that presume the availability of large amounts of target data and computing
power for retraining, in this work we address the even greater challenge of
designing an approach that requires minimal amounts of new annotated data as
well as training time. We do so by designing specialized contrastive losses
that leverage the few annotated samples very efficiently. A large set of
results show that 3 to 5 annotations lead to models with accuracy that: 1)
significantly mitigate the covariate shift effects; 2) matches or surpasses
other adaptation methods; 3) even approaches methods that have been fully
retrained on the target distribution. The adaptation training is only a few
minutes, paving a path towards a balance between model performance, computing
requirements and expert-level annotation needs.",None,-1
Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion,0.877174,"Digital art synthesis is receiving increasing attention in the multimedia
community because of engaging the public with art effectively. Current digital
art synthesis methods usually use single-modality inputs as guidance, thereby
limiting the expressiveness of the model and the diversity of generated
results. To solve this problem, we propose the multimodal guided artwork
diffusion (MGAD) model, which is a diffusion-based digital artwork generation
approach that utilizes multimodal prompts as guidance to control the
classifier-free diffusion model. Additionally, the contrastive language-image
pretraining (CLIP) model is used to unify text and image modalities. Extensive
experimental results on the quality and quantity of the generated digital art
paintings confirm the effectiveness of the combination of the diffusion model
and multimodal guidance. Code is available at
https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion.",https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion,-1
Preserving Fine-Grain Feature Information in Classification via Entropic Regularization,0.151416,"Labeling a classification dataset implies to define classes and associated
coarse labels, that may approximate a smoother and more complicated ground
truth. For example, natural images may contain multiple objects, only one of
which is labeled in many vision datasets, or classes may result from the
discretization of a regression problem. Using cross-entropy to train
classification models on such coarse labels is likely to roughly cut through
the feature space, potentially disregarding the most meaningful such features,
in particular losing information on the underlying fine-grain task. In this
paper we are interested in the problem of solving fine-grain classification or
regression, using a model trained on coarse-grain labels only. We show that
standard cross-entropy can lead to overfitting to coarse-related features. We
introduce an entropy-based regularization to promote more diversity in the
feature space of trained models, and empirically demonstrate the efficacy of
this methodology to reach better performance on the fine-grain problems. Our
results are supported through theoretical developments and empirical
validation.",https://anonymous.4open.science/r/FIERCE-repo-CFE2/README.md,-1
"Global Counterfactual Explanations: Investigations, Implementations and Improvements",0.568708,"Counterfactual explanations have been widely studied in explainability, with
a range of application dependent methods emerging in fairness, recourse and
model understanding. However, the major shortcoming associated with these
methods is their inability to provide explanations beyond the local or
instance-level. While some works touch upon the notion of a global explanation,
typically suggesting to aggregate masses of local explanations in the hope of
ascertaining global properties, few provide frameworks that are either reliable
or computationally tractable. Meanwhile, practitioners are requesting more
efficient and interactive explainability tools. We take this opportunity to
investigate existing global methods, with a focus on implementing and improving
Actionable Recourse Summaries (AReS), the only known global counterfactual
explanation framework for recourse.",None,-1
Scaling-up Generalized Planning as Heuristic Search with Landmarks,0.288773,"Landmarks are one of the most effective search heuristics for classical
planning, but largely ignored in generalized planning. Generalized planning
(GP) is usually addressed as a combinatorial search in a given space of
algorithmic solutions, where candidate solutions are evaluated w.r.t.~the
instances they solve. This type of solution evaluation ignores any sub-goal
information that is not explicit in the representation of the planning
instances, causing plateaus in the space of candidate generalized plans.
Furthermore, node expansion in GP is a run-time bottleneck since it requires
evaluating every child node over the entire batch of classical planning
instances in a GP problem. In this paper we define a landmark counting
heuristic for GP (that considers sub-goal information that is not explicitly
represented in the planning instances), and a novel heuristic search algorithm
for GP (that we call PGP) and that progressively processes subsets of the
planning instances of a GP problem. Our two orthogonal contributions are
analyzed in an ablation study, showing that both improve the state-of-the-art
in GP as heuristic search, and that both benefit from each other when used in
combination.",https://github.com/aig-upf/pgp-landmarks,-1
Shift-tolerant Perceptual Similarity Metric,0.96691,"Existing perceptual similarity metrics assume an image and its reference are
well aligned. As a result, these metrics are often sensitive to a small
alignment error that is imperceptible to the human eyes. This paper studies the
effect of small misalignment, specifically a small shift between the input and
reference image, on existing metrics, and accordingly develops a shift-tolerant
similarity metric. This paper builds upon LPIPS, a widely used learned
perceptual similarity metric, and explores architectural design considerations
to make it robust against imperceptible misalignment. Specifically, we study a
wide spectrum of neural network elements, such as anti-aliasing filtering,
pooling, striding, padding, and skip connection, and discuss their roles in
making a robust metric. Based on our studies, we develop a new deep neural
network-based perceptual similarity metric. Our experiments show that our
metric is tolerant to imperceptible shifts while being consistent with the
human similarity judgment.",https://tinyurl.com/5n85r28r,-1
MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure,0.127134,"In this paper, we propose a comprehensive benchmark to investigate models'
logical reasoning capabilities in complex real-life scenarios. Current
explanation datasets often employ synthetic data with simple reasoning
structures. Therefore, it cannot express more complex reasoning processes, such
as the rebuttal to a reasoning step and the degree of certainty of the
evidence. To this end, we propose a comprehensive logical reasoning explanation
form. Based on the multi-hop chain of reasoning, the explanation form includes
three main components: (1) The condition of rebuttal that the reasoning node
can be challenged; (2) Logical formulae that uncover the internal texture of
reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The
fine-grained structure conforms to the real logical reasoning scenario, better
fitting the human cognitive process but, simultaneously, is more challenging
for the current models. We evaluate the current best models' performance on
this new explanation form. The experimental results show that generating
reasoning graphs remains a challenging task for current models, even with the
help of giant pre-trained language models.",https://github.com/tencent-ailab/MetaLogic,-1
Memorizing Transformers,0.746492,"Language models typically need to be trained or finetuned in order to acquire
new knowledge, which involves updating their weights. We instead envision
language models that can simply read and memorize new data at inference time,
thus acquiring new knowledge immediately. In this work, we extend language
models with the ability to memorize the internal representations of past
inputs. We demonstrate that an approximate kNN lookup into a non-differentiable
memory of recent (key, value) pairs improves language modeling across various
benchmarks and tasks, including generic webtext (C4), math papers (arXiv),
books (PG-19), code (Github), as well as formal theorems (Isabelle). We show
that the performance steadily improves when we increase the size of memory up
to 262K tokens. On benchmarks including code and mathematics, we find that the
model is capable of making use of newly defined functions and theorems during
test time.",None,-1
Balancing Stability and Plasticity through Advanced Null Space in Continual Learning,0.483086,"Continual learning is a learning paradigm that learns tasks sequentially with
resources constraints, in which the key challenge is stability-plasticity
dilemma, i.e., it is uneasy to simultaneously have the stability to prevent
catastrophic forgetting of old tasks and the plasticity to learn new tasks
well. In this paper, we propose a new continual learning approach, Advanced
Null Space (AdNS), to balance the stability and plasticity without storing any
old data of previous tasks. Specifically, to obtain better stability, AdNS
makes use of low-rank approximation to obtain a novel null space and projects
the gradient onto the null space to prevent the interference on the past tasks.
To control the generation of the null space, we introduce a non-uniform
constraint strength to further reduce forgetting. Furthermore, we present a
simple but effective method, intra-task distillation, to improve the
performance of the current task. Finally, we theoretically find that null space
plays a key role in plasticity and stability, respectively. Experimental
results show that the proposed method can achieve better performance compared
to state-of-the-art continual learning approaches.",None,-1
DiGamma: Domain-aware Genetic Algorithm for HW-Mapping Co-optimization for DNN Accelerators,0.887689,"The design of DNN accelerators includes two key parts: HW resource
configuration and mapping strategy. Intensive research has been conducted to
optimize each of them independently. Unfortunately, optimizing for both
together is extremely challenging due to the extremely large cross-coupled
search space. To address this, in this paper, we propose a HW-Mapping
co-optimization framework, an efficient encoding of the immense design space
constructed by HW and Mapping, and a domain-aware genetic algorithm, named
DiGamma, with specialized operators for improving search efficiency. We
evaluate DiGamma with seven popular DNNs models with different properties. Our
evaluations show DiGamma can achieve (geomean) 3.0x and 10.0x speedup,
comparing to the best-performing baseline optimization algorithms, in edge and
cloud settings.",https://github.com/maestro-project/digamma,16433
On the Forward Invariance of Neural ODEs,0.271577,"We propose a new method to ensure neural ordinary differential equations
(ODEs) satisfy output specifications by using invariance set propagation. Our
approach uses a class of control barrier functions to transform output
specifications into constraints on the parameters and inputs of the learning
system. This setup allows us to achieve output specification guarantees simply
by changing the constrained parameters/inputs both during training and
inference. Moreover, we demonstrate that our invariance set propagation through
data-controlled neural ODEs not only maintains generalization performance but
also creates an additional degree of robustness by enabling causal manipulation
of the system's parameters/inputs. We test our method on a series of
representation learning tasks, including modeling physical dynamics and
convexity portraits, as well as safe collision avoidance for autonomous
vehicles.",https://weixy21.github.io/invariance/,-1
GCS-Q: Quantum Graph Coalition Structure Generation,0.684159,"The problem of generating an optimal coalition structure for a given
coalition game of rational agents is to find a partition that maximizes their
social welfare and is known to be NP-hard. This paper proposes GCS-Q, a novel
quantum-supported solution for Induced Subgraph Games (ISGs) in coalition
structure generation. GCS-Q starts by considering the grand coalition as
initial coalition structure and proceeds by iteratively splitting the
coalitions into two nonempty subsets to obtain a coalition structure with a
higher coalition value. In particular, given an $n$-agent ISG, the GCS-Q solves
the optimal split problem $\mathcal{O} (n)$ times using a quantum annealing
device, exploring $\mathcal{O}(2^n)$ partitions at each step. We show that
GCS-Q outperforms the currently best classical solvers with its runtime in the
order of $n^2$ and an expected worst-case approximation ratio of $93\%$ on
standard benchmark datasets.",None,-1
A Memory Transformer Network for Incremental Learning,0.679776,"We study class-incremental learning, a training setup in which new classes of
data are observed over time for the model to learn from. Despite the
straightforward problem formulation, the naive application of classification
models to class-incremental learning results in the ""catastrophic forgetting""
of previously seen classes. One of the most successful existing methods has
been the use of a memory of exemplars, which overcomes the issue of
catastrophic forgetting by saving a subset of past data into a memory bank and
utilizing it to prevent forgetting when training future tasks. In our paper, we
propose to enhance the utilization of this memory bank: we not only use it as a
source of additional training data like existing works but also integrate it in
the prediction process explicitly.Our method, the Memory Transformer Network
(MTN), learns how to combine and aggregate the information from the nearest
neighbors in the memory with a transformer to make more accurate predictions.
We conduct extensive experiments and ablations to evaluate our approach. We
show that MTN achieves state-of-the-art performance on the challenging
ImageNet-1k and Google-Landmarks-1k incremental learning benchmarks.",None,-1
Reinforced Approximate Exploratory Data Analysis,0.545916,"Exploratory data analytics (EDA) is a sequential decision making process
where analysts choose subsequent queries that might lead to some interesting
insights based on the previous queries and corresponding results. Data
processing systems often execute the queries on samples to produce results with
low latency. Different downsampling strategy preserves different statistics of
the data and have different magnitude of latency reductions. The optimum choice
of sampling strategy often depends on the particular context of the analysis
flow and the hidden intent of the analyst. In this paper, we are the first to
consider the impact of sampling in interactive data exploration settings as
they introduce approximation errors. We propose a Deep Reinforcement Learning
(DRL) based framework which can optimize the sample selection in order to keep
the analysis and insight generation flow intact. Evaluations with 3 real
datasets show that our technique can preserve the original insight generation
flow while improving the interaction latency, compared to baseline methods.",https://anonymous.4open.science/r/approxEDA-53D3/,-1
Information-Theoretic Odometry Learning,0.0558926,"In this paper, we propose a unified information theoretic framework for
learning-motivated methods aimed at odometry estimation, a crucial component of
many robotics and vision tasks such as navigation and virtual reality where
relative camera poses are required in real time. We formulate this problem as
optimizing a variational information bottleneck objective function, which
eliminates pose-irrelevant information from the latent representation. The
proposed framework provides an elegant tool for performance evaluation and
understanding in information-theoretic language. Specifically, we bound the
generalization errors of the deep information bottleneck framework and the
predictability of the latent representation. These provide not only a
performance guarantee but also practical guidance for model design, sample
collection, and sensor selection. Furthermore, the stochastic latent
representation provides a natural uncertainty measure without the needs for
extra structures or computations. Experiments on two well-known odometry
datasets demonstrate the effectiveness of our method.",None,-1
FFC-SE: Fast Fourier Convolution for Speech Enhancement,0.590635,"Fast Fourier convolution (FFC) is the recently proposed neural operator
showing promising performance in several computer vision problems. The FFC
operator allows employing large receptive field operations within early layers
of the neural network. It was shown to be especially helpful for inpainting of
periodic structures which are common in audio processing. In this work, we
design neural network architectures which adapt FFC for speech enhancement. We
hypothesize that a large receptive field allows these networks to produce more
coherent phases than vanilla convolutional models, and validate this hypothesis
experimentally. We found that neural networks based on Fast Fourier convolution
outperform analogous convolutional models and show better or comparable results
with other speech enhancement baselines.",None,-1
Evidential Temporal-aware Graph-based Social Event Detection via Dempster-Shafer Theory,0.458979,"The rising popularity of online social network services has attracted lots of
research on mining social media data, especially on mining social events.
Social event detection, due to its wide applications, has now become a trivial
task. State-of-the-art approaches exploiting Graph Neural Networks (GNNs)
usually follow a two-step strategy: 1) constructing text graphs based on
various views (\textit{co-user}, \textit{co-entities} and
\textit{co-hashtags}); and 2) learning a unified text representation by a
specific GNN model. Generally, the results heavily rely on the quality of the
constructed graphs and the specific message passing scheme. However, existing
methods have deficiencies in both aspects: 1) They fail to recognize the noisy
information induced by unreliable views. 2) Temporal information which works as
a vital indicator of events is neglected in most works. To this end, we propose
ETGNN, a novel Evidential Temporal-aware Graph Neural Network. Specifically, we
construct view-specific graphs whose nodes are the texts and edges are
determined by several types of shared elements respectively. To incorporate
temporal information into the message passing scheme, we introduce a novel
temporal-aware aggregator which assigns weights to neighbours according to an
adaptive time exponential decay formula. Considering the view-specific
uncertainty, the representations of all views are converted into mass functions
through evidential deep learning (EDL) neural networks, and further combined
via Dempster-Shafer theory (DST) to make the final detection. Experimental
results on three real-world datasets demonstrate the effectiveness of ETGNN in
accuracy, reliability and robustness in social event detection.",None,-1
Effective Seed-Guided Topic Discovery by Integrating Multiple Types of Contexts,0.571937,"Instead of mining coherent topics from a given text corpus in a completely
unsupervised manner, seed-guided topic discovery methods leverage user-provided
seed words to extract distinctive and coherent topics so that the mined topics
can better cater to the user's interest. To model the semantic correlation
between words and seeds for discovering topic-indicative terms, existing
seed-guided approaches utilize different types of context signals, such as
document-level word co-occurrences, sliding window-based local contexts, and
generic linguistic knowledge brought by pre-trained language models. In this
work, we analyze and show empirically that each type of context information has
its value and limitation in modeling word semantics under seed guidance, but
combining three types of contexts (i.e., word embeddings learned from local
contexts, pre-trained language model representations obtained from
general-domain training, and topic-indicative sentences retrieved based on seed
information) allows them to complement each other for discovering quality
topics. We propose an iterative framework, SeedTopicMine, which jointly learns
from the three types of contexts and gradually fuses their context signals via
an ensemble ranking process. Under various sets of seeds and on multiple
datasets, SeedTopicMine consistently yields more coherent and accurate topics
than existing seed-guided topic discovery approaches.",https://github.com/yzhan238/SeedTopicMine,-1
ProQA: Structural Prompt-based Pre-training for Unified Question Answering,0.616363,"Question Answering (QA) is a longstanding challenge in natural language
processing. Existing QA works mostly focus on specific question types,
knowledge domains, or reasoning skills. The specialty in QA research hinders
systems from modeling commonalities between tasks and generalization for wider
applications. To address this issue, we present ProQA, a unified QA paradigm
that solves various tasks through a single model. ProQA takes a unified
structural prompt as the bridge and improves the QA-centric ability by
structural prompt-based pre-training. Through a structurally designed
prompt-based input schema, ProQA concurrently models the knowledge
generalization for all QA tasks while keeping the knowledge customization for
every specific QA task. Furthermore, ProQA is pre-trained with structural
prompt-formatted large-scale synthesized corpus, which empowers the model with
the commonly-required QA ability. Experimental results on 11 QA benchmarks
demonstrate that ProQA consistently boosts performance on both full data
fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore,
ProQA exhibits strong ability in both continual learning and transfer learning
by taking the advantages of the structural prompt.",https://github.com/zhongwanjun/ProQA,-1
Exploring Target Representations for Masked Autoencoders,0.889032,"Masked autoencoders have become popular training paradigms for
self-supervised visual representation learning. These models randomly mask a
portion of the input and reconstruct the masked portion according to the target
representations. In this paper, we first show that a careful choice of the
target representation is unnecessary for learning good representations, since
different targets tend to derive similarly behaved models. Driven by this
observation, we propose a multi-stage masked distillation pipeline and use a
randomly initialized model as the teacher, enabling us to effectively train
high-capacity models without any efforts to carefully design target
representations. Interestingly, we further explore using teachers of larger
capacity, obtaining distilled students with remarkable transferring ability. On
different tasks of classification, transfer learning, object detection, and
semantic segmentation, the proposed method to perform masked knowledge
distillation with bootstrapped teachers (dBOT) outperforms previous
self-supervised methods by nontrivial margins. We hope our findings, as well as
the proposed method, could motivate people to rethink the roles of target
representations in pre-training masked autoencoders.The code and pre-trained
models are publicly available at https://github.com/liuxingbin/dbot.",https://github.com/liuxingbin/dbot,28838
Denoising-based image reconstruction from pixels located at non-integer positions,0.0843615,"Digital images are commonly represented as regular 2D arrays, so pixels are
organized in form of a matrix addressed by integers. However, there are many
image processing operations, such as rotation or motion compensation, that
produce pixels at non-integer positions. Typically, image reconstruction
techniques cannot handle samples at non-integer positions. In this paper, we
propose to use triangulation-based reconstruction as initial estimate that is
later refined by a novel adaptive denoising framework. Simulations reveal that
improvements of up to more than 1.8 dB (in terms of PSNR) are achieved with
respect to the initial estimate.",None,-1
Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models,0.87582,"We present a novel way of conditioning a pretrained denoising diffusion
speech model to produce speech in the voice of a novel person unseen during
training. The method requires a short (~3 seconds) sample from the target
person, and generation is steered at inference time, without any training
steps. At the heart of the method lies a sampling process that combines the
estimation of the denoising model with a low-pass version of the new speaker's
sample. The objective and subjective evaluations show that our sampling method
can generate a voice similar to that of the target speaker in terms of
frequency, with an accuracy comparable to state-of-the-art methods, and without
training.",None,-1
Instance-based Learning for Knowledge Base Completion,0.278136,"In this paper, we propose a new method for knowledge base completion (KBC):
instance-based learning (IBL). For example, to answer (Jill Biden, lived city,?
), instead of going directly to Washington D.C., our goal is to find Joe Biden,
who has the same lived city as Jill Biden. Through prototype entities, IBL
provides interpretability. We develop theories for modeling prototypes and
combining IBL with translational models. Experiments on various tasks confirmed
the IBL model's effectiveness and interpretability.
  In addition, IBL shed light on the mechanism of rule-based KBC models.
Previous research has generally agreed that rule-based models provide rules
with semantically compatible premises and hypotheses. We challenge this view.
We begin by demonstrating that some logical rules represent {\it instance-based
equivalence} (i.e. prototypes) rather than semantic compatibility. These are
denoted as {\it IBL rules}. Surprisingly, despite occupying only a small
portion of the rule space, IBL rules outperform non-IBL rules in all four
benchmarks. We use a variety of experiments to demonstrate that rule-based
models work because they have the ability to represent instance-based
equivalence via IBL rules. The findings provide new insights of how rule-based
models work and how to interpret their rules.",https://github.com/chenxran/InstanceBasedLearning,-1
LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling,0.449597,"Compared with traditional task-irrelevant downsampling methods, task-oriented
neural networks have shown improved performance in point cloud downsampling
range. Recently, Transformer family of networks has shown a more powerful
learning capacity in visual tasks. However, Transformer-based architectures
potentially consume too many resources which are usually worthless for low
overhead task networks in downsampling range. This paper proposes a novel
light-weight Transformer network (LighTN) for task-oriented point cloud
downsampling, as an end-to-end and plug-and-play solution. In LighTN, a
single-head self-correlation module is presented to extract refined global
contextual features, where three projection matrices are simultaneously
eliminated to save resource overhead, and the output of symmetric matrix
satisfies the permutation invariant. Then, we design a novel downsampling loss
function to guide LighTN focuses on critical point cloud regions with more
uniform distribution and prominent points coverage. Furthermore, We introduce a
feed-forward network scaling mechanism to enhance the learnable capacity of
LighTN according to the expand-reduce strategy. The result of extensive
experiments on classification and registration tasks demonstrates LighTN can
achieve state-of-the-art performance with limited resource overhead.",None,14870
Open-set Recognition via Augmentation-based Similarity Learning,0.0686769,"The primary assumption of conventional supervised learning or classification
is that the test samples are drawn from the same distribution as the training
samples, which is called closed set learning or classification. In many
practical scenarios, this is not the case because there are unknowns or unseen
class samples in the test data, which is called the open set scenario, and the
unknowns need to be detected. This problem is referred to as the open set
recognition problem and is important in safety-critical applications. We
propose to detect unknowns (or unseen class samples) through learning pairwise
similarities. The proposed method works in two steps. It first learns a closed
set classifier using the seen classes that have appeared in training and then
learns how to compare seen classes with pseudo-unseen (automatically generated
unseen class samples). The pseudo-unseen generation is carried out by
performing distribution shifting augmentations on the seen or training samples.
We call our method OPG (Open set recognition based on Pseudo unseen data
Generation). The experimental evaluation shows that the learned
similarity-based features can successfully distinguish seen from unseen in
benchmark datasets for open set recognition.",https://github.com/dimitymiller/cac-openset,-1
Self-Supervision Can Be a Good Few-Shot Learner,0.756771,"Existing few-shot learning (FSL) methods rely on training with a large
labeled dataset, which prevents them from leveraging abundant unlabeled data.
From an information-theoretic perspective, we propose an effective unsupervised
FSL method, learning representations with self-supervision. Following the
InfoMax principle, our method learns comprehensive representations by capturing
the intrinsic structure of the data. Specifically, we maximize the mutual
information (MI) of instances and their representations with a low-bias MI
estimator to perform self-supervised pre-training. Rather than supervised
pre-training focusing on the discriminable features of the seen classes, our
self-supervised model has less bias toward the seen classes, resulting in
better generalization for unseen classes. We explain that supervised
pre-training and self-supervised pre-training are actually maximizing different
MI objectives. Extensive experiments are further conducted to analyze their FSL
performance with various training settings. Surprisingly, the results show that
self-supervised pre-training can outperform supervised pre-training under the
appropriate conditions. Compared with state-of-the-art FSL methods, our
approach achieves comparable performance on widely used FSL benchmarks without
any labels of the base classes.",None,-1
Attribution-aware Weight Transfer: A Warm-Start Initialization for Class-Incremental Semantic Segmentation,0.625651,"In class-incremental semantic segmentation (CISS), deep learning
architectures suffer from the critical problems of catastrophic forgetting and
semantic background shift. Although recent works focused on these issues,
existing classifier initialization methods do not address the background shift
problem and assign the same initialization weights to both background and new
foreground class classifiers. We propose to address the background shift with a
novel classifier initialization method which employs gradient-based attribution
to identify the most relevant weights for new classes from the classifier's
weights for the previous background and transfers these weights to the new
classifier. This warm-start weight initialization provides a general solution
applicable to several CISS methods. Furthermore, it accelerates learning of new
classes while mitigating forgetting. Our experiments demonstrate significant
improvement in mIoU compared to the state-of-the-art CISS methods on the
Pascal-VOC 2012, ADE20K and Cityscapes datasets.",https://github.com/dfki-av/AWT-for-CISS,-1
Efficiently Tuned Parameters are Task Embeddings,0.181243,"Intermediate-task transfer can benefit a wide range of NLP tasks with
properly selected source datasets. However, it is computationally infeasible to
experiment with all intermediate transfer combinations, making choosing a
useful source task a challenging problem. In this paper, we anticipate that
task-specific parameters updated in parameter-efficient tuning methods are
likely to encode task-specific information. Therefore, such parameters can be
predictive for inter-task transferability. Thus, we propose to exploit these
efficiently tuned parameters as off-the-shelf task embeddings for the efficient
selection of source datasets for intermediate-task transfer. We experiment with
11 text classification tasks and 11 question answering tasks. Experimental
results show that our approach can consistently outperform existing inter-task
transferability prediction methods while being conceptually simple and
computationally efficient. Our analysis also reveals that the ability of
efficiently tuned parameters on transferability prediction is disentangled with
their in-task performance. This allows us to use parameters from early
checkpoints as task embeddings to further improve efficiency.",https://github.com/JetRunner/TuPaTE,-1
SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers,0.446176,"This paper aims to improve the performance of text-to-SQL parsing by
exploring the intrinsic uncertainties in the neural network based approaches
(called SUN). From the data uncertainty perspective, it is indisputable that a
single SQL can be learned from multiple semantically-equivalent
questions.Different from previous methods that are limited to one-to-one
mapping, we propose a data uncertainty constraint to explore the underlying
complementary semantic information among multiple semantically-equivalent
questions (many-to-one) and learn the robust feature representations with
reduced spurious associations. In this way, we can reduce the sensitivity of
the learned representations and improve the robustness of the parser. From the
model uncertainty perspective, there is often structural information
(dependence) among the weights of neural networks. To improve the
generalizability and stability of neural text-to-SQL parsers, we propose a
model uncertainty constraint to refine the query representations by enforcing
the output representations of different perturbed encoding networks to be
consistent with each other. Extensive experiments on five benchmark datasets
demonstrate that our method significantly outperforms strong competitors and
achieves new state-of-the-art results. For reproducibility, we release our code
and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/sunsql.",https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/sunsql,-1
TAD: Transfer Learning-based Multi-Adversarial Detection of Evasion Attacks against Network Intrusion Detection Systems,0.772581,"Nowadays, intrusion detection systems based on deep learning deliver
state-of-the-art performance. However, recent research has shown that specially
crafted perturbations, called adversarial examples, are capable of
significantly reducing the performance of these intrusion detection systems.
The objective of this paper is to design an efficient transfer learning-based
adversarial detector and then to assess the effectiveness of using multiple
strategically placed adversarial detectors compared to a single adversarial
detector for intrusion detection systems. In our experiments, we implement
existing state-of-the-art models for intrusion detection. We then attack those
models with a set of chosen evasion attacks. In an attempt to detect those
adversarial attacks, we design and implement multiple transfer learning-based
adversarial detectors, each receiving a subset of the information passed
through the IDS. By combining their respective decisions, we illustrate that
combining multiple detectors can further improve the detectability of
adversarial traffic compared to a single detector in the case of a parallel IDS
design.",None,-1
Online Segmentation of LiDAR Sequences: Dataset and Algorithm,0.537489,"Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles.
However, most semantic datasets and algorithms used for LiDAR sequence
segmentation operate on $360^\circ$ frames, causing an acquisition latency
incompatible with real-time applications. To address this issue, we first
introduce HelixNet, a $10$ billion point dataset with fine-grained labels,
timestamps, and sensor rotation information necessary to accurately assess the
real-time readiness of segmentation algorithms. Second, we propose Helix4D, a
compact and efficient spatio-temporal transformer architecture specifically
designed for rotating LiDAR sequences. Helix4D operates on acquisition slices
corresponding to a fraction of a full sensor rotation, significantly reducing
the total latency. Helix4D reaches accuracy on par with the best segmentation
algorithms on HelixNet and SemanticKITTI with a reduction of over $5\times$ in
terms of latency and $50\times$ in model size. The code and data are available
at: https://romainloiseau.fr/helixnet",https://romainloiseau.fr/helixnet,-1
LocalBins: Improving Depth Estimation by Learning Local Distributions,0.95091,"We propose a novel architecture for depth estimation from a single image. The
architecture itself is based on the popular encoder-decoder architecture that
is frequently used as a starting point for all dense regression tasks. We build
on AdaBins which estimates a global distribution of depth values for the input
image and evolve the architecture in two ways. First, instead of predicting
global depth distributions, we predict depth distributions of local
neighborhoods at every pixel. Second, instead of predicting depth distributions
only towards the end of the decoder, we involve all layers of the decoder. We
call this new architecture LocalBins. Our results demonstrate a clear
improvement over the state-of-the-art in all metrics on the NYU-Depth V2
dataset. Code and pretrained models will be made publicly available.",https://github.com/shariqfarooq123/LocalBins,21729
Exploiting Contrastive Learning and Numerical Evidence for Confusing Legal Judgment Prediction,0.262445,"Given the fact description text of a legal case, legal judgment prediction
(LJP) aims to predict the case's charge, law article and penalty term. A core
problem of LJP is how to distinguish confusing legal cases, where only subtle
text differences exist. Previous studies fail to distinguish different
classification errors with a standard cross-entropy classification loss, and
ignore the numbers in the fact description for predicting the term of penalty.
To tackle these issues, in this work, first, we propose a moco-based supervised
contrastive learning to learn distinguishable representations, and explore the
best strategy to construct positive example pairs to benefit all three subtasks
of LJP simultaneously. Second, in order to exploit the numbers in legal cases
for predicting the penalty terms of certain cases, we further enhance the
representation of the fact description with extracted crime amounts which are
encoded by a pre-trained numeracy model. Extensive experiments on public
benchmarks show that the proposed method achieves new state-of-the-art results,
especially on confusing legal cases. Ablation studies also demonstrate the
effectiveness of each component.",https://github.com/leileigan/ContrastiveLJP,-1
RbA: Segmenting Unknown Regions Rejected by All,0.97515,"Standard semantic segmentation models owe their success to curated datasets
with a fixed set of semantic categories, without contemplating the possibility
of identifying unknown objects from novel categories. Existing methods in
outlier detection suffer from a lack of smoothness and objectness in their
predictions, due to limitations of the per-pixel classification paradigm.
Furthermore, additional training for detecting outliers harms the performance
of known classes. In this paper, we explore another paradigm with region-level
classification to better segment unknown objects. We show that the object
queries in mask classification tend to behave like one \vs all classifiers.
Based on this finding, we propose a novel outlier scoring function called RbA
by defining the event of being an outlier as being rejected by all known
classes. Our extensive experiments show that mask classification improves the
performance of the existing outlier detection methods, and the best results are
achieved with the proposed RbA. We also propose an objective to optimize RbA
using minimal outlier supervision. Further fine-tuning with outliers improves
the unknown performance, and unlike previous methods, it does not degrade the
inlier performance.",None,-1
Adaptive Testing of Computer Vision Models,0.658418,"Vision models often fail systematically on groups of data that share common
semantic characteristics (e.g., rare objects or unusual scenes), but
identifying these failure modes is a challenge. We introduce AdaVision, an
interactive process for testing vision models which helps users identify and
fix coherent failure modes. Given a natural language description of a coherent
group, AdaVision retrieves relevant images from LAION-5B with CLIP. The user
then labels a small amount of data for model correctness, which is used in
successive retrieval rounds to hill-climb towards high-error regions, refining
the group definition. Once a group is saturated, AdaVision uses GPT-3 to
suggest new group descriptions for the user to explore. We demonstrate the
usefulness and generality of AdaVision in user studies, where users find major
bugs in state-of-the-art classification, object detection, and image captioning
models. These user-discovered groups have failure rates 2-3x higher than those
surfaced by automatic error clustering methods. Finally, finetuning on examples
found with AdaVision fixes the discovered bugs when evaluated on unseen
examples, without degrading in-distribution accuracy, and while also improving
performance on out-of-distribution datasets.",https://github.com/i-gao/adavision,-1
Using Argumentation Schemes to Model Legal Reasoning,0.106144,"We present argumentation schemes to model reasoning with legal cases. We
provide schemes for each of the three stages that take place after the facts
are established: factor ascription, issue resolution and outcome determination.
The schemes are illustrated with examples from a specific legal domain, US
Trade Secrets law, and the wider applicability of these schemes is discussed.",None,-1
OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions,0.111609,"The pretrain-finetune paradigm in modern computer vision facilitates the
success of self-supervised learning, which tends to achieve better
transferability than supervised learning. However, with the availability of
massive labeled data, a natural question emerges: how to train a better model
with both self and full supervision signals? In this paper, we propose
Omni-suPErvised Representation leArning with hierarchical supervisions (OPERA)
as a solution. We provide a unified perspective of supervisions from labeled
and unlabeled data and propose a unified framework of fully supervised and
self-supervised learning. We extract a set of hierarchical proxy
representations for each image and impose self and full supervisions on the
corresponding proxy representations. Extensive experiments on both
convolutional neural networks and vision transformers demonstrate the
superiority of OPERA in image classification, segmentation, and object
detection. Code is available at: https://github.com/wangck20/OPERA.",https://github.com/wangck20/OPERA,-1
GIFS: Neural Implicit Function for General Shape Representation,0.844034,"Recent development of neural implicit function has shown tremendous success
on high-quality 3D shape reconstruction. However, most works divide the space
into inside and outside of the shape, which limits their representing power to
single-layer and watertight shapes. This limitation leads to tedious data
processing (converting non-watertight raw data to watertight) as well as the
incapability of representing general object shapes in the real world. In this
work, we propose a novel method to represent general shapes including
non-watertight shapes and shapes with multi-layer surfaces. We introduce
General Implicit Function for 3D Shape (GIFS), which models the relationships
between every two points instead of the relationships between points and
surfaces. Instead of dividing 3D space into predefined inside-outside regions,
GIFS encodes whether two points are separated by any surface. Experiments on
ShapeNet show that GIFS outperforms previous state-of-the-art methods in terms
of reconstruction quality, rendering efficiency, and visual fidelity. Project
page is available at https://jianglongye.com/gifs .",https://jianglongye.com/gifs,25908
PETR: Position Embedding Transformation for Multi-View 3D Object Detection,1.0,"In this paper, we develop position embedding transformation (PETR) for
multi-view 3D object detection. PETR encodes the position information of 3D
coordinates into image features, producing the 3D position-aware features.
Object query can perceive the 3D position-aware features and perform end-to-end
object detection. PETR achieves state-of-the-art performance (50.4% NDS and
44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.
It can serve as a simple yet strong baseline for future research. Code is
available at \url{https://github.com/megvii-research/PETR}.",https://github.com/megvii-research/PETR,-1
Keypoint Cascade Voting for Point Cloud Based 6DoF Pose Estimation,0.462479,"We propose a novel keypoint voting 6DoF object pose estimation method, which
takes pure unordered point cloud geometry as input without RGB information. The
proposed cascaded keypoint voting method, called RCVPose3D, is based upon a
novel architecture which separates the task of semantic segmentation from that
of keypoint regression, thereby increasing the effectiveness of both and
improving the ultimate performance. The method also introduces a pairwise
constraint in between different keypoints to the loss function when regressing
the quantity for keypoint estimation, which is shown to be effective, as well
as a novel Voter Confident Score which enhances both the learning and inference
stages. Our proposed RCVPose3D achieves state-of-the-art performance on the
Occlusion LINEMOD (74.5%) and YCB-Video (96.9%) datasets, outperforming
existing pure RGB and RGB-D based methods, as well as being competitive with
RGB plus point cloud methods.",https://github.com/aaronWool/rcvpose3d,3559
Learning-based Motion Planning in Dynamic Environments Using GNNs and Temporal Encoding,0.583777,"Learning-based methods have shown promising performance for accelerating
motion planning, but mostly in the setting of static environments. For the more
challenging problem of planning in dynamic environments, such as multi-arm
assembly tasks and human-robot interaction, motion planners need to consider
the trajectories of the dynamic obstacles and reason about temporal-spatial
interactions in very large state spaces. We propose a GNN-based approach that
uses temporal encoding and imitation learning with data aggregation for
learning both the embeddings and the edge prioritization policies. Experiments
show that the proposed methods can significantly accelerate online planning
over state-of-the-art complete dynamic planning algorithms. The learned models
can often reduce costly collision checking operations by more than 1000x, and
thus accelerating planning by up to 95%, while achieving high success rates on
hard instances as well.",None,-1
Editable Indoor Lighting Estimation,0.638021,"We present a method for estimating lighting from a single perspective image
of an indoor scene. Previous methods for predicting indoor illumination usually
focus on either simple, parametric lighting that lack realism, or on richer
representations that are difficult or even impossible to understand or modify
after prediction. We propose a pipeline that estimates a parametric light that
is easy to edit and allows renderings with strong shadows, alongside with a
non-parametric texture with high-frequency information necessary for realistic
rendering of specular objects. Once estimated, the predictions obtained with
our model are interpretable and can easily be modified by an artist/user with a
few mouse clicks. Quantitative and qualitative results show that our approach
makes indoor lighting estimation easier to handle by a casual user, while still
producing competitive results.",None,6586
TASTEset -- Recipe Dataset and Food Entities Recognition Benchmark,0.310313,"Food Computing is currently a fast-growing field of research. Natural
language processing (NLP) is also increasingly essential in this field,
especially for recognising food entities. However, there are still only a few
well-defined tasks that serve as benchmarks for solutions in this area. We
introduce a new dataset -- called \textit{TASTEset} -- to bridge this gap. In
this dataset, Named Entity Recognition (NER) models are expected to find or
infer various types of entities helpful in processing recipes, e.g.~food
products, quantities and their units, names of cooking processes, physical
quality of ingredients, their purpose, taste.
  The dataset consists of 700 recipes with more than 13,000 entities to
extract. We provide a few state-of-the-art baselines of named entity
recognition models, which show that our dataset poses a solid challenge to
existing models. The best model achieved, on average, 0.95 $F_1$ score,
depending on the entity type -- from 0.781 to 0.982. We share the dataset and
the task to encourage progress on more in-depth and complex information
extraction from recipes.",https://github.com/taisti/TASTEset,-1
ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields,0.47641,"While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline
methods with an emphasis on visual fidelity, our paper addresses the online use
case that prioritises real-time adaptability. We present ParticleNeRF, a new
approach that dynamically adapts to changes in the scene geometry by learning
an up-to-date representation online, every 200ms. ParticleNeRF achieves this
using a novel particle-based parametric encoding. We couple features to
particles in space and backpropagate the photometric reconstruction loss into
the particles' position gradients, which are then interpreted as velocity
vectors. Governed by a lightweight physics system to handle collisions, this
lets the features move freely with the changing scene geometry. We demonstrate
ParticleNeRF on various dynamic scenes containing translating, rotating,
articulated, and deformable objects. ParticleNeRF is the first online dynamic
NeRF and achieves fast adaptability with better visual fidelity than
brute-force online InstantNGP and other baseline approaches on dynamic scenes
with online constraints. Videos of our system can be found at our project
website https://sites.google.com/view/particlenerf.",None,-1
Region2Vec: Community Detection on Spatial Networks Using Graph Embedding with Node Attributes and Spatial Interactions,0.345791,"Community Detection algorithms are used to detect densely connected
components in complex networks and reveal underlying relationships among
components. As a special type of networks, spatial networks are usually
generated by the connections among geographic regions. Identifying the spatial
network communities can help reveal the spatial interaction patterns,
understand the hidden regional structures and support regional development
decision-making. Given the recent development of Graph Convolutional Networks
(GCN) and its powerful performance in identifying multi-scale spatial
interactions, we proposed an unsupervised GCN-based community detection method
""region2vec"" on spatial networks. Our method first generates node embeddings
for regions that share common attributes and have intense spatial interactions,
and then applies clustering algorithms to detect communities based on their
embedding similarity and spatial adjacency. Experimental results show that
while existing methods trade off either attribute similarities or spatial
interactions for one another, ""region2vec"" maintains a great balance between
both and performs the best when one wants to maximize both attribute
similarities and spatial interactions within communities.",None,-1
FairGer: Using NLP to Measure Support for Women and Migrants in 155 Years of German Parliamentary Debates,0.13863,"We measure support with women and migrants in German political debates over
the last 155 years. To do so, we (1) provide a gold standard of 1205 text
snippets in context, annotated for support with our target groups, (2) train a
BERT model on our annotated data, with which (3) we infer large-scale trends.
These show that support with women is stronger than support with migrants, but
both have steadily increased over time. While we hardly find any direct
anti-support with women, there is more polarization when it comes to migrants.
We also discuss the difficulty of annotation as a result of ambiguity in
political discourse and indirectness, i.e., politicians' tendency to relate
stances attributed to political opponents. Overall, our results indicate that
German society, as measured from its political elite, has become fairer over
time.",https://github.com/DominikBeese/DeuParl-v2,-1
When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment,0.684473,"AI systems are becoming increasingly intertwined with human life. In order to
effectively collaborate with humans and ensure safety, AI systems need to be
able to understand, interpret and predict human moral judgments and decisions.
Human moral judgments are often guided by rules, but not always. A central
challenge for AI safety is capturing the flexibility of the human moral mind --
the ability to determine when a rule should be broken, especially in novel or
unusual situations. In this paper, we present a novel challenge set consisting
of rule-breaking question answering (RBQA) of cases that involve potentially
permissible rule-breaking -- inspired by recent moral psychology studies. Using
a state-of-the-art large language model (LLM) as a basis, we propose a novel
moral chain of thought (MORALCOT) prompting strategy that combines the
strengths of LLMs with theories of moral reasoning developed in cognitive
science to predict human moral judgments. MORALCOT outperforms seven existing
LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to
capture the flexibility of the human moral mind. We also conduct a detailed
error analysis to suggest directions for future work to improve AI safety using
RBQA. Our data is open-sourced at
https://huggingface.co/datasets/feradauto/MoralExceptQA and code at
https://github.com/feradauto/MoralCoT",https://github.com/feradauto/MoralCoT,-1
McQueen: a Benchmark for Multimodal Conversational Query Rewrite,0.291658,"The task of query rewrite aims to convert an in-context query to its
fully-specified version where ellipsis and coreference are completed and
referred-back according to the history context. Although much progress has been
made, less efforts have been paid to real scenario conversations that involve
drawing information from more than one modalities. In this paper, we propose
the task of multimodal conversational query rewrite (McQR), which performs
query rewrite under the multimodal visual conversation setting. We collect a
large-scale dataset named McQueen based on manual annotation, which contains
15k visual conversations and over 80k queries where each one is associated with
a fully-specified rewrite version. In addition, for entities appearing in the
rewrite, we provide the corresponding image box annotation. We then use the
McQueen dataset to benchmark a state-of-the-art method for effectively tackling
the McQR task, which is based on a multimodal pre-trained model with pointer
generator. Extensive experiments are performed to demonstrate the effectiveness
of our model on this task\footnote{The dataset and code of this paper are both
available in \url{https://github.com/yfyuan01/MQR}",None,-1
Revisiting AP Loss for Dense Object Detection: Adaptive Ranking Pair Selection,0.425304,"Average precision (AP) loss has recently shown promising performance on the
dense object detection task. However,a deep understanding of how AP loss
affects the detector from a pairwise ranking perspective has not yet been
developed.In this work, we revisit the average precision (AP)loss and reveal
that the crucial element is that of selecting the ranking pairs between
positive and negative samples.Based on this observation, we propose two
strategies to improve the AP loss. The first of these is a novel Adaptive
Pairwise Error (APE) loss that focusing on ranking pairs in both positive and
negative samples. Moreover,we select more accurate ranking pairs by exploiting
the normalized ranking scores and localization scores with a clustering
algorithm. Experiments conducted on the MSCOCO dataset support our analysis and
demonstrate the superiority of our proposed method compared with current
classification and ranking loss. The code is available at
https://github.com/Xudangliatiger/APE-Loss.",https://github.com/Xudangliatiger/APE-Loss,-1
Towards Generalized Models for Task-oriented Dialogue Modeling on Spoken Conversations,0.141679,"Building robust and general dialogue models for spoken conversations is
challenging due to the gap in distributions of spoken and written data. This
paper presents our approach to build generalized models for the
Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations
Challenge of DSTC-10. In order to mitigate the discrepancies between spoken and
written text, we mainly employ extensive data augmentation strategies on
written data, including artificial error injection and round-trip text-speech
transformation. To train robust models for spoken conversations, we improve
pre-trained language models, and apply ensemble algorithms for each sub-task.
Typically, for the detection task, we fine-tune \roberta and ELECTRA, and run
an error-fixing ensemble algorithm. For the selection task, we adopt a
two-stage framework that consists of entity tracking and knowledge ranking, and
propose a multi-task learning method to learn multi-level semantic information
by domain classification and entity selection. For the generation task, we
adopt a cross-validation data process to improve pre-trained generative
language models, followed by a consensus decoding algorithm, which can add
arbitrary features like relative \rouge metric, and tune associated feature
weights toward \bleu directly. Our approach ranks third on the objective
evaluation and second on the final official human evaluation.",https://github.com/microsoft/unilm/tree/master/s2s-ft,-1
Closed-Loop View of the Regulation of AI: Equal Impact across Repeated Interactions,0.143083,"There has been much recent interest in the regulation of AI. We argue for a
view based on civil-rights legislation, built on the notions of equal treatment
and equal impact. In a closed-loop view of the AI system and its users, the
equal treatment concerns one pass through the loop. Equal impact, in our view,
concerns the long-run average behaviour across repeated interactions. In order
to establish the existence of the average and its properties, one needs to
study the ergodic properties of the closed-loop and its unique stationary
measure.",None,-1
Keep Me Updated! Memory Management in Long-term Conversations,0.662878,"Remembering important information from the past and continuing to talk about
it in the present are crucial in long-term conversations. However, previous
literature does not deal with cases where the memorized information is
outdated, which may cause confusion in later conversations. To address this
issue, we present a novel task and a corresponding dataset of memory management
in long-term conversations, in which bots keep track of and bring up the latest
information about users while conversing through multiple sessions. In order to
support more precise and interpretable memory, we represent memory as
unstructured text descriptions of key information and propose a new mechanism
of memory management that selectively eliminates invalidated or redundant
information. Experimental results show that our approach outperforms the
baselines that leave the stored memory unchanged in terms of engagingness and
humanness, with larger performance gap especially in the later sessions.",https://github.com/naver-ai/carecall-memory,6394
"Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic",0.672362,"As natural language processing systems become more widespread, it is
necessary to address fairness issues in their implementation and deployment to
ensure that their negative impacts on society are understood and minimized.
However, there is limited work that studies fairness using a multilingual and
intersectional framework or on downstream tasks. In this paper, we introduce
four multilingual Equity Evaluation Corpora, supplementary test sets designed
to measure social biases, and a novel statistical framework for studying
unisectional and intersectional social biases in natural language processing.
We use these tools to measure gender, racial, ethnic, and intersectional social
biases across five models trained on emotion regression tasks in English,
Spanish, and Arabic. We find that many systems demonstrate statistically
significant unisectional and intersectional social biases.",https://github.com/ascamara/,-1
Multimodal Token Fusion for Vision Transformers,0.998234,"Many adaptations of transformers have emerged to address the single-modal
vision tasks, where self-attention modules are stacked to handle input sources
like images. Intuitively, feeding multiple modalities of data to vision
transformers could improve the performance, yet the inner-modal attentive
weights may also be diluted, which could thus undermine the final performance.
In this paper, we propose a multimodal token fusion method (TokenFusion),
tailored for transformer-based vision tasks. To effectively fuse multiple
modalities, TokenFusion dynamically detects uninformative tokens and
substitutes these tokens with projected and aggregated inter-modal features.
Residual positional alignment is also adopted to enable explicit utilization of
the inter-modal alignments after fusion. The design of TokenFusion allows the
transformer to learn correlations among multimodal features, while the
single-modal transformer architecture remains largely intact. Extensive
experiments are conducted on a variety of homogeneous and heterogeneous
modalities and demonstrate that TokenFusion surpasses state-of-the-art methods
in three typical vision tasks: multimodal image-to-image translation, RGB-depth
semantic segmentation, and 3D object detection with point cloud and images. Our
code is available at https://github.com/yikaiw/TokenFusion.",https://github.com/yikaiw/TokenFusion,-1
AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models,0.229715,"Knowledge distillation (KD) methods compress large models into smaller
students with manually-designed student architectures given pre-specified
computational cost. This requires several trials to find a viable student, and
further repeating the process for each student or computational budget change.
We use Neural Architecture Search (NAS) to automatically distill several
compressed students with variable cost from a large model. Current works train
a single SuperLM consisting of millions of subnetworks with weight-sharing,
resulting in interference between subnetworks of different sizes. Our framework
AutoDistil addresses above challenges with the following steps: (a)
Incorporates inductive bias and heuristics to partition Transformer search
space into K compact sub-spaces (K=3 for typical student sizes of base, small
and tiny); (b) Trains one SuperLM for each sub-space using task-agnostic
objective (e.g., self-attention distillation) with weight-sharing of students;
(c) Lightweight search for the optimal student without re-training. Fully
task-agnostic training and search allow students to be reused for fine-tuning
on any downstream task. Experiments on GLUE benchmark against state-of-the-art
KD and NAS methods demonstrate AutoDistil to outperform leading compression
techniques with upto 2.7x reduction in computational cost and negligible loss
in task performance.",None,-1
Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations,0.674969,"Neural networks have achieved tremendous success in a large variety of
applications. However, their memory footprint and computational demand can
render them impractical in application settings with limited hardware or energy
resources. In this work, we propose a novel algorithm to find efficient
low-rank subnetworks. Remarkably, these subnetworks are determined and adapted
already during the training phase and the overall time and memory resources
required by both training and evaluating them are significantly reduced. The
main idea is to restrict the weight matrices to a low-rank manifold and to
update the low-rank factors rather than the full matrix during training. To
derive training updates that are restricted to the prescribed manifold, we
employ techniques from dynamic model order reduction for matrix differential
equations. This allows us to provide approximation, stability, and descent
guarantees. Moreover, our method automatically and dynamically adapts the ranks
during training to achieve the desired approximation accuracy. The efficiency
of the proposed method is demonstrated through a variety of numerical
experiments on fully-connected and convolutional networks.",https://github.com/COMPiLELab/DLRT,-1
PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models,0.986813,"Generalizable 3D part segmentation is important but challenging in vision and
robotics. Training deep models via conventional supervised methods requires
large-scale 3D datasets with fine-grained part annotations, which are costly to
collect. This paper explores an alternative way for low-shot part segmentation
of 3D point clouds by leveraging a pretrained image-language model, GLIP, which
achieves superior performance on open-vocabulary 2D detection. We transfer the
rich knowledge from 2D to 3D through GLIP-based part detection on point cloud
rendering and a novel 2D-to-3D label lifting algorithm. We also utilize
multi-view 3D priors and few-shot prompt tuning to boost performance
significantly. Extensive evaluation on PartNet and PartNet-Mobility datasets
shows that our method enables excellent zero-shot 3D part segmentation. Our
few-shot version not only outperforms existing few-shot approaches by a large
margin but also achieves highly competitive results compared to the fully
supervised counterpart. Furthermore, we demonstrate that our method can be
directly applied to iPhone-scanned point clouds without significant domain
gaps.",None,-1
Personal Attribute Prediction from Conversations,0.335546,"Personal knowledge bases (PKBs) are critical to many applications, such as
Web-based chatbots and personalized recommendation. Conversations containing
rich personal knowledge can be regarded as a main source to populate the PKB.
Given a user, a user attribute, and user utterances from a conversational
system, we aim to predict the personal attribute value for the user, which is
helpful for the enrichment of PKBs. However, there are three issues existing in
previous studies: (1) manually labeled utterances are required for model
training; (2) personal attribute knowledge embedded in both utterances and
external resources is underutilized; (3) the performance on predicting some
difficult personal attributes is unsatisfactory. In this paper, we propose a
framework DSCGN based on the pre-trained language model with a noise-robust
loss function to predict personal attributes from conversations without
requiring any labeled utterances. We yield two categories of supervision, i.e.,
document-level supervision via a distant supervision strategy and
contextualized word-level supervision via a label guessing method, by mining
the personal attribute knowledge embedded in both unlabeled utterances and
external resources to fine-tune the language model. Extensive experiments over
two real-world data sets (i.e., a profession data set and a hobby data set)
show our framework obtains the best performance compared with all the twelve
baselines in terms of nDCG and MRR.",https://github.com/CodingPerson/DSCGN,-1
"""My nose is running.""""Are you also coughing?"": Building A Medical Diagnosis Agent with Interpretable Inquiry Logics",0.358487,"With the rise of telemedicine, the task of developing Dialogue Systems for
Medical Diagnosis (DSMD) has received much attention in recent years. Different
from early researches that needed to rely on extra human resources and
expertise to help construct the system, recent researches focused on how to
build DSMD in a purely data-driven manner. However, the previous data-driven
DSMD methods largely overlooked the system interpretability, which is critical
for a medical application, and they also suffered from the data sparsity issue
at the same time. In this paper, we explore how to bring interpretability to
data-driven DSMD. Specifically, we propose a more interpretable decision
process to implement the dialogue manager of DSMD by reasonably mimicking real
doctors' inquiry logics, and we devise a model with highly transparent
components to conduct the inference. Moreover, we collect a new DSMD dataset,
which has a much larger scale, more diverse patterns and is of higher quality
than the existing ones. The experiments show that our method obtains 7.7%,
10.0%, 3.0% absolute improvement in diagnosis accuracy respectively on three
datasets, demonstrating the effectiveness of its rational decision process and
model design. Our codes and the GMD-12 dataset are available at
https://github.com/lwgkzl/BR-Agent.",https://github.com/lwgkzl/BR-Agent,-1
SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels,0.833861,"Deep neural networks are prone to overfitting noisy labels, resulting in poor
generalization performance. To overcome this problem, we present a simple and
effective method self-ensemble label correction (SELC) to progressively correct
noisy labels and refine the model. We look deeper into the memorization
behavior in training with noisy labels and observe that the network outputs are
reliable in the early stage. To retain this reliable knowledge, SELC uses
ensemble predictions formed by an exponential moving average of network outputs
to update the original noisy labels. We show that training with SELC refines
the model by gradually reducing supervision from noisy labels and increasing
supervision from ensemble predictions. Despite its simplicity, compared with
many state-of-the-art methods, SELC obtains more promising and stable results
in the presence of class-conditional, instance-dependent, and real-world label
noise. The code is available at https://github.com/MacLLL/SELC.",https://github.com/MacLLL/SELC,-1
Network Pruning via Feature Shift Minimization,0.204435,"Channel pruning is widely used to reduce the complexity of deep network
models. Recent pruning methods usually identify which parts of the network to
discard by proposing a channel importance criterion. However, recent studies
have shown that these criteria do not work well in all conditions. In this
paper, we propose a novel Feature Shift Minimization (FSM) method to compress
CNN models, which evaluates the feature shift by converging the information of
both features and filters. Specifically, we first investigate the compression
efficiency with some prevalent methods in different layer-depths and then
propose the feature shift concept. Then, we introduce an approximation method
to estimate the magnitude of the feature shift, since it is difficult to
compute it directly. Besides, we present a distribution-optimization algorithm
to compensate for the accuracy loss and improve the network compression
efficiency. The proposed method yields state-of-the-art performance on various
benchmark networks and datasets, verified by extensive experiments. Our codes
are available at: https://github.com/lscgx/FSM.",https://github.com/lscgx/FSM,-1
Equivariant Self-Supervision for Musical Tempo Estimation,0.905989,"Self-supervised methods have emerged as a promising avenue for representation
learning in the recent years since they alleviate the need for labeled
datasets, which are scarce and expensive to acquire. Contrastive methods are a
popular choice for self-supervision in the audio domain, and typically provide
a learning signal by forcing the model to be invariant to some transformations
of the input. These methods, however, require measures such as negative
sampling or some form of regularisation to be taken to prevent the model from
collapsing on trivial solutions. In this work, instead of invariance, we
propose to use equivariance as a self-supervision signal to learn audio tempo
representations from unlabelled data. We derive a simple loss function that
prevents the network from collapsing on a trivial solution during training,
without requiring any form of regularisation or negative sampling. Our
experiments show that it is possible to learn meaningful representations for
tempo estimation by solely relying on equivariant self-supervision, achieving
performance comparable with supervised methods on several benchmarks. As an
added benefit, our method only requires moderate compute resources and
therefore remains accessible to a wide research community.",https://github.com/Quint-e/equivariant-self-supervision-tempo,152
Misspelling Semantics In Thai,0.183911,"User-generated content is full of misspellings. Rather than being just random
noise, we hypothesise that many misspellings contain hidden semantics that can
be leveraged for language understanding tasks. This paper presents a
fine-grained annotated corpus of misspelling in Thai, together with an analysis
of misspelling intention and its possible semantics to get a better
understanding of the misspelling patterns observed in the corpus. In addition,
we introduce two approaches to incorporate the semantics of misspelling:
Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST).
Experiments on a sentiment analysis task confirm our overall hypothesis:
additional semantics from misspelling can boost the micro F1 score up to
0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",None,21
Symmetry Regularization and Saturating Nonlinearity for Robust Quantization,0.0521038,"Robust quantization improves the tolerance of networks for various
implementations, allowing reliable output in different bit-widths or fragmented
low-precision arithmetic. In this work, we perform extensive analyses to
identify the sources of quantization error and present three insights to
robustify a network against quantization: reduction of error propagation, range
clamping for error minimization, and inherited robustness against quantization.
Based on these insights, we propose two novel methods called symmetry
regularization (SymReg) and saturating nonlinearity (SatNL). Applying the
proposed methods during training can enhance the robustness of arbitrary neural
networks against quantization on existing post-training quantization (PTQ) and
quantization-aware training (QAT) algorithms and enables us to obtain a single
weight flexible enough to maintain the output quality under various conditions.
We conduct extensive studies on CIFAR and ImageNet datasets and validate the
effectiveness of the proposed methods.",None,-1
TripleRE: Knowledge Graph Embeddings via Tripled Relation Vectors,0.84798,"Translation-based knowledge graph embedding has been one of the most
important branches for knowledge representation learning since TransE came out.
Although many translation-based approaches have achieved some progress in
recent years, the performance was still unsatisfactory. This paper proposes a
novel knowledge graph embedding method named TripleRE with two versions. The
first version of TripleRE creatively divide the relationship vector into three
parts. The second version takes advantage of the concept of residual and
achieves better performance. In addition, attempts on using NodePiece to encode
entities achieved promising results in reducing the parametric size, and solved
the problems of scalability. Experiments show that our approach achieved
state-of-the-art performance on the large-scale knowledge graph dataset, and
competitive performance on other datasets.",https://github.com/ZJULearning/TransAt,360
TEVR: Improving Speech Recognition by Token Entropy Variance Reduction,0.115972,"This paper presents TEVR, a speech recognition model designed to minimize the
variation in token entropy w.r.t. to the language model. This takes advantage
of the fact that if the language model will reliably and accurately predict a
token anyway, then the acoustic model doesn't need to be accurate in
recognizing it. We train German ASR models with 900 million parameters and show
that on CommonVoice German, TEVR scores a very competitive 3.64% word error
rate, which outperforms the best reported results by a relative 16.89%
reduction in word error rate. We hope that releasing our fully trained speech
recognition pipeline to the community will lead to privacy-preserving offline
virtual assistants in the future.",https://huggingface.co/fxtentacle/wav2vec2-xls-r-1b-tevr,-1
ILDAE: Instance-Level Difficulty Analysis of Evaluation Data,0.252541,"Knowledge of questions' difficulty level helps a teacher in several ways,
such as estimating students' potential quickly by asking carefully selected
questions and improving quality of examination by modifying trivial and hard
questions. Can we extract such benefits of instance difficulty in NLP? To this
end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE)
in a large-scale setup of 23 datasets and demonstrate its five novel
applications: 1) conducting efficient-yet-accurate evaluations with fewer
instances saving computational cost and time, 2) improving quality of existing
evaluation datasets by repairing erroneous and trivial instances, 3) selecting
the best model based on application requirements, 4) analyzing dataset
characteristics for guiding future data creation, 5) estimating Out-of-Domain
performance reliably. Comprehensive experiments for these applications result
in several interesting findings, such as evaluation using just 5% instances
(selected via ILDAE) achieves as high as 0.93 Kendall correlation with
evaluation using complete dataset and computing weighted accuracy using
difficulty scores leads to 5.2% higher correlation with Out-of-Domain
performance. We release the difficulty scores and hope our analyses and
findings will bring more attention to this important yet understudied field of
leveraging instance difficulty in evaluations.",https://github.com/nrjvarshney/ILDAE,-1
Representation Uncertainty in Self-Supervised Learning as Variational Inference,0.146583,"In this study, a novel self-supervised learning (SSL) method is proposed,
which considers SSL in terms of variational inference to learn not only
representation but also representation uncertainties. SSL is a method of
learning representations without labels by maximizing the similarity between
image representations of different augmented views of an image. Meanwhile,
variational autoencoder (VAE) is an unsupervised representation learning method
that trains a probabilistic generative model with variational inference. Both
VAE and SSL can learn representations without labels, but their relationship
has not been investigated in the past. Herein, the theoretical relationship
between SSL and variational inference has been clarified. Furthermore, a novel
method, namely variational inference SimSiam (VI-SimSiam), has been proposed.
VI-SimSiam can predict the representation uncertainty by interpreting SimSiam
with variational inference and defining the latent space distribution. The
present experiments qualitatively show that VI- SimSiam could learn uncertainty
by comparing input images and predicted uncertainties. Additionally, we
described a relationship between estimated uncertainty and classification
accuracy.",None,-1
Fine-grained Contrastive Learning for Relation Extraction,0.377591,"Recent relation extraction (RE) works have shown encouraging improvements by
conducting contrastive learning on silver labels generated by distant
supervision before fine-tuning on gold labels. Existing methods typically
assume all these silver labels are accurate and treat them equally; however,
distant supervision is inevitably noisy -- some silver labels are more reliable
than others. In this paper, we propose fine-grained contrastive learning
(FineCL) for RE, which leverages fine-grained information about which silver
labels are and are not noisy to improve the quality of learned relationship
representations for RE. We first assess the quality of silver labels via a
simple and automatic approach we call ""learning order denoising,"" where we
train a language model to learn these relations and record the order of learned
training instances. We show that learning order largely corresponds to label
accuracy -- early-learned silver labels have, on average, more accurate labels
than later-learned silver labels. Then, during pre-training, we increase the
weights of accurate labels within a novel contrastive learning objective.
Experiments on several RE benchmarks show that FineCL makes consistent and
significant performance gains over state-of-the-art methods.",https://github.com/wphogan/finecl,4518
"Revisiting Crowd Counting: State-of-the-art, Trends, and Future Perspectives",0.918591,"Crowd counting is an effective tool for situational awareness in public
places. Automated crowd counting using images and videos is an interesting yet
challenging problem that has gained significant attention in computer vision.
Over the past few years, various deep learning methods have been developed to
achieve state-of-the-art performance. The methods evolved over time vary in
many aspects such as model architecture, input pipeline, learning paradigm,
computational complexity, and accuracy gains etc. In this paper, we present a
systematic and comprehensive review of the most significant contributions in
the area of crowd counting. Although few surveys exist on the topic, our survey
is most up-to date and different in several aspects. First, it provides a more
meaningful categorization of the most significant contributions by model
architectures, learning methods (i.e., loss functions), and evaluation methods
(i.e., evaluation metrics). We chose prominent and distinct works and excluded
similar works. We also sort the well-known crowd counting models by their
performance over benchmark datasets. We believe that this survey can be a good
resource for novice researchers to understand the progressive developments and
contributions over time and the current state-of-the-art.",None,-1
Linear Leaky-Integrate-and-Fire Neuron Model Based Spiking Neural Networks and Its Mapping Relationship to Deep Neural Networks,0.782246,"Spiking neural networks (SNNs) are brain-inspired machine learning algorithms
with merits such as biological plausibility and unsupervised learning
capability. Previous works have shown that converting Artificial Neural
Networks (ANNs) into SNNs is a practical and efficient approach for
implementing an SNN. However, the basic principle and theoretical groundwork
are lacking for training a non-accuracy-loss SNN. This paper establishes a
precise mathematical mapping between the biological parameters of the Linear
Leaky-Integrate-and-Fire model (LIF)/SNNs and the parameters of ReLU-AN/Deep
Neural Networks (DNNs). Such mapping relationship is analytically proven under
certain conditions and demonstrated by simulation and real data experiments. It
can serve as the theoretical basis for the potential combination of the
respective merits of the two categories of neural networks.",None,-1
DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering,0.705743,"Spoken Question Answering (SQA) is to find the answer from a spoken document
given a question, which is crucial for personal assistants when replying to the
queries from the users. Existing SQA methods all rely on Automatic Speech
Recognition (ASR) transcripts. Not only does ASR need to be trained with
massive annotated data that are time and cost-prohibitive to collect for
low-resourced languages, but more importantly, very often the answers to the
questions include name entities or out-of-vocabulary words that cannot be
recognized correctly. Also, ASR aims to minimize recognition errors equally
over all words, including many function words irrelevant to the SQA task.
Therefore, SQA without ASR transcripts (textless) is always highly desired,
although known to be very difficult.
  This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging
unlabeled data for pre-training and fine-tuned by the SQA downstream task. The
time intervals of spoken answers can be directly predicted from spoken
documents. We also release a new SQA benchmark corpus, NMSQA, for data with
more realistic scenarios. We empirically showed that DUAL yields results
comparable to those obtained by cascading ASR and text QA model and robust to
real-world data. Our code and model will be open-sourced.",None,-1
Magpie: Automatically Tuning Static Parameters for Distributed File Systems using Deep Reinforcement Learning,0.597459,"Distributed file systems are widely used nowadays, yet using their default
configurations is often not optimal. At the same time, tuning configuration
parameters is typically challenging and time-consuming. It demands expertise
and tuning operations can also be expensive. This is especially the case for
static parameters, where changes take effect only after a restart of the system
or workloads. We propose a novel approach, Magpie, which utilizes deep
reinforcement learning to tune static parameters by strategically exploring and
exploiting configuration parameter spaces. To boost the tuning of the static
parameters, our method employs both server and client metrics of distributed
file systems to understand the relationship between static parameters and
performance. Our empirical evaluation results show that Magpie can noticeably
improve the performance of the distributed file system Lustre, where our
approach on average achieves 91.8% throughput gains against default
configuration after tuning towards single performance indicator optimization,
while it reaches 39.7% more throughput gains against the baseline.",https://github.com/dos-group/magpie,-1
Unsupervised Learning of Temporal Abstractions with Slot-based Transformers,0.28656,"The discovery of reusable sub-routines simplifies decision-making and
planning in complex reinforcement learning problems. Previous approaches
propose to learn such temporal abstractions in a purely unsupervised fashion
through observing state-action trajectories gathered from executing a policy.
However, a current limitation is that they process each trajectory in an
entirely sequential manner, which prevents them from revising earlier decisions
about sub-routine boundary points in light of new incoming information. In this
work we propose SloTTAr, a fully parallel approach that integrates sequence
processing Transformers with a Slot Attention module and adaptive computation
for learning about the number of such sub-routines in an unsupervised fashion.
We demonstrate how SloTTAr is capable of outperforming strong baselines in
terms of boundary point discovery, even for sequences containing variable
amounts of sub-routines, while being up to 7x faster to train on existing
benchmarks.",https://github.com/agopal42/slottar,-1
X-PuDu at SemEval-2022 Task 6: Multilingual Learning for English and Arabic Sarcasm Detection,0.444137,"Detecting sarcasm and verbal irony from people's subjective statements is
crucial to understanding their intended meanings and real sentiments and
positions in social scenarios. This paper describes the X-PuDu system that
participated in SemEval-2022 Task 6, iSarcasmEval - Intended Sarcasm Detection
in English and Arabic, which aims at detecting intended sarcasm in various
settings of natural language understanding. Our solution finetunes pre-trained
language models, such as ERNIE-M and DeBERTa, under the multilingual settings
to recognize the irony from Arabic and English texts. Our system ranked second
out of 43, and ninth out of 32 in Task A: one-sentence detection in English and
Arabic; fifth out of 22 in Task B: binary multi-label classification in
English; first out of 16, and fifth out of 13 in Task C: sentence-pair
detection in English and Arabic.",None,-1
Adversarial Detector with Robust Classifier,0.0758117,"Deep neural network (DNN) models are wellknown to easily misclassify
prediction results by using input images with small perturbations, called
adversarial examples. In this paper, we propose a novel adversarial detector,
which consists of a robust classifier and a plain one, to highly detect
adversarial examples. The proposed adversarial detector is carried out in
accordance with the logits of plain and robust classifiers. In an experiment,
the proposed detector is demonstrated to outperform a state-of-the-art detector
without any robust classifier.",None,-1
Few-Shot Table-to-Text Generation with Prefix-Controlled Generator,0.102137,"Neural table-to-text generation approaches are data-hungry, limiting their
adaptation for low-resource real-world applications. Previous works mostly
resort to Pre-trained Language Models (PLMs) to generate fluent summaries of a
table. However, they often contain hallucinated contents due to the
uncontrolled nature of PLMs. Moreover, the topological differences between
tables and sequences are rarely studied. Last but not least, fine-tuning on
PLMs with a handful of instances may lead to over-fitting and catastrophic
forgetting. To alleviate these problems, we propose a prompt-based approach,
Prefix-Controlled Generator (i.e., PCG), for few-shot table-to-text generation.
We prepend a task-specific prefix for a PLM to make the table structure better
fit the pre-trained input. In addition, we generate an input-specific prefix to
control the factual contents and word order of the generated text. Both
automatic and human evaluations on different domains (humans, books and songs)
of the Wikibio dataset show substantial improvements over baseline approaches.",None,2888
Adversarial Contrastive Learning for Evidence-aware Fake News Detection with Graph Neural Networks,0.72153,"The prevalence and perniciousness of fake news have been a critical issue on
the Internet, which stimulates the development of automatic fake news detection
in turn. In this paper, we focus on evidence-based fake news detection, where
several evidences are utilized to probe the veracity of news (i.e., a claim).
Most previous methods first employ sequential models to embed the semantic
information and then capture the claim-evidence interaction based on attention
mechanisms. Despite their effectiveness, they still suffer from three
weaknesses. Firstly, sequential models fail to integrate the relevant
information that is scattered far apart in evidences. Secondly, they
underestimate much redundant information in evidences may be useless or
harmful. Thirdly, insufficient data utilization limits the separability and
reliability of representations captured by the model. To solve these problems,
we propose a unified Graph-based sEmantic structure mining framework with
ConTRAstive Learning, namely GETRAL in short. Specifically, we first model
claims and evidences as graph-structured data to capture the long-distance
semantic dependency. Consequently, we reduce information redundancy by
performing graph structure learning. Then the fine-grained semantic
representations are fed into the claim-evidence interaction module for
predictions. Finally, an adversarial contrastive learning module is applied to
make full use of data and strengthen representation learning. Comprehensive
experiments have demonstrated the superiority of GETRAL over the
state-of-the-arts and validated the efficacy of semantic mining with graph
structure and contrastive learning.",https://github.com/CRIPAC-DIG/GETRAL,-1
Meta Self-Refinement for Robust Learning with Weak Supervision,0.316988,"Training deep neural networks (DNNs) under weak supervision has attracted
increasing research attention as it can significantly reduce the annotation
cost. However, labels from weak supervision can be noisy, and the high capacity
of DNNs enables them to easily overfit the label noise, resulting in poor
generalization. Recent methods leverage self-training to build noise-resistant
models, in which a teacher trained under weak supervision is used to provide
highly confident labels for teaching the students. Nevertheless, the teacher
derived from such frameworks may have fitted a substantial amount of noise and
therefore produce incorrect pseudo-labels with high confidence, leading to
severe error propagation. In this work, we propose Meta Self-Refinement (MSR),
a noise-resistant learning framework, to effectively combat label noise from
weak supervision. Instead of relying on a fixed teacher trained with noisy
labels, we encourage the teacher to refine its pseudo-labels. At each training
step, MSR performs a meta gradient descent on the current mini-batch to
maximize the student performance on a clean validation set. Extensive
experimentation on eight NLP benchmarks demonstrates that MSR is robust against
label noise in all settings and outperforms state-of-the-art methods by up to
11.4% in accuracy and 9.26% in F1 score.",https://github.com/uds-lsv/msr,-1
Analyzing Wrap-Up Effects through an Information-Theoretic Lens,0.679312,"Numerous analyses of reading time (RT) data have been implemented -- all in
an effort to better understand the cognitive processes driving reading
comprehension. However, data measured on words at the end of a sentence -- or
even at the end of a clause -- is often omitted due to the confounding factors
introduced by so-called ""wrap-up effects,"" which manifests as a skewed
distribution of RTs for these words. Consequently, the understanding of the
cognitive processes that might be involved in these wrap-up effects is limited.
In this work, we attempt to learn more about these processes by examining the
relationship between wrap-up effects and information-theoretic quantities, such
as word and context surprisals. We find that the distribution of information in
prior contexts is often predictive of sentence- and clause-final RTs (while not
of sentence-medial RTs). This lends support to several prior hypotheses about
the processes involved in wrap-up effects.",https://github.com/rycolab/wrap-up-effects,-1
Lighting (In)consistency of Paint by Text,0.389544,"Whereas generative adversarial networks are capable of synthesizing highly
realistic images of faces, cats, landscapes, or almost any other single
category, paint-by-text synthesis engines can -- from a single text prompt --
synthesize realistic images of seemingly endless categories with arbitrary
configurations and combinations. This powerful technology poses new challenges
to the photo-forensic community. Motivated by the fact that paint by text is
not based on explicit geometric or physical models, and the human visual
system's general insensitivity to lighting inconsistencies, we provide an
initial exploration of the lighting consistency of DALL-E-2 synthesized images
to determine if physics-based forensic analyses will prove fruitful in
detecting this new breed of synthetic media.",None,-1
Optimal Fine-Grained N:M sparsity for Activations and Neural Gradients,0.479383,"In deep learning, fine-grained N:M sparsity reduces the data footprint and
bandwidth of a General Matrix multiply (GEMM) by x2, and doubles throughput by
skipping computation of zero values. So far, it was only used to prune weights.
We examine how this method can be used also for activations and their gradients
(i.e., ""neural gradients""). To this end, we first establish a tensor-level
optimality criteria. Previous works aimed to minimize the mean-square-error
(MSE) of each pruned block. We show that while minimization of the MSE works
fine for pruning the activations, it catastrophically fails for the neural
gradients. Instead, we show that optimal pruning of the neural gradients
requires an unbiased minimum-variance pruning mask. We design such specialized
masks, and find that in most cases, 1:2 sparsity is sufficient for training,
and 2:4 sparsity is usually enough when this is not the case. Further, we
suggest combining several such methods together in order to potentially speed
up training even more. A reference implementation is supplied in
https://github.com/brianchmiel/Act-and-Grad-structured-sparsity.",https://github.com/brianchmiel/Act-and-Grad-structured-sparsity,-1
Formal Algorithms for Transformers,0.703051,"This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs.",None,-1
Curriculum-Based Self-Training Makes Better Few-Shot Learners for Data-to-Text Generation,0.133377,"Despite the success of text-to-text pre-trained models in various natural
language generation (NLG) tasks, the generation performance is largely
restricted by the number of labeled data in downstream tasks, particularly in
data-to-text generation tasks. Existing works mostly utilize abundant unlabeled
structured data to conduct unsupervised pre-training for task adaption, which
fail to model the complex relationship between source structured data and
target texts. Thus, we introduce self-training as a better few-shot learner
than task-adaptive pre-training, which explicitly captures this relationship
via pseudo-labeled data generated by the pre-trained model. To alleviate the
side-effect of low-quality pseudo-labeled data during self-training, we propose
a novel method called Curriculum-Based Self-Training (CBST) to effectively
leverage unlabeled data in a rearranged order determined by the difficulty of
text generation. Experimental results show that our method can outperform
fine-tuning and task-adaptive pre-training methods, and achieve
state-of-the-art performance in the few-shot setting of data-to-text
generation.",https://github.com/kepei1106/CBST,-1
Parameter-Parallel Distributed Variational Quantum Algorithm,0.149433,"Variational quantum algorithms (VQAs) have emerged as a promising near-term
technique to explore practical quantum advantage on noisy intermediate-scale
quantum (NISQ) devices. However, the inefficient parameter training process due
to the incompatibility with backpropagation and the cost of a large number of
measurements, posing a great challenge to the large-scale development of VQAs.
Here, we propose a parameter-parallel distributed variational quantum algorithm
(PPD-VQA), to accelerate the training process by parameter-parallel training
with multiple quantum processors. To maintain the high performance of PPD-VQA
in the realistic noise scenarios, a alternate training strategy is proposed to
alleviate the acceleration attenuation caused by noise differences among
multiple quantum processors, which is an unavoidable common problem of
distributed VQA. Besides, the gradient compression is also employed to overcome
the potential communication bottlenecks. The achieved results suggest that the
PPD-VQA could provide a practical solution for coordinating multiple quantum
processors to handle large-scale real-word applications.",None,-1
On Gap-dependent Bounds for Offline Reinforcement Learning,0.325363,"This paper presents a systematic study on gap-dependent sample complexity in
offline reinforcement learning. Prior work showed when the density ratio
between an optimal policy and the behavior policy is upper bounded (the optimal
policy coverage assumption), then the agent can achieve an
$O\left(\frac{1}{\epsilon^2}\right)$ rate, which is also minimax optimal. We
show under the optimal policy coverage assumption, the rate can be improved to
$O\left(\frac{1}{\epsilon}\right)$ when there is a positive sub-optimality gap
in the optimal $Q$-function. Furthermore, we show when the visitation
probabilities of the behavior policy are uniformly lower bounded for states
where an optimal policy's visitation probabilities are positive (the uniform
optimal policy coverage assumption), the sample complexity of identifying an
optimal policy is independent of $\frac{1}{\epsilon}$. Lastly, we present
nearly-matching lower bounds to complement our gap-dependent upper bounds.",None,-1
RuArg-2022: Argument Mining Evaluation,0.425209,"Argumentation analysis is a field of computational linguistics that studies
methods for extracting arguments from texts and the relationships between them,
as well as building argumentation structure of texts. This paper is a report of
the organizers on the first competition of argumentation analysis systems
dealing with Russian language texts within the framework of the Dialogue
conference. During the competition, the participants were offered two tasks:
stance detection and argument classification. A corpus containing 9,550
sentences (comments on social media posts) on three topics related to the
COVID-19 pandemic (vaccination, quarantine, and wearing masks) was prepared,
annotated, and used for training and testing. The system that won the first
place in both tasks used the NLI (Natural Language Inference) variant of the
BERT architecture, automatic translation into English to apply a specialized
BERT model, retrained on Twitter posts discussing COVID-19, as well as
additional masking of target entities. This system showed the following
results: for the stance detection task an F1-score of 0.6968, for the argument
classification task an F1-score of 0.7404. We hope that the prepared dataset
and baselines will help to foster further research on argument mining for the
Russian language.",https://github.com/dialogue-evaluation/RuArg,-1
InterpretTime: a new approach for the systematic evaluation of neural-network interpretability in time series classification,0.0876221,"We present a novel approach to evaluate the performance of interpretability
methods for time series classification, and propose a new strategy to assess
the similarity between domain experts and machine data interpretation. The
novel approach leverages a new family of synthetic datasets and introduces new
interpretability evaluation metrics. The approach addresses several common
issues encountered in the literature, and clearly depicts how well an
interpretability method is capturing neural network's data usage, providing a
systematic interpretability evaluation framework. The new methodology
highlights the superiority of Shapley Value Sampling and Integrated Gradients
for interpretability in time-series classification tasks.",https://github.com/hturbe/InterpretTime,-1
Detection of Tool based Edited Images from Error Level Analysis and Convolutional Neural Network,0.222681,"Image Forgery is a problem of image forensics and its detection can be
leveraged using Deep Learning. In this paper we present an approach for
identification of authentic and tampered images done using image editing tools
with Error Level Analysis and Convolutional Neural Network. The process is
performed on CASIA ITDE v2 dataset and trained for 50 and 100 epochs
respectively. The respective accuracies of the training and validation sets are
represented using graphs.",None,-1
Multimodality Multi-Lead ECG Arrhythmia Classification using Self-Supervised Learning,0.123508,"Electrocardiogram (ECG) signal is one of the most effective sources of
information mainly employed for the diagnosis and prediction of cardiovascular
diseases (CVDs) connected with the abnormalities in heart rhythm. Clearly,
single modality ECG (i.e. time series) cannot convey its complete
characteristics, thus, exploiting both time and time-frequency modalities in
the form of time-series data and spectrogram is needed. Leveraging the
cutting-edge self-supervised learning (SSL) technique on unlabeled data, we
propose SSL-based multimodality ECG classification. Our proposed network
follows SSL learning paradigm and consists of two modules corresponding to
pre-stream task, and down-stream task, respectively. In the SSL-pre-stream
task, we utilize self-knowledge distillation (KD) techniques with no labeled
data, on various transformations and in both time and frequency domains. In the
down-stream task, which is trained on labeled data, we propose a gate fusion
mechanism to fuse information from multimodality.To evaluate the effectiveness
of our approach, ten-fold cross validation on the 12-lead PhysioNet 2020
dataset has been conducted.",https://github.com/UARK-AICV/ECG SSL,6361
MLP-Hash: Protecting Face Templates via Hashing of Randomized Multi-Layer Perceptron,0.396402,"Applications of face recognition systems for authentication purposes are
growing rapidly. Although state-of-the-art (SOTA) face recognition systems have
high recognition accuracy, the features which are extracted for each user and
are stored in the system's database contain privacy-sensitive information.
Accordingly, compromising this data would jeopardize users' privacy. In this
paper, we propose a new cancelable template protection method, dubbed MLP-hash,
which generates protected templates by passing the extracted features through a
user-specific randomly-weighted multi-layer perceptron (MLP) and binarizing the
MLP output. We evaluated the unlinkability, irreversibility, and recognition
accuracy of our proposed biometric template protection method to fulfill the
ISO/IEC 30136 standard requirements. Our experiments with SOTA face recognition
systems on the MOBIO and LFW datasets show that our method has competitive
performance with the BioHashing and IoM Hashing (IoM-GRP and IoM-URP) template
protection algorithms. We provide an open-source implementation of all the
experiments presented in this paper so that other researchers can verify our
findings and build upon our work.",https://gitlab.idiap.ch/bob/bob.bio.face,-1
Enhanced Bi-directional Motion Estimation for Video Frame Interpolation,0.83706,"We present a novel simple yet effective algorithm for motion-based video
frame interpolation. Existing motion-based interpolation methods typically rely
on a pre-trained optical flow model or a U-Net based pyramid network for motion
estimation, which either suffer from large model size or limited capacity in
handling complex and large motion cases. In this work, by carefully integrating
intermediateoriented forward-warping, lightweight feature encoder, and
correlation volume into a pyramid recurrent framework, we derive a compact
model to simultaneously estimate the bidirectional motion between input frames.
It is 15 times smaller in size than PWC-Net, yet enables more reliable and
flexible handling of challenging motion cases. Based on estimated
bi-directional motion, we forward-warp input frames and their context features
to intermediate frame, and employ a synthesis network to estimate the
intermediate frame from warped representations. Our method achieves excellent
performance on a broad range of video frame interpolation benchmarks. Code and
trained models are available at \url{https://github.com/srcn-ivl/EBME}.",https://github.com/srcn-ivl/EBME,-1
Hyperbolic Vision Transformers: Combining Improvements in Metric Learning,0.819661,"Metric learning aims to learn a highly discriminative model encouraging the
embeddings of similar classes to be close in the chosen metrics and pushed
apart for dissimilar ones. The common recipe is to use an encoder to extract
embeddings and a distance-based loss function to match the representations --
usually, the Euclidean distance is utilized. An emerging interest in learning
hyperbolic data embeddings suggests that hyperbolic geometry can be beneficial
for natural data. Following this line of work, we propose a new
hyperbolic-based model for metric learning. At the core of our method is a
vision transformer with output embeddings mapped to hyperbolic space. These
embeddings are directly optimized using modified pairwise cross-entropy loss.
We evaluate the proposed model with six different formulations on four datasets
achieving the new state-of-the-art performance. The source code is available at
https://github.com/htdt/hyp_metric.",https://github.com/htdt/hyp_metric,-1
Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation,0.641134,"Federated learning (FL) can be essential in knowledge representation,
reasoning, and data mining applications over multi-source knowledge graphs
(KGs). A recent study FedE first proposes an FL framework that shares entity
embeddings of KGs across all clients. However, entity embedding sharing from
FedE would incur a severe privacy leakage. Specifically, the known entity
embedding can be used to infer whether a specific relation between two entities
exists in a private client. In this paper, we introduce a novel attack method
that aims to recover the original data based on the embedding information,
which is further used to evaluate the vulnerabilities of FedE. Furthermore, we
propose a Federated learning paradigm with privacy-preserving Relation
embedding aggregation (FedR) to tackle the privacy issue in FedE. Besides,
relation embedding sharing can significantly reduce the communication cost due
to its smaller size of queries. We conduct extensive experiments to evaluate
FedR with five different KG embedding models and three datasets. Compared to
FedE, FedR achieves similar utility and significant improvements regarding
privacy-preserving effect and communication efficiency on the link prediction
task.",None,23079
Annotating Norwegian Language Varieties on Twitter for Part-of-Speech,0.739761,"Norwegian Twitter data poses an interesting challenge for Natural Language
Processing (NLP) tasks. These texts are difficult for models trained on
standardized text in one of the two Norwegian written forms (Bokm{\aa}l and
Nynorsk), as they contain both the typical variation of social media text, as
well as a large amount of dialectal variety. In this paper we present a novel
Norwegian Twitter dataset annotated with POS-tags. We show that models trained
on Universal Dependency (UD) data perform worse when evaluated against this
dataset, and that models trained on Bokm{\aa}l generally perform better than
those trained on Nynorsk. We also see that performance on dialectal tweets is
comparable to the written standards for some models. Finally we perform a
detailed analysis of the errors that models commonly make on this data.",https://github.com/noklesta/,-1
Positive-Unlabeled Learning with Adversarial Data Augmentation for Knowledge Graph Completion,0.812222,"Most real-world knowledge graphs (KG) are far from complete and
comprehensive. This problem has motivated efforts in predicting the most
plausible missing facts to complete a given KG, i.e., knowledge graph
completion (KGC). However, existing KGC methods suffer from two main issues, 1)
the false negative issue, i.e., the sampled negative training instances may
include potential true facts; and 2) the data sparsity issue, i.e., true facts
account for only a tiny part of all possible facts. To this end, we propose
positive-unlabeled learning with adversarial data augmentation (PUDA) for KGC.
In particular, PUDA tailors positive-unlabeled risk estimator for the KGC task
to deal with the false negative issue. Furthermore, to address the data
sparsity issue, PUDA achieves a data augmentation strategy by unifying
adversarial training and positive-unlabeled learning under the
positive-unlabeled minimax game. Extensive experimental results on real-world
benchmark datasets demonstrate the effectiveness and compatibility of our
proposed method.",https://github.com/lilv98/PUDA-IJCAI22,-1
Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,0.999935,"Social intelligence and Theory of Mind (ToM), i.e., the ability to reason
about the different mental states, intents, and reactions of all people
involved, allow humans to effectively navigate and understand everyday social
interactions. As NLP systems are used in increasingly complex social
situations, their ability to grasp social dynamics becomes crucial. In this
work, we examine the open question of social intelligence and Theory of Mind in
modern NLP systems from an empirical and theory-based perspective. We show that
one of today's largest language models (GPT-3; Brown et al., 2020) lacks this
kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et
al., 2019), which measures models' ability to understand intents and reactions
of participants of social interactions, and ToMi (Le et al., 2019), which
measures whether models can infer mental states and realities of participants
of situations. Our results show that models struggle substantially at these
Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on
SocialIQa and ToMi, respectively. To conclude, we draw on theories from
pragmatics to contextualize this shortcoming of large language models, by
examining the limitations stemming from their data, neural architecture, and
training paradigms. Challenging the prevalent narrative that only scale is
needed, we posit that person-centric NLP approaches might be more effective
towards neural Theory of Mind.
  In our updated version, we also analyze newer instruction tuned and RLFH
models for neural ToM. We find that even ChatGPT and GPT-4 do not display
emergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on
the ToMi questions related to mental states and realities.",None,-1
Quantification of emotions in decision making,0.249611,"The problem of quantification of emotions in the choice between alternatives
is considered. The alternatives are evaluated in a dual manner. From one side,
they are characterized by rational features defining the utility of each
alternative. From the other side, the choice is affected by emotions labeling
the alternatives as attractive or repulsive, pleasant or unpleasant. A decision
maker needs to make a choice taking into account both these features, the
utility of alternatives and their attractiveness. The notion of utility is
based on rational grounds, while the notion of attractiveness is vague and
rather is based on irrational feelings. A general method, allowing for the
quantification of the choice combining rational and emotional features is
described. Despite that emotions seem to avoid precise quantification, their
quantitative evaluation is possible at the aggregate level. The analysis of a
series of empirical data demonstrates the efficiency of the approach, including
the realistic behavioral problems that cannot be treated by the standard
expected utility theory.",None,-1
Learning 3D Scene Priors with 2D Supervision,0.465831,"Holistic 3D scene understanding entails estimation of both layout
configuration and object geometry in a 3D environment. Recent works have shown
advances in 3D scene estimation from various input modalities (e.g., images, 3D
scans), by leveraging 3D supervision (e.g., 3D bounding boxes or CAD models),
for which collection at scale is expensive and often intractable. To address
this shortcoming, we propose a new method to learn 3D scene priors of layout
and shape without requiring any 3D ground truth. Instead, we rely on 2D
supervision from multi-view RGB images. Our method represents a 3D scene as a
latent vector, from which we can progressively decode to a sequence of objects
characterized by their class categories, 3D bounding boxes, and meshes. With
our trained autoregressive decoder representing the scene prior, our method
facilitates many downstream applications, including scene synthesis,
interpolation, and single-view reconstruction. Experiments on 3D-FRONT and
ScanNet show that our method outperforms state of the art in single-view
reconstruction, and achieves state-of-the-art results in scene synthesis
against baselines which require for 3D supervision.",https://yinyunie.github.io/sceneprior-page/,-1
OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers,0.918396,"We present OSFormer, the first one-stage transformer framework for
camouflaged instance segmentation (CIS). OSFormer is based on two key designs.
First, we design a location-sensing transformer (LST) to obtain the location
label and instance-aware parameters by introducing the location-guided queries
and the blend-convolution feedforward network. Second, we develop a
coarse-to-fine fusion (CFF) to merge diverse context information from the LST
encoder and CNN backbone. Coupling these two components enables OSFormer to
efficiently blend local features and long-range context dependencies for
predicting camouflaged instances. Compared with two-stage frameworks, our
OSFormer reaches 41% AP and achieves good convergence efficiency without
requiring enormous training data, i.e., only 3,040 samples under 60 epochs.
Code link: https://github.com/PJLallen/OSFormer.",https://github.com/PJLallen/OSFormer,220689
